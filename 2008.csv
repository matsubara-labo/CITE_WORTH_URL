sentence
Automatic processing of medical dictations poses a significant challenge.
"We approach the problem by introducing a statistical frame-work capable of identifying types and bound-aries of sections, lists and other structures occurring in a dictation, thereby gaining ex-plicit knowledge about the function of such elements."
Training data is created semi-automatically by aligning a parallel corpus of corrected medical reports and correspond-ing transcripts generated via automatic speech recognition.
"We highlight the properties of our statistical framework, which is based on conditional random fields (CRFs) and im-plemented as an efficient, publicly available toolkit."
"Finally, we show that our approach is effective both under ideal conditions and for real-life dictation involving speech recog-nition errors and speech-related phenomena such as hesitation and repetitions."
"It is quite common to dictate reports and leave the typing to typists – especially for the medical domain, where every consultation or treatment has to be doc-umented."
Automatic Speech Recognition (ASR) can support professional typists in their work by provid-ing a transcript of what has been dictated.
"However, manual corrections are still needed."
"In particular, speech recognition errors have to be corrected."
"Fur-thermore, speaker errors, such as hesitations or rep-etitions, and instructions to the transcriptionist have to be removed."
"Finally, and most notably, proper structuring and formatting of the report has to be performed."
"For the medical domain, fairly clear guidelines exist with regard to what has to be dic-tated, and how it should be arranged."
"Thus, missing headings may have to be inserted, sentences must be grouped into paragraphs in a meaningful way, enu-meration lists may have to be introduced, and so on."
The goal of the work presented here was to ease the job of the typist by formatting the dictation ac-cording to its structure and the formatting guide-lines.
The prerequisite for this task is the identifi-cation of the various structural elements in the dic-tation which will be be described in this paper.
"Figure 1 shows a fragment of a typical report as recognized by ASR, exemplifying some of the prob-lems we have to deal with: • Punctuation and enumeration markers may be dictated or not, thus sentence boundaries and numbered items often have to be inferred; • the same holds for (sub)section headings; • finally, recognition errors complicate the task."
"Dehydration, weakness and diarrhea."
HISTORY OF PRESENT ILLNESS
Mr. Wilson is a 81-year-old Caucasian gentleman who came in here with fever and persistent diarrhea.
He was sent to the emergency department by his primary care physician due to him being dehydrated. ...
"He is alert and oriented times three, not in acute distress."
VITAL SIGNS: Stable. ...
1 B EGIN - I NSIDE - O UTSIDE
Chronic diarrhea with dehydration.
He also has hypokalemia. 2.
"Thromboctopenia, probably due to liver cirrhosis. ..."
PLAN AND DISCUSSION
The plan was discussed with the patient in detail.
Will transfer him to a nursing facility for further care. ...
Fig. 2: A typical medical report
"When properly edited and formatted, the same dictation appears significantly more comprehensi-ble, as can be seen in figure 2."
"In order to arrive at this result it is necessary to identify the inherent structure of the dictation, i.e. the various hierarchi-cally nested segments."
We will recast the segmenta-tion problem as a multi-tiered tagging problem and show that indeed a good deal of the structure of med-ical dictations can be revealed.
"The main contributions of our paper are as fol-lows: First, we introduce a generic approach that can be integrated seamlessly with existing ASR solu-tions and provides structured output for medical dic-tations."
"Second, we provide a freely available toolkit for factorial conditional random fields (CRFs) that forms the basis of aforementioned approach and is also applicable to numerous other problems (see sec-tion 6)."
"The structure recognition problem dealt with here is closely related to the field of linear text segmen-tation with the goal to partition text into coherent blocks, but on a single level."
"Thus, our task general-izes linear text segmentation to multiple levels."
"A meanwhile classic approach towards domain-independent linear text segmentation, C99, is pre-sented[REF_CITE]."
C99 is the baseline which many current algorithms are compared to.
"Choi’s al-gorithm surpasses previous work[REF_CITE], who proposed the Texttiling algorithm."
The best re-sults published to date are – to the best of our knowl-edge – those[REF_CITE].
"The automatic detection of (sub)section topics plays an important role in our work, since changes of topic indicate a section boundary and appropri-ate headings can be derived from the section type."
Topic detection is usually performed using methods similar to those of text classification (see[REF_CITE]for a survey).
"Thus, the aims of his work are similar to ours."
"However, he is not concerned with the more fine-grained elements, and also uses a different machinery."
"When dealing with tagging problems, statistical frameworks such as HMMs[REF_CITE]or, re-cently, CRFs[REF_CITE]are most com-monly applied."
"Whereas HMMs are generative models, CRFs are discriminative models that can in-corporate rich features."
"However, other approaches to text segmentation have also been pursued."
"E.g.,[REF_CITE]present a model based on multilabel classification, allowing for natural han-dling of overlapping or non-contiguous segments."
"Finally, the work[REF_CITE]bears similarities to ours."
"They apply CRFs to the pars-ing of hierarchical lists and outlines in handwritten notes, and thus have the same goal of finding deep structure using the same probabilistic framework."
"For representing our segmentation problem we use a trick that is well-known from chunking and named entity recognition, and recast the problem as a tag-ging problem in the so-called BIO 1 notation."
"Since we want to assign a type to every segment, O UTSIDE labels are not needed."
"However, we perform seg- tokens level 1 &lt; level 2 &lt; level 3 &lt; ... step t 1 B-T 1 B-T 3 B-T 4 ... t 2 I-T 3 I-T 1 I-T 4 ... t 3 B-T 2 I-T 4 I-T 3 ... t 4 I-T 2 I-T 3 I-T 4 ... t 5 B-T 2 B-T 3 I-T 4 ... t 6 I-T 2 I-T 3 I-T 4 ... ... ... ... ... ..."
"Fig. 3: Multi-level segmentation as tagging problem mentation on multiple levels, therefore multiple la-bel chains are required."
"Furthermore, we also want to assign types to certain segments, thus the labels need an encoding for the type of segment they rep-resent."
"Figure 3 illustrates this representation: B-T i denotes the beginning of a segment of type T i , while I-T i indicates that the segment of type T i continues."
"By adding label chains, it is possible to group the segments of the previous chain into coarser units."
Tree-like structures of unlimited depth can be ex-pressed this way [Footnote_2] .
"2 Note, that since we omit a redundant top-level chain, this structure technically is a hedge rather than a tree."
The gray lines in figure 3 denote dependencies between nodes.
Node labels also de-pend on the input token sequence in an arbitrarily wide context window.
"The raw data available to us consists of two paral-lel corpora of 2007 reports from the area of medi-cal consultations, dictated by physicians."
"The first corpus, C RCG , consists of the raw output of ASR (figure 1), the other one, C COR , contains the corre-sponding corrected and formatted reports (figure 2)."
"In order to arrive at an annotated corpus in a for- mat suitable for the tagging problem, we first have to analyze the report structure and define appropri-ate labels for each segmentation level."
"Then, every token has to be annotated with the appropriate begin or inside labels."
"A report has 625 tokens on average, so the manual annotation of roughly 1.25 million to-kens seemed not to be feasible."
Thus we decided to produce the annotations programmatically and re-strict manual work to corrections.
"When inspecting reports in C COR , a human reader can easily identify the various elements a report con-sists of, such as headings – written in bold on a sepa-rate line – introducing sections, subheadings – writ-ten in bold followed by a colon – introducing sub-sections, and enumerations starting with indented numbers followed by a period."
"Going down further, there are paragraphs divided into sentences."
"Using these structuring elements, a hierarchic data struc-ture comprising all report elements can be induced."
Sections and subsections are typed according to their heading.
"There exist clear recommendations on structuring medical reports, such[REF_CITE]-02[REF_CITE]."
"However, actual med-ical reports still vary greatly with regard to their structure."
"Using the aforementioned standard, we assigned the (sub)headings that actually appeared in the data to the closest type, introducing new types only when absolutely necessary."
"Finally we arrived at a structure model with three label chains: • Sentence level, with 4 labels: Heading, Subheading, Sentence, Enummarker • Subsection level, with 45 labels: Paragraph, Enumelement,[REF_CITE]subsection types (e.g. VitalSigns, Cardiovascular ...) • Section level, with 23 section types (e.g. ReasonForEncounter, Findings, Plan ...)"
Since the reports in C COR are manually edited they are reliable to parse.
We employed a broad-coverage dictionary (handling also multi-word terms) and a domain-specific grammar for parsing and layout in-formation.
A regular heading grammar was used for mapping (sub)headings to the defined (sub)section labels (for details see[REF_CITE]).
The output of the parser is a hedge data structure from which the annotation labels can be derived easily.
"However, our goal is to develop a model for rec-ognizing the report structure from the dictation, thus we have to map the newly created annotation of re-ports in C COR onto the corresponding reports in C RCG ."
The basic idea here is to align the tokens of C COR with the tokens in C RCG and to copy the annotations (cf. figure 4 3 ).
"There are some peculiar-ities we have to take care of during alignment: 1. non-dictated items in C COR (e.g. punctuation, headings) 2. dictated words that do not occur in C COR (meta instructions, repetitions) [Footnote_3]. non-identical but corresponding items (recog-nition errors, reformulations)"
3 This approach can easily be generalized to multiple label chains.
"Since it is particularly necessary to correctly align items of the third group, standard string-edit dis-tance based methods[REF_CITE]need to be augmented."
Therefore we use a more sophisticated cost function.
"It assigns tokens that are similar (ei-ther from a semantic or phonetic point of view) a low cost for substitution, whereas dissimilar tokens re-ceive a prohibitively expensive score."
Costs for dele-tion and insertion are assigned inversely.
Seman-tic similarity is computed using Wordnet[REF_CITE]and UMLS[REF_CITE].
"For pho-netic matching, the Metaphone algorithm[REF_CITE]was used (for details see[REF_CITE])."
The annotation discussed above is the first step to-wards building a training corpus for a CRF-based approach.
"What remains to be done is to provide ob-servations for each time step of the observed entity, i.e. for each token of a report; these are expected to give hints with regard to the annotation labels that are to be assigned to the time step."
"The observa-tions, associated with one or more annotation labels, are usually called features in the machine learning literature."
"During CRF training, the parameters of these features are determined such that they indicate the significance of the observations for a certain la-bel or label combination; this is the basis for later tagging of unseen reports."
"We use the following features for each time step of the reports in C COR and C RCG : • Lexical features covering the local context of ± 2 tokens (e.g., patient@0, the@-1, is@1) • Syntactic features indicating the possible syn-tactic categories of the tokens (e.g., NN@0, JJ@0, DT@-1 and be+VBZ+aux@1) • Bag-of-word (BOW) features intend to cap-ture the topic of a text segment in a wider context of ± 10 tokens, without encoding any order."
"Tokens are lemmatized and replaced by their UMLS concept IDs, if available, and weighed by TF."
"Thus, different words describ-ing the same concept are considered equal. • Semantic type features as above, but using UMLS semantic types instead of concept IDs provide a coarser level of description. • Relative position features: The report is di-vided into eight parts corresponding to eight bi-nary features; only the feature corresponding to the part of the current time step is set."
Conditional random fields[REF_CITE]are conditional models in the exponential family.
"They can be considered a generalization of multinomial logistic regression to output with non-trivial internal structure, such as sequences, trees or other graphical models."
We loosely follow the general notation[REF_CITE]in our presentation.
"Assuming an undirected graphical model G over an observed entity x and a set of discrete, inter-dependent random variables [Footnote_4] y, a conditional ran-dom field describes the conditional distribution: p(y|x; θ) = 1 Y φ (1) c (y c , x; θ c )"
"4 In our case, the discrete outcomes of the random variables y correspond to the annotation labels described in the previous section."
"The normalization term Z(x) sums over all possible joint outcomes of y, i.e.,"
Z(x) = X p(y 0 |x; θ) (2) y 0 and ensures the probabilistic interpretation of p(y|x).
The graphical model G describes interde-pendencies between the variables y; we can then model p(y|x) via factors φ c (·) that are defined over cliques c ∈ G.
"The factors φ c (·) are computed from sufficient statistics {f ck (·)} of the distribution (cor-responding to the features mentioned in the previous section) and depend on possibly overlapping sets of parameters θ c ⊆ θ which together form the param-eters θ of the conditional distribution:  |θ c |  φ c (y c , x; θ c ) = exp X λ ck f ck (x, y c )  (3) k=1"
"In practice, for efficiency reasons, independence as-sumptions have to be made about variables y ∈ y, so G is restricted to small cliques (say, (|c| ≤ 3)."
"Thus, the sufficient statistics only depend on a lim-ited number of variables y c ⊆ y; they can, however, access the whole observed entity x."
"This is in con-trast to generative approaches which model a joint distribution p(x, y) and therefore have to extend the independence assumptions to elements x ∈ x."
"The factor-specific parameters θ c of a CRF are typically tied for certain cliques, according to the problem structure (i.e., θ c 1 = θ c 2 for two cliques c 1 ,c 2 with tied parameters)."
"E.g., parameters are usually tied across time if G is a sequence."
"The factors can then be partitioned into a set of clique templates C = {C 1 , C 2 , . . ."
"C P }, where each clique template C p is a set of factors with tied parameters θ p and corresponding sufficient statistics {f pk (·)}."
The CRF can thus be rewritten as: 1 p(y|x) =
"Y Y φ c (y c , x; θ p ) (4) Z(x) C p ∈C φ c ∈C p"
"Furthermore, in practice, the sufficient statistics {f pk (·)} are computed from a subset x c ⊆ x that is relevant to a factor φ c (·)."
"In a sequence labelling task, tokens x ∈ x that are in temporal proximity to an output variable y ∈ y are typically most useful."
"Nevertheless, in our notation, we will let factors de-pend on the whole observed entity x to denote that all of x can be accessed if necessary."
"For our structure recognition task, the graphical model G exhibits the structure shown in figure 3, i.e., there are multiple connected chains of variables with factors defined over single-node cliques and two-node cliques within and between chains; the pa-rameters of factors are tied across time."
This corre-sponds to the factorial CRF structure described[REF_CITE].
"Structure recognition using conditional random fields then involves two separate steps: parameter estimation, or training, is concerned with selecting the parameters of a CRF such that they fit the given training data."
"Prediction, or testing, determines the best label assignment for unknown examples."
"Given IID training data D = {x (i) , y (i) } Ni=1 , param-eter estimation determines:"
"N ! θ ∗ = argmax X p(y (i) |x (i) ; θ 0 ) (5) θ 0 i i.e., those parameters that maximize the conditional probability of the CRF given the training data."
"In the following, we will not explicitly sum over N i=1 ;[REF_CITE]note, the train-ing instances x (i) ,y (i) can be considered discon-nected components of a single undirected model G."
We thus assume G and its factors φ c (·) to extend over all training instances.
"Unfortunately, (5) cannot be solved analytically."
"Typically, one performs max-imum likelihood estimation (MLE) by maximizing the conditional log-likelihood numerically: |θ p | `(θ) = X X X λ pk f pk (x, y c ) − log Z(x) C p ∈C φ c ∈C p k=1 (6)"
"Currently, limited-memory gradient-based methods such as LBFGS[REF_CITE]are most com-monly employed for that purpose [Footnote_5] ."
"5 Recently, stochastic gradient descent methods such as On-line LBFGS[REF_CITE]have been shown to per-form competitively."
"These require the partial derivatives of (6), which are given by: ∂` = X f pk (x, y c ) − X f pk (x, y 0c )p(y 0c |x) ∂λ pk φ c ∈C p y c0 (7) and expose the intuitive form of a difference be-tween the expectation of a sufficient statistic accord-ing to the empiric distribution and the expectation according to the model distribution."
"The latter term requires marginal probabilities for each clique c, de-noted by p(y 0c |x)."
Inference on the graphical model G (see sec 5.2) is needed to compute these.
"Depending on the structure of G, inference can be very expensive."
"In order to speed up parameter es-timation, which requires inference to be performed for every training example and for every iteration of the gradient-based method, alternatives to MLE have been proposed that do not require inference."
"We show here a factor-based variant of pseudolike-lihood as proposed[REF_CITE]: ` p (θ) = X X log p(y c |x,MB(φ c )) (8) C p ∈C φ c ∈C p where the factors are conditioned on the Markov blanket, denoted by MB [Footnote_6] ."
"6 Here, the Markov blanket of a factor φ c denotes the set of variables occurring in factors that share variables with φ c , non-inclusive of the variables of φ c"
"The gradient of (8) can be computed similar to (7), except that the marginals p c (y 0c |x) are also conditioned on the Markov blan-ket, i.e., p c (y 0c |x, MB(φ c ))."
"Due to its dependence on the Markov blanket of factors, pseudolikelihood cannot be applied to prediction, but only to param-eter estimation, where the “true” assignment of a blanket is known."
We employ a Gaussian prior for training of CRFs in order to avoid overfitting.
"Hence, if f(θ) is the original objective function (e.g., log-likelihood or log-pseudolikelihood), we optimize a penalized ver-sion f 0 (θ) instead, such that: |θ| f 0 (θ) = f(θ) − X λ k and ∂f 0 = ∂f − λ k . 2 2σ 2 σ 2 ∂λ k ∂λ k k=1"
The tuning parameter σ 2 determines the strength of the penalty; lower values lead to less overfitting.
Gaussian priors are a common choice for parame-ter estimation of log-linear models ( cf.[REF_CITE]).
"Inference on a graphical model G is needed to ef-ficiently compute the normalization term Z(x) and marginals p c (y 0c |x) for MLE, cf. equation (6)."
"Using belief propagati[REF_CITE], more precisely its sum-product variant, we can com-pute the beliefs for all cliques c ∈"
"G. In a tree-shaped graphical model G, these beliefs correspond exactly to the marginal probabilities p c (y 0c |x)."
"How-ever, if the graph contains cycles, so-called loopy belief propagation must be performed."
The mes-sage updates are then re-iterated according to some schedule until the messages converge.
We use a TRP schedule as described[REF_CITE].
The resulting beliefs are then only approximations to the true marginals.
"Moreover, loopy belief propa-gation is not guaranteed to terminate in general – we investigate this phenomenon in section 6.5."
"With regard to the normalization term Z(x), as equation (2) shows, naive computation requires summing over all assignments of y."
This is too ex-pensive to be practical.
"Fortunately, belief propaga-tion produces an alternative factorization of p(y|x); i.e., the conditional distribution defining the CRF can be expressed in terms of the marginals gained during sum-product belief propagation."
"This repre-sentation does not require any additional normaliza-tion, so Z(x) need not be computed."
"Once the parameters θ have been estimated from training data, a CRF can be used to predict the la-bels of unknown examples."
"The goal is to find: y ∗ = argmax p(y 0 |x;θ) (9) y 0 i.e., the assignment of y that maximizes the condi-tional probability of the CRF."
"Again, naive computa-tion of (9) is intractable."
"However, the max-product variant of loopy belief propagation can be applied to approximately find the MAP assignment of y (max-product can be seen as a generalization of the well-known Viterbi algorithm to graphical models)."
"For structure recognition in medical reports, we employ a post-processing step after label prediction with the CRF model."
"As[REF_CITE], this step enforces the constraints of the BIO notation and ap-plies some trivial non-local heuristics that guarantee a consistent global view of the resulting structure."
"For evaluation, we generally performed 3-fold cross-validation for all performance measures."
"We cre-ated training data from the reports in C COR so as to simulate a scenario under ideal conditions, i.e., perfect speech recognition and proper dictation of punctuation and headings, without hesitation or rep-etitions."
"In contrast, the data from C RCG reflects real-life conditions, with a wide variety of speech recognition error rates and speakers frequently hes-itating, repeating themselves and omitting punctua-tion and/or headings."
"Depending on the experiment, two different sub-sets of the two corpora were considered: • C {COR,RCG} - ALL :[REF_CITE]reports were used, resulting in 1338 training examples and 669 testing examples at each CV-iteration. • C {COR,RCG} - BEST :"
The corpus was restricted to those 1002 reports that yielded the lowest word error rate during alignment (see section 4.2).
Each CV-iteration hence amounts to 668 training examples and 334 testing examples.
"From the crossvalidation runs, a 95%-confidence interval for each measure was estimated as follows: s s"
"Ȳ ± t (α/2,N−1) √ = Ȳ ± t (0.025,2) √ (10) N 3 where Ȳ is the sample mean, s is the sample stan-dard deviation, N is the sample size (3), α is the de-sired significance level (0.05) and t (α/2,N−1) is the upper critical value of the t-distribution with N − 1 degrees of freedom."
"The confidence intervals are in-dicated in the ± column of tables 1, 2 and 3."
"For CRF training, we minimized the penalized, negative log-pseudolikelihood using LBFGS with m = 3."
The variance of the Gaussian prior was set to σ 2 = 1000.
"All supported features were used for univariate factors, while the bivariate factors within chains and between chains were restricted to bias weights."
"For testing, loopy belief propagation with a TRP schedule was used in order to determine the maximum a posteriori (MAP) assignment."
"We use VieCRF, our own implementation of factorial CRFs, which is freely available at the author’s homepage [URL_CITE] ."
"In order to determine the number of required train-ing iterations, an experiment was performed that compares the progress of the Accuracy measure on a validation set to the progress of the loss function on a training set."
The data was randomly split into a training set (2/3 of the instances) and a validation set.
Accuracy on the validation set was computed using the intermediate CRF parameters θ t every 5 iterations of LBFGS.
"The resulting plot (figure 5) demonstrates that the progress of the loss function corresponds well to that of the Accuracy measure, thus an “early stopping” approach might be tempt-ing to cut down on training times."
"However, during earlier stages of training, the CRF parameters seem to be strongly biased towards high-frequency labels, so other measures such as macro-averaged F1 might suffer from early stopping."
"Hence, we decided to allow up to 800 iterations of LBFGS."
Table 1 shows estimated accuracies for C COR - ALL and C RCG - ALL .
"Overall, high accuracy (&gt; 97%) can be achieved on C COR - ALL , showing that the ap-proach works very well under ideal conditions."
Per-formance is still fair on the noisy data (C RCG - ALL ; Accuracy &gt; 86%).
"It should be noted that the la-bels are unequally distributed, especially in chain 0 (there are very few B EGIN labels)."
"Thus, the base-line is substantially high for this chain, and other measures may be better suited for evaluating seg-mentation quality (cf. section 6.4)."
"Measuring the effect of the imprecise reference an-notation of C RCG is difficult without a correspond-ing, manually created golden standard."
"However, to get a feeling for the impact of the noise induced by speech recognition errors and sloppy dictation on the quality of the semi-automatically generated annotation, we conducted an experiment with sub-sets C COR - BEST and C RCG - BEST ."
The results are shown in table 2.
"Comparing these results to ta-ble 1, one can see that overall accuracy decreased for C COR - BEST , whereas we see an increase for C RCG - BEST ."
"This effect can be attributed to two different phenomena: • In C COR - BEST , no quality gains in the anno-tation could be expected."
The smaller number of training examples therefore results in lower accuracy. • Fewer speech recognition errors and more con-sistent dictation in C RCG - BEST allow for bet-ter alignment and thus a better reference anno-tation.
"This increases the actual prediction per-formance and, furthermore, reduces the num-ber of label predictions that are erroneously counted as a misprediction."
"Thus, it is to be expected that manual correction of the automatically created annotation results in sig-nificant performance gains."
Preliminary annotation experiments have shown that this is indeed the case.
"Accuracy is not the best measure to assess segmen-tation quality, therefore we also conducted experi-ments using the WindowDiff measure as proposed[REF_CITE]."
WindowDiff re-turns 0 in case of a perfect segmentation; 1 is the worst possible score.
"However, it only takes into account segment boundaries and disregards segment types."
Table 3 shows the WindowDiff scores for C COR - ALL and C RCG - ALL .
"Overall, the scores are quite good and are consistently below 0.2."
"Further-more, C RCG - ALL scores do not suffer as badly from inaccurate reference annotation, since “near misses” are penalized less strongly."
"In section 5.2, we mentioned that loopy BP is not guaranteed to converge in a finite number of itera-tions."
"Since we optimize pseudolikelihood for pa-rameter estimation, we are not affected by this limi-tation in the training phase."
"However, we use loopy BP with a TRP schedule during testing, so we must expect to encounter non-convergence for some ex-amples."
Theoretical results on this topic are dis-cussed[REF_CITE].
"We give here an empir-ical observation of convergence behaviour of loopy BP in our setting; the maximum number of itera-tions of the TRP schedule was restricted to 1,000."
"Table 4 shows the percentage of examples converg-ing within this limit and the average number of iter-ations required by the converging examples, broken down by the different corpora."
"From these results, we conclude that there is a connection between the quality of the annotation and the convergence be-haviour of loopy BP."
"In practice, even though loopy BP didn’t converge for some examples, the solutions after 1,000 iterations where satisfactory."
"We have presented a framework which allows for identification of structure in report dictations, such as sentence boundaries, paragraphs, enumerations, (sub)sections, and various other structural elements; even if no explicit clues are dictated."
"Furthermore, meaningful types are automatically assigned to sub-sections and sections, allowing – for instance – to automatically assign headings, if none were dic-tated."
For the preparation of training data a mechanism has been presented that exploits the potential of par-allel corpora for automatic annotation of data.
"Us-ing manually edited formatted reports and the cor-responding raw output of ASR, reference annotation can be generated that is suitable for learning to iden- tify structure in ASR output."
"For the structure recognition task, a CRF frame-work has been employed and multiple experiments have been performed, confirming the practicability of the approach presented here."
One result deserving further investigation is the effect of noisy annotation.
We have shown that segmentation results improve when fewer errors are present in the automatically generated annotation.
"Thus, manual correction of the reference annotation will yield further improvements."
"Finally, the framework presented in this paper opens up exciting possibilities for future work."
"In particular, we aim at automatically transform-ing report dictations into properly formatted and rephrased reports that conform to the requirements of the relevant domain."
Such tasks are greatly facili-tated by the explicit knowledge gained during struc-ture recognition.
Contradiction Detection (CD) in text is a difficult NLP task.
"We investigate CD over functions (e.g., BornIn(Person)=Place), and present a domain-independent algorithm that automatically discovers phrases denoting functions with high precision."
Previous work on CD has investigated hand-chosen sentence pairs.
"In contrast, we automatically harvested from the Web pairs of sentences that appear contradictory, but were surprised to find that most pairs are in fact consistent."
"For example, “Mozart was born in Salzburg” does not con-tradict “Mozart was born in Austria” despite the functional nature of the phrase “was born in”."
"We show that background knowledge about meronyms (e.g., Salzburg is in Austria), synonyms, functions, and more is essential for success in the CD task."
"Detecting contradictory statements is an important and challenging NLP task with a wide range of potential applications including analysis of politi-cal discourse, of scientific literature, and more (de[REF_CITE])."
"De Marneffe et al. present a model of CD that defines the task, analyzes different types of contradictions, and reports on a CD system."
"Although RTE-3 contains a wide variety of contradictions, it does not reflect the prevalence of seeming contradictions and the paucity of genuine contradictions, which we have found in our corpus."
"Our paper is motivated in part by de Marneffe et al.’s work, but with some important differences."
"First, we introduce a simple logical foundation for the CD task, which suggests that extensive world knowl-edge is essential for building a domain-independent CD system."
"Second, we automatically generate a large corpus of apparent contradictions found in ar-bitrary Web text."
"We show that most of these appar-ent contradictions are actually consistent statements due to meronyms (Alan Turing was born in London and in England), synonyms (George Bush is mar-ried to both Mrs. Bush and Laura Bush), hypernyms (Mozart died of both renal failure and kidney dis-ease), and reference ambiguity (one John Smith was born in 1997 and a different John[REF_CITE])."
"Next, we show how background knowledge enables a CD system to discard seeming contradictions and focus on genuine ones."
"De Marneffe et al. introduced a typology of con-tradiction in text, but focused primarily on contra-dictions that can be detected from linguistic evi-dence (e.g. negation, antonymy, and structural or lexical disagreements)."
We extend their analysis to a class of contradictions that can only be detected utilizing background knowledge.
Consider for ex-ample the following sentences: 1) “Mozart was born in Salzburg.” 2) “Mozart was born in Vienna.” 3) “Mozart visited Salzburg.” 4) “Mozart visited Vienna.”
"Sentences 1 &amp; 2 are contradictory, but 3 &amp; 4 are not."
Why is that?
The distinction is not syntactic.
"Rather, sentences 1 and 2 are contradictory because the relation expressed by the phrase “was born in” can be characterized here as a function from peo-ple’s names to their unique birthplaces."
"In contrast, “visited” does not denote a functional relation. [Footnote_1]"
"1 Although we focus on function-based CD in our case study, we believe that our observations apply to other types of CD as well."
"We cannot assume that a CD system knows, in advance, all the functional relations that might ap-pear in a corpus."
"Thus, a central challenge for a function-based CD system is to determine which re-lations are functional based on a corpus."
"Intuitively, we might expect that “functional phrases” such as “was born in” would typically map person names to unique place names, making function detection easy."
"But, in fact, function detection is surprisingly difficult because name ambiguity (e.g., John Smith), common nouns (e.g., “dad” or “mom”), definite de-scriptions (e.g., “the president”), and other linguistic phenomena can mask functions in text."
"For example, the two sentences “John Smith was born in 1997.” and “John Smith was born in 1883.” can be viewed as either evidence that “was born in” does not de-note a function or, alternatively, that “John Smith” is ambiguous."
"We report on the A U C ONTRAIRE CD system, which addresses each of the above challenges."
"First, A U - C ONTRAIRE identifies “functional phrases” statis-tically (Section 3)."
"Second, A U C ONTRAIRE uses these phrases to automatically create a large cor-pus of apparent contradictions (Section 4.2)."
"Fi-nally, A U C ONTRAIRE sifts through this corpus to find genuine contradictions using knowledge about synonymy, meronymy, argument types, and ambi-guity (Section 4.3)."
"Instead of analyzing sentences directly, A U C ON - TRAIRE relies on the T EXT R UNNER Open Informa-tion Extraction system[REF_CITE]to map each sentence to one or more tuples that represent the entities in the sen-tences and the relationships between them (e.g., was born in(Mozart,Salzburg))."
"Using extracted tu-ples greatly simplifies the CD task, because nu-merous syntactic problems (e.g., anaphora, rela-tive clauses) and semantic challenges (e.g., quantifi-cation, counterfactuals, temporal qualification) are delegated to T EXT R UNNER or simply ignored."
"Nev-ertheless, extracted tuples are a convenient approxi-mation of sentence content, which enables us to fo-cus on function detection and function-based CD."
"Our contributions are the following: • We present a novel model of the Contradiction Detection (CD) task, which offers a simple log-ical foundation for the task and emphasizes the central role of background knowledge. • We introduce and evaluate a new EM-style al-gorithm for detecting whether phrases denote functional relations and whether nouns (e.g., “dad”) are ambiguous, which enables a CD sys-tem to identify functions in arbitrary domains. • We automatically generate a corpus of seem-ing contradictions from Web text, and report on a set of experiments over this corpus, which provide a baseline for future work on statistical function identification and CD. [Footnote_2]"
2 The corpus is available[URL_CITE]
On what basis can a CD system conclude that two statements T and H are contradictory?
"Logically, contradiction holds when T |= ¬H."
"As de Marneffe et al. point out, this occurs when T and H contain antonyms, negation, or other lexical elements that suggest that T and H are directly contradictory."
But other types of contradictions can only be detected with the help of a body of background knowledge K:
"In these cases, T and H alone are mutually con-sistent."
T |\= ¬H ∧ H |=\ ¬T
A contradiction between T and H arises only in the context of K.
That is: ((K ∧ T ) |= ¬H) ∨ ((K ∧ H) |= ¬T )
Consider the example of Mozart’s birthplace in the introduction.
"To detect a contradiction, a CD system must know that A) “Mozart” refers to the same entity in both sentences, that B) “was born in” denotes a functional relation, and that C) Vienna and Salzburg are inconsistent locations."
"Of course, world knowledge, and reasoning about text, are often uncertain, which leads us to associate probabilities with a CD system’s conclusions."
"Nev-ertheless, the knowledge base K is essential for CD."
We now turn to a probabilistic model that helps us simultaneously estimate the functionality of re-lations (B in the above example) and ambiguity of argument values (A above).
Section 4 describes the remaining components of A U C ONTRAIRE .
This section introduces a formal model for comput-ing the probability that a phrase denotes a function based on a set of extracted tuples.
"An extracted tuple takes the form R(x, y) where (roughly) x is the sub-ject of a sentence, y is the object, and R is a phrase denoting the relationship between them."
"If the re-lation denoted by R is functional, then typically the object y is a function of the subject x."
"Thus, our dis-cussion focuses on this possibility, though the anal-ysis is easily extended to the symmetric case."
"Logically, a relation R is functional in a vari-able x if it maps it to a unique variable y: ∀x , y 1 , y 2 R(x, y 1 ) ∧ R(x, y 2 ) ⇒ y 1 = y 2 ."
"Thus, given a large random sample of ground instances of R, we could detect with high confidence whether R is functional."
"In text, the situation is far more com-plex due to ambiguity, polysemy, synonymy, and other linguistic phenomena."
Deciding whether R is functional becomes a probabilistic assessment based on aggregated textual evidence.
"The main evidence that a relation R(x, y) is func-tional comes from the distribution of y values for a given x value."
"If R denotes a function and x is unambiguous, then we expect the extractions to be predominantly a single y value, with a few outliers due to noise."
We aggregate the evidence that R is locally functional for a particular x value to assess whether R is globally functional for all x.
"We refer to a set of extractions with the same relation R and argument x as a contradiction set R(x,·)."
Figure 1 shows three example contradic-tion sets.
Each example illustrates a situation com-monly found in our data.
"Example A in Figure 1 shows strong evidence for a functional relation. 66 out of 70 T EXT R UNNER extractions for was born in (Mozart, PLACE) have the same y value."
"An am-biguous x argument, however, can make a func- tional relation appear non-functional."
"Example B depicts a distribution of y values that appears less functional due to the fact that “John Adams” refers to multiple, distinct real-world individuals with that name."
"Finally, example C exhibits evidence for a non-functional relation."
"To decide whether R is functional in x for all x, we first consider how to detect whether R is lo-cally functional for a particular value of x. The local functionality of R with respect to x is the probabil-ity that R is functional estimated solely on evidence from the distribution of y values in a contradiction set R(x, ·)."
"To decide the probability that R is a function, we define global functionality as the average local func-tionality score for each x, weighted by the probabil-ity that x is unambiguous."
"Below, we outline an EM-style algorithm that alternately estimates the proba-bility that R is functional and the probability that x is ambiguous."
"Let R ∗x indicate the event that the relation R is locally functional for the argument x, and that x is locally unambiguous for R. Also, let D indicate the set of observed tuples, and define D R(x,·) as the multi-set containing the frequencies for extractions of the form R(x, ·)."
"For example the distribution of extractions from Figure 1 for example A is Dw as born in( M ozart,·) = {66, 3, 1}."
"Let θ fR be the probability that R(x,·) is locally functional for a random x, and let Θ f be the vector of these parameters across all relations R. Likewise, θ xu represents the probability that x is locally unam-biguous for random R, and Θ u the vector for all x."
"We wish to determine the maximum a pos-teriori (MAP) functionality and ambiguity pa-rameters given the observed data D, that is arg max Θ f ,Θ u P (Θ f , Θ u |D)."
By Bayes Rule:
"P (Θ f , Θ u |D) ="
"P (D|Θ f , Θ u )P (Θ f , Θ u ) (1) P (D)"
"We outline a generative model for the data, P (D|Θ f , Θ u )."
"Let us assume that the event R x∗ de-pends only on θ Rf and θ xu , and further assume that given these two parameters, local ambiguity and lo-cal functionality are conditionally independent."
We obtain the following expression for the probability of R ∗x given the parameters:
"P (R x∗ |Θ f , Θ u ) = θ fR θ xu"
"We assume each set of data D R(x,·) is gener-ated independently of all other data and parameters, given R ∗x ."
From this and the above we have:
"P (D|Θ f , Θ u ) ="
"Y P (D R(x,·) |R x∗ )θ"
"Rf θ ux R,x +P (D R(x,·) |¬R x∗ )(1 − θ Rf θ xu ) (2)"
"These independence assumptions allow us to ex-press P(D|Θ f ,Θ u ) in terms of distributions over D R(x,·) given whether or not R x∗ holds."
We use the U RNS model as described[REF_CITE]to estimate these probabilities based on binomial distributions.
"In the single-urn U RNS model that we utilize, the extraction process is modeled as draws of labeled balls from an urn, where the labels are either correct extractions or errors, and different labels can be repeated on varying numbers of balls in the urn."
"Let k = maxD R(x,·) , and let n = P D R(x,·) ; we will approximate the distribution over D R(x,·) in terms of k and n."
"If R(x,·) is locally func-tional and unambiguous, there is exactly one cor-rect extraction label in the urn (potentially repeated multiple times)."
"Because the probability of correct-ness tends to increase with extraction frequency, we make the simplifying assumption that the most fre-quently extracted element is correct. 3"
"In this case, k is the number of correct extractions, which by the"
"U RNS model has a binomial distribution with pa-rameters n and p, where p is the precision of the ex-traction process."
"If R(x, ·) is not locally functional and unambiguous, then we expect k to typically take on smaller values."
"Empirically, the underlying fre-quency of the most frequent element in the ¬R ∗x case tends to follow a Beta distribution."
"Under the model, the probability of the evidence given R x∗ is: !"
"P (D R(x,·) |R x∗ ) ≈ P (k, n|R ∗x ) = n p k (1 − p) n−k k"
And the probability of the evidence given ¬R ∗x is:
"P (D R(x,·) |¬R ∗x ) ≈ P (k, n|¬R x∗ ) = nk R 01 p 0k+αf−1 (1−p 0 ) n+βf−1−k dp 0B(α f ,β f ) n Γ(n − k + β f )Γ(α f + k) = k ([Footnote_3]) B(α f , β f )Γ(α f + β f + n) where n is the sum over D R(x,·) , Γ is the Gamma function and B is the Beta function. α f and β f are the parameters of the Beta distribution for the ¬R x∗ case."
"3 As this assumption is invalid when there is not a unique maximal element, we default to the prior P(R x∗ ) in that case."
"These parameters and the prior distributions are estimated empirically, based on a sample of the data set of relations described in Section 5.1."
Substituting Equation 3 into Equation 2 and apply-ing an appropriate prior gives the probability of pa-rameters Θ f and Θ u given the observed data D.
"However, Equation 2 contains a large product of sums—with two independent vectors of coefficients, Θ f and Θ u —making it difficult to optimize analyti-cally."
"If we knew which arguments were ambiguous, we would ignore them in computing the function-ality of a relation."
"Likewise, if we knew which rela-tions were non-functional, we would ignore them in computing the ambiguity of an argument."
"Instead, we initialize the Θ f and Θ u arrays randomly, and then execute an algorithm similar to Expectation-Maximization (EM)[REF_CITE]to arrive at a high-probability setting of the parameters."
"Note that if Θ u is fixed, we can compute the ex-pected fraction of locally unambiguous arguments x for which R is locally functional, using D R(x 0 ,·) and"
"Likewise, for fixed Θ f , for any given x we can compute the expected fraction of locally functional relations R that are locally unambiguous for x."
"Specifically, we repeat until convergence: 1. Set θ Rf = s1 P P(R ∗x |D R(x,·) )θ xu for all R. R x 2. Set θ xu = s1 x P R P(R x∗ |D R(x,·) ) θ Rf for all x."
"In both steps above, the sums are taken over only those x or R for which D R(x,·) is non-empty."
"Also, the normalizer s R = P x θ xu and likewise s x = P θ f ."
"As in standard EM, we iteratively update our pa-rameter values based on an expectation computed over the unknown variables."
"However, we alter-nately optimize two disjoint sets of parameters (the functionality and ambiguity parameters), rather than just a single set of parameters as in standard EM."
Investigating the optimality guarantees and conver-gence properties of our algorithm is an item of future work.
"By iteratively setting the parameters to the expec-tations in steps 1 and 2, we arrive at a good setting of the parameters."
Section 5.2 reports on the perfor-mance of this algorithm in practice.
"A U C ONTRAIRE identifies phrases denoting func-tional relations and utilizes these to find contradic-tory assertions in a massive, open-domain corpus of text."
"A U C ONTRAIRE begins by finding extractions of the form R(x,y), and identifies a set of relations R that have a high probability of being functional."
"Next, A U C ONTRAIRE identifies contradiction sets of the form R(x, ·)."
"In practice, most contradiction sets turned out to consist overwhelmingly of seem-ing contradictions—assertions that do not actually contradict each other for a variety of reasons that we enumerate in section 4.3."
"Thus, a major chal-lenge for A U C ONTRAIRE is to tease apart which pairs of assertions in R(x, ·) represent genuine con-tradictions."
Here are the main components of A U C ONTRAIRE as illustrated in Figure 2:
Extractor: Create a set of extracted assertions E from a large corpus of Web pages or other docu-ments.
"Each extraction R(x, y) has a probability p of being correct."
Function Learner: Discover a set of functional re-lations F from among the relations in E. Assign to each relation in F a probability p f that it is func-tional.
"Contradiction Detector: Query E for assertions with a relation R in F, and identify sets C of po-tentially contradictory assertions."
"Filter out seeming contradictions in C by reasoning about synonymy, meronymy, argument types, and argument ambigu-ity."
Assign to each potential contradiction a proba-bility p c that it is a genuine contradiction.
"A U C ONTRAIRE needs to explore a large set of factual assertions, since genuine contradictions are quite rare (see Section 5)."
"We used a set of extrac-tions E from the Open Information Extraction sys-tem, T EXT R UNNER[REF_CITE], which was run on a set of 117 million Web pages."
"T EXT R UNNER does not require a pre-defined set of relations, but instead uses shallow linguistic anal-ysis and a domain-independent model to identify phrases from the text that serve as relations and phrases that serve as arguments to that relation."
T EXT R UNNER creates a set of extractions in a sin-gle pass over the Web page collection and provides an index to query the vast set of extractions.
"Although its extractions are noisy, T EXT R UNNER provides a probability that the extractions are cor- rect, based in part on corroboration of facts from different Web pages[REF_CITE]."
The next step of A U C ONTRAIRE is to find contra-diction sets in E.
"We used the methods described in Section 3 to estimate the functionality of the most frequent rela-tions in E. For each relation R that A U C ONTRAIRE has judged to be functional, we identify contradic-tion sets R(x, ·), where a relation R and domain ar-gument x have multiple range arguments y."
"For a variety of reasons, a pair of extractions R(x, y 1 ) and R(x, y 2 ) may not be actually contra-dictory."
"The following is a list of the major sources of false positives—pairs of extractions that are not genuine contradictions, and how they are handled by A U C ONTRAIRE ."
"The features indicative of each condition are combined using Logistic Regression, in order to estimate the probability that a given pair, {R(x, y 1 ), R(x, y 2 )} is a genuine contradiction."
"Synonyms: The set of potential contradictions died from(Mozart,·) may contain assertions that Mozart died from renal failure and that he died from kidney failure."
"These are distinct values of y, but do not contradict each other, as the two terms are synonyms."
A U C ONTRAIRE uses a variety of knowl-edge sources to handle synonyms.
"WordNet is a re-liable source of synonyms, particularly for common nouns, but has limited recall."
A U C ONTRAIRE also utilizes synonyms generated by R ESOLVER[REF_CITE]— a system that identifies syn-onyms from T EXT R UNNER extractions.
"Addition-ally, A U C ONTRAIRE uses edit-distance and token-based string similarity[REF_CITE]between apparently contradictory values of y to identify syn-onyms."
"For some relations, there is no con-tradiction when y 1 and y 2 share a meronym, i.e. “part of” relation."
"For example, in the set born in(Mozart,·) there is no contradiction be-tween the y values “Salzburg” and “Austria”, but “Salzburg” conflicts with “Vienna”."
"Although this is only true in cases where y occurs in an up-ward monotone context[REF_CITE], in practice genuine contradictions between y-values sharing a meronym relationship are ex-tremely rare."
We therefore simply assigned contra-dictions between meronyms a probability close to zero.
"We used the Tipster Gazetteer [Footnote_4] and WordNet to identify meronyms, both of which have high pre-cision but low coverage."
Argument Typing: Two y values are not contra-dictory if they are of different argument types.
"For example, the relation born in can take a date or a location for the y value."
"While a person can be born in only one year and in only one city, a per-son can be born in both a year and a city."
"To avoid such false positives, A U C ONTRAIRE uses a sim-ple named-entity tagger [URL_CITE] in combination with large dictionaries of person and location names to as-sign high-level types (person, location, date, other) to each argument."
A U C ONTRAIRE filters out ex-tractions from a contradiction set that do not have matching argument types.
"Ambiguity: As pointed out in Section 3, false con-tradictions arise when a single x value refers to mul-tiple real-world entities."
"For example, if the con-tradiction set born in(John Sutherland, ·) includes birth years of both 1827 and 1878, is one of these a mistake, or do we have a grandfather and grandson with the same name?"
A U C ONTRAIRE computes the probability that an x value is unambiguous as part of its Function Learner (see Section 3).
An x value can be identified as ambiguous if its distribution of y values is non-functional for multiple functional re-lations.
"If a pair of extractions, {R(x, y 1 ), R(x, y 2 )}, does not fall into any of the above categories and R is functional, then it is likely that the sentences under-lying the extractions are indeed contradictory."
"We combined the various knowledge sources described above using Logistic Regression, and used 10-fold cross-validation to automatically tune the weights associated with each knowledge source."
"In addi-tion, the learning algorithm also utilizes the follow-ing features: • Global functionality of the relation, θ Rf • Global unambiguity of x, θ xu • Local functionality of R(x, ·) • String similarity (a combination of token-based similarity and edit-distance) between y 1 and y 2 • The argument types (person, location, date, or other)"
"The learned model is then used to estimate how likely a potential contradiction {R(x, y 1 ), R(x, y 2 )} is to be genuine."
We evaluated several aspects of A U C ONTRAIRE : its ability to detect functional relations and to de-tect ambiguous arguments (Section 5.2); its preci-sion and recall in contradiction detection (Section 5.3); and the contribution of A U C ONTRAIRE ’s key knowledge sources (Section 5.4).
To evaluate A U C ONTRAIRE we used T EXT R UN - NER ’s extractions from a corpus of 117 million Web pages.
"We restricted our data set to the 1,000 most frequent relations, in part to keep the experiments tractable and also to ensure sufficient statistical sup-port for identifying functional relations."
"We labeled each relation as functional or not, and computed an estimate of the probability it is functional as described in section 3.2."
Section 5.2 presents the results of the Function Learner on this set of relations.
"We took the top 2% (20 relations) as F, the set of functional relations in our exper-iments."
"Out of these, 75% are indeed functional."
"Some examples include: was born in, died in, and was founded by."
"There were 1.2 million extractions for all thou-sand relations, and about 20,000 extractions in 6,000 contradiction sets for all relations in F."
"We hand-tagged 10% of the contradiction sets R(x, ·) where R ∈ F, discarding any sets with over 20 distinct y values since the x argument for that set is almost certainly ambiguous."
"This resulted in a data set of 567 contradiction sets containing a total of 2,564 extractions and 8,844 potentially contradic-tory pairs of extractions."
"We labeled each of these 8,844 pairs as contradic-tory or not."
"In each case, we inspected the original sentences, and if the distinction was unclear, con-sulted the original source Web pages, Wikipedia ar-ticles, and Web search engine results."
"In our data set, genuine contradictions over func-tional relations are surprisingly rare."
"We found only 110 genuine contradictions in the hand-tagged sam-ple, only 1.2% of the potential contradiction pairs."
We ran A U C ONTRAIRE ’s EM algorithm on the thousand most frequent relations.
Performance con-verged after 5 iterations resulting in estimates of the probability that each relation is functional and each x argument is unambiguous.
We used these proba-bilities to generate the precision-recall curves shown in Figure 3.
"The graph on the left shows results for function-ality, while the graph on the right shows precision at finding unambiguous arguments."
"The solid lines are results after 5 iterations of EM, and the dashed lines are from computing functionality or ambiguity with-out EM (i.e. assuming uniform values of Θ c when computing Θ f and vice versa)."
"The EM algorithm improved results for both functionality and ambigu-ity, increasing area under curve (AUC) by 19% for functionality and by 31% for ambiguity."
"Of course, the ultimate test of how well A U C ON - TRAIRE can identify functional relations is how well the Contradiction Detector performs on automati-cally identified functional relations."
We conducted experiments to evaluate how well A U C ONTRAIRE distinguishes genuine contradic-tions from false positives.
The bold line in Figure 4 depicts A U C ONTRAIRE performance on the distribution of contradictions and seeming contradictions found in actual Web data.
The dashed line shows the performance of A U - C ONTRAIRE on an artificially “balanced” data set that we constructed to contain 50% genuine contra-dictions and 50% seeming ones.
Previous research in CD presented results on manually selected data sets with a relatively bal-anced mix of positive and negative instances.
"As Figure 4 suggests, this is a much easier problem than CD “in the wild”."
"The data gathered from the Web is badly skewed, containing only 1.2% genuine con-tradictions."
We carried out an ablation study to quantify how much each knowledge source contributes to A U - C ONTRAIRE ’s performance.
"Since most of the knowledge sources do not apply to numeric argu-ment values, we excluded the extractions where y is a number in this study."
"As shown in Figure 5, performance of A U C ONTRAIRE degrades with no knowledge of synonyms (NS), with no knowledge of meronyms (NM), and especially without argu-ment typing (NT)."
"Conversely, improvements to any of these three components would likely improve the performance of A U C ONTRAIRE ."
"The relatively small drop in performance from no meronyms does not indicate that meronyms are not essential to our task, only that our knowledge sources for meronyms were not as useful as we hoped."
The Tipster Gazetteer has surprisingly low coverage for our data set.
It contains only 41% of the y values that are locations.
"Many of these are matches on a different location with the same name, which results in incorrect meronym information."
We estimate that a gazetteer with complete coverage would increase area under the curve by approxi-mately 40% compared to a system with meronyms from the Tipster Gazetteer and WordNet.
"To analyze the errors made by A U C ONTRAIRE , we hand-labeled all false-positives at the point of maximum F-score: 29%[REF_CITE]% Precision."
Figure 6 reveals the central importance of world knowledge for the CD task.
"About half of the errors (49%) are due to ambiguous x-arguments, which we found to be one of the most persistent obstacles to discovering genuine contradictions."
"A sizable por-tion is due to missing meronyms (34%) and missing synonyms (14%), suggesting that lexical resources with broader coverage than WordNet and the Tipster Gazetteer would substantially improve performance."
"Surprisingly, only 3% are due to errors in the extrac-tion process."
All of our experimental results are based on the automatically discovered set of functions F. We would expect A U C ONTRAIRE ’s performance to im-prove substantially if it were given a large set of functional relations as input.
"RTE-3 included an optional task, requiring sys-tems to make a 3-way distinction: {entails, contra-dicts, neither}[REF_CITE]."
"The average per-formance for contradictions on the RTE-3 was preci-sion 0.11 at recall 0.12, and the best system had pre-cision 0.23 at recall 0.19."
We did not run A U C ON - TRAIRE on the RTE data sets because they contained relatively few of the “functional contradictions” that A U C ONTRAIRE tackles.
"On our Web-based data sets, we achieved a precision of 0.62 at recall 0.19, and precision 0.92 at recall 0.51 on the balanced data set."
"Of course, comparisons across very different data sets are not meaningful, but merely serve to un-derscore the difficulty of the CD task."
"In contrast to previous work, A U C ONTRAIRE is the first to do CD on data automatically extracted from the Web."
"This is a much harder problem than using an artificially balanced data set, as shown in Figure 4."
Automatic discovery of functional relations has been addressed in the database literature as Func-tional Dependency Mining[REF_CITE].
"This focuses on dis-covering functional relationships between sets of at-tributes, and does not address the ambiguity inherent in natural language."
We have described a case study of contradiction de-tection (CD) based on functional relations.
"In this context, we introduced and evaluated the A U C ON - TRAIRE system and its novel EM-style algorithm for determining whether an arbitrary phrase is func-tional."
"We also created a unique “natural” data set of seeming contradictions based on sentences drawn from a Web corpus, which we make available to the research community."
We have drawn two key lessons from our case study.
"First, many seeming contradictions (approx-imately 99% in our experiments) are not genuine contradictions."
"Thus, the CD task may be much harder on natural data than on RTE data as sug-gested by Figure 4."
"Second, extensive background knowledge is necessary to tease apart seeming con-tradictions from genuine ones."
"We believe that these lessons are broadly applicable, but verification of this claim is a topic for future work."
Regular expressions have served as the dom-inant workhorse of practical information ex-traction for several years.
"However, there has been little work on reducing the manual ef-fort involved in building high-quality, com-plex regular expressions for information ex-traction tasks."
"In this paper, we propose Re-LIE, a novel transformation-based algorithm for learning such complex regular expressions."
We evaluate the performance of our algorithm on multiple datasets and compare it against the CRF algorithm.
"We show that ReLIE, in ad-dition to being an order of magnitude faster, outperforms CRF under conditions of limited training data and cross-domain data."
"Finally, we show how the accuracy of CRF can be im-proved by using features extracted by ReLIE."
A large class of entity extraction tasks can be ac-complished by the use of carefully constructed reg-ular expressions (regexes).
"Examples of entities amenable to such extractions include email ad-dresses and software names (web collections), credit card numbers and social security numbers (email compliance), and gene and protein names (bioinfor-matics), etc."
These entities share the characteristic that their key representative patterns (features) are expressible in standard constructs of regular expres-sions.
"At first glance, it may seem that constructing a regex to extract such entities is fairly straightfor-ward."
"In reality, robust extraction requires the use of rather complex expressions, as illustrated by the following example."
Example 1 ( Phone number extraction) .
An obvious pattern for identifying phone numbers is “blocks of digits separated by hyphens” represented as R 1 = (\d+\-)+\d+ . 1
"While R 1 matches valid phone numbers like 800-865-1125 and 725-1234, it suffers from both “precision” and “recall” problems."
"Not only does R 1 produce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.865.1125, and (800)865-CARE."
"An improved regex that addresses these problems is R 2 = (\d{3}[-.\ ()]){[Footnote_1],2}[\dA-Z]{4} ."
"1 Throughout this paper, we use the syntax of the standard Java regex engine[REF_CITE]."
"While multiple machine learning approaches have been proposed for information extraction in recent years[REF_CITE], manually created regexes remain a widely adopted practical solution for information extracti[REF_CITE]."
"Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques."
"In this paper, we propose a novel formulation of the problem of learn- ing regexes for information extraction tasks."
We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort.
"To motivate our approach, we first discuss prior work in the area of learning regexes and describe some of the limitations of these techniques."
"The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extracti[REF_CITE]."
Much of this work assumes that the target regex is small and compact thereby allowing the learn-ing algorithm to exploit this information.
"Consider, for example, the learning of patterns motivated by DNA sequencing applications[REF_CITE]."
Here the input sequence is viewed as multiple atomic events separated by gaps.
"Since each atomic event is easily described by a small and compact regex, the problem reduces to one of learn-ing simple regexes."
"Similarly, in XML DTD infer-ence[REF_CITE], it is possible to exploit the fact that the XML docu-ments of interest are often described using simple DTDs."
"E.g., in an online books store, each book has a title, one or more authors and price."
This in-formation can be described in a DTD as hbooki ← htitleihauthori + hpricei .
"However, as shown in Ex-ample 1, regexes for information extraction rely on more complex constructs."
"In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes."
"A common theme[REF_CITE]is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching."
"Thus, the alphabet is defined by the space of possible tags output by these analysis steps."
A similar approach has been proposed[REF_CITE]for POS disambiguation.
"In contrast, our paper ad-dresses extraction tasks that require “fine-grained” control to accurately capture the structural features of the entity of interest."
"Consequently, the domain of interest consists of all characters thereby dramat-ically increasing the size of the alphabet."
"To enable this scale-up, the techniques presented in this paper exploit advanced syntactic constructs (such as char-acter classes and quantifiers) supported by modern regex languages."
"Finally, we note that almost all of the above de-scribed work define the learning problem over a restricted class of regexes."
"Typically, the restric-tions involve either disallowing or limiting the use of Kleene disclosure and disjunction operations."
"How-ever, our work imposes no such restrictions."
"In a key departure from prior formulations, the learning algorithm presented in this work takes as input not just labeled examples but also an initial regular expression."
The use of an initial regex has two major advantages.
"First, this expression pro-vides a natural mechanism for a domain expert to provide domain knowledge about the structure of the entity being extracted."
"Second, as we show in Sec-tion 2, the space of output regular expressions un-der consideration can be meaningfully restricted by appropriately defining their relationship to the input expression."
Such a principled approach to restrict the search space permits the learning algorithm to consider complex regexes in a tractable manner.
"In contrast, prior work defined a tractable search space by placing restrictions on the target class of regular expressions."
"Our specific contributions are: • A novel regex learning problem consisting of learn-ing an “improved” regex given an initial regex and labeled examples • Formulation of this learning task as an optimization problem over a search space of regexes • ReLIE , a regex learning algorithm that employs transformations to navigate the search space • Extensive experimental results over multiple datasets to show the effectiveness of ReLIE and a comparison study with the Conditional Random Field (CRF) algorithm • Finally, experiments that demonstrate the benefits of using ReLIE as a feature extractor for CRF and possibly other machine learning algorithms."
"Consider the task of identifying instances of some entity E. Let R 0 denote the input regex provided by the user and let M(R 0 , D) denote the set of matches obtained by evaluating R 0 over a document col-lection D. Let M p (R 0 ,D) = {x ∈ M(R 0 ,D) : x instance of E} and M n (R 0 ,D) = {x ∈ M(R 0 ,D) : x not an instance of E} denote the set of positive and negative matches for R 0 ."
Note that a match is pos-itive if it corresponds to an instance of the entity of interest and is negative otherwise.
The goal of our learning task is to produce a regex that is “better” than R 0 at identifying instances of E.
"Given a candidate regex R, we need a mechanism to judge whether R is indeed a better extractor for E than R 0 ."
"To make this judgment even for just the original document collection D, we must be able to label each instance matched by R (i.e., each element of M(R,D)) as positive or negative."
"Clearly, this can be accomplished if the set of matches produced by R are contained within the set of available labeled examples, i.e., if M(R, D) ⊆ M(R 0 , D)."
"Based on this observation, we make the following assumption: Assumption 1."
"Given an input regex R 0 over some al-phabet Σ, any other regex R over Σ is a candidate for our learning algorithm only if L(R) ⊆ L(R 0 ). (L(R) denotes the language accepted by R)."
"Even with this assumption, we are left with a po-tentially infinite set of candidate regexes from which our learning algorithm must choose one."
"To explore this set in a principled fashion, we need a mecha-nism to move from one element in this space to an-other, i.e., from one candidate regex to another."
"In addition, we need an objective function to judge the extraction quality of each candidate regex."
We ad-dress these two issues below.
"Regex Transformations To systematically ex-plore the search space, we introduce the concept of regex transformations."
Definition 1 (Regex Transformation).
Let R Σ denote the set of all regular expressions over some alphabet Σ.
"A regex transformation is a function T : R Σ → 2 R Σ such that ∀R 0 ∈ T (R), L(R 0 ) ⊆ L(R)."
"For example, by replacing different occurrences of the quantifier + in R 1 from Example 1 with specific ranges (such as {1,2} or {3} ), we obtain expressions such as R 3 = (\d+\-){1,2}\d+ and"
R 4 = (\d{3}\-)+\d+ .
The operation of replacing quantifiers with restricted ranges is an example of a particular class of transformations that we describe further in Section 3.
"For the present, it is sufficient to view a transformation as a function applied to a regex R that produces, as output, a set of regexes that accept sublanguages of L(R)."
We now define the search space of our learning algorithm as follows: Definition 2 (Search Space).
"Given an input regex R 0 and a set of transformations T , the search space of our learning algorithm is T (R 0 ), the set of all regexes ob-tained by (repeatedly) applying the transformations in T to R 0 ."
"For instance, if the operation of restricting quanti-fiers that we described above is part of the transfor-mation set, then R 3 and R 4 are in the search space of our algorithm, given R 1 as input."
"Objective Function We now define an objective function, based on the well known F-measure, to compare the extraction quality of different candidate regexes in our search space."
"Using M p (R, D) (resp."
"M n (R, D) ) to denote the set of positive (resp. nega-tive) matches of a regex R , we define"
"M p (R, D) precision(R, D) ="
"M p (R, D) + M n (R, D) M p (R, D) recall(R, D) ="
"M p (R 0 , D) 2 · precision(R, D) · recall(R, D) F(R, D) = precision(R, D) + recall(R, D)"
The regex learning task addressed in this paper can now be formally stated as the following opti-mization problem: Definition 3 (Regex Learning Problem).
"Given an input regex R 0 , a document collection D, labeled sets of positive and negative examples M p (R 0 ,D) and M n (R 0 ,D), and a set of transformations T , compute the output regex R f = argmax R∈T (R 0 )"
"In this section, we describe how transformations can be implemented by exploiting the syntactic con-structs of modern regex engines."
"To help with our description, we introduce the following task:"
Example 2 ( Software name extraction) .
Consider the task of identifying names of software products in text.
"A simple pattern for this task is: “one or more capital-ized words followed by a version number”, represented as R 5 = ([A-Z]\w*\s*)+[Vv]?(\d+\.?)+ ."
"When applied to a collection of University web pages, we discovered that R 5 identified correct in-stances such as Netscape 2.0,[REF_CITE]and Installation Designer v1.1."
"However, R 5 also ex-tracted incorrect instances such as course numbers (e.g.[REF_CITE]), room numbers (e.g.[REF_CITE]), and section headings (e.g. Chapter 2.2)."
"To eliminate spurious matches such[REF_CITE], let us enforce the condition that “each word is a single upper-case letter followed by one or more lower-case letters”."
"To accomplish this, we focus on the sub-expression of R 5 that identifies capital-ized words, R 5 1 = ([A-Z]\w*\s*)+ , and replace it with R 5 1a = ([A-Z][a-z]*\s*)+ ."
The regex result-ing from R 5 by replacing R 5 1 with R 5 1a will avoid matches such[REF_CITE].
"An alternate way to improve R 5 is by explicitly disallowing matches against strings like ENGLISH, Room and Chapter."
"To accomplish this, we can exploit the negative lookahead operator supported in modern regex engines."
Lookaheads are special constructs that allow a sequence of characters to be checked for matches against a regex with-out the characters themselves being part of the match.
"As an example, (?!R a )R b (“ ?! ” being the negative lookahead operator) returns matches of regex R b but only if they do not match R a ."
"Thus, by replacing R 5 1 in our original regex with R 5 1b = (?! ENGLISH|Room|Chapter)[A-Z]\w*\s* , we produce an improved regex for software names."
The above examples illustrate the general prin-ciple of our transformation technique.
"In essence, we isolate a sub-expression of a given regex R and modify it such that the resulting regex accepts a sub-language of R. We consider two kinds of modifica-tions – drop-disjunct and include-intersect."
"In drop-disjunct, we operate on a sub-expression that corre-sponds to a disjunct and drop one or more operands of that disjunct."
"In include-intersect, we restrict the chosen sub-expression by intersecting it with some other regex."
"Formally, Definition 4 (Drop-disjunct Transformation)."
"Let R ∈ R Σ be a regex of the form R = R a ρ(X)R b , where ρ(X) denotes the disjunction R 1 |R 2 | . .. |R n of any non-empty set of regexes X = {R 1 , R 2 , . . . , R n }."
"The drop-disjunct transformation DD(R, X, Y ) for some Y ⊂ X, Y =6 ∅ results in the new regex R a ρ(Y )"
R b .
Definition 5 (Include-Intersect Transformation).
"R ∈ R Σ be a regex of the form R = R a XR b for some X ∈ R Σ , X 6= ∅."
"The include-intersect transformation II(R, X, Y ) for some Y ∈ R Σ , Y =6 ∅ results in the new regex R a (X ∩ Y )R b ."
"We state the following proposition (proof omit-ted in the interest of space) that guarantees that both drop-disjunct and include-intersect restrict the lan-guage of the resulting regex, and therefore are valid transformations according to Definition 1."
"Given regexes R,X 1 ,Y 1 ,X 2 and Y 2 from R Σ such that DD(R,X 1 ,Y 1 ) and II(R,X 2 ,Y 2 ) are applicable, L(DD(R, X 1 , Y 1 )) ⊆ L(R) and L(II(R, X 2 , Y 2 )) ⊆ L(R)."
We now proceed to describe how we use differ-ent syntactic constructs to apply drop-disjunct and include-intersect transformations.
"Character Class Restrictions Character classes are short-hand notations for denoting the disjunction of a set of characters ( \d is equivalent to (0|1...|9) ; \w is equivalent to (a|. . .|z|A|. . .|Z|0|1. . .|9| ) ; etc.). 2 Figure 1 illustrates a character class hierarchy in which each node is a stricter class than its parent (e.g., \d is stricter than \w )."
A replacement of any of these character classes by one of its descendants is an instance of the drop-disjunct transformation.
"Notice that in Example [Footnote_2], when replacing R 5 1 with R 5 1a , we were in effect applying a character class restriction."
2 Note that there are two distinct character classes \W and \w
Quantifier Restrictions Quantifiers are used to define the range of valid counts of a repetitive se-quence.
"For instance, a{m,n} looks for a sequence of a ’s of length at least m and at most n ."
"Since quantifiers are also disjuncts (e.g., a{1,3} is equiv-alent to a|aa|aaa ), the replacement of an expres-sion R{m, n} with an expression R{m 1 , n 1 } (m ≤ m 1 ≤ n 1 ≤ n) is an instance of the drop-disjunct transformation."
"For example, given a subexpres-sion of the form a{1,3} , we can replace it with one of a{1,1}, a{1,2}, a{2,2}, a{2,3} , or a{3,3} ."
"Note that, before applying this transformation, wild-card expressions such as a+ and a* are replaced by a{0,maxCount} and a{1,maxCount} respectively, where maxCount is a user configured maximum length for the entity being extracted."
Negative Dictionaries Observe that the include-intersect transformation (Definition 5) is applicable for every possible sub-expression of a given regex R. Note that a valid sub-expression in R is any portion of R where a capturing group can be intro-duced. [Footnote_3] Consider a regex R = R a XR b with a sub-expression X; the application of include-intersect requires another regex Y to yield R a (X ∩Y )
"3 For instance, the sub-expressions of ab{1,2}c are a, ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c."
R b .
We would like to construct Y such that R a (X ∩Y )
R b is “better” than R for the task at hand.
"Therefore, we construct Y as ¬Y 0 where Y 0 is a regex constructed from negative matches of R. Specifically, we look at each negative match of R and identify the substring of the match that corresponds to X. We then apply a greedy heuristic (see below) to these substrings to yield a negative dictionary Y 0 ."
"Finally, the trans-formed regex R a (X ∩¬Y 0 )"
R b is implemented using the negative lookahead expression R a (?!
Y’)XR b .
Greedy Heuristic for Negative Dictionaries Im-plementation of the above procedure requires cer-tain judicious choices in the construction of the neg-ative dictionary to ensure tractability of this trans-formation.
"Let S(X) denote the distinct strings that correspond to the sub-expression X in the neg-ative matches of R. [Footnote_4] Since any subset of S(X) is a candidate negative dictionary, we are left with an exponential number of possible transformations."
"4 S(X) can be obtained automatically by identifying the sub-string corresponding to the group X in each entry in M n (R,D)"
"In our implementation, we used a greedy heuris-tic to pick a single negative dictionary consisting of all those elements of S(X) that individually improve the F-measure."
"For instance, in Exam-ple 2, if the independent substitution of R 5 1 with (?!ENGLISH)[A-Z]\w*\s* , (?!Room)[A-Z] \w*\s* , and (?!Chapter)[A-Z]\w*\s* each im-proves the F-measure, we produce a nega-tive dictionary consisting of ENGLISH , Room , and Chapter ."
This is precisely how the disjunct ENGLISH|Room|Chapter is constructed in R 5 1b .
Figure 2 describes the ReLIE algorithm for the Regex Learning Problem (Definition 3) based on the transformations described in Section 3.
"ReLIE is a greedy hill climbing search procedure that chooses, at every iteration, the regex with the highest F-measure."
"An iteration in ReLIE consists of: • Applying every transformation on the current regex R new to obtain a set of candidate regexes • From the candidates, choosing the regex R 0 whose F-measure over the training dataset is maximum"
"To avoid overfitting, ReLIE terminates when either of the following conditions is true: (i) there is no improvement in F-measure over the training set; (ii) there is a drop in F-measure when applying R 0 on the validation set."
The following proposition provides an upper bound for the running time of the ReLIE algorithm.
"Given any valid set of inputs M tr , M val , R 0 , and T , the ReLIE algorithm terminates in at most | M n (R 0 , M tr )| iterations."
"The running time of the algorithm T Total (R 0 , M tr , M val ) ≤ | M n (R 0 , M tr )| ∗ t 0 , where t 0 is the time taken for the first iteration of the algorithm. 



 which a single quantifier appearing in R can be restricted to a smaller range."
Let F cc denote the maximum fanout [Footnote_5] of the character class hierarchy.
5 Fanout is the number of ways in which a character class may be restricted as defined by the hierarchy (e.g. Figure 1).
Let T ReEval (D) denote the average time taken to evaluate a regex over dataset D.
Let R i denote the regex at the beginning of iteration i.
"The number of candidate regexes obtained by applying the three transformations is NumRE(R i , M tr ) ≤ n cc (R i ) ∗F cc +n q (R i ) ∗MaxQ(R i )+|R i | 2"
The time taken to enumerate the character class and quantifier restriction transformations is proportional to the resulting number of candidate regexes.
The time taken for the negative dictionaries transformation is given by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is given by (for some constant c)
It can be shown that the time taken in each iteration decreases monotonically (details omitted in the interest of space).
"Therefore, the total running time of the algorithm is given by"
"T Total (R 0 , M tr , M val ) ="
"X T I (R i , M tr , M val ) ≤ | M n (R 0 , M tr )| ∗ t 0 . where t 0 ="
"T I (R 0 , M tr , M val ) is the running time of the first iteration of the algorithm."
"In this section, we present an empirical study of the ReLIE algorithm using four extraction tasks over three real-life data sets."
The goal of this study is to evaluate the effectiveness of ReLIE in learning com-plex regexes and to investigate how it compares with standard machine learning algorithms.
"Data Set The datasets used in our experiments are: • EWeb : A collection of 50,000 web pages crawled from a corporate intranet. • AWeb : A set of 50,000 web pages obtained from the publicly available University of Michigan Web page collecti[REF_CITE], including a sub-collection of 10,000 pages ( AWeb-S ). • Email : A collection of 10,000 emails obtained from the publicly available Enron email collec-ti[REF_CITE]."
"Extraction Tasks SoftwareNameTask , CourseNum-berTask and PhoneNumberTask were evaluated on EWeb , AWeb and Email , respectively."
"Since web pages have large number of URLs, to keep the la-beling task manageable, URLTask was evaluated on AWeb-S ."
"Gold Standard For each task, the gold standard was created by manually labeling all matches for the initial regex."
"Note that only exact matches with the gold standard are considered correct in our evalua-tions. [Footnote_6] Comparison Study To evaluate ReLIE for entity extraction vis-a-vis existing algorithms, we used the popular conditional random field (CRF)."
6 The labeled data will be made publicly available[URL_CITE].
"Specifi-cally, we used the MinorThird[REF_CITE]imple-mentation of CRF to train models for all four extrac-tion tasks."
For training the CRF we provided it with the set of positive and negative matches from the ini-tial regex with a context of 200 characters on either side of each match [Footnote_7] .
"7 Ideally, we would have preferred to let MinorThird extract appropriate features from complete documents in the training-set but could not get it to load our large datasets."
"Since it is unlikely that useful features are located far away from the entity, we be-lieve that 200 characters on either side is sufficient context."
The CRF used the base features described[REF_CITE].
"To ensure fair compari-son with ReLIE , we also included the matches corre-sponding to the input regex as a feature to the CRF."
"In practice, more complex features (e.g., dictionar-ies, simple regexes) derived by domain experts are often provided to CRFs."
"However, such features can also be used to refine the initial regex given to ReLIE ."
"Hence, with a view to investigating the “raw” learn-ing capability of the two approaches, we chose to run all our experiments without any additional man-ually derived features."
"In fact, the patterns learned by ReLIE through transformations are often similar a For SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program failed repeatedly during the training phase. to the features that domain experts may provide to CRF."
We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to evaluate the effectiveness of ReLIE and CRF.
"We di-vided each dataset into 10 equal parts and used X% of the dataset for training (X=10, 40 and 80), 10% for validation, and remaining (90-X)% for testing."
All results are reported on the test set. 5.2 Results
"Four extraction tasks were chosen to reflect the enti-ties commonly present in the three datasets. • SoftwareNameTask : Extracting software names such as Lotus Notes 8.0, Open[REF_CITE]. • CourseNumberTask : Extracting university course numbers such[REF_CITE]. • PhoneNumberTask : Extracting phone numbers such as 1-800-[REF_CITE]123 5678 . • URLTask : Extracting URLs such[URL_CITE]and lsa.umich.edu/ foo/ . [Footnote_8]"
"8 URLTask may appear to be simplistic. However, extracting URLs without the leading protocol definitions (e.g.[URL_CITE]can be challenging."
This section summarizes the results of our empir-ical evaluation comparing ReLIE and CRF.
"Raw Extraction Quality The cross-validated re - sults across all four tasks are presented in Figure 3. •[REF_CITE]% training data, ReLIE outperforms CRF on three out of four tasks with a difference in F-measure ranging from 0.1 to 0.2. • As training data increases, both algorithms perform better with the gap between the two reducing for all the four tasks."
"For CourseNumberTask and URL-Task , CRF does slightly better than ReLIE for larger training dataset."
"For the other two tasks, ReLIE re-tains its advantage over CRF. [Footnote_9]"
"9 For SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program failed repeatedly dur-ing the training phase."
The above results indicate that ReLIE performs comparably with CRF with a slight edge in condi-tions of limited training data.
"Indeed, the capability to learn high-quality extractors using a small train-ing set is important because labeled data is often ex-pensive to obtain."
"For precisely this same reason, we would ideally like to learn the extractors once and then apply them to other datasets as needed."
"Since these other datasets may be from a different domain, we next performed a cross-domain test (i.e., training testing on another."
"The scenarios chosen are: (i) SoftwareNameTask see that ReLIE significantly out- performs CRF for all three tasks, ing on the same dataset, there is a"
"Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced with reduction in F-measure (less than features learned by ReLIE). 0.1 in many cases) when the regex learned by ReLIE is applied to a dif- ferent dataset, while the drop for CRF is much more significant (over 0.5 in many cases). [Footnote_11] Training Time Another issue of practical consid-eration is the efficiency of the learning algorithm."
11 Similar cross-domain performance deterioration for a ma-chine learning approach has been observed[REF_CITE].
Table 2 reports the average training and testing time for both algorithms on the four tasks.
On average Re-LIE is an order of magnitude faster than CRF in both building the model and applying the learnt model.
Robustness to Variations in Input Regexes
The transformations done by ReLIE are based on the structure of the input regex.
"Therefore given differ-ent input regexes, the final regexes learned by ReLIE will be different."
"To evaluate the impact of the struc-ture of the input regex on the quality of the regex learned by ReLIE , we started with different regexes [Footnote_12] for the same task."
12 Recall that the search space of ReLIE is limited by L(R 0 )
We found that ReLIE is robust to variations in input regexes.
"For instance, on Soft-wareNameTask , the standard deviation in F-measure (Assumption 1)."
"Thus to ensure meaningful comparison, for the same task any two given input regexes R 0 and R 00 are cho-sen in such a way that although their structures are different, M p (R 0 , D) = M p (R 00 , D) and M n (R 0 , D) = M n (R 00 , D). of the final regexes generated from six different in-put regexes was less than 0.05."
Further details of this experiment are omitted in the interest of space.
The results of our comparison study (Figure 3) in-dicates that for raw extraction quality ReLIE has a slight edge over CRF for small training data.
"How-ever, in cross-domain performance (Table 1) ReLIE is significantly better than CRF (by 0.41 on aver-age) ."
"To understand this discrepancy, we examined the final regex learned by ReLIE and compared that with the features learned by CRF."
Examples of ini-tial regexes with corresponding final regexes learnt[REF_CITE]% training data are listed in Ta-ble 4.
"Recall, from Section 3, that ReLIE transfor-mations include character class restrictions, quanti-fier restrictions and addition of negative dictionar-ies."
"For instance, in the SoftwareNameTask , the final regex listed was obtained by restricting [a-zA-Z] to [a-z] , \w to [a-zA-Z] , and adding the nega-tive dictionary (Copyright|Fall| · · · |Issue) ."
"Sim-ilarly, for the PhoneNumberTask , the final regex involved two negative dictionaries (expressed as (?! [,]) and (?! [,:]) ) [Footnote_13] and quantifier restric-tions (e.g. the first [A-Z\d]{2,4} was transformed into [A-Z\d]{3,3} )."
"13 To obtain these negative dictionaries, ReLIE not only needs to correctly identify the dictionary entries from negative matches but also has to place the corresponding negative looka-head expression at the appropriate place in the regex."
"After examining the features learnt by CRF, it was clear that while CRF could learn features such as the negative dictionary it is unable to learn character-level features."
This should not be surprising since our CRF was trained with primarily tokens as fea-tures (cf. Section 5.1).
"While this limitation was less of a factor in experiments involving data from the same domain (some effects were seen with smaller training data), it does explain the significant differ-ence between the two algorithms in cross-domain tasks where the vocabulary can be significantly dif-ferent."
"Indeed, in practical usage of CRF, the main challenge is to come up with additional complex fea-tures (often in the form of dictionary and regex pat-terns) that need to be given to the CRF[REF_CITE]."
Such complex features are largely hand-crafted and thus expensive to obtain.
"Since the Re-LIE transformations are operations over characters, a natural question to ask is: “Can the regex learned by ReLIE be used to provide features to CRF?”"
We answer this question below.
"To understand the effect of incorporating ReLIE -identified features into CRF, we chose the two tasks ( CourseNumberTask and PhoneNumberTask ) with the least F-measure in our experiments to determine raw extraction quality."
We examined the final regex pro-duced by ReLIE and manually extracted portions to serve as features.
"For example, the negative dictionary learned by ReLIE for the CourseNumber-Task (At|Between| · · · |Volume) was incorporated as a feature into CRF."
"To help isolate the effects, for each task, we only incorporated features correspond-ing to a single transformation: negative dictionar-ies for CourseNumberTask and quantifier restrictions for PhoneNumberTask ."
The results of these experi-ments are shown in Table 3.
The first point worthy of note is that performance has improved in all but one case.
"Second, despite the F-measure on CourseNum-berTask being lower than PhoneNumberTask (presum-ably more potential for improvement), the improve-ments on PhoneNumberTask are significantly higher."
This observation is consistent with our conjecture in Section 5.1 that CRF learns token-level features; therefore incorporating negative dictionaries as extra feature provides only limited improvement.
Admit-tedly more experiments are needed to understand the full impact of incorporating ReLIE -identified fea-tures into CRF.
"However, we do believe that this is an exciting direction of future research."
We proposed a novel formulation of the problem of learning complex character-level regexes for entity extraction tasks.
We introduced the concept of regex transformations and described how these could be realized using the syntactic constructs of modern regex languages.
"We presented ReLIE, a powerful regex learning algorithm that exploits these ideas."
"Our experiments demonstrate that ReLIE is very ef-fective for certain classes of entity extraction, partic-ularly under conditions of cross-domain and limited training data."
Our preliminary results also indicate the possibility of using ReLIE as a powerful feature extractor for CRF and other machine learning algo-rithms.
Further investigation of this aspect of ReLIE presents an interesting avenue of future work.
A human annotator can provide hints to a machine learner by highlighting contextual “rationales” for each of his or her annotations[REF_CITE].
How can one exploit this side information to better learn the desired parameters θ?
"We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales."
"Thus, observing the rationales helps us infer the true θ."
We collect substring rationales for a sentiment classification task[REF_CITE]and use them to obtain significant accuracy improvements for each annotator.
Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach.
"It is also more principled, and could be adapted to help learn other kinds of probabilistic classi-fiers for quite different tasks."
Many recent papers aim to reduce the amount of an-notated data needed to train the parameters of a sta-tistical model.
"Well-known paradigms include ac-tive learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from ex-isting annotated data."
"A rather different paradigm is to change the ac-tual task that is given to annotators, giving them a greater hand in shaping the learned classifier."
"Af-ter all, human annotators themselves are more than just black-box classifiers to be run on training data."
They possess some introspective knowledge about their own classification procedure.
The hope is to mine this knowledge rapidly via appropriate ques-tions and use it to help train a machine classifier.
"How to do this, however, is still being explored."
An obvious option is to have the annotators directly express their knowledge by hand-crafting rules.
This approach remains “data-driven” if the annotators re-peatedly refine their system against a corpus of la-beled or unlabeled examples.
"This achieves high performance in some domains, such as NP chunk-ing[REF_CITE], but requires more analyt-ical skill from the annotators."
One empirical study[REF_CITE]found that it also re-quired more annotation time than active learning.
More recent work has focused on statistical classi-fiers.
Training such classifiers faces the “credit as-signment problem.”
"Given a training example x with many features, which features are responsible for its annotated class y?"
It may take many training exam-ples to distinguish useful vs. irrelevant features. [Footnote_1]
"1 Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vo-cabulary, often conjoined with lexical or non-lexical context."
"To reduce the number of training examples needed, one can ask annotators to examine or pro-pose some candidate features."
This is possible even for the very large feature sets that are typically used in NLP.
"In document classification,[REF_CITE]show that feature selection by an oracle could be helpful, and that humans are both rapid and rea-sonably good at distinguishing highly useful n-gram features from randomly chosen ones, even when viewing these n-grams out of context."
The above methods consider features out of context.
An annotator might have an easier time examining features in context to recognize whether they appear relevant.
"This is particularly true for features that are only modestly or only sometimes helpful, which may be abundant in NLP tasks."
"Thus,[REF_CITE]propose an active learning method in which, while classifying a train-ing document, the annotator also identifies some fea-tures of that document as particularly relevant."
"E.g., the annotator might highlight particular unigrams as he or she reads the document."
"In their proposal, a feature that is highlighted in any document is as-sumed to be globally more relevant."
"Its dimension in feature space is scaled by a factor of 10 so that this feature has more influence on distances or inner products, and hence on the learned classifier."
"Despite the success of the above work, we have several concerns about asking annotators to identify globally relevant features."
"First, a feature in isolation really does not have a well-defined worth."
"A feature may be useful only in conjunction with other features, [Footnote_2] or be useful only to the extent that other correlated features are not selected to do the same work."
"2 For example, a linear classifier can learn that most training examples satisfy A → B by setting θ A = −5 and θ A∧B = +5, but this solution requires selecting both A and A∧B as features. More simply, a polynomial kernel can consider the conjunction A ∧ B only if both A and B are selected as features."
"Second, it is not clear how an annotator would easily view and highlight features in context, ex-cept for the simplest feature sets."
"In the phrase Apple shares up 3%, there may be several fea-tures that fire on the substring Apple—responding to the string Apple, its case-invariant form apple, its lemma apple- (which would also respond to ap-ples), its context-dependent sense Apple 2 , its part of speech noun, etc."
How does the annotator indi-cate which of these features are relevant?
"Third, annotating features is only appropriate when the feature set can be easily understood by a human."
This is not always the case.
"It would be hard for annotators to read, write, or evaluate a descrip-tion of a complex syntactic configuration in NLP or a convolution filter in machine vision."
"Fourth, traditional annotation efforts usually try to remain agnostic about the machine learning methods and features to be used."
"The project’s cost is justi-fied by saying that the annotations will be reused by many researchers (perhaps in a “shared task”), who are free to compete on how they tackle the learning problem."
"Unfortunately, feature annotation commits to a particular feature set at annotation time."
"Subse-quent research cannot easily adjust the definition of the features, or obtain annotation of new features."
"To solve these problems, we propose that annotators should not select features but rather mark relevant portions of the example."
"In earlier work[REF_CITE], we called these markings “rationales.”"
"For example, when classifying a movie review as positive or negative, the annotator would also high-light phrases that supported that judgment."
Figure 1 shows two such rationales.
"A multi-annotator timing study[REF_CITE]found that highlighting rationale phrases while reading movie reviews only doubled annota-tion time, although annotators marked 5–11 ratio-nale substrings in addition to the simple binary class."
The benefit justified the extra time.
"Furthermore, much of the benefit could have been obtained by giv-ing rationales for only a fraction of the reviews."
"In the visual domain, when classifying an im-age as containing a zoo, the annotator might circle some animals or cages and the sign reading “Zoo.”"
The Peekaboom game (v[REF_CITE]) was in fact built to elicit such approximate yet relevant re-gions of images.
"Further scenarios were discussed[REF_CITE]: rationale annotation for named entities, linguistic relations, or handwritten digits."
"Annotating rationales does not require the anno-tator to think about the feature space, nor even to know anything about it."
Arguably this makes an-notation easier and more flexible.
It also preserves the reusability of the annotated data.
"Anyone is free to reuse our collected rationales (section 4) to aid in learning a classifier with richer features, or a dif-ferent kind of classifier altogether, using either our procedures or novel procedures."
"As rationales are more indirect than explicit features, they present a trickier machine learning problem."
We wish to learn the parameters θ of some classi-fier.
How can the annotator’s rationales help us to do this without many training examples?
"We will have to exploit a presumed relationship between the rationales and the optimal value of θ (i.e., the value that we would learn on an infinite training set)."
"This paper exploits an explicit, parametric model of that relationship."
The model’s parameters φ are intended to capture what that annotator is doing when he or she marks rationales.
"Most importantly, they capture how he or she is influenced by the true θ."
"Given this, our learning method will prefer values of θ that would adequately explain the rationales (as well as the training classifications)."
"For concreteness, we will assume that the task is document classification."
"Our training data consists of n triples {(x 1 , y 1 , r 1 ), ..., (x n , y n , r n )}), where x i is a document, y i is its annotated class, and r i is its rationale markup."
"At test time we will have to pre-dict y n+1 from x n+1 , without any r n+1 ."
"We propose to jointly choose parameter vectors θ and φ to maximize the following regularized condi-tional likelihood: [Footnote_3] n Y p(y i , r i | x i , θ, φ) · p prior (θ, φ) (1) i=1 n = def Y p θ (y i | x i ) · p φ (r i | x i , y i , θ) · p prior (θ, φ) i=1"
"3 It would be preferable to integrate out φ (and even θ), but more difficult."
"Here we are trying to model all the annotations, both y i and r i ."
"The first factor predicts y i using an ordi-nary probabilistic classifier p θ , while the novel sec-ond factor predicts r i using a model p φ of how an-notators generate the rationale annotations."
The crucial point is that the second factor depends on θ (since r i is supposed to reflect the relation be-tween x i and y i that is modeled by θ).
"As a result, the learner has an incentive to modify θ in a way that increases the second factor, even if this some-what decreases the first factor on training data. [Footnote_4]"
"4 Interestingly, even examples where the annotation y i is wrong or unhelpful can provide useful information about θ via the pair (y i , r i ). Two annotators marking the same movie re-view might disagree on whether it is overall a positive or nega-"
"After training, one should simply use the first fac-tor p θ (y | x) to classify test documents x."
"The sec-ond factor is irrelevant for test documents, since they have not been annotated with rationales r."
"The second factor may likewise be omitted for any training documents i that have not been annotated with rationales, as there is no r i to predict in those cases."
"In the extreme case where no documents are annotated with rationales, equation (1) reduces to the standard training procedure."
"Like ordinary class annotations, rationale annota-tions present us with a “credit assignment problem,” albeit a smaller one that is limited to features that fire “in the vicinity” of the rationale r."
Some of these θ-features were likely responsible for the classifica-tion y and hence triggered the rationale.
Other such θ-features were just innocent bystanders.
"Thus, the interesting part of our model is p φ (r | x, y, θ), which models the rationale annotation pro-cess."
"The rationales r reflect θ, but in noisy ways."
"Taking this noisy channel idea seriously, p φ (r | x, y, θ) should consider two questions when assess-ing whether r is a plausible set of rationales given θ."
"First, it needs a “language model” of rationales: does r consist of rationales that are well-formed a priori, i.e., before θ is considered?"
"Second, it needs a “channel model”: does r faithfully signal the fea-tures of θ that strongly support classifying x as y?"
"If a feature contributes heavily to the classification of document x as class y, then the channel model should tell us which parts of document x tend to be highlighted as a result."
The channel model must know about the partic-ular kinds of features that are extracted by f and scored by θ.
"Suppose the feature not . . . gripping, [Footnote_5] with weight θ h , is predictive of the annotated class y."
"5 Our current experiments use only unigram features, to match past work, but we use this example to outline how our approach generalizes to complex linguistic (or visual) features."
"This raises the probabilities of the annotator’s high-lighting each of various words, or combinations of words, in a phrase like not the most gripping ban-quet on film."
"The channel model parameters in φ should specify how much each of these probabilities is raised, based on the magnitude of θ h ∈ R, the class y, and the fact that the feature is an instance of the template &lt;Neg&gt; . . . &lt;Adjective&gt;. (Thus, φ has no parameters specific to the word gripping; it is a low-dimensional vector that only describes the annotator’s general style in translating θ into r.)"
"The language model, however, is independent of the feature set θ."
"It models what rationales tend to look like in the input domain—e.g., documents or images."
"In the document case, φ should describe: How frequent and how long are typical rationales?"
Do their edges tend to align with punctuation or ma-jor syntactic boundaries in x?
"Are they rarer in the middle of a document, or in certain documents? [Footnote_6]"
"6 Our current experiments do not model this last point. How-ever, we imagine that if the document only has a few θ-features that support the classification, the annotator will probably mark most of them, whereas if such features are abundant, the anno-tator may lazily mark only a few of the strongest ones. A simple approach would equip φ with a different “bias” or “threshold” parameter φ x for each rationale training document x, to mod-ulate the a priori probability of marking a rationale in x. By fitting this bias parameter, we deduce how lazy the annotator was (for whatever reason) on document x. If desired, a prior on φ x could consider whether x has many strong θ-features, whether the annotator has recently had a coffee break, etc."
"Thanks to the language model, we do not need to posit high θ features to explain every word in a ratio-nale."
"The language model can “explain away” some words as having been highlighted only because this annotator prefers not to end a rationale in mid-phrase, or prefers to sweep up close-together fea-tures with a single long rationale rather than many short ones."
"Similarly, the language model can help explain why some words, though important, might not have been included in any rationale of r."
"If there are multiple annotators, one can learn dif-ferent φ parameters for each annotator, reflecting their different annotation styles. [Footnote_7] We found this to be useful (section 8.2)."
"7 Given insufficient rationale data to recover some annota-tor’s φ well, one could smooth using data from other annotators. But in our situation, φ had relatively few parameters to learn."
"We remark that our generative modeling approach (equation (1)) would also apply if r were not ratio-nale markup, but some other kind of so-called “side information,” such as the feature annotations dis-cussed in section 1."
"For example,[REF_CITE]assume that if feature h is relevant—a bi- nary distinction—iff it was selected in at least one document."
"But it might be more informative to ob-serve that h was selected in 3 of the 10 documents where it appeared, and to predict this via a model p φ (3 of 10 | θ h ), where φ describes (e.g.) how to de-rive a binomial parameter nonlinearly from θ h ."
"This approach would not how often h was marked and in-fer how relevant is feature h (i.e., infer θ h )."
"In this case, p φ is a simple channel that transforms relevant features into direct indicators of the feature."
"Our side information merely requires a more complex transformation—from relevant features into well-formed rationales, modulated by documents."
9 Polarity dataset version 2.0.
"10 We avoid annotator names A1–A2, which were already used[REF_CITE]."
All our experiments use F 9 as their final blind test set.
"The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F 0 –F 8 of the movie review set with rationales (in the form of textual substrings) that supported the gold-standard classifications."
We will use A0’s data to determine the improvement of our method over a (log-linear) baseline model without rationales.
We also use A0 to compare against the “masking SVM” method and SVM baseline[REF_CITE].
"Since φ can be tuned to a particular annotator, we would also like to know how well this works with data from annotators other than A0."
We randomly selected 100 reviews (50 positive and 50 negative) and collected both class and rationale annotation data from each of six new annotators A3–[REF_CITE]fol-lowing the same procedures[REF_CITE].
"We report results using only data from A3–A5, since we used the data from A6–A8 as development data in the early stages of our work."
We use this new rationale-enriched dataset 8 to de-termine if our method works well across annotators.
"We will only be able to carry out that comparison at small training set sizes, due to limited data from A3–A[Footnote_8]."
The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes.
"We define the basic classifier p θ in equation (1) to be a standard conditional log-linear model: def exp(θ~ · f~(x, y)) def u(x, y) p θ (y | x) = = (2) Z θ (x) Z θ (x) where f~(·) extracts a feature vector from a classified document, ~θ are the corresponding weights of those features, and Z θ (x) = def P u(x, y) is a normalizer. y"
We use the same set of binary features as in pre-vious work on this dataset[REF_CITE].
"Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus."
"Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise."
"Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite."
"This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales."
"Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2."
The rationales collected in this task are textual seg-ments of a document to be classified.
"The docu-ment itself is a word token sequence ~x = x 1 , ..., x M ."
"We encode its rationales as a corresponding tag se-quence r~ = r 1 ,...,r M , as illustrated in Figure 1."
"Here r m ∈ {I,O} according to whether the token x m is in a rationale (i.e., x m was at least partly high-lighted) or outside all rationales. x 1 and x M are special boundary symbols, tagged with O."
We predict the full tag sequence ~r at once using a conditional random field[REF_CITE].
"A CRF is just another conditional log-linear model: p φ (r |x, y, ~θ) def = exp(φ~ · g~(r, x, y, ~θ)) = def u(r, x, y, ~θ) Z φ (x, y, ~θ) Z φ (x, y, ~θ) where ~g(·) extracts a feature vector, ~φ are the corresponding weights of those features, and Z φ (x, y, ~θ) def = P r u(r, x, y, ~θ) is a normalizer."
"As usual for linear-chain CRFs, g~(·) extracts two kinds of features: first-order “emission” features that relate r m to (x m ,y,θ), and second-order “transi-tion” features that relate r m to r m−1 (although some of these also look at x)."
These two kinds of features respectively capture the “channel model” and “language model” of sec-tion 3.2.
The former says r m is I because x m is associated with a relevant θ-feature.
The latter says r m is I simply because it is next to another I.
Recall that our θ-features (at present) correspond to unigrams.
"Given (~x, y, ~θ), let us say that a unigram w ∈ ~x is relevant, irrelevant, or anti-relevant if y · θ w is respectively 0, ≈ 0, or 0."
"That is, w is relevant if its presence in x strongly supports the annotated class y, and anti-relevant if its presence strongly supports the opposite class −y. function family B s in equation (3), shown for s ∈ {10, 2, −2, −10}."
"We would like to learn the extent φ rel to which annotators try to include relevant unigrams in their rationales, and the (usually lesser) extent φ antirel to which they try to exclude anti-relevant unigrams."
This will help us infer θ~ from the rationales.
The details are as follows. φ rel and φ antirel are the weights of two emission features extracted by g~:
"M g rel (~x, y, ~r, θ~) def = X I(r m = I) · B 10 (y · θ x m ) m=1 M g antirel (~x, y, r~, ~θ) = def X I(r m = I) · B −10 (y · θ x m ) m=1"
"Here I(·) denotes the indicator function, returning 1 or 0 according to whether its argument is true or false."
"Relevance and negated anti-relevance are re-spectively measured by the differentiable nonlinear functions B 10 and B −10 , which are defined by"
B s (a) = (log(1 + exp(a · s)) − log(2))/s (3) and graphed in Figure 2.
Sample values of B 10 and g rel are shown in Figure 1.
How does this work?
The g rel feature is a sum over all unigrams in the document ~x.
"It does not fire strongly on the irrelevant or anti-relevant unigrams, since B 10 is close to zero there. [Footnote_11]"
"11 B 10 sets the threshold for relevance to be about 0. One could also include versions of the g rel feature that set a higher threshold, using B 10 (y · θ x m − threshold)."
"But it fires posi-tively on relevant unigrams w if they are tagged with I, and the strength of such firing increases approxi-mately linearly with θ w ."
"Since the weight φ rel &gt; 0 in practice, this means that raising a relevant unigram’s θ w (if y = +1) will proportionately raise its log-odds of being tagged with I. Symmetrically, since φ antirel &gt; 0 in practice, lowering an anti-relevant un-igram’s θ w (if y = +1) will proportionately lower its log-odds of being tagged with I, though not nec-essarily at the same rate as for relevant unigrams. [Footnote_12]"
"12 If the two rates are equal (φ rel = φ antirel ), we get a simpler model in which the log-odds change exactly linearly with θ w for each w, regardless of w’s relevance/irrelevance/anti-relevance. This follows from the fact that B s (a)+B −s (a) simplifies to a."
"Should φ also include traditional CRF emis-sion features, which would recognize that particular words like great tend to be tagged as I?"
Such features would undoubtedly do a better job predict-ing the rationales and hence increasing equation (1).
"However, crucially, our true goal is not to predict the rationales but to recover the classifier parame-ters θ."
"Thus, if great tends to be highlighted, then the model should not be permitted to explain this directly by increasing some feature φ great , but only indirectly by increasing θ great ."
"We therefore permit our rationale prediction model to consider only the two emission features g rel and g antirel , which see the words in ~x only through their θ-values."
"Annotators highlight more than just the relevant un-igrams. (After all, they aren’t told that our current θ-features are unigrams.)"
"They tend to mark full phrases, though perhaps taking care to exclude anti-relevant portions. φ models these phrases’ shape, via weights for several “language model” features."
"Most important are the 4 traditional CRF tag tran-sition features g O-O , g O-I , g I-I , g I-O ."
"For example, g O-I counts the number of O-to-I transitions in r~ (see Figure 1)."
"Other things equal, an annotator with high φ O-I is predicted to have many rationales per 1000 words."
"And if φ I-I is high, rationales are pre-dicted to be long phrases (including more irrelevant unigrams around or between the relevant ones)."
"We also learn more refined versions of these fea-tures, which consider how the transition probabil-ities are influenced by the punctuation and syntax of the document ~x (independent of ~θ)."
These re-fined features are more specific and hence more sparsely trained.
"Their weights reflect deviations from the simpler, “backed-off” transition features such as g O-I . (Again, see Figure 1 for examples.)"
Conditioning on left word.
"A feature of the form g t 1 (v)-t 2 is specified by a pair of tag types t 1 ,t 2 ∈ {I, O} and a vocabulary word type v. It counts the number of times an t 1 –t 2 transition occurs in r~ con-ditioned on v appearing as the first of the two word tokens where the transition occurs."
"Our experiments include g t 1 (v)-t 2 features that tie I-O and O-I tran-sitions to the 4 most frequent punctuation marks v (comma, period, ?, !)."
Conditioning on right word.
"A feature g t 1 -t 2 (v) is similar, but v must appear as the second of the two word tokens where the transition occurs."
"Again here, we use g t 1 -t 2 (v) features that tie I-O and O-I transitions to the four punctuation marks mentioned above."
"We also include five features that tie O-I transitions to the words no, not, so, very, and quite, since in our development data, those words were more likely than others to start rationales. [Footnote_13]"
"13 These are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate. We do not use any other lexical φ-features that reference ~x, for fear that they would enable the learner to explain the rationales without changing θ as desired (see the end of section 5.3)."
Conditioning on syntactic boundary.
We parsed each rationale-annotated training document (no parsing is needed at test time). [Footnote_14]
"14 We parse each sentence with the Collins parser[REF_CITE]. Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC."
"We then marked each word bigram x 1 -x 2 with three nonterminals: N End is the nonterminal of the largest constituent that contains x 1 and not x 2 , N Start is the nontermi-nal of the largest constituent that contains x 2 and not x 1 , and N Cross is the nonterminal of the smallest constituent that contains both x 1 and x 2 ."
"For a nonterminal N and pair of tag types (t 1 , t 2 ), we define three features, g t 1 -t 2 /E=N , g t 1 -t 2 /S=N , and g t 1 -t 2 /C=N , which count the number of times a t 1 -t 2 transition occurs in ~r with N matching the N End , N Start , or N Cross nonterminal, respectively."
"Our experiments include these features for 11 com-mon nonterminal types N (DOC, TOP, S, SBAR, FRAG, PRN, NP, VP, PP, ADJP, QP)."
"To train our model, we use L-BFGS to locally max-imize the log of the objective function (1): [Footnote_15] +C(X log p φ (r i | x i , y i , θ)) − 1 kφk 2 (4) i=1 2σ 2φ"
"15 One might expect this function to be convex because p θ and p φ are both log-linear models with no hidden variables. How-ever, log p φ (r i | x i , y i , θ) is not necessarily convex in θ."
"This defines p prior from (1) to be a standard diago-nal Gaussian prior, with variances σ 2θ and σ φ2 for the two sets of parameters."
We optimize σ θ2 in our ex-periments.
"As for σ φ2 , different values did not affect the results, since we have a large number of {I,O} rationale tags to train relatively few φ weights; so we simply use σ φ2 = 1 in all of our experiments."
Note the new C factor in equation (4).
"Our ini-tial experiments showed that optimizing equation (4) without C led to an increase in the likelihood of the rationale data at the expense of classification accu-racy, which degraded noticeably."
"This is because the second sum in (4) has a much larger magnitude than the first: in a set of 100 documents, it predicts around 74,000 binary {I,O} tags, versus the one hundred binary class labels."
"While we are willing to reduce the log-likelihood of the training classifi-cations (the first sum) to a certain extent, focusing too much on modeling rationales (the second sum) is clearly not our ultimate goal, and so we optimize C on development data to achieve some balance be-tween the two terms of equation (4)."
Typical values of C range from 3001 to 501 . [Footnote_16]
16 C also balances our confidence in the classifications y against our confidence in the rationales r; either may be noisy.
"We perform alternating optimization on θ and φ: 1. Initialize θ to maximize equation (4) but with C = 0 (i.e. based only on class data). 2. Fix θ, and find φ that maximizes equation (4). 3. Fix φ, and find θ that maximizes equation (4). 4. Repeat 2 and 3 until convergence."
The L-BFGS method requires calculating the gra-dient of the objective function (4).
"The partial derivatives with respect to components of θ and φ involve calculating expectations of the feature func-tions, which can be computed in linear time (with respect to the size of the training set) using the forward-backward algorithm for CRFs."
"The par-tial derivatives also involve the derivative of (3), to determine how changing θ will affect the firing strength of the emission features g rel and g antirel ."
We report on two sets of experiments.
"In the first set, we use the annotation data that A3–A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents)."
"In the second set, we used A0’s abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method[REF_CITE]."
The learning curves reported in section 8.1 are gen-erated exactly as[REF_CITE].
"Each curve shows classification accuracy at training set sizes T = 1, 2, ..., 9 folds (i.e. 200, 400, ..., 1600 training documents)."
"For a given size T, the reported accu-racy is an average of 9 experiments with different subsets of the entire training set, each of size T : 8 1 X acc(F 9 | F i+1 ∪ . . . ∪ F i+T ) (5) 9 i=0 where F j denotes the fold numbered j mod 9, and acc(F 9 | Y ) means classification accuracy on the held-out test set F 9 after training on set Y ."
"We use an appropriate paired permutation test, de-tailed[REF_CITE], to test differences in (5)."
We call a difference significant at p &lt; 0.05.
We compare our method to the “masking SVM” method[REF_CITE].
"Briefly, that method used rationales to construct several so-called con-trast examples from every training example."
A con-trast example is obtained by “masking out” one of the rationales highlighted to support the training ex-ample’s class.
A good classifier should have more trouble on this modified example.
"Hence,[REF_CITE]required the learned SVM to classify each contrast example with a smaller margin than the cor-responding original example (and did not require it to be classified correctly)."
The masking SVM learner relies on a simple geo-metric principle; is trivial to implement on top of an existing SVM learner; and works well.
"However, we believe that the generative method we present here is more interesting and should apply more broadly."
"First, the masking method is specific to improving an SVM learner, whereas our method can be used to improve any classifier by adding a rationale-based regularizer (the second half of equation (4)) to its objective function during training."
"More important, there are tasks where it is unclear how to generate contrast examples."
"For the movie review task, it was natural to mask out a rationale by pretending its words never occurred in the doc-ument."
"After all, most word types do not appear in most documents, so it is natural to consider the non-presence of a word as a “default” state to which we can revert."
"But in an image classification task, how should one modify the image’s features to ignore some spatial region marked as a rationale?"
There is usually no natural “default” value to which we could set the pixels.
"Our method, on the other hand, elim-inates contrast examples altogether."
Fig. 3 shows learning curves for four methods.
"A log-linear model shows large and significant im-provements, at all training sizes, when we incor-porate rationales into its training via equation (4)."
"Moreover, the resulting classifier consistently out-performs [Footnote_17] prior work, the masking SVM, which starts with a slightly better baseline classifier (an SVM) but incorporates the rationales more crudely."
"17 Differences are not significant at sizes 200, 1000, and 1600."
"To confirm that we could successfully model an-notators other than A0, we performed the same comparison for annotators A3–A5; each had pro-vided class and rationale annotations on a small 100-document training set."
We trained a separate φ for each annotator.
"Table 1 shows improvements over baseline, usually significant, at 2 training set sizes."
Examining the learned weights φ~ gives insight into annotator behavior.
"High weights include I-O and O-I transitions conditioned on punctuation, e.g., φ I(.)-O = 3.55, [Footnote_18] as well as rationales ending at the end of a major phrase, e.g., φ I-O/E=VP = 1.88."
18 When trained on folds F 4 –F 8 with A0’s rationales.
"The large emission feature weights, e.g., φ rel = 14.68 and φ antirel = 15.30, tie rationales closely to θ values, as hoped."
"For example, in Figure 1, the word w = succeeds, with θ w = 0.13, drives up p(I)/p(O) by a factor of 7 (in a positive document) relative to a word with θ w = 0."
"In fact, feature ablation experiments showed that almost all the classification benefit from rationales can be obtained by using only these 2 emission φ-features and the 4 unconditioned transition φ-features."
Our full φ (115 features) merely improves our ability to predict the rationales (whose likeli-hood does increase significantly with more features).
We also checked that annotators’ styles differ enough that it helps to tune φ to the “target” annota-tor A who gave the rationales.
"Table 3 shows that a φ model trained on A’s own rationales does best at pre-dicting new rationales from A. Table 2 shows that as a result, classification performance on the test set is usually best if it was A’s own φ that was used to help learn θ from A’s rationales."
"In both cases, however, a different annotator’s φ is better than nothing."
"We have demonstrated a effective method for elic-iting extra knowledge from naive annotators, in the form of lightweight “rationales” for their an-notations."
"By explicitly modeling the annotator’s rationale-marking process, we are able to infer a bet-ter model of the original annotations."
"We showed that our method performs signifi-cantly better than two strong baseline classifiers, and also outperforms our previous discriminative method for exploiting rationales[REF_CITE]."
We also saw that it worked across four anno-tators who have different rationale-marking styles.
"In future, we are interested in new domains that can adaptively solicit rationales for some or all training examples."
"Our new method, being essen-tially Bayesian inference, is potentially extensible to many other situations—other tasks, classifier archi-tectures, and more complex features."
"Having seen a news title “Alba denies wedding reports”, how do we infer that it is primar-ily about Jessica Alba, rather than about wed-dings or reports?"
"We probably realize that, in a randomly driven sentence, the word “Alba” is less anticipated than “wedding” or “reports”, which adds value to the word “Alba” if used."
Such anticipation can be modeled as a ratio between an empirical probability of the word (in a given corpus) and its estimated proba-bility in general English.
"Aggregated over all words in a document, this ratio may be used as a measure of the document’s topicality."
"As-suming that the corpus consists of on-topic and off-topic documents (we call them the core and the noise), our goal is to determine which documents belong to the core."
We pro-pose two unsupervised methods for doing this.
"First, we assume that words are sampled i.i.d., and propose an information-theoretic frame-work for determining the core."
"Second, we relax the independence assumption and use a simple graphical model to rank documents according to their likelihood of belonging to the core."
We discuss theoretical guarantees of the proposed methods and show their useful-ness for Web Mining and Topic Detection and Tracking (TDT).
"Many intelligent applications in the text domain aim at determining whether a document (a sentence, a snippet etc.) is on-topic or off-topic."
"In some appli-cations, topics are explicitly given."
"In binary text classification, for example, the topic is described in terms of positively and negatively labeled docu-ments."
"In information retrieval, the topic is imposed by a query."
"In many other applications, the topic is unspecified, however, its existence is assumed."
"Examples of such applications are within text sum-marization (extract the most topical sentences), text clustering (group documents that are close topi-cally), novelty detection (reason whether or not test documents are on the same topic as training docu-ments), spam filtering (reject incoming email mes-sages that are too far topically from the content of a personal email repository), etc."
"Under the (standard) Bag-Of-Words (BOW) rep-resentation of a document, words are the functional units that bear the document’s topic."
"Since some words are topical and some are not, the problem of detecting on-topic documents has a dual formulation of detecting topical words."
This paper deals with the following questions: (a) Which words can be con-sidered topical? (b) How can topical words be de-tected? (c) How can on-topic documents be detected given a set of topical words?
The BOW formalism is usually translated into the generative modeling terms by representing doc-uments as multinomial word distributions.
"For the on-topic/off-topic case, we assume that words in a document are sampled from a mixture of two multi-nomials: one over topical words and another one over general English (i.e. the background)."
"Obvi-ously enough, the support of the “topic” multinomial is significantly smaller than the support of the back-ground."
A document’s topicality is then determined by aggregating the topicality of its words (see below for details).
Note that by introducing the background distribution we refrain from explicitly modeling the class of off-topic documents—a document is sup-posed to be off-topic if it is “not topical enough”.
"Such a formulation of topicality prescribes us-ing the one-class modeling paradigm, as opposed to sticking to the binary case."
"Besides being much less widely studied and therefore much more attrac-tive from the scientific point of view, one-class mod-els appear to be more adequate for many real-world tasks, where negative examples are not straightfor-wardly observable."
One-class models separate the desired class of data instances (the core) from other data instances (the noise).
"Structure of noise is either unknown, or too complex to be explicitly modeled."
"One-class problems are traditionally approached using vector-space methods, where a convex deci-sion boundary is built around the data instances of the desired class, separating it from the rest of the universe."
"In the text domain, however, those vector-space models are questionably applicable—unlike effective binary vector-space models."
"In binary models, decision boundaries are linear 1 , whereas in (vector-space) one-class models, the boundaries are usually hyperspherical."
"Intuitively, since core docu-ments tend to lie on a lower-dimensional manifold[REF_CITE], inducing hyperspherical bound-aries may be sub-optimal as they tend to either cap-ture just a portion of the core, or capture too much space around it (see illustration in Figure 1)."
"Here we propose alternative ways for detecting the core, which work well in text."
One-class learning problems have been studied as either outlier detection or identifying a small coher-ent subset.
"In one-class outlier detecti[REF_CITE], the goal is to identify a few outliers from the given set of exam-ples, where the vast majority of the examples are considered relevant."
"Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers ([REF_CITE];"
Most of the one-class approaches employ ge-ometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes[REF_CITE]or hyperspheres[REF_CITE].
In this paper we adopt the latter approach: we formulate one-class clustering in text as an opti-mization task of identifying the most coherent subset (the core) of k documents drawn from a given pool of n &gt; k documents. [Footnote_2]
"2 The parameter k is analogous to the number of clusters in (multi-class) clustering, as well as to the number of outliers[REF_CITE]or the radius of Bregmanian ball[REF_CITE]—in other formulations of one-class clustering."
"Given a collection D of on-topic and off-topic documents, we assume that on-topic documents share a portion of their vocabulary that consists of “relatively rare” words, i.e. words that are used in D more often than they are used in general English."
We call them topical words.
"For example, if some doc-uments in D share words such as “Bayesian”, “clas-sifier”, “reinforcement” and other machine learning terms (infrequent in general English), whereas other documents do not seem to share any subset of words (besides stopwords), then we conclude that the ma-chine learning documents compose the core of D, while non-machine learning documents are noise."
"We express the level of topicality of a word w in terms of the ratio ρ(w) = pq((ww)) , where p(w) is w’s empirical probability (in D), and q(w) is its es-timated probability in general English."
"We discuss an interesting characteristic of ρ(w): if D is large enough, then, with high probability, ρ(w) values are greater for topical words than for non-topical words."
"Therefore, ρ(w) can be used as a mean to measure the topicality of w."
"Obviously, the quality of this measure depends on the quality of estimating q(w), i.e. the general En-glish word distribution, which is usually estimated over a large text collection."
"The larger the collec-tion is, the better would be the estimation."
"Recently, Google has released the Web [Footnote_1]T dataset [Footnote_3] that pro-vides q(w) estimated on a text collection of one tril-lion tokens."
"1 As such, or after applying the kernel trick[REF_CITE]"
We use it in our experimentation.
We propose two methods that use the ρ ratio to solve the one-class clustering problem.
"First, we ex-press documents’ topicality in terms of aggregating their words’ ρ ratios into an information-theoretic “topicality measure”."
The core is then composed of k documents with the highest topicality measure.
We show that the proposed measure is optimal for constructing the core cluster among documents of equal length.
"However, our method is not useful in a setup where some long documents have a top-ical portion: such documents should be considered on-topic, but their heavy tail of background words overcomes the topical words’ influence."
We gener-alize our method to non-equally-long documents by first extracting words that are supposed to be topi-cal and then projecting documents over those words.
Such projection preserves the optimality characteris-tic and results in constructing a more accurate core cluster in practice.
We call such a method of choos-ing both topical words and core documents One-Class Co-Clustering (OCCC).
It turns out that our OCCC method’s performance depends heavily on choosing the number of topical words.
We propose a heuristic for setting this num-ber.
"As another alternative, we propose a method that does not require tuning this parameter: we use words’ ρ ratios to initialize an EM algorithm that computes the likelihood of documents to be-long to the core—we then choose k documents of maximal likelihood."
We call this model the Latent Topic/Background (LTB) model.
LTB outperforms OCCC in most of our test cases.
Our one-class clustering models have interesting cross-links with models applied to other Informa-tion Retrieval tasks.
"For example, a model that resembles our OCCC, is proposed[REF_CITE]for query performance prediction."
These types of cross-links are common for the models that are general enough and relatively simple.
"In this paper we put particular emphasis on the simplicity of our models, such that they are feasible for theoretical analysis as well as for efficient implementation."
Recall that we use the ρ(w) = qp((ww)) ratios to express the level of our “surprise” of seeing the word w.
"A high value of ρ(w) means that w is used in the cor-pus more frequently than in general English, which, we assume, implies that w is topical."
"The more top-ical words a document contains, the more “topical” it is—k most topical documents compose the core D k ⊂ D."
An important question is whether or not the ρ ra-tios are sufficient to detecting the actually topical words.
"To address this question, let us model the corpus D using a simple graphical model (Figure 2 left)."
"In this model, the word distribution p(w) is represented as a mixture of two multinomial distri-butions: p r over a set R of topical words, and p g over all the words G ⊃ R in D. For each word w ij in a document d i , we toss a coin Z ij , such that, if Z ij = 1, then w ij is sampled from p r , otherwise it is sampled from p g ."
"Define π , p(Z ij = 1)."
"If |G| À |R| À 0, and if π À 0, then top-ical words would tend to appear more often than non-topical words."
"However, we cannot simply base our conclusions on word counts, as some words are naturally more frequent than others (in general En-glish)."
Figure 3 (left) illustrates this observation: it shows words’ p(w) values sorted by their q(w) val-ues.
"It is hard to fit a curve that would separate be-tween R and G \ R. We notice however, that we can “flatten” this graph by drawing ρ(w) values instead (see Figure 3 right)."
"Here, naturally frequent words are penalized by the q factor, so we can assume that, when re-normalized, ρ(w) behaves as a mixture of two discrete uniform distributions."
A simple thresh-old can then separate between R and G \ R.
"Proposition 1 Under the uniformity assumption, it is sufficient to have a log-linear size sample (in |G|) in order to determine the set R with high probability."
"The proposi-tion states that in corpora of practical size [Footnote_4] the set of topical words can be almost perfectly detected, sim-ply by taking words with the highest ρ ratios."
"4 N = O(m log m), where N is the number of word tokens in D, and m = |G| is the size of the vocabulary."
"Con-sequently, the core D k will consist of k documents, each of which contains more topical words than any document from D \"
D k .
"To illustrate this theoretical result, we followed the generative process as described above, and con-structed an artificial dataset with characteristics sim-ilar to those of our WAD dataset (see Section 5.1)."
"In particular, we fixed the size of the artificial dataset to be equal to the size of the WAD dataset (N = 330,000)."
We set the ratio of topical words to 0.2 and assumed uniformity of the ρ values.
"In this setup, we were able to detect the set of topical words with a 98.5% accuracy."
"In this section, we propose a simple information-theoretic algorithm for identifying the core D k , and show that it is optimal under the uniformity assump-tion."
"Given the ρ ratios of words, the aggregated topicality of the corpus D can be expressed in terms of the KL-divergence:"
"X p(w) KL(p||q) = p(w) log q(w) w∈G X p(w) = p(d, w) log q(w). d∈D,w∈G"
A document d’s contribution to the aggregated topi-cality measure will assess the topicality of d:
"X KL d (p||q) = p(d, w) log p(w). (1) q(w) w∈G"
The core D k will be composed of documents with the highest topicality scores.
"A simple, greedy algo-rithm for detecting D k is then: 1. Sort documents according to their topicality value (1), in decreasing order. 2. Select the first k documents."
Since the algorithm chooses documents with high values of the KL divergence we call it the Max-KL algorithm.
We now argue that it is optimal under the uniformity assumption.
"Indeed, if the corpus D is large enough, then according to Proposition 1 (with high probability) any topical word w has a lower ρ ratio than any non-topical word."
Assume that all documents are of the same length (|d| is con-stant).
"The Max-KL algorithm chooses documents that contain more topical words than any other doc-ument in the corpus—which is exactly the definition of the core, as presented in Section 1."
"We summarize this observation in the following proposition: Proposition 2 If the corpus D is large enough, and all the documents are of the same length, then the Max-KL algorithm is optimal for the one-class clus-tering problem under the uniformity assumption."
"In contrast to the (quite natural) uniformity assump-tion, the all-the-same-length assumption is quite re-strictive."
Let us now propose an algorithm that over-comes this issue.
"As accepted in Information Retrieval, we decide that a document is on-topic if it has a topical portion, no matter how long its non-topical portion is."
"There-fore, we decide about documents’ topicality based on topical words only—non-topical words can be completely disregarded."
"This observation leads us to proposing a one-class co-clustering (OCCC) algo-rithm: we first detect the set R of topical words, rep-resent documents over R, and then detect D k based on the new representation. [Footnote_5]"
"5 OCCC is the simplest, sequential co-clustering algorithm, where words are clustered prior to clustering documents (see, e.g.,[REF_CITE]). In OCCC, word clustering is analogous to feature selection. More complex algorithms can be considered, where this analogy is less obvious."
We reexamine the document’s topicality score (1) and omit non-topical words.
The new score is then:
"X p(w) KL rd (p||q) = p 0 (d, w) log q(w), (2) w∈R where p 0 (d, w) = p(d, w)/(P w∈R p(d, w)) is a joint distribution of documents and (only) topical words."
"The OCCC algorithm first uses ρ(w) to choose the most topical words, then it projects doc-uments on these words and apply the Max-KL algo-rithm, as summarized below: 1. Sort words according to their ρ ratios, in de-creasing order. 2. Select a subset R of the first m r words. 3."
"Represent documents as bags-of-words over R (delete counts of words from G \ R). 4. Sort documents according to their topicality score (2), in decreasing order. 5. Select a subset D k of the first k documents."
"Considerations analogous to those presented in Sec-tion 2.1, lead us to the following result: Proposition 3 If the corpus D is large enough, the OCCC algorithm is optimal for one-class clustering of documents, under the uniformity assumption."
"Despite its simplicity, the OCCC algorithm shows excellent results on real-world data (see Section 5)."
"OCCC’s time complexity is particularly appealing: O(N), where N is the number of word tokens in D."
The choice of m r = |R| can be crucial.
We propose a useful heuristic for choosing it.
"We assume that the distribution of ρ ratios for w ∈ R is a Gaussian with a mean µ r À 1 and a variance σ r2 , and that the distribution of ρ ratios for w ∈ G \ R is a Gaussian with a mean µ nr = 1 and a variance σ nr2 ."
We also assume that all the words with ρ(w) &lt; 1 are non-topical.
"Since Gaussians are symmetric, we further assume that the number of non-topical words with ρ(w) &lt; 1 equals the number of non-topical words with ρ(w) ≥ 1."
"Thus, our estimate of |G\R| is twice the number of words with ρ(w) &lt; 1, and then the number of topical words can be estimated as m r = |G| − 2 · #{words with ρ(w) &lt; 1}."
"Instead of sharply thresholding topical and non-topical words, we can have them all, weighted with a probability of being topical."
"Also, we notice that our original generative model (Figure 2 left) assumes that words are i.i.d. sampled, which can be relaxed by deciding on the document topicality first."
"In our new generative model (Figure 2 right), for each doc-ument d i , Y i is a Bernoulli random variable where"
Algorithm 1 EM algorithm for one-class clustering using the LTB model.
P t P t p tr+1 (w l ) = i α i P j δ(wP ij = w l ) β ij i α ti j β tij
P t P t p gt+1 (w l ) =
N w − i α i P j δ(Pw ij = w l ) β ij N − i α ti j β tij
Y i = 1 corresponds to d i being on-topic.
"As be-fore, Z ij decides on the topicality of a word token w ij , but now given Y i ."
"Since not all words in a core document are supposed to be topical, then for each word of a core document we make a separate decision (based on Z ij ) whether it is sampled from p r (W ) or p g (W )."
"However, if a document does not belong to the core (Y i = 0), each its word is sampled from p g (W ), i.e. p(Z ij = 0|Y i = 0) = 1."
"Inspired[REF_CITE], we use the Expectation-Maximization (EM) algorithm to exactly estimate parameters of our model from the dataset."
"We now describe the model parameters Θ. First, the probability of any document to belong to the core is denoted by p(Y i = 1) = kn = p d (this parameter is fixed and will not be learnt from data)."
"Second, for each document d i , we maintain a proba-bility of each its word to be topical given that the document is on-topic, p(Z ij = 1|Y i = 1) = π i for i = 1,...,n."
"Third, for each word w l (for k = 1...m), we let p(w l |Z l = 1) = p r (w l ) and p(w l |Z l = 0) = p g (w l )."
"The overall number of pa- rameters is n + 2m + 1, one of which (p d ) is preset."
The dataset likelihood is then: j=1
"At each iteration t of the EM algorithm, we first perform the E-step, where we compute the poste-rior distribution of hidden variables {Y i } and {Z ij } given the current parameter values Θ t and the data D. Then, at the M-step, we compute the new pa-rameter values Θ t+1 that maximize the model log-likelihood given Θ t , D and the posterior distribution."
The initialization step is crucial for the EM al-gorithm.
"Our pilot experimentation showed that if distributions p r (W) and p g (W) are initialized as uniform, the EM performance is close to random."
"Therefore, we decided to initialize word probabili-ties using normalized ρ scores."
"We do not propose the optimal way to initialize π i parameters, however, as we show later in Section 5, our LTB model ap-pears to be quite robust to the choice of π i ."
The EM procedure is presented in Algorithm 1.
"For details, see[REF_CITE]."
"After T itera-tions, we sort the documents according to α i in de-creasing order and choose the first k documents to be the core."
The complexity of Algorithm 1 is lin-ear: O(TN).
"To avoid overfitting, we set T to be a small number: in our experiments we fix T = 5."
"We evaluate our OCCC and LTB models on two ap-plications: a Web Mining task (Section 5.1), and a Topic Detection and Tracking (TDT)[REF_CITE]task (Section 5.2)."
"To define our evaluation criteria, let C be the con-structed cluster and let C r be its portion consisting of documents that actually belong to the core."
"We define precision as Prec = |C r |/|C|, recall as Rec = |C r |/k and F-measure as (2 Prec Rec)/(Prec+Rec)."
"Unless stated otherwise, in our experiments we fix |C| = k, such that precision equals recall and is then called one-class clustering accuracy, or just accu-racy."
We applied our one-class clustering methods in four setups: • OCCC with the heuristic to choose m r (from Section 3.1). • OCCC with optimal m r .
We unfairly choose the number m r of topical words such that the resulting accuracy is maximal.
"This setup can be considered as the upper limit of the OCCC’s performance, which can be hypotheti-cally achieved if a better heuristic for choosing m r is proposed. • LTB initialized with π i = 0.5 (for each i)."
"As we show in Section 5.1 below, the LTB model demonstrates good performance with this straightforward initialization. • LTB initialized with π i = p d ."
"Quite naturally, the number of topical words in a dataset de-pends on the number of core documents."
"For example, if the core is only 10% of a dataset, it is unrealistic to assume that 50% of all words are topical."
"In this setup, we condition the ratio of topical words on the ratio of core documents."
We compare our methods with two existing al-gorithms: (a) One-Class SVM clustering [Footnote_6][REF_CITE]; (b) One-Class Rate Distortion (OC-RD)[REF_CITE].
6 We used Chih-Jen Lin’s LibSVM with the -s 2 parame-ter. We provided the core size using the -n parameter.
The later is considered a state-of-the-art in one-class clustering.
"Also, to es-tablish the lowest baseline, we show the result of a random assignment of documents to the core D k ."
The OC-RD algorithm is based on rate-distortion theory and expresses the one-class problem as a lossy coding of each instance into a few possible instance-dependent codewords.
"Each document is represented as a distribution over words, and the KL-divergence is used as a distortion function (gener-ally, it can be any Bregman function)."
The algo-rithm also uses an “inverse temperature” parameter (denoted by β) that represents the tradeoff between compression and distortion.
"An annealing process is employed, in which the algorithm is applied with a sequence of increasing values of β, when initial-ized with the result obtained at the previous itera- tion."
The outcome is a sequence of cores with de-creasing sizes.
The annealing process is stopped once the largest core size is equal to k.
Web appearance disambiguation (WAD) is proposed[REF_CITE]as the problem of reasoning whether a particular mention of a per-son name in the Web refers to the person of interest or to his or her unrelated namesake.
"The problem is solved given a few names of people from one social network, where the objective is to construct a cluster of Web pages that mention names of related people, while filtering out pages that mention their unrelated namesakes."
"WAD is a classic one-class clustering task, that is tackled by Bekkerman and McCallum with simu-lated one-class clustering: they use a sophisticated agglomerative/conglomerative clustering method to construct multiple clusters, out of which one cluster is then selected."
They also use a simple link struc-ture (LS) analysis method that matches hyperlinks of the Web pages in order to compose a cloud of pages that are close to each other in the Web graph.
The authors suggest that the best performance can be achieved by a hybrid of the two approaches.
"We test our models on the WAD dataset, [Footnote_7] which consists of 1085 Web pages that mention 12 people names of AI researchers, such as Tom Mitchell and Leslie Kaelbling."
"Out of the 1085 pages, 420 are on-topic, so we apply our algorithms with k = 420."
"At a preprocessing step, we binarize document vec-tors and remove low frequent words (both in terms of p(w) and q(w))."
The results are summarized in the middle column of Table 1.
"We can see that both OCCC and LTB dramatically outperform their com-petitors, while showing practically indistinguishable results compared to each other."
"Note that when the size of the word cluster in OCCC is unfairly set to its optimal value, m r = 2200, the OCCC method is able to gain a 2% boost."
"However, for obvious reasons, the optimal value of m r may not always be obtained in practice."
Table 2 lists a few most topical words according to the OCCC and LTB models.
"The OCCC algo-rithm sorts words according to their ρ scores, such that words that often occur in the dataset but rarely in the Web, are on the top of the list."
"These are mostly last names or login names of researchers, venues etc."
"The EM algorithm of LTB is the given ρ scores as an input to initialize p 1r (w) and p 1g (w), which are then updated at each M-step."
"In the LTB columns, words are sorted by p 5r (w)."
High quality of the LTB list is due to conditional dependencies in our generative model (via the Y i nodes).
"Solid lines in Figure 4 demonstrate the robustness of our models to tuning their main parameters (m r for OCCC, and the π i initialization for LTB)."
"As can be seen from the left panel, OCCC shows robust performance: the accuracy above 80% is obtained when the word cluster is of any size in the 1000– 3000 range."
The heuristic from Section 3.1 suggests a cluster size of 1000.
"The LTB is even more robust: practically any value of π i (besides the very large ones, π i ≈ 1) can be chosen."
"To perform a fair comparison of our results with those obtained[REF_CITE], we construct hybrids of their link struc-ture (LS) analysis model with our OCCC and LTB, as follows."
"First, we take their LS core cluster, which consists of 360 documents."
"Second, we pass over all the WAD documents in the order as they were ranked by either OCCC or LTB, and enlarge the LS core with 60 most highly ranked documents that did not occur in the LS core."
"In either case, we end up with a hybrid core of 420 documents."
Dotted lines in Figure 4 show accuracies of the resulting models.
"As the F-measure of the hy-brid model proposed[REF_CITE]is 80.3%, we can see that it is signifi-cantly inferior to the results of either OCCC+LS or LTB+LS, when their parameters are set to a small value (m r &lt; 3000 for OCCC, π i &lt; 0.06 for LTB)."
Such a choice of parameter values can be explained by the fact that we need only 60 docu-ments to expand the LS core cluster to the required size k = 420.
"When the values of m r and π i are small, both OCCC and LTB are able to build very small and very precise core clusters, which is exactly what we need here."
"The OCCC+LS hybrid is par-ticularly successful, because it uses non-canonical words (see Table 2) to compose a clean core that al-most does not overlap with the LS core."
"Remark-ably, the OCCC+LS model obtains 86.4% accuracy with m r = 100, which is the state-of-the-art result on the WAD dataset."
"To answer the question how much our models are sensitive to the choice of the core size k, we com-puted the F-measure of both OCCC and LTB as a function of k (Figure 5)."
It turns out that our meth-ods are quite robust to tuning k: choosing any value in the 300–500 range leads to good results.
Real-world data rarely consists of a clean core and uniformly distributed noise.
"Usually, the noise has some structure, namely, it may contain coherent components."
"With this respect, one-class clustering can be used to detect the largest coherent compo-nent in a dataset, which is an integral part of many applications."
"In this section, we solve the problem of automatically detecting the Topic of the Week (TW) in a newswire stream, i.e. detecting all articles in a weekly news roundup that refer to the most broadly discussed event."
"We evaluate the TW detection task on the bench-mark TDT-5 dataset [URL_CITE] , which consists of 250 news events spread over a time period of half a year, and 9,812 documents in English, Arabic and Chinese (translated to English), annotated by their relation-ship to those events. [Footnote_9] The largest event in TDT-5 dataset (#55106, titled “Bombing in Riyadh, Saudi Arabia”) has 1,144 documents, while 66 out of the 250 events have only one document each."
"9 We take into account only labeled documents, while ignor-ing unlabeled documents that can be found in the TDT-5 data."
"We split the dataset to 26 weekly chunks (to have 26 full weeks, we delete all the documents dated with the last day in the dataset, which decreases the dataset’s size to 9,781 documents)."
Each chunk contains from 138 to 1292 documents.
"The one-class clustering accuracies, macro-averaged over the 26 weekly chunks, are presented in the right column of Table 1."
"As we can see, both LTB models, as well as OCCC with the optimal m r , outperform our baselines."
"Interestingly, even the op-timal choice of m r does not lead OCCC to signif-icantly superior results while compared with LTB."
The dataset-dependent initialization of LTB’s π i pa-rameters (π i = p d ) appears to be preferable over the dataset-independent one (π i = 0.5).
Accuracies per week are shown in Figure 6.
These results reveal two interesting observations.
"First, OCCC tends to outperform LTB only on data chunks where the results are quite low in general (less than 60% accuracy)."
"Specifically, on weeks 2, 4, 11, and 16 the LTB models show extremely poor per-formance."
"While investigating this phenomenon, we discovered that in two of the four cases LTB was able to construct very clean core clusters, however, those clusters corresponded to the second largest topic, while we evaluate our methods on the first largest topic. [Footnote_10] Second, the (completely unsuper- vised) LTB model can obtain very good results on some of the data chunks."
"10 For example, on the week-4 data, topic #55077 (“River ferry sinks on Bangladeshi river”) was discovered by LTB as the largest and most coherent one. However, in that dataset, topic #55077 is represented by 20 documents, while topic #55063 (“SARS Quarantined medics in Taiwan protest”) is represented by 27 documents, such that topic #55077 is in fact the second largest one."
"For example, on weeks 5, 8, 19, 21, 23, 24, and 25 the LTB’s accuracy is above 90%, with a striking 100% on week-23."
We have developed the theory and proposed practi-cal methods for one-class clustering in the text do-main.
"The proposed algorithms are very simple, very efficient and still surprisingly effective."
More sophisticated algorithms (e.g. an iterative [Footnote_11] version of OCCC) are emerging.
"11 See, e.g.,[REF_CITE]"
We thank Erik Learned-Miller for the inspiration on this project.
"We also thank Gunjan Gupta, James Allan, and Fernando Diaz for fruitful dis-cussions."
This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number[REF_CITE]- 06-C-0023.
"Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsor."
We propose a new approach to language mod-eling which utilizes discriminative learning methods.
"Our approach is an iterative one: starting with an initial language model, in each iteration we generate &apos;false&apos; sentences from the current model, and then train a clas-sifier to discriminate between them and sen-tences from the training corpus."
"To the extent that this succeeds, the classifier is incorpo-rated into the model by lowering the probabil-ity of sentences classified as false, and the process is repeated."
We demonstrate the effec-tiveness of this approach on a natural lan-guage corpus and show it provides an 11.4% improvement in perplexity over a modified kneser-ney smoothed trigram.
"Language modeling is a fundamental task in natu-ral language processing and is routinely employed in a wide range of applications, such as speech recognition, machine translation, etc’."
"Tradition-ally, a language model is a probabilistic model which assigns a probability value to a sentence or a sequence of words."
We refer to these as generative language models.
"A very popular example of a generative language model is the n-gram, which conditions the probability of the next word on the previous (n-1)-words."
"Although simple and widely-applicable, it has proven difficult to allow n-grams, and other forms of generative language models as well, to take ad- vantage of non-local and overlapping features. [Footnote_1] These sorts of features, however, pose no problem for standard discriminative learning methods, e.g. large-margin classifiers."
"1 Conditional maximum entropy models[REF_CITE]provide somewhat of a counter-example, but there, too, many kinds of global and non-local features are difficult to use[REF_CITE]."
"For this reason, a new class of language model, the discriminative lan-guage model, has been proposed recently to aug-ment generative language models[REF_CITE]."
"Instead of providing probability values, discriminative language models directly classify sentences as either correct or in-correct, where the definition of correctness de-pends on the application (e.g. grammatical / ungrammatical, correct translation / incorrect trans-lation, etc&apos;)."
Discriminative learning methods require negative samples.
"Given that the corpora used for training language models contain only real sentences, i.e. positive samples, obtaining these can be problematic."
"In most work on discriminative language modeling this was not a major issue as the work was concerned with specific applications, and these provided a natural definition of negative samples."
"For instance,[REF_CITE]proposed a discriminative language model for a speech recognition task."
"Given an acoustic sequence, a baseline recognizer was used to generate a set of possible transcriptions."
"The correct transcription was taken as a positive sample, while the rest were taken as negative samples."
"More recently, however,[REF_CITE]showed that a discriminative language model can be trained independently of a specific application by using a generative language model to obtain the negative samples."
"Using a non-linear large-margin learning algorithm, they successfully trained a classifier to discriminate real sentences from sentences generated by a trigram."
In this paper we extend this line of work to study the extent to which discriminative learning methods can lead to better generative language models per-se.
"The basic intuition is the following: if a classifier can be used to discriminate real sen-tences from &apos;false&apos; sentences generated by a lan-guage model, then it can also be used to improve that language model by taking probability mass away from sentences classified as false and trans-ferring it to sentences classified as real."
"If the re-sulting language model can be efficiently sampled from, then this process can be repeated, until gen-erated sentences can no longer be distinguished from real ones."
"The remainder of the paper is structured as follows: In the next section we formally develop this intuition, providing a quick overview of the whole-sentence maximum-entropy model and of self-supervised boosting, two previous works on which we rely."
"We also present the method we use for sampling from the current model, which for the present work is far more efficient than the classical Gibbs sampling."
"Our experimental results are presented in section 3, and section 4 concludes with a discussion and a future outlook."
The vast majority of statistical language models estimate the probability of a given sentence as a product of conditional probabilities via the chain rule: def def n P(s) =
P(w 1 ...w n ) = ∏
P(w i | h i ) (1) i=1 def where h i = w 1 ... w i−1 is called the history of the word w i .
Most work on language modeling therefore is directed at the estimation of P(w i | h i ) .
"While this is theoretically correct, it makes it difficult to incorporate global information about the sentence into the model, e.g. length, grammaticality, etc&apos;."
"For this reason, the whole-sentence maximum-entropy model was proposed[REF_CITE]."
In the WSME model the probability of a sentence is defined directly as:
P(s) = 1 P (s)⋅exp( ∑ λ f (s)) i i (2) Z 0 i
"P 0 (s) is some baseline model, def Z = ∑ P 0 (s)⋅exp( ∑ λ i f i (s)) is a normalization s i constant and the {f i }&apos;s are features encoding some information about the sentence."
"Most generally, a feature is a function from the set of word sequences to R, the set of real numbers."
"However, in most applications, as in our work, the features are taken to be binary."
"Lastly, the {λ i }&apos;s are real coefficients encoding the relative importance of their corresponding features."
"In the WSME framework the set of features {f i } is given ahead of training by the modeler, and learning consists of estimating the coefficients {λ i }."
This is done by stipulating the constraints def 1 N N ∑ f (s ) E p ( f i ) =
"E pɶ ( f i ) = (3) i j j=1 where pɶ is the empirical distribution defined by the training set {s 1 , ... s N }. [Footnote_2] If these constraints are consistent then there is a unique solution in {λ i } that satisfies them."
2 Sometimes a smoothed version of (3) is used instead (e.g.[REF_CITE]).
This solution is guaranteed to be the one closest to P 0 in the Kullback-Leblier sense among all solutions satisfying (3).
It is also guaranteed to be the maximum likelihood solution for the exponential family.
"For more details, see[REF_CITE]."
A different approach to learning the same sort of model as in (2) was proposed[REF_CITE].
"Here, instead of having all the features pre-given, they are learned one at a time along with their corresponding coefficients."
"Welling et al. show that adding a new feature to (2) can be interpreted as gradient ascent on the log-likelihood function, and show that the optimal feature is the one that best discriminates real data from data sampled from the current model."
"To see this, let"
E ( s ) = ln( P 0 ( s )) + ∑ λ i f i ( s ) (4) i denote the energy associated with sentence s. [Footnote_3] Equation (2) can now be rewritten as -
"3 In[REF_CITE]the term for P 0 does not appear, which is equivalent to taking the uniform distribution as the baseline model."
P(s) = 1 exp(E(s)) (5) Z where Z is a normalization constant as before.
"The derivative of the log-likelihood with respect to an arbitrary parameter β is then – ∂L = − 1 ∑ N ∂E(s i ) + ∑ P(s) ∂E(s) (6) ∂ β N i=1 ∂ β ∂ β s∈S where {s 1 , ... s N } is once again the training corpus, and the second sum runs over the set of all word sequences."
"Now, suppose we change the energy function by adding an infinitesimal multiple of a new feature f * ."
The log-likelihood after adding the feature can be approximated by –
L(E(s) + ε f * (s)) ≈ L(E(s)) + ε ∂L (7) ∂ ε where the derivative of L is taken at ε =0 .
"Because the optimal feature is the one that maximizes the increase in log-likelihood, we are searching for a feature that maximizes this derivative."
Using equation (6) and noting that ∂E = f * we have – ∂ ε ∂L = − 1 ∑ f (s ) + ∑ P(s) f (s) N * * i (8) ∂ ε N i=1 s∈S
"This expression cannot be computed in practice, because the set of all word sequences S is infinite."
The second term however can approximated using samples {u i } from the current model – ∂L ≈− 1 ∑ f (s )+ 1 ∑ f (u ) N N * * i (9) i ∂ ε N i=1 N i=1
"In other words, given a set of N samples {u i } from the model, the optimal feature to add is one that gives high scores to sampled sentences and low ones to real sentences."
"By labeling real sentences with 0 and sampled sentences with 1, the task of learning the feature translates into the task of training a classifier to discriminate between these two classes of sentences."
In the remainder of the paper we will use feature and classifier interchangeably.
"Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling."
"Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data."
Đn both cases essentially linear classifiers were used as features.
"As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples."
"Unfortunately, as shown[REF_CITE], with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non-linear large-margin classifier (see section 3 for details)."
"While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function – taking time that can be linear in the size of their training data."
"This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications."
For this reason we use a different sampling scheme which we refer to as rejection sampling.
"This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn&apos;t too far removed from the baseline."
"We will start by describing the sampling process, and then show that the probability distribution it samples from has the form of equation (2)."
"To sample a sentence from the cur-rent model, we generate one from the baseline model, and then pass it through each of the classi-fiers in the model ."
"If a given classifier classifies the sentence as a model sentence, then it is rejected with a certain probability associated with this clas-sifier."
Only if a sentence is accepted by all classifi-ers is it taken as a sample sentence.
"Otherwise, the sampling process is restarted."
Let us derive an expression for the probability of a sentence s generated in this manner.
"To simplify notation, assume that at this point we added but a single feature f to the baseline model P 0 , and let p rej stand for the rejection probability associated with it."
"Furthermore, let p - stand for the accuracy of f in classifying sentences sampled from P 0 (negative samples)."
"Formally, p − = E P ( f ) (10) 0"
First let&apos;s assume that f ( s ) =1 .
"The probability for generating s is a sum of the probabilities of two disjoint outcomes – the probability of generating s as the first sentence and having it survive the rejection, plus the probability of generating in the first iteration some sentence s&apos; such that f ( s &apos;) =1 , rejecting that, and then generating s in one of the subsequent iterations."
"Formally, this means that –"
P 1 (s) = (1− p rej )
"P 0 (s) + p − p rej P 1 (s) (11) Rearranging, we have – 1− p rej P 1 (s) = P 0 (s) (12) 1− p − p rej"
"Similarly, the probability for a sentence s for which f (s) = 0 is the probability of generating s as the first sentence, plus the probability of generating some other sentence s&apos; for which f (s&apos;) =1 , rejecting it, and then generating s in a future iteration."
P 1 (s) = P 0 (s) + p − p rej P 1 (s) (13) and hence – 1 P 1 (s) = P 0 (s) (14) 1− p − p rej
"Letting Z =1− p − p rej , and letting λ = ln(1− p rej ) , we have, for all s – 1 P 1 (s) = P 0 (s)⋅exp( λ ⋅ f (s)) (15) Z"
This process can be trivially generalized for N features.
"Let – p − i = E P ( f ) (16) i−1 i stand for f i &apos;s accuracy in classifying sentences generated from P i-1 , and let p reji be the rejection probability associated with the i&apos;th feature."
Sampling from the model then proceeds by sampling a sentence s from P 0 .
"For each 1 ≤ i ≤ N , in order, if f i ( s ) =1 , then we attempt to reject s with probability p reji ."
"If s survives all the rejection attempts, it is returned as the next sample."
Using similar arguments as before it&apos;s possible to show that if we take λ i = ln(1− p reji ) and –
N Z = ∏ (1− p i − p irej ) (17) i = 1 then the probability of a sentence s sampled by this process is given by equation (2).
"Conversely, this shows that rejection sampling can be used for obtaining negative samples from the model given in (2) by taking p reji =1−exp( λ i ) , as long as 0 &lt; exp( λ i ) ≤1 ."
"In section 3 we show that in our experimental setup, rejection sampling brings about enormous savings in the number of classifications necessary during training, as compared with Gibbs sampling."
"Given the current model P i and a new feature f i+1 , we wish to find the optimal λ i+1 , or equivalently its optimal rejection probability p reji +1 ."
"In the WSME framework, the weights of the features are set in such a way that the expected value of the features on sentences sampled from the model equals their expected value on real sentences."
A possible way to set the weight of a new feature is therefore to set p reji+1 such that the constraint:
E P ( f ) =
"E ɶp ( f i+1 ) (18) i+1 i+1 is satistfied, where p ɶ is once again the empirical distribution defined by the training set."
"Intuitively, this means that the new feature could no longer be used to discriminate between sentences sampled from P i+1 and real sentences."
"However, setting p reji+1 in this manner may violate the constraints (18) associated with the features already existing in the model, thus hampering the model&apos;s performance."
"Therefore, we set the new feature&apos;s rejection probability by directly searching for the one that minimizes an estimate of P i+1 &apos;s perplexity on a set of held out real sentences."
"To do this, we first sample a new set of sentences from P i , independently of the set that was used for training f i + 1 , and use it to estimate p −i+1 ."
"For any arbitrarily determined p irej+1 , this enables us to calculate an estimate for the normalization constant Z (equation 17), and therefore an estimate for P i+1 ."
We do this for a range of possible values for p reji+1 and pick the one that leads to the largest reduction in perplexity on the held out data. [Footnote_4]
"4 Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001. This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated."
We tested our approach on the ATIS natural language corpus[REF_CITE].
"We split the corpus into a training set of 11,000 sentences, a held-out set containing 1,045 sentences, and a test set containing 1,000 sentences which were reserved for measuring perplexity."
The corpus was pre-processed so that every word appearing less than three times was replaced by a special UNK symbol.
The resulting lexicon contained 603 word types.
Our learning framework leaves open a number of design choices: 1.
"Baseline language model: For P 0 we used a trigram with modified kneser-ney smoothing [[REF_CITE]], which is still considered one of the best smoothing methods for n-gram language models. 2."
"Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained."
"A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled."
T he value of the n&apos;th coordinate in the vector representation of sentence s was set to the number of times the corresponding n-gram appeared in s. 3.
Type of classifiers: For our features we used large-margin classifiers trained using the online algorithm described[REF_CITE].
The code for the classifier was generously provided by Daisuke Okanohara.
This code was extensively optimized to take advantage of the very sparse sentence representation described above.
"As shown[REF_CITE], using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences."
"Therefore, we used a 3rd order polynomial kernel, which was found to give good results."
No special effort was otherwise made in order to optimize the parameters of the classifiers. 4.
Stopping criterion: The process of adding features to the model was continued until the classification performance of the next feature was within 2% of chance performance.
We refer to the language model obtained by this approach as the boosted model to distinguish it from the baseline model.
To estimate the boosted model&apos;s perplexity we needed to estimate the normalization constant Z in equation (2).
Since this constant is equal to E P (exp( ∑ λ i f i )) it can be 0 i estimated from a large-enough sample from P 0 .
"5 Alternatively we could have used our estimate for P N (s) de-scribed in section 2.4. A large sample of sentences would still be necessary though, to get a good estimate for equation (16)."
The algorithm converged after 21 features were added to the model.
Figure 1 presents the model&apos;s perplexity on the test set estimated after each iteration.
The perplexity of the final model is 9.02.
"In comparison, the perplexity of the modified kneser-ney smoothed trigram on this corpus is 10.18."
This is an 11.4% improvement relative to the baseline model.
Figure 2 shows the accuracy of the trained features on held-out data.
"The held-out data was composed of equal parts real and model sentences, so 50% accuracy is chance performance."
"As might have been expected, the classifiers start out with a relatively high accuracy of 68%, which dwindles down to little over 50% as more features are added to the model."
"Not surprisingly, there is a strong correlation between the accuracy of a feature and the reduction in perplexity it engenders (spearman correlation coefficient r=0.89, p&lt;10 -5 .)"
In tables 1 and 2 we show a representative sample of sentences from the baseline model and from the final model.
"As the baseline model is a trigram, it cannot capture dependencies that span a range longer than two words."
Hence sentences that start out seemingly in one topic and then veer off to another are common.
The global information available to the features used by the boosted model greatly reduces this phenomenon.
"To get a quantitative sense of this, we generated 200 sentences from each model and submitted them for grammaticality testing by a proficient (though non-native) English speaker."
"Of the trigram-generated please list costs in at pittsburgh what type of airplane is have an early morning what types of aircraft is that a meal what not nineteen forty two between boston and atlanta on august fifteenth which airlines fly american flying on what is the flight leaving pittsburgh after six p m what is the cost of flight d l three seventeen sixty five what time does flight at eight thirty eight a m and six p m what does fare code q w mean what kind of aircraft will i be flying on flights from philadelphia on saturday what is the fare for flight two nine six what is the cost of coach transcontinental flight u a three oh two from denver to san francisco sentences, 86 were deemed grammatical (43%), while of those generated by the boosted model 132 were grammatical (66%)."
This difference is statistically significant with p&lt;10 -5 .
"Finally, let us quantify the computational savings obtained from using rejection sampling."
Let |V| stand for the lexicon size (here |V|=603) and |L| for the average sentence length (|L|=14).
"In Gibbs sampling, a sentence is sampled by starting out with a random sequence of words."
"For each word position, the current word is replaced with each word in the lexicon, and the probability of the resulting sentence is calculated."
Then one of the words is randomly selected for this position in proportion to the calculated probabilities.
The sentence has to be scanned in this manner several times for the sample to approximate the model distribution.
"Assuming we perform only 3 scans for each sentence, Gibbs sampling would have thus required us to classify 3 |V || L |≈ 25,000 sentences per sampled sentence."
"Given that in each iteration we generate 12,045 sentences, and that in the n&apos;th iteration each sentence has to be classified by n features, this gives a total of roughly 7 ⋅10 10 classifications after 21 iterations."
"In contrast, using rejection sampling, we used only 6.7⋅10 7 classifications in total – a difference of over three orders of magnitude."
In this work we presented a method that enables using discriminative learning methods for refining generative language models.
Utilizing large-margin classifiers that are trained to discriminate real sentences from model sentences we showed that sizeable improvements in perplexity over a state-of-the-art smoothed trigram are possible.
Our method bears some similarity to the recently developed Contrastive Estimation method[REF_CITE].
Contrastive estimation (CE) was proposed as a means for training log-linear prob-abilistic models.
"As all training methods, contras-tive estimation pushes probability mass unto positive samples."
"Unlike other methods, CE takes this probability mass from the &apos;neighborhood&apos; of each positive sample."
"For example, given a real sentence s, CE might give it more probability by taking away probability from similar sentences which are likely to be ungrammatical, for instance sentences that are formed by taking s and switch-ing the order of two adjacent words in it."
"This is intuitively similar to our approach – effectively, our model gives probability mass to positive sam-ples, taking it away from sentences classified as model sentences."
"A major difference between the two approaches, however, is that in CE the defini-tion of the sentence&apos;s neighborhood must be speci-fied in advance by the modeler."
"In our work, the &apos;neighborhood&apos; is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used."
"The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach."
"In future work we plan to experiment with richer representations, e.g. including long-range n-grams[REF_CITE], class n-grams[REF_CITE], grammatical features[REF_CITE], etc&apos;."
The main computational bottleneck in our approach is the generation of negative samples from the current model.
Rejection sampling allowed us to use computationally intensive classifiers as our features by reducing the number of classifications that had to be performed during the sampling process.
"However, if the boosted model strays too far from the baseline P 0 , these savings will be negated by the very large sentence rejection probabilities that will ensue."
"This is likely to be the case when richer representations as suggested above are used, necessitating a return to Gibbs sampling."
"Therefore, in future work we plan to experiment with classifiers whose decision function is cheaper to compute, such as neural networks and decision trees."
Another possible direction would be using the recently proposed Deep Belief Network formalism[REF_CITE].
DBNs utilize semi-linear features which are stacked recursively and thus very efficiently model non-linearities in their data.
"These have been used in the past for language modeling[REF_CITE], but not within the whole-sentence framework."
We present a discriminative method for learn-ing selectional preferences from unlabeled text.
"Positive examples are taken from ob-served predicate-argument pairs, while nega-tives are constructed from unobserved combi-nations."
We train a Support Vector Machine classifier to distinguish the positive from the negative instances.
We show how to parti-tion the examples for efficient training with 57 thousand features and 6.5 million training instances.
"The model outperforms other re-cent approaches, achieving excellent correla-tion with human plausibility judgments."
"Com-pared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and re-solves 37% more pronouns correctly in a pro-noun resolution experiment."
Selectional preferences (SPs) tell us which argu-ments are plausible for a particular predicate.
"For example, Table 2 (Section 4.4) lists plausible and implausible direct objects (arguments) for particu-lar verbs (predicates)."
"SPs can help resolve syntac-tic, word sense, and reference ambiguity[REF_CITE], and so gathering them has received a lot of attention in the NLP community."
One way to determine SPs is from co-occurrences of predicates and arguments in text.
"Unfortunately, no matter how much text we use, many acceptable pairs will be missing."
"In our parsed corpus (Section 4.1), for example, we find eat with nachos, burritos, and tacos, but not with the equally tasty quesadillas, chimichangas, or tostadas."
"Rather than solely re-lying on co-occurrence counts, we would like to use them to generalize to unseen pairs."
"In particular, we would like to exploit a number of arbitrary and potentially overlapping properties of predicates and arguments when we assign SPs."
"We do this by representing these properties as fea-tures in a linear classifier, and training the weights using discriminative learning."
"Positive examples are taken from observed predicate-argument pairs, while pseudo-negatives are constructed from unob-served combinations."
We train a Support Vector Ma-chine (SVM) classifier to distinguish the positives from the negatives.
We refer to our model’s scores as Discriminative Selectional Preference (D SP ).
"By creating training vectors automatically, D SP enjoys all the advantages of supervised learning, but with-out the need for manual annotation of examples."
We evaluate D SP on the task of assigning verb-object selectional preference.
We encode a noun’s textual distribution as feature information.
"The learned feature weights are linguistically interesting, yielding high-quality similar-word lists as latent in-formation."
"Despite its representational power, D SP scales to real-world data sizes: examples are parti-tioned by predicate, and a separate SVM is trained for each partition."
This allows us to efficiently learn with over 57 thousand features and 6.5 million ex-amples.
"D SP outperforms recently proposed alterna-tives in a range of experiments, and better correlates with human plausibility judgments."
It also shows strong gains over a Mutual Information-based co- occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data.
"Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argu-ment, following[REF_CITE]."
"For example, we might have a class Mexican Food and learn that the entire class is suitable for eating."
"Usually, the classes are from WordNet[REF_CITE], although they can also be inferred from clustering[REF_CITE]."
Another line of research generalizes using simi-lar words.
"Suppose we are calculating the proba-bility of a particular noun, n, occurring as the ob-ject argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood esti-mate from observed text."
Pr SIM (n|v) =
"X Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS (v) where Sim(v ′ , v) returns a real-valued similarity be-tween two verbs v ′ and v (normalized over all pair similarities in the sum)."
"In contrast,[REF_CITE]generalizes by substituting similar arguments, while[REF_CITE]use the cross-product of simi-lar pairs."
"One key issue is how to define the set of similar words, S IMS (w)."
"Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet."
Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules[REF_CITE].
Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain argu- ments X and Y. We follow[REF_CITE]in us-ing automatically-extracted semantic classes to help characterize plausible arguments.
Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling.
This approach can therefore learn which specific arguments occur with a particular predicate.
"In comparison, our fea-tures are second-order: we learn what kinds of argu-ments occur with a predicate by encoding features of the arguments."
Recent distributed and latent-variable models also represent words with feature vectors[REF_CITE].
Many of these approaches learn both the feature weights and the feature representation.
"Vectors must be kept low-dimensional for tractability, while learn-ing and inference on larger scales is impractical."
"By partitioning our examples by predicate, we can effi-ciently use high-dimensional, sparse vectors."
Our technique of generating negative examples is similar to the approach[REF_CITE].
They learn a classifier to disambiguate ac-tual sentences from pseudo-negative examples sam-pled from an N-gram language model.
They perturb their input sequence (e.g. the sentence word order) to create a neighborhood of implicit negative evidence.
"We create negatives by substitution rather than perturbation, and use corpus-wide statistics to choose our negative instances."
"To learn a discriminative model of selectional pref-erence, we create positive and negative training ex-amples automatically from raw text."
"To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data."
We measure this association using pointwise Mutual Information (MI)[REF_CITE].
"The MI between a verb predicate, v, and its object argument, n, is:"
"MI(v, n) = log Pr(v, n) = log Pr(n|v) (2) Pr(v)Pr(n) Pr(n)"
"If MI&gt;0, the probability v and n occur together is greater than if they were independently distributed."
"We create sets of positive and negative examples separately for each predicate, v. First, we extract all pairs where MI(v, n)&gt;τ as positives."
"For each pos-itive, we create pseudo-negative examples, (v,n ′ ), by pairing v with a new argument, n ′ , that either has MI below the threshold or did not occur with v in the corpus."
We require each negative n ′ to have a similar frequency to its corresponding n. This prevents our learning algorithm from focusing on any accidental frequency-based bias.
"We mix in K negatives for each positive, sampling without replacement to cre-ate all the negatives for a particular predicate."
"For each v, K1+1 of its examples will be positive."
The threshold τ represents a trade-off between capturing a large number of positive pairs and ensuring these pairs have good association.
"Similarly, K is a trade-off between the number of examples and the com-putational efficiency."
"Ultimately, these parameters should be optimized for task performance."
"Of course, some negatives will actually be plau-sible arguments that were unobserved due to sparse-ness."
"Fortunately, modern discriminative methods like soft-margin SVMs can learn in the face of label error by allowing slack, subject to a tunable regular-ization penalty[REF_CITE]."
"If MI is a sparse and imperfect model of SP, what can D SP gain by training on MI’s scores?"
"We can regard D SP as learning a view of SP that is or-thogonal to MI, in a co-training sense[REF_CITE]."
MI labels the data based solely on co-occurrence; D SP uses these labels to iden-tify other regularities – ones that extend beyond co-occurring words.
"For example, many instances of n where MI(eat, n)&gt;τ also have MI(buy, n)&gt;τ and MI(cook,n)&gt;τ."
"Also, compared to other nouns, a disproportionate number of eat-nouns are lower-case, single-token words, and they rarely contain digits, hyphens, or begin with a human first name like Bob."
D SP encodes these interdependent prop-erties as features in a linear classifier.
"This classi-fier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs."
"Similarity-smoothed models can make use of the regularities across similar verbs, but not the finer-grained string- and token-based features."
"Our training examples are similar to the data cre-ated for pseudodisambiguation, the usual evalua-tion task for SP models[REF_CITE]."
"This data con-sists of triples (v, n, n ′ ) where v, n is a predicate-argument pair observed in the corpus and v, n ′ has not been observed."
The models score correctly if they rank observed (and thus plausible) argu-ments above corresponding unobserved (and thus likely implausible) ones.
We refer to this as Pair-wise Disambiguation.
"Unlike this task, we classify each predicate-argument pair independently as plau-sible/implausible."
"We also use MI rather than fre-quency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. [Footnote_1]"
"1 For a fixed verb, MI is proportional[REF_CITE]’s conditional probability scores for pseudodisambigua-tion of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs[REF_CITE]."
"After creating our positive and negative training pairs, we must select a feature representation for our examples."
"Let Φ be a mapping from a predicate-argument pair (v,n) to a feature vector, Φ : (v,n) → hφ 1 ...φ k i. Predictions are made based on a weighted combination of the features, y = λ · Φ(v, n), where λ is our learned weight vector."
We can make training significantly more efficient by using a special form of attribute-value features.
"Let every feature φ i be of the form φ i (v, n) = hv = v̂ ∧ f(n)i."
"That is, every feature is an intersection of the occurrence of a particular predicate, v̂, and some feature of the argument f(n)."
"For example, a fea-ture for a verb-object pair might be, “the verb is eat and the object is lower-case.”"
"In this representation, features for one predicate will be completely inde-pendent from those for every other predicate."
"Thus rather than a single training procedure, we can actu-ally partition the examples by predicate, and train a classifier for each predicate independently."
"The pre-diction becomes y v = λ v · Φ v (n), where λ v are the learned weights corresponding to predicate v and all features Φ v (n)=f(n) depend on the argument only."
Some predicate partitions may have insufficient examples for training.
"Also, a predicate may oc-cur in test data that was unseen during training."
"To handle these instances, we decided to cluster low-frequency predicates."
"In our experiments assigning SP to verb-object pairs, we cluster all verbs that have less than 250 positive examples, using clusters gen-erated by the CBC algorithm[REF_CITE]."
"For example, the low-frequency verbs incarcerate, parole, and court-martial are all mapped to the same partition, while more-frequent verbs like arrest and execute each have their own partition."
"About 5.5% of examples are clustered, corresponding to 30% of the 7367 total verbs. 40% of verbs (but only 0.6% of examples) were not in any CBC cluster; these were mapped to a single backoff partition."
"The parameters for each partition, λ v , can be trained with any supervised learning technique."
"We use SVM (Section 4.1) because it is effective in simi-lar high-dimensional, sparse-vector settings, and has an efficient implementati[REF_CITE]."
"In SVM, the sign of y v gives the classification."
We can also use the scalar y v as our D SP score (i.e. the posi-tive distance from the separating SVM hyperplane).
"This section details our argument features, f(n), for assigning verb-object selectional preference."
"For a verb predicate (or partition) v and object argument n, the form of our classifier is y v = P λ f (n). v i i i"
"We provide features for the empirical probability of the noun occurring as the object argument of other verbs, Pr(n|v ′ )."
"If we were to only use these features (indexing the feature weights by each verb v ′ ), the form of our classifier would be: y v = X λ vv ′ Pr(n|v ′ ) (3) v ′"
Note the similarity between Equation (3) and Equa-tion (1).
"Now the feature weights, λ vv ′ , take the role of the similarity function, Sim(v ′ , v)."
"Unlike Equa-tion (1), however, these weights are not set by an external similarity algorithm, but are optimized to discriminate the positive and negative training ex-amples."
"We need not restrict ourselves to a short list of similar verbs; we include Pr obj (n|v ′ ) features for every verb that occurs more than 10 times in our cor-pus. λ vv ′ may be positive or negative, depending on the relation between v ′ and v. We also include fea-tures for the probability of the noun occurring as the subject of other verbs, Pr subj (n|v ′ )."
"For example, nouns that can be the object of eat will also occur as the subject of taste and contain."
"Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated."
The advantage of tuning similarity to the appli-cation of interest has been shown previously[REF_CITE].
They optimize a few meta-parameters separately for the tasks of thesaurus gen-eration and pseudodisambiguation.
"Our approach, on the other hand, discriminatively sets millions of individual similarity values."
"We include several simple character-based fea-tures of the noun string: the number of tokens, the case, and whether it contains digits, hyphens, an apostrophe, or other punctuation."
"We also include a feature for the first and last token, and fire indicator features if any token in the noun occurs on in-house lists of given names, family names, cities, provinces, countries, corporations, languages, etc."
We also fire a feature if a token is a corporate designation (like inc. or ltd.) or a human one (like Mr. or Sheik).
"Motivated by previous SP models that make use of semantic classes, we generated word clusters us-ing CBC[REF_CITE]on a 10 GB corpus, giving 3620 clusters."
"If a noun belongs in a cluster, a corresponding feature fires."
"If a noun is in none of the clusters, a no-class feature fires."
"As an example,[REF_CITE]contains: sidewalk, driveway, roadway, footpath, bridge, highway, road, runway, street, alley, path, Interstate, . . ."
"In our training data, we have examples like widen highway, widen road and widen motorway."
"If we see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc."
We also made use of the person-name/instance pairs automatically extracted[REF_CITE]. [Footnote_2]
"This data provides counts for pairs such as “Edwin Moses, hurdler” and “William Farley, in-dustrialist.”"
We have features for all concepts and therefore learn their association with each verb.
"We parsed the 3 GB AQUAINT corpus[REF_CITE]using Minipar[REF_CITE], and collected verb-object and verb-subject frequencies, building an empirical MI model from this data."
"Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved."
Passive sub-jects (the car was bought) were converted to objects (bought car).
"We set the MI-threshold, τ, to be 0, and the negative-to-positive ratio, K, to be 2."
Numerous previous pseudodisambiguation evalu-ations only include arguments that occur between 30 and 3000 times[REF_CITE].
"Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparse-ness."
"We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty co-occurrence features (Section 3.3.1))."
"We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise."
"Next, we omit verb co-occurrence features for nouns that oc-cur less than 10 times, and instead fire a low-count feature."
"When we move to a new corpus, previously-unseen nouns are treated like these low-count train-ing nouns."
"This processing results in a set of 6.8 million pairs, divided into 2318 partitions (192 of which are verb clusters (Section 3.2))."
"For each parti-tion, we take 95% of the examples for training, 2.5% for development and 2.5% for a final unseen test set."
"We provide full results for two models: D SP cooc which only uses the verb co-occurrence fea-tures, and D SP all which uses all the features men- tioned in Section 3.3."
Feature values are normalized within each feature type.
"We train our (linear kernel) discriminative models using SVM light[REF_CITE]on each partition, but set meta-parameters C (regularization) and j (cost of positive vs. nega-tive misclassifications: max at j=2) on the macro-averaged score across all development partitions."
Note that we can not use the development set to op-timize τ and K because the development examples are obtained after setting these values.
It is interesting to inspect the feature weights re-turned by our system.
"In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarity-ranking of other verb contexts."
"The D SP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish)."
"Discrimina-tive, context-specific training seems to yield a bet-ter set of similar predicates, e.g. the highest-ranked contexts for D SP cooc on the verb join, [Footnote_3] lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better S IMS (join) for Equation (1) than the top similarities returned[REF_CITE]: participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign 0.142, meet 0.142, include 0.141, leave 0.140, work 0.137"
"3 Which all correspond to nouns occurring in the object po-sition of the verb (e.g. Pr obj (n|lead)), except “launch (subj)” which corresponds to Pr subj (n|launch)."
Other features are also weighted intuitively.
"Note that case is a strong indicator for some arguments, for example the weight on being lower-case is high for become (0.972) and eat (0.505), but highly nega-tive for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations."
"We first evaluate D SP on disambiguating posi-tives from pseudo-negatives, comparing to recently- proposed systems that also require no manually-compiled resources like WordNet."
We convert[REF_CITE]’s similarity-smoothed probability to MI by replacing the empirical Pr(n|v) in Equa-tion (2) with the smoothed Pr SIM from Equation (1).
We also test an MI model inspired[REF_CITE]:
X ′ )
"MI SIM (n, v) = log Sim(n ′ , n) Pr(v, n Pr(v)Pr(n ′ ) n ′ ∈S IMS (n)"
"We gather similar words using[REF_CITE], mining similar verbs from a comparable-sized parsed cor-pus, and collecting similar nouns from a broader 10 GB corpus of English text. [Footnote_4]"
"4 For both the similar-noun and similar-verb smoothing, we only smooth over similar pairs that occurred in the corpus. While averaging over all similar pairs tends to underestimate the probability, averaging over only the observed pairs tends to overestimate it. We tested both and adopt the latter because it resulted in better performance on our development set."
We also use[REF_CITE]’s approach to obtaining web-counts.
"Rather than mining parse trees, this technique retrieves counts for the pattern “V Det N” in raw online text, where V is any in-flection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun."
"We compute a web-based MI by collecting Pr(n, v), Pr(n), and Pr(v) using all inflections, ex-cept we only use the root form of the noun."
"Rather than using a search engine, we obtain counts from the Google Web [Footnote_5]-gram Corpus. [Footnote_5]"
"5 Available from the LDC[REF_CITE]. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the hUNKi symbol, and only N-grams appear-ing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable."
"5 Available from the LDC[REF_CITE]. This collection was generated from approximately 1 trillion tokens of online text. Unfortunately, tokens appearing less than 200 times have been mapped to the hUNKi symbol, and only N-grams appear-ing more than 40 times are included. Unlike results from search engines, however, experiments with this corpus are replicable."
All systems are thresholded at zero to make a clas-sification.
"Unlike D SP , the comparison systems may not be able to provide a score for each example."
The similarity-smoothed examples will be undefined if S IMS (w) is empty.
"Also, the[REF_CITE]approach will be undefined if the pair is un-observed on the web."
"As a reasonable default for these cases, we assign them a negative decision."
"We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F)."
Table 1 gives the results of our comparison.
"In the MacroAvg results, we weight each example equally."
"For MicroAvg, we weight each example by the fre-quency of the noun."
"To more directly compare with previous work, we also reproduced Pairwise Disam-biguation by randomly pairing each positive with one of the negatives and then evaluating each system by the percentage it ranks correctly (Acc)."
"For the comparison approaches, if one score is undefined, we choose the other one."
"If both are undefined, we abstain from a decision."
Coverage (Cov) is the per-cent of pairs where a decision was made. [Footnote_6]
6 I.e. we use the “half coverage” condition[REF_CITE].
"Our simple system with only verb co-occurrence features, D SP cooc , outperforms all comparison ap-proaches."
"Using the richer feature set in D SP all results in a statistically significant gain in perfor-mance, up to an F-Score of 0.65 and a pairwise disambiguation accuracy of 0.81. [Footnote_7] D SP all has both broader coverage and better accuracy than all com-peting approaches."
"7 The differences between D SP all and all comparison sys-tems are statistically significant (McNemar’s test, p&lt;0.01)."
"In the remainder of the experi-ments, we use D SP all and refer to it simply as D SP ."
Some errors are because of plausible but unseen arguments being used as test-set pseudo-negatives.
"For example, for the verb damage, D SP ’s three most high-scoring false positives are the nouns jetliner, carpet, and gear."
"While none occur with damage in our corpus, all intuitively satisfy the verb’s SPs."
MacroAvg performance is worse than MicroAvg because all systems perform better on frequent nouns.
"When we plot F-Score by noun frequency (Figure 1), we see that D SP outperforms comparison approaches across all frequencies, but achieves its biggest gains on the low-frequency nouns."
A richer feature set allows D SP to make correct inferences on examples that provide minimal co-occurrence data.
These are also the examples for which we would ex-pect co-occurrence models like MI to fail.
"As a further experiment, we re-trained D SP but with only the string-based features removed."
Overall macro-averaged F-score dropped from 0.65 to 0.64 (a statistically significant reduction in performance).
"The system scored nearly identically to D SP on the high-frequency nouns, but performed roughly 15% worse on the nouns that occurred less than ten times."
"This shows that the string-based features are impor-tant for selectional preference, and particularly help-ful for low-frequency nouns."
Table 2 compares some of our systems on data used[REF_CITE](also Appendix 2[REF_CITE]).
"The plausibility of these pairs was initially judged based on the experimenters’ intuitions, and later confirmed in a human experiment."
"We include the scores of Resnik’s system, and note that its errors were attributed to sense ambiguity and other limi-tations of class-based approaches[REF_CITE]. [Footnote_8]"
"8 For example, warn-engine scores highly because engines are in the class entity, and physical entities (e.g. people) are often objects of warn. Unlike D SP , Resnik’s approach cannot learn that for warn, “the property of being a person is more"
"The other comparison approaches also make a num-ber of mistakes, which can often be traced to a mis-guided choice of similar word to smooth with."
"We also compare to our empirical MI model, trained on our parsed corpus."
"It has no statistics, however, for many of the implausible ones."
"D SP can make finer decisions than MI, recognizing that “warning an engine” is more absurd than “judging a climate.”"
"We next compare MI and D SP on a much larger set of plausible examples, and also test how well the models generalize across data sets."
We took the MI and D SP systems trained on AQUAINT and asked them to rate observed (and thus likely plausible) verb-object pairs taken from an unseen corpus.
We extracted the pairs by parsing the San Jose Mercury News (SJM) section of the TIPSTER corpus[REF_CITE].
Each unique verb-object pair is a single instance in this evaluation.
"Table 3 gives recall across all pairs (All) and grouped by pair-frequency in the unseen corpus (1, 2, 3, &gt;3)."
"D SP accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combi-nation as plausible (57%)."
"Recall is higher on more frequent verb-object pairs, but 70% of the pairs oc-curred only once in the corpus."
"Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modi-fied KN-smoothing[REF_CITE], the recall of MI&gt;0 on SJM only increases from 44.1% to 44.9%, still far below D SP ."
Frequency-based models have fundamentally low coverage.
"As fur-important than the property of being an entity”[REF_CITE]. ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation ex-periment (Section 4.3), MI&gt;0 gets a MacroAvg pre-cision of 86% but a MacroAvg recall of only 12%. 9"
"Finally, we evaluate D SP on a common application of selectional preferences: choosing the correct an-tecedent for pronouns in text[REF_CITE]."
"We study the cases where a pronoun is the direct object of a verb predicate, v. A pronoun’s antecedent must obey v’s selectional pref-erences."
"If we have a better model of SP, we should be able to better select pronoun antecedents."
We parsed the[REF_CITE]coreference corpus and extracted all pronouns in a direct object rela-tion.
"For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence."
Thirty-nine anaphoric pronouns had an antecedent in this window and are used in the evaluation.
"For each p, let N(p) + by the set of preceding nouns coreferent with p, and let N(p) − be the remaining non-coreferent nouns."
"We take all (v, n + ) where n + ∈ N(p) + as positive, and all other pairs (v, n − ), n − ∈ N(p) − as negative."
"We compare MI and D SP on this set, classifying every (v,n) with MI&gt;T (or D SP &gt;T) as positive."
"By varying T, we get a precision-recall curve (Fig-ure 2)."
"Precision is low because, of course, there are many nouns that satisfy the predicate’s SPs that are not coreferent."
D SP &gt;0 has both a higher recall and higher precision than accepting every pair pre-viously seen in text (the right-most point on MI&gt;T ).
The D SP &gt;T system achieves higher precision than MI&gt;T for points where recall is greater than 60% (where MI&lt;0).
"Interestingly, the recall of MI&gt;0 is higher here than it is for general verb-objects (Sec-tion 4.5)."
"On the subset of pairs with strong empir-ical association (MI&gt;0), MI generally outperforms D SP at equivalent recall values."
We next compare MI and D SP as stand-alone pro-noun resolution systems (Table 4).
"As a standard baseline, for each pronoun, we choose the most recent noun in text as the pronoun’s antecedent, achieving 17.9% resolution accuracy."
"This baseline is quite low because many of the most-recent nouns are subjects of the pronoun’s verb phrase, and there-fore resolution violates syntactic coreference con-straints."
"If instead we choose the previous noun with the highest MI as antecedent, we get an accuracy of 28.2%, while choosing the previous noun with the highest D[REF_CITE].5%."
D[REF_CITE]% more pronouns correctly than MI.
"We leave as fu-ture work a full-scale pronoun resolution system that incorporates both MI and D SP as backed-off, inter-polated, or separate semantic features."
"We have presented a simple, effective model of se-lectional preference based on discriminative train-ing."
"Supervised techniques typically achieve higher performance than unsupervised models, and we du-plicate these gains with D SP ."
"Here, however, these gains come at no additional labeling cost, as train-ing examples are generated automatically from un-labeled text."
"D SP allows an arbitrary combination of features, including verb co-occurrence features that yield high-quality similar-word lists as latent output."
"This work only scratches the surface of possible fea-ture mining; information from WordNet relations, Wikipedia categories, or parallel corpora could also provide valuable clues to SP."
"Also, if any other sys-tem were to exceed D SP ’s performance, it could also be included as one of D SP ’s features."
"It would be interesting to expand our co- occurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web."
"We could also reverse the role of noun and verb in our training, having verb-specific features and discriminating separately for each argument noun."
The latent information would then be lists of similar nouns.
"Finally, note that while we focused on word-word co-occurrences, sense-sense SPs can also be learned with our algorithm."
"If our training corpus was sense-labeled, we could run our algorithm over the senses rather than the words."
The resulting model would then require sense-tagged input if it were to be used within an application like parsing or coreference res-olution.
"Also, like other models of SP, our technique can also be used for sense disambiguations: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb."
We present a PropBank semantic role label-ing system for English that is integrated with a dependency parser.
"To tackle the problem of joint syntactic–semantic analysis, the sys-tem relies on a syntactic and a semantic sub-component."
"The syntactic model is a projec-tive parser using pseudo-projective transfor-mations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers."
The complete syntactic–semantic output is selected from a candidate pool gen-erated by the subsystems.
We evaluate the system on the[REF_CITE]test sets using segment-based and dependency-based metrics.
"Using the segment-based[REF_CITE]metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently."
"Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set[REF_CITE]."
Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.
"Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a use-ful intermediate step in NLP applications perform-ing semantic analysis."
It has obvious applications for template-filling tasks such as information extrac-tion and question answering[REF_CITE].
"It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems[REF_CITE]."
"In addi-tion, role-semantic features have recently been used to extend vector-space representations in automatic document categorizati[REF_CITE]."
"The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried[REF_CITE]."
"Following the seminal work[REF_CITE], there have been many extensions in ma-chine learning models, feature engineering[REF_CITE], and inference procedures[REF_CITE]."
"With very few exceptions (e.g.[REF_CITE]), published SRL methods have used some sort of syntactic structure as input[REF_CITE]."
"Most sys-tems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank[REF_CITE], although there has also been much re-search on the use of shallow syntax[REF_CITE]in SRL."
"In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument rela-tions."
"Furthermore, the few systems based on de-pendencies that have been presented have generally performed much worse than their constituent-based counterparts."
"For instance,[REF_CITE]re-ported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser:"
"The F-measure[REF_CITE]dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation."
"In a similar vein,[REF_CITE]reported that parse tree path fea-tures extracted from a rule-based dependency parser are much less reliable than those from a modern con-stituent parser."
"In contrast, we recently carried out a de-tailed comparis[REF_CITE]between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers."
We suggested that the previous lack of progress in dependency-based SRL was due to low parsing ac-curacy.
"The experiments showed that the grammat-ical function information available in dependency representations results in a steeper learning curve when training semantic role classifiers, and it also seemed that the dependency-based role classifiers were more resilient to lexical problems caused by change of domain."
The recent[REF_CITE]Shared Task[REF_CITE]was an attempt to show that SRL can be accurately carried out using only dependency syn-tax.
"However, these results are not easy to compare to previously published results since the task defini-tions and evaluation metrics were different."
This paper compares the best-performing sys-tem in the[REF_CITE]Shared Task[REF_CITE]with previously published constituent-based SRL systems.
The system carries out joint dependency-syntactic and semantic anal-ysis.
"We first describe its implementation in Sec-tion 2, and then compare the system with the best system in the[REF_CITE]Shared Task in Section 3."
"Since the outputs of the two systems are differ-ent, we carry out two types of evaluations: first by using the traditional segment-based metric used in the[REF_CITE]Shared Task, and then by using the dependency-based metric from the[REF_CITE]Shared Task."
"Both evaluations require a transforma-tion of the output of one system: For the segment-based metric, we have to convert the dependency- based output to segments; and for the dependency-based metric, a head-finding procedure is needed to select heads in segments."
"For the first time for a sys-tem using only dependency syntax, we report results for PropBank-based semantic role labeling of En-glish that are close to the state of the art, and for some measures even superior."
The training corpus that we used is the dependency-annotated Penn Treebank from the 2008 CoNLL Shared Task on joint syntactic–semantic analysis[REF_CITE].
Figure 1 shows a sentence annotated in this framework.
"The CoNLL task in-volved semantic analysis of predicates from Prop-Bank (for verbs, such as plan) and NomBank (for nouns, such as investment); in this paper, we report the performance on PropBank predicates only since we compare our system with previously published PropBank-based SRL systems."
We model the problem of constructing a syntac-tic and a semantic graph as a task to be solved jointly.
"Intuitively, syntax and semantics are highly interdependent and semantic interpretation should help syntactic disambiguation, and joint syntactic– semantic analysis has a long tradition in deep-linguistic formalisms."
"Using a discriminative model, we thus formulate the problem of finding a syntactic tree ŷ syn and a semantic graph ŷ sem for a sentence x as maximizing a function F joint that scores the complete syntactic–semantic structure: hŷ syn , ŷ sem i = arg max F joint (x, y syn , y sem ) y syn ,y sem"
The dependencies in the feature representation used to compute F joint determine the tractability of the search procedure needed to perform the maximiza-tion.
"To be able to use complex syntactic features such as paths when predicting semantic structures, exact search is clearly intractable."
"This is true even with simpler feature representations – the problem is a special case of multi-headed dependency analy-sis, which is NP-hard even if the number of heads is bounded[REF_CITE]."
This means that we must resort to a simplifica-tion such as an incremental method or a rerank-ing approach.
We chose the latter option and thus created syntactic and semantic submodels.
The joint syntactic–semantic prediction is selected from a small list of candidates generated by the respective subsystems.
Figure 2 shows the architecture.
"We model the process of syntactic parsing of a sentence x as finding the parse tree ŷ syn = arg max y syn F syn (x, y syn ) that maximizes a scoring function F syn ."
The learning problem consists of fit-ting this function so that the cost of the predictions is as low as possible according to a cost function ρ syn .
"In this work, we consider linear scoring functions of the following form:"
"F syn (x, y syn ) ="
"Ψ syn (x, y syn ) · w where Ψ syn (x, y syn ) is a numeric feature represen-tation of the pair (x, y syn ) and w a vector of feature weights."
"We defined the syntactic cost ρ syn as the sum of link costs, where the link cost was 0 for a correct dependency link with a correct label, 0.5 for a correct link with an incorrect label, and 1 for an incorrect link."
"A widely used discriminative framework for fit-ting the weight vector is the max-margin model[REF_CITE], which is a generalization of the well-known support vector machines to gen-eral cost-based prediction problems."
"Since the large number of training examples and features in our case make an exact solution of the max-margin op-timization problem impractical, we used the on-line passive–aggressive algorithm[REF_CITE], which approximates the optimization process in two ways: • The weight vector w is updated incrementally, one example at a time. • For each example, only the most violated con-straint is considered."
The algorithm is a margin-based variant of the per-ceptron (preliminary experiments show that it out-performs the ordinary perceptron on this task).
Al-gorithm 1 shows pseudocode for the algorithm.
"Algorithm 1 The Online PA Algorithm input Training set T = {(x t , y t )} Tt=1 let τ t = min C, F(xkΨ t ,ỹ( t )x−F,y (x t ,y t )+ρ(y t ,ỹ t ) t )−Ψ(x,ỹ t )k 2 w ← w + τ t (Ψ(x, y t ) − Ψ(x, ỹ t )) return waverage"
"We used a C value of 0.01, and the number of iterations was 6."
"The feature function Ψ syn is a factored represen-tation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors:"
"Ψ(x, y) · w = X ψ(x, f) · w f∈y"
"We used a second-order factorizati[REF_CITE], meaning that the factors are subtrees consisting of four links: the governor–dependent link, its sibling link, and the leftmost and rightmost dependent links of the depen-dent."
"This factorization allows us to express useful fea-tures, but also forces us to adopt the expensive search procedure[REF_CITE], which ex-tends Eisner’s span-based dynamic programming al-gorithm (1996) to allow second-order feature depen-dencies."
"This algorithm has a time complexity of O(n 4 ), where n is the number of words in the sen-tence."
The search was constrained to disallow mul-tiple root links.
"To evaluate the argmax in Algorithm 1 during training, we need to handle the cost function ρ syn in addition to the factor scores."
"Since the cost function ρ syn is based on the cost of single links, this can easily be integrated into the factor-based search."
"Although only 0.4% of the links in the training set are nonprojective, 7.6% of the sentences con-tain at least one nonprojective link."
Many of these links represent long-range dependencies – such as wh-movement – that are valuable for semantic pro-cessing.
Nonprojectivity cannot be handled by span-based dynamic programming algorithms.
"For parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead."
"However, this algorithm cannot be generalized to the second-order setting –[REF_CITE]proved that this problem is NP-hard, and described an approximate greedy search algorithm."
"To simplify implementation, we instead opted for the pseudo-projective approach[REF_CITE], in which nonprojective links are lifted up-wards in the tree to achieve projectivity, and spe-cial trace labels are used to enable recovery of the nonprojective links at parse time."
"The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once."
"Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels."
Our semantic model consists of three parts: • A SRL classifier pipeline that generates a list of candidate predicate–argument structures. • A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments. • A global reranker that assigns scores to predicate–argument structures in the filtered candidate list.
"Rather than training the models on gold-standard syntactic input, we created an automatically parsed training set by 5-fold cross-validation."
"Training on automatic syntax makes the semantic classifiers more resilient to parsing errors, in particular adjunct labeling errors."
"The SRL pipeline consists of classifiers for pred-icate disambiguation, argument identification, and argument labeling."
"For the predicate disambigua-tion classifiers, we trained one subclassifier for each lemma."
"All classifiers in the pipeline were L2-regularized linear logistic regression classifiers, im-plemented using the efficient L IBLINEAR package[REF_CITE]."
"For multiclass problems, we used the one-vs-all binarization method, which makes it easy to prevent outputs not allowed by the PropBank frame."
"Since our classifiers were logistic, their output values could be meaningfully interpreted as prob-abilities."
This allowed us to combine the scores from subclassifiers into a score for the complete predicate–argument structure.
"To generate the can-didate lists used by the global SRL models, we ap-plied beam search based on these scores using a beam width of 4."
The argument identification classifier was pre-ceded by a pruning step similar to the constituent-based pruning[REF_CITE].
"The features used by the classifiers are listed in Table 1, and are described in Appendix A. We se-lected the feature sets by greedy forward subset se-lection."
The following three global constraints were used to filter the candidates generated by the pipeline.
C ORE A RGUMENT C ONSISTENCY .
Core argu-ment labels must not appear more than once.
D ISCONTINUITY C ONSISTENCY .
"If there is a la-bel C-X, it must be preceded by a label X."
R EFERENCE C ONSISTENCY .
"If there is a label R-X and the label is inside an attributive relative clause, it must be preceded by a label X."
We therefore created a global SRL classifier using the following global features in addition to the fea-tures from the pipeline:
C ORE A RGUMENT L ABEL S EQUENCE .
The com-plete sequence of core argument labels.
"The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1."
M ISSING C ORE A RGUMENT L ABELS .
The set of core argument labels declared in the PropBank frame that are not present in the predicate– argument structure.
"Similarly to the syntactic submodel, we trained the global SRL model using the online passive– aggressive algorithm."
The cost function ρ was defined as the number of incorrect links in the predicate–argument structure.
The number of iter-ations was 20 and the regularization parameter C was 0.01.
"Interestingly, we noted that the global SRL model outperformed the pipeline even when no global features were added."
This shows that the global learning model can correct label bias prob-lems introduced by the pipeline architecture.
"As described previously, we carried out reranking on the candidate set of complete syntactic–semantic structures."
"To do this, we used the top 16 trees from the syntactic module and applied a linear model:"
"F joint (x, y syn , y sem ) ="
"Ψ joint (x, y syn , y sem ) · w"
"Our baseline joint feature representation Ψ joint con-tained only three features: the log probability of the syntactic tree and the log probability of the seman-tic structure according to the pipeline and the global model, respectively."
This model was trained on the complete training set using cross-validation.
The probabilities were obtained using the multinomial logistic function (“softmax”).
"We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline."
Time prevented us from exploring this direction conclusively.
"To compare our results with previously published results in SRL, we carried out an experiment com-paring our system to the top system[REF_CITE]in the[REF_CITE]Shared Task."
"How-ever, comparison is nontrivial since the output of the[REF_CITE]systems was a set of labeled seg-ments, while the[REF_CITE]systems (including ours) produced labeled semantic dependency links."
"To have a fair comparison of our link-based sys-tem against previous segment-based systems, we carried out a two-way evaluation: In the first eval-uation, the dependency-based output was converted to segments and evaluated using the segment scorer[REF_CITE]and in the second evaluation, we applied a head-finding procedure to the output of a segment-based system and scored the result using the link-based[REF_CITE]scorer."
It can be discussed which of the two metrics is most correlated with application performance.
"The traditional metric used in the[REF_CITE]task treats SRL as a bracketing problem, meaning that the entities scored by the evaluation procedure are labeled snippets of text; however, it is questionable whether this is the proper way to evaluate a task whose purpose is to find semantic relations between logical entities."
We believe that the same criticisms that have been leveled at the PARSEVAL metric for constituent structures are equally valid for the bracket-based evaluation of SRL systems.
The in-appropriateness of the traditional metric has led to a number of alternative metrics[REF_CITE].
"To be able to score the output of a dependency-based SRL system using the segment scorer, a conversion step is needed."
Algorithm 2 shows how a set of seg-ments is constructed from an argument dependency node.
"For each argument node, the algorithm com-putes the yield Y of the argument node, i.e. the set of dependency nodes to include in the bracketing."
"This set is then partitioned into contiguous parts, from which the segments are computed."
"In most cases, the yield is just the subtree dominated by the argu-ment node."
"However, if the argument dominates the predicate, then the branch containing the predicate is removed."
"Table 2 shows the performance figures of our system on the WSJ and Brown corpora: preci-sion, recall, F 1 -measure, and complete proposition accuracy (PP)."
"These figures are compared to the best-performing system in the[REF_CITE]Shared Task[REF_CITE], referred to as Pun-yakanok in the table, and the best result currently published[REF_CITE], referred to as Sur-deanu."
"To validate the sanity of the segment cre-ation algorithm, the table also shows the result of ap-plying segment creation to gold-standard syntactic–"
Algorithm 2 Segment creation from an argument dependency node. input
"Predicate node p, argument node a if a does not dominate p"
Y ← {n : a dominates n} else c ← the child of a that dominates p
"Y ← {n : a dominates n} \ {n : c dominates n} end if S ← partition of Y into contiguous subsets return {(min-index s, max-index s) : s ∈ S} semantic trees."
"We see that the two conversion pro-cedures involved (constituent-to-dependency con-version by the[REF_CITE]Shared Task organizers, and our dependency-to-segment conversion) work satisfactorily although the process is not completely lossless."
"During inspection of the output, we noted that many errors arise from inconsistent punctuation at-tachment in PropBank/Treebank."
We therefore nor-malized the segments to exclude punctuation at the beginning or end of a segment.
The results of this evaluation is shown in Table 3.
This table does not include the Surdeanu system since we did not have
"The results on the WSJ test set clearly show that dependency-based SRL systems can rival constituent-based systems in terms of performance – it clearly outperforms the Punyakanok system, and has a higher recall and complete proposition accu-racy than the Surdeanu system."
"We interpret the high recall as a result of the dependency syntactic repre-sentation, which makes the parse tree paths simpler and thus the arguments easier to find."
"For the Brown test set, on the other hand, the dependency-based system suffers from a low pre-cision compared to the constituent-based systems."
"Our error analysis indicates that the domain change caused problems with prepositional attachment for the dependency parser – it is well-known that prepo-sitional attachment is a highly lexicalized problem, and thus sensitive to domain changes."
"We believe that the reason why the constituent-based systems are more robust in this respect is that they utilize a combination strategy, using inputs from two differ-ent full constituent parsers, a clause bracketer, and a chunker."
"However, caution is needed when draw-ing conclusions from results on the Brown test set, which is only 7,585 words, compared to the 59,100 words in the WSJ test set."
"It has previously been noted[REF_CITE]that a segment-based evaluation may be unfavorable to a dependency-based system, and that an evalua-tion that scores argument heads may be more indica-tive of its true performance."
We thus carried out an evaluation using the evaluation script of the[REF_CITE]Shared Task.
"In this evaluation method, an ar-gument is counted as correctly identified if its head and label are correct."
"Note that this is not equivalent to the segment-based metric: In a perfectly identi-fied segment, we may still pick out the wrong head, and if the head is correct, we may infer an incorrect segment."
The evaluation script also scores predicate disambiguation performance; we did not include this score since the 2005 systems did not output predi-cate sense identifiers.
"Since[REF_CITE]-style segments have no in-ternal tree structure, it is nontrivial to extract a head."
"It is conceivable that the output of the parsers used by the Punyakanok system could be used to extract heads, but this is not recommendable be-cause the Punyakanok system is an ensemble sys-tem and a segment does not always exactly match a constituent in a parse tree."
"Furthermore, the[REF_CITE]constituent-to-dependency conversion method uses a richer structure than just the raw con-stituents: empty categories, grammatical functions, and named entities."
"To recreate this additional infor-mation, we would have to apply automatic systems and end up with unreliable results."
"Instead, we thus chose to find an upper bound on the performance of the segment-based system."
We applied a simple head-finding procedure (Algo-rithm 3) to find a set of head nodes for each seg-ment.
"Since the[REF_CITE]output does not in-clude dependency information, the algorithm uses gold-standard dependencies and intersects segments with the gold-standard segments."
"This will give us an upper bound, since if the segment contains the correct head, it will always be counted as correct."
"The algorithm looks for dependencies leaving the segment, and if multiple outgoing edges are found, a couple of simple heuristics are applied."
"We found that the best performance is achieved when selecting only one outgoing edge. “Small clauses,” which are split into an object and a predicative complement in the dependency framework, are the only cases where we select two heads."
Table 4 shows the results of the dependency-based evaluation.
"In the table, the output of the"
"Algorithm 3 Finding head nodes in a segment. input Argument segment a if a overlaps with a segment in the gold standard a ← intersection of a and gold standard F ← {n : governor of n outside a} if |F | = 1 return F remove punctuation nodes from F if |F | = 1 return F if F = {n 1 , n 2 , . . . } where n 1 is an object and n 2 is the predicative part of a small clause return {n 1 , n 2 } if F contains a node n that is a subject or an object return {n} else return {n}, where n is the leftmost node in F dependency-based system is compared to the seman-tic dependency links automatically extracted from the segments of the Punyakanok system."
"In this evaluation, the dependency-based system has a higher F[Footnote_1]-measure than the Punyakanok sys-tem on both test sets."
1 Our system is freely available for download[URL_CITE]
"This suggests that the main ad-vantage of using a dependency-based semantic role labeler is that it is better at finding the heads of semantic arguments, rather than finding segments."
"The results are also interesting in comparison to the multi-view system described[REF_CITE], which has a reported head F[Footnote_1] measure of 85.2 on the WSJ test set."
1 Our system is freely available for download[URL_CITE]
"The figure is not exactly compatible with ours, however, since that system used a different head extraction mechanism."
We have described a dependency-based system 1 for semantic role labeling of English in the PropBank framework.
Our evaluations show that the perfor-mance of our system is close to the state of the art.
This holds regardless of whether a segment-based or a dependency-based metric is used.
"In-terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points."
Our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance.
"Evaluation and comparison is a difficult issue since the natural output of a dependency-based sys-tem is a set of semantic links rather than segments, as is normally the case for traditional systems."
"To handle this situation fairly to both types of systems, we carried out a two-way evaluation: conversion of dependencies to segments for the dependency-based system, and head-finding heuristics for segment-based systems."
"However, the latter is difficult since no structure is available inside segments, and we had to resort to computing upper-bound results using gold-standard input; despite this, the dependency-based system clearly outperformed the upper bound of the performance of the segment-based system."
The comparison can also be slightly misleading since the dependency-based system was optimized for the dependency metric and previous systems for the segment metric.
"Our evaluations suggest that the dependency-based SRL system is biased to finding argument heads, rather than argument text snippets, and this is of course perfectly logical."
"Whether this is an ad-vantage or a drawback will depend on the applica-tion – for instance, a template-filling system might need complete segments, while an SRL-based vector space representation for text categorization, or a rea-soning application, might prefer using heads only."
"In the future, we would like to further investigate whether syntactic and semantic analysis could be in-tegrated more tightly."
"In this work, we used a sim- plistic loose coupling by means of reranking a small set of complete structures."
"The same criticisms that are often leveled at reranking-based models clearly apply here too: The set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early."
"An example of a method to mitigate this shortcoming is the forest reranking[REF_CITE], in which complex features are evalu-ated as early as possible."
Features Used in Predicate Disambiguation
Most Web-based Q/A systems work by find-ing pages that contain an explicit answer to a question.
"These systems are helpless if the answer has to be inferred from multiple sen-tences, possibly on different pages."
"To solve this problem, we introduce the H OLMES sys-tem, which utilizes textual inference (TI) over tuples extracted from text."
"Whereas previous work on TI (e.g., the lit-erature on textual entailment) has been ap-plied to paragraph-sized texts, H OLMES uti-lizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages."
"Given only a few minutes, H OLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition)."
"Importantly, H OLMES ’s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-defined sense."
"Numerous researchers have identified the Web as a rich source of answers to factual questions, e.g.,[REF_CITE], but often the desired information is not stated explicitly even in a textual corpus as massive as the Web."
Consider the question “What vegetables help prevent osteoporo-sis?”
"Since there is likely no sentence on the Web directly stating “Kale prevents osteoporosis”, a sys-tem must infer that kale is an answer by combining facts from multiple sentences, possibly from differ-ent pages, which justify that conclusion: i.e., that kale is a vegetable, kale contains calcium, and cal-cium helps prevent osteoporosis."
Textual Inference (TI) methods have advanced in recent years.
"For example, textual entailment tech-niques aim to determine whether one textual frag-ment (the hypothesis) follows from another (the text)[REF_CITE]."
"While most TI researchers have focused on high-quality inferences from a small source text, we seek to utilize sizable chunks of the Web corpus as our source text."
"In order to do this, we must confront two major challenges."
"The first is uncertainty: TI is an imperfect process, particularly when applied to the Web corpus, hence probabilistic methods help to assess the confidence in inferences."
The second challenge is scalability: how does infer-ence time scale given increasingly large corpora as input?
"This paper describes H OLMES , an implemented sys-tem, which addresses both challenges by carrying out scalable, probabilistic inference over ground assertions extracted from the Web."
"The input to H OLMES is a conjunctive query, a set of inference rules expressed as Horn clauses, and large sets of ground assertions extracted from the Web, WordNet, and other knowledge bases."
"As shown in Figure 1, H OLMES chains backward from the query, using the inference rules to construct a forest of proof trees from the ground assertions."
This forest is converted into a Markov network (a form of Knowledge-Based Model Construction (KBMC)[REF_CITE]) and evaluated using approximate prob-abilistic inference.
"H OLMES operates in an anytime fashion — if desired it can keep iterating: search-ing for more proofs, and elaborating the Markov net-work."
H OLMES makes some important simplifying as-sumptions.
"Specifically, we use simple ground tuples to represent extracted assertions (e.g., contains(kale, calcium))."
"Syntactic prob-lems (e.g., anaphora, relative clauses) and seman-tic challenges (e.g., quantification, counterfactuals, temporal qualification) are delegated to the extrac-tion system or simply ignored."
This paper focuses on scalability for this subset of the TI task.
"We tested H[REF_CITE]million distinct ground assertions extracted from the Web by the T EX - T R UNNER system[REF_CITE], coupled with 159 thousand ground assertions from Word-Net[REF_CITE], and a compact set of hand-coded inference rules."
"Given a total of 55 to 145 seconds, H OLMES was able to produce high-quality inferences that doubled the number of answers to example queries in three disparate domains: geog-raphy, business, and nutrition."
We also evaluated how the speed of H OLMES scaled with the size of its input corpus.
"In the general case, logical inference over a Horn theory (needed in order to produce the probabilistic net-work) is polynomial in the number of ground asser-tions, and hence in the size of the textual corpus. 1"
"Unfortunately, this is prohibitive, since even low-order polynomial growth is fatal on a 117 million-page corpus, let alone the full Web."
"Fortunately, the Web’s long tail works in our favor."
"The relations we extract from text are approximately pseudo-functional (APF), as we formalize in Sec-tion 3, and this property leads to runtime that scales linearly with the corpus."
"To see the underlying in-tuition, consider the APF relation denoted by the phrase “is married to;” most of the time it maps a person’s name to a small number of spousal names so this relation is APF."
"Section 3 shows why this APF property ensures linear scaling, and Section 4 demonstrates linear scaling in practice."
"H OLMES is a system designed to answer complex queries over large, noisy knowledge bases."
"As a mo-tivating example, we consider the question “What vegetables help prevent osteoporosis?”"
"As of this writing, Google has no pages explicitly stating ‘kale helps prevent osteoporosis’, making it challenging to return “kale” as an answer."
"However, there are numerous web pages stating that “kale is high in cal-cium” and others declaring that “calcium helps pre-vent osteoporosis”."
If we could combine those facts we could easily infer that “kale” is an answer to the question “What vegetables help prevent osteoporo-sis?”
H OLMES was designed to make such infer-ences while accounting for uncertainty in the pro-cess.
"Given a query, expressed as a conjunctive Datalog rule, H OLMES generates a probabilistic model using knowledge-based model construction (KBMC)[REF_CITE]."
"Specifically, H OLMES utilizes fast, logical inference to find the subset of ground assertions and inference rules that may influence the answers to the query — enabling the construction of a small and focused Markov net-work."
"Since this graphical model is much smaller than one incorporating all ground assertions, prob-abilistic inference will be much faster than if naive compilation were used."
Figure 1 summarizes the operation of H OLMES .
"As with many theorem provers or KBMC systems, H OLMES takes three inputs: [Footnote_1]."
"1 In fact, it is P-complete — as hard as any polynomial-time problem."
"A set of knowledge bases — databases of ground relational assertions, each with an estimate of its probability, which can be generated by TextRunner[REF_CITE]or Kyl[REF_CITE]."
"In our example, we would extract the as-sertions and IsHighIn(kale, calcium) Prevents(calcium, osteoporosis) from those sentences. 2."
"A domain theory – A set of probabilis-tic inference rules written as Markov logic Horn clauses, which can be used to de-rive new assertions."
The weight associ-ated with each clause specifies its reliability.
"In Section 2.3 we identify several domain-independent rules, but a user may (optionally) specify additional, domain-specific rules if de-sired."
"In our example, we assume we are given the domain-specific rule: Prevents(X,Z) :- IsHighIn(X,Y) ∧ Prevents(Y,Z) 3."
A conjunctive query is specified as a Datalog rule.
"For example, the question “What vegeta-bles help prevent osteoporosis?” could be writ-ten as: query(X) :- IS-A(X,Vegetable) ∧ Prevents(X,osteoporosis) and returns a set of answers to the query, each with an associated probability."
"To find these answers and their associated proba-bilities, H OLMES first finds all ground assertions in the knowledge bases that are potentially relevant to the query."
This is efficiently done using the infer-ence rules to chain backwards from the query.
"Note that the generated candidate answers, themselves, are less important than the associated proof trees."
"Furthermore, since H OLMES uses these ‘trees’ (ac-tually, DAGs) to generate a probabilistic graphical model, H OLMES seeks to find as many proof trees as possible for each query result — each may influ-ence the final belief in that result."
Figure 2 shows a partial proof tree for our example query.
"To handle uncertainty, H OLMES now constructs a ground Markov network from the proof trees and the Markov-logic-encoded inference rules."
"Markov net- works[REF_CITE]model the joint distribution of a set of variables by creating an undirected graph with one node for each random variable, and represent-ing dependencies between variables with cliques in the graph."
"Each clique has a corresponding poten-tial function φ k , which returns a non-negative value based on the state of variables in the clique."
"The probability of a state, x, is given by"
"P(x) = 1 Y φ k (x {k} ) Z where the partition function Z is a normalizing term, and x {k} denotes the state of all the variables in clique k."
H OLMES converts the proof trees into a Markov network in a manner pioneered by the Markov Logic framework[REF_CITE].
A Boolean variable is created to represent the truth of each assertion in the proof forest.
"Next, H OLMES adds edges to the Markov network to create a clique corresponding to each application of an inference rule in the proof forest."
"Following the Markov Logic framework, the po-tential function of a clique has form φ(x) = e w if all member nodes are true (w denotes the weight of the inference rule), and φ(x) = 1 otherwise."
"The proba-bilities of leaf nodes are derived from the underlying knowledge base, [Footnote_2] and inferred nodes are biased with an exponential prior."
"2 In our experiments, ground assertions from WordNet get a uniformly high probability of correctness (0.9), but those ex-tracted from the Web are assigned probabilities derived from redundancy statistics, following the intuition that frequently ex-tracted facts are more likely to be true[REF_CITE]."
"Finally, H OLMES computes the approximate probability of each answer by running a variant of loopy belief propagati[REF_CITE]over the Markov network."
"In our experience this method performs well on networks derived from our Horn clause proof forest, but one could use Monte Carlo techniques or even exact methods if desired."
Note that this architecture allows H OLMES to combine information from multiple web pages to in-fer assertions not explicitly seen in the textual cor-pus.
"Because this inference is done using a Markov network, it correctly handles uncertain extractions and probabilistic dependencies."
"By using KBMC to create a custom, focused network for each query, the amount of probabilistic inference is reduced to man-ageable proportions."
"Because exact probabilistic inference is #P-complete, H OLMES uses approximate methods, but even these techniques have problems if the Markov network gets too large."
"As a result, H OLMES creates the network incrementally."
"After the first proof trees are generated, H OLMES creates the model and per-forms approximate probabilistic inference."
If more time is available then H OLMES searches for addi-tional proof trees and updates the network (Fig-ure 1).
This incremental process allows H OLMES to return initial results (with preliminary probability estimates) as soon as they are discovered.
"For efficiency, H OLMES exploits standard Data-log optimizations (e.g., it only expands proofs of re-cently added nodes and it uses an approximation to magic sets[REF_CITE], rather than simple back-wards chaining)."
"For tractability, we also allow the user to limit the number of transitive inference steps for any inference rule."
H OLMES also includes a few enhancements for dealing with information extracted from natural lan-guage.
"For example, H OLMES ’s inference rules sup-port substring/regex matching of ground assertions, to accommodate simple variations in text."
"H OLMES also can be restricted to only operate over proper nouns, which is useful for queries involving named entities."
"H OLMES is given the following set of six domain-independent rules, which are similar to the up-ward monotone rules introduced[REF_CITE]. 1. Observed relations are likely to be true: R(X,Y) :- ObservedInCorpus(X, R, Y) 2. Synonym substitution preserves meaning: R TR (X’,Y) :- R TR (X,Y) ∧ Synonym(X, X’) 3. R TR (X,Y’) :- R TR (X,Y) ∧ Synonym(Y, Y’) 4."
Generalizations preserve meaning:
"R TR (X’,Y) :- R TR (X,Y) ∧ IS-A(X, X’) 5."
"R TR (X,Y’) :- R TR (X,Y) ∧ IS-A(Y, Y’) 6."
"Transitivity of Part Meronyms: R TR (X,Y’) :- R TR (X,Y) ∧ Part-Of(Y, Y’) where R TR matches ‘* in’ (e.g., ‘born in’)."
"For example, if Q(X):-BornIn(X,‘France’) , and we know from WordNet that Paris is in France, then by inference rule 6, we know that BornIn(X,‘Paris’) will yield valid results for Q(X) ."
"Although all of these rules contain at most two relations in the body, H OLMES allows an arbitrary number of relations in the query and rule bodies."
"However, we have found that even simple rules can dramatically improve some queries."
"We set the rule weights to capture the intuition that deeper inferences decrease the likelihood (as there are more chances to make mistakes), whereas additional, independent proof trees increase the likelihood (as there is more supporting evidence)."
"Specifically, in our experiments we set the prior on inferred facts to -0.75, the weight on rule 1 to 1.5, and the weights on all other rules to 0.6."
"At present, we define these weights manually, but we expect to learn the parameter values in the future."
"If TI is applied to a corpus containing hundreds of millions or even billions of pages, its run time has to be at most linear in the size of the corpus."
This sec-tion shows that under some reasonable assumptions inference does scale linearly.
We start our analysis with two simplifications.
"First, we assume that the number of distinct, ground assertions in the KBs, |A|, grows at most linearly with the size of the textual corpus."
"This is cer-tainly true for assertions extracted by TextRunner and Kylin, and follows from our exclusion of texts with complex quantified sentences."
Our analysis now proceeds to consider scaling with respect to |A| for a fixed query and set of inference rules.
"Our second assumption is that the size of every proof tree is bounded by some constant, m. This is a strong assumption and one that depends on the precise set of inference rules and pattern of ground assertions."
"However, it holds in our experience, and if necessary could be enforced by terminating the search for proof trees at a certain depth, e.g., log(m)."
H OLMES ’s knowledge-based model construction has two parts: construction of the proof forest and conversion of the forest into a Markov network.
"Since the Markov network is essentially isomorphic to the proof forest, the conversion will be O(|A|) if the forest is linear in size, which is ensured if the time to construct the proof trees is O(|A|)."
We show this in the remainder of this section.
Recall that H OLMES requires inference rules to be function-free Horn clauses.
"While this limits ex-pressivity to some degree, it provides a huge speed benefit — logical inference over Horn clauses can be done in polynomial time, whereas general propo-sitional inference (i.e., from grounded first-order rules) is NP-complete."
"Alas, even low-order polynomial blowup is un-acceptable when the textual corpus reaches Web scale; we seek linear growth."
"Intuitively, there are two places where polynomial expansion could cause trouble."
"First, the number of different types of proofs (i.e., first order proofs) could grow too quickly, and secondly, a given type of proof tree might apply to too many ground assertions (“tuples” in database lingo)."
We treat these issues in turn.
"Under our assumptions, each proof tree can be represented as an expression in relational algebra with at most m equijoins[REF_CITE], [Footnote_3] each stemming from the application of an inference rule."
"3 Note that an inference rule of the form H(X) :- R 1 (X,Y),R 2 (Y,Z) is equivalent to the algebraic expression π X (R 1 ./ R 2 ). First a join is performed between R 1 and R 2 testing for equality between values of Y ; then a projection elim-inates all columns besides X."
"Since the number of rules is fixed, as is m, there are a constant number of possible first-order proof trees."
"The bigger concern is that any one of these first-order trees might result in a polynomial number of ground trees; if so, the size of the ground forest (and corresponding Markov network) could grow too quickly."
"In fact, polynomial growth is a common phenomena in database query evaluation."
"Luckily, most relations in the Web corpus behave more fa-vorably."
"We introduce a property of relations that ensures m-way joins, and therefore all proof trees up to size m, can be computed in O(|A|) time."
"The intuition is that most relations derived from large corpora have a ‘heavy-tailed’ distribution, wherein a few objects appear many times in a rela-tion, but most appear only once or twice, thus joins involving rare objects lead to a small number of re-sults, and so the main limitation on scalability is common objects."
"We now prove that if these com-mon objects account for a small enough fraction of the relation, then joins will still scale linearly."
"We focus on binary relations, but these results can eas-ily be extended to relations of larger arity."
An m-way equijoin over relations that are PF in the join variables will have at most k m ∗ |R| results.
"Since k m is constant for a given join and |R| scales linearly in the size of the textual corpus, proof tree construction over PF relations also scales linearly."
"However, due to their heavy-tailed distributions, most relations extracted from the Web fit the pseudo-functional definition in most, but not all values of X ."
"Fortunately, it turns out that in most cases these “bad” values of X are rare and hence don’t influence the join size significantly."
We formalize this intu-ition by defining a class of approximately pseudo-functional (APF) relations and proving that joining two APF relations produces at most a linear number of results.
"Definition 2 A relation, R, is approximately pseudo-functional (APF) in x with degree k, if X can be partitioned into two sets X G and X B such that for all x ∈ X G R is PF with degree k and X |{y|(x, y) ∈"
R}| ≤ k ∗ log(|R|) x∈X B Theorem 1.
If relation R 1 is APF in y with de-gree k 1 and R 2 is APF in y with degree k 2 then the relation Q = R 1 ./
"R 2 has size at most O(max(|R 1 |, |R 2 |))."
"Since R 1 and R 2 are APF, we know that Y can be partitioned into four groups: Y BB = Y B1 T Y B2 , Y BG = Y B1 T Y G2 , Y GB = Y G1 T Y B2 , Y GG = Y G1 TY G2 . [Footnote_4] We can show that each group leads to at most O(|A|) entries in Q. For y ∈ Y BB there are at most k 1 ∗ k 2 ∗ log(|R 1 |) ∗ log(|R 2 |) en-tries in Q."
"4 Y BB are the “doubly bad” values of y that violate the PF definition for both relations, Y GG are the values that do not vio-late the PF definition for either relation, and Y BG and Y GB are the values that violate it in only R 1 or R 2 , resp."
"The y ∈ Y GB and y ∈ Y BG lead to at most k 1 ∗ k 2 ∗ log(|R 2 |) and k 1 ∗ k 2 ∗ log(|R 1 |) entries, respectively."
"For y ∈ Y GG there are at most k 1 ∗ k 2 ∗ max(|R 1 |,|R 2 |)."
"Summing the re-sults from the four partitions, we see that |Q| is O(max(|R 1 |, |R 2 |)), thus it is O(|A|)."
"This theorem and proof can easily be extended to an m-way equijoin, as long as each relation is APF in all arguments that are being joined."
"If Q is the relation obtained by an equi-join over m relations R 1..m , each having size at most O(|A|), and if all R 1..m are APF in all arguments that they are joined in with degree at most k max , and if Y log(|R i |) ≤ |A|, then |Q| is O(|A|). 1≤i≤m"
"The inequality in Theorem 2 relates the sizes of the relations (|R|), the join (m) and the number of ground assertions (|A|)."
"However, in many cases we are interested in much smaller values of m than the inequality enables."
"We can relax the APF definition to allow a broader, but still scalable, class of m-way- APF relations."
"If Q is the relation obtained by an m-way join, and if each participating relation is APF in their joined variables with a bound of k i ∗ m p|R i | instead of k i ∗ log(|R i |), then the join is O(|A|)."
"The final step in our scaling argument concerns probabilistic inference, which is #P-Complete if per-formed exactly."
This is addressed in two ways.
"First, H OLMES uses approximate methods, e.g., loopy be-lief propagation, which avoids the cost of exact in-ference — at the cost of reduced precision."
"Sec-ondly, at a practical level, H OLMES ’s incremental construction of the graphical model (Figure 1) al-lows it to bound the size of the network by terminat-ing the search for additional proofs."
"This section reports on measurements that confirm that linear scaling with |A| occurs in practice, and that H OLMES ’s inference is not only scalable but also improves precision/recall on sample queries in a diverse set of domains."
"After describing the exper-imental domains and queries, Section 4.2 reports on the boost to the area under the precision/recall curve for a set of example queries in three domains: ge-ography, business, and nutrition."
"Section 4.3 then shows that APF relations are very common in the Web corpus, and finally Section 4.4 demonstrates empirically that H OLMES ’s inference time scales linearly with the number of pages in the corpus."
H OLMES utilized two knowledge bases in these ex-periments: T EXT R UNNER and WordNet.
"T EX - T R UNNER contains approximately 183 million dis- tinct ground assertions extracted from over 117 mil-lion web pages, and[REF_CITE]thousand manually created IS-A, Part-Of, and Synonym asser-tions."
"In all queries, H OLMES utilizes the domain-independent inference rules described in Sec-tion 2.3."
"H OLMES additionally makes use of two domain-specific inference rules in the Nutrition domain, to demonstrate the benefits of including domain-specific information."
Estimating the preci-sion and relative recall of H OLMES requires exten-sive and careful manual tagging of H OLMES output.
"To make this feasible, we restricted ourselves to a set of twenty queries in three domains, but made the domains diverse to illustrate the broad scope of the system."
We now describe each domain briefly.
Geography: the query issued is: “Who was born in one of the following countries?”
"More formally, Q(X) :- BornIn(X,{country}) where {country} is bound to each of the following nine countries in turn {France, Germany, China, Thailand, Kenya, Morocco, Peru, Columbia, Guatemala}, yielding a total of nine queries."
"Because Web text often refers to a person’s birth city rather than birth country, this query il-lustrates how combining an ground assertion (e.g., BornIn(Alberto Fujimori, Lima) ) with back-ground knowledge (e.g., LocatedIn(Lima, Peru) ) enables the system to draw new conclusions (e.g., BornIn(Alberto Fujimori, Peru) )."
Business: we issued the following two queries. 1) Which companies are acquiring software com-panies?
"Formally, Q(X) :- Acquired(X, Y) ∧ Develops(Y, ‘software’)"
This query tests H OLMES ’s ability to scalably join a large number of assertions from multiple pages. 2) Which companies are headquartered in the USA?
"Q(X) :- HeadquarteredIn(X, ‘USA’) ∧ IS-A(X, ‘company’)"
"Answering this query comprehensively requires H OLMES to combine a join (over the relations Head-quarteredIn and IS-A) with transitive inference on PartOf (e.g., Seattle is PartOf"
"Washington which is PartOf the USA) and on IS-A (e.g., Microsoft IS-A software company which IS-A company)."
The IS-A assertions came from both T EXT R UNNER (using patterns[REF_CITE]) and WordNet.
Nutrition: the nine queries issued are instances of “What foods prevent disease?”
"Where a food is a member of one of the classes: fruit, vegetable, or grain, and a disease is one of: anemia, scurvy, or osteoporosis."
"More formally, Q(X, {disease}) :-"
To measure the cost and benefit of H OLMES ’s in-ference we need to define a baseline for compar-ison.
"Answering the conjunctive queries in the business and nutrition domains requires computing joins, which T EXT R UNNER does not do."
"Thus, we defined a baseline system, B ASELINE , which has access to the underlying Knowledge Bases (KBs) (T EXT R UNNER and WordNet), and the ability to compute joins using information explicitly stated in either KB, but does not have the ability to infer new assertions."
We compared H OLMES with B ASELINE in all three domains.
Figure 3 depicts the combined pre-cision/relative recall curves for the nine Geography queries.
"H OLMES yields substantially higher re-call (the shaded region) at modestly lower preci-sion, doubling the area under the precision/recall curve (AuC)."
The other precision/recall curves also showed a slight drop in precision for substantial gains in recall.
"Table 1 summarizes the results, along with the total runtime needed for inference."
"Because relations in the business domain are much larger than in the other domains (i.e., 100x ground asser-tions), inference is slower in this domain."
We note that inference is particularly helpful with rarely mentioned instances.
"However, inference can lead to errors when the proof tree contains joins on generic terms (e.g., “company”) or common extrac-tion errors (e.g., “LLC” as a company name)."
This is a key area for future work.
"To determine the prevalence of APF relations in Web text, we examined a sample of 500 binary relations selected randomly from T EXT R UNNER ’s ground as-sertions."
"The surface forms of the relations and ar-guments may misrepresent the true properties of the underlying concepts, so to better estimate the true properties we merged synonymous values as given by Resolver[REF_CITE]or the most frequent sense of the word in WordNet."
"For exam-ple, we would consider BornIn(baby, hospital) and BornAt(infant, infirmary) to represent the same concept, and so would merge them into one instance of the ‘Born In’ relation."
"The largest two re-lations had over 1.25 million unique instances each, and 52% of the relations had more than 10,000 in-stances."
"For each relation R, we first found all instances of R extracted by T EXT R UNNER and merged all synonymous instances as described above."
"Then, for each argument of R we computed the smallest value, K min , such that R is APF with degree K min ."
"Since many interesting assertions can be inferred by simply joining two relations, we also considered the special case of 2-way joins using Corollary 3."
"We computed the smallest value, K 2./ , such that the re-lation is two-way-APF with degree K 2./ ."
Figure 4 shows the fraction of relations with K min and K 2./ of at most K as a function of varying values of K.
The results are averaged over both ar-guments of each binary relation.
"For arbitrary joins in this[REF_CITE]% of the relations are APF with de-gree less than 496; for 2-way joins (like the ones in our inference rules and test queries), 80% of the rela-tions are APF with degree less than 65."
"These results indicate that the majority of relations T EXT R UNNER extracted from text are APF, and so we can expect H OLMES ’s techniques will allow efficient inference over most relations."
"While Theorem 2 guarantees that joins over those relations will be O(|R|), that notation hides a poten-tially large constant factor of K minm ."
Fortunately the constant factor is significantly smaller in prac-tice.
"To see why, we re-examine the proof: the large factor comes from assuming that all of R’s first ar-guments which meet the PF definition are associated with exactly K min distinct second arguments."
"How-ever, in our corpus 83% of first arguments are as-sociated with only one second argument."
"Clearly, our worst-case analysis substantially over-estimates inference time for most queries."
"Moreover, in ad-ditional experiments (omitted due to space limita-tions), measured join sizes grew linearly in the size of the corpus, but were on average two to three or-ders of magnitude smaller than the bounds given in the theory."
This observation held across relations with different sizes and values of K min .
"While the results in Figure 4 may vary for other sets of relations, we believe the general trends hold."
"This is promising for Question Answering and Textual Inference systems, since if true it implies that combining information from multiple difference source is feasible, and can allow such systems to in-fer answers not explicitly seen in any source."
"Since the previous subsection showed that most re-lations are APF in their arguments, our theory pre-dicts H OLMES ’s inference will scale linearly."
"We tested this hypothesis empirically by running infer-ence over the test queries in our three domains, while varying the number of pages in the textual corpus."
Figure 5 shows how the inference time H OLMES used to answer all queries in each domain scales with KB size.
"For these queries, and several oth-ers we tested (not shown here), inference time grows linearly with the size of the KB."
Based on these re-sults we believe that H OLMES can provide scalable inference over a wide variety of domains.
"Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T[REF_CITE]."
"While many approaches have addressed this problem, our work is most closely related to that[REF_CITE], which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms."
"For in-stance,[REF_CITE]represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin- ear program derived from that representation."
"These approaches and related ones (e.g.,[REF_CITE]) use highly expressive representations, enabling them to ex-press negation, temporal information, and more."
H OLMES ’s representation is much simpler— Markov Logic Horn Clauses for inference rules coupled with a massive database of ground asser-tions.
"However, this simplification allows H OLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph."
"A sec-ond, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, H OLMES tries to find all consequents that match a conjunctive query."
"H OLMES is also related to open-domain question-answering systems such as Mulder[REF_CITE],"
"AskMSR[REF_CITE], and others[REF_CITE]."
"How-ever, these Q/A systems attempt to find individual documents or sentences containing the answer."
"They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail."
"In contrast, H OLMES utilizes TI and attempts to combine information from multiple different sen-tences in a scalable way."
"While its ability to combine information from multiple sources is promising, H OLMES has several limitations these Q/A systems do not have."
"Since H OLMES relies on an information extraction sys-tem to convert sentences into ground predicates, any limitations of the IE system will be propagated to H OLMES ."
"Additionally, the logical representa-tion H OLMES uses limits the reasoning and types of questions it can answer."
"H OLMES is geared to-wards answering questions which are naturally ex-pressed as properties and relations of entities, and is not well suited to answering more abstract or open ended questions."
"Although we have demonstrated that H OLMES is scalable, further work is needed to make it to run at interactive speeds."
"Finally, research in statistical relational learning such as MLNs[REF_CITE], RMNs[REF_CITE], and others[REF_CITE]have studied techniques for com-bining logical and probabilistic inference."
"Our in-ference rules are more restrictive than those al-lowed in MLNs, but this trade-off allows us to ef- ficiently scale inference to large, open domain cor-pora."
"By constructing only cliques for satisfied in-ference rules, H OLMES explicitly models the intu-ition behind LazySAT inference[REF_CITE]as used in MLNs."
"I.e., most Horn clause inference rules will be trivially satisfied since their antecedents will be false, so we only need to worry about ones where the antecedent is true."
"This paper makes three main contributions: 1. We introduce and evaluate the H OLMES sys-tem, which leverages KBMC methods in order to scale a class of TI methods to the Web. 2."
"We define the notion of Approximately Pseudo-Functional (APF) relations and prove that, for a APF relations, H OLMES ’s inference time in-creases linearly with the size of the input cor-pus."
"We show empirically that APF relations appear to be prevalent in our Web corpus (Fig-ure 4), and that H OLMES ’s runtime does scale linearly with the size of its input (Figure 5), tak-ing only a few CPU minutes when run over 183 million distinct ground assertions. 3."
"We present experiments demonstrating that, for a set of queries in the domains of geography, business, and nutrition, H OLMES substantially improves the quality of answers (measured by AuC) relative to a “no inference” baseline."
"In the future, we plan more extensive tests to char-acterize when H OLMES ’s inference is helpful."
We also hope to examine in what cases jointly perform-ing extraction and inference (as opposed to perform-ing them separately) is feasible at scale.
"Finally, we plan to examine methods for H OLMES to learn both rule weights and new inference rules."
This paper proposes a novel maximum en-tropy based rule selection (MERS) model for syntax-based statistical machine transla-tion (SMT).
The MERS model combines lo-cal contextual information around rules and information of sub-trees covered by variables in rules.
"Therefore, our model allows the de-coder to perform context-dependent rule se-lection during decoding."
"We incorporate the MERS model into a state-of-the-art linguis-tically syntax-based SMT model, the tree-to-string alignment template model."
Experi-ments show that our approach achieves signif-icant improvements over the baseline system.
Syntax-based statistical machine translation (SMT) models[REF_CITE]capture long distance reorderings by us-ing rules with structural and linguistical information as translation knowledge.
"Typically, a translation rule consists of a source-side and a target-side."
"How-ever, the source-side of a rule usually corresponds to multiple target-sides in multiple rules."
"Therefore, during decoding, the decoder should select a correct target-side for a source-side."
We call this rule selec-tion.
Rule selection is of great importance to syntax-based SMT systems.
"Comparing with word selec-tion in word-based SMT and phrase selection in phrase-based SMT, rule selection is more generic and important."
"This is because that a rule not only contains terminals (words or phrases), but also con- tains nonterminals and structural information."
"Ter-minals indicate lexical translations, while nontermi-nals and structural information can capture short or long distance reorderings."
See rules in Figure 1 for illustration.
These two rules share the same syntactic tree on the source side.
"However, on the target side, either the translations for terminals or the phrase re-orderings for nonterminals are quite different."
"Dur-ing decoding, when a rule is selected and applied to a source text, both lexical translations (for terminals) and reorderings (for nonterminals) are determined."
"Therefore, rule selection affects both lexical transla-tion and phrase reordering."
"However, most of the current syntax-based sys-tems ignore contextual information when they se-lecting rules during decoding, especially the infor-mation of sub-trees covered by nonterminals."
"For example, the information of X 1 and X 2 is not recorded when the rules in Figure 1 extracted from the training examples in Figure 2."
This makes the decoder hardly distinguish the two rules.
"Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed to be helpful for rule selection."
Recent research showed that contextual infor-mation can help perform word or phrase selec-tion.
"Similar to WSD,[REF_CITE]used con-textual information to solve the ambiguity prob-lem for phrases."
They integrated a phrase-sense-disambiguation (PSD) model into a phrase-based SMT system and achieved improvements.
"In this paper, we propose a novel solution for rule selection for syntax-based SMT."
We use the maximum entropy approach to combine rich con-textual information around a rule and the informa-tion of sub-trees covered by nonterminals in a rule.
"For each ambiguous source-side of translation rules, a maximum entropy based rule selection (MERS) model is built."
Thus the MERS models can help the decoder to perform a context-dependent rule selec-tion.
"Comparing with WSD (or PSD), there are some advantages of our approach: • Our approach resolves ambiguity for rules with multi-level syntactic structure, while WSD re-solves ambiguity for strings that have no struc-tures; • Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; • Our method takes WSD as a special case, since a rule may only consists of terminals."
"In our previous work[REF_CITE], we re-ported improvements by integrating a MERS model into a formally syntax-based SMT model, the hier-archical phrase-based model[REF_CITE]."
"In this paper, we incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model[REF_CITE]."
The basic differences are: • The MERS model here combines rich informa-tion of source syntactic tree as features since the translation model is linguistically syntax-based.
"In the TAT model, a TAT can be considered as a translation rule which describes correspondence be-tween source syntactic tree and target string."
TAT can capture linguistically motivated reorderings at short or long distance.
"Experiments show that by incorporating MERS model, the baseline system achieves statistically significant improvement."
This paper is organized as follows: Section 2 reviews the TAT model; Section 3 introduces the MERS model and describes feature definitions; Sec-tion 4 demonstrates a method to incorporate the MERS model into the translation model; Section 5 reports and analyzes experimental results; Section 6 gives conclusions.
"Our baseline system is Lynx[REF_CITE], which is a linguistically syntax-based SMT system."
"For translating a source sentence f 1J = f 1 ...f j ...f J , Lynx firstly employs a parser to produce a source syntactic tree T(f 1J ), and then uses the source syntactic tree as the input to search translations:"
I I J (1) ! e 1 = argmax e !I1 P r(e 1 |f 1 ) = argmax e !
I P r(T (f 1J ) |f 1J )
P r(e I1 |T (f 1J )) 1
"In doing this, Lynx uses tree-to-string alignment template to build relationship between source syn-tactic tree and target string."
"A TAT is actually a translation rule: the source-side is a parser tree with leaves consisting of words and nonterminals, the target-side is a target string consisting of words and nonterminals."
"TAT can be learned from word-aligned, source-parsed parallel corpus."
"Figure 4 shows three types of TATs extracted from the training example in Fig-ure 3: lexicalized (the left), partially lexicalized (the middle), unlexicalized (the right)."
"Lexicalized TAT contains only terminals, which is similar to phrase-to-phrase translation in phrase-based model except that it is constrained by a syntactic tree on the source-side."
"Partially lexicalized TAT contains both terminals and non-terminals, which can be used for both lexical translation and phrase reordering."
Un-lexicalized TAT contains only nonterminals and can only be used for phrase reordering.
"Lynx builds translation model in a log-linear framework[REF_CITE]: (2) P (e I1 |T (f 1J )) = exp[ &quot; m λ m h m (e I1 , T (f 1J ))] &quot; e ! exp[ &quot; m λ m h m (e I1 , T (f 1J ))]"
"Following features are used: • Translation probabilities: P ( ! e|T ! ) and P (T ! |e ! ); • Lexical weights: P w ( ! e|T ! ) and P w (T ! | ! e); • TAT penalty: exp(1), which is analogous to phrase penalty in phrase-based model; • Language model P lm (e I1 ); • Word penalty I."
"In Lynx, rule selection mainly depends on trans-lation probabilities and lexical weights."
"These four scores describe how well a source tree links to a tar-get string, which are estimated on the training cor-pus according to occurrence times of ! e and T ! ."
"There are no features in Lynx that can capture contextual information during decoding, except for the n-gram language model which considers the left and right neighboring n-1 target words."
But this information it very limited.
"In this paper, we focus on using contextual infor-mation to help the TAT model perform context-dependent rule selection."
"We consider the rule se-lection task as a multi-class classification task: for a source syntactic tree T ! , each corresponding target string e ! is a label."
"Thus during decoding, when a TAT !T ! , e ! ! &quot; is selected, T ! is classified into label e ! ! , actually."
A good way to solve the classification problem is the maximum entropy approach:
"P rs (e ! |T ! , T (X k )) = (3) exp[ &quot; i λ i h i (e ! , C(T ! ), T (X ))] k &quot; exp[ &quot; i λ i h i (e ! ! , C(T ! ), T (X k ))] e ! ! where T ! and e ! are the source tree and target string of a TAT, respectively. h i is a binary feature functions and λ i is the feature weight of h i ."
C(T ! ) defines local contextual information of T ! .
"X k is a nonterminal in the source tree T ! , where k is an index."
T (X k ) is the source sub-tree covered by X k .
The advantage of the MERS model is that it uses rich contextual information to compute posterior probability for e ! given T ! .
"However, the transla-tion probabilities and lexical weights in Lynx ignore these information."
"Note that for each ambiguous source tree, we build a MERS model."
"That means, if there are N source trees extracted from the training corpus are ambiguous (the source tree which corresponds to multiple translations), thus for each ambiguous source tree T i (i = 1,...,N), a MERS model M i (i = 1, ..., N) is built."
"Since a source tree may cor-respond to several hundreds of target translations at most, the feature space of a MERS model is not pro-hibitively large."
Thus the complexity for training a MERS model is low.
"Let !T ! ,e ! &quot; be a translation rule in the TAT model."
We use f(T ! ) to represent the source phrase covered by T ! .
"To build a MERS model for the source tree T ! , we explore various features listed below. 1. Lexical Features (LF) These features are defined on source words."
"Specifically, there are two kinds of lexical fea-tures: external features f −1 and f +1 , which are the source words immediately to the left and right of f(T ! ), respectively; internal fea-tures f L (T(X k )) and f R (T(X k )), which are the left most and right most boundary words of the source phrase covered by T(X k ), respec-tively."
See Figure 5 (a) for illustration.
"In this example, f −1 =tı́gāo, f +1 =zhı̀zào, f L (T (X 1 ))=gōngyè, f R (T (X 1 ))=chǎnpı̌n. 2."
Parts-of-speech (POS) Features (POSF)
"These features are the POS tags of the source words defined in the lexical features: P −1 , P +1 , P L (T(X k )), P R (T(X k )) are the POS tags of f −1 , f +1 , f L (T (X k )), f R (T (X k )), re- spectively."
POS tags can generalize over all training examples.
Figure 5 (b) shows POS features.
"P −1 =VV, P +1 =NN, P L (T (X 1 ))=NN, P R (T (X 1 ))=NN. 3."
Span Features (SPF)
These features are the length of the source phrase f(T (X k )) covered by T (X k ).
"In Liu’s TAT model, the knowledge learned from a short span can be used for a larger span."
This is not reliable.
Thus we use span features to allow the MERS model to learn a preference for short or large span.
"In Figure 5 (c), the span of X 1 is 2. 4."
Parent Feature (PF)
The parent node of T ! in the parser tree of the source sentence.
The same source sub-tree may have different parent nodes in different training examples.
"Therefore, this feature may provide information for distinguishing source sub-trees."
Figure 5 (d) shows that the parent is a NP node. 5.
Sibling Features (SBF)
The siblings of the root of T ! .
This feature con-siders neighboring nodes which share the same parent node.
"In Figure 5 (e), the source tree has one sibling node NPB."
"Those features make use of rich information around a rule, including the contextual information of a rule and the information of sub-trees covered by nonterminals."
They are never used in Liu’s TAT model.
Figure 5 shows features for a partially lexicalized source tree.
"Furthermore, we also build MERS mod-els for lexicalized and unlexicalized source trees."
"Note that for lexicalized tree, features do not include the information of sub-trees since there is no nonter-minals."
The features can be easily obtained by modify-ing the TAT extraction algorithm described[REF_CITE].
"When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees."
Then we use the toolkit implemented[REF_CITE]to train MERS models for the am-biguous source syntactic trees separately.
We set the iteration number to 100 and Gaussian prior to 1.
We integrate the MERS models into the TAT model during the translation of each source sentence.
Thus the MERS models can help the decoder perform context-dependent rule selection during decoding.
"For integration, we add two new features into the log-linear translation model: • P rs (e ! |T ! , T(X k ))."
"This feature is computed by the MERS model according to equation (3), which gives a probability that the model select-ing a target-side ! e given an ambiguous source-side T ! , considering rich contextual informa-tion. • P ap = exp(1)."
"During decoding, if a source tree has multiple translations, this feature is set to exp(1), otherwise it is set to exp(0)."
"Since the MERS models are only built for ambiguous source trees, the first feature P rs (e ! |T ! , T (X k )) for non-ambiguous source tree will be set to 1.0."
"Therefore, the decoder will prefer to use non-ambiguous TATs."
"However, non-ambiguous TATs usually occur only once in the training corpus, which are not reliable."
Thus we use this feature to reward ambiguous TATs.
The advantage of our integration is that we need not change the main decoding algorithm of Lynx.
"Furthermore, the weights of the new features can be trained together with other features of the translation model."
We carry out experiments on Chinese-to-English translation.
"The training corpus is the FBIS cor-pus, which contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words."
"For the language model, we use SRI Language Modeling Toolkit[REF_CITE]with modified Kneser-Ney smoothing[REF_CITE]to train two tri-gram language models on the English portion of the training corpus and the Xinhua portion of the Gi-gaword corpus, respectively."
"The translation quality is evaluated by BLEU met-ric[REF_CITE], as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n = 4."
"To train the translation model, we first run GIZA++[REF_CITE]to obtain word alignment in both translation directions."
Then the word alignment is refined by performing “grow-diag-final” method[REF_CITE].
We use a Chinese parser de-veloped by Deyi Xiong[REF_CITE]to parse the Chinese sentences of the training corpus.
"Our TAT extraction algorithm is similar[REF_CITE], except that we make some tiny modifica-tions to extract contextual features for MERS mod-els."
"To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5."
See[REF_CITE]for specific definitions of these parameters.
Table 1 shows statistical information of TATs which are filtered by the two test sets.
"For each type (lexicalized, partially lexicalized, unlexicalized) of TATs, a great portion of the source trees are am-biguous."
The number of ambiguous source trees ac- counts for 78.34% of the total source trees.
This in-dicates that the TAT model faces serious rule selec-tion problem during decoding.
We use Lynx as the baseline system.
"Then the MERS models are incorporated into Lynx, and the system is called Lynx+MERS."
"To run the decoder, Lynx and Lynx+MERS share the same settings: tatTable-limit=30, tatTable-threshold=0, stack-limit=100, stack-threshold=0.00001."
The meanings of the pruning parameters are the same[REF_CITE].
We perform minimum error rate training[REF_CITE]to tune the feature weights for the log-linear model to maximize the systems’s BLEU score on the development set.
The weights are shown in Table 2.
These weights are then used to run Lynx and Lynx+MERS on the test sets.
Table 3 shows the results.
Lynx obtains BLEU scores of 26.15[REF_CITE].09[REF_CITE].
"Using all features described in Section 3.2, Lynx+MERS finally ob-tains BLEU scores of 27.05[REF_CITE].28[REF_CITE]."
"The absolute improvements is 0.90 and 1.19, respectively."
"Using the sign-test described[REF_CITE], both improvements are statistically significant at p &lt; 0.01."
"Moreover, Lynx+MERS also achieves higher n-gram preci-sions than Lynx."
The baseline system only uses four features for rule selection: the translation probabilities P( ! e|T ! ) and P(T ! |e ! ); and the lexical weights P w ( ! e|T ! ) and P w (T ! |e ! ).
"These features are estimated on the train-ing corpus by the maximum likelihood approach, which does not allow the decoder to perform a con-text dependent rule selection."
"Although Lynx uses language model as feature, the n-gram language model only considers the left and right n-1 neigh-boring target words."
The MERS models combines rich contextual in-formation as features to help the decoder perform rule selection.
Table 4 shows the effect of different feature sets.
We test two classes of feature sets: the single feature (the top four rows of Table 4) and the combination of features (the bottom five rows of Ta-ble 4).
"For the single feature set, the POS tags are the most useful and stable features."
"Using this fea-ture, Lynx+MERS achieves improvements on both the test sets."
"The reason is that POS tags can be gen-eralized over all training examples, which can alle-viate the data sparseness problem."
"Although we find that some single features may hurt the BLEU score, they are useful in combina-tion of features."
This is because one of the strengths of the maximum entropy model is that it can in-corporate various features to perform classification.
"Therefore, using all features defined in Section 3.2, we obtain statistically significant improvements (the last row of Table 4)."
"In order to know how the MERS models improve translation quality, we in-spect the 1-best outputs of Lynx and Lynx+MERS."
"We find that the first way that the MERS models help the decoder is that they can perform better selection for words or phrases, similar to the effect of WSD or PSD."
This is because that lexicalized and partially lexicalized TAT contains terminals.
Considering the following examples: • Source: • Reference: Malta is located in southern Eu-rope • Lynx: Malta in southern Europe • Lynx+MERS: Malta is located in southern Eu-rope
Here the Chinese word “ ” is incor-rectly translated into “in” by the baseline system.
Lynx+MERS produces the correct translation “is lo-cated in”.
"That is because, the MERS model consid-ers more contextual information for rule selection."
"In the MERS model, P rs (in| ) = 0.09, which is smaller than P rs (is located in| ) = 0.14."
"There-fore, the MERS model prefers the translation “is lo-cated in”."
"Note that here the source tree (VV ) is lexicalized, and the role of the MERS model is actually the same as WSD."
The second way that the MERS models help the decoder is that they can perform better phrase re-orderings.
Considering the following examples: • Source: [ ] 1 [ ] 2 ... • Reference:
According to its [development strategy] 2 [in the Chinese market] 1 ... • Lynx: Accordance with [the Chinese market] 1 [development strategy] 2 ... • Lynx+MERS:
According to the [development strategy] 2 [in the Chinese market] 1
The syntactic tree of the Chinese phrase “ ” is shown in Figure 6.
"How-ever, there are two TATs which can be applied to the source tree, as shown in Figure 7."
The baseline sys-tem selects the left TAT and produces a monotone translation of the subtrees “X 1 :PP” and “X 2 :NPB”.
"However, Lynx+MERS uses the right TAT and per-forms correct phrase reordering by swapping the two source phrases."
"Here the source tree is partially lex-icalized, and both the contextual information and the information of sub-trees covered by nontermi-nals are considered by the MERS model."
"In this paper, we propose a maximum entropy based rule selection model for syntax-based SMT."
"We use two kinds information as features: the local-contextual information of a rule, the information of sub-trees matched by nonterminals in a rule."
"During decoding, these features allow the decoder to per-form a context-dependent rule selection."
"However, this information is never used in most of the current syntax-based SMT models."
"The advantage of the MERS model is that it can help the decoder not only perform lexical selection, but also phrase reorderings."
"We demonstrate one way to incorporate the MERS models into a state-of-the-art linguistically syntax-based SMT model, the tree-to-string alignment model."
"Experiments show that by incorporating the MERS models, the baseline system achieves statistically significant im-provements."
We find that rich contextual information can im-prove translation quality for a syntax-based SMT system.
"In future, we will explore more sophisti-cated features for the MERS model."
"Moreover, we will test the performance of the MERS model on large scale corpus."
This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.
An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.
"Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty."
The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.
Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.
System combination has been applied successfully to various machine translation tasks.
"Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output ([REF_CITE])."
"A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores."
"The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by using posterior probability estimates, or by using a combination of these measures and other features."
"Constructing a confusion network requires choosing one of the hypotheses as the backbone (also called “skeleton” in the literature), and other hypotheses are aligned to it at the word level."
High quality hypothesis alignment is crucial to the performance of the resulting system combination.
"However, there are two challenging issues that make MT hypothesis alignment difficult."
"First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other."
"Second, correct translations may have different word orderings in different hypotheses and these words need to be properly reordered in hypothesis alignment."
"In this paper, we propose an indirect hidden Markov model (IHMM) for MT hypothesis alignment."
The HMM provides a way to model both synonym matching and word ordering.
"Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty, without using large amount of training data."
Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluati[REF_CITE].
The current state-of-the-art is confusion-network-based MT system combination as described by
Rosti and colleagues ([REF_CITE]).
The major steps are illustrated in Figure 1.
"In Fig. 1 (a), hypotheses from different MT systems are first collected."
"Then in Fig. 1 (b), one of the hypotheses is selected as the backbone for hypothesis alignment."
This is usually done by a sentence-level minimum Bayes risk (MBR) method which selects a hypothesis that has the minimum average distance compared to all hypotheses.
The backbone determines the word order of the combined output.
"Then as illustrated in Fig. 1 (c), all other hypotheses are aligned to the backbone."
"Note that in Fig. 1 (c) the symbol ε denotes a null word, which is inserted by the alignment normalization algorithm described in section 3.4."
"Fig. 1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis."
"Then in Fig. 1 (d), a confusion network is constructed based on the aligned hypotheses, which consists of a sequence of sets in which each word is aligned to a list of alternative words (including null) in the same set."
"Then, a set of global and local features are used to decode the confusion network."
"In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone."
"One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment ([REF_CITE])."
"In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment."
"Let e 1I  (e 1 ,...,e I ) denote the backbone, e 1 "
"J (e 1 ,...,e J ) a hypothesis to be aligned to e 1I , and a 1J (a 1 ,...,a J ) the alignment that specifies the position of the backbone word aligned to each hypothesis word."
We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence.
"We use a first-order HMM, assuming that the emission probability p(e |e ) depends only on the j a j backbone word, and the transition probability p(a j |a j1 ,I) depends only on the position of the last state and the length of the backbone."
"Treating the alignment as hidden variable, the conditional probability that the hypothesis is generated by the backbone is given by"
"J p(e 1  J |e 1I )   p(a |a ,I)p(e |e ) (1) j a j  j j1 a 1J j1"
"As in HMM-based bilingual word alignment[REF_CITE], we also associate a null with each backbone word to allow generating hypothesis words that do not align to any backbone word."
"In HMM-based hypothesis alignment, emission probabilities model the similarity between a backbone word and a hypothesis word, and will be referred to as the similarity model."
"The transition probabilities model word reordering, and will be called the distortion model."
"The similarity model, which specifies the emission probabilities of the HMM, models the similarity between a backbone word and a hypothesis word."
"Since both words are in the same language, the similarity model can be derived based on both semantic similarity and surface similarity, and the overall similarity model is a linear interpolation of the two: p(e | e )    p (e | e ) (1  ) p (e | e ) (2) j i sem j i sur j i where p sem (e j | e i ) and p sur (e j | e i ) reflect the semantic and surface similarity between e and j e i , respectively, and α is the interpolation factor."
"Since the semantic similarity between two target words is source-dependent, the semantic similarity model is derived by using the source word sequence as a hidden layer: p sem (e j | e i )"
"K   p( f | e ) p(e | f ,e ) k i j k i k0 K   p( f |e ) p(e | f ) k i (3) j k k0 where f 1K  ( f 1 ,..., f K ) is the source sentence."
"Moreover, in order to handle the case that two target words are synonyms but neither of them has counter-part in the source sentence, a null is introduced on the source side, which is represented by f 0 ."
The last step in (3) assumes that first e i generates all source words including null.
Then e j ’ is generated by all source words including null.
"In the common SMT scenario where a large amount of bilingual parallel data is available, we can estimate the translation probabilities from a source word to a target word and vice versa via conventional bilingual word alignment."
"Then both p( f k |e i ) and p(e j  | f k ) in (3) can be derived: p(e | f )  p (e | f ) j k s2t j k where p s2t (e j | f k ) is the translation model from the source-to-target word alignment model, and p( f k |e i ) , which enforces the sum-to-[Footnote_1] constraint over all words in the source sentence, takes the following form, p( f k | e i )  K p t2s ( f k | e i )  p ( f | e ) t2s k i k0 where p t2s (f k |e i ) is the translation model from the target-to-source word alignment model."
"1 The other direction, p s2t (e i |null) , is available from the source-to-target translation model."
"In our method, p t2s (null | e i ) for all target words is simply a constant p null , whose value is optimized on held-out data 1 ."
The surface similarity model can be estimated in several ways.
"A very simple model could be based on exact match: the surface similarity model, p sur (e j |e i ) , would take the value 1.0 if e’= e, and 0 otherwise [Footnote_2] ."
2 Usually a small back-off value is assigned instead of 0.
"However, a smoothed surface similarity model is used in our method."
"If the target language uses alphabetic orthography, as English does, we treat words as letter sequences and the similarity measure can be the length of the longest matched prefix (LMP) or the length of the longest common subsequence (LCS) between them."
"Then, this raw similarity measure is transformed to a surface similarity score between 0 and 1 through an exponential mapping, p sur (e j | e i )  exp   s(e,e ) 1  j i (4) where s(e,e ) is computed as j i s(e,e )  M(e,e ) j i max(| e |,| e |) j i j i and M(e,e ) is the raw similarity measure of e j ’ j i e i , which is the length of the LMP or LCS of e j ’ and e i . and ρ is a smoothing factor that characterizes the mapping, Thus as ρ approaches infinity, p sur (e j |e i ) backs off to the exact match model."
We found the smoothed similarity model of (4) yields slightly better results than the exact match model.
Both LMP- and LCS- based methods achieve similar performance but the computation of LMP is faster.
"Therefore, we only report results of the LMP-based smoothed similarity model."
"The distortion model, which specifies the transition probabilities of the HMM, models the first-order dependencies of word ordering."
"In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities p(a j  i | a j1  i,I) depend only on the jump distance (i - i&apos;)[REF_CITE]: p(i |i,I)  c(i i) (5)  c(l i)"
"As suggested[REF_CITE], we can group the distortion parameters {c(d)}, d= i - i&apos;, into a few buckets."
"In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6)."
The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided.
"By doing this, only a handful of c(d) parameters need to be estimated."
"Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments."
"Since both the backbone and the hypothesis are in the same language, It seems intuitive that the distortion model should favor monotonic alignment and only allow non-monotonic alignment with a certain penalty."
"This leads us to use a distortion model of the following form, where K is a tuning factor optimized on held-out data."
"As shown in Fig. 2, the value of distortion score peaks at d=1, i.e., the monotonic alignment, and decays for non-monotonic alignments depending on how far it diverges from the monotonic alignment."
"Given an HMM, the Viterbi alignment algorithm can be applied to find the best alignment between the backbone and the hypothesis, aˆ 1J  argmax  p(a |a ,I)p(e |e ) J (7) j a j  j j1 a 1J j1"
"However, the alignment produced by the algorithm cannot be used directly to build a confusion network."
There are two reasons for this.
"First, the alignment produced may contain 1-N mappings between the backbone and the hypothesis whereas 1-1 mappings are required in order to build a confusion network."
"Second, if hypothesis words are aligned to a null in the backbone or vice versa, we need to insert actual nulls into the right places in the hypothesis and the backbone, respectively."
"Therefore, we need to normalize the alignment produced by Viterbi search."
E B … e 2 ε 2 … … ε e 2 ε ε … e 1 &apos; e 2 &apos; e 3 &apos; e 4 &apos; E h e 1 &apos; e 2 &apos; e 3 &apos; e 4 &apos; (a) hypothesis words are aligned to the backbone null
E B e 1 ε 1 e 2 ε 2 e 3 ε 3 … e 1 e 2 e 3 … e 2 &apos; ε e 1 &apos; E h e 1 &apos; e 2 &apos; … (b) a backbone word is aligned to no hypothesis word
"First, whenever more than one hypothesis words are aligned to one backbone word, we keep the link which gives the highest occupation probability computed via the forward-backward algorithm."
The other hypothesis words originally aligned to the backbone word will be aligned to the null associated with that backbone word.
"Second, for the hypothesis words that are aligned to a particular null on the backbone side, a set of nulls are inserted around that backbone word associated with the null such that no links cross each other."
"As illustrated in Fig. 3 (a), if a hypothesis word e 2 ’ is aligned to the backbone word e 2 , a null is inserted in front of the backbone word e 2 linked to the hypothesis word e 1 ’ that comes before e 2 ’."
Nulls are also inserted for other hypothesis words such as e 3 ’ and e 4 ’ after the backbone word e 2 .
"If there is no hypothesis word aligned to that backbone word, all nulls are inserted after that backbone word . [Footnote_3]"
3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word.
"For a backbone word that is aligned to no hypothesis word, a null is inserted on the hypothesis side, right after the hypothesis word which is aligned to the immediately preceding backbone word."
An example is shown in Fig. 3 (b).
The two main hypothesis alignment methods for system combination in the previous literature are GIZA++ and TER-based methods.
"This approach uses the conventional HMM model bootstrapped from IBM Model-1 as implemented in GIZA++, and heuristically combines results from aligning in both directions."
System combination based on this approach gives an improvement over the best single system.
"However, the number of hypothesis pairs for training is limited by the size of the test corpus."
"Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples."
"Therefore, GIZA++ training on such a data set may be unreliable."
"TER[REF_CITE]measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis."
The best alignment is the one that gives the minimum number of translation edits.
"TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007)."
"However, when searching for the optimal alignment, the TER-based method uses a strict surface hard match for counting edits."
"Therefore, it is not able to handle synonym matching well."
"Moreover, although TER-based alignment allows phrase shifts to accommodate the non-monotonic word ordering, all non-monotonic shifts are penalized equally no matter how short or how long the move is, and this penalty is set to be the same as that for substitution, deletion, and insertion edits."
"Therefore, its modeling of non-monotonic word ordering is very coarse-grained."
"In contrast to the GIZA++-based method, our IHMM-based method has a similarity model estimated using bilingual word alignment HMMs that are trained on a large amount of bi-text data."
"Moreover, the surface similarity information is explicitly incorporated in our model, while it is only used implicitly via parameter initialization for IBM Model-1 training[REF_CITE]."
"On the other hand, the TER-based alignment model is similar to a coarse-grained, non-normalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model."
There have been other hypothesis alignment methods.
"In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-to- English (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluati[REF_CITE]."
We compare to the TER-based method used[REF_CITE].
"In the following experiments, the NIST BLEU score is used as the evaluation metric[REF_CITE], which is reported as a percentage in the following sections."
"In our implementation, the backbone is selected with MBR."
Only the top hypothesis from each single system is considered as a backbone.
A uniform posteriori probability is assigned to all hypotheses.
TER is used as loss function in the MBR computation.
"Similar[REF_CITE], each word in the confusion network is associated with a word posterior probability."
"Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r) η , where r is the rank of the hypothesis, and η is a rank smoothing parameter."
The system specific rank-based score of a word w for a given system S is the sum of all the rank-based scores of the hypotheses in system S that contain the word w at the given position (after hypothesis alignment).
This score is then normalized by the sum of the scores of all the alternative words at the same position and from the same system S to generate the system specific word posterior.
"Then, the total word posterior of w over all systems is a sum of these system specific posteriors weighted by system weights."
"Beside the word posteriors, we use language model scores and a word count as features for confusion network decoding."
"Therefore, for an M-way system combination that uses N LMs, a total of M+N+1 decoding parameters, including M-1 system weights, one rank smoothing factor, N language model weights, and one weight for the word count feature, are optimized using Powell’s method[REF_CITE]to maximize BLEU score on a development set [Footnote_4] ."
4 The parameters of IHMM are not tuned by maximum-BLEU training.
Two language models are used in our experiments.
"One is a trigram model estimated from the English side of the parallel training data, and the other is a 5-gram model trained on the English GigaWord corpus from LDC using the MSRLM toolkit[REF_CITE]."
"In order to reduce the fluctuation of BLEU scores caused by the inconsistent translation output length, an unsupervised length adaptation method has been devised."
We compute an expected length ratio between the MT output and the source sentences on the development set after maximum- BLEU training.
"Then during test, we adapt the length of the translation output by adjusting the weight of the word count feature such that the expected output/source length ratio is met."
"In our experiments, we apply length adaptation to the system combination output at the level of the whole test corpus."
The development (dev) set used for system combination parameter training contains 1002 sentences sampled from the previous NIST MT Chinese-to-English test sets: 35%[REF_CITE]%[REF_CITE]and 10%[REF_CITE]-newswire.
"The test set is the[REF_CITE]Chinese-to-English “current” test set, which includes 1357 sentences from both newswire and web-data genres."
Both dev and test sets have four references per sentence.
"As inputs to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems."
All outputs on the[REF_CITE]test set were true-cased before scoring using a log-linear conditional Markov model proposed[REF_CITE].
"However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead."
"In our main experiments, outputs from a total of eight single MT systems were combined."
"As listed in Table 1, Sys-1 is a tree-to-string system proposed[REF_CITE]; Sys-2 is a phrase-based system with fast pruning proposed[REF_CITE]; Sys-3 is a phrase-based system with syntactic source reordering proposed[REF_CITE]; Sys-[Footnote_4] is a syntax-based pre-ordering system proposed by Li et. al. (2007); Sys-5 is a hierarchical system proposed[REF_CITE]; Sys-6 is a lexicalized re-ordering system proposed[REF_CITE]; Sys-7 is a two-pass phrase-based system with adapted LM proposed[REF_CITE]; and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed[REF_CITE]."
4 The parameters of IHMM are not tuned by maximum-BLEU training.
All systems were trained within the confines of the constrained training condition[REF_CITE]evaluation.
These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data.
"The bilingual translation models used to compute the semantic similarity are from the word-dependent HMMs proposed[REF_CITE], which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition[REF_CITE]."
"In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2."
These settings are optimized on the dev set.
"Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test sets, are presented in Table 1."
"The TER-based hypothesis alignment tool used in our experiments is the publicly available TER Java program, TERCOM[REF_CITE]."
Default settings of TERCOM are used in the following experiments.
"On the dev set, the case insensitive BLEU score of the IHMM-based 8-way system combination output is about 5.8 points higher than that of the best single system."
"Compared to the TER-based method, the IHMM-based method is about 1.5 BLEU points better."
"On the[REF_CITE]test set, the IHMM-based system combination gave a case sensitive BLEU score of 30.89%."
It outperformed the best single system by 4.7 BLEU points and the TER-based system combination by 1.0 BLEU points.
Note that the best single system on the dev set and the test set are different.
"The different single systems are optimized on different tuning sets, so this discrepancy between dev set and test set results is presumably due to differing degrees of mismatch between the dev and test sets and the various tuning sets."
"In order to evaluate how well our method performs when we combine more systems, we collected MT outputs[REF_CITE]from seven additional single systems as summarized in Table 2."
These systems belong to two groups.
Sys-9 to Sys-12 are in the first group.
They are syntax-augmented hierarchical systems similar to those described[REF_CITE]using different Chinese word segmentation and language models.
The second group has Sys-13 to Sys-15.
"Sys-13 is a phrasal system proposed[REF_CITE], Sys-14 is a hierarchical system proposed[REF_CITE], and Sys-15 is a syntax-based system proposed[REF_CITE]."
All seven systems were trained within the confines of the constrained training condition[REF_CITE]evaluation.
No MT outputs on our dev set are available from them at present.
"Therefore, we directly adopt system combination parameters trained for the previous 8-way system combination, except the system weights, which are re-set by the following heuristics: First, the total system weight mass 1.0 is evenly divided among the three groups of single systems: {Sys-1~8}, {Sys-9~12}, and {Sys-13~15}."
Each group receives a total system weight mass of 1/3.
"Then the weight mass is further divided in each group: in the first group, the original weights of systems 1~8 are multiplied by 1/3; in the second and third groups, the weight mass is evenly distributed within the group, i.e., 1/12 for each system in group 2, and 1/9 for each system in group 3 5 ."
"Length adaptation is applied to control the final output length, where the same expected length ratio of the previous 8-way system combination is adopted."
The results of the 15-way system combination are presented in Table 3.
It shows that the IHMM-based method is still about 1 BLEU point better than the TER-based method.
"Moreover, combining 15 single systems gives an output that has a NIST BLEU score of 34.82%, which is 3.9 points better than the best submission to the[REF_CITE]constrained training track[REF_CITE]."
"To our knowledge, this is the best result reported on this task. 0, in which only the surface similarity model is used for the overall similarity model, the performance degrades by about 0.2 point."
"Therefore, the surface similarity information seems more important for monolingual hypothesis alignment, but both sub-models are useful."
We investigate the effect of the distance-based distortion model by varying the controlling factor K in (6).
"For example, setting K=1.0 gives a linear-decay distortion model, and setting K=2.0 gives a quadratic smoothed distance-based distortion model."
"As shown in Table [Footnote_5], the optimal result can be achieved using a properly smoothed distance-based distortion model."
5 This is just a rough guess because no dev set is available. We believe a better set of system weights could be obtained if MT outputs on a common dev set were available.
"In this section, we evaluate the effect of the semantic similarity model and the surface similarity model by varying the interpolation weight α of (2)."
The results on both the dev and test sets are reported in Table 4.
"In one extreme case, α = 1, the overall similarity model is based only on semantic similarity."
"This gives a case insensitive BLEU score of 41.70% and a case sensitive BLEU score of 28.92% on the dev and test set, respectively."
The accuracy is significantly improved to 43.62% on the dev set and 30.89% on test set when α = 0.3.
"In another extreme case, α ="
Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.
"In this paper, an IHMM-based method is proposed for hypothesis alignment."
It uses a similarity model for synonym matching and a distortion model for word ordering.
"In contrast to previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical to monolingual word alignment, and a smoothed distance-based distortion model is used to model the first-order dependency of word ordering, which is shown to be better than simpler approaches."
Our experimental results show that the IHMM-based hypothesis alignment method gave superior results on the[REF_CITE]C2E test set compared to the TER-based method.
"Moreover, we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission[REF_CITE]."
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.
"We propose a multipass, coarse-to-fine approach in which the language model complexity is incremen-tally introduced."
"In contrast to previous order-based bigram-to-trigram approaches, we fo-cus on encoding-based methods, which use a clustered encoding of the target language."
"Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score."
"Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder."
"In the absence of an n-gram language model, decod-ing a synchronous CFG translation model is very efficient, requiring only a variant of the CKY al-gorithm."
"As in monolingual parsing, dynamic pro-gramming items are simply indexed by a source lan-guage span and a syntactic label."
"Complexity arises when n-gram language model scoring is added, be-cause items must now be distinguished by their ini-tial and final few target language words for purposes of later combination."
"This lexically exploded search space is a root cause of inefficiency in decoding, and several methods have been suggested to combat it."
"The approach most relevant to the current work is[REF_CITE], which begins with an ini-tial bigram pass and uses the resulting chart to guide a final trigram pass."
"Substantial speed-ups are ob-tained, but computation is still dominated by the ini-tial bigram pass."
"The key challenge is that unigram models are too poor to prune well, but bigram mod-els are already huge."
"In short, the problem is that there are too many words in the target language."
"In this paper, we propose a new, coarse-to-fine, mul-tipass approach which allows much greater speed-ups by translating into abstracted languages."
"That is, rather than beginning with a low-order model of a still-large language, we exploit language projec-tions, hierarchical clusterings of the target language, to effectively reduce the size of the target language."
"In this way, initial passes can be very quick, with complexity phased in gradually."
Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1).
"The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one."
"There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass."
We demon-strate that likelihood-based hierarchical EM train-ing[REF_CITE]and cluster-based language modeling methods[REF_CITE]are superior to both rank-based and random-projection methods.
"In addition, we demonstrate that more than two passes are beneficial and show that our computa-tion is equally distributed over all passes."
"In our experiments, passes with less than 16-cluster lan-guage models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly."
"To follow related work and to focus on the effects of the language model, we present translation re-sults under an inversion transduction grammar (ITG) translation model[REF_CITE]trained on the Eu-roparl corpus[REF_CITE], described in detail in Section 3, and using a trigram language model."
"We show that, on a range of languages, our coarse-to-fine decoding approach greatly outperforms base-line beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding."
"In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points."
This increase is a mixture of improved search and subtly advantageous coarse-to-fine effects which are further discussed below.
"In coarse-to-fine decoding, we create a series of ini-tially simple but increasingly complex search prob-lems."
"We then use the solutions of the simpler prob-lems to prune the search spaces for more complex models, reducing the total computational cost."
"Taken broadly, the coarse-to-fine approach is not new to machine translation (MT) or even syntactic MT."
"Many common decoder precomputations can be seen as coarse-to-fine methods, including the A*-like forward estimates used in the Moses decoder[REF_CITE]."
"In an ITG framework like ours,[REF_CITE]consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass."
"In their two-pass approach, the coarse bigram pass becomes computationally dominant."
Our work differs in two ways.
"First, we use posterior pruning rather than A* search."
"Unlike A* search, posterior pruning allows multipass methods."
"Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective."
"For example, in mono-lingual parsing, posterior pruning methods[REF_CITE]have led to greater speedups than their more cautious A* analogues[REF_CITE], though at the cost of guaran-teed optimality."
"Second, we focus on an orthogonal axis of ab-straction: the size of the target language."
"The in-troduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation."
"Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntac-tic MT."
"For example,[REF_CITE]con-siders a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results."
"When decoding in a syntactic translation model with an n-gram language model, search states are spec-ified by a grammar nonterminal X as well as the the n-1 left-most target side words l n−1 , . .. , l 1 and right-most target side words r 1 , . .. , r n−1 of the gen-erated hypothesis."
"We denote the resulting lexical-ized state as l n−1 , . . . , l 1 -X-r 1 , . .. , r n−1 ."
"Assum-ing a vocabulary V and grammar symbol set G, the state space size is up to |V | 2(n−1) |G|, which is im-mense for a large vocabulary when n &gt; 1."
We consider two ways to reduce the size of this search space.
"First, we can reduce the order of the lan-guage model."
"Second, we can reduce the number of words in the vocabulary."
Both can be thought of as projections of the search space to smaller ab- stracted spaces.
Figure 2 illustrates those two or-thogonal axes of abstraction.
Order-based projections are simple.
"As shown in Figure 2, they simply strip off the appropriate words from each state, collapsing dynamic program-ming items which are identical from the standpoint of their left-to-right combination in the lower or-der language model."
"However, having only order-based projections is very limiting."
"The only lower-order pass possible uses a unigram model, which provides no information about the interaction of the language model and translation model reorderings."
We there-fore propose encoding-based projections.
These projections reduce the size of the target language vo-cabulary by deterministically projecting each target language word to a word cluster.
"This projection ex-tends to the whole search state in the obvious way: assuming a bigram language model, the state l-X-r projects to c(l)-X-c(r), where c(·) is the determin-istic word-to-cluster mapping."
"In our multipass approach, we will want a se-quence c 1 . . . c n of such projections."
"This requires a hierarchical clustering of the target words, as shown in Figure 1."
Each word’s cluster membership can be represented by an n-bit binary string.
Each prefix of length k declares that word’s cluster assignment at the k-bit level.
"As we vary k, we obtain a sequence of projections c k (·), each one mapping words to a more refined clustering."
"When performing inference in a k-bit projection, we replace the detailed original language model over words with a coarse language model LM k over the k-bit word clusters."
"In addition, we replace the phrase table with a projected phrase table, which further increases the speed of projected passes."
"In Section 4, we describe the various clus-tering schemes explored, as well as how the coarse LM k are estimated."
"Unlike previous work, where the state space exists only at two levels of abstraction (i.e. bigram and tri-gram), we have multiple levels to choose from (Fig-ure 2)."
"Because we use both encoding-based and order-based projections, our options form a lattice of coarser state spaces, varying from extremely sim-ple (a bigram model with just two word clusters) to nearly the full space (a trigram model with 10 bits or 1024 word clusters)."
We use this lattice to perform a series of coarse passes with increasing complexity.
"More formally, we decode a source sentence multiple times, in a sequence of state spaces S 0 , S 1 , . . . , S n =S, where each S i is a refinement of S i−1 in either language model order, language encoding size, or both."
The state spaces S i and S j (i &lt; j) are related to each other via a projection operator π j→i (·) which maps refined states deterministically to coarser states.
We start by decoding an input x in the simplest state space S 0 .
"In particular, we compute the chart of the posterior distributions p 0 (s) = P (s|x) for all states s ∈ S 0 ."
These posteriors will be used to prune the search space S 1 of the following pass.
States s whose posterior falls below a threshold t trigger the removal of all more refined states s 0 in the subse-quent pass (see Figure 3).
"This technique is poste-rior pruning, and is different from A* methods in two main ways."
"First, it can be iterated in a multi-pass setting, and, second, it is generally more effi- cient with a potential cost of increased search errors (see Section 2.1 for more discussion)."
"Looking at Figure 2, multipass coarse-to-fine de-coding can be visualized as a walk from a coarse point somewhere in the lower left to the most re-fined point in the upper right of the grid."
Many coarse-to-fine schedules are possible.
"In practice, we might start decoding with a 1-bit word bigram pass, followed by an 3-bit word bigram pass, fol-lowed by a 5-bit word trigram pass and so on (see Section 5.3 for an empirical investigation)."
"In terms if time, we show that coarse-to-fine gives substantial speed-ups."
"There is of course an additional mem-ory requirement, but it is negligible."
As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language mod-els.
"In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams)."
"While our approach applies in principle to a vari-ety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach[REF_CITE]to facili-tate comparison with previous work[REF_CITE]as well as to focus on language model complexity."
ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules.
Preterminal unary productions produce terminal strings on both sides (words or phrases): X → e/f. Binary in-order pro-ductions combine two phrases monotonically (X → [Y Z]).
"Finally, binary inverted productions invert the order of their children (X → hY Zi)."
These pro-ductions are associated with rewrite weights in the standard way.
"Without a language model, SCFG decoding is just like (monolingual) CFG parsing."
"The dynamic pro-gramming states are specified by i X j , where hi, ji is a source sentence span and X is a nonterminal."
"The only difference is that whenever we apply a CFG production on the source side, we need to remem-ber the corresponding synchronous production on the target side and store the best obtainable transla-tion via a backpointer."
"Once we integrate an n-gram language model, the state space becomes lexicalized and combining dy-namic programming items becomes more difficult."
"Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: l n−1 , ..., l 1 - i X j -r 1 , ..., r n−1 ."
"Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporat-ing the score of any language model features which cross the target side boundaries of the two concate-nated items[REF_CITE]."
"Decoding with an in-tegrated language model is computationally expen-sive for two reasons: (1) the need to keep track of a large number of lexicalized hypotheses for each source span, and (2) the need to frequently query the large language model for each hypothesis combina-tion."
Multipass coarse-to-fine decoding can alleviate both computational issues.
"We start by decoding in an extremely coarse bigram search space, where there are very few possible translations."
"We com-pute standard inside/outside probabilities (iS/oS), as follows."
"Consider the application of non-inverted binary rule: we combine two items l b - i B k -r b and l c - k C j -r c spanning hi, ki and hk, ji respectively to form a larger item l b - i"
"A j -r c , spanning hi,ji."
The += + l b - i
A j -r c l b - i B k -r b l c - k C j -r c l b r c l b r b l c r c iS(l b - i
"A j -r c ) += p(X→[Y Z]) ·iS(l b - i B k -r b ) ·LM(r b ,l c ) · iS(l c - k C j -r c ) inside score of the new item is incremented by: iS(l b - i"
A j -r c ) += p(X → [Y Z]) · iS(l b - i B k -r b ) · iS(l c - k C j -r c ) ·
"LM(r b , l c )"
This process is also illustrated in Figure 4.
"Of course, we also loop over the split point k and ap-ply the other two rule types (inverted concatenation, terminal generation)."
"We omit those cases from this exposition, as well as the update for the outside pass; they are standard and similar."
"Once we have com-puted the inside and outside scores, we compute pos-terior probabilities for all items: iS(l a - i A j -r a )oS(l a - i A j -r a ) p(l a - i A j -r a ) = iS(root) where iS(root) is sum of all translations’ scores."
States with low posteriors are then pruned away.
"We proceed to compute inside/outside score in the next, more refined search space, using the projec-tions π i→i−1 to map between states in S i and S i−1 ."
"In each pass, we skip all items whose projection into the previous stage had a probability below a stage-specific threshold."
This process is illustrated in Fig-ure 3.
"When we reach the most refined search space S ∞ , we do not prune, but rather extract the Viterbi derivation instead. [Footnote_1]"
"1 Other final decoding strategies are possible, of course, in-cluding variational methods and minimum-risk methods[REF_CITE]."
Central to our encoding-based projections (see Sec-tion 2.2) are hierarchical clusterings of the tar-get language vocabulary.
"In the present work, these clusterings are each k-bit encodings and yield sequences of coarse language models LM k and phrasetables PT k ."
"Given a hierarchical clustering, we estimate the corresponding LM k from a corpus obtained by re-placing each token in a target language corpus with the appropriate word cluster."
"As with our original refined language model, we estimate each coarse language model using the SRILM toolkit[REF_CITE]."
The phrasetables PT k are similarly estimated by replacing the words on the target side of each phrase pair with the corresponding cluster.
This pro-cedure can potentially map two distinct phrase pairs to the same coarse translation.
In such cases we keep only one coarse phrase pair and sum the scores of the colliding originals.
There are many possible schemes for creating hi-erarchical clusterings.
"Here, we consider several di-visive clustering methods, where coarse word clus-ters are recursively split into smaller subclusters."
The simplest approach to splitting a cluster is to ran-domly assign each word type to one of two new sub-clusters.
"Random projections have been shown to be a good and computationally inexpensive dimension-ality reduction technique, especially for high dimen-sional data[REF_CITE]."
"Although our best performance does not come from random projections, we still obtain substantial speed-ups over a single pass fine decoder when using random projections in coarse passes."
"In frequency clustering, we allocate words to clus-ters by frequency."
"At each level, the most frequent words go into one cluster and the rarest words go into another one."
"Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass."
This ap-proach can be seen as a radically simplified version[REF_CITE].
"It can, and does, result in highly imbalanced cluster hierarchies."
An approach found to be effective[REF_CITE]for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.
We adopt this approach here by identifying each clus-ter with a latent state in an HMM and determiniz-ing the emissions so that each word type is emitted by only one state.
"When splitting a cluster s into s 1 and s 2 , we initially clone and mildly perturb its corresponding state."
"We then use EM to learn pa-rameters, which splits the state, and determinize the result."
"Specifically, each word w is assigned to s 1 if P(w|s 1 ) &gt; P(w|s 2 ) and s [Footnote_2] otherwise."
2 The software for this clustering technique is available[URL_CITE]
"Because of this determinization after each round of EM, a word in one cluster will be allocated to exactly one of that cluster’s children."
"This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed[REF_CITE]."
"Be-cause the emissions are sparse, learning is very effi-cient."
An example of some of the words associated with early splits can be seen in Figure 1.
This is accomplished by incrementally swapping words between clusters to locally mini-mize entropy. [Footnote_2]
2 The software for this clustering technique is available[URL_CITE]
"This clustering algorithm was devel-oped with a slightly different application in mind, but fits very well into our framework, because the hierarchical clusters it produces are trained to maxi-mize predictive likelihood."
We applied the above clustering algorithms to our monolingual language model data to obtain hierar- chical clusters.
We then trained coarse language models of varying granularity and evaluated them on a held-out set.
To measure the quality of the coarse language models we use perplexity (exponentiated cross-entropy). [Footnote_3] Figure 5 shows that HMM clus-tering and JClustering have lower perplexity than frequency and random based clustering for all com-plexities.
3 We assumed that each cluster had a uniform distribution over all the words in that cluster.
In the next section we will present a set of machine translation experiments using these coarse language models; the clusterings with better per-plexities generally produce better decoders.
"We ran our experiments on the Europarl corpus[REF_CITE]and show results on Spanish, French and German to English translation."
We used the setup and preprocessing steps detailed in the 2008 Workshop on Statistical Machine Translation. [Footnote_4] Our baseline decoder uses an ITG with an integrated tri-gram language model.
Phrase translation parame-ters are learned from parallel corpora with approx-imately 8.5 million words for each of the language pairs.
The English language model is trained on the entire corpus of English parliamentary proceedings provided with the Europarl distribution.
We report results on the 2000 development test set sentences of length up to 126 words (average length was 30 words).
Our ITG translation model is broadly competitive with state-of-the-art phrase-based-models trained on the same data.
"For example, on the Europarl devel-opment test set, we fall short of Moses[REF_CITE]by less than one BLEU point."
"On Spanish-English we get 29.47 BLEU (compared to Moses’s 30.40),[REF_CITE].34 (vs. 29.95), and 23.80 (vs. 24.64) on German-English."
These differ-ences can be attributed primarily to the substantially richer distortion model used by Moses.
The multipass coarse-to-fine architecture that we have introduced presents many choice points.
"In the following, we investigate various axes individu-ally."
"We present our findings as BLEU-to-time plots, where the tradeoffs were generated by varying the complexity and the number of coarse passes, as well as the pruning thresholds and beam sizes."
"Unless otherwise noted, the experiments are on Spanish-English using trigram language models."
"When different decoder settings are applied to the same model, MERT weights[REF_CITE]from the unpro-jected single pass setup are used and are kept con-stant across runs."
"In particular, the same MERT weights are used for all coarse passes; note that this slightly disadvantages the multipass runs, which use MERT weights optimized for the single pass de-coder."
"In section Section 4, HMM clustering and JCluster-ing gave lower perplexities than frequency and ran-dom clustering when using the same number of bits for encoding the language model."
"To test how these models perform at pruning, we ran our decoder sev-eral times, varying only the clustering source."
"In each case, we used a 2-bit trigram model as a sin-gle coarse pass, followed by a fine output pass."
Fig-ure 6 shows that we can obtain significant improve-ments over the single-pass baseline regardless of the clustering.
"To no great surprise, HMM clustering and JClustering yield better results, giving a 30-fold speed-up at the same accuracy, or improvements of about 0.3 BLEU when given the same time as the single pass decoder."
We discuss this increase in ac-curacy over the baseline in Section 5.5.
"Since the performance differences between those two cluster-ing algorithms are negligible, we will use the sim-pler HMM clustering in all subsequent experiments."
"Given a hierarchy of coarse language models, all trigam for the moment, we need to decide on the number of passes and the granularity of the coarse language models used in each pass."
Figure 7 shows how decoding time varies for different multipass schemes to achieve the same translation quality.
A single coarse pass with a 4-bit language model cuts decoding time almost in half.
"However, one can further cut decoding time by starting with even coarser language models."
"In fact, the best results are achieved by decoding in sequence with 1-, 2-and 3-bit language models before running the final fine trigram pass."
"Interestingly, in this setting, each pass takes about the same amount of time."
"A simi-lar observation was reported in the parsing literature, where coarse-to-fine inference with multiple passes of roughly equal complexity produces tremendous speed-ups[REF_CITE]."
"As described in Section 2, the language model com-plexity can be reduced either by decreasing the vo-cabulary size (encoding-based projection) or by low-ering the language model order from trigram to bi-gram (order-based projection)."
Figure 7 shows that both approaches alone yield comparable improve-ments over the single pass baseline.
"Fortunately, the two approaches are complimentary, allowing us to obtain further improvements by combining both."
"We found it best to first do a series of coarse bigram passes, followed by a fine bigram pass, followed by a fine trigram pass."
Figure 9 compares our multipass coarse-to-fine de-coder using language refinement to single pass de-coding on three different languages.
On each lan-guage we get significant improvements in terms of efficiency as well as accuracy.
"Overall, we can achieve up to 50-fold speed-ups at the same accu-racy, or alternatively, improvements of 0.4 BLEU points over the best single pass run."
"In absolute terms, our decoder translates on aver-age about two Spanish sentences per second at the highest accuracy setting. [Footnote_5]"
"5 Of course, the time for an average sentence is much lower, since long sentences dominate the overall translation time."
"This compares favorably to the Moses decoder[REF_CITE], which takes almost three seconds per sentence."
"In multipass coarse-to-fine decoding, we noticed that in addition to computational savings, BLEU scores tend to improve."
"A first hypothesis is that coarse-to-fine decoding simply improves search quality, where fewer good items fall off the beam compared to a simple fine pass."
"However, this hy-pothesis turns out to be incorrect."
Table 1 shows the percentage of test sentences for which the BLEU score or log-likelihood changes when we switch from single pass decoding to coarse-to-fine multi-pass decoding.
"For the rest, coarse-to-fine decoding mostly finds translations with lower likeli-hood, but higher BLEU score, than single pass de-coding. [Footnote_6] An increase of the underlying objectives of interest when pruning despite an increase in model-score search errors has also been observed in mono-lingual coarse-to-fine syntactic parsing[REF_CITE]."
6 We compared the influence of multipass decoding on the TM score and the LM score; both decrease.
This effect may be because coarse-to-fine approximates certain min-imum Bayes risk objective.
It may also be an effect of model intersection between the various passes’ models.
"In any case, both possibilities are often per-fectly desirable."
It is also worth noting that the num-ber of search errors incurred in the coarse-to-fine approach can be dramatically reduced (at the cost of decoding time) by increasing the pruning thresh-olds.
"However, the fortuitous nature of coarse-to-fine search errors seems to be a substantial and de-sirable effect."
We have presented a coarse-to-fine syntactic de-coder which utilizes a novel encoding-based lan-guage projection in conjunction with order-based projections to achieve substantial speed-ups.
"Un-like A* methods, a posterior pruning approach al-lows multiple passes, which we found to be very beneficial for total decoding time."
"When aggres-sively pruned, coarse-to-fine decoding can incur ad-ditional search errors, but we found those errors to be fortuitous more often than harmful."
"Our frame-work applies equally well to other translation sys-tems, though of course interesting new challenges arise when, for example, the underlying SCFGs be-come more complex."
"In this paper, we present a novel method based on CRFs in response to the two special characteristics of “contextual dependency” and “label redundancy” in sentence sentiment classification."
We try to capture the contextual constraints on sentence sentiment using CRFs.
"Through introducing redundant labels into the original sentimental label set and organizing all labels into a hierarchy, our method can add redundant features into training for capturing the label redundancy."
"The experimental results prove that our method outperforms the traditional methods like NB, SVM, MaxEnt and standard chain CRFs."
"In comparison with the cascaded model, our method can effectively alleviate the error propagation among different layers and obtain better performance in each layer."
"There are a lot of subjective texts in the web, such as product reviews, movie reviews, news, editorials and blogs, etc."
"Extracting these subjective texts and analyzing their orientations play significant roles in many applications such as electronic commercial, etc."
One of the most important tasks in this field is sentiment *
"Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn classification, which can be performed in several levels: word level, sentence level, passage level, etc."
This paper focuses on sentence level sentiment classification.
"Commonly, sentiment classification contains three layers of sub-tasks."
"From upper to lower, (1) Subjective/Objective classification: the subjective texts are extracted from the corpus teeming with both subjective and objective texts. (2) Polarity classification: a subjective text is classified into “positive” or “negative” according to the sentimental expressions in the text. (3) Sentimental strength rating: a subjective text is classified into several grades which reflect the polarity degree of “positive” or “negative”."
"It is a special multi-class classification problem, where the classes are ordered."
"In machine learning, this kind of problem is also regarded as an ordinal regression problem[REF_CITE]."
"In this paper, we mainly focus on this problem in sentiment classification."
Sentiment classification in sentence level has its special characteristics compared with traditional text classification tasks.
"Firstly, the sentiment of each sentence in a discourse is not independent to each other."
"In other words, the sentiment of each sentence is related to those of other adjacent sentences in the same discourse."
The sentiment of a sentence may vary in different contexts.
"If we detach a sentence from the context, its sentiment may not be inferred correctly."
"Secondly, there is redundancy among the sentiment classes, especially in sentimental strength classes."
For example: “I love the scenario of “No country for old man” very much!!” “This movie sounds good.”
The first sentence is labeled as “highly praised” class and the second one is labeled as “something good” class.
"Both the sentences express positive sentiment for the movie, but the former expresses stronger emotion than the latter."
"We can see that both “highly praised” and “something good” belong to an implicit class “positive”, which can be regarded as the relation between them."
"If we add these implicit classes in the label set, the sentiment classes will form a hierarchical structure."
"For example, “positive” can be regarded as the parent class of “highly praised” and “something good”, “subjective” can be regarded as the parent class of “positive” and “negative”."
This implicit hierarchical structure among labels should not be neglected because it may be beneficial for improving the accuracy of sentiment classification.
"In the paper, we call this characteristic of sentiment classification as “label redundancy”."
"Unfortunately, in our knowledge most of the current research treats sentiment classification as a traditional multi-classification task or an ordinal regression task, which regard the sentimental classes being independent to each other and each sentence is also independent to the adjacent sentences in the context."
"In other words, they neglect the contextual information and the redundancy among sentiment classes."
"In order to consider the contextual information in the process of the sentence sentiment classification, some research defines contextual features and some uses special graph-based formulation,[REF_CITE]."
"In order to consider the label redundancy, one potential solution is to use a cascaded framework which can combine subjective/objective classification, polarity classification and sentimental strength classification together, where the classification results of the preceding step will be the input of the subsequent one."
"However, the subsequent classification cannot provide constraint and correction to the results of the preceding step, which will lead to the accumulation and propagation of the classification errors."
"As a result, the performance of sentiment analysis of sentences is often not satisfactory."
This paper focuses on the above two special characteristics of the sentiment classification problem in the sentence level.
"To the first characteristic, we regard the sentiment classification as a sequence labeling problem and use conditional random field (CRFs) model to capture the relation between two adjacent sentences in the context."
"To the second characteristic, we propose a novel method based on a CRF model, in which the original task is mapped to a classification on a hierarchical structure, which is formed by the original label set and some additional implicit labels."
"In the hierarchical classification framework, the relations between the labels can be represented as the additional features in classification."
"Because these features are related to the original labels but unobserved, we name them as “redundant features” in this paper."
They can be used to capture the redundant and hierarchical relation between different sentiment classes.
"In this way, not only the performance of sentimental strength rating is improved, the accuracies of subjective/objective classification and polarity classification are also improved compared with the traditional sentiment classification method."
"And in comparison with the cascaded method, the proposed approach can effectively alleviate error propagation."
The experimental results on movie reviews prove the validity of our method.
"For capturing the influence of the contexts to the sentiment of a sentence, we treat original sentiment classification as a sequence labeling problem."
"We regard the sentiments of all the sentences throughout a paragraph as a sequential flow of sentiments, and we model it using a conditional model."
"In this paper, we choose Conditional Random Fields (CRFs)[REF_CITE]because it has better performance than other sequence labeling tools in most NLP applications."
CRFs are undirected graphical models used to calculate the conditional probability of a set of labels given a set of input variables.
We cite the definitions of CRFs[REF_CITE].
"It defines the conditional probability proportional to the product of potential functions on cliques of the graph, exp λ ⋅ F(Y, X ) P λ (Y | X ) = (1) Z(X ) where X is a set of input random variables and Y is a set of random labels."
"F(Y,X) is an arbitrary feature function over its arguments, λ is a learned weight for each feature function and Z (X ) = ∑ exp( λ ⋅ F(Y, X )) . y"
The training of CRFs is based on Maximum Likelihood Principle[REF_CITE].
The log likelihood function is
"L( λ ) = ∑ [ λ ⋅ F(Y k , X k ) − log Z λ (X k ) ] k"
"Therefore, Limited-memory BFGS (L-BFGS) algorithm is used to find this nonlinear optimization parameters."
"In this section, we explain the “label redundancy” in sentiment classification mentioned in the first section."
We will analyze the effect of the label redundancy on the performance of sentiment classification from the experimental view.
We conduct the experiments of polarity classification and sentimental strength rating on the corpus which will be introduced in section 5 later.
The class set is also illustrated in that section.
"Polarity classification is a three-class classification process, and sentimental strength rating is a five-class classification process."
"We use first 200 reviews as the training set which contains 6,079 sentences, and other 49 reviews, totally 1,531 sentences, are used as the testing set."
Both the three-class classification and the five-class classification use standard CRFs model with the same feature set.
"The results are shown in Table 1, 2 and 3, where “Answer” denotes the results given CRFs model ， “Correct” denotes the number of by human, “Results” denotes the results given by correct samples which is labeled by CRFs model."
"We use precision, recall and F1 value as the evaluation metrics."
Table 1 gives the result of sentimental strength rating.
Table 2 shows the polarity classification results extracted from the results of sentimental strength rating in Table 1.
The extraction process is as follows.
"In the sentimental strength rating results, we combine the sentences with “PP” class and the sentences with “P” class into “Pos” class, and the sentences with “NN” class and the sentences with “N” class into “Neg” class."
So the results of five-class classification are transformed into the results of three-class classification.
Table 3 is the results of performing polarity classification in the data set by CRFs directly.
"From the results we can find the following phenomena. (1) The corpus is severely unbalanced, the objective sentences take the absolute majority in the corpus, which leads to the poor accuracy for classifying subjective sentences."
"The experiment in Table 1 puts polarity classification and sentimental strength rating under a unique CRFs model, without considering the redundancy and hierarchical structure between different classes."
"As a result, the features for polarity classification will usually cover the features for sentimental strength rating."
"These reasons can explain why there is only one sample labeled as “NN” correctly and only 5 samples labeled as “PP” correctly. (2) Comparing Table 2 with 3, we can find that, the F1 value of the polarity classification results extracted from sentimental strength rating results is lower than that of directly conducting polarity classification."
That is because the redundancy between sentimental strength labels makes the classifier confused to determine the polarity of the sentence.
"Therefore, we should deal with the sentiment analysis in a hierarchical frame which can consider the redundancy between the different classes and make full use of the subjective and polarity information implicitly contained in sentimental strength classes."
"As mentioned above, it’s important for a classifier to consider the redundancy between different labels."
"However, from the standard CRFs described in formula (1), we can see that the training of CRFs only maximizes the probabilities of the observed labels Y in the training corpus."
"Actually, the redundant relation between sentiment labels is unobserved."
The standard CRFs still treats each class as an isolated item so that its performance is not satisfied.
"In this section, we propose a novel method for sentiment classification, which can capture the redundant relation between sentiment labels through adding redundant features."
"In the following, we firstly show how to add these redundant features, then illustrate the characteristics of this method."
"After that, for the sentiment analysis task, the process of feature generation will be presented."
Adding redundant features has two steps.
"Firstly, an implicit redundant label set is designed, which can form a multi-layer hierarchical structure together with the original labels."
"Secondly, in the hierarchical classification framework, the implicit labels, which reflect the relations between the original labels, can be used as redundant features in the training process."
We will use the following example to illustrate the first step for sentimental strength rating task.
"For the task of sentimental strength rating, the original label set is {“PP (highly praised)”, “P (something good)”, “Neu (objective description)”, “N (something that needs improvement)” and “NN (strong aversion)”}."
"In order to introduce redundant labels, the 5-class classification task is decomposed into the following three layers shown in Figure 1."
"The label set in the first layer is {“subjective”, “objective”}, The label set in the second layer is for polarity classification {“positive”, “objective”, “negative”}, and the label set in the third layer is the original set."
"Actually, the labels in the first and second layers are unobserved redundant labels, which will not be reflected in the final classification result obviously."
"In the second step, with these redundant labels, some implicit features can be generated for CRFs."
"So the standard CRFs can be rewritten as follows. where T = (Y 1 , Y 2 ,... Y j ..., Y m ) , and Y j denotes the label sequence in the j th layer."
"F (X,Y ) denotes j j the arbitrary feature function in the j th layer."
"From the formula (2), we can see that the original label set is rewritten as T = (Y 1 , Y 2 ,..."
"Y j ..., Y m ) , which contains implicit labels in the hierarchical structure shown in Figure 1."
The difference between our method and the standard chain CRFs is that we make some implicit redundant features to be active when training.
"The original feature function F(Y,X) is replaced by m ∑ F j (X ,Y j ) ."
We use an example to illustrate the j=1 process of feature generation.
"When a sentence including the word “good” is labeled as “PP”, our model not only generate the state feature (good, “PP”), but also two implicit redundant state feature (good, “positive”) and (good, “subjective”)."
"Through adding larger-granularity labels “positive” and “negative” into the model, our method can increase the probability of “positive” and decrease the probability of “negative”."
"Furthermore, “P” and “PP” will share the probability gain of “positive”, therefore the probability of “P” will be larger than that of “N”."
"For the transition feature, the same strategy is used."
"Therefore the complexity of its m training procedure is O(M × N × ∑ F j × l) where M j is the number of the training samples, N is the average sentence length, F j is the average number of activated features in the j th layer, l is the number of the original labels and m is the number of the layers."
"For the complexity of the decoding m procedure, our method has O ( N × ∑ F j × l ) . j"
"It’s worth noting that, (1) transition features are extracted in each layer separately rather than across different layers."
"For example, feature (good, “subjective”, “positive”) will never be extracted because “subjective” and “positive” are from different layers; (2) if one sentence is labeled as “Neu”, no implicit redundant features will be generated."
Our method allows that the label sets are dependent and redundant.
"As a result, it can improve the performance of not only the classifier for the original sentimental strength rating task, but also the classifiers for other tasks in the hierarchical frame, i.e. polarity classification and subjective/objective classification."
"This kind of dependency and redundancy can lead to two characteristics of the proposed method for sentiment classification compared with traditional methods, such as the cascaded method. (1) Error-correction: Two dependent tasks in the neighboring layers can correct the errors of each other relying on the inconsistent redundant information."
"For example, if in the first layer, the features activated by “objective” get larger scores than the features activated by “subjective”, and in the second layer the features activated by “positive” get larger scores than the features activated by “objective”, then inconsistency emerges."
"At this time, our method can globally select the label with maximum probability."
"This characteristic can make up the deficiency of the cascaded method which may induce error propagation. (2) Differentiating the ordinal relation among sentiment labels: Our method organizes the ordinal sentiment labels into a hierarchy through introducing redundant labels into standard chain CRFs, in this way the degree of classification errors can be controlled."
"In the different layers of sentiment analysis task, the granularities of classification are different."
"Therefore, when an observation cannot be correctly labeled on a smaller-granularity label set, our method will use the larger-granularity labels in the upper layer to control the final classification labels."
"For feature selection, our method selects different features for each layer in the hierarchical frame."
"In the top layer of the frame shown in Figure 1, for subjective/objective classification task, we use not only adjectives and the verbs which contain subjective information (e.g., “believe”, “think”) as the features, but also the topic words."
The topic words are defined as the nouns or noun phases which frequently appear in the corpus.
We believe that some topic words contain subjective information.
"In the middle and bottom layers, we not only use the features in the first layer, but also some special features as follows. (1) The prior orientation scores of the sentiment words: Firstly, a sentiment lexicon is generated by extending the synonymies and antonyms in WordNet [URL_CITE] from a positive and negative seed list."
"Then, the positive score and the negative score of a sentiment word are individually accumulated and weighted according to the polarity of its synonymies and antonyms."
"At last we scale the normalized distance of the two scores into 5 levels, which will be the prior orientation of the word."
"When there is a negative word, like {not, no, can’t, merely, never, …}, occurring nearby the feature word in the range of 3 words size window, the orientation of this word will be reversed and “NO” will be added in front of the original feature word for creating a new feature word. (2) Sentence transition features: We consider two types of sentence transition features."
The first type is the conjunctions and the adverbs occurring in the beginning of this sentence.
"These conjunctions and adverbs are included in a word list which is manually selected, like {and, or, but, though, however, generally, contrarily, …}."
The second type of the sentence transition feature is the position of the sentence in one review.
"The reason lies in that: the reviewers often follow some writing patterns, for example some reviewers prefer to concede an opposite factor before expressing his/her real sentiment."
"Therefore, we divide a review into five parts, and assign each sentence with the serial number of the part which the sentence belongs to."
"In order to evaluate the performance of our method, we conducted experiments on a sentence level annotation corpus obtained from Purdue University, which is also used[REF_CITE]."
"This corpus contains 249 movie reviews and 7,610 sentences totally, which is randomly selected from the Cornell sentence polarity dataset v1.0."
"Each sentence was hand-labeled with one of five classes: PP (highly praised), P (something good), Neu (objective description), N (something that needs improvement) and NN (strong aversion), which contained the orientation polarity of each sentence."
"Based on the [URL_CITE]-class manually labeled results mentioned above, we also assigned each sentence with one of three classes:"
"Pos (positive polarity), Neu (objective description), Neg (negative polarity)."
Data statistics for the corpus are given in Table 4.
There is a problem in the dataset that more than 70% of the sentences are labeled as “Neu” and labels are seriously unbalanced.
"As a result, the “Neu” label is over-emphasized."
"For this problem,[REF_CITE]made a balanced data set (equal number sentences for different labels) which is sampled in the original corpus."
"Since randomly sampling sentences from the original corpus will break the intrinsic relationship between two adjacent sentences in the context, we don’t create balanced label data set."
"For the evaluation of our method, we choose accuracy as the evaluation metrics and some classical methods as the baselines."
"They are Naïve Bayes (NB), Support Vector Machine (SVM), Maximum Entropy (MaxEnt)[REF_CITE]and standard chain CRFs[REF_CITE]."
We also regard cascaded-CRFs as our baseline for comparing our method with the cascaded-based method.
"For NB, we use Laplace smoothing method."
"For SVM, we use the LibSVM [URL_CITE] with a linear kernel function [URL_CITE] ."
"For MaxEnt, we use the implementation in the toolkit Mallet 5 ."
"For CRFs, we use the implementation in Flex-CRFs 6 ."
We set the iteration number to 120 in the training process of the method based on CRFs.
"In the cascaded model we set 3 layers for sentimental strength rating, where the first layer is subjective/objective classification, the second layer is polarity classification and the last layer is sentimental strength classification."
The upper layer passes the results as the input to the next layer.
"In the first experiment, we evaluate the performance of our method for sentimental strength rating."
Experimental results for each method are given in Table 5.
"We not only give the overall accuracy of each method, but also the performance for each sentimental strength label."
"All baselines use the same feature space mentioned in section 4.3, which combine all the features in the three layers together, except cascaded CRFs and our method."
"In cascaded-CRFs and our method, we use different features in different layers mentioned in section 4.3."
These results were gathered using 5-fold cross validation with one fold for testing and the other 4 folds for training.
"From the results, we can obtain the following conclusions. (1) The three versions of CRFs perform consistently better than Naïve Bayes,"
SVM and MaxEnt methods.
"We think that is because CRFs model considers the contextual influence of each sentence. (2) Comparing the performance of cascaded CRFs with that of standard sequence CRFs, we can see that not only the overall accuracy but also the accuracy for each sentimental strength label are improved, where the overall accuracy is increased by 3%."
It proves that taking the hierarchical relationship between labels into account is very essential for sentiment classification.
"The reason is that: the cascaded model performs sentimental strength rating in three hierarchical layers, while standard chain CRFs model treats each label as an independent individual."
So the performance of the cascaded model is superior to the standard chain CRFs. (3) The experimental results also show that our method performs better than the Cascaded CRFs.
The classification accuracy is improved from 71.53% to 75.21%.
"We think that is because our method adds the label redundancy among the sentimental strength labels into consideration through adding redundant features into the feature sets, and the three subtasks in the cascaded model are merged into a unified model."
So the output result is a global optimal result.
"In this way, the problem of error propagation in the cascaded frame can be alleviated."
"In the second experiment, we evaluate the performance of our method for sentiment polarity classification."
"Our method is based on a hierarchical frame, which can perform different tasks in different layers at the same time."
"For example, it can determine the polarity of sentences when sentimental strength rating is performed."
"Here, the polarity classification results of our method are extracted from the results of the sentimental strength rating mentioned above."
"In the sentimental strength rating results, we combine the sentences with PP label and the sentences with P label into one set, and the sentences with NN label and the sentences with N label into one set."
So the results of 5-class classification are transformed into the results of 3-class classification.
"Other methods like NB, SVM, MaxEnt, standard chain CRFs perform 3-class classification directly, and their label sets in the training corpus is {Pos, Neu, Neg}."
The parameter setting is the same as sentimental strength rating.
"For the cascaded-CRFs method, we firstly perform subjective/objective classification, and then determine the polarity of the sentences based on the subjective sentences."
The experimental results are given in Table 6.
"From the experimental results, we can obtain the following conclusion for sentiment polarity classification, which is similar to the conclusion for sentimental strength rating mentioned in section 5.2."
"That is both our model and the cascaded model can get better performance than other traditional methods, such as NB, SVM, MaxEnt, etc."
But the performance of the cascaded CRFs (76.94%) is lower than that of our method (78.55%).
"This indicates that because our method exploits the label redundancy in the different layers, it can increase the accuracies of both polarity classification and sentimental strength rating at the same time compared with other methods."
"In the last experiment, we test our method for subjective/objective classification."
The subjective/objective label of the data is extracted from its original label like section 5.3.
"As the same as the experiment for polarity classification, all baselines perform subjective/objective classification directly."
It’s no need to perform the cascaded-based method because it’s a 2-class task.
The results of our method are extracted from the results of the sentimental strength rating too.
The results are shown in Table 7.
"From it, we can obtain the similar conclusion, i.e. our method outperforms other methods and has the 80.18% classification accuracy."
"Our method, which introduces redundant features into training, can increase the accuracies of all tasks in the different layers at the same time compared with other baselines."
It proves that considering label redundancy are effective for promoting the performance of a sentimental classifier.
"Recently, many researchers have devoted into the problem of the sentiment classification."
"Most of researchers focus on how to extract useful textual features (lexical, syntactic, punctuation, etc.) for determining the semantic orientation of the sentences using machine learning algorithm ([REF_CITE])."
But fewer researchers deal with this problem using CRFs model.
"For identifying the subjective sentences, there are several research,[REF_CITE]."
"For polarity classification on sentence level,[REF_CITE]judged the sentiment by classifying a pseudo document composed of synonyms of indicators in one sentence.[REF_CITE]proposed a semi-supervised machine learning method based on subjectivity detection and minimum-cut in graph."
Cascaded models for sentiment classification were studied[REF_CITE].
Their work mainly used the cascaded frame for determining the orientation of a document and the sentences.
"In that work, an initial model is used to determine the orientation of each sentence firstly, then the top subjective sentences are input into a document -level model to determine the document’s orientation."
The CRFs has previously been used for sentiment classification.
Those methods based on CRFs are related to our work.[REF_CITE]used a sequential CRFs regression model to measure the polarity of a sentence in order to determine the sentiment flow of the authors in reviews.
"However, this method must manually select a word set for constraints, where each selected word achieved the highest correlation with the sentiment."
The performance of isotonic CRFs is strongly related to the selected word set.[REF_CITE]proposed a structured model based on CRFs for jointly classifying the sentiment of text at varying levels of granularity.
They put the sentence level and document level sentiment analysis in an integrated model and employ the orientation of the document to influence the decision of sentence’s orientation.
Both the above two methods didn’t consider the redundant and hierarchical relation between sentimental strength labels.
So their methods cannot get better results for the problem mentioned in this paper.
"Another solution to this problem is to use a joint multi-layer model, such as dynamic CRFs, multi-layer CRFs, etc."
Such kind of models can treat the three sub-tasks in sentiment classification as a multi-task problem and can use a multi-layer or hierarchical undirected graphic to model the sentiment of sentences.
The main difference between our method and theirs is that we consider the problem from the feature representation view.
Our method expands the feature set according to the number of layers in the hierarchical frame.
"So the complexity of its decoding procedure is lower than theirs, for example the complexity of the multi-layer CRFs is O ( N × F × ∏ l j ) when j decoding and our method only has O ( N × ∑ F j × l ) , j where N is the average sentence length, F j is the average number of activated features in the j th layer, l is the number of the original labels."
"In the paper, we propose a novel method for sentiment classification based on CRFs in response to the two special characteristics of “contextual dependency” and “label redundancy” in sentence sentiment classification."
We try to capture the contextual constraints on the sentence sentiment using CRFs.
"For capturing the label redundancy among sentiment classes, we generate a hierarchical framework through introducing redundant labels, under which redundant features can be introduced."
"The experimental results prove that our method outperforms the traditional methods (like NB, SVM, ME and standard chain CRFs)."
"In comparison with cascaded CRFs, our method can effectively alleviate error propagation among different layers and obtain better performance in each layer."
"For our future work, we will explore other hierarchical models for sentimental strength rating because the experiments presented in this paper prove this hierarchical frame is effective for ordinal regression."
"We would expand the idea in this paper into other models, such as Semi-CRFs and Hierarchical-CRFs."
"Although research in other languages is in-creasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic re-sources and tools that are available for this lan-guage."
"In this paper, we propose and evalu-ate methods that can be employed to transfer a repository of subjectivity resources across lan-guages."
"Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate re-sources for subjectivity analysis in other lan-guages."
"Through comparative evaluations on two different languages (Romanian and Span-ish), we show that automatic translation is a viable alternative for the construction of re-sources and tools for subjectivity analysis in a new target language."
"We have seen a surge in interest towards the ap-plication of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity)."
"A large number of text process-ing applications have already employed techniques for automatic subjectivity analysis, including auto-matic expressive text-to-speech synthesis[REF_CITE], text semantic analysis[REF_CITE], tracking sen-timent timelines in on-line forums and news[REF_CITE], mining opinions from product reviews[REF_CITE], and ques-tion answering[REF_CITE]."
"A significant fraction of the research work to date in subjectivity analysis has been applied to English, which led to several resources and tools available for this language."
"In this paper, we explore multiple paths that employ machine translation while lever-aging on the resources and tools available for En-glish, to automatically generate resources for sub-jectivity analysis for a new target language."
"Through experiments carried out with automatic translation and cross-lingual projections of subjectivity annota-tions, we try to answer the following questions."
"First, assuming an English corpus manually an-notated for subjectivity, can we use machine trans-lation to generate a subjectivity-annotated corpus in the target language?"
"Second, assuming the availabil-ity of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for sub-jectivity in the target language by using automatic subjectivity annotations of English text and machine translation?"
"Finally, third, can these automatically generated resources be used to effectively train tools for subjectivity analysis in the target language?"
"Since our methods are particularly useful for lan-guages with only a few electronic tools and re-sources, we chose to conduct our initial experiments on Romanian, a language with limited text process-ing resources developed to date."
"Furthermore, to validate our results, we carried a second set of ex-periments on Spanish."
"Note however that our meth-ods do not make use of any target language specific knowledge, and thus they are applicable to any other language as long as a machine translation engine ex-ists between the selected language and English."
"Research in sentiment and subjectivity analysis has received increasingly growing interest from the nat-ural language processing community, particularly motivated by the widespread need for opinion-based applications, including product and movie reviews, entity tracking and analysis, opinion summarization, and others."
"Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used[REF_CITE], Chinese data are used[REF_CITE], and German data are used[REF_CITE]."
"In addition, several participants in the Chinese and Japanese Opinion Extraction tasks of NTCIR-6[REF_CITE]performed subjectivity and sentiment analysis in languages other than En-glish."
"In general, efforts on building subjectivity analy-sis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language."
"To address this gap, we focus on leveraging resources already developed for one language to derive subjectivity analysis tools for a new language."
"This motivates the direction of our research, in which we use machine translation coupled with cross-lingual annotation projections to generate the resources and tools required to perform subjectivity classification in the target language."
"The work closest to ours is the one reported[REF_CITE], where a bilingual lexicon and a manually translated parallel text are used to generate the resources required to build a subjectiv-ity classifier in a new language."
"In that work, we found that the projection of annotations across par-allel texts can be successfully used to build a cor-pus annotated for subjectivity in the target language."
"However, parallel texts are not always available for a given language pair."
"Therefore, in this paper we explore a different approach where, instead of rely-ing on manually translated parallel corpora, we use machine translation to produce a corpus in the new language."
We explore the possibility of using machine transla-tion to generate the resources required to build sub-jectivity annotation tools in a given target language.
We focus on two main scenarios.
"First, assuming a corpus manually annotated for subjectivity exists in the source language, we can use machine translation to create a corpus annotated for subjectivity in the target language."
"Second, assuming a tool for auto-matic subjectivity analysis exists in the source lan-guage, we can use this tool together with machine translation to create a corpus annotated for subjec-tivity in the target language."
"In order to perform a comprehensive investiga-tion, we propose three experiments as described be-low."
"The first scenario, based on a corpus manu-ally annotated for subjectivity, is exemplified by the first experiment."
"The second scenario, based on a corpus automatically annotated with a tool for sub-jectivity analysis, is subsequently divided into two experiments depending on the direction of the trans-lation and on the dataset that is translated."
"In all three experiments, we use English as a source language, given that it has both a corpus man-ually annotated for subjectivity (MPQA[REF_CITE]) and a tool for subjectivity analysis (Opin-ionFinder[REF_CITE])."
"In this experiment, we use a corpus in the source language manually annotated for subjectivity."
"The corpus is automatically translated into the target lan-guage, followed by a projection of the subjectivity labels from the source to the target language."
The experiment is illustrated in Figure 1.
"We use the MPQA corpus[REF_CITE], which is a collection of 535 English-language news articles from a variety of news sources manually an-notated for subjectivity."
"Although the corpus was originally annotated at clause and phrase level, we use the sentence-level annotations associated with the dataset[REF_CITE]."
"From the total of 9,700 sentences in this corpus, 55% of the sen-tences are labeled as subjective while the rest are objective."
"After the automatic translation of the cor- pus and the projection of the annotations, we obtain a large corpus of 9,700 subjectivity-annotated sen-tences in the target language, which can be used to train a subjectivity classifier."
"In the second experiment, we assume that the only resources available are a tool for subjectivity anno-tation in the source language and a collection of raw texts, also in the source language."
The source lan-guage text is automatically annotated for subjectiv-ity and then translated into the target language.
"In this way, we produce a subjectivity annotated cor-pus that we can use to train a subjectivity annotation tool for the target language."
Figure 2 illustrates this experiment.
"In order to generate automatic subjectivity anno-tations, we use the OpinionFinder tool developed[REF_CITE]."
OpinionFinder includes two classifiers.
The first one is a rule-based high-precision classifier that labels sentences based on the presence of subjective clues obtained from a large lexicon.
"The second one is a high-coverage classi-fier that starts with an initial corpus annotated us-ing the high-precision classifier, followed by several bootstrapping steps that increase the size of the lex-icon and the coverage of the classifier."
For most of our experiments we use the high-coverage classifier.
"As a raw corpus, we use a subset of the SemCor corpus[REF_CITE], consisting of 107 docu-ments with roughly 11,000 sentences."
"This is a bal-anced corpus covering a number of topics in sports, politics, fashion, education, and others."
"The reason for working with this collection is the fact that we also have a manual translation of the SemCor docu-ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4)."
"Note that in this experiment the annotation of sub-jectivity is carried out on the original source lan-guage text, and thus expected to be more accurate than if it were applied on automatically translated text."
"However, the training data in the target lan-guage is produced by automatic translation, and thus likely to contain errors."
"The third experiment is similar to the second one, except that we reverse the direction of the transla-tion."
"We translate raw text that is available in the target language into the source language, and then use a subjectivity annotation tool to label the auto-matically translated source language text."
"After the annotation, the labels are projected back into the tar-get language, and the resulting annotated corpus is used to train a subjectivity classifier."
Figure 3 illus-trates this experiment.
"As before, we use the high-coverage classifier available in OpinionFinder, and the SemCor corpus."
We use a manual translation of this corpus available in the target language.
"In this experiment, the subjectivity annotations are carried out on automatically generated source text, and thus expected to be less accurate."
"How-ever, since the training data was originally written in the target language, it is free of translation errors, and thus training carried out on this data should be more robust."
"For comparison purposes, we also propose an ex-periment which plays the role of an upper bound on the methods described so far."
This experiment in-volves the automatic translation of the test data from the target language into the source language.
"The source language text is then annotated for subjectiv-ity using OpinionFinder, followed by a projection of the resulting labels back into the target language."
"Unlike the previous three experiments, in this experiment we only generate subjectivity-annotated resources, and we do not build and evaluate a stan-dalone subjectivity analysis tool for the target lan-guage."
"Further training of a machine learning algo-rithm, as in experiments two and three, is required in order to build a subjectivity analysis tool."
"Thus, this fourth experiment is an evaluation of the resources generated in the target language, which represents an upper bound on the performance of any machine learning algorithm that would be trained on these re-sources."
Figure 4 illustrates this experiment.
Our initial evaluations are carried out on Romanian.
The performance of each of the three methods is evaluated using a dataset manually annotated for subjectivity.
"To evaluate our methods, we generate a Romanian training corpus annotated for subjectivity on which we train a subjectivity classifier, which is then used to label the test data."
We evaluate the results against a gold-standard corpus consisting of 504 Romanian sentences man-ually annotated for subjectivity.
"These sentences represent the manual translation into Romanian of a small subset of the SemCor corpus, which was removed from the training corpora used in experi-ments two and three."
This is the same evaluation dataset as used[REF_CITE].
"Two Romanian native speakers annotated the sentences individually, and the differences were adjudicated through discussions."
"The agreement of the two an-notators is 0.83% (κ = 0.67); when the uncertain an-notations are removed, the agreement rises to 0.89 (κ = 0.77)."
"The two annotators reached consensus on all sentences for which they disagreed, resulting in a gold standard dataset with 272 (54%) subjective sentences and 232 (46%) objective sentences."
More details about this dataset are available[REF_CITE].
"In order to learn from our annotated data, we ex-periment with two different classifiers, Naı̈ve Bayes and support vector machines (SVM), selected for their performance and diversity of learning method-ology."
"For Naı̈ve Bayes, we use the multinomial model[REF_CITE]with a thresh-old of 0.3."
"For SVM[REF_CITE], we use the LibSVM implementati[REF_CITE]with a linear kernel."
"The automatic translation of the MPQA and of the SemCor corpus was performed using Language Weaver, [URL_CITE] a commercial statistical machine transla-tion software."
"The resulting text was post-processed by removing diacritics, stopwords and numbers."
"For training, we experimented with a series of weight-ing schemes, yet we only report the results obtained for binary weighting, as it had the most consistent behavior."
The results obtained by running the three experi-ments on Romanian are shown in Table 2.
"The base-line on this data set is 54.16%, represented by the percentage of sentences in the corpus that are sub-jective, and the upper bound (UB) is 71.83%, which is the accuracy obtained under the scenario where the test data is translated into the source language and then annotated using the high-coverage Opin-ionFinder tool."
"Perhaps not surprisingly, the SVM classifier out-performs Naı̈ve Bayes by 2% to 6%, implying that SVM may be better fitted to lessen the amount of noise embedded in the dataset and provide more ac-curate classifications."
"The first experiment, involving the automatic translation of the MPQA corpus enhanced with man-ual annotations for subjectivity at sentence level, does not seem to perform well when compared to the experiments in which automatic subjectivity classi- fication is used."
"This could imply that a classifier cannot be so easily trained on the cues that humans use to express subjectivity, especially when they are not overtly expressed in the sentence and thus can be lost in the translation."
"Instead, the automatic annotations produced with a rule-based tool (Opin-ionFinder), relying on overt mentions of words in a subjectivity lexicon, seems to be more robust to translation, further resulting in better classification results."
"To exemplify, consider the following sub-jective sentence from the MPQA corpus, which does not include overt clues of subjectivity, but was an-notated as subjective by the human judges because of the structure of the sentence: It is the Palestini-ans that are calling for the implementation of the agreements, understandings, and recommendations pertaining to the Palestinian-Israeli conflict."
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SemCor subjectivity-annotated corpus.
"In[REF_CITE], we used the manual translation of the SemCor corpus into Romanian to form an English-Romanian par-allel data set."
"The English side was annotated us-ing the Opinion Finder tool, and the subjectivity la-bels were projected on the Romanian text."
A Naı̈ve Bayes classifier was then trained on the subjectivity annotated Romanian corpus and tested on the same gold standard as used in our experiments.
Table 3 shows the results obtained in those experiments by using the high-coverage OpinionFinder classifier.
"Among our experiments, experiments two and three are closest to those proposed[REF_CITE]."
"By using machine translation, from"
"English into Romanian (experiment two) or Roma-nian into English (experiment three), and annotating this dataset with the high-coverage OpinionFinder classifier, we obtain an F-measure of 63.69%, and 65.87% respectively, using Naı̈ve Bayes (the same machine learning classifier as used[REF_CITE])."
"This implies that at most 4% in F-measure can be gained by using a parallel corpus as compared to an automatically translated corpus, fur-ther suggesting that machine translation is a viable alternative to devising subjectivity classification in a target language leveraged on the tools existent in a source language."
"As English is a language with fewer inflections when compared to Romanian, which accommodates for gender and case as a suffix to the base form of a word, the automatic translation into English is closer to a human translation (experiment three)."
"Therefore labeling this data using the OpinionFinder tool and projecting the labels onto a fully inflected human-generated Romanian text provides more accurate classification results, as compared to a setup where the training is carried out on machine-translated Ro-manian text (experiment two)."
We also wanted to explore the impact that the cor- pus size may have on the accuracy of the classifiers.
We re-ran experiments two and three with 20% cor-pus size increments at a time (Figures 5 and 6).
It is interesting to note that a corpus of approximately 6000 sentences is able to achieve a high enough F-measure (around 66% for both experiments) to be considered viable for training a subjectivity classi-fier.
"Also, at a corpus size over 10,000 sentences, the Naı̈ve Bayes classifier performs worse than SVM, which displays a directly proportional trend between the number of sentences in the data set and the ob-served F-measure."
"This trend could be explained by the fact that the SVM classifier is more robust with regard to noisy data, when compared to Naı̈ve Bayes."
"To test the validity of the results on other languages, we ran a portability experiment on Spanish."
"To build a test dataset, a native speaker of Span-ish translated the gold standard of 504 sentences into Spanish."
We maintain the same subjectivity anno-tations as for the Romanian dataset.
"To create the training data required by the first two experiments, we translate both the MPQA corpus and the Sem-Cor corpus into Spanish using the Google Transla-tion service, [Footnote_2] a publicly available machine transla-tion engine also based on statistical machine transla-tion."
"We were therefore able to implement all the ex-periments but the third, which would have required a manually translated version of the SemCor corpus."
"Although we could have used a Spanish text to carry out a similar experiment, due to the fact that the dataset would have been different, the results would not have been directly comparable."
The results of the two experiments exploring the portability to Spanish are shown in Table 4.
"Inter-estingly, all the figures are higher than those ob-tained for Romanian."
"We assume this occurs be-cause Spanish is one of the six official United Na-tions languages, and the Google translation engine is using the United Nations parallel corpus to train their translation engine, therefore implying that a better quality translation is achieved as compared to the one available for Romanian."
"We can therefore conclude that the more accurate the translation en-gine, the more accurately the subjective content is translated, and therefore the better the results."
"As it was the case for Romanian, the SVM classifier pro-duces the best results, with absolute improvements over the Naı̈ve Bayes classifier ranging from 0.2% to 3.5%."
"Since the Spanish automatic translation seems to be closer to a human-quality translation, we are not surprised that this time the first experiment is able to generate a more accurate training corpus as com-pared to the second experiment."
"The MPQA corpus, since it is manually annotated and of better quality, has a higher chance of generating a more reliable data set in the target language."
"As in the experiments on Romanian, when performing automatic transla-tion of the test data, we obtain the best results with an F-measure of 73.41%, which is also the upper bound on our proposed experiments."
"Based on our experiments, we can conclude that ma-chine translation offers a viable approach to gener-ating resources for subjectivity annotation in a given target language."
The results suggest that either a manually annotated dataset or an automatically an-notated one can provide sufficient leverage towards building a tool for subjectivity analysis.
"Since the use of parallel corpora[REF_CITE]requires a large amount of manual labor, one of the reasons behind our experiments was to asses the ability of machine translation to transfer subjec-tive content into a target language with minimal ef-fort."
"As demonstrated by our experiments, machine translation offers a viable alternative in the construc-tion of resources and tools for subjectivity classifica-tion in a new target language, with only a small de-crease in performance as compared to the case when a parallel corpus is available and used."
"To gain further insights, two additional experi-ments were performed."
"First, we tried to isolate the role played by the quality of the subjectivity anno-tations in the source-language for the cross-lingual projections of subjectivity."
"To this end, we used the high-precision OpinionFinder classifier to annotate the English datasets."
"As shown in Table 1, this clas-sifier has higher precision but lower recall as com-pared to the high-coverage classifier we used in our previous experiments."
"We re-ran the second exper-iment, this time trained on the 3,700 sentences that were classified by the OpinionFinder high-precision classifier as either subjective or objective."
"For Ro-manian, we obtained an F-measure of 69.05%, while for Spanish we obtained an F-measure of 66.47%."
"Second, we tried to isolate the role played by language-specific clues of subjectivity."
"To this end, we decided to set up an experiment which, by com-parison, can suggest the degree to which the lan-guages are able to accommodate specific markers for subjectivity."
"First, we trained an English classifier using the SemCor training data automatically anno-tated for subjectivity with the OpinionFinder high-coverage tool."
The classifier was then applied to the English version of the manually labeled test data set (the gold standard described in Section 4).
"Next, we ran a similar experiment on Romanian, using a clas-sifier trained on the Romanian version of the same"
"SemCor training data set, annotated with subjectiv-ity labels projected from English."
The classifier was tested on the same gold standard data set.
"Thus, the two classifiers used the same training data, the same test data, and the same subjectivity annotations, the only difference being the language used (English or Romanian)."
The results for these experiments are compiled in Table 5.
"Interestingly, the experiment conducted on Romanian shows an improvement of 3.5% to 9.5% over the results obtained on English, which indi-cates that subjective content may be easier to learn in Romanian versus English."
"The fact that Roma-nian verbs are inflected for mood (such as indicative, conditional, subjunctive, presumptive), enables an automatic classifier to identify additional subjective markers in text."
"Some moods such as conditional and presumptive entail human judgment, and there-fore allow for clear subjectivity annotation."
"More-over, Romanian is a highly inflected language, ac-commodating for forms of various words based on number, gender, case, and offering an explicit lex-icalization of formality and politeness."
All these features may have a cumulative effect in allowing for better classification.
"At the same time, English entails minimal inflection when compared to other Indo-European languages, as it lacks both gender and adjective agreement (with very few notable ex-ceptions such as beautiful girl and handsome boy)."
"Verb moods are composed with the aid of modals, while tenses and expressions are built with the aid of auxiliary verbs."
"For this reason, a machine learn-ing algorithm may not be able to identify the same amount of information on subjective content in an English versus a Romanian text."
"It is also interesting to note that the labeling of the training set was per-formed using a subjectivity classifier developed for English, which takes into account a large, human-annotated, subjectivity lexicon also developed for English."
One would have presumed that any clas-sifier trained on this annotated text would therefore provide the best results in English.
"Yet, as explained earlier, this was not the case."
"In this paper, we explored the use of machine trans-lation for creating resources and tools for subjec- tivity analysis in other languages, by leveraging on the resources available in English."
"We introduced and evaluated three different approaches to generate subjectivity annotated corpora in a given target lan-guage, and exemplified the technique on Romanian and Spanish."
"The experiments show promising results, as they are comparable to those obtained using manually translated corpora."
"While the quality of the trans-lation is a factor, machine translation offers an effi-cient and effective alternative in capturing the sub-jective semantics of a text, coming within 4% F-measure as compared to the results obtained using human translated corpora."
"In the future, we plan to explore additional language-specific clues, and integrate them into the subjectivity classifiers."
"As shown by some of our experiments, Romanian seems to entail more subjec-tivity markers compared to English, and this factor motivates us to further pursue the use of language-specific clues of subjectivity."
"Our experiments have generated corpora of about 20,000 sentences annotated for subjectivity in Ro-manian and Spanish, which are available for down-load[URL_CITE]along with the manually annotated data sets."
This paper presents two approaches to ranking reader emotions of documents.
"Past studies assign a document to a single emotion cate-gory, so their methods cannot be applied di-rectly to the emotion ranking problem."
"Furthermore, whereas previous research ana-lyzes emotions from the writer’s perspective, this work examines readers’ emotional states."
The first approach proposed in this paper minimizes pairwise ranking errors.
"In the sec-ond approach, regression is used to model emotional distributions."
"Experiment results show that the regression method is more ef-fective at identifying the most popular emo-tion, but the pairwise loss minimization method produces ranked lists of emotions that have better correlations with the correct lists."
Emotion analysis is an increasingly popular re-search topic due to the emergence of large-scale emotion data on the web.
"Previous work primarily studies emotional contents of texts from the writer&apos;s perspective, where it is typically assumed that a writer expresses only a single emotion in a document."
"Unfortunately, this premise does not hold when analyzing a document from the reader&apos;s perspective, because readers rarely agree unani-mously on the emotion that a document instills."
Figure 1 illustrates this phenomenon.
"In the figure, readers’ responses are distributed among different emotion categories."
"In fact, none of the emotions in Figure 1 has a majority (i.e., more than 50%) of the votes."
"Intuitively, it is better to provide a ranking of emotions according to their popularity rather than associating a single reader emotion with a document."
"As a result, current writer-emotion analysis techniques for classifying a document into a single emotion category are not suitable for ana-lyzing reader emotions."
New methods capable of ranking emotions are required.
Reader-emotion analysis has potential applica-tions that differ from those of writer-emotion analysis.
"For example, by integrating emotion ranking into information retrieval, users will be able to retrieve documents that contain relevant contents and at the same time produce desired feel-ings."
"In addition, reader-emotion analysis can as-sist writers in foreseeing how their work will influence readers emotionally."
"In this paper, we present two approaches to ranking reader emotions."
The first approach is in-spired by the success of the pairwise loss minimi-zation framework used in information retrieval to rank documents.
"Along a similar line, we devise a novel scheme to minimize the number of incor-rectly-ordered emotion pairs in a document."
"In the second approach, regression is used to model reader-emotion distributions directly."
"Experiment results show that the regression method is more effective at identifying the most popular emotion, but the pairwise loss minimization method pro-duces ordered lists of emotions that have better correlations with the correct lists."
The rest of this paper is organized as follows.
Section 2 describes related work.
"In Section 3, de-tails about the two proposed approaches are pro-vided."
Section 4 introduces the corpus and Section 5 presents how features are extracted from the cor-pus.
Section 6 shows the experiment procedures and results.
Section 7 concludes the paper.
Only a few studies in the past deal with the reader aspect of emotion analysis.
"For example, Lin et al. (2007; 2008) classify documents into reader-emotion categories."
Most previous work focuses on the writer’s perspective.
They discover that using Support Vec-tor Machines (SVM) with word unigram features results in the best performance.
"Since then, more work has been done to find features better than unigrams."
"In[REF_CITE], word sentiment in-formation is exploited to achieve better classifica-tion accuracy."
Experiments have been done to extract emo-tional information from texts at granularities finer than documents.
"As for the task of ranking, many machine-learning algorithms have been proposed in infor-mation retrieval."
These techniques generate rank-ing functions which predict the relevance of a document.
One class of algorithms minimizes the errors resulting from ordering document pairs in-correctly.
"In par-ticular, the training phase of the Joachims’ Rank-ing SVM[REF_CITE]is formulated as the following SVM optimization problem: min w, ξ i,j,k 12 w T w + C ∑ ξ i, j,k subject to: ∀(q k ,d i ), (q k ,d j )∈V | s k,i &gt; s k, j : w T ("
"Φ(q k ,d i ) − Φ(q k ,d j )) ≥ 1− ξ i, j,k (1) ∀i∀j∀k : ξ i, j,k ≥ 0 where V is the training corpus, Φ(q k , d i ) is the fea-ture vector of document d i with respect to query q k , s k,i is the relevance score of d i with respect to q k , w is a weight vector, C is the SVM cost parameter, and ξ i,j,k are slack variables."
The set of constraints at (1) means that document pairwise orders should be preserved.
"Unfortunately, the above scheme for exploiting pairwise order information cannot be applied di-rectly to the emotion ranking task, because the task requires us to rank emotions within a document rather than provide a ranking of documents."
"In par-ticular, the definitions of Φ(q k ,d i ), Φ(q k ,d j ), s k,i and s k,j do not apply to emotion ranking."
"In the next section, we will show how the pairwise loss mini-mization concept is adapted for emotion ranking."
"In this section, we provide the formal description of the reader-emotion ranking problem."
Then we describe the pairwise loss minimization (PLM) approach and the emotional distribution regression (EDR) approach to ranking emotions.
The reader emotion ranking problem is defined as follows.
"Let D = {d 1 , d 2 , …, d N } be the document space, and E = {e 1 , e 2 , …, e M } be the emotion space."
Let f i : E → ℜ be the emotional probability function of d i ∈D.
"That is, f i (e j ) outputs the fraction of readers who experience emotion e j after reading document d i ."
"Our goal is to find a function r : D → E M such that r(d i ) = (e π (1) , e π (2) , …, e π (M) ) where π is a permutation on {1, 2, …, M}, and f i (e π (1) ) ≥ f i (e π (2) ) ≥ … ≥ f i (e π (M) )."
"As explained in Section 2, the information retrieval framework for exploiting pairwise order informa-tion cannot be applied directly to the emotion rank-ing problem."
"Hence, we introduce a novel formulation of the emotion ranking problem into an SVM optimization problem with constraints based on pairwise loss minimization."
"Whereas Ranking SVM generates only a single ranking function, our method creates a pairwise ranking function g jk :"
"D → ℜ for each pair of emo-tions e j and e k , aiming at satisfying the maximum number of the inequalities: ∀d i ∈D | f i (e j ) &gt; f i (e k ) : g jk (d i ) &gt; 0 ∀d i ∈D | f i (e j ) &lt; f i (e k ) : g jk (d i ) &lt; 0"
"In other words, we want to minimize the number of incorrectly-ordered emotion pairs."
"We further re-quire g jk (d i ) to have the linear form w T Ω(d i ) + b, where w is a weight vector, b is a constant, and Ω(d i ) is the feature vector of d i ."
Details of feature extraction will be presented in Section 5.
"However, an approximate solution to finding g ik can be obtained by solving the following SVM optimization problem: min w , b, ξ i 21 w T w + C ∑ ξ i subject to: ∀d i ∈Q | f i ( e j ) &gt; f i ( e k ) : w T Ω ( d i ) + b ≥ 1 − ξ i ∀d i ∈Q | f i ( e j ) &lt; f i ( e k ) : − ( w T Ω ( d i ) + b ) ≥ 1 − ξ i ∀i : ξ i ≥ 0 where C is the SVM cost parameter, ξ i are slack variables, and Q is the training corpus."
We assume each document d i ∈Q is labeled with f i (e j ) for every emotion e j ∈E.
"When formulated as an SVM optimization prob-lem, finding g jk is equivalent to training an SVM classifier for classifying a document into the e j or e k category."
"Hence, we use LIBSVM, which is an SVM implementation, to obtain the solution. 1"
We now describe how we rank the emotions of a previously unseen document using the M(M – [URL_CITE])/2 pairwise ranking functions g jk created during the training phase.
"First, all of the pairwise ranking functions are applied to the unseen document, which generates the relative orders of every pair of emotions."
These pairwise orders need to be com-bined together to produce a ranked list of all the emotions.
Algorithm 1 does exactly this.
"In Algorithm 1, the confidence of an emotion ordered pair at Line 3 is the probability value re-turned by a LIBSVM classifier for predicting the order."
LIBSVM’s method for generating this prob-ability is described[REF_CITE].
Lines 4 and 5 resolve the problem of conflicting emotion ordered pairs forming a loop in the ordering of emotions.
The ordered list of emotions returned by Algorithm 1 at Line 7 is the final output of the PLM method.
"In the second approach to ranking emotions, we use regression to model f i directly."
A regression function h j :
"D → ℜ is generated for each e j ∈E by learning from the examples (Ω(d i ), f i (e j )) for all documents d i in the training corpus."
"The regression framework we adopt is Support Vector Regression (SVR), which is a regression analysis technique based on SVM[REF_CITE]."
"We require h j to have the form w T Ω(d i ) + b. Finding h j is equivalent to solving the following optimization problem: min w , b, ξ i ,1 , ξ i ,2 12 w T w + C ∑ ( ξ i,1 + ξ i,2 ) subject to: ∀d i ∈Q : f i (e j ) − (w T Ω(d i ) + b) ≥ ε − ξ i,1 (w T Ω(d i ) + b) − f i (e j ) ≥ ε − ξ i,2 ∀i : ξ i,1 , ξ i,2 ≥ 0 where C is the cost parameter, ε is the maximum difference between the predicted and actual values we wish to maintain, ξ i,1 and ξ i,2 are slack variables, and Q is the training corpus."
"To solve the above optimization problem, we use SVM light ’s SVR im-plementation. [URL_CITE]"
"When ranking the emotions of a previously un-seen document d k , we sort the emotions e j ∈E in descending order of h j (d k )."
The training and test corpora used in this study comprise Chinese news articles from Yahoo!
"Kimo News [URL_CITE] , which allows a user to cast a vote for one of eight emotions to express how a news article makes her feel."
Each Yahoo! news article contains a list of eight emotions at the bottom of the web-page.
A reader may select one of the emotions and click on a submit button to submit the emotion.
"As with many websites which collect user responses, such as the Internet Movie Database, users are not forced to submit their responses."
"After submitting a response, the user can view a distribution of emo-tions indicating how other readers feel about the same article."
Figure 1 shows the voting results of a Yahoo! news article.
"The eight available emotions are happy, sad, angry, surprising, boring, heartwarming, awesome, and useful."
Useful is not a true emotion.
"Rather, it means that a news article contains practical infor-mation."
The value f i (e j ) is derived by normalizing the number of votes for emotion e j in document d i by the total number votes in d i .
"The entire corpus consists of 37,416 news arti-cles dating[REF_CITE]."
"News articles prior[REF_CITE]form the training corpus (25,975 articles), and the remaining ones form the test corpus (11,441 articles)."
We collect articles a week after their publication dates to ensure that the vote counts have stabilized.
"As mentioned earlier, readers rarely agree unanimously on the emotion of a document."
Figure 2 illustrates this.
"After obtaining news articles, the next step is to determine how to convert them into feature vectors for SVM and SVR."
"That is, we want to instantiate Ω. For this purpose, three types of features are ex-tracted."
"The first feature type consists of Chinese charac-ter bigrams, which are taken from the headline and content of each news article."
The presence of a bi-gram is indicated by a binary feature value.
Chinese words form the second type of features.
"Unlike English words, consecutive Chinese words in a sentence are not separated by spaces."
"To deal with this problem, we utilize Stanford NLP Group’s Chinese word segmenter to split a sen-tence into words. [URL_CITE]"
"As in the case of bigrams, bi-nary feature values are used."
We use character bigram features in addition to word features to increase the coverage of Chinese words.
A Chinese word is formed by one or more contiguous Chinese characters.
"As mentioned ear-lier, Chinese words in a sentence are not separated by any boundary symbol (e.g., a space), so a Chi-nese word segmentation tool is always required to extract words from a sentence."
"However, a word segmenter may identify word boundaries errone-ously, resulting in the loss of correct Chinese words."
This problem is particularly severe if there are a lot of out-of-vocabulary words in a dataset.
"In Chinese, around 70% of all Chinese words are Chinese character bigrams[REF_CITE]."
"Thus, using Chinese character bigrams as features will allow us to identify a lot of Chinese words, which when combined with the words extracted by the word segmenter, will give us a wider coverage of Chinese words."
The third feature type is extracted from news metadata.
"A news article’s metadata are its news category, agency, hour of publication, reporter, and event location."
Examples of news categories in-clude sports and political.
"Again, we use binary feature values."
News metadata are used because they may contain implicit emotional information.
"The experiments are designed to achieve the fol-lowing four goals: (i) to compare the ranking per-formance of different methods, (ii) to analyze the pairwise ranking quality of PLM, (iii) to analyze the distribution estimation quality of EDR, and (iv) to compare the ranking performance of different feature sets."
News training and test corpora presented in Section 4 are used in all ex-periments.
"We employ three metrics as indicators of ranking quality: ACC@k, NDCG@k and SACC@k."
"ACC@k stands for accuracy at position k. Ac-cording to ACC@k, a predicted ranked list is cor-rect if the list’s first k items are identical (i.e., same items in the same order) to the true ranked list’s first k items."
"If two emotions in a list have the same number of votes, then their positions are in-terchangeable."
ACC@k is computed by dividing the number of correctly-predicted instances by the total number of instances.
"NDCG@k, or normalized discounted cumulative gain at position k[REF_CITE], is a metric frequently used in information retrieval to judge the quality of a ranked list when multiple levels of relevance are considered."
This metric is defined as
"NDCG@ k = z k ∑ ik=1 logrel i 2 (i +1) where rel i is the relevance score of the predicted item at position i, and z k is a normalizing factor which ensures that a correct ranked list has an NDCG@k value of 1."
"In the emotion ranking prob-lem, rel i is the percentage of reader votes received by the emotion at position i. Note that the log 2 (i+1) value in the denominator is a discount factor which decreases the weights of items ranked later in a list."
"NDCG@k has the range [0, 1], where 1 is the best."
"In the experiment results, NDCG@k values are averaged over all instances in the test corpus."
NDCG@k is used because ACC@k has the dis-advantage of not taking emotional distributions into account.
Take Figure 1 as an example.
"In the figure, heartwarming and happy have 31.3% and 30.7% of the votes, respectively."
"Since the two percentages are very close, it is reasonable to say that predicting happy as the first item in a ranked list may also be acceptable."
"However, doing so would be completely incorrect according to ACC@k."
"In contrast, NDCG@k would consider it to be partially correct, and the extent of correctness depends on how much heartwarming and happy’s percentages of votes differ."
"To be exact, if happy is predicted as the first item, then the corresponding NDCG@1 would be 30.7% / 31.3% = 0.98."
"The third metric is SACC@k, or set accuracy at k."
It is a variant of ACC@k.
"According to SACC@k, a predicted ranked list is correct if the set of its first k items is the same as the true ranked list’s set of first k items."
"In effect, SACC@k evalu-ates a ranking method’s ability to place the top k most important items in the first k positions."
"SVM and SVR are employed in PLM and EDR, respectively."
"Both SVM and SVR have the adjust-able C cost parameter, and SVR has an additional ε parameter."
"To estimate the optimal C value for a combination of SVM and features, we perform 4-fold cross-validation on the Yahoo!"
"News training corpus, and select the C value which results in the highest binary classification accuracy during cross-validation."
The same procedure is used to estimate the best C and ε values for a combination of SVR and features.
The C- ε pair which results in the lowest mean squared error during cross-validation is chosen.
"The candidate C values for both SVM and SVR are 2 -10 , 2 -9 , …, 2 -6 ."
The candidate ε val-ues[REF_CITE]-2 and 10 -1 .
All cross-validations are performed solely on the training data.
The test data are not used to tune the parameters.
"Also, SVM and SVR allow users to specify the type of kernel to use."
Linear kernel is selected for both SVM and SVR.
The nearest neighbor (NN) method is used as the baseline.
The ranked emotion list of a news article in the test corpus is predicted as follows.
"First, the test news article is compared to every training news article using cosine similarity, which is de-fined as cos(d i ,d j ) = | D i ∩ D j | |"
"D i | × | D i | where d i and d j are two news articles, and D i and D j are sets of Chinese character bigrams in d i and d j , respectively."
The ranked emotion list of the train-ing article having the highest cosine similarity with the test article is used as the predicted ranked list.
Figures 3 to 5 show the performance of different ranking methods on the test corpus.
"For both PLM and EDR, all of the bigram, word, and news meta-data features are used."
"In Figure 3, EDR’s ACC@1 (0.751) is higher than those of PLM and NN, and the differences are statistically significant with p-value &lt; 0.01."
"So, EDR is the best method at predicting the most popular emotion."
"However, PLM has the best ACC@k for k ≥ 2, and the differences from the other two methods are all significant with p-value &lt; 0.01."
This means that PLM’s predicted ranked lists better resemble the true ranked lists.
Figure 3 displays a sharp decrease in ACC@k values as k increases.
This trend indicates the hard-ness of predicting a ranked list correctly.
"Looking from a different angle, the ranking task under the ACC@k metric is equivalent to the classification of news articles into one of 8!/(8 – k)! classes, where we regard each unique emotion sequence of length k as a class."
"In fact, computing ACC@8 for a ranking method is the same as evaluating the method’s ability to classify a news article into one of 8! = 40,320 classes."
"So, producing a completely-correct ranked list is a difficult task."
"In Figure 4, all of PLM and EDR’s NDCG@k improvements over NN are statistically significant with p-value &lt; 0.01."
"For some values of k, the dif-ference in NDCG@k between PLM and EDR is not significant."
"The high NDCG@k values (i.e., greater than 0.8) of PLM and EDR imply that al-though it is difficult for PLM and EDR to generate completely-correct ranked lists, these two methods are effective at placing highly popular emotions to the beginning of ranked lists."
"In Figure 5, PLM outperforms the other two methods for 2 ≤ k ≤ 7, and the differences are all statistically significant with p-value &lt; 0.01."
"For small values of k (e.g., 2 ≤ k ≤ 3), PLM’s higher SACC@k values mean that PLM is better at plac-ing the highly popular emotions in the top posi-tions of a ranked list."
"To further compare PLM and EDR, we examine their performance on individual test instances."
"Fig-ure 6 shows the percentage of test instances where both PLM and EDR give incorrect lists, only PLM gives correct lists, only EDR gives ranked lists, and both methods give correct lists."
"The “Only PLM Correct” and “Only EDR Correct” categories are nonzero, so neither PLM nor EDR is always better than the other."
"In summary, EDR is the best at predicting the most popular emotion according to ACC@1, NDCG@1 and SACC@1."
"However, PLM gener-ates ranked lists that better resemble the correct ranked lists according to ACC@k and SACC@k for k ≥ 2."
Further analysis shows that neither method is always better than the other.
"In this subsection, we evaluate the performance of PLM in predicting pairwise orders."
We first examine the quality of ranked lists gen-erated by PLM in terms of pairwise orders.
"To do this, we use Kendall’s τ b correlation coefficient, which is a statistical measure for determining the correlation between two ranked lists when there may be ties between two items in a list[REF_CITE]."
The value of τ b is determined based on the number of concordant pairwise orders and the number of discordant pairwise orders between two ranked lists.
"Therefore, this measure is appropriate for evaluating the effectiveness of PLM at predict-ing pairwise orders correctly. τ b has the range [-1, 1], where 1 means a perfect positive correlation, and -1 means two lists are the reverse of each other."
"When computing τ b of two ranked lists, we also calculate a p-value to indicate whether the correla-tion is statistically significant."
We compute τ b statistics between a predicted ranked list and the corresponding true ranked list.
Table 1 shows the results.
"In Table 1, numbers in the “Average τ b ” and “Average p-value” columns are averaged over all test instances."
The statistics for EDR and NN are also included for comparison.
"From the table, we see that PLM has the highest average τ b value and the lowest average p-value, so PLM is better at preserving pairwise orders than EDR and NN methods."
This observation verifies that PLM’s minimization of pairwise loss leads to better prediction of pairwise orders.
We now look at the individual performance of the 28 pairwise emotion rankers g jk .
"As mentioned in Section 3.2, each pairwise emotion ranker g jk is equivalent to a binary classifier for classifying a document into the e j or e k category."
"So, we look at their classification accuracies in Table 2."
"In the table, accuracy ranges from 0.75 for the awesome-surprising pair to 0.91 for the useful-boring pair."
"From the psychological perspective, the rela-tively low accuracy of the awesome-surprising pair is expected, because awesome is surprising in a positive sense."
"So, readers should have a hard time distinguishing between these two emotions."
"And the SVM classifier, which models reader responses, should also find it difficult to discern these two emotions."
"Based on this observation, we suspect that the pairwise classification performance actu-ally reflects the underlying emotional ambiguity experienced by readers."
"To verify this, we quantify the degree of ambiguity between two emotions, and compare the result to pairwise classification accuracy."
"To quantify emotional ambiguity, we introduce the concept of discrimination value between two emotions e j and e k in a document d i , which is de-fined as follows: f i (e j ) − f i (e k ) f i (e j ) + f i (e k ) where f i is the emotional probability function de-fined in Section 3.1."
"Intuitively, the larger the dis-crimination value is, the smaller the degree of ambiguity between two emotions is."
Figure 7 shows the relationship between pair-wise classification accuracy and the average dis-crimination value of the corresponding emotion pair.
"The general pattern is that as accuracy in-creases, the discrimination value also increases."
"To provide concrete evidence, we use Pearson’s prod-uct-moment correlation coefficient, which has the range of [-1, 1], where 1 means a perfect positive correlati[REF_CITE]."
The coefficient for the data in Figure 7 is 0.726 with p-value &lt; 0.01.
"Thus, pairwise emotion classification accuracy reflects the emotional ambiguity experienced by readers."
"In summary, PLM’s pairwise loss minimization leads to better pairwise order predictions than EDR and NN."
"Also, the pairwise classification results reveal the inherent ambiguity between emotions."
"In this subsection, we evaluate EDR’s performance in estimating the emotional probability function f i ."
"With the prior knowledge that a news article’s f i values sum to 1 over all emotions, and f i is between 0 and 1, we adjust EDR’s f i predictions to produce proper distributions."
It is done as follows.
"A pre-dicted f i value greater than 1 or less than 0 is set to 1 and 0, respectively."
Then the predicted f i values are normalized to sum to 1 over all emotions.
NN’s distribution estimation performance is in-cluded for comparison.
"For NN, the predicted f i values of a test article are taken from the emotional distribution of the most similar training article."
Figure 8 shows the mean squared error of EDR and NN for predicting f i .
"In the figure, the error generated by EDR is less than those by NN, and all the differences are statistically significant with p-value &lt; 0.01."
"Thus, EDR’s use of regression leads to better estimation of f i than the NN."
Figure 9 shows each of the three feature type’s ACC@k for predicting test instances’ ranked lists when PLM is used.
"The feature comparison graph for EDR is not shown, because it exhibits a very similar trend as PLM."
"For both PLM and EDR, bigrams are better than words, which are in turn better than news metadata."
"In Figure 9, the combi-nation of all three feature sets achieves the best performance."
"For both PLM and EDR, the im-provements in ACC@k of using all features over words and metadata are all significant with p-value &lt; 0.01, and the improvements over bigrams are significant for k ≤ 2."
"Hence, in general, it is better to use all three feature types together."
This paper presents two methods to ranking reader emotions.
"The PLM method minimizes pairwise loss, and the EDR method estimates emotional dis-tribution through regression."
"Experiments with significant tests show that EDR is better at predict-ing the most popular emotion, but PLM produces ranked lists that have higher correlation with the correct lists."
"We further verify that PLM has better pairwise ranking performance than the other two methods, and EDR has better distribution estima-tion performance than NN."
"As for future work, there are several directions we can pursue."
"An observation is that PLM ex-ploits pairwise order information, whereas EDR exploits emotional distribution information."
We plan to combine these two methods together.
An-other research direction is to improve EDR by finding better features.
We would also like to inte-grate emotion ranking into information retrieval.
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
"We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference."
"As a parsing algorithm, BP is both asymptotically and em-pirically efficient."
"Even with second-order features or la-tent variables, which would make exact parsing consider-ably slower or NP-hard, BP needs only O(n 3 ) time with a small constant factor."
"Furthermore, such features sig-nificantly improve parse accuracy over exact first-order methods."
Incorporating additional features would in-crease the runtime additively rather than multiplicatively.
Computational linguists worry constantly about run-time.
"Sometimes we oversimplify our models, trad-ing linguistic nuance for fast dynamic programming."
"Alternatively, we write down a better but intractable model and then use approximations."
"The CL com-munity has often approximated using heavy pruning or reranking, but is beginning to adopt other meth-ods from the machine learning community, such as Gibbs sampling, rejection sampling, and certain variational approximations."
"We propose borrowing a different approximation technique from machine learning, namely, loopy be-lief propagation (BP)."
"In this paper, we show that BP can be used to train and decode complex pars-ing models."
"Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem. 1"
"We wish to make a dependency parse’s score de-pend on higher-order features, which consider ar- bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels)."
Such features can help accuracy—as we show.
"Alas, they raise the polynomial runtime of projective parsing, and ren-der non-projective parsing NP-hard."
Hence we seek approximations.
We will show how BP’s “message-passing” disci-pline offers a principled way for higher-order fea-tures to incrementally adjust the numerical edge weights that are fed to a fast first-order parser.
Thus the first-order parser is influenced by higher-order interactions among edges—but not asymptotically slowed down by considering the interactions itself.
BP’s behavior in our setup can be understood intu-itively as follows.
"Inasmuch as the first-order parser finds that edge e is probable, the higher-order fea-tures will kick in and discourage other edges e 0 to the extent that they prefer not to coexist with e. 2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e 0 . (The method is approximate because a first-order parser must equally penalize all parses containing e 0 , even those that do not in fact contain e.)"
"This behavior is somewhat similar to parser stack-ing[REF_CITE], in which a first-order parser derives some of its input features from the full 1-best output of an-other parser."
"In our method, a first-order parser de-rives such input features from its own previous full output (but probabilistic output rather than just [Footnote_1]-best)."
"1 As do constraint relaxati[REF_CITE]and forest reranking[REF_CITE]. In contrast, generic NP-hard solution techniques like Integer Linear Programming[REF_CITE]know nothing about optimal substructure."
This circular process is iterated to conver-gence.
Our method also permits the parse to in-teract cheaply with other variables.
"Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another."
"Our method and its numerical details emerge nat-urally as an instance of the well-studied loopy BP algorithm, suggesting several potential future im- provements to accuracy[REF_CITE]and efficiency[REF_CITE]."
"Loopy BP has occasionally been used before in NLP, with good results, to handle non-local fea-tures[REF_CITE]or joint decod-ing[REF_CITE]."
"However, our application to parsing requires an innovation to BP that we ex-plain in §[Footnote_5]—a global constraint to enforce that the parse is a tree."
"5 Our overall model is properly called a dynamic MRF, since we must construct different-size MRFs for input sentences of different lengths. Parameters are shared both across and within these MRFs, so that only finitely many parameters are needed."
"The tractability of some such global constraints points the way toward applying BP to other computationally intensive NLP problems, such as syntax-based alignment of parallel text."
"To apply BP, we must formulate dependency parsing as a search for an optimal assignment to the vari-ables of a graphical model."
We encode a parse using the following variables: Sentence.
The n-word input sentence W is fully observed (not a lattice).
"Let W = W 0 W 1 ···W n , where W 0 is always the special symbol ROOT ."
"If desired, the variables T = T 1 T 2 · · · T n may specify tags on the n words, drawn from some tagset T (e.g., parts of speech)."
These variables are needed iff the tags are to be inferred jointly with the parse.
"The O(n 2 ) boolean variables {L ij : 0 ≤ i ≤ n, 1 ≤ j ≤ n, i 6= j} correspond to the possible links in the dependency parse. 3 L ij = true is in-terpreted as meaning that there exists a dependency link from parent i → child j. [Footnote_4] Link roles, etc."
"4 We could have chosen a different representation with O(n) integer variables {P j : 1 ≤ j ≤ n}, writing P j = i instead of L ij = true. This representation can achieve the same asymp-totic runtime for BP by using sparse messages, but some con-straints and algorithms would be somewhat harder to explain."
"It would be straightforward to add other variables, such as a binary variable L irj that is true iff there is a link i → r j labeled with role r (e.g., AGENT , PATIENT , TEMPORAL ADJUNCT )."
"We wish to define a probability distribution over all configurations, i.e., all joint assignments A to these variables."
"Our distribution is simply an undirected graphical model, or Markov random field (MRF): 5 p(A) = def 1 YF m (A) (1) Z m specified by the collection of factors F m : A 7→ ≥0"
Each factor is a function that consults only a subset of A.
"We say that the factor has degree d if it depends on the values of d variables in A, and that it is unary, binary, ternary, or global if d is respectively 1, 2, [Footnote_3], or unbounded (grows with n)."
"3 “Links” are conventionally called edges, but we reserve the term “edge” for describing the graphical model’s factor graph."
A factor function F m (A) may also depend freely on the observed variables—the input sentence W and a known (learned) parameter vector θ.
"For no-tational simplicity, we suppress these extra argu-ments when writing and drawing factor functions, and when computing their degree."
"In this treatment, these observed variables are not specified by A, but instead are absorbed into the very definition of F m ."
"In defining a factor F m , we often define the cir-cumstances under which it fires."
These are the only circumstances that allow F m (A) =6 1.
"When F m does not fire, F m (A) = 1 and does not affect the product in equation (1)."
A hard factor F m fires only on parses A that violate some specified condition.
"It has value 0 on those parses, acting as a hard constraint to rule them out."
T REE .
A hard global constraint on all the L ij vari-ables at once.
"It requires that exactly n of these vari-ables be true, and that the corresponding links form a directed tree rooted at position 0."
PT REE .
This stronger version of T REE requires further that the tree be projective.
"That is, it pro-hibits L ij and L k` from both being true if i → j crosses k → `. (These links are said to cross if one of k, ` is strictly between i and j while the other is strictly outside that range.)"
E XACTLY 1.
"A family of O(n) hard global con-straints, indexed by 1 ≤ j ≤ n. E XACTLY 1 j re-quires that j have exactly one parent, i.e., exactly one of the L ij variables must be true."
Note that E X - ACTLY 1 is implied by T REE or PT REE .
A T M OST 1.
A weaker version.
A T M OST 1 j re-quires j to have one or zero parents.
A family of hard binary constraints.
"NAND ij,k` requires that L ij and L k` may not both be true."
We will be interested in certain subfamilies.
N OT 2.
"Shorthand for the family of O(n 3 ) bi-nary constraints { NAND ij,kj }."
"These are collectively equivalent to A T M OST 1, but expressed via a larger number of simpler constraints, which can make the BP approximation less effective (footnote 30)."
N O 2C YCLE .
"Shorthand for the family of O(n 2 ) binary constraints { NAND ij,ji }."
A soft factor F m acts as a soft constraint that prefers some parses to others.
"In our experiments, it is al-ways a log-linear function returning positive values: def F m (A) = exp X θ h f h (A, W, m) (2) h∈features(F m ) where θ is a learned, finite collection of weights and f is a corresponding collection of feature functions, some of which are used by F m . (Note that f h is permitted to consult the observed input W. It also sees which factor F m it is scoring, to support reuse of a single feature function f h and its weight θ h by unboundedly many factors in a model.)"
L INK .
A family of unary soft factors that judge the links in a parse A individually.
L INK ij fires iff
"L ij = true, and then its value depends on (i,j), W , and θ."
Our experiments use the same features[REF_CITE].
"A first-order (or “edge-factored”) parsing model[REF_CITE]contains only L INK factors, along with a global T REE or PT REE factor."
"Though there are O(n 2 ) link factors (one per L ij ), only n of them fire on any particular parse, since the global factor ensures that exactly n are true."
We’ll consider various higher-order soft factors:
"The binary factor PAIR ij,k` fires with some value iff L ij and L k` are both true."
"Thus, it penal-izes or rewards a pair of links for being simultane-ously present."
This is a soft version of NAND .
G RAND .
"Shorthand for the family of O(n 3 ) binary factors { PAIR ij,jk }, which evaluate grandparent-parent-child configurations, i → j → k."
"For exam-ple, whether preposition j attaches to verb i might depend on its object k."
"In non-projective parsing, we might prefer (but not require) that a parent and child be on the same side of the grandparent."
S IB .
"Shorthand for the family of O(n 3 ) binary fac-tors { PAIR ij,ik }, which judge whether two children of the same parent are compatible."
"E.g., a given verb may not like to have two noun children both to its left. [Footnote_6] The children do not need to be adjacent."
"6 A similar binary factor could directly discourage giving the verb two SUBJECT s, if the model has variables for link roles."
C HILD S EQ .
A family of O(n) global factors.
C HILD S EQ i scores i’s sequence of children; hence it consults all variables of the form L ij .
The scor-ing follows the parametrization of a weighted split head-automaton grammar[REF_CITE].
"If 5 has children 2, 7, 9 under A, then C HILD S EQ i is a product of subfactors of the form PAIR 5#,57 ,[REF_CITE],[REF_CITE]5# (right child sequence) and PAIR 5#,52 ,[REF_CITE]5# (left child sequence)."
N O C ROSS .
A family of O(n 2 ) global constraints.
"If the parent-to-j link crosses the parent-to-` link, then N O C ROSS j` fires with a value that depends only on j and `. (If j and ` do not each have ex-actly one parent, N O C ROSS j` fires with value 0; i.e., it incorporates E XACTLY 1 j and E XACTLY 1 ` .) [Footnote_7] T AG i is a unary factor that evaluates whether T i ’s value is consistent with W (especially W i )."
"7 In effect, we have combined the O(n 4 ) binary factors P AIR ij,k` into O(n 2 ) groups, and made them more precise by multiplying in E XACTLY O NE constraints (see footnote 30). This will permit O(n 3 ) total computation if we are willing to sacrifice the ability of the P AIR weights to depend on i and k."
"T AG L INK ij is a ternary version of the L INK ij fac-tor whose value depends on L ij , T i and T j (i.e., its feature functions consult the tag variables to decide whether a link is likely)."
One could similarly enrich the other features above to depend on tags and/or link roles; T AG L INK is just an illustrative example.
T RIGRAM is a global factor that evaluates the tag sequence T according to a trigram model.
"It is a product of subfactors, each of which scores a tri-gram of adjacent tags T i−2 , T i−1 , T i , possibly also considering the word sequence W (as in CRFs)."
"MacKay (2003, chapters 16 and 26) provides an excellent introduction to belief propagation, a gen- eralization of the forward-backward algorithm that is deeply studied in the graphical models literature ([REF_CITE]for example)."
We briefly sketch the method in terms of our parsing task.
The basic BP idea is simple.
"8 Or, more precisely—this is the tricky part—based on ver-sions of those other distributions that do not factor in L 34 ’s re-ciprocal influence on them. This prevents (e.g.) L 34 and T 3 from mutually reinforcing each other’s existing beliefs."
Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.
"In Gibbs sampling, L 34 ’s value is periodically resam-pled based on the current values of other variables."
Loopy BP works not with random samples but their expectations.
Hence it is approximate but tends to converge much faster than Gibbs sampling will mix.
"It is convenient to visualize an undirected factor graph (Fig. 1), in which each factor is connected to the variables it depends on."
Many factors may connect to—and hence influence—a given variable such as L 34 .
"If X is a variable or a factor, N(X) denotes its set of neighbors."
"Given an input sentence W and a parameter vector θ, the collection of factors F m defines a probabil-ity distribution (1)."
The parser should determine the values of the individual variables.
"In other words, we would like to marginalize equation (1) to obtain the distribution p(L 34 ) over L 34 = true vs. false, the distribution p(T 4 ) over tags, etc."
"If the factor graph is acyclic, then BP com-putes these marginal distributions exactly."
"Given an HMM, for example, BP reduces to the forward-backward algorithm."
BP’s estimates of these distributions are called be-liefs about the variables.
"BP also computes be-liefs about the factors, which are useful in learn-ing θ (see §7)."
"E.g., if the model includes the factor T AG L INK ij , which is connected to variables L ij , T i , T j , then BP will estimate the marginal joint distribu-tion p(L ij , T i , T j ) over (boolean, tag, tag) triples."
"When the factor graph has loops, BP’s beliefs are usually not the true marginals of equation (1) (which are in general intractable to compute)."
"Indeed, BP’s beliefs may not be the true marginals of any distribu-tion p(A) over assignments, i.e., they may be glob-ally inconsistent."
"All BP does is to incrementally adjust the beliefs till they are at least locally con-sistent: e.g., the beliefs at factors T AG L INK ij and T AG L INK ik must both imply [Footnote_9] the same belief about variable T i , their common neighbor."
"9 In the sense that marginalizing the belief p(L ij , T i , T j ) at the factor yields the belief p(T i ) at the variable."
This iterated negotiation among the factors is han-dled by message passing along the edges of the fac-tor graph.
A message to or from a variable is a (pos-sibly unnormalized) probability distribution over the values of that variable.
"The variable V sends a message to factor F , say-ing “My other neighboring factors G jointly suggest that I have posterior distribution q V→F (assuming that they are sending me independent evidence).”"
"Meanwhile, factor F sends messages to V , saying, “Based on my factor function and the messages re-ceived from my other neighboring variables U about their values (and assuming that those messages are independent), I suggest you have posterior distribu-tion r F→V over your values.”"
"To be more precise, BP at each iteration k (until convergence) updates two kinds of messages: (k+1) r (G→Vk) (v) Y q V →F (v) = κ (3) G∈N(V ) \F from variables to factors, and (4) F (A) Y q U(k→F) (A[U]) (k+1) X r F→V (v) = κ"
A s.t. A[V ]=v U∈N(F)\V from factors to variables.
"Each message is a proba-bility distribution over values v of V , normalized by a scaling constant κ."
"Alternatively, messages may be left as unnormalized distributions, choosing κ 6= 1 only as needed to prevent over- or underflow."
Mes-sages are initialized to uniform distributions.
"Whenever we wish, we may compute the beliefs at V and F : b (Vk→+1) (v) = def κ Y r (G→Vk) (v) (5) G∈N(V ) b (Fk→+1) (A) def = κ F (A) Y q U(k→F) (A[U]) (6) U∈N(F)"
"These beliefs do not truly characterize the ex-pected behavior of Gibbs sampling (§4.1), since the products in (5)–(6) make conditional independence assumptions that are valid only if the factor graph is acyclic."
"Furthermore, on cyclic (“loopy”) graphs, BP might only converge to a local optimum[REF_CITE], or it might not converge at all."
"Still, BP often leads to good, fast approximations."
One iteration of standard BP simply updates all the messages as in equations (3)–(4): one message per edge of the factor graph.
"Therefore, adding new factors to the model in-creases the runtime per iteration additively, by in-creasing the number of messages to update."
We believe this is a compelling advantage over dy-namic programming—in which new factors usually increase the runtime and space multiplicatively by exploding the number of distinct items. 10
But how long does updating each message take?
"The runtime of summing over all assignments P A in 10 For example, with unknown tags T, a model with PT REE +T AG L INK will take only O(n 3 + n 2 g 2 ) time for BP, compared to O(n 3 g 2 ) time for dynamic programming (Eisner &amp;[REF_CITE])."
"Adding T RIGRAM , which is string-local rather than tree-local, will increase this only to O(n 3 + n 2 g 2 + ng 3 ), compared to O(n 3 g 6 ) for dynamic programming."
"Even more dramatic, adding the S IB family of O(n 3 ) PAIR ij,ik factors will add only O(n 3 ) to the runtime of BP equation (4) may appear prohibitive."
"Crucially, how-ever, F (A) only depends on the values in A of F ’s its neighboring variables N (F )."
So this sum is pro-portional to a sum over restricted assignments to just those variables. [Footnote_11]
11 The constant of proportionality may be folded into κ; it is the number of assignments to the other variables.
"For example, computing a message from T AG L INK ij → T i only requires iterating over all (boolean, tag, tag) triples. [Footnote_12] The runtime to update that message is therefore O(2 · |T | · |T |)."
"12 Separately for each value v of T i , get v’s probability by summing over assignments to (L ij , T i , T j ) s.t. T i = v."
The above may be tolerable for a ternary factor.
But how about global factors?
E XACTLY 1 j has n neigh-boring boolean variables: surely we cannot iterate over all 2 n assignments to these!
"T REE is even worse, with 2 O(n 2 ) assignments to consider."
We will give specialized algorithms for handling these sum-mations more efficiently.
A historical note is in order.
"Traditional constraint satisfaction corresponds to the special case of (1) where all factors F m are hard constraints (with val-ues in {0,1})."
"In that case, loopy BP reduces to an algorithm for generalized arc consistency[REF_CITE], and updating a factor’s outgoing messages is known as constraint propagation."
"In the same spirit, we will demonstrate efficient propagators for our global constraints, e.g. by adapt-ing combinatorial algorithms for weighted parsing."
"We are unaware of any previous work on global fac-tors in sum-product BP, although for max-product[REF_CITE]independently showed that a global 1-to-1 alignment constraint—a kind of weighted A LL D IFFERENT —permits an efficient propagator based on weighted bipartite matching."
Table 1 shows our asymptotic runtimes for all fac-tors in §§3.3–3.4.
"Remember that if several of these factors are included, the total runtime is additive. 14"
Propagating the local factors is straightforward (§5.1).
We now explain how to handle the global factors.
Our main trick is to work backwards from marginal beliefs.
Let F be a factor and V be one of its neighboring variables.
"At any time, F has a marginal belief about V (see footnote 9), (k+1) X b F→ (V = v) = b (Fk→+1) (A) (7) A s.t. A[V ]=v a sum over (6)’s products of incoming messages."
"By the definition of r F→V in (4), and distributivity, we can also express the marginal belief (7) as a point-wise product of outgoing and incoming messages [Footnote_15] b F→ (V = v) = r F→V (v) · q V(k→F) (v) (k+1) (k+1) (8) up to a constant."
"15 E.g., the familiar product of forward and backward mes-sages that is used to extract posterior marginals from an HMM."
"If we can quickly sum up the marginal belief (7), then (8) says we can divide out"
Note that the marginal belief and both messages are unnormalized distributions over values v of V .
"F and k are clear from context below, so we simplify the notation so that (7)–(8) become b(V = v) = X b(A) = r V (v) · q V (v) A s.t. A[V ]=v"
T RIGRAM must sum over assignments to the tag sequence
T. The belief (6) in a given assignment is a product of trigram scores (which play the role of transition weights) and incoming messages q T j (playing the role of emission weights).
"The marginal belief (7) needed above, b(T i = t), is found by sum-ming over assignments where T i = t."
All marginal beliefs are computed together in O(ng 3 ) total time by the forward-backward algorithm. [Footnote_16] E XACTLY 1 j is a sparse hard constraint.
"16 Which is itself an exact BP algorithm, but on a different graph—a junction tree formed from the graph of T RIGRAM sub-factors. Each variable in the junction tree is a bigram. If we had simply replaced the global T RIGRAM factor with its subfactors in the full factor graph, we would have had to resort to General-ized BP[REF_CITE]to obtain the same exact results."
"Even though there are 2 n assignments to its n neighboring variables {L ij }, the factor function returns 1 on only n assignments and 0 on the rest."
"In fact, for a given i, b(L ij = true) in (7) is defined by (6) to have exactly one non-zero summand, in which A puts L ij = true and all other L i 0 j = false."
We compute the marginal beliefs for all i together in O(n) total time: def Q 1.
Pre-compute π = i q L ij (false). [Footnote_17] 2.
"17 But taking π = 1 gives the same results, up to a constant."
"For each i, compute the marginal belief b(L ij = true) as π · q̄ L ij , where q̄ L ij ∈ R de-notes the odds ratio q L ij (true)/q L ij (false). [Footnote_18] 3."
"18 As a matter of implementation, this odds ratio q̄ L ij can be used to represent the incoming message q L ij everywhere."
The partition function b() denotes P A b(A); compute it in this case as P i b(L ij = true). 4.
"For each i, compute b(L ij = false) by subtrac-tion, as b() − b(L ij = true)."
T REE and PT REE must sum over assignments to the O(n 2 ) neighboring variables {L ij }.
"There are now exponentially many non-zero summands, those in which A corresponds to a valid tree."
"Nonetheless, we can follow the same approach as for E XACTLY 1."
"Steps 1 and 4 are modified to iterate over all i, j such that L ij is a variable."
"In step 3, the partition function P b(A) is now π times the total weight of all trees, A where the weight of a given tree is the product of the q̄ L ij values of its n edges."
"In step 2, the marginal belief b(L ij = true) is now π times the total weight of all trees having edge i → j."
"We perform these combinatorial sums by calling a first-order parsing algorithm, with edge weights q̄ ij ."
"Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors."
"The parsing algorithm simultaneously computes the partition function b(), and all O(n 2 ) marginal beliefs b(L ij = true)."
"For PT REE (projective), it is the inside-outside version of a dynamic program-ming algorithm[REF_CITE]."
"For T REE (non-projective),[REF_CITE]and[REF_CITE]show how to employ the matrix-tree theorem."
"In both cases, the total time is O(n 3 ). [Footnote_19] N O C ROSS j` must sum over assignments to O(n) neighboring variables {L ij } and {L k` }."
"19 A dynamic algorithm could incrementally update the out-going messages if only a few incoming messages have changed (as in asynchronous BP). In the case of T REE , dynamic matrix inverse allows us to update any row or column (i.e., messages from all parents or children of a given word) and find the new inverse in O(n 2 ) time[REF_CITE]."
The non-zero summands are assignments where j and ` def each have exactly one parent.
"At step 1, π = Q q (false) ·"
Q k q L k` (false).
"At step 2, the i L ij marginal belief b(L ij = true) sums over the n non-zero assignments containing i → j. It is π · q̄ L ij · P q̄ k L k` · PAIR ij,k` , where PAIR ij,k` is x j` if i → j crosses k → ` and is 1 otherwise. x j` is some factor value defined by equation (2) to penalize or reward the crossing."
Steps 3–4 are just as in E XACTLY 1 j .
"The question is how to compute b(L ij = true) for each i in only O(1) time, [Footnote_20] so that we can propagate each of the O(n 2 ) N O C ROSS j` in O(n) time."
"20 Symmetrically, we compute b(L k` = true) for each k."
"This is why we allowed x j` to depend only on j,`."
We can rewrite the sum b(L ij = true) as π · q̄ L ij · (x j` · X q̄ L k` + 1 ·X q̄ L k` ) (9) crossing k noncrossing k
"To find this in O(1) time, we precompute for each def P ` an array of partial sums Q ` [s, t] = s≤k≤t q̄ L k` ."
"Since Q ` [s, t] = Q ` [s, t−1]+q̄ L t` , we can compute each entry in O(1) time."
"The total precomputation time over all `, s, t is then O(n 3 ), with the array Q ` shared across all factors N O C ROSS j 0 ` ."
"The crossing sum is respectively Q ` [0, i−1]+Q ` [j+1, n], Q ` [i+ 1, j − 1], or 0 according to whether ` ∈ (i, j), ` ∈/ [i, j], or ` = i. [Footnote_21]"
21 There are no N O C ROSS j` factors with ` = j.
"The non-crossing sum is Q ` [0, n] minus the crossing sum."
"C HILD S EQ i , like T RIGRAM , is propagated by a forward-backward algorithm."
"In this case, the al-gorithm is easiest to describe by replacing C HILD - S EQ i in the factor graph by a collection of local subfactors, which pass messages in the ordinary way. [Footnote_22][REF_CITE]at each j ∈ [1,n], we introduce a new variable C ij —a hidden state whose value is the position of i’s previous child, if any (so 0 ≤ C ij &lt; j)."
"22 We still treat C HILD S EQ i as a global factor and compute all its correct outgoing messages on a single BP iteration, via serial forward and backward sweeps through the subfactors. Handling the subfactors in parallel, (3)–(4), would need O(n) iterations."
"So the ternary sub-factor on (C ij ,L ij ,C i,j+1 ) has value 1 if L ij = false and C i,j+1 = C i,j ; a sibling-bigram score ( PAIR iC ij ,iC i,j+1 ) if L ij = true and C i,j+1 = j; and 0 otherwise."
"The sparsity of this factor, which is 0 almost everywhere, is what gives C HILD S EQ i a total runtime of O(n 2 ) rather than O(n 3 )."
"It is equivalent to forward-backward on an HMM with n observa-tions (the L ij ) and n states per observation (the C j ), with a deterministic (thus sparse) transition function."
"BP computes local beliefs, e.g. the conditional prob-ability that a link L ij is present."
"But if we wish to output a single well-formed dependency tree, we need to find a single assignment to all the {L ij } that satisfies the T REE (or PT REE ) constraint."
"Our final belief about the T REE factor is a distri-bution over such assignments, in which a tree’s prob-ability is proportional to the probability of its edge weights q̄ L ij (incoming messages)."
"We could simply return the mode of this distribution (found by using a 1-best first-order parser) or the k-best trees, or take samples."
"In our experiments, we actually take the edge weights to be not the messages q̄ L ij from the links, def but the full beliefs b̄ L ij at the links (where b̄ L ij = log b L ij (true)/b L ij (false))."
These are passed into a fast algorithm for maximum spanning tree[REF_CITE]or maximum projective spanning tree[REF_CITE].
This procedure is equivalent to minimum Bayes risk (MBR) parsing[REF_CITE]with a dependency accuracy loss function.
Notice that the above decoding approaches do not enforce any hard constraints other than T REE in the final output.
"In addition, they only recover values of the L ij variables."
They marginalize over other variables such as tags and link roles.
This solves the problem of “nuisance” variables (which merely fragment probability mass among refinements of a parse).
"On the other hand, it may be undesirable for variables whose values we desire to recover. 24"
"Our training method also uses beliefs computed by BP, but at the factors."
We choose the weight vector θ by maximizing the log-probability of training data 24 An alternative is to attempt to find the most probable (“MAP”) assignment to all variables—using the max-product algorithm (footnote 13) or one of its recent variants.
"The esti-mated marginal beliefs become “max marginals,” which assess the 1-best assignment consistent with each value of the variable."
We can indeed build max-product propagators for our global constraints.
PT REE still propagates in O(n 3 ) time: simply change the first-order parser’s semiring[REF_CITE]to use max instead of sum.
"T REE requires O(n 4 ) time: it seems that the O(n 2 ) max marginals must be computed separately, each requiring a separate call to an O(n 2 ) maximum spanning tree algorithm[REF_CITE]."
"If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique."
"However, max-product BP tends to be unstable on loopy graphs, and we may not wish to wait for full convergence in any case."
"A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation along some spanning subtree of the factor graph."
A slower but potentially more stable alternative is determin-istic annealing.
"Replace each factor F m (A) with F m (A) 1/T , where T &gt; 0 is a temperature."
"As T → 0 (“quenches”), the under equation (1), regularizing only by early stop-ping."
"If all variables are observed in training, this objective function is convex (as for any log-linear model)."
"The difficult step in computing the gradient of our objective is finding ∇ θ log Z, where Z in equa-tion (1) is the normalizing constant (partition func-tion) that sums over all assignments A. (Recall that Z, like each F m , depends implicitly on W and θ.)"
"As usual for log-linear models, ∇ θ log Z = X E p(A) [∇ θ F m (A)] (10) m"
"Since ∇ θ F m (A) only depends on the assignment A’s values for variables that are connected to F m in the factor graph, its expectation under p(A) de-pends only on the marginalization of p(A) to those variables jointly."
"Fortunately, BP provides an esti-mate of that marginal distribution, namely, its belief about the factor F m , given W and θ (§4.2). [Footnote_25]"
"25 One could use coarser estimates at earlier stages of training, by running fewer iterations of BP."
Note that the hard constraints do not depend on θ at all; so their summands in equation (10) will be 0.
"We employ stochastic gradient descent[REF_CITE], since this does not require us to compute the objective function itself but only to (approxi-mately) estimate its gradient as explained above."
"Al-ternatively, given any of the MAP decoding proce-dures from §6, we could use an error-driven learning method such as the perceptron or MIRA. [Footnote_26]"
"26 The BP framework makes it tempting to extend an MRF model with various sorts of latent variables, whose values are not specified in training data. It is straightforward to train under these conditions. When counting which features fire on a train-ing parse or (for error-driven training) on an current erroneous parse, we can find expected counts if these parses are not fully observed, by using BP to sum over latent variables."
"We asked: (1) For projective parsing, where higher-order factors have traditionally been incorporated into slow but exact dynamic programming (DP), what are the comparative speed and quality of the BP approximation? (2) How helpful are such higher-order factors—particularly for non-projective pars-ing, where BP is needed to make them tractable? (3) Do our global constraints (e.g., T REE ) contribute to the goodness of BP’s approximation?"
We trained and tested on three languages from the CoNLL Dependency Parsing Shared Task[REF_CITE].
The English data for that task were converted from the Penn Treebank to dependen-cies using a trace-recovery algorithm that induced some very slight non-projectivity—about 1% of links crossed other links.
Danish is a slightly more non-projective language (3% crossing links).
Dutch is the most non-projective language in the corpus (11%).
"In all cases, the test input W consists of part-of-speech-tagged words, so T variables were not used."
"Although BP makes it cheap to incorporate many non-local features and latent variables at once, we kept our models relatively simple in this paper."
Our first-order L INK ij factors replicate[REF_CITE].
"Following equation (2), they are defined using binary features that look at words i and j, the distance j − i, and the tags (provided in W ) of words at, around, and between i and j."
Our second-order features are similar.
"In the G RAND factors, features fire for particular triples of tags and of coarse tags."
"A feature also fires if the grandparent falls between the child and parent, inducing crossing dependency links."
"The C HILD - S EQ factors included features for tags, and like-wise coarse tags, on adjacent sibling pairs and parent-sibling-sibling triples."
Each of these fea-tures also have versions that were conjoined with link direction—pairs of directions in the grandpar-ent case—or with signed link length of the child or farther sibling.
Lengths were binned per[REF_CITE].
"The N O C ROSS j` factors consider the tag and coarse tag attributes of the two child words j and `, separately or jointly."
We trained all models using stochastic gradient de-scent (§7).
SGD initialized θ~ = 0 and ran for 10 con-secutive passes over the data; we picked the stopping point that performed best on held-out data.
"When comparing runtimes for projective parsers, we took care to produce comparable implementa-tions."
All beliefs and dynamic programming items were stored and indexed using the high-level[REF_CITE]while all inference and propagation was written in C++.
"The BP parser averaged 1.8 seconds per sentence for non-projective parsing and 1.5 sec-onds per sentence for projective parsing (1.2 and 0.9 seconds/sentence for ≤ 40 words), using our stan-dard setup, which included five iterations of BP and the final MBR tree decoding pass."
"In our tables, we boldface the best result in each column along with any results that are not signifi-cantly worse (paired permutation test, p &lt; .05)."
"We built a first-order projective parser—one that uses only factors PT REE and L INK —and then com-pared the cost of incorporating second-order factors, G RAND and C HILD S EQ , by BP versus DP. [Footnote_28]"
"28 We trained these parsers using exact DP, using the inside-outside algorithm to compute equation (10). The training and test data were English, and for this section we filtered out sen-tences with non-projective links."
"Under DP, the first-order runtime of O(n 3 ) is in-creased to O(n 4 ) with G RAND , and to O(n 5 ) when we add C HILD S EQ as well."
"BP keeps runtime down to O(n 3 )—although with a higher constant factor, since it takes several rounds to converge, and since it computes more than just the best parse. [Footnote_29]"
"29 Viterbi parsing in the log domain only needs the (max, +) semiring, whereas both BP and any MBR parsing must use the slower (+, log+) so that they can compute marginals."
Figures 2–3 compare the empirical runtimes for various input sentence lengths.
"With only the G RAND factor, exact DP can still find the Viterbi parse (though not the[REF_CITE]) faster than ten iterations of the asymptotically better BP (Fig. 2), at least for sentences with n ≤ 75."
"However, once we add the C HILD S EQ factor, BP is always faster— dramatically so for longer sentences (Fig. 3)."
More complex models would widen BP’s advantage.
Fig. 4 shows the tradeoff between runtime and search error of BP in the former case (G RAND only).
"To determine BP’s search error at finding the MBR parse, we measured its dependency accuracy not against the gold standard, but against the optimal MBR parse under the model, which DP is able to find."
More BP iterations may help accuracy.
"In future work, we plan to compare BP’s speed-accuracy curve on more complex projective models with the speed-accuracy curve of pruned or reranked DP."
The BP approximation can be used to improve the accuracy of non-projective parsing by adding higher-order features.
These would be NP-hard to incorporate exactly; DP cannot be used.
"We used BP with a non-projective T REE factor to train conditional log-linear parsing models of two highly non-projective languages, Danish and Dutch, as well as slightly non-projective English (§8.1)."
"In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links."
"We thus added N O C ROSS factors, as well as G RAND and C HILD S EQ as before."
"All of these significantly improve the first-order baseline, though not necessarily cumulatively (Table 2)."
"Finally, Table 2 compares loopy BP to a previ-ously proposed “hill-climbing” method for approx-imate inference in non-projective parsing[REF_CITE]."
"Hill-climbing decodes our richest non-projective model by finding the best pro-jective parse under that model—using slow, higher-order DP—and then greedily modifies words’ par-ents until the parse score (1) stops improving. with T REE , decoding it with weaker constraints is asymp-totically faster (except for N OT 2) but usually harm-ful. (Parenthetical numbers show that the harm is com-pounded if the weaker constraints are used in training as well; even though this matches training to test con-ditions, it may suffer more from BP’s approximate gradi-ents.)"
Decoding the T REE model with the even stronger PT REE constraint can actually be helpful for a more pro-jective language.
All results use 5 iterations of BP.
BP for non-projective languages is much faster and more accurate than the hill-climbing method.
"Also, hill-climbing only produces an (approximate) 1-best parse, but BP also obtains (approximate) marginals of the distribution over all parses."
"Given the BP architecture, do we even need the hard T REE constraint?"
Or would it suffice for more local hard constraints to negotiate locally via BP?
We investigated this for non-projective first-order parsing.
"Table 3 shows that global constraints are indeed important, and that it is essential to use T REE during training."
"At test time, the weaker but still global E XACTLY 1 may suffice (followed by MBR decoding to eliminate cycles), for total time O(n 2 )."
"Table 3 includes N OT 2, which takes O(n 3 ) time, merely to demonstrate how the BP approximation becomes more accurate for training and decoding when we join the simple N OT 2 constraints into more global A T M OST 1 constraints."
"This does not change the distribution (1), but makes BP enforce stronger local consistency requirements at the factors, rely-ing less on independence assumptions."
"In general, one can get better BP approximations by replacing a group of factors F m (A) with their product. [Footnote_30]"
"30 In the limit, one could replace the product (1) with a sin-gle all-purpose factor; then BP would be exact—but slow. (In constraint satisfaction, joining constraints similarly makes arc consistency slower but better at eliminating impossible values.)"
"The above experiments concern gold-standard accuracy under a given first-order, non-projective model."
"Flipping all three of these parameters for Danish, we confirmed the pattern by instead mea-suring search error under a higher-order, projective model (PT REE +L INK +G RAND ), when PT REE was weakened during decoding."
"Compared to the MBR parse under that model, the search errors from de-coding with weaker hard constraints were 2.2% for N OT 2, 2.1% for E XACTLY 1, 1.7% for E XACTLY 1 + N O [Footnote_2]C YCLE , and 0.0% for PT REE ."
2 This may be reminiscent of adjusting a Lagrange multiplier on e 0 until some (hard) constraint is satisfied.
Belief propagation improves non-projective depen-dency parsing with features that would make ex-act inference intractable.
"For projective parsing, it is significantly faster than exact dynamic program-ming, at the cost of small amounts of search error,"
"We are interested in extending these ideas to phrase-structure and lattice parsing, and in try-ing other higher-order features, such as those used in parse reranking[REF_CITE]and history-based parsing[REF_CITE]."
"We could also introduce new variables, e.g., nonterminal refinements[REF_CITE], or secondary links"
"M ij (not constrained by T REE /PT REE ) that augment the parse with repre-sentations of control, binding, etc.[REF_CITE]."
Other parsing-like problems that could be at-tacked with BP appear in syntax-based machine translation.
Decoding is very expensive with a syn-chronous grammar composed with an n-gram lan-guage model[REF_CITE]—but our footnote 10 suggests that BP might incorporate a language model rapidly.
String alignment with synchronous grammars is quite expensive even for simple syn-chronous formalisms like ITG[REF_CITE]—but
"Finally, we can take advantage of improvements to BP proposed in the context of other applications."
"For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue[REF_CITE]. 31"
We explore a stacked framework for learn-ing to predict dependency structures for natu-ral language sentences.
"A typical approach in graph-based dependency parsing has been to assume a factorized model, where local fea-tures are used but a global function is opti-mized[REF_CITE]."
"We show that this is an example of stacked learning, in which a second pre-dictor is trained to improve the performance of the first."
"Further, we argue that this tech-nique is a novel way of approximating rich non-local features in the second parser, with-out sacrificing efficient, model-optimal pre-diction."
Experiments on twelve languages show that stacking transition-based and graph-based parsers improves performance over ex-isting state-of-the-art dependency parsers.
In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning[REF_CITE].
"This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing[REF_CITE]are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for non-projective dependency parsing[REF_CITE]. • Feature-rich parsers must resort to search or greediness,[REF_CITE], so that parsing solutions are inexact and learned models may be subject to certain kinds of bi[REF_CITE]."
A solution that leverages the complementary strengths of these two approaches—described in de-tail[REF_CITE]—was recently and successfully explored[REF_CITE].
Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers.
"We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s fea-ture space."
"Specifically, we view stacked learning as a way of approximating non-local features in a lin-ear model, rather than making empirically dubious independence[REF_CITE]or structural assumptions (e.g., projectivity,[REF_CITE]), using search approximations[REF_CITE], solving a (generally NP-hard) integer linear program[REF_CITE], or adding latent variables[REF_CITE]."
"Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser."
"Related approaches are the belief propagation algo-rithm[REF_CITE], and the “trading of structure for features” explored by Liang et al."
Figure 2: Non-projective dependency graph.
"Figure Figure 1: A projective 2: Non-projective dependency dependency parse (top) graph , and a . non-projective those that dependency assume each parse dependency (bottom) decision for two English is in-sentences dependent ; examples modulo from the McDonald global structural and[REF_CITE]. those that dependency that assume graphs each dependency must be trees decision ."
Such mod- is in-dependent modulo the global structural constraint (2008 that els ) are . dependency commonly graphs referred must to as be edge-factored trees.
Such since mod-
"This their paper parameters focuses factor on relative dependency to individual parsing, edges which els of are the commonly graph (Paskin referred , 2001 to as ; edge-factored McDonald et since al., has their 2005a become parameters )."
"Edge-factored widely factor used models relative in relation have to individual many extraction computa- edges (Cu-lotta of tional and the benefits graph Sorensen , most (Paskin , notably 2004 , 2001 ), that ; machine McDonald inference translation for et non- al., ([REF_CITE]projective and )."
"Palmer Edge-factored dependency , 2005) models , graphs question have can answering be many achieved computa- (Wang in et tional polynomial al., 2007 benefits ), time and , most (McDonald many notably other et that al. NLP inference , 2005b applications )."
The for non- pri- .
"We projective mary show problem that dependency stacking in treating methods graphs each can dependency outperform be achieved as the in- ap- in proximate polynomial dependent “second-order is time that (McDonald it is ” not parser a et realistic al., of 2005b McDonald assumption )."
The pri- and .
"Pereira mary Non-local ( problem 2006 information ) on in twelve treating , languages such each as dependency arity and (or can valency be as used in- ) within dependent and that neighbouring approximation is that it dependencies is not to a achieve realistic , can even be assumption crucial better to re- . sults Non-local obtaining ."
"These high results information parsing are similar , accuracies such in as spirit arity (Klein to (or and (Nivre valency Man- and ) McDonald and ning, neighbouring 2002 , 2008 ; McDonald ), but dependencies with and the Pereira following , can , 2006 be crucial novel )."
"How- con- to tributions obtaining ever, in : the high data-driven parsing accuracies parsing setting (Klein this and can Man- be ning partially , 2002 adverted ;"
"McDonald by incorporating and Pereira rich , 2006 feature )."
"How- rep- • a ever resentations stacking , in the interpretation over data-driven the input parsing , (McDonald setting et al this ., 2005a can be ). • a partially richer The goal feature adverted of this set by that work incorporating includes is to further non-local rich feature our current features rep- ( resentations understanding shown here over to of improve the the input computational performance (McDonald nature et ), al and ., of 2005a non- ). • a projective variety The goal of parsing of stacking this algorithms work architectures is to for further both . learning our current and understanding inference within of the the data-driven computational setting nature ."
We start of non- by
"Using projective investigating stacking parsing and with extending algorithms rich features the for edge-factored , both we obtain learning model results and competitive inference of McDonald within with et Nivre the al. ( data-driven 2005b and McDonald )."
"In setting particular . (2008 We , we start ) while ap- by preserving investigating peal to the the Matrix and fast extending Tree quadratic Theorem the parsing edge-factored for multi-digraphs time model of arc-factored of to McDonald design spanning polynomial-time et tree al. (2005b algorithms algorithms )."
"In . particular for calculat- , we ap-"
The peal ing both to paper the the Matrix is organized partition Tree function Theorem as follows and for . multi-digraphs edge We discuss expecta- re-lated to tions prior design over work all polynomial-time possible on dependency dependency algorithms parsing graphs for and for calculat- a stacking given in §2 ing sentence .
Our both . model the To partition motivate is given function these in §3 algorithms .
"A and novel edge , analysis we expecta- show of stacking tions that they over in linear can all possible be models used dependency in is many given important in graphs §4."
Experiments for learning a given are sentence and inference .
"To motivate problems §5 these including algorithms min-risk , we decod- show that ing presented , they training can in globally be . used normalized in many important log-linear learning mod- 2 and els Background , syntactic inference language problems and Related modeling including , Work and min-risk unsupervised decod-ing, training globally normalized log-linear mod-"
"We els briefly , syntactic review language work modeling on the NLP , and task unsupervised of depen-dency parsing and the machine learning framework known as stacked learning. learning via the EM algorithm – none of which have previously learning 2.1 Dependency via been the known EM algorithm Parsing to have exact – none non-projective of which have implementations previously Dependency been . known syntax to is have a lightweight exact non-projective syntactic rep-implementations resentation We then switch that . focus models to models a sentence that account as a graph for where non-local the We words then information switch are vertices focus , in particular to and models syntactic arity that and relationships account neigh- for are bouring non-local directed parse information edges decisions (arcs , . in ) For connecting particular systems arity that heads model and to neigh- their ar- argu-ity bouring constraints parse we decisions give a reduction ."
"For systems from that the Hamilto- model ar-nian ity ments constraints graph and problem modifiers we give suggesting a . reduction that from the parsing the Hamilto- prob-lem nian Dependency is graph intractable problem in parsing suggesting this case is . that often For the neighbouring viewed parsing prob- computa-parse lem tionally is decisions intractable as a , structured we in extend this case prediction the . work For of problem neighbouring McDonald : for each and parse input Pereira decisions sentence (2006 , we ) x and , extend with show n the words that work modeling , exponentially of McDonald vertical many neighbourhoods and candidate Pereira ( dependency 2006 makes ) and parsing show trees that intractable y ∈ modeling Y(x) are in vertical addi- possible in tion neighbourhoods principle to modeling ."
"We horizontal makes denote parsing each neighbourhoods tree intractable by its . set in A con- of addi- vertices sequence tion and to directed modeling of these arcs horizontal results , y = is neighbourhoods that (V y , it A is y ) unlikely ."
"A . legal A that con- depen-exact sequence dency non-projective tree of these has n results + dependency 1 vertices is that parsing , it each is unlikely corresponding is tractable that to for exact one any non-projective word model plus assumptions a “wall dependency ” weaker symbol parsing than , $, assumed those is tractable made to be the by for hidden the any edge-factored model root assumptions of the models sentence . weaker ."
"In than a valid those dependency made 1 by .1 tree the Related , edge-factored each vertex Work except models the . root has exactly one par-ent."
"In the projective case, arcs cannot cross when There 1. depicted 1 Related has been on Work one extensive side work of the on sentence data-driven ; in de- the non-pendency parsing for both projective parsing (Eis-ner There projective , 1996 has ; Paskin been case extensive , , 2001 this ; constraint Yamada work on and is data-driven Matsumoto not imposed de- , (see 2003 pendency Fig ; ."
"Nivre 1). parsing and Scholz for both , 2004 projective ; McDonald parsing et ( al Eis- ., 2005a ner 2. , 1 1996 . ) 1 and ; Graph-based Paskin non-projective , 2001; vs Yamada parsing . transition-based and systems Matsumoto (Nivre models , and 2003 Nilsson ; Nivre , 2005 and ; Scholz Hall and , 2004 Nóvák ; McDonald , 2005; McDon- et al., ald 2005a et Most al ) . and , 2005b recent non-projective ). work These on approaches parsing dependency systems can parsing often (Nivre be can be classified and categorized Nilsson into , 2005 two as broad graph-based ; Hall categories and Nóvák or . ,"
In transition-based 2005 the ; first McDon- cat- .
"In egory ald graph-based et are al., those 2005b methods parsing )."
"These , that dependency approaches employ approximate can trees often are be scored inference classified by factoring , into typically two the broad through tree categories into the its use . arcs of In linear the , and first time parsing cat- is shift-reduce egory performed are those parsing by methods searching algorithms that for employ ( the Yamada highest approximate and scoring Mat- tree sumoto inference (Eisner , 2003 , 1996 typically ; Nivre ; McDonald through and Scholz the et , al use 2004 ., 2005b of ; Nivre linear )."
"Transition- and time Nilsson shift-reduce based , 2005 parsers parsing )."
"In model the algorithms second the sequence category (Yamada of are and decisions those Mat- of that sumoto a shift-reduce employ , 2003 exhaustive ; Nivre parser and inference , given Scholz previous , algorithms 2004; Nivre decisions , usu- and and ally Nilsson current by making , 2005 state ) strong , . and In the independence parsing second is category performed assumptions are by those , as greedily is that choosing the employ case for the exhaustive edge-factored highest inference scoring models transition algorithms ([REF_CITE]out , usu- ; of each McDonald ally successive by making et al parsing . strong , 2005a independence state ; McDonald or by searching et assumptions al., 2005b for , ) the . as best Recently is sequence the case there for of have edge-factored transitions also been ( models proposals Ratnaparkhi (Paskin for exhaus- et , 2001 al., ; 1994; tive McDonald Yamada methods and et that al."
"Matsumoto , weaken 2005a; the McDonald edge-factored , 2003; Nivre et al. assump- , 2005b et al., ). 2004; tion Recently Sagae , including and there Lavie both have approximate , also 2005 been ; Hall proposals methods et al., 2006 for (McDon- exhaus- ). ald tive and methods Both Pereira approaches that , 2006 weaken ) and most exact the commonly edge-factored methods through use assump- linear in- mod-teger tion, including linear programming both approximate (Riedel and methods Clarke (McDon- , 2006) or ald els branch-and-bound and to Pereira assign , 2006 scores algorithms ) and to exact arcs ( methods Hirakawa or decisions through , 2006 , so ) in- . that a work teger For scorelearned on linear grammar is empirical weighta programming dot-product basedsystems vector models w ( for Riedel of. therenon-projective a feature and has Clarke been vector limited , pars- 2006 f ) and a ing or Forbranch-and-bound systems In grammar sum , , notable these based two exceptions algorithmsmodels lines there of ( include Hirakawa research has been the , use 2006limited work different ). of work approximations Wang on and empirical Harper systems to (2004 achieve ). for Theoretical non-projective tractability studies ."
"Transition- pars- of note based systems include approaches , the notable work of solve exceptions Neuhaus a sequence and include Böker of the (1997 local work ) prob-showing ingof lems Wang in that and sequence the Harper recognition , sacrificing (2004). problem Theoretical global for optimality studies a mini- of guar-note antees include and the possibly work of expressive Neuhaus and power Böker ([REF_CITE]et al., showing 1999). that Graph-based the recognition methods problem perform for a mini- global in-ference using score factorizations that correspond to strong independence assumptions (discussed in §2.1.2)."
"Recently,[REF_CITE]pro-posed combining a graph-based and a transition-based parser and have shown a significant improve-ment for several languages by letting one of the parsers “guide” the other."
Our stacked formalism (to be described in §3) generalizes this approach.
"In the successful graph-based method[REF_CITE], an arc factorization independence assumption is used to ensure tractability."
"This as-sumption forbids any feature that depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a ∈"
A y and on the input sequence x).
"This in-duces a decomposition of the feature vector f(x, y) as:"
"f(x,y) = X f a (x). a∈A y"
"Parsing amounts to solving arg max y∈Y(x) w &gt; f(x,y), where w is a weight vector."
"With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming[REF_CITE], and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists[REF_CITE], as shown[REF_CITE]."
"In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime[REF_CITE], but not in the nonprojective case[REF_CITE], where finding the highest-scoring tree becomes NP-hard."
In §3 we adopt a framework that maintains O(n 2 ) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features.
Stacked generalization was first proposed[REF_CITE]and[REF_CITE]for regression.
The idea is to include two “levels” of predictors.
"The first level, “level 0,” includes one or more predictors g 1 , . . . , g K : R d → R; each receives input x ∈ R d and outputs a prediction g k (x)."
"The second level, “level 1,” consists of a single function h :"
"R d+K → R that takes as input hx, g 1 (x), . . . g K (x)i and out-puts a final prediction ŷ = h(x, g 1 (x), . . . g K (x))."
"The predictor, then, combines an ensemble (the g k ) with a meta-predictor (h)."
"Training is done as follows: the training data are split into L partitions, and L instances of the level 0 predictor are trained in a “leave-one-out” basis."
"Then, an augmented dataset is formed by letting each instance output predictions for the partition that was left out."
"Finally, each level 0 predictor is trained using the original dataset, and the level 1 predictor is trained on the augmented dataset, simulating the test-time setting when h is applied to a new instance x concatenated with hg k (x)i k ."
"This framework has also been applied to classifi-cation, for example with structured data."
Some ap-plications (including here) use only one classifier at level 0; recent work includes sequence labeling (Co-hen and de[REF_CITE]) and inference in condi-tional random fields[REF_CITE].
Stack-ing is also intuitively related to transformation-based learning[REF_CITE].
"We next describe how to use stacked learning for efficient, rich-featured dependency parsing."
The architecture consists of two levels.
At level 0 we include a single dependency parser.
"At runtime, this “level 0 parser” g processes an input sentence x and outputs the set of predicted edges that make up its estimation of the dependency tree, ŷ 0 = g(x)."
"At level [URL_CITE], we apply a dependency parser—in this work, always a graph-based dependency parser—that uses basic factored features plus new ones from the edges predicted by the level 0 parser."
"The final parser pre-dicts parse trees as h(x, g(x)), so that the total run-time is additive in calculating h(·) and g(·)."
The stacking framework is agnostic about the form of g and h and the methods used to learn them from data.
"In this work we use two well-known, publicly available dependency parsers, MSTParser[REF_CITE], 1 which implements ex- act first-order arc-factored nonprojective parsing (§2.1.2) and approximate second-order nonprojec-tive parsing, and MaltParser[REF_CITE], which is a state-of-the-art transition-based parser. 2 We do not alter the training algorithms used in prior work for learning these two parsers from data."
"Us-ing the existing parsers as starting points, we will combine them in a variety of ways."
"Regardless of our choices for the specific parsers and learning algorithms at level 0 and level 1, training is done as sketched in §2.2."
"Let D be a set of training examples {hx i , y i i} i . 1."
"Split training data D into L partitions D 1 , . . . , D L . [URL_CITE]."
"Train L instances of the level 0 parser in the fol-lowing way: the l-th instance, g l , is trained on D −l ="
D \ D l .
Then use g l to output predic-tions for the (unseen) partition D l .
"At the end, an augmented dataset D̃ = S Ll=1 D̃ l is built, so that D̃ = {hx i , g(x i ), y i i} i . [Footnote_3]. Train the level 0 parser g on the original training data D. 4. Train the level 1 parser h on the augmented train-ing data D̃."
3 We use ^ to denote vector concatenation.
"The runtime of this algorithm is O(LT 0 +T 1 ), where T 0 and T 1 are the individual runtimes required for training level 0 and level 1 alone, respectively."
We next describe two motivations for stacking parsers: as a way of augmenting the features of a graph-based dependency parser or as a way to ap-proximate higher-order models.
Suppose that the level 1 classifier is an arc-factored graph-based parser.
"The feature vectors will take the form 3 f(x, y) = f 1 (x, y) ^ f 2 (x, ŷ 0 , y) = X f 1,a (x) ^ f 2,a (x, g(x)), a∈A y where f 1 (x, y)"
"P = a∈A y f 1,a (x) are regu- lar arc-factored features, and f 2 (x, ŷ 0 , y) = a∈A y f 2,a (x,g(x)) are the stacked features."
"An P example of a stacked feature is a binary feature f 2,a (x, g(x)) that fires if and only if the arc a was predicted by g, i.e., if a ∈ A g(x) ; such a feature was used[REF_CITE]."
"It is difficult in general to decide whether the in-clusion of such a feature yields a better parser, since features strongly correlate with each other."
"How-ever, a popular heuristic for feature selection con-sists of measuring the information gain provided by each individual feature."
"In this case, we may obtain a closed-form expression for the information gain that f 2,a (x, g(x)) provides about the existence or not of the arc a in the actual dependency tree y."
Let A and A 0 be binary random variables associated with the events a ∈
A y and a 0 ∈
"A g(x) , respectively."
"We have: 0 ) I(A; A 0 ) = X p(a, a 0 ) log 2 p(a, a p(a)p(a 0 ) a,a 0 ∈{0,1} = H(A 0 ) − X p(a)H(A 0 |A = a). a∈{0,1}"
"Assuming, for simplicity, that at level 0 the prob-ability of false positives equals the probability of false negatives (i.e., P err , p(a 0 = 0|a = 1) = p(a 0 = 1|a = 0)), and that the probability of true positives equals the probability of true negatives (1 − P err = p(a 0 = 0|a = 0) = p(a 0 = 1|a = 1)), the expression above reduces to:"
"I(A; A 0 ) = H(A 0 ) + P err log 2 P err + (1 − P err ) log 2 (1 − P err ) = H(A 0 ) − H err , where H err denotes the entropy of the probability of error on each arc’s prediction by the level 0 classi-fier."
"If P err ≤ 0.5 (i.e. if the level 0 classifier is better than random), then the information gain pro-vided by this simple stacked feature increases with (a) the accuracy of the level 0 classifier, and (b) the entropy H(A 0 ) of the distribution associated with its arc predictions."
"Another way of interpreting the stacking framework is as a means to approximate a higher order model, such as one that is not arc-factored, by using stacked features that make use of the predicted structure around a candidate arc."
"Consider a second-order model where the features decompose by arc and by arc pair:   f(x,y) ="
"X  f a 1 (x) ^ X f a 1 ,a 2 (x)  . a 1 ∈A y a 2 ∈A y"
"Exact parsing under such model, with arbitrary second-order features, is intractable[REF_CITE]."
Let us now consider a stacked model in which the level 0 predictor outputs a parse ŷ.
"At level 1, we use arc-factored features that may be written as   f̃(x,y) ="
"X  f a 1 (x) ^ X f a 1 ,a 2 (x)  ; a 1 ∈A y a 2 ∈A ŷ this model differs from the previous one only by re-placing A y by A ŷ in the index set of the second sum-mation."
"Since ŷ is given, this makes the latter model arc-factored, and therefore, tractable."
"We can now view f̃(x, y) as an approximation of f(x, y); indeed, we can bound the score approximation error, ∆s(x, y) = w̃ &gt; f̃(x, y) − w &gt; f(x, y) , where w̃ and w stand respectively for the parameters learned for the stacked model and those that would be learned for the (intractable) exact second order model."
"We can bound ∆s(x,y) by spliting it into two terms: ∆s(x, y) = (w̃ − w) &gt; f̃(x, y) + w &gt; (f̃(x, y) − f(x, y)) ≤ (w̃ − w) &gt; f̃(x, y) + w &gt; (f̃(x, y) − f(x, y)) ; | {z } | {z } ,∆s tr (x,y) ,∆s dec (x,y) where we introduced the terms ∆s tr and ∆s dec that reflect the portion of the score approximation error that are due to training error (i.e., different parame-terizations of the exact and approximate models) and decoding error (same parameterizations, but differ-ent feature vectors)."
"Using Hölder’s inequality, the former term can be bounded as: ∆s tr (x, y) = (w̃ − w) &gt; f̃(x, y) ≤ kw̃ − wk 1 · kf̃(x, y)k ∞ ≤ kw̃ − wk 1 ; where k.k 1 and k.k ∞ denote the ` 1 -norm and sup-norm, respectively, and the last inequality holds when the features are binary (so that kf̃(x, y)k ∞ ≤ 1)."
The proper way to bound the term kw̃ − wk 1 depends on the training algorithm.
"As for the de-coding error term, it can bounded for a given weight vector w, sentence x, candidate tree y, and level 0 prediction ŷ."
"Decomposing the weighted vector as w = w 1 ^ w 2 , w 2 being the sub-vector associ-ated with the second-order features, we have respec-tively: ∆s dec (x, y) = w &gt; (f̃(x, y) − f(x, y))   ="
"X w 2&gt;  X f a 1 ,a 2 (x) − X f a 1 ,a 2 (x)  a 1 ∈A y a 2 ∈A ŷ a 2 ∈A y w &gt;2 f a 1 ,a 2 (x) X X ≤ a 1 ∈A y a 2 ∈A ŷ ∆A y ≤ X |A ŷ ∆A y | · max w 2&gt; f a 1 ,a 2 (x) a 2 ∈A ŷ ∆A y a 1 ∈A y = X 2L(y,ŷ) · max w &gt;2 f a 1 ,a 2 (x) , a 2 ∈A ŷ ∆A y a 1 ∈A y where A ŷ ∆A y , (A ŷ − A y ) ∪ (A y − A ŷ ) denotes the symmetric difference of the sets A ŷ and A y , which has cardinality 2L(y, ŷ), i.e., twice the Ham-ming distance between the sequences of heads that characterize y and the predicted parse ŷ."
"Using Hölder’s inequality, we have both w 2&gt; f a 1 ,a 2 (x) ≤ kw 2 k 1 · kf a 1 ,a 2 (x)k ∞ and w 2&gt; f a 1 ,a 2 (x) ≤ kw 2 k ∞ · kf a 1 ,a 2 (x)k 1 ."
"Assuming that all features are binary valued, we have that kf a 1 ,a 2 (x)k ∞ ≤ 1 and that kf a 1 ,a 2 (x)k 1 ≤ N f,2 , where N f,2 denotes the maximum number of active second order features for any possible pair of arcs (a 1 , a 2 )."
"Therefore: ∆s dec (x, y) ≤ 2nL(y, ŷ) min{kw 2 k 1 , N f,2 ·kw 2 k ∞ }, where n is the sentence length."
"Although this bound can be loose, it suggests (intuitively) that the score approximation degrades as the predicted tree ŷ gets farther away from the true tree y (in Hamming dis-tance)."
"It also degrades with the magnitude of weights associated with the second-order features, which suggests that a separate regularization of the first-order and stacked features might be beneficial in a stacking framework."
"As a side note, if we set each component of the weight vector to one, we obtain a bound on the ` 1 -norm of the feature vector difference, f̃(x, y) − f(x, y) ≤ 2nL(y, ŷ)N f,2 . 1"
In the following experiments we demonstrate the ef-fectiveness of stacking parsers.
"As noted in §3.1, we make use of two component parsers, the graph-based MSTParser and the transition-based MaltParser."
The publicly available version of MSTParser per-forms parsing and labeling jointly.
"We adapted this system to first perform unlabeled parsing, then la-bel the arcs using a log-linear classifier with access to the full unlabeled parse ([REF_CITE];"
"In stacking experiments, the arc labels from the level 0 parser are also used as a feature. [Footnote_4]"
"4 We made other modifications to MSTParser, implement-ing many of the successes described[REF_CITE]. Our version of the code is publicly available[URL_CITE]The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing mor-phology/word and morphology/lemma features with morphol-ogy/POS features."
"In the following subsections, we refer to our mod-ification of the MSTParser as MST 1O (the arc-factored version) and MST 2O (the second-order arc-pair-factored version)."
All our experiments use the non-projective version of this parser.
We refer to the MaltParser as Malt.
"We report experiments on twelve languages from the CoNLL-X shared task[REF_CITE]. [Footnote_5] All experiments are evaluated using the labeled attachment score (LAS), using the default settings. [URL_CITE] Statistical significance is measured us-ing Dan Bikel’s randomized parsing evaluation com-parator with 10,000 iterations. [Footnote_7] The additional fea-tures used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Ta-ble 2."
"5 The CoNLL-X shared task actually involves thirteen lan-guages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen languages."
"The PredEdge features are exactly the six fea-tures used[REF_CITE]in their MST Malt parser; therefore, feature set A is a repli-cation of this parser except for modifications noted in footnote 4."
"In all our experiments, the number of partitions used to create D̃ is L = 2."
Our first experiment stacks the highly accurate MST 2O parser with itself.
"At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (Table 2)."
The results are shown in Table 3.
"While we see improvements over the single-parser baseline for nine languages, the improvements are small (less than 0.5%)."
"One of the biggest concerns about this model is the fact that it stacks two predictors that are very similar in nature: both are graph-based and share the features f 1,a (x)."
"It has been pointed out[REF_CITE], among others, that the success of ensemble methods like stacked learning strongly de-pends on how uncorrelated the individual decisions made by each predictor are from the others’ deci-sions. [Footnote_8] This experiment provides further evidence for the claim."
"8 This claim has a parallel in the cotraining method[REF_CITE], whose performance is bounded by the de-gree of independence between the two feature sets."
We next use MaltParser at level 0 and the second-order arc-pair-factored MST 2O at level 1.
"This extends the experiments[REF_CITE], replicated in our feature subset A."
Table 4 enumerates the results.
"Note that the best-performing stacked configuration for each and every language outperforms MST 2O , corroborat-ing results reported[REF_CITE]."
"The best performing stacked configuration outper-forms Malt as well, except for Japanese and Turk-ish."
"Further, our non-arc-factored features largely outperform subset A, except on Bulgarian, Chinese, and Japanese."
"On average, the best feature config-uration is E, which is statistically significant over Malt and MST 2O with p &lt; 0.0001, and over fea-ture subset A with p &lt; 0.01."
"Finally, we consider stacking MaltParser with the first-order, arc-factored MSTParser."
"We view this approach as perhaps the most promising, since it is an exact parsing method with the quadratic runtime complexity of MST 1O ."
Table 5 enumerates the results.
"For all twelve languages, some stacked configuration outperforms MST 1O and also, surprisingly, MST 2O , the sec-ond order model."
"This provides empirical evi-dence that using rich features from MaltParser at level 0, a stacked level 1 first-order MSTParser can outperform the second-order MSTParser. [Footnote_9]"
"9 Recall that MST 2O uses approximate search, as opposed to stacking, which uses approximate features."
"In only two cases (Japanese and Turkish), the MaltParser slightly outperforms the stacked parser."
"On average, feature configuration D performs the best, and is statistically significant over Malt, MST 1O , and MST 2O with p &lt; 0.0001, and over feature subset"
A with p &lt; 0.05.
"Encouragingly, this configuration is barely outperformed by configura- tion A of Malt + MST 2O (see Table 4), the dif-ference being statistically insignificant (p &gt; 0.05)."
"This shows that stacking Malt with the exact, arc-factored MST 1O bridges the difference between the individual MST 1O and MST 2O models, by approx-imating higher order features, but maintaining an O(n 2 ) runtime and finding the model-optimal parse."
"In pipelines or semisupervised settings, it is use-ful when a parser can provide a confidence measure alongside its predicted parse tree."
"Because stacked predictors use ensembles with observable outputs, differences among those outputs may be used to es-timate confidence in the final output."
"In stacked de-pendency parsing, this can be done (for example) by measuring the Hamming distance between the out-puts of the level 0 and 1 parsers, L(g(x), h(x))."
"In-deed, the bound derived in §4.2 suggests that the second-order approximation degrades for candidate parses y that are Hamming-far from g(x); therefore, if L(g(x), h(x)) is large, the best score s(x, h(x)) may well be “biased” due to misleading neighbor-ing information provided by the level 0 parser."
"We illustrate this point with an empirical analysis of the level 0/1 disagreement for the set of exper-iments described in §5.3; namely, we compare the level 0 and level 1 predictions under the best overall configuration (configuration E of Malt + MST 2O )."
"Figure 2 depicts accuracy as a function of level 0-level 1 disagreement (in number of tokens), aver-aged over all datasets."
"We can see that performance degrades steeply when the disagreement between levels 0 and 1 in-creases in the range 0–4, and then behaves more ir-regularly but keeping the same trend."
"This suggests that the Hamming distance L(g(x),h(x)) is infor-mative about parser performance and may be used as a confidence measure."
"In this work, we made use of stacked learning to im-prove dependency parsing."
"We considered an archi-tecture with two layers, where the output of a stan-dard parser in the first level provides new features for a parser in the subsequent level."
"During learning, the second parser learns to correct mistakes made by the first one."
The novelty of our approach is in the exploitation of higher-order predicted edges to simu-late non-local features in the second parser.
"We pro-vided a novel interpretation of stacking as feature approximation, and our experimental results show rich-featured stacked parsers outperforming state-of-the-art single-layer and ensemble parsers."
"No- tably, using a simple arc-factored parser at level 1, we obtain an exact O(n 2 ) stacked parser that outper-forms earlier approximate methods[REF_CITE]."
"The authors thank the anonymous reviewers for helpful comments, Vitor Carvalho, William Cohen, and David Smith for interesting discussions, and Ryan McDonald and Joakim Nivre for providing us their code and preprocessed datasets."
A.M. was supported by a grant from FCT through the CMU-Portugal Program and the Information and Com-munications Technologies Institute (ICTI) at CMU.
N.S. was supported by NSF[REF_CITE]and an IBM faculty award.
E.X. was supported by NSF[REF_CITE]and[REF_CITE].
We present a study on how grammar binariza-tion empirically affects the efficiency of the CKY parsing.
"We argue that binarizations af-fect parsing efficiency primarily by affecting the number of incomplete constituents gener-ated, and the effectiveness of binarization also depends on the nature of the input."
We pro-pose a novel binarization method utilizing rich information learnt from training corpus.
"Ex-perimental results not only show that differ-ent binarizations have great impacts on pars-ing efficiency, but also confirm that our learnt binarization outperforms other existing meth-ods."
Furthermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance.
"Binarization, which transforms an n-ary grammar into an equivalent binary grammar, is essential for achieving an O(n 3 ) time complexity in the context-free grammar parsing."
"O(n 3 ) tabular parsing al-gorithms, such as the CKY algorithm[REF_CITE], the GHR parser[REF_CITE], the Earley algorithm[REF_CITE]and the chart parsing algorithm[REF_CITE]all convert their grammars into bi-nary branching forms, either explicitly or implicitly[REF_CITE]."
"In fact, the number of all possible binarizations of a production with n + 1 symbols on its right hand side is¡ known¢ to be the nth Catalan Number C n = n+11 2nn ."
"All binarizations lead to the same parsing accuracy, but maybe different parsing effi-ciency, i.e. parsing speed."
We are interested in in-vestigating whether and how binarizations will af-fect the efficiency of the CKY parsing.
Do different binarizations lead to different pars-ing efficiency?
Figure 1 gives an example to help answer this question.
Figure 1(a) illustrates the cor-rect parse of the phrase “get the bag and go”.
We assume that NP → NP CC NP is in the original grammar.
The symbols enclosed in square brackets in the figure are intermediate symbols.
"If a left binarized grammar is used, see Fig-ure 1(b), an extra constituent [NP CC] spanning “the bag and” will be produced."
Because rule [NP CC] → NP CC is in the left binarized gram-mar and there is an NP over “the bag” and a CC over the right adjacent “and”.
"Having this con-stituent is unnecessary, because it lacks an NP to the right to complete the production."
"However, if a right binarization is used, as shown in Figure 1(c), such unnecessary constituent can be avoided."
"One observation from this example is that differ-ent binarizations affect constituent generation, thus affect parsing efficiency."
"Another observation is that for rules like X → Y CC Y , it is more suitable to binarize them in a right branching way."
"This can be seen as a linguistic nature: for “and”, usually the right neighbouring word can indicate the correct parse."
A good binarization should reflect such ligu-istic nature.
"In this paper, we aim to study the effect of bina-rization on the efficiency of the CKY parsing."
"To our knowledge, this is the first work on this problem."
We propose the problem to find the optimal bina-rization in terms of parsing efficiency (Section 3).
"We argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituents generated, and the effectiveness of bi-narization also depends on the nature of the input (Section 4)."
Therefore we propose a novel binariza-tion method utilizing rich information learnt from training corpus (Section 5).
Experimental results show that our binarization outperforms other exist-ing methods (Section 7.2).
"Since binarization is usually a preprocessing step before parsing, we argue that better performance can be achieved by combining other parsing speed-up techniques with our binarization (Section 6)."
We conduct experiments to confirm this (Section 7.3).
"In this paper we assume that the original gram-mar, perhaps after preprocessing, contains no ²-productions or useless symbols."
"However, we allow the existence of unary productions, since we adopt an extended version of the CKY algorithm which can handle the unary productions."
Moreover we do not distinguish nonterminals and terminals explic-itly.
We treat them as symbols.
What we focus on is the procedure of binarization.
"A binarization is a function π, map-ping an n-ary grammar G to an equivalent binary grammar G 0 ."
"We say that G 0 is a binarized grammar of G, denoted as π(G)."
Two grammars are equivalent if they define the same probability distribution over strings[REF_CITE].
"We use the most widely used left binarizati[REF_CITE]to show the procedure of binarization, as illustrated in Table 1, where p and q are the probabilities of the productions."
"In the binarized grammar, symbols of form [A B] are new (also called intermediate) nonterminals."
Left binarization always selects the left most pair of symbols and combines them to form an intermedi-ate nonterminal.
This procedure is repeated until all productions are binary.
"In this paper, we assume that all binarizations fol-low the fashion above, except that the choice of pair of symbols for combination can be arbitrary."
Next we show three other known binarizations.
"Right binarization is almost the same with left binarization, except that it always selects the right most pair, instead of left, to combine."
Head binarization always binarizes from the head outward[REF_CITE].
Please refer[REF_CITE]for more details.
Compact binarizati[REF_CITE]tries to minimize the size of the binarized grammar.
It leads to a compact grammar.
We therefore call it compact binarization.
It is done via a greedy approach: it al-ways selects the pair that occurs most on the right hand sides of rules to combine.
The optimal binarization should help CKY parsing to achieve its best efficiency.
We formalize the idea as follows: Definition 2.
"The optimal binarization is π ∗ , for a given n-ary grammar G and a test corpus C: π ∗ = arg min T (π(G), C) (1) π where T(π(G), C) is the running time for CKY to parse corpus C, using the binarized grammar π(G)."
It is hard to find the optimal binarization directly from Definition 2.
We next give an empirical anal-ysis of the running time of the CKY algorithm and simplify the problem by introducing assumptions.
It is known that the complexity of the CKY algo-rithm is O(n 3 L).
The constant L depends on the bi- narized grammar in use.
Therefore binarization will affect L.
Our goal is to find a good binarization that makes parsing more efficient.
"It is also known that in the inner most loop of CKY as shown in Algorithm 1, the for-statement in Line 1 can be implemented in several different meth-ods."
The choice will affect the efficiency of CKY.
We present here four possible methods:
"M1 Enumerate all rules X → Y Z, and check if Y is in left span and Z in right span."
"M2 For each Y in left span, enumerate all rules X → Y Z, and check if Z is in right span."
"M3 For each Z in right span, enumerate all rules X → Y Z, and check if Y is in left span."
"M4 Enumerate each Y in left span and Z in right span 1 , check if there are any rules X → Y Z."
"The inner most loop of CKY [Footnote_1]: for X → Y Z, Y in left span and Z in right span [Footnote_2]: Add X to parent span"
1 Note that we should skip Y (Z) if it never appears as the first (second) symbol on the right hand side of any rule.
"2 More precisely, it is more than a parse tree for it contains all symbols recognized in parsing."
We have shown that both binarization and the for-statement implementation in the inner most loop of CKY will affect the parsing speed.
"About the for-statement implementations, no pre-vious study has addressed which one is superior."
The actual choice may affect our study on binariza-tion.
"If using M1, since it enumerates all rules in the grammar, the optimal binarization will be the one with minimal number of rules, i.e. minimal bi-narized grammar size."
"However, M1 is usually not preferred in practice[REF_CITE]."
"For other methods, it is hard to tell which binarization is op-timal theoretically."
"In this paper, for simplicity rea-sons we do not consider the effect of for-statement implementations on the optimal binarization."
"On the other hand, it is well known that reduc-ing the number of constituents produced in parsing can greatly improve CKY parsing efficiency."
That is how most thresholding systems[REF_CITE]speed up CKY parsing.
"Apparently, the number of constituents produced in parsing is not affected by for-statement implementations."
Therefore we assume that the running time of CKY is primarily determined by the number of con-stituents generated in parsing.
"We simplify the opti-mal binarization to be: π ∗ ≈ arg min E(π(G), C) (2) π where E(π(G),C) is the number of constituents generated when CKY parsing C with π(G)."
"We next discuss how binarizations affect the num-ber of constituents generated in parsing, and present our algorithm for finding a good binarization."
"Throughout this section and the next, we will use an example to help illustrate the idea."
The grammar is:
X → ABCD Y → ABC C → CD Z → ABCE W → F CDE
The input sentence is 0
"A 1 B 2 C 3 D 4 E 5 , where the subscripts are used to indicate the positions of spans."
"For example, [1, 3] stands for B C. The final parse 2 is shown in Figure 2."
"Symbols surrounded by dashed circles are fictitious, which do not actually exist in the parse."
"In the procedure of CKY parsing, there are two kinds of constituents generated: complete and incomplete."
Complete constituents (henceforth CCs) are those composed by the original grammar symbols and spans.
"For example in Figure 2, X :[0, 4], Y :[0, 3] and Y :[0, 4] are all CCs."
Incomplete constituents (henceforth ICs) are those labeled by intermediate symbols.
"Figure 2 does not show them directly, but we can still read the possible ones."
"For example, if the binarized gram-mar in use contains an intermediate symbol [A B C], then there will be two related ICs [A B C]:[0, 3] and [A B C]:[0, 4] (the latter is due to C:[2, 4]) produced in parsing."
ICs represent the intermediate steps to recognize and complete CCs.
Binarizations do not affect whether a CC will be pro-duced.
"If there is a CC in the parse, whatever bi-narization we use, it will be produced."
The differ-ence merely lies on what intermediate ICs are used.
"Therefore given a grammar and an input sentence, no matter what binarization is used, the CKY pars-ing will generate the same set of CCs."
"For example in Figure 2 there is a CC X :[0, 4], which is associated with rule X → AB C D."
"No matter what binarization we use, this CC will be rec-ognized eventually."
"For example if using left bina-rization, we will get [A B]:[0, 2], [A B C]:[0, 3] and finally X:[0, 4]; if using right binarization, we will get [C D]:[2, 4], [B C D]:[1, 4] and again X:[0, 4]."
"Binarizations do affect the generation of ICs, be-cause they generate different intermediate symbols."
We discuss the impact on two aspects:
Some ICs can be used to generate multiple CCs in parsing.
We call them shared.
"If a binarization can lead to more shared ICs, then over-all there will be fewer ICs needed in parsing."
"For example, in Figure 2, if we use left binariza-tion, then [A B]:[0, 2] can be shared to generate both X:[0, 4] and Y : [0, 3], in which we can save one IC overall."
"However, if right binarization is used, there will be no common ICs to share in the generation steps of X:[0, 4] and Y : [0, 3], and overall there are one more IC generated."
"For a CC, if it can be recognized even-tually by applying an original rule of length k, what-ever binarization to use, we will have to generate the same number of k − 2 ICs before we can complete the CC."
"However, if the CC cannot be fully recog- nized but only partially recognized, then the number of ICs needed will be quite different."
"For example, in Figure 2, the rule W → F C D E can be only partially recognized over [2, 5], so it can-not generate the corresponding CC."
"Right binariza-tion needs two ICs ([D E]:[3, 5] and [C D E]:[2, 5]) to find that the CC cannot be recognized, while left binarization needs none."
"As mentioned earlier, ICs are auxiliary means to generate CCs."
"If an IC cannot help generate any CCs, it is totally useless and even harmful."
"We call such an IC failed, otherwise it is successful."
"There-fore, if a binarization can help generate fewer failed ICs then parsing would be more efficient."
Now we show that the impact of binarization also depends on the actual input.
"When the input changes, the impact may also change."
"For example, in the previous example about the rule W → F C D E in Figure 2, we believe that left binarization is better based on the observation that there are more snippets of [C D E] in the in-put which lack for F to the left."
"If there are more snippets of [F C D] in the input lacking for E to the right, then right binarization would be better."
"The discussion above confirms such a view: the effect of binarization depends on the nature of the input language, and a good binarization should re-flect this nature."
This accords with our intuition.
So we use training corpus to learn a good binarization.
And we verify the effectiveness of the learnt bina-rization using a test corpus with the same nature.
"In summary, binarizations affect the efficiency of parsing primarily by affecting the number of ICs generated, where more shared and fewer failed ICs will help lead to higher efficiency."
"Meanwhile, the effectiveness of binarization also depends on the na-ture of its input language."
"Based on the analysis in the previous section, we employ a greedy approach to find a good binariza-tion."
We use training corpus to compute metrics for every possible intermediate symbol.
We use this information to greedily select the best pair to com-bine.
"Given the original grammar G and training corpus C, for every sentence in C, we firstly obtain the final parse (like Figure 2)."
"For every possible intermedi-ate symbol, i.e. every ngram of the original symbols, denoted by w, we compute the following two met-rics: 1. How many ICs labeled by w can be generated in the final parse, denoted by num(w) (number of related ICs). 2. How many CCs can be generated via ICs la-beled by w, denoted by ctr(w) (contribution of related ICs)."
"For example in Figure 2, for a possible inter-mediate symbol [A B C], there are two related ICs ([A B C] : [0, 3] and [A B C] : [0, 4]) in the parse, so we have num([AB C]) = 2."
"Meanwhile, four CCs (Y :[0, 3], X :[0, 4], Y : [0, 4] and Z : [0, 5]) can be generated from the two related ICs."
Therefore ctr([A B C]) = 4.
We list the two metrics for every ngram in Figure 2 in Table 2.
We will discuss how to compute these two metrics in Section 5.2.
The two metrics indicate the goodness of a possi-ble intermediate symbol w: num(w) indicates how many ICs labeled by w are likely to be generated in parsing; while ctr(w) represents how much w can contribute to the generation of CCs.
"If ctr(w) is larger, the corresponding ICs are more likely to be shared."
"If ctr is zero, those ICs are surely failed."
"Therefore the smaller num(w) is and the larger ctr(w) is, the better w would be."
"Combining num and ctr, we define a utility func-tion for each ngram w in the original grammar: utility(w) = f(num(w), ctr(w)) (3) where f is a ranking function, satisfying that f(x, y) is larger when x is smaller and y is larger."
We will discuss more details about it in Section 5.3.
"Using utility as the ranking function, we sort all pairs of symbols and choose the best to combine."
The formal algorithm is as follows:
"S1 For every symbol pair of hv 1 ,v 2 i (where v 1 and v 2 can be original symbols or intermediate symbols generated in previous rounds), let w 1 and w 2 be the ngrams of original symbols represented by v 1 and v 2 , respectively."
Let w = w 1 w 2 be the ngram rep-resented by the symbol pair.
"S2 Select the ngram w with the highest utility(w), let it be w ∗ (in case of a tie, select the one with a smaller num)."
"Let the corresponding symbol pair be hv ∗1 , v ∗2 i."
"S3 Add a new intermediate symbol v ∗ , and replace all the occurrences of hv ∗1 , v 2∗ i on the right hand sides of rules with v ∗ ."
S4 Add a new rule v ∗ → v ∗1 v 2∗ : 1.0.
"S5 Repeat S1 ∼ S4, until there are no rules with more than two symbols on the right hand side."
"In this section, we discuss how to compute num and ctr in details."
Computing ctr is straightforward.
First we get final parses like in Figure 2 for training sentences.
"From a final parse, we traverse along every parent node and enumerate every subsequence of its child nodes."
"For example in Figure 2, from the parent node of X : [0,4], we can enumerate the follow-ing: [A B]:[0, 2], [A B C]:[0, 3], [A B C D]:[0, 4], [B C]:[1, 3], [B C D]:[1, 4], [C D]:[2, 4]."
"We add 1 to all the ctr of these ngrams, respectively."
"To compute num, we resort to the same idea of dynamic programming as in CKY."
We perform a normal left binarization except that we add all ngrams in the original grammar G as intermediate symbols into the binarized grammar G 0 .
"For exam-ple, for the rule of S → A B C : p, the constructed grammar is as follows: [AB] → A B : 1.0 S → [AB] C : p [B C] → B C : 1.0"
"Using the constructed G 0 , we employ a normal CKY parsing on the training corpus and compute how many constituents are produced for each ngram."
The result is num.
"Suppose the length of the train-ing sentence is n, the original grammar G has N symbols, and the maximum length of rules is k, then the complexity of this method can be written as O(N k n 3 )."
We discuss the details of the ranking function f used to compute the utility of each ngram w.
"We come up with two forms for f: linear and log-linear 1. linear: f(x, y) = −λ 1 x + λ 2 y 2. log-linear 3 : f(x, y) = −λ 1 log(x) + λ 2 log(y) where λ 1 and λ 2 are non-negative weights subject to λ 1 + λ 2 = 1 [Footnote_4] ."
"4 Since f is used for ranking, the magnitude is not important."
We will use development set to determine which form is better and to learn the best weight settings.
Binarization usually plays a role of preprocessing in the procedure of parsing.
Grammars are binarized before they are fed into the stage of parsing.
There are many known works on speeding up the CKY parsing.
"So we can expect that if we replace the part of binarization by a better one while keeping the subsequent parsing unchanged, the parsing will be more efficient."
We will conduct experiment to confirm this idea in the next section.
We would like to make more discussions be-fore we advance to the experiments.
The first is about parsing accuracy in combining binarization with other parsing speed-up techniques.
Binariza-tion itself does not affect parsing accuracy.
"When combined with exact inference algorithms, like the iterative CKY[REF_CITE], the ac-curacy will be the same."
"However, if combined with other inexact pruning techniques like beam-pruning[REF_CITE]or coarse-to-fine parsing[REF_CITE], binarization may interact with those pruning methods in a complicated way to af-fect parsing accuracy."
This is due to different bina-rizations generate different sets of intermediate sym- bols.
"With the same complete constituents, one bi-narization might derive incomplete constitutes that could be pruned while another binarization may not."
This would affect the accuracy.
"We do not address this interaction on in this paper, but leave it to the future work."
In Section 7.[Footnote_3] we will use the iterative CKY for testing.
"3 For log-linear form, if num(w) = 0 (and consequently ctr(w) = 0), we set f(num(w), ctr(w)) = 0; if num(w) &gt;"
"In addition, we believe there exist some speed-up techniques which are incompatible with our bina-rization."
"One such example may be the top-down left-corner filtering[REF_CITE], which seems to be only applicable to the pro-cess of left binarization."
A detailed investigation on this problem will be left to the future work.
"The last issue is how our binarization performs on a lexicalized parser,[REF_CITE]."
Our in-tuition is that we cannot apply our binarization[REF_CITE].
"The key fact in lexicalized parsers is that we cannot explicitly write down all rules and compute their probabilities precisely, due to the great number of rules and the severe data sparsity problem."
Therefore[REF_CITE]grammar rules are already factorized into a set of probabilities.
"In order to capture the dependency relationship be-tween lexcial heads[REF_CITE]breaks down the rules from head outwards, which prevents us from factorizing them in other ways."
Therefore our bina-rization cannot apply to the lexicalized parser.
"How-ever, there are state-of-the-art unlexicalized parsers[REF_CITE], to which we believe our binarization can be applied."
We conducted two experiments on Penn Treebank II corpus[REF_CITE].
The first is to com-pare the effects of different binarizations on parsing and the second is to test the feasibility to combine our work with iterative CKY parsing[REF_CITE]to achieve even better efficiency.
"Following conventions, we learnt the grammar from Wall Street Journal (WSJ) section 2 to 21 and mod-ified it by discarding all functional tags and empty nodes."
The parser obtained this way is a pure un-lexicalized context-free parser with the raw treebank grammar.
"Its accuracy turns out to be 72.46% in terms of F1 measure, quite the same as 72.62% as stated[REF_CITE]."
We adopt this parser in our experiment not only because of sim-plicity but also because we focus on parsing effi-ciency.
"For all sentences with no more than 40 words in section 22, we use the first 10% as the development set, and the last 90% as the test set."
"We use the whole 2,416 sentences in section 23 as the training set."
We use the development set to determine the bet-ter form of the ranking function f as well as to tune its weights.
Both metrics of num and ctr are normalized before use.
"Since there is only one free variable in λ 1 and λ 2 , we can just enumerate 0 ≤ λ 1 ≤ 1, and set λ 2 = 1 − λ 1 ."
"The increasing step is firstly set to 0.05 for the approximate loca-tion of the optimal weight, then set to 0.001 to learn more precisely around the optimal."
"We find that the optimal is 5,773,088 (constituents produced in parsing development set) with λ 1 = 0.014 for linear form, while for log-linear form the optimal is 5,905,292 with λ 1 = 0.691."
Therefore we determine that the better form for the ranking func-tion is linear with λ 1 = 0.014 and λ 2 = 0.986.
The size of each binarized grammar used in the experiment is shown in Table 3. “Original” refers to the raw treebank grammar. “Ours” refers to the learnt binarized grammar by our approach.
For the rest please refer to Section 2.
We also tested whether the size of the training set would have significant effect.
"We use the first 10%, 20%, · · · , up to 100% of section 23 as the training set, respectively, and parse the development set."
"We find that all sizes examined have a similar impact, since the numbers of constituents produced are all around 5,780,000."
It means the training corpus does not have to be very large.
The entire experiments are conducted on a server with an Intel Xeon 2.33 GHz processor and 8 GB memory.
"In this part, we use CKY to parse the entire test set and evaluate the efficiency of different binarizations."
The for-statement implementation of the inner most loop of CKY will affect the parsing time though it won’t affect the number of constituents produced as discussed in Section 3.2.
The best im-plementations may be different for different bina-rized grammars.
"We examine M1∼M4, testing their parsing time on the development set."
"Results show that for right binarization the best method is M3, while for the rest the best is M2."
We use the best method for each binarized grammar when compar-ing the parsing time in Experiment 1.
Table 4 reports the total number of constituents and total time required for parsing the entire test set.
It shows that different binarizations have great im-pacts on the efficiency of CKY.
"With our binariza-tion, the number of constituents produced is nearly 20% of that required by right binarization and nearly 25% of that by the widely-used left binarization."
"As for the parsing time, CKY with our binarization is about 2.5 times as fast as with right binarization and about 1.75 times as fast as with left binarization."
This illustrates that our binarization can significantly improve the efficiency of the CKY parsing.
"Figure 3 reports the detailed number of complete constituents, successful incomplete constituents and failed incomplete constituents produced in parsing."
"The result proves that our binarization can signifi-cantly reduce the number of failed incomplete con-stituents, by a factor of 10 in contrast with left bi-narization."
"Meanwhile, the number of successful in- complete constituents is also reduced by a factor of 2 compared to left binarization."
Another interesting observation is that parsing with a smaller grammar does not always yield a higher efficiency.
"Our binarized grammar is more than twice the size of compact binarization, but ours is more efficient."
It proves that parsing efficiency is related to both the size of grammar in use as well as the number of constituents produced.
"In Section 1, we used an example of “get the bag and go” to illustrate that for rules like X → Y CC Y , right binarization is more suitable."
We also investigated the corresponding linguistic nature that the word to the right of “and” is more likely to indicate the true relationship represented by “and”.
We argued that a better binarization can reflect such linguistic nature of the input language.
"To our sur-prise, our learnt binarization indeed captures this lin-guistic insight, by binarizing NP → NP CC NP from right to left."
"Finally, we would like to acknowledge the limi-tation of our assumption made in Section 3.2."
Ta-ble 4 shows that the parsing time of CKY is not always monotonic increasing with the number of constituents produced.
Head binarization produces fewer constituents than left binarization but con-sumes more parsing time.
"In this part, we test the performance of combining our binarization with the iterative CKY[REF_CITE](henceforth T&amp;T) algorithm."
"Iterative CKY is a procedure of multiple passes of normal CKY: in each pass, it uses a threshold to prune bad constituents; if it cannot find a successful parse in one pass, it will relax the threshold and start another; this procedure is repeated until a successful parse is returned."
T&amp;T used left binarization.
We re-implement their experiments and combine itera-tive CKY with our binarization.
Note that iterative CKY is an exact inference algorithm that guarantees to return the optimal parse.
"As discussed in Sec-tion 6, the parsing accuracy is not changed in this experiment."
T&amp;T used a held-out set to learn the best step of threshold decrease.
They reported that the best step was 11 (in log-probability).
"We found that the best step was indeed 11 for left binarization; for our bina-rizaiton, the best step was 17."
T&amp;T used M4 as the for-statement implementation of CKY.
"In this part, we follow the same method."
The result is shown in Table 5.
We can see that iterative CKY can achieve better performance by us-ing a better binarization.
We also see that the reduc-tion by binarization with pruning is less significant than without pruning.
It seems that the pruning itself in iterative CKY can counteract the reduction effect of binarization to some extent.
Still the best per-formance is archieved by combining iterative CKY with a better binarization.
Almost all work on parsing starts from a binarized grammar.
Usually binarization plays a role of pre-processing.
Left binarization is widely used[REF_CITE]while right binarization is rarely used in the literature.
"Compact binarization was in-troduced[REF_CITE], based on the intuition that a more compact grammar will help acheive a highly efficient CKY parser, though from our exper-iment it is not always true."
"We define the fashion of binarizations in Sec-tion 2, where we encode an intermediate symbol us-ing the ngrams of original symbols (content) it de-rives."
"This encoding is known as the Inside-Trie (I-Trie)[REF_CITE], in which they also mentioned another encoding called Outside-Trie (O-Trie)."
O-Trie encodes an intermediate sym-bol using the its parent and the symbols surrounding it in the original rule (context).
We plan to investigate binarization defined by O-Trie in the future.
"Both I-Trie and O-Trie are equivalent encodings, resulting in equivalent grammars, because they both encode using the complete content or context infor-mation of an intermediate symbol."
"If we use part of the information to encode, for example just parent in O-Trie case, the encoding will be non-equivalent."
Proper non-equivalent encodings are used to gen-eralize the grammar and prevent the binarized gram-mar becoming too specific[REF_CITE].
"It is equipped with head binarization to help improve parsing accuracy, following the traditional linguistic insight that phrases are organized around the head[REF_CITE]."
"In con-trast, we focus our attention on parsing efficiency not accuracy in this paper."
"Binarization also attracts attention in the syntax-based models for machine translation, where trans-lation can be modeled as a parsing problem and bi-narization is essential for efficient parsing[REF_CITE]."
Their binarization is restricted to be a mix-ture of left and right binarization.
This constraint may decrease the power of binarization when ap-plied to speeding up parsing in our problem.
We have studied the impact of grammar binarization on parsing efficiency and presented a novel bina-rization which utilizes rich information learnt from training corpus.
"Experiments not only showed that our learnt binarization outperforms other existing ones in terms of parsing efficiency, but also demon- strated the feasibility to combine our binarization with known parsing speed-up techniques to achieve even better performance."
An advantage of our approach to finding a good binarization would be that the training corpus does not need to be parsed sentences.
Only POS tagged sentences will suffice for training.
This will save the effort to adapt the model to a new domain.
Our approach is based on the assumption that the efficiency of CKY parsing is primarily determined by the number of constituents produced.
"This is a fairly sound one, but not always true, as shown in Section 7.2."
One future work will be relaxing the assumption and finding a better appraoch.
Another future work will be to apply our work to chart parsing.
"It is known that binarization is also essential for an O(n 3 ) complexity of chart parsing, where dotted rules are used to binarize the grammar implicitly from left."
"As shown[REF_CITE], we can binarize explicitly and use intermedi-ate symbols to replace dotted rules in chart parsing."
Therefore chart parsing can use multiple binariza-tions.
We expect that a better binarization will also help improve the efficiency of chart parsing.
"We thank the anonymous reviwers for their pertinent comments, Yoshimasa Tsuruoka for the detailed ex-planations on his referred paper, Yunbo Cao, Shu-jian Huang, Zhenxing Wang , John Blitzer and Liang Huang for their valuable suggestions in preparing the paper."
We present a novel unsupervised sentence fu-sion method which we apply to a corpus of bi-ographies in German.
"Given a group of related sentences, we align their dependency trees and build a dependency graph."
"Using integer lin-ear programming we compress this graph to a new tree, which we then linearize."
We use GermaNet and Wikipedia for checking seman-tic compatibility of co-arguments.
In an eval-uation with human judges our method out-performs the fusion approach of Barzilay &amp;[REF_CITE]with respect to readability.
Automatic text summarization is a rapidly develop-ing field in computational linguistics.
Summariza-tion systems can be classified as either extractive or abstractive ones[REF_CITE].
"To date, most systems are extractive: sentences are selected from one or several documents and then ordered."
"This method exhibits problems, because input sentences very often overlap and complement each other at the same time."
As a result there is a trade-off between non-redundancy and completeness of the output.
"Al-though the need for abstractive approaches has been recognized before (e.g.[REF_CITE]), so far almost all attempts to get closer to abstractive summarization using scalable, statistical techniques have been limited to sentence compression."
"The main reason why there is little progress on ab-stractive summarization is that this task seems to re-quire a conceptual representation of the text which is not yet available (see e.g. Hovy (2003, p.589))."
"Sen-tence fusion (Barzilay &amp;[REF_CITE]), where a new sentence is generated from a group of related sentences and where complete semantic and con-ceptual representation is not required, can be seen as a middle-ground between extractive and abstrac-tive summarization."
Our work regards a corpus of biographies in German where multiple documents about the same person should be merged into a sin-gle one.
"An example of a fused sentence (3) with the source sentences (1,2) is given below: (1) Bohr studierte an der Universität Kopenhagen Bohr studied at the University Copenhagen und erlangte dort seine Doktorwürde. and got there his PhD ’Bohr studied at the University of Copenhagen and got his PhD there’ (2) Nach dem Abitur studierte er Physik und After the school studied he physics and Mathematik an der Universität Kopenhagen. mathematics at the University Copenhagen ’After school he studied physics and mathemat-ics at the University of Copenhagen’ (3) Nach dem Abitur studierte Bohr Physik und After the school studied Bohr physics and Mathematik an der Universität Kopenhagen mathematics at the University Copenhagen und erlangte dort seine Doktorwürde. and got there his PhD ’After school Bohr studied physics and mathe-matics at the University of Copenhagen and got his PhD there’"
Having both (1) and (2) in a summary would make it redundant.
"Selecting only one of them would not give all the information from the input. (3), fused from both (1) and (2), conveys the necessary infor-mation without being redundant and is more appro-priate for a summary."
"To this end, we present a novel sentence fusion method based on dependency structure alignment and semantically and syntactically informed phrase aggregation and pruning."
We address the problem in an unsupervised manner and use integer linear pro-gramming (ILP) to find a globally optimal solution.
We argue that our method has three important advan-tages compared to existing methods.
"First, we ad-dress the grammaticality issue empirically by means of knowledge obtained from an automatically parsed corpus."
"We do not require such resources as subcat-egorization lexicons or hand-crafted rules, but de-cide to retain a dependency based on its syntactic importance score."
The second point concerns inte-grating semantics.
"Being definitely important, ”this source of information remains relatively unused in work on aggregation [Footnote_1] within NLG” (Reiter &amp;[REF_CITE]p.141)."
1 We follow Barzilay &amp;[REF_CITE]and refer to aggre-gation within text-to-text generation as sentence fusion.
"To our knowledge, in the text-to-text generation field, we are the first to use semantic in-formation not only for alignment but also for aggre-gation in that we check coarguments’ compatibility."
"Apart from that, our method is not limited to sen-tence fusion and can be easily applied to sentence compression."
In Filippova &amp;[REF_CITE]we com-press English sentences with the same approach and achieve state-of-the-art performance.
The paper is organized as follows: Section 2 gives an overview of related work and Section 3 presents our data.
Section 4 introduces our method and Sec-tion 5 describes the experiments and discusses the results of the evaluation.
The conclusions follow in the final section.
Most studies on text-to-text generation concern sen-tence compression where the input consists of ex-actly one sentence ([REF_CITE]; Hori &amp;[REF_CITE]; Clarke &amp;[REF_CITE]inter alia).
"In such set-ting, redundancy, incompleteness and compatibility issues do not arise."
"Apart from that, there is no obvious way of how existing sentence compression methods can be adapted to sentence fusion."
Barzilay &amp;[REF_CITE]present a sentence fusion method for multi-document news summariza-tion which crucially relies on the assumption that in-formation appearing in many sources is important.
"Consequently, their method produces an intersec-tion of input sentences by, first, finding the centroid of the input, second, augmenting it with informa-tion from other sentences and, finally, pruning a pre-defined set of constituents (e.g. PPs)."
"The resulting structure is not necessarily a tree and allows for ex-traction of several trees, each of which can be lin-earized in many ways."
Marsi &amp;[REF_CITE]extend the approach of Barzilay &amp; McKeown to do not only intersection but also union fusion.
"Like Barzilay &amp;[REF_CITE], they find the best linearization with a lan-guage model which, as they point out, often pro-duces inadequate rankings being unable to deal with word order, agreement and subcategorization con-straints."
In our work we aim at producing a valid dependency tree structure so that most grammatical-ity issues are resolved before the linearization stage.
They formulate the prob-lem as a search for a maximum spanning tree which is incrementally constructed by connecting words or phrases with dependency relations.
The grammat-icality issue is addressed by a number of hard con-straints.
"As Wan et al. point out, one of the problems with their method is that the output built up from dependencies found in a corpus might have a mean-ing different from the intended one."
"Since we build our trees from the input dependencies, this problem does not arise with our method."
"Apart from that, in our opinion, the optimization formulation we adopt is more appropriate as it allows to integrate many constraints without complex rescoring rules."
The comparable corpus we work with is a collection of about 400 biographies in German gathered from the Internet 2 .
"These biographies describe 140 differ-ent people, and the number of articles for one person ranges from 2 to 4, being 3 on average."
"Despite ob-vious similarities between articles about one person, neither identical content nor identical ordering of in-formation can be expected."
Fully automatic preprocessing in our system com-prises the following steps: sentence boundaries are identified with a Perl CPAN module 3 .
Then the sentences are split into tokens and the TnT tagger[REF_CITE]and the TreeTagger[REF_CITE]are used for tagging and lemmatization respectively.
"Finally, the biographies are parsed with the CDG de-pendency parser (Foth &amp;[REF_CITE])."
We also identify references to the biographee (pronominal as well as proper names) and temporal expressions (ab-solute and relative) with a few rules.
Groups of related sentences serve as input to a sen-tence fusion system and thus need to be identified first (4.1).
Then the dependency trees of the sen-tences are modified (4.2) and aligned (4.[Footnote_3]).
3[URL_CITE]∼ holsten/ Lingua-DE-Sentence-0.07/Sentence.pm
Syntac-tic importance (4.4) and word informativeness (4.5) scores are used to extract a new dependency tree from a graph of aligned trees (4.6).
"Finally, the tree is linearized (4.7)."
Sentence alignment for comparable corpora requires methods different from those used in machine trans-lation for parallel corpora.
"For example, given two biographies of a person, one of them may follow the timeline from birth to death whereas the other may group events thematically or tell only about the sci-entific contribution of the person."
Thus one can-not assume that the sentence order or the content is the same in two biographies.
"Shallow methods like word or bigram overlap, (weighted) cosine or Jaccard similarity are appealing as they are cheap and robust."
"In particular, Nelken &amp;[REF_CITE]demonstrate the efficacy of a sentence-based tf*idf score when applied to comparable corpora."
"Follow-ing them, we define the similarity of two sentences sim(s 1 , s 2 ) as qPP t w S 1 (t) · w S 2 (t) S 1 · S 2 = (1) |S 1 | · |S 2 | t w S2 2 (t) where S is the set of all lemmas but stop-words from t w S2 1 (t) P s, and w S (t) is the weight of the term t: 1 w S (t) ="
"S(t)N t ([Footnote_2]) where S(t) is the indicator function of S, N t is the number of sentences in the biographies of one per-son which contain t. We enhance the similarity mea-sure by looking up synonymy in GermaNet (Lem-nitzer &amp;[REF_CITE])."
"We discard identical or nearly identical sen-tences (sim(s 1 ,s 2 ) &gt; 0.8) and greedily build sentence clusters using a hierarchical groupwise-average technique."
"As a result, one sentence may belong to one cluster at most."
These sentence clus-ters serve as input to the fusion algorithm.
We apply a set of transformations to a dependency tree to emphasize its important properties and elim-inate unimportant ones.
These transformations are necessary for the compression stage.
An example of a dependency tree and its modifed version are given in Fig. 1.
"PREP preposition nodes (an, in) are removed and placed as labels on the edges to the respective nouns;"
CONJ a chain of conjuncts (Mathematik und Physik) is split and each node is attached to the parent node (studierte) provided they are not verbs;
APP a chain of words analyzed as appositions by CDG (Niels Bohr) is collapsed into one node;
"FUNC function words like determiners (der), aux-iliary verbs or negative particles are removed from the tree and memorized with their lexical heads (memorizing negative particles preserves negation in the output);"
ROOT every dependency tree gets an explicit root which is connected to every verb node;
BIO all occurrences of the biographee (Niels Bohr) are replaced with the bio tag.
"Once we have a group of two to four strongly related sentences and their transformed dependency trees, we aim at finding the best node alignment."
"We use a simple, fast and transparent method and align any two words provided that they 1. are content words; 2. have the same part-of-speech; 3. have identical lemmas or are synonyms."
"In case of multiple possibilities, which are extremely rare in our data, the choice is made randomly."
By merging all aligned nodes we get a dependency graph which consists of all dependencies from the input trees.
"In case it contains a cycle, one of the alignments from the cycle is eliminated."
We prefer this very simple method to bottom-up ones (Barzilay &amp;[REF_CITE]; Marsi &amp;[REF_CITE]) for two main reasons.
"Pursuing local subtree alignments, bottom-up methods may leave identical words unaligned and thus prohibit fusion of complementary information."
"On the other hand, they may force alignment of two unrelated words if the subtrees they root are largely aligned."
"Although in some cases it helps discover paraphrases, it con-siderably increases chances of generating ungram-matical output which we want to avoid at any cost."
Given a dependency graph we want to get a new de-pendency tree from it.
"Intuitively, we want to re-tain obligatory dependencies (e.g. subject) while re-moving less important ones (e.g. adv)."
"When de-ciding on pruning an argument, previous approaches either used a set of hand-crafted rules (e.g. Barzilay &amp;[REF_CITE]), or utilized a subcategorization lexicon (e.g.[REF_CITE])."
The hand-crafted rules are often too general to ensure a grammatical argu-ment structure for different verbs (e.g. PPs can be pruned).
Subcategorization lexicons are not readily available for many languages and cover only verbs.
"E.g. they do not tell that the noun son is very of-ten modified by a PP using the preposition of, as in the son of Niels Bohr, and that the NP without a PP modifier may appear incomplete."
"To overcome these problems, we decide on prun-ing an edge by estimating the conditional proba-bility of its label given its head, P(l|h) [Footnote_4] ."
"4 The probabilities are calculated from a corpus of approx. 3,000 biographies from Wikipedia which we annotated auto-matically as described in Section 3."
"For ex-ample, P(subj|studieren) – the probability of the label subject given the verb study – is higher than P(in|studieren), and therefore the subject will be preserved whereas the prepositional label and thus the whole PP can be pruned, if needed."
Table 1 presents the probabilities of several labels given that the head is studieren and shows that some preposi-tions are more important than other ones.
Note that if we did not apply the PREP modification we would be unable to distinguish between different prepo-sitions and could only calculate P(pp|studieren) which would not be very informative.
We also want to retain informative words in the out-put tree.
There are many ways in which word im-portance can be defined.
"Here, we use a formula introduced by Clarke &amp;[REF_CITE]which is a modification of the significance score of Hori &amp;[REF_CITE]: l F A I(w i ) = · f i log (3) N F i w i is the topic word (either noun or verb), f i is the frequency of w i in the aligned biographies, F i is the frequency of w i in the corpus, and F A is the sum of frequencies of all topic words in the corpus. l is the number of clause nodes above w and N is the maximum level of embedding of the sentence which w belongs to."
"By defining word importance differ-ently, e.g. as relatedness of a word to the topic, we could apply our method to topic-based summariza-ti[REF_CITE]."
We formulate the task of getting a tree from a depen-dency graph as an optimization problem and solve it with ILP [Footnote_5] .
5 We use lp solve in our implementati[URL_CITE]
"In order to decide which edges of the graph to remove, for each directed dependency edge from head h to word w we introduce a binary vari-able x lh,w , where l stands for the label of the edge: x lh,w = (10 ifotherwisethe dependency is preserved (4)"
"The goal is to find a subtree of the graph which gets the highest score of the objective function (5) to which both the probability of dependencies (P (l|h) ) and the importance of dependent words (I(w)) con-tribute: f(X) = X x lh ,w · P (l|h) ·"
I(w) (5) x
"The objective function is subject to four types of constraints presented below (W stands for the set of graph nodes minus root, i.e. the set of words)."
"STRUCTURAL constraints allow to get a tree from the graph: (6) ensures that each word has one head at most. (7) ensures connectivity in the tree. (8) is optional and restricts the size of the resulting tree to α words (α = min(0.6̄ · |W |, 10)). ∀w ∈ W, X x lh ,w ≤ 1 (6) h,l 1 l ∀w ∈ W, X x lh,w − |W | X x w,u ≥ 0 (7) h,l u,l"
"X x lh ,w ≤ α (8) x"
SYNTACTIC constraints ensure the syntactic validity of the output tree and explicitly state which argu-ments should be preserved.
"We have only one syn-tactic constraint which guarantees that a subordinat-ing conjunction (sc) is preserved (9) if and only if the clause it belongs to serves as a subordinate clause (sub) in the output. ∀x scw,u , X x sub h,w − x scw,u = 0 (9) h,l"
SEMANTIC constraints restrict coordination to se-mantically compatible elements.
The idea behind these constraints is the following (see Fig. 2).
"It can be that one sentence says He studied math and another one He studied physics, so the output may unite the two words under coordination: He studied math and physics."
"But if the input sentences are He studied physics and He studied sciences, then one should not unite both, because sciences is the gen-eralization of physics."
Neither should one unite two unrelated words: He studied with pleasure and He studied with Bohr cannot be fused into He studied with pleasure and Bohr.
"To formalize these intuitions we define two func-tions hm(w,u) and rel(w,u): hm(w,u) is a binary func-tion, whereas rel(w,u) returns a value from [0, 1]."
"We also introduce additional variables y lw,u (represented by dashed lines in Fig. 2): y wl ,u = (01 otherwiseif ∃h, l : x lh ,w = 1 ∧ x lh,u = 1 (10)"
For two edges sharing a head and having identical labels to be retained we check in GermaNet and in the taxonomy derived from Wikipedia[REF_CITE]that their dependents are not in the hyponymy or meronymy relation (11).
We prohibit verb coordination unless it is found in one of the input sentences.
"If the dependents are nouns, we also check that their semantic relatedness as mea-sured with WikiRelate! (Strube &amp;[REF_CITE]) is above a certain threshold (12)."
"We empirically determined the value of β = 0.36 by calculating an average similarity of coordinated nouns in the cor-pus. ∀y wl ,u , hm(w, u) · y wl ,u = 0 (11) ∀y wl ,u , (rel(w, u) − β) · y wl ,u ≥ 0 (12) (11) prohibits that physics (or math) and sciences ap-pear together since, according to GermaNet, physics (Physik) is a hyponym of science (Wissenschaft). (12) blocks taking both pleasure (Freude) and Bohr because rel(Freude,Bohr) = 0.17. math and physics are neither in ISA, nor part-of relation and are suffi-ciently related (rel(Mathematik, Physik) = 0.67) to become conjuncts."
"META constraints (equations (13) and (14)) guar-antee that y lw,u = x lh,w × x lh,u i.e. they ensure that the semantic constraints are applied only if both the labels from h to w and from h to u are preserved. ∀y wl ,u , x lh,w + x lh,u ≥ 2y lw,u (13) ∀y wl ,u , 1 − x lh,w + 1 − x lh,u ≥ 1 − y wl ,u (14)"
The “overgenerate-and-rank” approach to statisti-cal surface realization is very common (Langk-ilde &amp;[REF_CITE]).
"Unfortunately, in its sim-plest and most popular version, it ignores syntac-tical constraints and may produce ungrammatical output."
"For example, an inviolable rule of Ger-man grammar states that the finite verb must be in the second position in the main clause."
"Since it is hard to enforce such rules with an ngram language model, syntax-informed linearization methods have been developed for German ([REF_CITE]; Filippova &amp;[REF_CITE])."
"We apply our recent method to order constituents and, using the CMU toolkit (Clarkson &amp;[REF_CITE]), build a tri-gram language model from Wikipedia (approx. 1GB plain text) to find the best word order within con-stituents."
Some constraints on word order are in-ferred from the input.
Only interclause punctuation is generated.
"We choose Barzilay &amp; McKeown’s system as a non-trivial baseline since, to our knowledge, there is no other system which outperforms theirs (Sec. 5.1)."
"It is important for us to evaluate the fusion part of our system, so the input and the linearization module of our method and the baseline are identical."
We are also interested in how many errors are due to the lin-earization module and thus define the readability up-per bound (Sec. 5.2).
We further present and discuss the experiments (Sec. 5.3 and 5.5).
"The algorithm of Barzilay &amp;[REF_CITE]pro-ceeds as follows: Given a group of related sentences, a dependency tree is built for each sentence."
These trees are modified so that grammatical features are eliminated from the representation and memorized; noun phrases are flattened to facilitate alignment.
A locally optimal pairwise alignment of modified dependency trees is recursively found with Word-Net and a paraphrase lexicon.
From the alignment costs the centroid of the group is identified.
Then this tree is augmented with information from other trees given that it appears in at least half of the sen-tences from this group.
"A rule-based pruning mod-ule prunes optional constituents, such as PPs or rel-ative clauses."
The linearization of the resulting tree (or graph) is done with a trigram language model.
"To adapt this system to German, we use the Ger-maNet API (Gurevych &amp;[REF_CITE]) instead of WordNet."
"We do not use a paraphrase lexicon, because there is no comparable corpus of sufficient size available for German."
We readjust the align-ment parameters of the system to prevent dissimi-lar nodes from being aligned.
The input to the al-gorithm is generated as described in Sec. 4.1.
The linearization is done as described in Sec. 4.7.
"In cases when there is a graph to linearize, all possible trees covering the maximum number of nodes are extracted from it and linearized."
The most probable string is selected as the final output with a language model.
For the rest of the reimplementation we fol-low the algorithm as presented.
"To find the upper bound on readability, we select one sentence from the input randomly, parse it and lin-earize the dependency tree as described in Sec. 4.7."
This way we obtain a sentence which may differ in form from the input sentences but whose content is identical to one of them.
It is notoriously difficult to evaluate generation and summarization systems as there are many dimen-sions in which the quality of the output can be as-sessed.
The goal of our present evaluation is in the first place to check whether our method is able to produce sensible output.
"We evaluated the three systems ( GRAPH - COMPRESSION , B ARZILAY &amp; M C K EOWN and READABILITY UB ) with 50 native German speakers on 120 fused sentences generated from 40 randomly drawn related sentences groups (3 × 40)."
"In an online experiment, the participants were asked to read a fused sentence preceded by the input and to rate its readability (read) and informativity in respect to the input (inf) on a five point scale."
The experiment was designed so that every participant rated 40 sentences in total.
No participant saw two sentences generated from the same input.
The results are presented in Table 2. len is an average length in words of the output.
"The main disadvantage of our method, as well as other methods designed to work on syntactic struc-tures, is that it requires a very accurate parser."
"In some cases, errors in the preprocessing made ex-tracting a valid dependency tree impossible."
The poor rating of READABILITY UB also shows that er-rors of the parser and of the linearization module af-fect the output considerably.
"Although the semantic constraints ruled out many anomalous combinations, the limited cover-age of GermaNet and the taxonomy derived from Wikipedia was the reason for some semantic oddi-ties in the sentences generated by our method."
"For example, it generated phrases like aus England und Großbritannien (from England and Great Britain)."
A larger taxonomy would presumably increase the recall of the semantic constraints which proved help-ful.
Such errors were not observed in the output of the baseline because it does not fuse within NPs.
"Both the baseline and our method made subcate-gorization errors, although these are more common for the baseline which aligns not only synonyms but also verbs which share some arguments."
"Also, the baseline pruned some PPs necessary for a sen-tence to be complete."
"For example, it pruned an der Atombombe (on the atom bomb) and generated an incomplete sentence Er arbeitete (He worked)."
"For the baseline, alignment of flattened NPs instead of words caused generating very wordy and redun-dant sentences when the input parse trees were in-correct."
"In other cases, our method made mistakes in linearizing constituents because it had to rely on a language model whereas the baseline used unmod-ified constituents from the input."
Absense of intra-clause commas caused a drop in readability in some otherwise grammatical sentences.
A paired t-test revealed significant differences be-tween the readability ratings of the three systems (p = 0.01) but found no significant differences be-tween the informativity scores of our system and the baseline.
Some participants reported informativity hard to estimate and to be assessable for grammat-ical sentences only.
The higher readability rating of our method supports our claim that the method based on syntactic importance score and global con-straints generates more grammatical sentences than existing systems.
An important advantage of our method is that it addresses the subcategorization is-sue directly without shifting the burden of selecting the right arguments to the linearization module.
The dependency structure it outputs is a tree and not a graph as it may happen with the method of Barzi-lay &amp;[REF_CITE].
"Moreover, our method can distinguish between more and less obligatory argu-ments."
"For example, it knows that at is more impor-tant than to for study whereas for go it is the other way round."
"Unlike our differentiated approach, the baseline rule states that PPs can generally be pruned."
"Since the baseline generates a new sentence by modifying the tree of an input sentence, in some cases it outputs a compression of this sentence."
"Un-like this, our method is not based on an input tree and generates a new sentence without being biased to any of the input sentences."
"Our method can also be applied to non-trivial sen-tence compression, whereas the baseline and similar methods, such as Marsi &amp;[REF_CITE], would then boil down to a few very general pruning rules."
We tested our method on the English compression corpus [Footnote_6] and evaluated the compressions automati-cally the same way as Clarke &amp;[REF_CITE]did.
6 The corpus is available[URL_CITE]
"The results (Filippova &amp;[REF_CITE]) were as good as or significantly better than the state-of-the-art, de-pending on the choice of dependency parser."
We presented a novel sentence fusion method which formulates the fusion task as an optimization prob-lem.
"It is unsupervised and finds a globally optimal solution taking semantics, syntax and word informa-tiveness into account."
The method does not require hand-crafted rules or lexicons to generate grammat-ical output but relies on the syntactic importance score calculated from an automatically parsed cor-pus.
An experiment with native speakers demon-strated that our method generates more grammatical sentences than existing systems.
There are several directions to explore in the fu-ture.
Recently query-based sentence fusion has been shown to be a better defined task than generic sen-tence fusi[REF_CITE].
"By modify-ing the word informativeness score, e.g. by giving higher scores to words semantically related to the query, one could force our system to retain words relevant to the query in the output."
To generate co-herent texts we plan to move beyond sentence gen-eration and add discourse constraints to our system.
"Acknowledgements: This work has been funded by the Klaus Tschira Foundation, Heidelberg, Ger-many."
The first author has been supported by a KTF grant (09.009.2004).
"Part of the data has been used with a permission of Bibliographisches Institut &amp; F. A. Brockhaus AG, Mannheim, Germany."
We would like to thank the participants in our online evalua-tion.
We are also grateful to Regina Barzilay and the three reviewers for their helpful comments.
"We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readabil-ity."
This is the first study to take into ac-count such a variety of linguistic factors and the first to empirically demonstrate that dis-course relations are strongly associated with the perceived quality of text.
We show that various surface metrics generally expected to be related to readability are not very good pre-dictors of readability judgments in our Wall Street Journal corpus.
We also establish that readability predictors behave differently de-pending on the task: predicting text readabil-ity or ranking the readability.
Our experi-ments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition.
"Way back in 1944 Robert Gunning Associates was set up, of-fering newspapers, magazines and business firms consultations on clear writing[REF_CITE]."
"In education, teaching good writing technique and grading student writing has always been of key importance[REF_CITE]."
"Linguists have also studied various aspects of text flow, with cohesion-building devices in English[REF_CITE], rhetorical structure the-ory[REF_CITE]and centering the- ory[REF_CITE]among the most influential contributions."
"Still, we do not have unified computational mod-els that capture the interplay between various as-pects of readability."
Most studies focus on a sin-gle factor contributing to readability for a given in-tended audience.
The use of rare words or technical terminology for example can make text difficult to read for certain audience types[REF_CITE].
Syntactic complexity is associated with delayed processing time in un-derstanding[REF_CITE]and is another factor that can decrease readability.
"Text organization (dis-course structure), topic development (entity coher-ence) and the form of referring expressions also de-termine readability."
But we know little about the rel-ative importance of each factor and how they com-bine in determining perceived text quality.
"In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence and discourse."
"We study the association between these features and reader assigned readability ratings, showing that dis-course and vocabulary are the factors most strongly linked to text quality."
"In the easier task of text qual-ity ranking, entity coherence and syntax features also become significant and the combination of fea-tures allows for ranking prediction accuracy of 88%."
Our study is novel in the use of gold-standard dis-course features for predicting readability and the si-multaneous analysis of various readability factors.
The definition of what one might consider to be a well-written and readable text heavily depends on the intended audience[REF_CITE].
"Obvi-ously, even a superbly written scientific paper will not be perceived as very readable by a lay person and a great novel might not be appreciated by a third grader."
"As a result, the vast majority of prior work on readability deals with labeling texts with the appropriate school grade level."
A key observa-tion in even the oldest work in this area is that the vocabulary used in a text largely determines its read-ability.
"More common words are easier, so some metrics measured text readability by the percent-age of words that were not among the N most fre-quent in the language."
"It was also observed that fre-quently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list."
"Standard indices were developed based on the link between word frequency/length and readabil-ity, such as Flesch-Kincaid[REF_CITE], Auto-mated Readability Index[REF_CITE], Gunning Fog[REF_CITE], SMOG[REF_CITE], and Coleman-Liau[REF_CITE]."
They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability.
"For example, Flesch-Kincaid uses the average num-ber of syllables per word to approximate vocabulary difficulty and the average number of words per sen-tence to approximate syntactic difficulty."
"In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public.[REF_CITE]classified words in medical texts as familiar or unfamiliar to a gen-eral audience based on their frequencies in corpora."
"When a description of the unfamiliar terms was pro-vided, the perceived readability of the texts almost doubled."
A more general and principled approach to using vocabulary information for readability decisions has been the use of language models.
"For any given text, it is easy to compute its likelihood under a given lan- guage model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level.[REF_CITE],[REF_CITE],[REF_CITE], and[REF_CITE]used language models to pre-dict the suitability of texts for a given school grade level."
But even for this type of task other factors besides vocabulary use are at play in determining readability.
"Syntactic complexity is an obvious fac-tor: indeed[REF_CITE]and[REF_CITE]also used syntactic features, such as parse tree height or the number of passive sen-tences, to predict reading grade levels."
"For the task of deciding whether a text is written for an adult or child reader,[REF_CITE]found that adding entity coherence[REF_CITE]’s list of features improves classification accu-racy by 10%."
"In linguistics and natural language processing, the text properties rather than those of the reader are em-phasized."
Text coherence is defined as the ease with which a person (tacitly assumed to be a competent language user) understands a text.
Coherent text is characterized by various types of cohesive links that facilitate text comprehensi[REF_CITE].
"In recent work, considerable attention has been devoted to entity coherence in text quality, espe-cially in relation to information ordering."
"In many applications such as text generation and summariza-tion, systems need to decide the order in which se-lected sentences or generated clauses should be pre-sented to the user."
"Most models attempting to cap-ture local coherence between sentences were based on or inspired by centering theory[REF_CITE], which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of refer-ence."
"In a detailed study of information ordering in three very different corpora, (Karamanis et al., to appear) assessed the performance of various formu-lations of centering."
"Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strat-egy for information ordering was based on avoid- ing rough shifts, that is, sequences of sentences that share no entities in common."
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features[REF_CITE].
"In a more powerful generalization of centering,[REF_CITE]developed a novel approach which doesn’t postulate a prefer-ence for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion."
"Their en-tity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level."
Form of reference is also important in well-written text and appropriate choices lead to im-proved readability.
Use of pronouns for reference to highly salient entities is perceived as more de-sirable than the use of definite noun phrases[REF_CITE].
The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subse-quent mentions[REF_CITE]and can be exploited for im-proving and predicting text coherence[REF_CITE].
"The objective of our study is to analyze various readability factors, including discourse relations, be-cause few empirical studies exist that directly link discourse structure with text quality."
"In the past, subsections of the Penn Treebank[REF_CITE]have been annotated for discourse relations[REF_CITE]."
For our study we chose to work with the newly released Penn Discourse Treebank which is the largest anno-tated resource which focuses exclusively on implicit local relations between adjacent sentences and ex-plicit discourse connectives.
The Penn Discourse Treebank[REF_CITE]is a new resource with annotations of discourse con-nectives and their senses in the Wall Street Journal portion of the Penn Treebank[REF_CITE].
All explicit relations (those marked with a discourse connective) are annotated.
"In addition, each adjacent pair of sentences within a paragraph is annotated."
"If there is a discourse relation, then it is marked im-plicit and annotated with one or more connectives."
"If there is a relation between the sentences but adding a connective would be inappropriate, it is marked Al-tLex."
If the consecutive sentences are only related by entity-based coherence[REF_CITE]they are annotated with EntRel.
"Otherwise, they are an-notated with NoRel."
"Besides labeling the connective, the PDTB also annotates the sense of each relation."
The relations are organized into a hierarchy.
"The top level rela-tions are Expansion, Comparison, Contingency, and Temporal."
"Briefly, an expansion relation means that the second clause continues the theme of the first clause, a comparison relation indicates that some-thing in the two clauses is being compared, contin-gency means that there is a causal relation between the clauses, and temporal means they occur either at the same time or sequentially."
We randomly selected thirty articles from the Wall Street Journal corpus that was used in both the Penn Treebank and the Penn Discourse Treebank. [Footnote_1]
"1 One of the selected articles was missing from the Penn Treebank. Thus, results that do not require syntactic informa-tion (Tables 1, 2, 4, and 6) are over all thirty articles, while Tables 3, 5, and 7 report results for the twenty-nine articles with Treebank parse trees."
"Each article was read by at least three college students, each of whom was given unlimited time to read the texts and perform the ratings. [Footnote_2] Subjects were asked the following questions: • How well-written is this article? • How well does the text fit together? • How easy was it to understand? • How interesting is this article?"
"2[REF_CITE]found that human ratings are significantly correlated with self-paced reading times, a more direct measure of processing effort which we plan to explore in future work."
"For each question, they provided a rating between [Footnote_1] and 5, with 5 being the best and 1 being the worst."
"1 One of the selected articles was missing from the Penn Treebank. Thus, results that do not require syntactic informa-tion (Tables 1, 2, 4, and 6) are over all thirty articles, while Tables 3, 5, and 7 report results for the twenty-nine articles with Treebank parse trees."
"After collecting the data, it turned out that most of the time subjects gave the same rating to all ques-tions."
"For competent language users, we view text readability and text coherence as equivalent prop-erties, measuring the extent to which a text is well written."
"Thus for all subsequent analysis, we will use only the first question (“On a scale of 1 to 5, how well written is this text?”)."
The score of an arti-cle was then the average of all the ratings it received.
"The article scores ranged from 1.5 to 4.33, with a mean of 3.2008 and a standard deviation of .7242."
The median score was [Footnote_3].286.
"3 For ease of reference, we number each non-baseline feature in the text and tables."
We define our task as predicting this average rat-ing for each article.
"Note that this task may be more difficult than predicting reading level, as each of these articles appeared in the Wall Street Journal and thus is aimed at the same target audience."
"We suspected that in classifying adult text, more subtle features might be necessary."
We first computed the Pearson correlation coeffi-cients between the simple metrics that most tradi-tional readability formulas use and the average hu-man ratings.
These results are shown in Table 1.
"We tested the average number of characters per word, average number of words per sentence, maximum number of words per sentence, and article length (F 7 ). 3 Article length (F 7 ) was the only significant baseline factor, with correlation of -0.37."
Longer ar-ticles are perceived as less well-written and harder to read than shorter ones.
None of the other baseline metrics were close to being significant predictors of readability.
"We use a unigram language model, where the prob-ability of an article is:"
Y P (w|M) C(w) (1) w
"P(w|M) is the probability of word-type w accord-ing to a background corpus M, and C(w) is the number of times w appears in the article."
The log likelihood of an article is then:
X C(w) log(P (w|M)) (2) w
Note that this model will be biased in favor of shorter articles.
"Since each word has probability less than 1, the log probability of each word is less than 0, and hence including additional words decreases the log likelihood."
We compensate for this by per-forming linear regressions with the unigram log like-lihood and with the number of words in the article as an additional variable.
The question then arises as to what to use as a background corpus.
"We chose to experiment with two corpora: the entire Wall Street Journal corpus and a collection of general AP news, which is gen-erally more diverse than the financial news found in the WSJ."
We predicted that the NEWS vocabulary would be more representative of the types of words our readers would be familiar with.
In both cases we used Laplace smoothing over the word frequencies and a stoplist.
"The vocabulary features we used are article like-lihood estimated from a language model from WSJ (F 5 ), and article likelihood according to a unigram language model from NEWS (F 6 )."
"We also combine the two likelihood features with article length, in or-der to get a better estimate of the language model’s influence on readability independent of the length of the article."
"Both vocabulary-based features (F 5 and F 6 ) are significantly correlated with the readability judg-ments, with p-values smaller than 0.05 (see Table 2)."
"The correlations are positive: the more probable an article was based on its vocabulary, the higher it was generally rated."
"As expected, the NEWS model that included more general news stories had a higher cor-relation with people’s judgments."
"When combined with the length of the article, the unigram language model from the NEWS corpus becomes very predic-tive of readability, with the correlation between the two as high as 0.63."
Syntactic constructions affect processing difficulty and so might also affect readability judgments.
"We examined the four syntactic features used[REF_CITE]: average parse tree height (F 1 ), average number of noun phrases per sentence (F 2 ), average number of verb phrases per sentence (F 3 ), and average number of subordinate clauses per sentence(SBARs in the Penn Treebank tagset) (F [Footnote_4] )."
"4 Other cohesion building devises discussed by Halliday and Hansan include lexical reiteration and discourse relations, which we address next."
The sentence “We’re talking about years ago [SBAR before anyone heard of asbestos having any questionable properties].” contains an example of an SBAR clause.
"Having multiple noun phrases (entities) in each sentence requires the reader to remember more items, but may make the article more interesting.[REF_CITE]found that articles writ-ten for adults tended to contain many more entities than articles written for children."
"While including more verb phrases in each sentence increases the sentence complexity, adults might prefer to have re-lated clauses explicitly grouped together."
The correlations between readability and syntac-tic features is shown in Table 3.
The strongest corre-lation is that between readability and number of verb phrases (0.42).
"This finding is in line with prescrip-tive clear writing advice[REF_CITE], but is to our knowledge novel in the compu-tational linguistics literature."
"As[REF_CITE]point out, the sentences in (1) are eas-ier to comprehend than the sentences in (2), even though they are longer. (1) It was late at night, but it was clear."
The stars were out and the moon was bright. (2) It was late at night.
It was clear.
The stars were out.
The moon was bright.
"Multiple verb phrases in one sentence may be in-dicative of explicit discourse relations, which we will discuss further in section 4.6."
"Surprisingly, the use of clauses introduced by a (possibly empty) subordinating conjunction (SBAR), are actually positively correlated (and al-most approaching significance) with readability."
"So while for children or less educated adults these con-structions might pose difficulties, they were favored by our assessors."
"On the other hand, the average parse tree height negatively correlated with readabil-ity as expected, but surprisingly the correlation is very weak (-0.06)."
"In their classic study of cohesion in English,[REF_CITE]discuss the various aspects of well written discourse, including the use of cohe-sive devices such as pronouns, definite descriptions and topic continuity from sentence to sentence. [Footnote_4] To measure the association between these features and readability rankings, we compute the number of pro-nouns per sentence (F 11 ) and the number of defi-nite articles per sentence (F 12 )."
"4 Other cohesion building devises discussed by Halliday and Hansan include lexical reiteration and discourse relations, which we address next."
"In order to qual-ify topic continuity from sentence to sentence in the articles, we compute average cosine similarity (F 8 ), word overlap (F 9 ) and word overlap over just nouns and pronouns (F 10 ) between pairs of adjacent sentences [Footnote_5] ."
5 Similar features have been used for automatic essay grad-ing as well[REF_CITE].
"Each sentence is turned into a vector of word-types, where each type’s value is its tf-idf (where document frequency is computed over all the articles in the WSJ corpus)."
"The cosine similarity metric is then: s · t cos (s, t) = (3) |s| |t|"
None of these features correlate significantly with readability as can be seen from the results in Ta-ble 4.
"The overlap features are particularly bad predictors of readability, with average word/cosine overlap in fact being negatively correlated with read-ability."
"The form of reference—use of pronouns and definite descriptions—exhibit a higher correla-tion with readability (0.23), but these values are not significant for the size of our corpus."
We use the Brown Coherence Toolkit [URL_CITE] to compute entity grids[REF_CITE]for each ar-ticle.
"In each sentence, an entity is identified as the subject (S), object (O), other (X) (for example, part of a prepositional phrase), or not present (N)."
The probability of each transition type is computed.
"For example, an S-O transition occurs when an entity is the subject in one sentence then an object in the next; X-N transition occurs when an entity appears in non-subject or object position in one sentence and not present in the next, etc. [Footnote_7]"
7 The Brown Coherence Toolkit identifies NPs as the same entity if they have identical head nouns.
"The entity coherence features are the probability of each of these pairs of transitions, for a total of 16 features (F 17−32 ; see complete results in Table 5)."
None of the entity grid features are significantly correlated with the readability ratings.
"One very in-teresting result is that the proportion of S-S transi-tions in which the same entity was mentioned in sub-ject position in two adjacent sentences, is negatively correlated with readability."
"In centering theory, this is considered the most coherent type of transition, keeping the same center of attention."
"Moreover, the feature most strongly correlated with readability is the S-N transition (0.31) in which the subject of one sentence does not appear at all in the following sen- tence."
"Of course, it is difficult to interpret the en-tity grid features one by one, since they are inter-dependent and probably it is the interaction of fea-tures (relative proportions of transitions) that capture overall readability patterns."
Discourse relations are believed to be a major factor in text coherence.
We computed another language model which is over discourse relations instead of words.
We treat each text as a bag of relations rather than a bag of words.
Each relation is annotated for both its sense and how it is realized (implicit or explicit).
"For example, one text might contain {Implicit Comparison, Explicit Temporal, NoRel}."
"We computed the probability of each of our articles according to a multinomial model, where the proba-bility of a text with n relation tokens and k relation types is: n! P (n) p x 1 ...p xk k (4) x 1 !...x k ! 1"
"P(n) is the probability of an article having length n, x i is the number of times relation i appeared, and p i is the probability of relation i based on the Penn Discourse Treebank."
"P(n) is the maximum likeli-hood estimation of an article having n discourse re-lations based on the entire Penn Discourse Treebank (the number of articles with exactly n discourse re-lations, divided by the total number of articles)."
The log likelihood of an article based on its dis-course relations (F 13 ) feature is defined as: k log(P (n)) + log(n!) + X (x i log(p i ) − log(x i !)) i=1 (5)
"The multinomial distribution is particularly suit-able, because it directly incorporates length, which significantly affects readability as we discussed ear-lier."
"It also captures patterns of relative frequency of relations, unlike the simpler unigram model."
Note also that this equation has an advantage over the un-igram model that was not present for vocabulary.
"While every article contains at least one word, some articles do not contain any discourse relations."
"Since the PDTB annotated all explicit relations and re-lations between adjacent sentences in a paragraph, an article with no discourse connectives and only single sentence paragraphs would not contain any annotated discourse relations."
"Under the unigram model, these articles’ probabilities cannot be com-puted."
"Under the multinomial model, the probabil-ity of an article with zero relations is estimated as Pr(N = 0), which can be calculated from the cor-pus."
"As in the case of vocabulary features, the presence of more relations will lead to overall lower probabil-ities so we also consider the number of discourse relations (F 14 ) and the log likelihood combined with the number of relations as features."
"In order to iso-late the effect of the type of discourse relation (ex-plicitly expressed by a discourse connective such as “because” or “however” versus implicitly expressed by adjacency), we also compute multinomial model features for the explicit discourse relations (F 15 ) and over just the implicit discourse relations (F 16 )."
"The likelihood of discourse relations in the text under a multinomial model is very highly and sig-nificantly correlated with readability ratings, espe-cially after text length is taken into account."
Cor- relations are 0.48 and 0.54 respectively.
The prob-ability of the explicit relations alone is not a suffi-ciently strong indicator of readability.
This fact is disappointing as the explicit relations can be iden-tified much more easily in unannotated text[REF_CITE].
Note that the sequence of just the im-plicit relations is also not sufficient.
This observa-tion implies that the proportion of explicit and im-plicit relations may be meaningful but we leave the exploration of this issue for later work.
"So far, we introduced six classes of factors that have been discussed in the literature as readability cor-relates."
Through statistical tests of associations we identified the individual factors significantly corre-lated with readability ratings.
"These are, in decreas-ing order of association strength:"
LogL of Discourse Relations (r = .4835)
"LogL, NEWS (r= .4497)"
"Average Verb Phrases (.4213) LogL, WSJ (r = .3723)"
Number of words (r = -.3713)
"Vocabulary and discourse relations are the strongest predictors of readability, followed by aver-age number of verb phrases and length of the text."
This empirical confirmation of the significance of discourse relations as a readability factor is novel for the computational linguistics literature.
Note though that for our work we use oracle discourse annota-tions directly from the PDTB and no robust systems for automatic discourse annotation exist today.
The significance of the average number of verb phrases as a readability predictor is somewhat sur-prising but intriguing.
"It would lead to reexamina-tion of the role of verbs/predicates in written text, which we also plan to address in future work."
"None of the other factors showed significant association with readability ratings, even though some correla-tions had relatively large positive values."
"In this section, we turn to the question of how the combination of various factors improves the predic-tion of readability."
"We use the leaps package in R to find the best subset of features for linear regres-sion, for subsets of size one to eight."
We use the squared multiple correlation coefficient (R 2 ) to as-sess the effectiveness of predictions.
R 2 is the pro-portion of variance in readability ratings explained by the model.
"If the model predicts readability per-fectly, R 2 = 1, and if the model has no predictive capability, R 2 = 0."
F 1 + F 6 + F 7 + F 10 + F 13 +
"F 22 + F 23 , R 2 = 0.7557 F 1 +F 6 +F 7 +F 10 +F 11 +F 13 +F 19 +F 30 , R 2 = 0.776."
The linear regression results confirm the expec-tation that the combination of different factors is a rather complex issue.
"As expected, discourse, vo-cabulary and length which were the significant in-dividual factors appear in the best model for each feature set size."
"Their combination gives the best result for regression with three predictors, and they explain half of the variance in readability ratings, R 2 = 0.5029."
"But the other individually significant feature, av-erage number of verb phrases per sentence (F 3 ) never appears in the best models."
"Instead, F 1 —the depth of the parse tree—appears in the best model with more than four features."
"Also unexpectedly, two of the superficial cohe-sion features appear in the larger models: F 10 is the average word overlap over nouns and pronouns and F 11 is the average number of pronouns per sen-tence."
"Entity grid features also make their way into the best models when more features are used for pre-diction: S-X, O-O, O-X, N-O transitions (F 19 , F 22 , F 23 , F 30 )."
In this section we consider the problem of pairwise ranking of text readability.
"That is, rather than try-ing to predict the readability of a single document, we consider pairs of documents and predict which one is better."
"This task may in fact be the more natu-ral one, since in most applications the main concern is with the relative quality of articles rather than their absolute scores."
"This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task."
"We thus create a classification problem: given two articles, is article 1 more readable than article 2?"
"For each pair of texts whose readability ratings on the 1 to 5 scale differed by at least 0.5, we form one data point for the ranking problem, resulting in 243 examples."
The predictors are the differences be-tween the two articles’ features.
"For classification, we used WEKA’s linear support vector implemen-tation (SMO) and performance was evaluated using 10-fold cross-validation."
The classification results are shown in Table 7.
"When all features are used for prediction, the ac-curacy is high, 88.88%."
"The length of the article can serve as a baseline feature—longer articles are ranked lower by the assessors, so this feature can be taken as baseline indicator of readability."
Only six features used by themselves lead to accuracies higher than the length baseline.
"These results indi-cate that the most important individual factors in the readability ranking task, in decreasing order of im-portance, are log likelihood of discourse relations, number of discourse relations, N-O transitions, O-N transitions, average number of VPs per sentence and text probability under a general language model."
"In terms of classes of features, the 16 entity grid features perform the best, leading to an accu-racy of 79.41%, followed by the combination of the four discourse features (77.36%), and syntax features (74.07%)."
"This is evidence for the fact that there is a complex interplay between readabil-ity factors: the entity grid factors which individ-ually have very weak correlation with readability combine well, while adding the three additional dis-course features to the likelihood of discourses rela-tions actually worsens performance slightly."
"Simi-lar indication for interplay between features is pro-vided by the class ablation classification results, in which classes of features are removed."
"Surprisingly, removing syntactic features causes the biggest dete-rioration in performance, a drop in accuracy from 88.88% to 82.71%."
"The removal of vocabulary, length, or discourse features has a minimal negative impact on performance, while removing the cohe-sion features actually boosts performance."
We have investigated which linguistic features cor-relate best with readability judgments.
"While sur-face measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syn-tactic, semantic, and discourse features that do cor-relate highly."
"The average number of verb phrases in each sentence, the number of words in the article, the likelihood of the vocabulary, and the likelihood of the discourse relations all are highly correlated with humans’ judgments of how well an article is written."
"While using any one out of syntactic, lexical, co-herence, or discourse features is substantally better than the baseline surface features on the discrim-ination task, using a combination of entity coher-ence and discourse relations produces the best per-formance."
This work was partially supported by an Inte-grative Graduate Education and Research Trainee-ship grant from National Science Foundation ([REF_CITE]487) and by NSF Grant IIS-07-05671.
"We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful com-ments."
We improve the quality of paraphrases ex-tracted from parallel corpora by requiring that phrases and their paraphrases be the same syn-tactic type.
This is achieved by parsing the En-glish side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs.
"In or-der to retain broad coverage of non-constituent phrases, complex syntactic labels are intro-duced."
A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.
Paraphrases are alternative ways of expressing the same information.
Being able to identify or gen-erate paraphrases automatically is useful in a wide range of natural language applications.
"Recent work has shown how paraphrases can improve question answering through query expansi[REF_CITE], automatic evaluation of translation and sum-marization by modeling alternative lexicalizati[REF_CITE], and machine translation both by dealing with out of vocabulary words and phrases[REF_CITE]and by expand-ing the set of reference translations for minimum er-ror rate training[REF_CITE]."
"While all ap-plications require the preservation of meaning when a phrase is replaced by its paraphrase, some addi-tionally require the resulting sentence to be gram-matical."
In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora[REF_CITE].
The paraphrasing technique employs various aspects of phrase-based statistical machine transla-tion including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments.
En-glish phrases are considered to be potential para-phrases of each other if they share a common for-eign language phrase among their translations.
Mul-tiple paraphrases are frequently extracted for each phrase and can be ranked using a paraphrase proba-bility based on phrase translation probabilities.
We find that the quality of the paraphrases that are generated in this fashion improves significantly when they are required to be the same syntactic type as the phrase that they are paraphrasing.
This con-straint: • Eliminates a trivial but pervasive error that arises from the interaction of unaligned words with phrase extraction heuristics. • Refines the results for phrases that can take on different syntactic labels. • Applies both to phrases which are linguistically coherent and to arbitrary sequences of words. • Results in much more grammatical output when phrases are replaced with their para-phrases.
A thorough manual evaluation of the refined para-phrasing technique finds a 19% absolute improve- ment in the number of paraphrases that are judged to be correct.
This paper is structured as follows: Section 2 describes related work in syntactic constraints on phrase-based SMT and work utilizing syntax in paraphrase discovery.
Section 3 details the prob-lems with extracting paraphrases from parallel cor-pora and our improvements to the technique.
Sec-tion 4 describes our experimental design and evalu-ation methodology.
"Section 5 gives the results of our experiments, and Section 6 discusses their implica-tions."
A number of research efforts have focused on em-ploying syntactic constraints in statistical machine translation.
The synchronized grammar places constraints on which words can be aligned across bilingual sen-tence pairs.
"To achieve computational efficiency, the original proposal used only a single non-terminal la-bel rather than a linguistic grammar."
Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints[REF_CITE].
"If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence."
Other recent work has incor-porated constituent and dependency subtrees into the translation rules used by phrase-based systems[REF_CITE].
Phrase-based rules have also been replaced with synchronous con-text free grammars[REF_CITE]and with tree fragments[REF_CITE].
"A number of techniques for generating para-phrases have employed syntactic information, either in the process of extracting paraphrases from mono-lingual texts or in the extracted patterns themselves."
They extracted paraphrase patterns that incorporate this information.
Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s para-phrasing method.
"For example, they extracted patterns such as con-sider NN → take NN into consideration."
"To ac-complish this, Zhao el al. used dependency parses on the English side of the parallel corpus."
"Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves."
"They give a probabilistic formation of paraphrasing which nat-urally falls out of the fact that they use techniques from phrase-based statistical machine translation: eˆ 2 = arg max p(e 2 |e 1 ) (1) e 2 :e 2 =6 e 1 where p(e 2 |e 1 ) = X p(f|e 1 )p(e 2 |f, e 1 ) (2) f ≈ X p(f|e 1 )p(e 2 |f) (3) f"
"Phrase translation probabilities p(f|e 1 ) and p(e 2 |f) are commonly calculated using maximum likelihood estimati[REF_CITE]: count(e, f) p(f|e) ="
"P (4) f count(e, f) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the word alignments for sentence pairs in a bilingual parallel corpus."
Various phrase extraction heuristics are possible.
"BP(f 1J , e I1 , A) = {(f jj +m , e ii+n ) : ∀(i 0 , j 0 ) ∈"
"A : j ≤ j 0 ≤ j + m ↔ i ≤ i 0 ≤ i + n ∧∃(i 0 , j 0 ) ∈"
"A : j ≤ j 0 ≤ j + m∧ ↔ i ≤ i 0 ≤ i + n} where f 1J is a foreign sentence, e I1 is an English sen-tence and A is a set of word alignment points."
The heuristic allows unaligned words to be in-cluded at the boundaries of the source or target lan-guage phrases.
"For example, when enumerating the consistent phrase pairs for the sentence pair given in Figure 1, la igualdad would align not only to equal, but also to create equal, and to create equal."
In SMT these alternative translations are ranked by the trans-lation probabilities and other feature functions dur-ing decoding.
The interaction between the phrase extraction heuristic and unaligned words results in an unde-sirable effect for paraphrasing.
"By Bannard and Callison-Burch’s definition, equal, create equal, and to create equal would be considered paraphrases be-cause they are aligned to the same foreign phrase."
Tables 1 and 2 show how sub- and super-phrases can creep into the paraphrases: equal can be paraphrased as equal rights and create equal can be paraphrased as equal.
Obviously when e 2 is substituted for e 1 the resulting sentence will generally be ungrammatical.
"The first case could result in equal equal rights, and the second would drop the verb."
This problem is pervasive.
"To test its extent we at-tempted to generate paraphrases for 900,000 phrases using Bannard and Callison-Burch’s method trained on the Europarl corpora (as described in Section 4)."
"It generated a total of 3.7 million paraphrases for 400,000 phrases in the list. 1"
We observed that 34% of the paraphrases (excluding the phrase itself) were super- or sub-strings of the original phrase.
The most probable paraphrase was a super- or sub-string of the phrase 73% of the time.
"There are a number of strategies that might be adopted to alleviate this problem: •[REF_CITE]rank their paraphrases with a language model when the paraphrases are substituted into a sentence. •[REF_CITE]sum over multiple parallel corpora C to reduce the prob-lems associated with systematic errors in the word alignments in one language pair: eˆ 2 = arg max X X p(f|e 1 )p(e 2 |f) (5) e 2 c∈C f • We could change the phrase extraction heuris-tic’s treatment of unaligned words, or we could attempt to ensure that we have fewer unaligned items in our word alignments. • The paraphrase criterion could be changed from being e 2 6= e 1 to specifying that e2 is not sub- or super-string of e[Footnote_1]."
1[REF_CITE]000 phrases could not be paraphrased either because e 2 =6 e 1 or because they were not consistently aligned to any foreign phrases.
In this paper we adopt a different strategy.
The essence of our strategy is to constrain paraphrases to be the same syntactic type as the phrases that they are paraphrasing.
Syntactic constraints can apply in two places: during phrase extraction and when sub-stituting paraphrases into sentences.
These are de-scribed in sections 3.1 and 3.2.
"When we apply syntactic constraints to the phrase extraction heuristic, we change how bilingual phrase pairs are enumerated and how the component proba-bilities of the paraphrase probability are calculated."
"We use the syntactic type s of e 1 in a refined ver-sion of the paraphrase probability: eˆ 2 = p(e 2 |e 1 , s(e 1 )) (6)arg max e 2 :e 2 =6 e 1 ∧s(e 2 )=s(e 1 ) where p(e 2 |e 1 , s(e 1 )) can be approximated as:"
"X P p(f|e 1 , s(e 1 ))p(e 2 |f, s(e 1 )) f (7) |C| c∈C"
"We define a new phrase extraction algorithm that op-erates on an English parse tree P along with foreign sentence f 1J , English sentence e I1 , and word align-ment A. We dub this SBP for syntactic bilingual phrases:"
"SBP(f 1J , e I1 , A, P) = {(f jj+m , e ii+n , s(e ii+n )) : ∀(i 0 , j 0 ) ∈"
"A : j ≤ j 0 ≤ j + m ↔ i ≤ i 0 ≤ i + n ∧∃(i 0 , j 0 ) ∈"
"A : j ≤ j 0 ≤ j + m∧ ↔ i ≤ i 0 ≤ i + n ∧∃ subtree ∈ P with label s spanning words (i, i + n)}"
"The SBP phrase extraction algorithm produces tu-ples containing a foreign phrase, an English phrase and a syntactic label (f,e,s)."
"After enumerating these for all phrase pairs in a parallel corpus, we can calculate p(f|e 1 , s(e 1 )) and p(e 2 |f, s(e 1 )) as: count(f, e 1 , s(e 1 )) p(f|e 1 , s(e 1 )) ="
"P f count(f, e 1 , s(e 1 )) count(f, e 2 , s(e 1 )) p(e 2 |f, s(e 1 )) ="
"P e 2 count(f, e 2 , s(e 1 ))"
By redefining the probabilities in this way we parti-tion the space of possible paraphrases by their syn-tactic categories.
In order to enumerate all phrase pairs with their syntactic labels we need to parse the English side of the parallel corpus (but not the foreign side).
This limits the potential applicability of our refined para-phrasing method to languages which have parsers.
Table 3 gives an example of the refined para-phrases for equal when it occurs as an adjective or adjectival phrase.
Note that most of the paraphrases that were possible under the baseline model (Table 1) are now excluded.
"We no longer get the noun equality, the verb equals, the adverb equally, the de-termier the or the NP equal rights."
"The paraphrases seem to be higher quality, especially if one considers their fidelity when they replace the original phrase in the context of some sentence."
We tested the rate of paraphrases that were sub-and super-strings when we constrain paraphrases based on non-terminal nodes in parse trees.
"The percent of the best paraphrases being substrings dropped from 73% to 24%, and the overall percent of paraphrases subsuming or being subsumed by the original phrase dropped from 34% to 12%."
"How-ever, the number of phrases for which we were able to generated paraphrases dropped from 400,000 to 90,000, since we limited ourselves to phrases that were valid syntactic constituents."
"The number of unique paraphrases dropped from several million to 800,000."
The fact that we are able to produce paraphrases for a much smaller set of phrases is a downside to using syntactic constraints as we have initially pro-posed.
It means that we would not be able to gen-erate paraphrases for phrases such as create equal.
"Many NLP tasks, such as SMT, which could benefit from paraphrases require broad coverage and may need to paraphrases for phrases which are not syn-tactic constituents."
Complex syntactic labels
"To generate paraphrases for a wider set of phrases, we change our phrase extraction heuristic again so that it produces phrase pairs for arbitrary spans in the sentence, including spans that aren’t syntactic constituents."
"We assign every span in a sentence a syntactic label using CCG-style notati[REF_CITE], which gives a syntactic role with elements missing on the left and/or right hand sides."
"SBP(f 1J , e 1I , A, P) = {(f jj+m , e ii+n , s) : ∀(i 0 , j 0 ) ∈"
"A : j ≤ j 0 ≤ j + m ↔ i ≤ i 0 ≤ i + n ∧∃(i 0 , j 0 ) ∈"
A : j ≤ j 0 ≤ j + m∧ ↔ i ≤ i 0 ≤ i + n ∧∃s ∈
"CCG-labels(e ii+n , P)}"
The function CCG-labels describes the set of CCG-labels for the phrase spanning positions i to i + n in a parse tree P .
It generates three complex syntactic labels for the non-syntactic constituent phrase create equal in the parse tree given in Figure 2: 1.
This label corresponds to the in-nermost circle.
It indicates that create equal is a verb phrase missing a noun phrase to its right.
That noun phrase in turn missing a plural noun (NNS) to its right. 2.
SQ\VBP NP/(VP/(NP/NNS)) –
This label corre-sponds to the middle circle.
"It indicates that create equal is an SQ missing a VBP and a NP to its left, and the complex VP to its right. 3."
SBARQ\WHADVP (SQ\VBP NP/(VP/(NP/NNS)))/. – This label corresponds to the outermost cir-cle.
"It indicates that create equal is an SBARQ missing a WHADVP and the complex SQ to its left, and a punctuation mark to its right."
We can use these complex labels instead of atomic non-terminal symbols to handle non-constituent phrases.
"For example, Table 4 shows the para-phrases and syntactic labels that are generated for the non-constituent phrase create equal."
The para-phrases are significantly better than the paraphrases generated for the phrase by the baseline method (re-fer back to Table 2).
The labels shown in the figure are a fraction of those that can be derived for the phrase in the paral-lel corpus.
"Each of these corresponds to a different syntactic context, and each has its own set of associ-ated paraphrases."
"We increase the number of phrases that are para-phrasable from the 90,000 in our initial definition[REF_CITE]000 when we use complex CCG la-bels."
"The number of unique paraphrases increases from 800,000 to 3.5 million, which is nearly as many paraphrases that were produced by the base-line method for the sample."
"In addition to applying syntactic constraints to our phrase extraction algorithm, we can also apply them when we substitute a paraphrase into a sentence."
"To do so, we limit the paraphrases to be the same syn-tactic type as the phrase that it is replacing, based on the syntactic labels that are derived from the phrase tree for a test sentence."
Since each phrase normally has a set of different CCG labels (instead of a sin-gle non-termal symbol) we need a way of choosing which label to use when applying the constraint.
There are several different possibilities for choos-ing among labels.
"We could simultaneously choose the best paraphrase and the best label for the phrase in the parse tree of the test sentence: eˆ 2 = arg max arg max p(e 2 |e 1 , s) (8) e 2 :e 2 =6 e 1 s∈CCG-labels(e 1 ,P)"
"Alternately, we could average over all of the labels that are generated for the phrase in the parse tree:"
"X eˆ 2 = arg max p(e 2 |e 1 , s) (9) e 2 :e 2 =6 e 1 s∈CCG-labels(e 1 ,P)"
The potential drawback of using Equations 8 and 9 is that the CCG labels for a particular sentence sig-nificantly reduces the paraphrases that can be used.
"For instance, VP/(NP/NNS) is the only label for the paraphrases in Table 4 that is compatible with the parse tree given in Figure 2."
"Because the CCG labels for a given sentence are so specific, many times there are no matches."
There-fore we also investigated a looser constraint.
"We choose the highest probability paraphrase with any label (i.e. the set of labels extracted from all parse trees in our parallel corpus): eˆ 2 = arg max p(e 2 |e 1 , s) (10)arg max e 2 :e 2 6=e 1 s∈∩ ∀T in C CCG-labels(e 1 ,T)"
"In our experiments, we evaluate the quality of the paraphrases that are generated using Equations 8, 9 and 10."
We compare their quality against the[REF_CITE]baseline.
We conducted a manual evaluation to evaluate para-phrase quality.
We evaluated whether paraphrases retained the meaning of their original phrases and whether they remained grammatical when they re-placed the original phrase in a sentence.
Our paraphrase model was trained using the Eu-roparl corpus[REF_CITE].
"We used ten par-allel corpora between English and (each of) Dan-ish, Dutch, Finnish, French, German, Greek, Ital-ian, Portuguese, Spanish, and Swedish, with approx-imately 30 million words per language for a total of 315 million English words."
Automatic word align-ments were created for these using Giza++[REF_CITE].
The English side of each parallel corpus was parsed using the Bikel parser[REF_CITE].
A total of 1.6 million unique sentences were parsed.
A trigram language model was trained on these En-glish sentences using the SRI language modeling toolkit[REF_CITE].
The paraphrase model and language model for the[REF_CITE]baseline were trained on the same data to ensure a fair comparison.
The test set was the English portion of test sets used in the shared translation task of the[REF_CITE]Workshop on Statistical Machine Translati[REF_CITE].
The test sentences were also parsed with the Bikel parser.
"The phrases to be evaluated were selected such that there was an even balance of phrase lengths (from one word long up to five words long), with half of the phrases being valid syntactic constituents and half being arbitrary sequences of words. 410 phrases were selected at random for evaluation. 30 items were excluded from our results subsequent to evaluation on the grounds that they consisted solely of punctuation and stop words like determin-ers, prepositions and pronouns."
This left a total of 380 unique phrases.
We produced paraphrases under the following eight conditions: 1.
Baseline – The paraphrase probability defined[REF_CITE].
Calcu-lated over multiple parallel corpora as given in Equation 5.
Note that under this condition the best paraphrase is the same for each occurrence of the phrase irrespective of which sentence it occurs in. 2.
Baseline + LM – The paraphrase probability (as above) combined with the language model probability calculated for the sentence with the phrase replaced with the paraphrase. 3. Extraction Constraints – This condition se-lected the best paraphrase according[REF_CITE].
It chooses the single best paraphrase over all labels.
"Conditions 3 and 5 only apply the syntactic constraints at the phrase extraction stage, and do not require that the paraphrase have the same syntactic label as the phrase in the sentence that it is being subtituted into. 4. Extraction Constraints + LM – As above, but the paraphrases are also ranked with a language model probability. 5."
"Substitution Constraints – This condition corresponds to Equation 8, which selects the highest probability paraphrase which matches at least one of the syntactic labels of the phrase in the test sentence."
Conditions 5–8 apply the syntactic constraints both and the phrase ex-traction and at the substitution stages. 6.
"Syntactic Constraints + LM – As above, but including a language model probability as well. 7."
"Averaged Substitution Constraints – This condition corresponds to Equation 9, which av-erages over all of the syntactic labels for the phrase in the sentence, instead of choosing the single one which maximizes the probability. 8."
Averaged Substitution Constraints + LM –
"As above, but including a language model probability."
We evaluated the paraphrase quality through a sub-stitution test.
We retrieved a number of sentences which contained each test phrase and substituted the phrase with automatically-generated paraphrases.
Annotators judged whether the paraphrases had the same meaning as the original and whether the re-sulting sentences were grammatical.
They assigned two values to each sentence using the 5-point scales given in Table 5.
"We considered an item to have the same meaning if it was assigned a score of 3 or greater, and to be grammatical if it was assigned a score of 4 or 5."
"We evaluated several instances of a phrase when it occurred multiple times in the test corpus, since paraphrase quality can vary based on context[REF_CITE]."
"There were an average of 3.1 instances for each phrase, with a maximum of 6."
"There were a total of 1,195 sentences that para- phrases were substituted into, with a total of 8,422 judgements collected."
Note that 7 different para-phrases were judged on average for every instance.
"This is because annotators judged paraphrases for eight conditions, and because we collected judg-ments for the 5-best paraphrases for many of the conditions."
"We measured inter-annotator agreement with the Kappa statistic[REF_CITE]using the 1,391 items that two annotators scored in common."
The two annotators assigned the same absolute score 47% of the time.
"If we consider chance agreement to be 20% for 5-point scales, then K = 0.33, which is commonly interpreted as “fair”[REF_CITE]."
"If we instead measure agreement in terms of how often the annotators both judged an item to be above or below the thresholds that we set, then their rate of agreement was 80%."
"In this case chance agreement would be 50%, so K = 0.61, which is “substantial”."
"In order to allow other researchers to recreate our re-sults or extend our work, we have prepared the fol-lowing materials for download 2 : • The complete set of paraphrases generated for the test set."
"This includes the [Footnote_3].7 million para-phrases generated by the baseline method and the [Footnote_3].5 million paraphrases generated with syn-tactic constraints. • The code that we used to produce these para-phrases and the complete data sets (including all 10 word-aligned parallel corpora along with their English parses), so that researchers can extract paraphrases for new sets of phrases. • The manual judgments about paraphrase qual-ity."
"3 Our results show a significantly lower score for the base-line than reported[REF_CITE]. This is potentially due to the facts that in this work we evaluated on out-of-domain news commentary data, and we randomly se-lected phrases. In the pervious work the test phrases were drawn from WordNet, and they were evaluated solely on in-domain European parliament data."
"3 Our results show a significantly lower score for the base-line than reported[REF_CITE]. This is potentially due to the facts that in this work we evaluated on out-of-domain news commentary data, and we randomly se-lected phrases. In the pervious work the test phrases were drawn from WordNet, and they were evaluated solely on in-domain European parliament data."
"These may be useful as development ma-terial for setting the weights of a log-linear for-mulation of paraphrasing, as suggested[REF_CITE]."
Table 6 summarizes the results of the manual eval-uation.
We can observe a strong trend in the syn-tactically constrained approaches performing better 2 Available[URL_CITE]than the baseline.
They retain the correct meaning more often (ranging from 4% to up to 15%).
"They are judged to be grammatical far more frequently (up to 26% more often without the language model, and 24% with the language model) ."
They perform nearly 20% better when both meaning and grammat-icality are used as criteria. 3
"Another trend that can be observed is that incor-porating a language model probability tends to result in more grammatical output (a 7–9% increase), but meaning suffers as a result in some cases."
"When the LM is applied there is a drop of 12% in correct meaning for the baseline, but only a slight dip of 1- 2% for the syntactically-constrained phrases."
"Note that for the conditions where the paraphrases were required to have the same syntactic type as the phrase in the parse tree, there was a reduction in the number of paraphrases that could be applied."
"For the first two conditions, paraphrases were posited for 1194 sentences, conditions 3 and 4 could be applied to 1142 of those sentences, but conditions 5–8 could only be applied to 876 sentences."
The substitution constraints reduce coverage to 73% of the test sen-tences.
"Given that the extraction constraints have better coverage and nearly identical performance on the meaning criterion, they might be more suitable in some circumstances."
In this paper we have presented a novel refinement to paraphrasing with bilingual parallel corpora.
We illustrated that a significantly higher performance can be achieved by constraining paraphrases to have the same syntactic type as the original phrase.
A thorough manual evaluation found an absolute im-provement in quality of 19% using strict criteria about paraphrase accuracy when comparing against a strong baseline.
"The syntactically enhanced para-phrases are judged to be grammatically correct over two thirds of the time, as opposed to the baseline method which was grammatically correct under half of the time."
This paper proposed constraints on paraphrases at two stages: when deriving them from parsed paral-lel corpora and when substituting them into parsed test sentences.
These constraints produce para-phrases that are better than the baseline and which are less commonly affected by problems due to un-aligned words.
"Furthermore, by introducing com-plex syntactic labels instead of solely relying on non-terminal symbols in the parse trees, we are able to keep the broad coverage of the baseline method."
"Syntactic constraints significantly improve the quality of this paraphrasing method, and their use opens the question about whether analogous con-straints can be usefully applied to paraphrases gen-erated from purely monolingual corpora."
"Our im-provements to the extraction of paraphrases from parallel corpora suggests that it may be usefully ap-plied to other NLP applications, such as generation, which require grammatical output."
"Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bi-text."
"The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors."
So we pro-pose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
"Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30-best parses."
"When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the base-line, and even outperforms the hierarchical system of Hiero by 0.7 points."
"Automatic extraction of translation rules is a funda-mental problem in statistical machine translation, es-pecially for many syntax-based models where trans-lation rules directly encode linguistic knowledge."
"Typically, these models extract rules using parse trees from both or either side(s) of the bitext."
"The former case, with trees on both sides, is often called tree-to-tree models; while the latter case, with trees on either source or target side, include both tree-to-string and string-to-tree models (see Table 1)."
"Leveraging from structural and linguistic informa-tion from parse trees, these models are believed to be better than their phrase-based counterparts in handling non-local reorderings, and have achieved promising translation results. [Footnote_1]"
"1 For example, in recent NIST Evaluations, some of these models[REF_CITE]ranked among top 10.[URL_CITE]"
"However, these systems suffer from a major limi-tation, that the rule extractor only uses [Footnote_1]-best parse tree(s), which adversely affects the rule set quality due to parsing errors."
"1 For example, in recent NIST Evaluations, some of these models[REF_CITE]ranked among top 10.[URL_CITE]"
"To make things worse, mod-ern statistical parsers are often trained on domains quite different from those used in MT."
"By contrast, formally syntax-based models[REF_CITE]do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts."
"To alleviate this problem, an obvious idea is to extract rules from k-best parses instead."
"However, a k-best list, with its limited scope, has too few vari-ations and too many redundancies[REF_CITE]."
This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space.
"In addition, many subtrees are repeated across different parses, so it is also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k 2 similar tree-pairs in tree-to-tree models)."
"We instead propose a novel approach that ex-tracts rules from packed forests (Section 3), which compactly encodes many more alternatives than k-best lists."
"Experiments (Section 5) show that forest-based extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system[REF_CITE], which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses."
"When combined with our previ-ous orthogonal work on forest-based decoding[REF_CITE], the forest-forest approach achieves a [Footnote_2].5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hi-ero, one of the best-performing systems to date."
"2 We swap the 1-best and 2-best parses of the example sen-tence from our earlier paper[REF_CITE], since the current 1-best parse is easier to illustrate the rule extraction algorithm."
"Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-to-tree models[REF_CITE]where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models."
"We review in this section the tree-based approach to machine translati[REF_CITE], and its rule extraction algorithm[REF_CITE]."
Current tree-based systems perform translation in two separate steps: parsing and decoding.
"The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transforma-tion rules."
"For example, consider the following ex-ample translating from Chinese to English: (1) Bùshı́ yǔ Shālóng jǔxı́ng le huı̀tán Bush and/with Sharon 1 hold past. meeting 2 “Bush held a meeting 2 with Sharon 1 ”"
Figure 2 shows how this process works.
"The Chi-nese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 steps."
"First, at the root node, we apply rule r 1 shown in Figure 1, which translates the Chinese coordina-tion construction (“... and ...”) into an English prepo-sitional phrase."
"Then, from step (c) we continue ap-plying rules to untranslated Chinese subtrees, until we get the complete English translation in (e). 2"
"More formally, a (tree-to-string) translation rule[REF_CITE]is a tuple hlhs(r),rhs(r),φ(r)i, where lhs(r) is the source-side tree fragment, whose internal nodes are la-beled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source-language words (like “yǔ”) or variables from a set X = {x 1 , x 2 , . . .}; rhs(r) is the target-side string expressed in target-language words (like “with”) and variables; and φ(r) is a mapping from X to nonter- minals."
Each variable x i ∈ X occurs exactly once in lhs(r) and exactly once in rhs(r).
"For example, for rule r 1 in Figure 1, lhs(r 1 ) ="
"IP ( NP(x 1 CC(yǔ) x 2 ) x 3 ), rhs(r 1 ) = x 1 x 3 with x 2 , φ(r 1 ) = {x 1 : NPB, x 2 : NPB, x 3 : VPB}."
These rules are being used in the reverse direction of the string-to-tree transducers[REF_CITE].
We now briefly explain the algorithm[REF_CITE]that can extract these translation rules from a word-aligned bitext with source-side parses.
Consider the example in Figure 3.
"The basic idea is to decompose the source (Chinese) parse into a se-ries of tree fragments, each of which will form a rule with its corresponding English translation."
"However, not every fragmentation can be used for rule extrac-tion, since it may or may not respect the alignment and reordering between the two languages."
So we say a fragmentation is well-formed with respect to an alignment if the root node of every tree fragment corresponds to a contiguous span on the target side; the intuition is that there is a “translational equiva-lence” between the subtree rooted at the node and the corresponding target span.
"For example, in Fig-ure 3, each node is annotated with its corresponding English span, where the NP node maps to a non-contiguous one “Bush ⊔ with Sharon”."
"More formally, we need a precise formulation to handle the cases of one-to-many, many-to-one, and many-to-many alignment links."
"Given a source-target sentence pair (σ, τ) with alignment a, the (tar-get) span of node v is the set of target words aligned to leaf nodes yield(v) under node v: span(v) , {τ i ∈ τ | ∃σ j ∈ yield(v), (σ j , τ i ) ∈ a}."
"For example, in Figure 3, every node in the parse tree is annotated with its corresponding span below the node, where most nodes have contiguous spans ex-cept for the NP node which maps to a gapped phrase “Bush ⊔ with Sharon”."
"But contiguity alone is not enough to ensure well-formedness, since there might be words within the span aligned to source words uncovered by the node."
"So we also define a span s to be faithful to node v if every word in it is only aligned to nodes dominated by v, i.e.: ∀τ i ∈ s, (σ j , τ i ) ∈ a ⇒ σ j ∈ yield(v)."
"For example, sibling nodes VV and AS in the tree have non-faithful spans (crossed out in the Figure), because they both map to “held”, thus neither of them can be translated to “held” alone."
"In this case, a larger tree fragment rooted at VPB has to be extracted."
"Nodes with non-empty, contiguous, and faithful spans form the admissible set (shaded nodes in the figure), which serve as potential cut-points for rule extraction. [Footnote_3]"
"3 Admissible set[REF_CITE]is also known as “fron-tier set”[REF_CITE]. For simplicity of presentation, we assume every target word is aligned to at least one source word; see[REF_CITE]for handling unaligned target words."
"With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we “cut” the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes."
"For example, the tree in Figure 3 is cut into 6 pieces, each of which corresponds to a rule on the right."
"These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r 1 in Fig. 1)[REF_CITE]."
Our experiments use composed rules.
We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees.
"Informally, a packed parse forest, or forest in short, is a compact representation of all the deriva-tions (i.e., parse trees) for a given sentence under a context-free grammar[REF_CITE]."
"For example, consider again the Chi-nese sentence in Example (1) above, which has (at least) two readings depending on the part-of-speech of the word yǔ: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order."
"These two parse trees can be represented as a single forest by sharing common subtrees such as NPB 0,1 and VPB 3,6 , as shown in Figure 4."
"Such a forest has a structure of a hypergraph[REF_CITE], where items like NP 0,3 are called nodes, whose indices denote the source span, and combinations like e 1 : IP 0,6 → NPB 0,3 VP 3,6 we call hyperedges."
"We denote head(e) and tails(e) to be the consequent and antecedant items of hyper-edge e, respectively."
"For example, head(e 1 ) ="
"IP 0,6 , tails(e 1 ) = {NPB 0,3 , VP 3,6 }."
"We also denote BS(v) to be the set of incoming hy-peredges of node v, being different ways of deriving it."
"For example, in Figure 4, BS(IP 0,6 ) = {e 1 , e 2 }."
"Like in tree-based extraction, we extract rules from a packed forest F in two steps: (1) admissible set computation (where to cut), and (2) fragmentation (how to cut)."
It turns out that the exact formulation developed for admissible set in the tree-based case can be ap-plied to a forest without any change.
"The fragmen-tation step, however, becomes much more involved since we now face a choice of multiple parse hyper-edges at each node."
"In other words, it becomes non-deterministic how to “cut” a forest into tree frag-ments, which is analogous to the non-deterministic pattern-match in forest-based decoding[REF_CITE]."
For example there are two parse hyperedges e 1 and e 2 at the root node in Figure 4.
"When we fol-low one of them to grow a fragment, there again will be multiple choices at each of its tail nodes."
"Like in tree-based case, a fragment is said to be complete if all its leaf nodes are admissible."
"Otherwise, an in-complete fragment can grow at any non-admissible frontier node v, where following each parse hyper-edge at v will split off a new fragment."
"For example, following e 2 at the root node will immediately lead us to two admissible nodes, NPB 0,1 and VP 1,6 (we will highlight admissible nodes by gray shades"
Algorithm 1 Forest-based Rule Extraction. in this section like in Figures 3 and 4).
"So this frag-ment, frag 1 = {e 2 }, is now complete and we can extract a rule,"
IP (x 1 :NPB x 2 :VP) → x 1 x 2 .
"However, following the other hyperedge e 1"
"IP 0,6 → NP 0,3 VPB 3,6 will leave the new fragment frag 2 = {e 1 } incom-plete with one non-admissible node NP 0,3 ."
We then grow frag 2 at this node by choosing hyperedge e 3
"NP 0,3 → NPB 0,1 CC 1,2 NPB 2,3 , and spin off a new fragment frag 3 = {e 1 , e 3 }, which is now complete since all its four leaf nodes are ad-missible."
We then extract a rule with four variables:
IP (NP(x 1 :NPB x 2 :CC x 3 :NPB) x 4 :VPB) → x 1 x 4 x 2 x 3 .
This procedure is formalized by a breadth-first search (BFS) in Pseudocode 1.
"The basic idea is to visit each frontier node v, and keep a queue open of actively growing fragments rooted at v. We keep expanding incomplete fragments from open, and ex-tract a rule if a complete fragment is found (line 10)."
"Each fragment is associated with a frontier (variable front in the Pseudocode), being the subset of non-admissible leaf nodes (recall that expansion stops at admissible nodes)."
"So each initial fragment along hyperedge e is associated with an initial frontier (line 5), front = tails(e) \ admset."
"A fragment is complete if its frontier is empty (line 9), otherwise we pop one frontier node u to expand, spin off new fragments by following hyper-edges of u, and update the frontier (lines 14-16), un-til all active fragments are complete and open queue is empty (line 7)."
"A single parse tree can also be viewed as a triv-ial forest, where each node has only one incoming hyperedge."
"So the[REF_CITE]algorithm for tree-based rule extraction (Sec. 2.2) can be consid-ered a special case of our algorithm, where the queue open always contains one single active fragment."
"In tree-based extraction, for each sentence pair, each rule extracted naturally has a count of one, which will be used in maximum-likelihood estimation of rule probabilities."
"However, a forest is an implicit collection of many more trees, each of which, when enumerated, has its own probability accumulated from of the parse hyperedges involved."
"In other words, a forest can be viewed as a virtual weighted k-best list with a huge k."
"So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms."
Inspired by the parsing literature on pruning[REF_CITE]we pe-nalize a rule r by the posterior probability of its tree fragment frag = lhs(r).
"This posterior probability, notated αβ(frag), can be computed in an Inside-Outside fashion as the product of three parts: the out-side probability of its root node, the probabilities of parse hyperedges involved in the fragment, and the inside probabilities of its leaf nodes, αβ(frag) =α(root(frag)) ·"
"Y P(e) e ∈ frag (2) · Y β(v) v ∈ yield(frag) where α(·) and β(·) denote the outside and inside probabilities of tree nodes, respectively."
"For example in Figure 4, αβ({e 2 , e 3 }) = α(IP 0,6 ) · P(e 2 ) · P(e 3 ) · β(NPB 0,1 )β(CC 1,2 )β(NPB 2,3 )β(VPB 3,6 )."
Now the fractional count of rule r is simply αβ(lhs(r)) c(r) = (3) αβ(TOP) where TOP denotes the root node of the forest.
"Like in the M-step in EM algorithm, we now extend the maximum likelihood estimation to frac-tional counts for three conditional probabilities re-garding a rule, which will be used in the experi-ments: c(r)"
P(r | lhs(r)) =
"P , (4) r ′ :lhs(r ′ )=lhs(r) c(r ′ ) c(r)"
P(r | rhs(r)) =
"P , (5) r ′ :rhs(r ′ )=rhs(r) c(r ′ )"
P(r |root(lhs(r))) c(r) (6) =
P . r ′ : root(lhs(r ′ ))=root(lhs(r)) c(r ′ )
"The concept of packed forest has been previously used in translation rule extraction, for example in rule compositi[REF_CITE]and tree bina-rizati[REF_CITE]."
"However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest."
"Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmen-tation (Algorithm 1) which we think is non-trivial."
"The forest concept is also used in machine transla-tion decoding, for example to characterize the search space of decoding with integrated language models[REF_CITE]."
The first direct appli-cation of parse forest in translation is our previous work[REF_CITE]which translates a packed for-est from a parser; it is also the base system in our experiments (see below).
"This work, on the other hand, is in the orthogonal direction, where we uti-lize forests in rule extraction instead of decoding."
Our experiments will use both default 1-best decod-ing and forest-based decoding.
"As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule ex-traction and decoding."
"There is also a parallel work on extracting rules from k-best parses and k-best alignments[REF_CITE], but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective."
Our experiments are on Chinese-to-English trans-lation based on a tree-to-string system similar[REF_CITE].
"Given a 1-best tree T , the decoder searches for the best deriva-tion d ∗ among the set of all possible derivations D: d ∗ = arg max λ 0 log"
"P(d | T ) + λ 1 log P lm (τ(d)) d∈D + λ 2 |d| + λ 3 |τ(d)| (7) where the first two terms are translation and lan-guage model probabilities, τ(d) is the target string (English sentence) for derivation d, and the last two terms are derivation and translation length penalties, respectively."
The conditional probability P(d | T) decomposes into the product of rule probabilities:
P(d | T ) = Y P(r). (8) r∈d
Each P(r) is in turn a product of five probabilities:
P(r | lhs(r)) λ 4 · P(r | rhs(r)) λ 5 · P(r | root(lhs(r))) λ 6 (9) ·
"P lex (lhs(r) | rhs(r)) λ 7 · P lex (rhs(r) | lhs(r)) λ 8 where the first three are conditional probabilities based on fractional counts of rules defined in Sec-tion 3.3, and the last two are lexical probabilities."
These parameters λ 1 . . . λ 8 are tuned by minimum error rate training[REF_CITE]on the dev sets.
We refer readers[REF_CITE]for details of the decoding algorithm.
We use the Chinese parser[REF_CITE]to parse the source side of the bitext.
"We will first report results trained on a small-scaled dataset with detailed analysis, and then scale to a larger one, where we also combine the technique of forest-based decoding[REF_CITE]."
"To test the effect of forest-based rule extraction, we parse the training set into parse forests and use three levels of pruning thresholds: p e = 2, 5, 8."
"Figure 6 plots the extraction speed and transla-tion quality of forest-based extraction with various pruning thresholds, compared to 1-best and 30-best baselines."
"Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower extraction, since each of the top-k trees has to be processed individually although they share many common subtrees."
"Forest extraction, by contrast, is much faster thanks to packing and produces consis-tently better BLEU scores."
"With pruning threshold p e = 8, forest-based extraction achieves a (case in-sensitive) BLEU score of 0.2533, which is an ab-solute improvement of 1.0% points over the 1-best baseline, and is statistically significant using the sign-test[REF_CITE](p &lt; 0.01)."
This is also 0.5 points better than (and twice as fast as) extracting on 30-best parses.
"These BLEU score re-sults are summarized in Table 2, which also shows that decoding with forest-extracted rules is less than twice as slow as with 1-best rules, and only fraction-ally slower than with 30-best rules."
We also investigate the question of how often rules extracted from non 1-best parses are used by the decoder.
"Table 3 shows the numbers of rules extracted from 1-best, 30-best and forest-based ex-tractions, and the numbers that survive after filter-ing on the dev set."
"Basically in the forest-based case we can use about twice as many rules as in the 1-best case, or about 1.5 times of 30-best extraction."
"But the real question is, are these extra rules really useful in generating the final (1-best) translation?"
The last row shows that 16.3% of the rules used in 1-best derivations are indeed only extracted from non 1-best parses in the forests.
Note that this is a stronger condition than changing the distribution of rules by considering more parses; here we introduce new rules never seen on any 1-best parses.
"We also conduct experiments on a larger training dataset, FBIS, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively."
We also use a bigger trigram model trained on the first 1/3 of the Xinhua portion of Gi-gaword corpus.
"To integrate with forest-based de-coding, we use both 1-best trees and packed forests during both rule extraction and decoding phases."
"Since the data scale is larger than the small data, we are forced to use harsher pruning thresholds, with p e = 5 for extraction and p d = 10 for decoding."
The final BLEU score results are shown in Ta-ble 4.
"With both tree-based and forest-based decod-ing, rules extracted from forests significantly outper-form those extracted from 1-best trees (p &lt; 0.01)."
"The final result with both forest-based extraction and forest-based decoding reaches a BLEU score of 0.2816, outperforming that of Hiero[REF_CITE], one of the best performing systems to date."
These re-sults confirm that our novel forest-based rule extrac-tion approach is a promising direction for syntax-based machine translation.
"In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses."
"Experiments on a state-of-the-art tree-to-string system show that this method improves BLEU score significantly, with reasonable extraction speed."
"When combined with our previ-ous work on forest-based decoding, the final result is even better than the hierarchical system Hiero."
"For future work we would like to apply this ap-proach to other types of syntax-based translation systems, namely the string-to-tree systems[REF_CITE]and tree-to-tree systems."
We advance the state-of-the-art for discrimi-natively trained machine translation systems by presenting novel probabilistic inference and search methods for synchronous gram-mars.
"By approximating the intractable space of all candidate translations produced by inter-secting an ngram language model with a synchronous grammar, we are able to train and decode models incorporating millions of sparse, heterogeneous features."
"Further, we demonstrate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance."
"The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention."
"Discriminative approaches are widely seen as a promising technique, poten-tially allowing us to further the state-of-the-art."
"Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms[REF_CITE], or rescaling a product of sub-models[REF_CITE]."
"Recent work[REF_CITE]has shown how translation can be framed as a probabilistic log-linear model, where the distribution over trans-lations is modelled in terms of a latent variable on derivations."
Their approach was globally opti-mised and discriminative trained.
"However, a lan-guage model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted."
"This was because adding a lan-guage model would mean that the normalising parti-tion function could no longer be exactly calculated, thereby preventing efficient parameter estimation."
"Here, we show how language models can be incorporated into large-scale discriminative transla-tion models, without losing the probabilistic inter-pretation of the model."
"The key insight is that we can use Monte-Carlo methods to approximate the partition function, thereby allowing us to tackle the extra computational burden associated with adding the language model."
This approach is theoreti-cally justified and means that the model contin-ues to be both probabilistic and globally optimised.
"As expected, using a language model dramatically increases translation performance."
Our second major contribution is an exploita-tion of syntactic features.
"By encoding source syn-tax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar[REF_CITE]."
We report on translation gains using this approach.
We begin by introducing the synchronous gram-mar approach to SMT in Section 2.
In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence.
In Section 4 we evaluate the abil-ity of our model to effectively estimate the highly dependent weights for the sparse features and real-valued language model.
In addition we describe how our model can easily integrate rich features over source syntax trees and compare our training meth-ods to a state-of-the-art benchmark.
"A synchronous context free grammar (SCFG,[REF_CITE]) describes the gener-ation of pairs of strings."
"A string pair is generated by applying a series of paired context-free rewrite rules of the form, X → hα, γ, ∼i, where X is a non-terminal, α and γ are strings of terminals and non-terminals and ∼ specifies a one-to-one alignment between non-terminals in α and γ."
"In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target trans-lati[REF_CITE]."
In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system[REF_CITE].
"Note however that our approach is general and could be used with other synchronous grammar transducers (e.g.,[REF_CITE])."
"SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production)."
"Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category."
Figure 1 shows an example deriva-tion for Chinese to English translation.
We start by defining a log-linear model for the con-ditional probability distribution over target transla-tions of a given source sentence.
"A sequence of SCFG rule applications which produce a translation from a source sentence is referred to as a derivation, and each translation may be produced by many dif-ferent derivations."
"As the training data only provides source and target sentences, the derivations are mod-elled as a latent variable."
"The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f, is given by: p Λ (d, e|f) = exp P k λ k H k (d, e, f) (1) Z Λ (f) where H k (d, e, f) = X h k (f, r, q(r, d)) (2) r∈d"
"Using Equation (1), the conditional probability of a target translation given the source is the sum over all of its derivations: p Λ (e|f) = X p Λ (d, e|f) d∈∆(e,f) where ∆(e,f) is the set of all derivations of the target sentence e from the source f."
"Here k ranges over the model’s features, and Λ = {λ k } are the model parameters (weights for their corresponding features)."
"The function q(r, d) returns the target ngram context, for a language model with order m, of rule r in derivation d."
"For a rule which spans the target words (i,j) and target yield(d) = {t 0 , · · · , t l }: t i ···t i+m−2 ?t j−m+2 ···t j if j − i &gt; m q(r, d) = t i ···t j otherwise"
"The feature functions h k are real-valued functions over the source and target sentences, and can include overlapping and non-independent features of the data."
"The features must decompose with the deriva-tion and the ngram context defined by the function q, as shown in Equation (2)."
"The features can reference the entire source sentence coupled with each rule, r, and its target context, in a derivation."
"By directly incorporating the language model context q into the model formulation, we will not be able to exactly compute the partition function Z Λ (f), which sums over all possible derivations."
"Even though a dynamic program over this space would still run in polynomial time, as shown[REF_CITE], a packed chart representation of the par-tition function for the binary Hiero grammars used in this work would require O(n 3 |T | 4(m−1) ) space, 1 which is far too large to be practical."
"Instead we approximate the partition function using a sum over a large subset of the possible derivations (∆(e, f)):"
"Z Λ (f) ≈ X X exp X λ k H k (d, e, f) e d∈{⊂∆(e,f)} k = Z̃ Λ (f)"
"This model formulation raises the questions of what an appropriate large subset of derivations for training is, and how to efficiently calculate the sum over all derivations in decoding."
In the following sections we elucidate and evaluate our solutions to these problems.
"The training and decoding algorithms presented in the following sections rely upon Monte-Carlo tech-niques, which in turn require the ability to draw derivation samples from the probability distribution defined by our log-linear model."
Here we adapt previously presented algorithms for sampling from a PCFG[REF_CITE]for use with our syn-chronous grammar model.
Algorithm 1 describes the algorithm for sampling derivations.
The sampling algorithm assumes the pre-existance of a packed chart representation of all derivations for a given source sentence.
The inside algorithm is then used to calculate the scores needed to define a multino-mial distribution over all partial derivations associ-ated with expanding a given child rule.
These ini-tial steps are performed once and then an unlim-ited number of samples can be drawn by calling the recursive SAMPLE procedure.
"MULTI draws a sample from the distribution over rules for a given chart cell, CHILDREN enumerates the chart cells connected to a rule as variables, and DERIVATION is a recursive tree data structure for derivations."
The algorithm is
"Algorithm 1 Top-down recursive derivation sam-pling algorithm. [Footnote_1]: procedure SAMPLE (X, i, k) 2: rule ← MULTI (inside chart(X, i, k)) 3: c = φ 4: for (child category,x,y) ∈ CHILDREN (rule) do 5: c ← c ∪ SAMPLE (child category, x, y) 6: end for 7: return DERIVATION (X, children) 8: end procedure first called on a category and chart cell spanning the entire chart, and then proceeds top down by using the function MULTI to draw the next rule to expand from the distribution defined by the inside scores."
"1 where |T| is the size of the terminal alphabet, i.e. the num-ber of unique English words."
Approximating the partition function with Z̃ Λ (f) could introduce biases into inference and in the fol-lowing discussion we describe measures taken to minimise the effects of the approximation bias.
"An obvious approach to approximating the parti-tion function, and the feature expectations required for calculating the derivative in training, is to use the packed chart of derivations produced by running the cube pruning beam search algorithm[REF_CITE]on the source sentence."
"In this case Z̃ Λ (f) includes all the derivations that fall within the cube pruning beam, hopefully representing the majority of the probability mass."
We denote the partition function estimated with this cube beam approxima-tion as Z̃ Λcb (f).
This approach has the advantage of using the same beam search dynamic program dur-ing training as is used for decoding.
"As the approxi-mated partition function does not contain all deriva-tions, it is possible that some, or all, of the deriva-tions of the reference translation from the parallel corpus may be excluded."
We must therefore intersect the packed chart built from the cube beam with that of the reference derivations to ensure consistency.
"Although, as would be done using cube-pruning, it would seem intuitively sensible to approximate the partition function using only high probability derivations, it is possible that doing so will bias our model in odd ways."
"The space of derivations contained within the beam will be tightly clustered about a maximum, and thus a model trained with such an approximation will only see a very small part of the overall distribution, possibly leading it astray."
"Consider the example of a language model feature: as this is a very strong indicator of transla-tion quality, we would expect all derivations within the beam to have a similar (high) language model score, thereby robbing this feature of its discriminat-ing power."
However if our model could also see the low probability derivations it would be clear that this feature is indeed very strongly correlated with good translations.
"Thus a good approximation of the space of derivations is one that includes both good and bad examples, not just a cluster around the maximum."
A principled solution to the problem of approx-imating the partition function would be to use a Markov Chain Monte Carlo (MCMC) sampler to estimate the sum with a large number of samples.
"Most of the sampled derivations would be in the high probability region of the distribution, however there would also be a number of samples drawn from the rest of the space, giving the model a more global view of the distribution, avoiding the pit-falls of the narrow view obtained by a beam search."
"Although effective, the computational cost of such an approach is prohibitive as we would need to draw hundreds of thousands of samples to obtain conver-gence, for every training iteration."
Here we mediate between the computational advantages of a beam and the broad view of the dis-tribution provided by sampling.
Using the algorithm outlined in Section 3.1 we draw samples from the distribution of derivations and then insert these sam-ples into a packed chart representation.
This process is illustrated in Figure 2.
The packed chart created by intersecting the sample derivations represents a space of derivations much greater than the original samples.
"In Figure 2 the chart is built from the first two sampled derivations, while the third derivation can be extracted from the chart even though it was never explicitly entered."
"This approximation of the partition function (denoted Z̃ Λsam (f)) allows us to build an efficient packed chart representation of a large number of derivations, centred on those with high probability while still including a significant representation of the low probability space."
Deriva-tions corresponding to the reference can be detected during sampling and thus we can build the chart for the reference derivations at the same time as the one approximating the partition function.
"This could lead to some, or none of, the possible ref-erence derivations being included, as they may not have been sampled."
"Although we could intersect all of the reference derivations with the sampled chart, this could distort the distribution over derivations, and we believe it to be advantageous to keep the distributions between the partition function and ref-erence charts consistent."
"Both of the approximations proposed above, Z̃ Λcb (f) and Z̃ Λsam (f), rely on the pre-existence of a trained translation model in order to either guide the cube-pruning beam, or define the probability distri-bution from which we draw samples."
"We solve this chicken and egg problem by first training an exact translation model without a language model, and then use this model to create the partition function approximations for training with a language model."
We denote the distribution without a language model as p −LMΛ (e|f) and that with as p +ΛLM (e|f).
A final training problem that we need to address is the appropriate initialisation of the model param-eters.
"In theory we could simply randomly initialise Λ for p +ΛLM (e|f), however in practice we found that this resulted in poor performance on the develop-ment data."
"This is due to the complex non-convex optimisation function, and the fact that many fea-tures will fall outside the approximated charts result-ing in random, or zero, weights in testing."
"We intro-duce a novel solution in which we use the Gaus-sian prior over model weights to tie the exact model trained without a language model, which assigns sensible values to all rule features, with the approx-imated model."
The prior over model parameters for p +ΛLM (e|f) is defined as: kλk−λ−LMk k[Footnote_2] p +LM (λ k ) ∝ e − 2σ2
"2 We have experimented with using a Metropolis Hastings sampler, with p −LMΛ (e|f) as the proposal distribution, to sam-ple from the true distribution with the language model. Unfor-tunately the sample rejection rate was very high such that this method proved infeasibly slow."
Here we have set the mean parameters of the Gaus-sian distribution for the approximated model to those learnt for the exact one.
This has the effect that any features that fall outside the approximated model will simply retain the weight assigned by the exact model.
"While for other feature weights the prior will penalise substantial deviations away from Λ −LM , essentially encoding the intuition that the rule rule parameters should not change substantially with the inclusion of language model features."
This results in the following log-likelihood objec-tive and corresponding gradient:
L = X log p + Λ LM (e i |f i ) + X log p + 0
"LM (λ k ) (e i ,f i )∈D k ∂L = E p +LM (d|e i ,f i ) [h k ] − E p +LM (e|f i ) [h k ] ∂λ k Λ Λ λ +LM − λ −LM k − k σ 2"
As stated in Equation 3 the probability of a given translation string is calculated as the sum of the probabilities of all the derivations that yield that string.
"In decoding, where the reference translation is not known, the exact calculation of this summa-tion is NP-Hard."
This problem also arises in mono-lingual parsing with probabilistic tree substitution grammars and has been tackled in the literature using Monte-Carlo sampling methods[REF_CITE].
Their approach is directly appli-cable to our SCFG decoding problem and we can use Algorithm 1 to draw sample translation derivations for the source sentence.
"The probability of a trans-lation can be calculated simply from the number of times a derivation that yields it was sampled, divided by the total number of samples."
For the p −LMΛ (e|f) model we can build the full chart of all possible derivations and thus sample from the true distribu-tion over derivations.
For the p +ΛLM (e|f) model we suffer the same problem as in training and cannot build the full chart.
Instead a chart is built using the cube-pruning algorithm with a wide beam and we then draw samples from this chart.
"Although sampling from a reduced chart will result in biased samples, in Section 4 we show this approach to be effective in practice. 2"
In Section 4 we compare our sampling approach to the heuristic beam search pro-posed[REF_CITE].
"It is of interest to compare our proposed decoding algorithms to minimum Bayes risk (MBR) decoding[REF_CITE], a commonly used decod-ing method."
"From a theoretical standpoint, the sum-ming of derivations for a given translation is exactly equivalent to performing MBR with a 0/1 loss func-tion over derivations."
"From a practical perspective, MBR is normally performed with B LEU as the loss and approximated using n-best lists."
These n-best lists are produced using algorithms tuned to remove multiple derivations of the same translation (which have previously been seen as undesirable).
"However, it would be simple to extend our sampling based decoding algorithm to calculate the MBR estimate using B LEU , in theory providing a lower variance estimate than attained with n-best lists."
"We evaluate our model on the[REF_CITE]Chinese to English translation task[REF_CITE], using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems."
The statistics for this data are presented in Table 1. [Footnote_3]
3 Development and test set statistics are for the single human translation reference.
"The training data made avail-able for this task consisted of 40k pairs of tran-scribed utterances, drawn from the travel domain."
"The development and test data for this task are some-what unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual anno-tators."
"Model performance is evaluated using the standard B LEU metric[REF_CITE]which measures average n-gram precision, n ≤ [Footnote_4], and we use the NIST definition of the brevity penalty for multiple reference test sets."
4 With the exception that we allow unaligned words at the boundary of rules. This improves training set coverage.
"We provide evaluation against both the entire multi-reference sets, and the single human translation."
"Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models[REF_CITE]. 4 As these heuristics aren’t based on a genera-tive model, and don’t guarantee that the target trans-lation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivation, leaving 38,405 sentences for training."
Our base model contains a single feature for each rule which counts the number of times it appeared in a particular derivation.
"For models which include a language model, we train a standard Kneser-Ney tri-gram model on the target side of the training corpus."
We also include a word penalty feature to compen-sate for the shortening effect of the language model.
In total our model contains 2.9M features.
"The aims of our evaluation are: (1) to deter-mine that our proposed training regimes are able to realise performance increase when training sparse rule features and a real valued language model fea-ture together, (2) that the model is able to effectively use rich features over the source sentence, and (3) to confirm that our model obtains performance com-petitive with the current state-of-the-art."
We have described a number of modelling choices which aim to compensate for the training biases introduced by incorporating a language model fea-ture through approximate inference.
Our a priori knowledge from other SMT systems suggests that incorporating a language model should lead to large increases in B LEU score.
In this evaluation we aim to determine whether our training regimes are able to realises these expected gains.
"Table 2 shows a matrix of development B LEU scores achieved by varying the approximation of the partition function in training, and varying the decod-ing algorithm."
If we consider the vertical axis we can see that the sampling method for approximat-ing the partition function has a small but consistent advantage over using the cube-pruning beam.
"The charts produced by the sampling approach occupy roughly half the disc space as those produced by the beam search, so in subsequent experiments we present results using the Z̃ Λsam (f) approximation."
Comparing the decoding algorithms on the hori-zontal axis we can reconfirm the findings[REF_CITE]that the max-translation decod-ing outperforms the Viterbi max-derivation approx-imation.
"It is also of note that this B LEU increase is robust to the introduction of the language model feature, assuaging fears that the max-translation approach may have been doing the job of the lan-guage model."
We also compare using Monte-Carlo sampling for decoding with the previously pro-posed heuristic beam search algorithm.
"The differ-ence between the two algorithms is small, however we feeling the sampling approach is more theoreti-cally justified and adopt it for our later experiments."
The most important result from this evaluation is that both our training regimes realise substantial gains from the introduction of the language model feature.
Thus we can be confident that our model is capable of modelling the distribution over trans-lations even when the space over all derivations is intractable to dynamically program exactly.
In the previous sections we’ve described and evalu-ated a statistical model of translation that is able to estimate a probability distribution over translations using millions of sparse features.
A prime motiva-tion for such a model is the ability to define com-plex features over more than just the surface forms of the source and target strings.
"There are limit-less options for such features, and previous work has focused on defining token based features such as part-of-speech and morphology[REF_CITE]."
"Although such features are applica-ble to our model, here we attempt to test the model’s ability to incorporate complex features over source-side syntax trees, essentially subsuming and extend-ing previous work on tree-to-string translation mod-els[REF_CITE]."
"We first parse the source side of our training, development and test corpora using the Stanford parser. [URL_CITE]"
"Next, while building the synchronous charts required for training, whenever a rule is used in a derivation a feature is activated which captures: (1) the constituent spanning the rule’s source side in the syntax tree (if any) (2) constituents spanning any variables in the rule, and (3) the rule’s target side surface form."
Figure 3 illustrates this process.
These syntactic features are equivalent to the grammar rules extracted for tree-to-string translation systems.
"The key difference in our model is that the source syntax tree is treated as conditioning context and it’s information encoded as features, thus this information can be used or ignored as the model sees fit."
"This avoids the problems associated with explic-itly encoding the source syntax in the grammar, such as sparsity and overly constraining the model."
"In addition we could easily incorporate features over multiple source trees, for example mixing labelled syntax trees with dependency graphs."
"We limit the extraction of syntactic features to those that appear in at least two training derivations, giving a total of 4.2M syntactic features, for an over-all total of 7.1M features."
Table 3 shows the results from applying our described models to the test set.
We benchmark our results against a model (Hiero) which was directly trained to optimise B LEU NIST using the standard MERT algorithm[REF_CITE]and the full set of translation and lexical weight features described for the Hiero model[REF_CITE].
As well as
"B LEU NIST (brevity penalty uses the shortest ref-erence), we also include results from the IBM (B LEU IBM ) metric (brevity penalty uses the closest reference), and using only the actual human transla-tion in the test set, not the monolingual paraphrase multiple references (B LEU HumanRef )."
The first result of interest is that we see an increase in performance through the incorporation of the source syntax features.
"This is an encourag-ing result as the transcribed speech source sentences are well out of the domain of the data on which the parser was trained, suggesting that our model is able to sift the good information from the noisy in the unreliable source syntax trees."
Table 4 shows illus-trative system output on the test set.
On the B LEU NIST metric we see that our mod-els under-perform the MERT trained system.
We hypothesise that this is predominately due to the interaction of the brevity penalty with the unusual nature of the multiple paraphrase reference train-ing and development data.
Their performance is however quite consistent across the different inter-pretations of the brevity penalty (NIST vs. IBM).
"This contrasts with the MERT trained model, which clearly over-fits to the NIST metric that it was trained on and underperforms our models when eval-uated on the single human test translations."
"If we directly compare the brevity penalties of the MERT model (0.868) and our discriminative model incor-porating source syntax (0.942), on the these single references, we can see that the MERT training has optimised to the shortest paraphrase reference."
From these results it is difficult to draw any hard conclusions on the relative performance of the dif-ferent training regimes.
However we feel confident in claiming that we have achieved our goal of train-ing a probabilistic model on millions of sparse fea-tures which obtains performance competitive with the current state-of-the-art training algorithm.
In this paper we have shown that statistical machine translation can be effectively modelled as a well posed machine learning task.
"In doing so we have described a model capable of estimating a probabil-ity distribution over translations using sparse com-plex features, and achieving performance compara-ble to the state-of-the-art on standard metrics."
"With further work on scaling these models to large data sets, and engineering high performance features, we believe this research has the potential to provide sig-nificant increases in translation quality."
Minimum-error-rate training (MERT) is a bot-tleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably opti-mize.
"Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT."
"We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or sur-pass MERT in terms of both translation qual-ity and computational cost."
"We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik’s soft syn-tactic constraints, and, second, we introduce a novel structural distortion model."
In both cases we obtain significant improvements in translation performance.
"Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the[REF_CITE]Arabic-English eval-uation data."
"Since its introduction[REF_CITE], minimum er-ror rate training (MERT) has been widely adopted for training statistical machine translation (MT) sys-tems."
"However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 fea-tures."
"One recent example of this limitation is a series of experiments[REF_CITE], in which they added syntactic features to Hiero[REF_CITE], which ordinarily uses no linguistically motivated syntactic information."
Each of their new features rewards or punishes a deriva-tion depending on how similar or dissimilar it is to a syntactic parse of the input sentence.
"They found that in order to obtain the greatest improve-ment, these features had to be specialized for par-ticular syntactic categories and weighted indepen-dently."
"Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of fea-tures."
But it would have been preferable to use a training method that can optimize the features all at once.
"There has been much work on improving MERT’s performance[REF_CITE], or on replacing MERT wholesale[REF_CITE]."
"This paper continues a line of research on online discriminative training[REF_CITE], extending that[REF_CITE], who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006)."
"Our guiding princi-ple is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploit-ing more of the parse forest, we obtain results us-ing MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task."
"Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure."
"First, we generalize Mar-ton and Resnik’s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model."
"We ob-tain significant improvements in both cases, and fur-ther large improvements when the two feature sets are combined."
The paper proceeds as follows.
We describe our training algorithm in section 2; our generalization of Marton and Resnik’s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5.
"The translation model is a standard linear model[REF_CITE], which we train using MIRA[REF_CITE], following[REF_CITE]."
We describe the basic algorithm first and then progressively refine it.
"Let e, by abuse of notation, stand for both output strings and their derivations."
We represent the fea-ture vector for derivation e as h(e).
Initialize the fea-ture weights w.
"Then, repeatedly: • Select a batch of input sentences f 1 , . . . , f m . • Decode each f i to obtain a set of hypothesis translations e i1 , . . . , e in . • For each i, select one of the e ij to be the oracle translation e ∗i , by a criterion described below."
"Let ∆h ij = h(e i∗ ) − h(e ij ). • For each e ij , compute the loss ` ij , which is some measure of how bad it would be to guess e ij instead of e ∗i . • Update w to the value of w 0 that minimizes: m 1 0 X kw − wk 2 + C max(` ij − ∆h ij · w 0 ) (1) 2 i=1 1≤j≤n where we set C = 0.01."
"The first term means that we want w 0 to be close to w, and second term (the generalized hinge loss) means that we want w 0 to score e ∗i higher than each e ij by a margin at least as wide as the loss ` ij ."
"When training is finished, the weight vectors from all iterations are averaged together. (If multiple passes through the training data are made, we only average the weight vectors from the last pass.)"
"The technique of averaging was introduced in the con-text of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice[REF_CITE]."
We follow[REF_CITE]in applying this technique to MIRA.
"Note that the objective (1) is not the same as that used by Watanabe et al.; rather, it is the same as that used[REF_CITE]and related to that[REF_CITE]."
"We solve this opti-mization problem using a variant of sequential min-imal optimizati[REF_CITE]: for each i, initialize α ij = C for a single value of j such that e ij = e ∗i , and initialize α ij = 0 for all other values of j. Then, repeatedly choose a sentence i and a pair of hypothe-ses j, j 0 , and let w 0 ← w 0 + δ(∆h ij − ∆h ij 0 ) (2) α ij ← α ij + δ (3) α ij 0 ← α ij 0 − δ (4) where δ = clip (` ij − ` ij 0 ) − (∆h ij − ∆h ij 0 ) · w 0 (5) [−α ij ,α ij0 ] k∆h ij − ∆h ij 0 k 2 where the function clip [x,y] (z) gives the closest num-ber to z in the interval [x, y]."
"Assuming B as the evaluation criterion, the loss ` ij of e ij relative to e ∗i should be related somehow to the difference between their B scores."
"How-ever, B was not designed to be used on individ-ual sentences; in general, the highest-B transla-tion of a sentence depends on what the other sen-tences in the test set are."
"Sentence-level approxi-mations to B exist[REF_CITE], but we found it most effective to per-form B computations in the context of a set O of previously-translated sentences, following[REF_CITE]."
"However, we don’t try to accu-mulate translations for the entire dataset, but simply maintain an exponentially-weighted moving average of previous translations."
"More precisely: For an input sentence f, let e be some hypothesis translation and let {r k } be the set of reference translations for f. Let c(e; {r k }), or simply c(e) for short, be the vector of the following counts: |e|, the effective reference length min k |r k |, and, for 1 ≤ n ≤ 4, the number of n-grams in e, and the num-ber of n-gram matches between e and {r k }."
"These counts are sufficient to calculate a B score, which we write as B(c(e))."
The pseudo-document O is an exponentially-weighted moving average of these vectors.
"That is, for each training sentence, let ê be the 1-best translation; after processing the sentence, we update O, and its input length O f :"
O ← 0.9(O + c(ê)) (6)
O f ← 0.9(O f + |f|) (7)
"We can then calculate the B score of hypothe-ses e in the context of O. But the larger O is, the smaller the impact the current sentence will have on the B score."
"To correct for this, and to bring the loss function roughly into the same range as typical margins, we scale the B score by the size of the input:"
"B(e; f, {r k }) = (O f + |f|) × B(O + c(e; {r k })) (8) which we also simply write as B(e)."
"Finally, the loss function is defined to be: ` ij = B(e ∗i ) − B(e ij ) (9)"
We now describe the selection of e ∗ .
We know of three approaches in previous work.
"The first is to force the decoder to output the reference sentence exactly, and select the derivation with the highest model score, which[REF_CITE]call bold up-dating."
"The second uses the decoder to search for the highest-B translati[REF_CITE], which[REF_CITE]call max-B updating."
"Liang et al. and Arun and Koehn experi-ment with these methods and both opt for a third method, which Liang et al. call local updating: gen-erate an n-best list of translations and select the highest-B translation from it."
"The intuition is that due to noise in the training data or reference transla-tions, a high-B translation may actually use pe-culiar rules which it would be undesirable to en-courage the model to use."
"Hence, in local updating, the search for the highest-B translation is limited to the n translations with the highest model score, where n must be determined experimentally."
"Here, we introduce a new oracle-translation selec-tion method, formulating the intuition behind local updating as an optimization problem: e ∗ = arg max (B(e) + h(e) · w) (10) e"
"Instead of choosing the highest-B translation from an n-best list, we choose the translation that maximizes a combination of (approximate) B and the model."
"We can also interpret (10) in the following way: we want e ∗ to be the max-B translation, but we also want to minimize (1)."
"So we balance these two criteria against each other: e ∗ = arg max (B(e) − µ(B(e) − h(e) · w)) (11) e where (B(e) − h(e) · w) is that part of (1) that de-pends on e ∗ , and µ is a parameter that controls how much we are willing to allow some translations to have higher B than e ∗ if we can better minimize (1)."
Setting µ = 0 would reduce to max-B up-dating; setting µ = ∞ would never update w at all.
Setting µ = 0.5 reduces to equation (10).
Figure 1 shows the 10-best unique translations for a single input sentence according to equation (11) under various settings of µ. The points at far right are the translations that are scored highest according to the model.
"The µ = 0 points in the upper-left corner are typical of oracle translations that would be se-lected under the max-B policy: they indeed have a very high B score, but are far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled µ = 1."
"Our scheme would select one of the µ = 0.5 points, which have B scores almost as high as the max-B transla-tions, yet are not very far from the translations pre-ferred by the model."
What is the set {e ij } of translation hypotheses?
"Ide-ally we would let it be the set of all possible transla-tions, and let the objective function (1) take all of them into account."
"This is the approach taken[REF_CITE], but their approach assumes that the loss function can be decomposed into local loss functions."
"Since our loss function cannot be so de-composed, we select: • the 10-best translations according to the model; we then rescore the forest to obtain • the 10-best translations according to equation (11) with µ = 0.5, the first of which is the oracle translation, and • the 10-best translations with µ = ∞, to serve as negative examples."
The last case is wh[REF_CITE]call max-loss updating (where “loss” refers to the gener-alized hinge loss) and[REF_CITE]call loss-augmented inference.
"The rationale here is that since the objective (1) tries to minimize max j (` ij − ∆h ij · w 0 ), we should include the translations that have the highest (` ij − ∆h ij · w) in order to approximate the effect of using the whole forest."
See Figure 1 again for an illustration of the hy-potheses selected for a single sentence.
The max- B points in the upper left are not included (and would have no effect even if they were included).
"The µ = ∞ points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left."
"To perform the forest rescoring, we need to use several approximations, since an exact search for B-optimal translations is NP-hard[REF_CITE]."
"For every derivation e in the forest, we calcu-late a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches[REF_CITE], that is, the number of matches for an n-gram can be greater than the number of occurrences of the n-gram in any reference translation."
This can be done efficiently by calculating c for every hyper-edge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the frac-tion of the input sentence consumed by the rule • the number of n-grams formed by the applica-tion of the rule (1 ≤ n ≤ 4) • the (unclipped) number of n-gram matches formed by the application of the rule (1 ≤ n ≤ 4)
We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder[REF_CITE].
"To find the best derivation in the forest, we tra-verse it bottom-up as usual, and for every set of al-ternative subtranslations, we select the one with the highest score."
"But here a rough approximation lurks, because we need to calculate B on the nodes of the forest, but B does not have the optimal substructure property, i.e., the optimal score of a parent node can-not necessarily be calculated from the optimal scores of its children."
"Nevertheless, we find that this rescor-ing method is good enough for generating high-B oracle translations and low-B negative examples."
"One convenient property of MERT is that it is em-barrassingly parallel: we decode the entire tuning set sending different sentences to different processors, and during optimization of feature weights, differ-ent random restarts can be sent to different proces-sors."
"In order to make MIRA comparable in effi-ciency to MERT, we must parallelize it."
"But with an online learning algorithm, parallelization requires a little more coordination."
"We run MIRA on each processor simultaneously, with each maintaining its own weight vector."
"A master process distributes dif-ferent sentences from the tuning set to each of the processors; when each processor finishes decoding a sentence, it transmits the resulting hypotheses, with their losses, to all the other processors and re-ceives any hypotheses waiting from other proces-sors."
"Those hypotheses were generated from differ-ent weight vectors, but can still provide useful in-formation."
The sets of hypotheses thus collected are then processed as one batch.
"When the whole train-ing process is finished, we simply average all the weight vectors from all the processors."
"Having described our training algorithm, which includes several practical improvements to Watan-abe et al.’s usage of MIRA, we proceed in the re-mainder of the paper to demonstrate the utility of the our training algorithm on models with large numbers of structurally sensitive features."
The first features we explore are based on a line of research introduced[REF_CITE]and im-proved on[REF_CITE].
"A hi-erarchical phrase-based translation model is based on synchronous context-free grammar, but does not normally use any syntactic information derived from linguistic knowledge or treebank data: it uses trans-lation rules that span any string of words in the input sentence, without regard for parser-defined syntac-tic constituency boundaries."
"This feature can be viewed as a soft syntactic constraint: it biases the model toward translations that respect syntactic structure, but does not force it to use them."
"However, this more syntactically aware model, when tested in Chinese-English translation, did not improve trans-lation performance."
"Recently,[REF_CITE]revisited the idea of constituency features, and succeeded in showing that finer-grained soft syntactic constraints yield substantial improvements in B score for both Chinese-English and Arabic-English transla-tion."
"In addition to adding separate features for dif- ferent syntactic nonterminals, they introduced a new type of constraint that penalizes rules when the source language side crosses the boundaries of a source syntactic constituent, as opposed to simply rewarding rules when they are consistent with the source-language parse tree."
Marton and Resnik optimized their features’ weights using MERT.
"But since MERT does not scale well to large numbers of feature weights, they were forced to test individual features and manu-ally selected feature combinations each in a sepa-rate model."
"Although they showed gains in trans-lation performance for several such models, many larger, potentially better feature combinations re-mained unexplored."
"Moreover, the best-performing feature subset was different for the two language pairs, suggesting that this labor-intensive feature se-lection process would have to be repeated for each new language pair."
"Here, we use MIRA to optimize Marton and Resnik’s finer-grained single-category features all at once."
"We define below two sets of features, a coarse-grained class that combines several constituency cat-egories, and a fine-grained class that puts different categories into different features."
"Both kinds of fea-tures were used by Marton and Resnik, but only a few at a time."
"Crucially, our training algorithm pro-vides the ability to train all the fine-grained features, a total of 34 feature weights, simultaneously."
"Coarse-grained features As the basis for coarse-grained syntactic features, we selected the following nonterminal labels based on their frequency in the tuning data, whether they frequently cover a span of more than one word, and whether they repre-sent linguistically relevant constituents: NP, PP, S, VP, SBAR, ADJP, ADVP, and QP."
"We define two new features, one which fires when a rule’s source side span in the input sentence matches any of the above-mentioned labels in the input parse, and an-other which fires when a rule’s source side span crosses a boundary of one of these labels (e.g., its source side span only partially covers the words in a VP subtree, and it also covers some or all or the words outside the VP subtree)."
"These two features are equivalent to Marton and Resnik’s XP = and XP + feature combinations, respectively."
"Fine-grained features We selected the following nonterminal labels that appear more than 100 times in the tuning data: NP, PP, S, VP, SBAR, ADJP, WHNP, PRT, ADVP, PRN, and QP."
"The labels that were excluded were parts of speech, nonconstituent labels like FRAG, or labels that occurred only two or three times."
"For each of these labels X, we added a separate feature that fires when a rule’s source side span in the input sentence matches X, and a second feature that fires when a span crosses a boundary of X. These features are similar to Marton and Resnik’s X = and X + , except that our set includes features for WHNP, PRT, and PRN."
"In addition to parser-based syntactic constraints, which were introduced in prior work, we introduce a completely new set of features aimed at improv-ing the modeling of reordering within Hiero."
"Again, the feature definition gives rise to a larger number of features than one would expect to train successfully using MERT."
"In a phrase-based model, reordering is per-formed both within phrase pairs and by the phrase-reordering model."
"Both mechanisms are able to learn that longer-distance reorderings are more costly than shorter-distance reorderings: phrase pairs, because phrases that involve more extreme re-orderings will (presumably) have a lower count in the data, and phrase reordering, because models are usually explicitly dependent on distance."
"By contrast, in a hierarchical model, all reordering is performed by a single mechanism, the rules of the grammar."
"In some cases, the model will be able to learn a preference for shorter-distance reorderings, as in a phrase-based system, but in the case of a word being reordered across a nonterminal, or two non-terminals being reordered, there is no dependence in the model on the size of the nonterminal or nonter-minals involved in reordering."
"So, for example, if we have rules"
"X → (il dit X 1 , he said X 1 ) (12) X → (il dit X 1 , X 1 he said) (13) we might expect that rule (12) is more common in general, but that rule (13) becomes more and more rare as X 1 gets larger."
The default Hiero features have no way to learn this.
"To address this defect, we can classify every nonterminal pair occurring on the right-hand side of each grammar rule as “reordered” or “not re-ordered”, that is, whether it intersects any other word alignment link or nonterminal pair (see Figure 2)."
We then define coarse- and fine-grained versions of the structural distortion model.
"Coarse-grained features Let R be a binary-valued random variable that indicates whether a non-terminal occurrence is reordered, and let S be an integer-valued random variable that indicates how many source words are spanned by the nonterminal occurrence."
"We can estimate P(R | S) via relative-frequency estimation from the rules as they are ex-tracted from the parallel text, and incorporate this probability as a new feature of the model."
"Fine-grained features A difficulty with the coarse-grained reordering features is that the gram-mar extraction process finds overlapping rules in the training data and might not give a sensible proba-bility estimate; moreover, reordering statistics from the training data might not carry over perfectly into the translation task (in particular, the training data may have some very freely-reordering translations that one might want to avoid replicating in transla-tion)."
"As an alternative, we introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define a separate binary feature for each value of (R,S), where R is as above and S ∈ {?, 1, . .. , 9, ≥10} and ? means any size."
"For example, if a nonterminal with span 11 has its contents reordered, then the features (true, ≥10) and (true, ?) would both fire."
Grouping all sizes of 10 or more into a single feature is de-signed to avoid overfitting.
"Again, using MIRA makes it practical to train with the full fine-grained feature set—coincidentally also a total of 34 features."
"We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task."
"It is worth noting that this exper-imentation is on a larger scale than Watanabe et al.’s (2007), and considerably larger than Marton and Resnik’s (2008)."
The baseline model was Hiero with the following baseline features[REF_CITE]: • two language models • phrase translation probabilities p(f | e) and p(e | f ) • lexical weighting in both directions[REF_CITE]• word penalty • penalties for: – automatically extracted rules – identity rules (translating a word into it-self) – two classes of number/name translation rules – glue rules
The probability features are base-100 log-probabilities.
"The rules were extracted from all the allow-able parallel text from the[REF_CITE]evalua-tion (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions)."
Hierarchical rules were extracted from the most in-domain corpora (4.[Footnote_2]+5.4 million words) and phrases were extracted from the remain-der.
"2 We chose this policy for MIRA to avoid overfitting. How-ever, we could have used the tuning set for this purpose, just as with MERT: in none of our runs would this change have made more than a 0.2 B difference on the development set."
"We trained the coarse-grained distortion model on 10,000 sentences of the training data."
"Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English."
"Both were 5-gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that[REF_CITE]but using minimal perfect hashing[REF_CITE]."
We partitioned the documents of the[REF_CITE](newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a develop-ment set (1298 sentences).
"The test data was the[REF_CITE]Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences)."
"To obtain syntactic parses for this data, we tok-enized it according to the Arabic Treebank standard using AMIRA[REF_CITE], parsed it with the Stanford parser[REF_CITE], and then forced the trees back into the MT system’s tok-enization. [Footnote_1]"
"1 The only notable consequence this had for our experimen-tation is that proclitic Arabic prepositions were fused onto the first word of their NP object, so that the PP and NP brackets were coextensive."
We ran both MERT and MIRA on the tuning set using 20 parallel processors.
"We stopped MERT when the score on the tuning set stopped increas-ing, as is common practice, and for MIRA, we used the development set to decide when to stop train-ing. 2"
"In our runs, MERT took an average of 9 passes through the tuning set and MIRA took an average of 8 passes. (For comparison, Watanabe et al. report de-coding their tuning data of 663 sentences 80 times.)"
Table 1 shows the results of our experiments with the training methods and features described above.
All significance testing was performed against the first line (MERT baseline) using paired bootstrap re-sampling[REF_CITE].
"First of all, we find that MIRA is competitive with MERT when both use the baseline feature set."
"In- deed, the MIRA system scores significantly higher on the test set; but if we break the test set down by genre, we see that the MIRA system does slightly worse on newswire and better on newsgroups. (This is largely attributable to the fact that the MIRA trans-lations tend to be longer than the MERT transla-tions, and the newsgroup references are also rela-tively longer than the newswire references.)"
"When we add more features to the model, the two training methods diverge more sharply."
"When train-ing with MERT, the coarse-grained pair of syntax features yields a small improvement, but the fine-grained syntax features do not yield any further im-provement."
"By contrast, when the fine-grained fea-tures are trained using MIRA, they yield substan-tial improvements."
"We observe similar behavior for the structural distortion features: MERT is not able to take advantage of the finer-grained features, but MIRA is."
"Finally, using MIRA to combine both classes of features, 56 in all, produces the largest im-provement, 2.6 B points over the MERT baseline on the full test set."
We also tested some of the differences between our training method and Watanabe et al.’s (2007); the results are shown in Table 2.
"Compared with local updating (line 2), our method of selecting the ora-cle translation and negative examples does better by 0.5 B points on the development data."
Using loss-augmented inference to add negative examples to lo-cal updating (line 3) does not appear to help.
"Never-theless, the negative examples are important: for if we use our method for selecting the oracle transla-tion without the additional negative examples (line 4), the algorithm fails, generating very long transla-tions and unable to find a weight setting to shorten them."
"It appears, then, that the additional negative examples enable the algorithm to reliably learn from the enhanced oracle translations."
"Finally, we compared our parallelization method against a simpler method in which all processors learn independently and their weight vectors are all averaged together (line 5)."
We see that sharing in-formation among the processors makes a significant difference.
"In this paper, we have brought together two existing lines of work: the training method[REF_CITE], and the models[REF_CITE]and[REF_CITE]."
"Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both trans-lation quality and computational cost."
This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT.
"In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways[REF_CITE], and extend phrase-based translation with source-side soft syntactic constraints[REF_CITE]."
"All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is par-ticularly appealing because it can be used unobtru-sively with any hierarchically-structured translation model."
"Here, we have shown that using MIRA to weight all the constraints at once removes the cru-cial drawback of the approach, the problem of fea-ture selection."
"Finally, we have introduced novel structural dis-tortion features to fill a notable gap in the hierar-chical phrase-based approach."
"By capturing how re-ordering depends on constituent length, these fea-tures improve translation quality significantly."
"In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation."
"Language comprehension, as with all other cases of the extraction of meaningful struc-ture from perceptual input, takes places un-der noisy conditions."
"If human language comprehension is a rational process in the sense of making use of all available infor-mation sources, then we might expect uncer-tainty at the level of word-level input to af-fect sentence-level comprehension."
"However, nearly all contemporary models of sentence comprehension assume clean input—that is, that the input to the sentence-level com-prehension mechanism is a perfectly-formed, completely certain sequence of input tokens (words)."
"This article presents a simple model of rational human sentence comprehension under noisy input, and uses the model to in-vestigate some outstanding problems in the psycholinguistic literature for theories of ra-tional human sentence comprehension."
"We argue that by explicitly accounting for input-level noise in sentence processing, our model provides solutions for these outstanding prob-lems and broadens the scope of theories of hu-man sentence comprehension as rational prob-abilistic inference."
"Considering the adversity of the conditions under which linguistic communication takes place in ev-eryday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as suc-cessful at it as we are."
"Perhaps the leading expla-nation of this success is that (a) the linguistic sig-nal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance ([REF_CITE]2003;[REF_CITE])."
"Given the difficulty of this task coupled with the availability of redun-dancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension."
This idea is either implicit or explicit in several interactivist the-ories of probabilistic language comprehensi[REF_CITE].
"However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonethe-less a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentence-comprehension process."
It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity[REF_CITE].
"On the other hand, it is also possible that this partition has been a theoretical convenience but that, in fact, evidence at the sub-word level plays an important role in sentence processing, and that sentence-level information can in turn affect word recognition."
"If the latter is the case, then the ques-tion arises of how we might model this type of infor-mation flow, and what consequences it might have for our understanding of human language compre-hension."
This article employs the well-understood formalisms of probabilistic context-free grammars (PCFGs) and weighted finite-state automata (wF- SAs) to propose a novel yet simple noisy-channel probabilistic model of sentence comprehension un-der circumstances where there is uncertainty about word-level representations.
Section 2 introduces this model.
We use this new model to investigate two outstanding problems for the theory of rational sen-tence comprehension: one involving global infer-ence—the beliefs that a human comprehender ar-rives at regarding the meaning of a sentence after reading it in its entirety (Section 3)—and one involv-ing incremental inference—the beliefs that a com-prehender forms and updates moment by moment while reading each part of it (Section 4).
The com-mon challenge posed by each of these problems is an apparent failure on the part of the comprehender to use information made available in one part of a sentence to rule out an interpretation of another part of the sentence that is inconsistent with this informa-tion.
"In each case, we will see that the introduction of uncertainty into the input representation, coupled with noisy-channel inference, provides a unified so-lution within a theory of rational comprehension."
"The use of generative probabilistic grammars for parsing is well understood (e.g.,[REF_CITE])."
"The problem of using a probabilistic grammar G to find the “best parse” T for a known input string w is formulated as [Footnote_1] arg max P G (T |w) (I) T but a generative grammar directly defines the joint distribution P G (T,w) rather than the conditional distribution."
"1 By assumption, G is defined such that its complete pro-ductions T completely specify the string, such that P(w|T) is non-zero for only one value of w."
"In this case, Bayes’ rule is used to find the posterior:"
"P (T, w) P G (T |w) ="
"P ( (II) w) ∝ P (T, w) (III)"
"If the input string is unknown, the problem changes."
Suppose we have some noisy evidence I that determines a probability distribution over input strings P(w|I).
We can still use Bayes’ rule to ob-tain the posterior:
"P (T, I) P G (T |I) ="
"P (I) (IV) ∝ X P (I|T, w)P (w|T )P (T ) (V) w"
"Likewise, if we are focused on inferring which words were seen given an uncertain input, we have"
"P G (w|I) ∝ X P (I|T, w)P (w|T )P (T ) (VI) T"
"This paper considers situations such as controlled psycholinguistic experiments where we (the re-searchers) know the sentence w ∗ presented to a comprehender, but do not know the specific input I that the comprehender obtains."
"In this case, if we are, for example, interested in the expected infer-ences of a rational comprehender about what word string she was exposed to, the probability distribu-tion of interest is"
P(w|w ∗ ) =
"Z P ( |I, C w w )P T (I|w ∗ ) dI (VII) ∗ I where P C is the probability distribution used by the comprehender to process perceived input, and P T is the “true” probability distribution over the inputs that might actually be perceived given the true sen-tence."
Since the comprehender does not observe w ∗ we must have conditional independence between w and w ∗ given I. We can then apply Bayes’ rule to (VII) to obtain
C (I|w)P C (w)P T (I| P (w|w ∗ ) =
Z P ∗ w ) dI
I P C (I) (VIII) =
P C (w) Z P C (I|w)P T (I|w ∗ ) dI
"I P C (I) (IX) ∝ P C (w)Q(w, w ∗ ) (X) where Q(w, w ∗ ) is proportional to the integral term in Equation (IX)."
The term P C (w) corresponds to the comprehender’s prior beliefs; the integral term is the effect of input uncertainty.
"If com-prehenders model noise rationally, then we should have P C (I|w) ="
"P T (I|w), and thus Q(w,w ∗ ) becomes a symmetric, non-negative function of w and w ∗ ; hence the effect of input uncertainty can be modeled by a kernel function on input string pairs. (Similar conclusions result when the poste-rior distribution of interest is over structures T.)"
It is an open question which kernel functions might best model the inferences made in human sentence comprehension.
"Most obviously the kernel func-tion should account for noise (environmental, per-ceptual, and attentional) introduced into the signal en route to the neural stage of abstract sentence processing."
"In addition, this kernel function might also be a natural means of accounting for modeling error such as disfluencies[REF_CITE], word/phrase swaps, and even well-formed ut-terances that the speaker did not intend."
"For pur-poses of this paper, we limit ourselves to a simple kernel based on the Levenshtein distance LD(w, w ′ ) between words and constructed in the form of a weighted finite-state automat[REF_CITE]."
Suppose that the input word string w ∗ consists of words w 1...n .
We define the Levenshtein-distance kernel as follows.
"Start with a weighted finite-state automaton in the log semiring over the vocabulary Σ with states 0... n, state 0 being the start state and n the (zero-cost) final state."
"We add two types of arcs to this automaton: (a) substitution/deletion arcs (i − 1, w ′ ) → i, i ∈ 1, . . . , n, each with cost λLD(w i ,w ′ ), for all w ′ ∈ Σ ∪ {ǫ}; and (b) in-sertion loop arcs (j,w ′ ) → j, j ∈ 0,... ,n, each with cost λ LD(ǫ, w ′ ), for all w ′ ∈ Σ. [Footnote_2]"
"2 For purposes of computing the Levenshtein distance be-tween words, the epsilon label ǫ is considered to be a zero-length letter string."
"The result-ing wFSA K LD (w ∗ ) defines a function over w such that the summed weight of paths through the wFSA accepting w is log Q(w, w ∗ )."
"This kernel allows for the possibility of word substitutions (represented by the transition arcs with labels that are neither w i nor ǫ), word deletions (represented by the transition arcs with ǫ labels), and even word insertions (represented by the loop arcs)."
The unnormalized probability of each type of operation is exponential in the Leven-shtein distance of the change induced by the oper-ation.
"The term λ is a free parameter, with smaller values corresponding to noisier input."
Figure 1 gives an example of the Levenshtein-distance kernel for a simple vocabulary and sentence. [Footnote_3]
"3 The Levenshtein-distance kernel can be seen to be sym-metric in w,w ∗ as follows. Any path accepting w in the wFSA generated from w ∗ involves the following non-zero-cost transitions: insertions w 1′I...i , deletions w D1...j , and substi-tutions (w → w ′ ) S1...k . For each such path P, there will be exactly one path P ′ in the wFSA generated from w that ac-cepts w ∗ with insertions w 1D...j , deletions w 1′I...i , and substitu-tions (w ′ → w) S1...k . Due to the symmetry of the Levenshtein distance, the paths P and P ′ will have identical costs. There-fore the kernel is indeed symmetric."
"The problem of finding structures or strings with high posterior probability given a particular input string w ∗ is quite similar to the problem faced in the parsing of speech, where the acoustic input I to a parser can be represented as a lattice of possible word sequences, and the edges of the lattice have weights determined by a model of acoustic realiza-tion of words, P(I|w) ([REF_CITE]2004)."
"The two major differ-ences between lattice parsing and our problem are (a) we have integrated out the expected effect of noise, which is thus implicit in our choice of kernel; and (b) the loops in the Levenshtein-distance kernel mean that the input to parsing is no longer a lattice."
This latter difference means that some of the tech-niques applicable to string parsing and lattice pars-ing – notably the computation of inside probabilities – are no longer possible using exact methods.
We return to this difference in Sections 3 and 4.
"One clear prediction of the uncertain-input model of (VII)–(X) is that under appropriate circumstances, the prior expectations P C (w) of the comprehen-der should in principle be able to override the lin-guistic input actually presented, so that a sentence is interpreted as meaning—and perhaps even be-ing—something other than it actually meant or was."
"At one level, it is totally clear that comprehenders do this on a regular basis: the ability to do this is required for someone to act as a copy editor— that is, to notice and (crucially) correct mistakes on the printed page."
"In many cases, these types of correction happen at a level that may be below consciousness—thus we sometimes miss a typo but interpret the sentences as it was intended, or ignore the disfluency of a speaker."
"What has not been pre-viously proposed in a formal model, however, is that this can happen even when an input is a completely grammatical sentence."
"Here, we argue that an ef-fect demonstrated[REF_CITE](see also[REF_CITE]) is an example of expec-tations overriding input."
"When presented sentences of the forms in (1) using methods that did not per-mit rereading, and asked questions of the type Did the man hunt the deer?, experimental participants gave affirmative responses significantly more often for sentences of type (1a), in which the substring the man hunted the deer appears, than for either (1b) or (1c). (1) a."
While the man hunted the deer ran into the woods. (G ARDEN P ATH ) b.
"While the man hunted the pheasant the deer ran into the woods. (T RANSITIVE ) c. While the man hunted, the deer ran into the woods. (C OMMA )"
"This result was interpreted[REF_CITE]and[REF_CITE]as reflecting (i) the fact that there is a syntactic garden path in (1a)—after reading the first six words of the sen-tence, the preferred interpretation of the substring the man hunted the deer is as a simple clause in-dicating that the deer was hunted by the man—and (ii) that readers were not always successful at revis-ing away this interpretation when they saw the dis-ambiguating verb ran, which signals that the deer is actually the subject of the main clause, and that hunted must therefore be intransitive."
"Furthermore (and crucially), for (1a) participants also responded affirmatively most of the time to questions of the type Did the deer run into the woods?"
This result is a puzzle for existing models of sentence compre-hension because no grammatical analysis exists of any substring of (1a) for which the deer is both the object of hunted and the subject of ran.
"In fact, no formal model has yet been proposed to account for this effect."
"The uncertain-input model gives us a means of accounting for these results, because there are near neighbors of (1a) for which there is a global gram-matical analysis in which either the deer or a coref-erent NP is in fact the object of the subordinate-clause verb hunted."
"In particular, inserting the word it either before or after the deer creates such a near neighbor: (2) a."
While the man hunted the deer it ran into the woods. b.
While the man hunted it the deer ran into the woods.
We formalize this intuition within our model by us-ing the wFSA representation of the Levenshtein- distance kernel.
"A probabilistic context-free gram-mar (PCFG) representing the comprehender’s gram-matical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG[REF_CITE]."
"This intersection thus repre-sents the unnormalized posterior P C (T, w|w ∗ )."
"Be-cause there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see[REF_CITE]for possible approaches to approximat-ing the normalization constant)."
"We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parse-tree pairs with highest posterior probability."
"To test our account of the rational noisy-channel in-terpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed"
Brown corpus. [Footnote_4] Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed Brown corpus.
"4 Counts of these rules were obtained using tgrep2/Tregex tree-matching patterns[REF_CITE], available online[URL_CITE]We have also in-vestigated the use of broad-coverage PCFGs estimated using standard treebank-based techniques, but found that the compu-tational cost of inference with treebank-sized grammars was prohibitive."
"For each of the sen-tence sets like (1) used in Experiments 1a, 1b, and 2[REF_CITE]that have complete lex-ical coverage in the parsed Brown corpus (22 sets in total), a noisy-input wFSA was constructed us-ing K LD , permitting all words occurring more than 2500 times in the parsed Brown corpus as possi-ble edit/insertion targets. [Footnote_5] Figure 3 shows the av-erage proportion of parse trees among the 100 best parses in the intersection between this PCFG and the wFSA for each sentence for which an interpretation is available such that the deer or a coreferent NP is the direct object of hunted. [Footnote_6] The Levenshtein dis-tance penalty λ is a free parameter in the model, but the results are consistent for a wide range of λ: in-terpretations of type (2) are more prevalent both in terms of number mass for (1a) than for either (1b) or (1c)."
5 The word-frequency cutoff was introduced for computa-tional speed; we have obtained qualitatively similar results with lower word-frequency cutoffs.
"6 We took a parse tree to satisfy this criterion if the NP the deer appeared either as the matrix-clause subject or the embedded-clause object, and a pronoun appeared in the other position. In a finer-grained grammatical model, number/gender agreement would be enforced between such a pronoun and the NP in the posterior, so that the probability mass for these parses would be concentrated on cases where the pronoun is it."
"Furthermore, across 9 noise values for 22 sentence sets, there were never more interpretations of type (2) for C OMMA sentences than for the cor-responding G ARDEN P ATH sentences, and in only one case were there more such interpretations for a T RANSITIVE sentence than for the corresponding G ARDEN P ATH sentence."
We begin taking up the role of input uncertainty for incremental comprehension by posing a question: what is the optimal way to read a sentence on a page[REF_CITE]?
"Presumably, the goal of read-ing is to find a good compromise between scanning the contents of the sentence as quickly as possible while achieving an accurate understanding of the sentence’s meaning."
"To a first approximation, hu-mans solve this problem by reading each sentence in a document from beginning to end, regardless of the actual layout; whether this general solution is best understood in terms of optimality or rather as para-sitic on spoken language comprehension is an open question beyond the immediate scope of the present paper."
"However, about 10–15% of eye movements in reading are regressive[REF_CITE], and we may usefully refine our question to when a regressive eye movement might be a good decision."
"In traditional models of sentence comprehension, the optimal an-swer would certainly be “never”, since past observa-tions are known with certainty."
"But once uncertainty about the past is accounted for, it is clear that there may in principle be situations in which regressive saccades may be the best choice."
What are these situations?
"One possible answer would be: when the uncertainty (e.g., measured by entropy) about an earlier part of the sentence is high."
There are some cases in which this is probably the correct answer: many regressive eye movements are very small and the consensus in the eye-movement literature is that they represent corrections for motor error at the saccadic level.
"That is, the eyes over-shoot the intended target and regress to obtain in- formation about what was missed."
"However, mo-tor error can account only for short, isolated regres-sions, and about one-sixth of regressions are part of a longer series back into the sentence, into a much earlier part of the text which has already been read."
We propose that these regressive saccades might be the best choice when the most recent observed in-put significantly changes the comprehender’s beliefs about the earlier parts of the sentence.
"To make the discussion more concrete, we turn to another recent result in the psycholinguistic literature that has been argued to be problematic for rational theories of sen-tence comprehension."
"It has been shown[REF_CITE]that sen-tences such as (3) below induce considerable pro-cessing difficulty at the word tossed, as measured in word-by-word reading times: (3) The coach smiled at the player tossed a fris-bee. (L OCALLY C OHERENT )"
"Both intuition and controlled experiments reveal that this difficulty seems due at least in part to the cat-egory ambiguity of the word tossed, which is oc-casionally used as a participial verb but is much more frequently used as a simple-past verb."
"Al-though tossed in (3) is actually a participial verb in-troducing a reduced relative clause (and the player is hence its recipient), most native English speakers find it extremely difficult not to interpret tossed as a main verb and the player as its agent—far more dif-ficult than for corresponding sentences in which the critical participial verb is morphologically distinct from the simple past form ((4a), (4c); c.f. threw) or in which the relative clause is unreduced and thus clearly marked ((4b), (4c)). (4) a."
The coach smiled at the player thrown a frisbee. (L OCALLY I NCOHERENT ) b.
The coach smiled at the player who was tossed a frisbee. c.
The coach smiled at the player who was thrown a frisbee.
"The puzzle here for rational approaches to sentence comprehension is that the preceding top-down con-text provided by The coach smiled at. . . should com-pletely rule out the possibility of seeing a main verb immediately after player, hence a rational com- prehender should not be distracted by the part-of-speech ambiguity. 7"
The solution we pursue to this puzzle lies in the fact that (3) has many near-neighbor sentences in which the word tossed is in fact a simple-past tense verb.
Several possibilities are listed below in (5): (5) a.
The coach who smiled at the player tossed a frisbee. b.
The coach smiled as the player tossed a frisbee. c.
The coach smiled and the player tossed a frisbee. d.
The coach smiled at the player who tossed a frisbee. e. The coach smiled at the player that tossed a frisbee. f.
The coach smiled at the player and tossed a frisbee.
The basic intuition we follow is that simple-past verb tossed is much more probable where it appears in any of (5a)-(5f) than participial tossed is in (3).
"Therefore, seeing this word causes the comprehen-der to shift her probability distribution about the ear-lier part of the sentence away from (3), where it had been peaked, toward its near neighbors such as the examples in (5)."
This change in beliefs about the past is treated as an error identification signal (EIS).
"In reading, a sensible response to an EIS would be a slowdown or a regressive saccade; in spoken lan-guage comprehension, a sensible response would be to allocate more working memory resources to the comprehension task."
We quantify our proposed error identification sig-nal as follows.
"Consider the probability distribution over the input up to, but not including, a position j in a sentence w:"
"P (w [0,j) ) (XI) We use the subscripting [0,j) to illustrate that this interval is “closed” through to include the beginning of the string, but “open” at position j—that is, it in-cludes all material before position j but does not in-clude anything at that position or beyond."
"Let us then define the posterior distribution after seeing all input up through and including word i as P i (w [0,j) )."
We define the EIS induced by reading a word w i as follows:
"D P i (w [0,i) )||P i−1 (w [0,i) ) (XII) P i (w) X ≡ P i (w) log P i−1 (w) (XIII) w∈ { w [0,i) } where D(q||p) is the Kullback-Leibler divergence, or relative entropy, from p to q, a natural way of quantifying the distance between probability distri-butions[REF_CITE]which has also been argued for previously in modeling attention and surprise in both visual and linguistic cogniti[REF_CITE]."
"As in Section 3, we use a small probabilistic gram-mar covering the relevant structures in the problem domain to represent the comprehender’s knowledge, and a wFSA based on the Levenshtein-distance ker-nel to represent noisy input."
We are interested in comparing the EIS at the word tossed in (3) versus the EIS at the word thrown in (4a).
"In this case, the interval w [0,j) contains all the material that could possibly have come before the word tossed/thrown, but does not contain material at or after the position introduced by the word itself."
"Loops in the prob-abilistic grammar and the Levenshtein-distance ker-nel pose a challenge, however, to evaluating the EIS, because the normalization constant of the resulting grammar/input intersection is essential to evaluat-ing Equation (XIII)."
"To circumvent this problem, we eliminate loops from the kernel by allowing only one insertion per inter-word space. [Footnote_8] (See Section 5 for a possible alternative). inference experiment of Section 4."
"8 Technically, this involves the following transformation of a Levenshtein-distance wFSA. First, eliminate all loop arcs."
Rule weights given as negative log-probabilities in bits.
"Figure 4 shows the (finite-state) probabilistic grammar used for the study, with rule probabilities once again determined from the parsed Brown cor-pus using relative frequency estimation."
"To calcu-late the distribution over strings after exposure to the i-th word in the sentence, we “cut” the input wFSA such that all transitions and arcs after state 2i+2 were removed and replaced with a sequence of states j = 2i + 3, . . . , m, with zero-cost transitions (j −1, w ′ ) → j for all w ′ ∈ Σ∪{ǫ}, and each new j being a zero-cost final state. [Footnote_9] Because the intersec-tion between this “cut” wFSA and the probabilistic grammar is loop-free, it can be renormalized, and the EIS can be calculated without difficulty."
"9 The number of states added had little effect on results, so long as at least as many states were added as words remained in the sentence."
All the computations in this section were carried out using the OpenFST library[REF_CITE].
Figure 5 shows the average magnitude of the EIS for sentences (3) versus (4a) at the critical word po-sition tossed/thrown.
"Once again, the Levenshtein-distance penalty λ is a free parameter in the model, so we show model behavior as a function of λ, for the eight sentence pairs in Experiment 1 of Tabor et al. with complete lexical and syntactic coverage for the grammar of Figure 4."
"For values of λ where the EIS is non-negligible, it is consistently larger at the critical word (tossed in (3), thrown in (4a)) in the COHERENT condition than in the INCOHERENT condition."
"Across a range of eight noise levels, 67% of sentence pairs had a higher EIS in the COHERENT condition than in the INCOHERENT condition."
"Fur-thermore, the cases where the INCOHERENT condi-tion had a larger EIS occurred only for noise levels below 1.1 and above 3.6, and the maximum such EIS was quite small, at 0.067."
"Overall, the model’s be-havior is consistent with the experimental results[REF_CITE], and can be explained through the intuition described at the end of Section 4.1."
In this paper we have outlined a simple model of ra-tional sentence comprehension under uncertain in-put and explored some of the consequences for out-standing problems in the psycholinguistic literature.
"The model proposed here will require further em-pirical investigation in order to distinguish it from other proposals that have been made in the liter-ature, but if our proposal turns out to be correct it has important consequences for both the theory of language processing and cognition more gener-ally."
"Most notably, it furthers the case for ratio-nality in sentence processing; and it eliminates one of the longest-standing modularity hypotheses im-plicit in work on the cognitive science of language: a partition between systems of word recognition and sentence comprehensi[REF_CITE]."
"Unlike the pessimistic picture originally painted by Fodor, however, the interactivist picture resulting from our model’s joint inference over possible word strings and structures points to many rich details that still need to be filled in."
"These include questions such as what kernel functions best account for human com-prehenders’ modeling of noise in linguistic input, and what kinds of algorithms might allow represen-tations with uncertain input to be computed incre-mentally."
The present work could also be extended in sev-eral more technical directions.
Perhaps most notable is the problem of the normalization constant for the posterior distribution over word strings and struc-tures; this problem was circumvented via a k-best approach in Section 3 and by removing loops from the Levenshtein-distance kernel in Section 4.
"We believe, however, that a more satisfactory solution may exist via sampling from the posterior distribu-tion over trees and strings."
"This may be possible either by estimating normalizing constants for the posterior grammar using iterative weight propaga-tion and using them to obtain proper production rule probabilities[REF_CITE], or by using reversible-jump Markov-chain Monte Carlo (MCMC) techniques to sample from the pos-terior[REF_CITE], and estimating the normaliz-ing constant with annealing-based techniques[REF_CITE]or nested sampling[REF_CITE]."
Scaling the model up for use with treebank- size grammars is another area for technical improve-ment.
"Finally, we note that the model here could poten-tially find practical application in grammar correc-tion."
"Although the noisy channel has been in use for many years in spelling correction, our model could be used more generally for grammar corrections, in-cluding insertions, deletions, and (with new noise functions) potentially changes in word order."
One major bottleneck in conversational sys-tems is their incapability in interpreting un-expected user language inputs such as out-of-vocabulary words.
"To overcome this problem, conversational systems must be able to learn new words automatically during human ma-chine conversation."
"Motivated by psycholin-guistic findings on eye gaze and human lan-guage processing, we are developing tech-niques to incorporate human eye gaze for au-tomatic word acquisition in multimodal con-versational systems."
This paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowl-edge in word acquisition.
Our experiment re-sults indicate that eye gaze provides a poten-tial channel for automatically acquiring new words.
The use of extra temporal and domain knowledge can significantly improve acquisi-tion performance.
Interpreting human language is a challenging prob-lem in human machine conversational systems due to the flexibility of human language behavior.
"When the encountered vocabulary is outside of the sys-tem’s knowledge, conversational systems tend to fail."
It is desirable that conversational systems can learn new words automatically during human ma-chine conversation.
"While automatic word acquisi-tion in general is quite challenging, multimodal con-versational systems offer an unique opportunity to explore word acquisition."
"In a multimodal conversa-tional system where users can talk and interact with a graphical display, users’ eye gaze, which occurs naturally with speech production, provides a poten-tial channel for the system to learn new words auto-matically during human machine conversation."
Psycholinguistic studies have shown that eye gaze is tightly linked to human language processing.
Eye gaze is one of the reliable indicators of what a per-son is “thinking about”[REF_CITE].
The direction of eye gaze carries informa-tion about the focus of the user’s attenti[REF_CITE].
The perceived visual context in-fluences spoken word recognition and mediates syn-tactic processing of spoken sentences[REF_CITE].
"In addition, directly before speaking a word, the eyes move to the mentioned object[REF_CITE]."
"Motivated by these psycholinguistic findings, we are investigating the use of eye gaze for automatic word acquisition in multimodal conversation."
"Par-ticulary, this paper investigates the use of tempo-ral information about speech and eye gaze and do-main semantic relatedness for automatic word ac-quisition."
The domain semantic and temporal in-formation are incorporated in statistical translation models for word acquisition.
Our experiments show that the use of domain semantic and temporal infor-mation significantly improves word acquisition per-formance.
"In the following sections, we first describe the ba-sic translation models for word acquisition."
"Then, we describe the enhanced models that incorporate temporal and semantic information about speech and eye gaze for word acquisition."
"Finally, we present the results of empirical evaluation."
Word acquisition by grounding words to visual en-tities has been studied in many language ground-ing systems.
"For example, given speech paired with video images of single objects, mutual information between audio and visual signals was used to acquire words by associating acoustic phone sequences with the visual prototypes (e.g., color, size, shape) of ob-jects[REF_CITE]."
Generative mod-els were used to acquire words by associating words with image regions given parallel data of pictures and description text[REF_CITE].
"Differ-ent from these works, in our work, the visual atten-tion foci accompanying speech are indicated by eye gaze."
"Eye gaze is an implicit and subconscious in-put, which brings additional challenges in word ac-quisition."
Eye gaze has been explored for word acquisition in previous work.
"In[REF_CITE], given speech paired with eye gaze information and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions."
A re-cent investigation on word acquisition from tran-scribed speech and eye gaze in human machine con-versation was reported[REF_CITE].
"In this work, a translation model was developed to asso-ciate words with visual objects on a graphical dis-play."
"Different from these previous works, here we investigate the incorporation of extra knowledge, specifically speech-gaze temporal information and domain knowledge, with eye gaze to facilitate word acquisition."
We recruited users to interact with a simplified mul-timodal conversational system to collect speech and eye gaze data.
We are working on a 3D room decoration domain.
Figure 1 shows the 3D room scene that was shown to the user in the experiments.
"During the human machine conversa-tion, the system verbally asked the user a question (e.g., “what do you dislike about the arrangement of the room?”) or issued a request (e.g., “describe the left wall”) about the room."
The user provided responses by speaking to the system.
"During the experiments, users’ speech was recorded through an open microphone and users’ eye gaze was captured by an Eye Link II eye tracker."
Eye gaze data consists of the screen coordinates of each gaze point that was captured by the eye tracker at a sampling rate of 250hz.
"As for speech data, we collected 357 spoken utter-ances from 7 users’ experiments."
"The vocabulary size is 480, among which 227 words are nouns and adjectives."
We manually transcribed the collected speech.
"As for gaze data, the first step is to identify gaze fixation from raw gaze points."
"As shown in Fig-ure 1(a), the collected raw gaze points are very noisy."
They can not be used directly for identifying gaze fixated entities in the scene.
We processed the raw gaze data to eliminate invalid and saccadic gaze points.
Invalid gaze points occur when users look off the screen.
Saccadic gaze points occur during ballistic eye movements between gaze fixations.
"Vi-sion studies have shown that no visual processing occurs in the human mind during saccades (i.e., sac-cadic suppression)[REF_CITE]."
"Since eyes do not stay still but rather make small, frequent jerky move-ments, we average nearby gaze points to better iden-tify gaze fixations."
The processed eye gaze fixations are shown in Figure 1(b).
Figure 2 shows an excerpt of the collected speech and gaze fixation in one experiment.
"In the speech stream, each word starts at a particular timestamp."
"In the gaze stream, each gaze fixation has a start-ing timestamp t s and an ending timestamp t e ."
Each gaze fixation also has a list of fixated entities (3D ob-jects).
An entity e on the graphical display is fixated by gaze fixation f if the area of e contains fixation point of f.
"Given the collected speech and gaze fixations, we build parallel speech-gaze data set as follows."
"For each spoken utterance and its accompanying gaze fixations, we construct a pair of word sequence and entity sequence (w, e)."
The word sequence w con-sists of only nouns and adjectives in the utterance.
"Each gaze fixation results in a fixated entity in the entity sequence e. When multiple entities are fix-ated by one gaze fixation due to the overlapping of the entities, the forefront one is chosen."
"Also, we merge the neighboring gaze fixations that contain the same fixated entities."
"For the parallel speech and gaze streams shown in Figure 2, the resulting word sequence is w = [room chandelier] and the entity sequence is e = [bed frame door chandelier]."
"Since we are working on conversational systems where users interact with a visual scene, we consider the task of word acquisition as associating words with visual entities in the domain."
"Given the par-allel speech and gaze fixated entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate word-entity association probabilities p(w|e)."
The words with the highest association probabilities are chosen as ac-quired words for entity e.
"Using the translation model I[REF_CITE], where each word is equally likely to be aligned with each entity, we have m l 1 p(w|e) ="
Y X p(w j |e i ) (1) (l + 1) m j=1 i=0 where l and m are the lengths of entity and word sequences respectively.
This is the model used[REF_CITE]and[REF_CITE].
We refer to this model as Model-1 throughout the rest of this paper.
"Using the translation model II[REF_CITE], where alignments are dependent on word/entity po-sitions and word/entity sequence lengths, we have m l p(w|e) ="
"Y X p(a j = i|j, m, l)p(w j |e i ) (2) j=1 i=0 where a j = i means that w j is aligned with e i ."
"When a j = 0, w j is not aligned with any entity (e 0 represents a null entity)."
We refer to this model as Model-2.
"Compared to Model-1, Model-2 considers the or-dering of words and entities in word acquisition."
EM algorithms are used to estimate the probabilities p(w|e) in the translation models.
"In Model-2, word-entity alignments are estimated from co-occurring word and entity sequences in an unsupervised way."
"The estimated alignments are de-pendent on where the words/entities appear in the word/entity sequences, not on when those words and gaze fixated entities actually occur."
"Motivated by the finding that users move their eyes to the mentioned object directly before speaking a word[REF_CITE], we make the word-entity alignments dependent on their temporal relation in a new model (referred as Model-2t): m l p(w|e) ="
"Y X p t (a j = i|j, e, w)p(w j |e i ) (3) j=1 i=0 where p t (a j = i|j, e, w) is the temporal alignment probability computed based on the temporal dis-tance between entity e i and word w j ."
"We define the temporal distance between e i and w j as d(e i , w j ) = 0 t s (e i ) ≤ t s (w j ) ≤ t e (e i ) t e (e i ) − t s (w j ) t s (w j ) &gt; t e (e i ) (4)  t s (e i ) − t s (w j ) t s (w j ) &lt; t s (e i ) where t s (w j ) is the starting timestamp (ms) of word w j , t s (e i ) and t e (e i ) are the starting and ending timestamps (ms) of gaze fixation on entity e i ."
"The alignment of word w j and entity e i is de-cided by their temporal distance d(e i ,w j )."
"Based on the psycholinguistic finding that eye gaze hap-pens before a spoken word, w j is not allowed to be aligned with e i when w j happens earlier than e i (i.e., d(e i ,w j ) &gt; 0)."
"When w j happens no earlier than e i (i.e., d(e i , w j ) ≤ 0), the closer they are, the more likely they are aligned."
"Specifically, the tem-poral alignment probability of w j and e i in each co-occurring instance (w, e) is computed as p t (a j = i|j, e, w) = (0 d(e i , w j ) &gt; 0 exp[α·d(e i ,w j )] (5) Σ i exp[α·d(e i ,w j )] d(e i , w j ) ≤ 0 where α is a constant for scaling d(e i ,w j )."
"In our experiments, α is set to 0.005."
An EM algorithm is used to estimate probabilities p(w|e) in Model-2t.
"For the purpose of evaluation, we manually anno-tated the truly aligned word and entity pairs."
Fig-ure 3 shows the histogram of those truly aligned word and entity pairs over the temporal distance of aligned word and entity.
"We can observe in the fig-ure that 1) almost no eye gaze happens after a spo-ken word, and 2) the number of word-entity pairs with closer temporal distance is generally larger than the number of those with farther temporal distance."
This is consistent with our modeling of the tempo-ral alignment probability of word and entity (Equa-tion (5)).
Speech-gaze temporal alignment and occurrence statistics sometimes are not sufficient to associate words to an entity correctly.
"For example, suppose a user says “there is a lamp on the dresser” while looking at a lamp object on a table object."
"Due to their co-occurring with the lamp object, words dresser and lamp are both likely to be associated with the lamp object in the translation models."
"As a result, word dresser is likely to be incorrectly ac-quired for the lamp object."
"For the same reason, the word lamp could be acquired incorrectly for the ta-ble object."
"To solve this type of association prob-lem, the semantic knowledge about the domain and words can be helpful."
"For example, the knowledge that the word lamp is more semantically related to the object lamp can help the system avoid associat- ing the word dresser to the lamp object."
"Therefore, we are interested in investigating the use of semantic knowledge in word acquisition."
"On one hand, each conversational system has a domain model, which is the knowledge representa-tion about its domain such as the types of objects and their properties and relations."
"On the other hand, there are available resources about domain indepen-dent lexical knowledge (e.g., WordNet[REF_CITE])."
The question is whether we can utilize the domain model and external lexical knowledge re-source to improve word acquisition.
"To address this question, we link the domain concepts in the domain model with WordNet concepts, and define semantic relatedness of word and entity to help the system ac-quire domain semantically compatible words."
"In the following sections, we first describe our domain modeling, then define the semantic related-ness of word and entity based on domain modeling and WordNet semantic lexicon, and finally describe different ways of using the semantic relatedness of word and entity to help word acquisition."
We model the 3D room decoration domain as shown in Figure 4.
The domain model contains all do-main related semantic concepts.
"These concepts are linked to the WordNet concepts (i.e., synsets in the format of “word#part-of-speech#sense-id”)."
"Each of the entities in the domain has one or more properties (e.g., semantic type, color, size) that are denoted by domain concepts."
"For example, the entity dresser 1 has domain concepts SEM DRESSER and COLOR."
These domain concepts are linked to “dresser#n#4” and “color#n#1” in WordNet.
"Note that in the domain model, the domain con-cepts are not specific to a certain entity, they are gen-eral concepts for a certain type of entity."
Multiple entities of the same type have the same properties and share the same set of domain concepts.
"We compute the semantic relatedness of a word w and an entity e based on the semantic similarity be-tween w and the properties of e. Specifically, se-mantic relatedness SR(e, w) is defined as where c ie is the i-th property of entity e, s(c ie ) is the synset of property c ie as designed in domain model, s j (w) is the j-th synset of word w as defined in WordNet, and sim(·, ·) is the similarity score of two synsets."
We computed the similarity score of two synsets based on the path length between them.
The similar-ity score is inversely proportional to the number of nodes along the shortest path between the synsets as defined in WordNet.
"When the two synsets are the same, they have the maximal similarity score of 1."
The WordNet-Similarity tool[REF_CITE]was used for the synset similarity computation.
"We can use the semantic relatedness of word and entity to help the system acquire semantically com-patible words for each entity, and therefore improve word acquisition performance."
"The semantic relat-edness can be applied for word acquisition in two ways: post process learned word-entity association probabilities by rescoring them with semantic relat-edness, or directly affect the learning of word-entity associations by constraining the alignment of word and entity in the translation models."
"In the acquired word list for an entity e i , each word w j has an association probability p(w j |e i ) that is learned from a translation model."
"We use the semantic relatedness SR(e i , w j ) to redistribute the probability mass for each w j ."
The new association probability is given by: p 0 (w j |e i ) =
Pp(w j |e i )
"SR(e i , w j ) (7) j p(w j |e i )"
"SR(e i , w j )"
"When used to constrain the word-entity alignment in the translation model, semantic relatedness can be used alone or used together with speech-gaze tempo-ral information to decide the alignment probability of word and entity. • Using only semantic relatedness to constrain word-entity alignments in Model-2s, we have m l p(w|e) ="
"Y X p s (a j = i|j, e, w)p(w j |e i ) j=1 i=0 (8) where p s (a j = i|j, e, w) is the alignment prob-ability based on semantic relatedness,"
"SR(e i , w j ) p s (a j = i|j, e, w) ="
"P (9) i SR(e i , w j ) • Using semantic relatedness and temporal infor-mation to constrain word-entity alignments in Model-2ts, we have m l p(w|e) ="
"Y X p ts (a j = i|j, e, w)p(w j |e i ) j=1 i=0 (10) where p ts (a j = i|j,e,w) is the alignment probability that is decided by both temporal re-lation and semantic relatedness of e i and w j , p ts (a j = i|j, e, w) = p s (a j = i|j, e, w)p t (a j = i|j, e, w) (11) P p (a = i|j, e, w)p t (a j = i|j, e, w) i s j where p s (a j = i|j, e, w) is the semantic align-ment probability in Equation (9), and p t (a j = i|j, e, w) is the temporal alignment probability given in Equation (5)."
EM algorithms are used to estimate p(w|e) in Model-2s and Model-2ts.
"As discussed above, based on translation models, we can incorporate temporal and domain semantic in-formation to obtain p(w|e)."
This probability only provides a means to ground words to entities.
"In conversational systems, the ultimate goal of word acquisition is to make the system understand the se-mantic meaning of new words."
Word acquisition by grounding words to objects is not always sufficient for identifying their semantic meanings.
"Suppose the word green is grounded to a green chair object, so is the word chair."
"Although the system is aware that green is some word describing the green chair, it does not know that word green refers to the chair’s color while the word chair refers to the chair’s se-mantic type."
"Thus, after learning the word-entity as-sociations p(w|e) by the translation models, we need to further ground words to domain concepts of entity properties."
We further apply WordNet to ground words to do-main concepts.
"For each entity e, based on asso-ciation probabilities p(w|e), we can choose the n-best words as acquired words for e. Those n-best words have the n highest association probabilities."
"For each word w acquired for e, the grounded con-cept c ∗e for w is chosen as the one that has the highest semantic relatedness with w: c ∗e = arg max max sim(s(c ie ), s j (w)) (12) i j where sim(s(c ie ), s j (w)) is the semantic similarity score defined in Equation (6)."
We evaluate word acquisition performance of differ-ent models on the data collected from our user stud-ies (see Section 3).
"The following metrics are used to evaluate the words acquired for domain concepts (i.e., entity properties) {c ie }. • Precision P P # words correctly acquired for c iee i P P # words acquired for c iee i • Recall"
P P # words correctly acquired for c iee i P P # ground-truth [Footnote_1] words of c iee i • F-measure 2 × precision × recall precision + recall
1 The ground-truth words were compiled and agreed upon by two human judges.
"The metrics of precision, recall, and F-measure are based on the n-best words acquired for the entity properties."
"Therefore, we have different precision, recall, and F-measure when n changes."
"The metrics of precision, recall, and F-measure only provide evaluation on the top n candidate words."
"To measure the acquisition performance on the entire ranked list of candidate words, we define a new metric as follows: where N e is the number of all ground-truth words {w ie } for entity e, index(w ei ) is the in-dex of word w ie in the ranked list of candidate words for entity e."
Entities may have a different number of ground-truth words.
"For each entity e, we calculate a Recip-rocal Rank Rate (RRR), which measures how close the ranks of the ground-truth words in the candidate word list is to the best scenario where the top N e words are the ground-truth words for e. RRR is in the range of (0, 1]."
"The higher the RRR, the better is the word acquisition performance."
The average of RRRs across all entities gives the Mean Reciprocal Rank Rate (MRRR).
"Note that MRRR is directly based on the learned word-entity associations p(w|e), it is in fact a mea-sure of grounding words to entities."
"To compare the effects of different speech-gaze alignments on word acquisition, we evaluate the fol-lowing models: • Model-1 – base model I without word-entity alignment (Equation (1)). • Model-2 – base model II with positional align-ment (Equation (2)). • Model-2t – enhanced model with temporal alignment (Equation (3)). • Model-2s – enhanced model with semantic alignment (Equation (8)). • Model-2ts – enhanced model with both tempo-ral and semantic alignment[REF_CITE]."
"To compare the different ways of incorporating semantic relatedness in word acquisition as dis-cussed in Section 6.3.1, we also evaluate the follow-ing models: ent speech-gaze alignments."
Figure 6 shows the re-sults of models with semantic relatedness rescoring.
"In Figure 5 &amp; 6, n-best means the top n word candi-dates are chosen as acquired words for each entity."
The Mean Reciprocal Rank Rates of all models are compared in Figure 7.
"As shown in Figure 5, Model-2 does not show a consistent improvement compared to Model-1 when a different number of n-best words are chosen as ac-quired words."
This result shows that it is not very helpful to consider the index-based positional align-ment of word and entity for word acquisition.
"Figure 5 also shows that models considering temporal or/and semantic information (Model-2t, Model-2s, Model-2ts) consistently perform better than the models considering neither temporal nor semantic information (Model-1, Model-2)."
"Among Model-2t, Model-2s, and Model-2ts, it is found that they do not make consistent differences."
"As shown in Figure 7, the MRRRs of different models are consistent with their performances on F-measure."
A t-test has shown that the difference be-tween the MRRRs of Model-1 and Model-2 is not statistically significant.
"Compared to Model-1, t- tests have confirmed that MRRR is significantly im-proved by Model-2t (t = 2.27, p &lt; 0.02), Model-2s (t = 3.40, p &lt; 0.01), and Model-2ts(t = 2.60, p &lt; 0.01)."
"T-tests have shown no significant differences among Model-2t, Model-2s, and Model-2ts."
Figure 6 shows that semantic relatedness rescor-ing improves word acquisition.
"After semantic re-latedness rescoring of the word-entity associations learned by Model-1, Model-1-r improves the F-measure consistently when a different number of n-best words are chosen as acquired words."
"Com-pared to Model-2t, Model-2t-r also improves the F-measure consistently."
"Comparing the two ways of using semantic relat-edness for word acquisition, it is found that rescor-ing word-entity association with semantic related-ness works better."
"When semantic relatedness is used together with temporal information to constrain word-entity alignments in Model-2ts, word acqui- sition performance is not improved compared to Model-2t."
"However, using semantic relatedness to rescore word-entity association learned by Model-2t, Model-2t-r further improves word acquisition."
"As shown in Figure 7, the MRRRs of Model-1-r and Model-2t-r are consistent with their per-formances on F-measure."
"Compared to Model-2t, Model-2t-r improves MRRR."
"A t-test has confirmed that this is a significant improvement (t = 1.97, p &lt; 0.03)."
"Compared to Model-1, Model-1-r signifi-cantly improves MRRR (t = 2.33, p &lt; 0.02)."
There is no significant difference between Model-1-r and Model-2t/Model-2s/Model-2ts.
"In Figures 5&amp;6, we also notice that the recall of the acquired words is still comparably low even when 10 best word candidates are chosen for each entity."
This is mainly due to the scarcity of those words that are not acquired in the data.
"Many of the words that are not acquired appear less than 3 times in the data, which makes them unlikely to be associated with any entity by the translation models."
"When more data is available, we expect to see higher recall."
Table 1 shows the 5-best words acquired by different models for the entity dresser 1 in the 3d room scene (see Figure 1).
"In the table, each word is followed by its word-entity association probability p(w|e)."
The correctly acquired words are shown in bold font.
"As shown in the example, the baseline Model-1 learned 2 correct words in the 5-best list."
"Consid-ering speech-gaze temporal information, Model-2t learned one more correct word vanity in the 5-best list."
"With semantic relatedness rescoring, Model-2t-r further acquired word desk in the 5-best list because of the high semantic relatedness of word desk and the type of entity dresser 1."
"Although nei-ther Model-1 nor Model-2t successfully acquired the word desk in the 5-best list, the rank (=7) of the word desk in Model-2t’s n-best list is much higher than the rank (=21) in Model-1’s n-best list."
"Motivated by the psycholinguistic findings, we in-vestigate the use of eye gaze for automatic word ac-quisition in multimodal conversational systems."
"Par-ticularly, we investigate the use of speech-gaze tem-poral information and word-entity semantic related-ness to facilitate word acquisition."
"Our experiments show that word acquisition is significantly improved when temporal information is considered, which is consistent with the previous psycholinguistic find-ings about speech and eye gaze."
"Moreover, using temporal information together with semantic relat-edness rescoring further improves word acquisition."
Eye tracking systems are no longer bulky sys-tems that prevent natural human machine commu-nication.
"Display mounted gaze tracking systems (e.g., Tobii) are completely non-intrusive, can toler-ate head motion, and provide high tracking quality."
Integrating eye tracking with conversational inter-faces is no longer beyond reach.
Recent works have shown that eye gaze can facilitate spoken language processing in conversational systems[REF_CITE].
Incorporating eye gaze with automatic word acquisition provides an-other potential approach to improve the robustness of human machine conversation.
Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming.
"We ex-plore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web."
"We investigate five tasks: af-fect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation."
"For all five, we show high agreement between Mechani-cal Turk non-expert annotations and existing gold standard labels provided by expert label-ers."
"For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effec-tive as using gold standard annotations from experts."
We propose a technique for bias correction that significantly improves annota-tion quality on two tasks.
We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.
"Large scale annotation projects such as TreeBank[REF_CITE], PropBank[REF_CITE], TimeBank[REF_CITE], FrameNet[REF_CITE], SemCor[REF_CITE], and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algo-rithms."
"The construction of these datasets, how-ever, is extremely expensive in both annotator-hours and financial cost."
"Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them[REF_CITE], one promising alternative for some tasks is the collection of non-expert annotations."
In this work we explore the use of Amazon Me-chanical Turk [URL_CITE] (AMT) to determine whether non-expert labelers can provide reliable natural language annotations.
"We chose five natural language under-standing tasks that we felt would be sufficiently nat-ural and learnable for non-experts, and for which we had gold standard labels from expert labelers, as well as (in some cases) expert labeler agree-ment information."
"The tasks are: affect recogni-tion, word similarity, recognizing textual entailment, event temporal ordering, and word sense disam-biguation."
"For each task, we used AMT to annotate data and measured the quality of the annotations by comparing them with the gold standard (expert) la-bels on the same data."
"Further, we compare machine learning classifiers trained on expert annotations vs. non-expert annotations."
"In the next sections of the paper we introduce the five tasks and the evaluation metrics, and offer methodological insights, including a technique for bias correction that improves annotation quality. [Footnote_2]"
"2 Please[URL_CITE]for a condensed version of this paper, follow-ups, and on-going public discussion. We encourage comments to be di-rected here in addition to email when appropriate. Dolores Labs Blog, “AMT is fast, cheap, and good for machine learning data,” Brendan O’Connor, Sept. 9, 2008. More related work[URL_CITE]."
The idea of collecting annotations from volunteer contributors has been used for a variety of tasks.
"Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, includ-ing the ESPGame for labeling images (v[REF_CITE]) and Verbosity for annotating word relations (v[REF_CITE])."
"The Open Mind Initiative[REF_CITE]has taken a similar approach, attempting to make such tasks as annotating word sense[REF_CITE]and common-sense word relations[REF_CITE]sufficiently “easy and fun” to entice users into freely labeling data."
There have been an increasing number of experi-ments using Mechanical Turk for annotation.
"In[REF_CITE]workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels."
Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4).
At least several studies have already used AMT without external gold standard comparisons.
In[REF_CITE]workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an au-tomatic method of noun compound paraphrasing.
"In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data."
It is powerful because independent annotations can be aggregated to achieve high reliability.
"Since we focus on empirically val-idating AMT as a data source, we tend to stick to simple aggregation methods."
In this section we describe Amazon Mechanical Turk and the general design of our experiments.
We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert label-ers.
AMT is an online labor market where workers are paid small amounts of money to complete small tasks.
The design of the system is as follows: one is required to have an Amazon account to either sub-mit tasks for annotations or to annotate submitted tasks.
"These Amazon accounts are anonymous, but are referenced by a unique Amazon ID."
"A Requester can create a group of Human Intelligence Tasks (or HITs), each of which is a form composed of an arbi-trary number of questions."
"The user requesting an-notations for the group of HITs can specify the num-ber of unique annotations per HIT they are willing to pay for, as well as the reward payment for each individual HIT."
"While this does not guarantee that unique people will annotate the task (since a single person could conceivably annotate tasks using mul-tiple accounts, in violation of the user agreement), this does guarantee that annotations will be collected from unique accounts."
"AMT also allows a requester to restrict which workers are allowed to annotate a task by requiring that all workers have a particular set of qualifications, such as sufficient accuracy on a small test set or a minimum percentage of previ-ously accepted submissions."
Annotators (variously referred to as Workers or Turkers) may then annotate the tasks of their choosing.
"Finally, after each HIT has been annotated, the Requester has the option of approving the work and optionally giving a bonus to individual workers."
"There is a two-way commu- nication channel between the task designer and the workers mediated by Amazon, and Amazon handles all financial transactions."
"In general we follow a few simple design principles: we attempt to keep our task descriptions as succinct as possible, and we attempt to give demonstrative examples for each class wherever possible."
We have published the full experimental design and the data we have collected for each task online [Footnote_3] .
3 All tasks and collected data are available[URL_CITE].
We have restricted our study to tasks where we require only a multiple-choice response or numeric input within a fixed range.
For every task we collect ten inde-pendent annotations for each unique item; this re-dundancy allows us to perform an in-depth study of how data quality improves with the number of inde-pendent annotations.
"We analyze the quality of non-expert annotations on five tasks: affect recognition, word similarity, rec-ognizing textual entailment, temporal event recogni-tion, and word sense disambiguation."
In this section we define each annotation task and the parameters of the annotations we request using AMT.
"Addition-ally we give an initial analysis of the task results, and summarize the cost of the experiments."
"This experiment is based on the affective text an-notation task proposed[REF_CITE], wherein each annotator is presented with a list of short headlines, and is asked to give numeric judgments in the interval [0,100] rating the headline for six emotions: anger, disgust, fear, joy, sadness, and surprise, and a single numeric rating in the inter-val [-100,100] to denote the overall positive or nega-tive valence of the emotional content of the headline, as in this sample headline-annotation pair:"
"Outcry at N Korea ‘nuclear test’ ([REF_CITE]), ([REF_CITE]), ([REF_CITE]), ( Joy,0 ), ([REF_CITE]), ([REF_CITE]), ( Valence,-50 )."
"For our experiment we select a 100-headline sample from the original SemEval test set, and collect 10 affect annotations for each of the seven label types, for a total of 7000 affect labels."
We then performed two comparisons to evaluate the quality of the AMT annotations.
"First, we asked how well the non-experts agreed with the experts."
We did this by comparing the interannotator agree-ment (ITA) of individual expert annotations to that of single non-expert and averaged non-expert anno-tations.
In the original experiment ITA is measured by calculating the Pearson correlation of one anno-tator’s labels with the average of the labels of the other five annotators.
"For each expert labeler, we computed this ITA score of the expert against the other five; we then average these ITA scores across all expert annotators to compute the average expert ITA (reported in Table 1 as “E vs. E”."
"We then do the same for individual non-expert annotations, averag-ing Pearson correlation across all sets of the five ex-pert labelers (“NE vs. E”)."
We then calculate the ITA for each expert vs. the averaged labels from all other experts and non-experts (marked as “E vs. All”) and for each non-expert vs. the pool of other non-experts and all experts (“NE vs. All”).
"We compute these ITA scores for each emotion task separately, aver-aging the six emotion tasks as “Avg. Emo” and the average of all tasks as “Avg. All”."
"The results in Table 1 conform to the expectation that experts are better labelers: experts agree with experts more than non-experts agree with experts, although the ITAs are in many cases quite close."
"But we also found that adding non-experts to the gold standard (“E vs. All”) improves agreement, suggest-ing that non-expert annotations are good enough to increase the overall quality of the gold labels."
Our first comparison showed that individual experts were better than individual non-experts.
In our next com-parison we ask how many averaged non-experts it would take to rival the performance of a single ex-pert.
"We did this by averaging the labels of each pos-sible subset of n non-expert annotations, for value of n in {1, 2, . . . , 10}."
"We then treat this average as though it is the output of a single ‘meta-labeler’, and compute the ITA with respect to each subset of five of the six expert annotators."
We then average the results of these studies across each subset size; the results of this experiment are given in Table 2 and in Figure 1.
"In addition to the single meta-labeler, we ask: what is the minimum number of non-expert an-notations k from which we can create a meta-labeler that has equal or better ITA than an expert annotator?"
"In Table 2 we give the minimum k for each emotion, and the averaged ITA for that meta-labeler consist-ing of k non-experts (marked “k-NE”)."
In Figure 1 we plot the expert ITA correlation as the horizontal dashed line.
"These results show that for all tasks except “Fear” we are able to achieve expert-level ITA with the held-out set of experts within 9 labelers, and fre-quently within only 2 labelers."
Pooling judgments across all 7 tasks we find that on average it re-quires only [Footnote_4] non-expert annotations per example to achieve the equivalent ITA as a single expert anno-tator.
"4[REF_CITE]and others originally used a numerical score of [0,4]."
"Given that we paid US$2.00 in order to collect the 7000 non-expert annotations, we may interpret our rate of 3500 non-expert labels per USD as at least 875 expert-equivalent labels per USD."
"This task replicates the word similarity task used[REF_CITE], following a previous task initially proposed[REF_CITE]."
"Specifically, we ask for numeric judgments of word similarity for 30 word pairs on a scale of [0,10], allowing fractional responses 4 ."
"These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string})."
"Nu-merous expert and non-expert studies have shown that this task typically yields very high interannota-tor agreement as measured by Pearson correlation;[REF_CITE]found a 0.97 correla-tion of the annotations of 38 subjects with the an-notations given by 51 subjects[REF_CITE], and a following study[REF_CITE]with 10 subjects found a 0.958 correlation[REF_CITE]."
"In our experiment we ask for 10 annotations each of the full 30 word pairs, at an offered price of $0.02 for each set of 30 annotations (or, equivalently, at the rate of 1500 annotations per USD)."
"The most surprising aspect of this study was the speed with which it was completed; the task of 300 annotations was completed by 10 annotators in less than 11 min- utes from the time of submission of our task to AMT, at the rate of 1724 annotations / hour."
As in the previous task we evaluate our non-expert annotations by averaging the numeric re-sponses from each possible subset of n annotators and computing the interannotator agreement with respect to the gold scores reported[REF_CITE].
"Our results are displayed in Figure 2, with Resnik’s 0.958 correlation plotted as the hor-izontal line; we find that at 10 annotators we achieve a correlation of 0.952, well within the range of other studies of expert and non-expert annotations."
This task replicates the recognizing textual entail-ment task originally proposed in the PASCAL Rec-ognizing Textual Entailment task[REF_CITE]; here for each question the annotator is pre-sented with two sentences and given a binary choice of whether the second hypothesis sentence can be inferred from the first.
"For example, the hypothesis sentence “Oil prices drop” would constitute a true entailment from the text “Crude Oil Prices Slump”, but a false entailment from “The government an-nounced last week that it plans to raise oil prices”."
For this dataset expert interannotator agreement studies have been reported as achieving 91% and 96% agreement over various subsections of the corpus.
"When con-sidering multiple non-expert annotations for a sen-tence pair we use simple majority voting, breaking ties randomly and averaging performance over all possible ways to break ties."
"5 It might seem pointless to consider an even number of an-notations in this circumstance, since the majority voting mech-anism and tie-breaking yields identical performance for 2n + 1 and 2n + 2 annotators; however, in Section 5 we will consider methods that can make use of the even annotations."
"This task is inspired by the TimeBank corpus[REF_CITE], which includes among its anno-tations a label for event-pairs that represents the tem-poral relation between them, from a set of fourteen relations (before, after, during, includes, etc.)."
"We implement temporal ordering as a simplified version of the TimeBank event temporal annotation task: rather than annotating all fourteen event types, we restrict our consideration to the two simplest labels: “strictly before” and “strictly after”."
"Furthermore, rather than marking both nouns and verbs in the text as possible events, we only consider possible verb events."
"We extract the 462 verb event pairs labeled as “strictly before” or “strictly after” in the Time-Bank corpus, and we present these pairs to annota-tors with a forced binary choice on whether the event described by the first verb occurs before or after the second."
"For example, in a dialogue about a plane explosion, we have the utterance: “It just blew up in the air, and then we saw two fireballs go down to the, to the water, and there was a big small, ah, smoke, from ah, coming up from that”."
"Here for each anno-tation we highlight the specific verb pair of interest (e.g., go/coming, or blew/saw) and ask which event occurs first (here, go and blew, respectively)."
The results of this task are presented in Figure 4.
"We achieve high agreement for this task, at a rate of 0.94 with simple voting over 10 annotators (4620 total annotations)."
"While an expert ITA of 0.77 was reported for the more general task involving all four-teen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task."
"In this task we consider a simple problem on which machine learning algorithms have been shown to produce extremely good results; here we annotate part of the SemEval Word Sense Disambiguation Lexical Sample task[REF_CITE]; specif-ically, we present the labeler with a paragraph of text containing the word “president” (e.g., a para-graph containing “Robert E. Lyons III...was ap-pointed president and chief operating officer...”) and ask the labeler which one of the following three sense labels is most appropriate: 1) executive officer of a firm, corporation, or university 2) head of a country (other than the U.S.) 3) head of the U.S., President of the United States"
"As shown in Figure 5, performing simple majority voting (with random tie-breaking) over an- notators results in a rapid accuracy plateau at a very high rate of 0.994 accuracy."
"In fact, further analy-sis reveals that there was only a single disagreement between the averaged non-expert vote and the gold standard; on inspection it was observed that the an-notators voted strongly against the original gold la-bel (9-to-1 against), and that it was in fact found to be an error in the original gold standard annotation. [Footnote_6] After correcting this error, the non-expert accuracy rate is 100% on the 177 examples in this task."
6 The example sentence began “The Egyptian president said he would visit Libya today...” and was mistakenly marked as the “head of a company” sense in the gold annotation (example id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
This is a specific example where non-expert annotations can be used to correct expert annotations.
"Since expert ITA was not reported per word on this dataset, we compare instead to the performance of the best automatic system performance for dis-ambiguating “president”[REF_CITE], with an accuracy of 0.98."
In Table 3 we give a summary of the costs asso-ciated with obtaining the non-expert annotations for each of our 5 tasks.
Here Time is given as the to-tal amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.
The reliability of individual workers varies.
"Some are very accurate, while others are more careless and make mistakes; and a small few give very noisy re-sponses."
"Furthermore, for most AMT data collec-tion experiments, a relatively small number of work-ers do a large portion of the task, since workers may do as much or as little as they please."
Figure 6 shows accuracy rates for individual workers on one task.
"Both the overall variability, as well as the prospect of identifying high-volume but low-quality workers, suggest that controlling for individual worker qual-ity could yield higher quality overall judgments."
"In general, there are at least three ways to enhance quality in the face of worker error."
"More work-ers can be used, as described in previous sections."
"Another method is to use Amazon’s compensation mechanisms to give monetary bonuses to highly-performing workers and deny payments to unreli-able ones; this is useful, but beyond the scope of this paper."
"In this section we explore a third alterna- tive, to model the reliability and biases of individual workers and correct for them."
A wide number of methods have been explored to correct for the bias of annotators.
They introduce an EM algorithm to simul-taneously estimate annotator biases and latent label classes.
A large literature in bio-statistics addresses this same problem for medical diagnosis.
"Here we take an approach based on gold standard labels, using a small amount of expert-labeled train-ing data in order to correct for the individual biases of different non-expert annotators."
The idea is to re-calibrate worker’s responses to more closely match expert behavior.
"We focus on categorical examples, though a similar method can be used with numeric data."
"Following Dawid and Skene, we model labels and workers with a multinomial model similar to Naive Bayes."
Every example i has a true label x i .
"For sim-plicity, assume two labels {Y,N}."
"Several differ-ent workers give labels y i1 , y i2 , . . . y iW ."
"A worker’s conditional probability of response is modeled as multinomial, and we model each worker’s judgment as conditionally independent of other workers given the true label x i , i.e.: iw |x i )! p(x ) P (y i1 , . . . , y iW , x i ) ="
Y P (y i w
"To infer the posterior probability of the true label for a new example, worker judgments are integrated via Bayes rule, yielding the posterior log-odds:"
P (x i = Y |y i1 . . . y iW ) log P (x i = N|y i1 . . . y iW )
P (y iw |x i = Y ) + log P (x i = Y ) =
X log P(y |x = N) w iw i P (x i = N)
"The worker response likelihoods P(y w |x = Y ) and P(y w |x = N) can be directly estimated from frequencies of worker performance on gold standard examples. (If we used maximum likelihood esti-mation with no Laplace smoothing, then each y w |x is just the worker’s empirical confusion matrix.)"
"For MAP label estimation, the above equation de-scribes a weighted voting rule: each worker’s vote is weighted by their log likelihood ratio for their given response."
"Intuitively, workers who are more than 50% accurate have positive votes; workers whose judgments are pure noise have zero votes; and an-ticorrelated workers have negative votes. (A simpler form of the model only considers accuracy rates, thus weighting worker votes by log 1−accacc ."
But we w w use the full unconstrained multinomial model here.) 5.1.1 Example tasks: RTE-1 and event annotation
"We used this model to improve accuracy on the RTE-1 and event annotation tasks. (The other cate-gorical task, word sense disambiguation, could not be improved because it already had maximum accu-racy.)"
First we took a sample of annotations giving k responses per example.
"Within this sample, we trained and tested via 20-fold cross-validation across examples."
"Worker models were fit using Laplace smoothing of 1 pseudocount; label priors were uni-form, which was reasonably similar to the empirical distribution for both tasks."
Figure 7 shows improved accuracy at different numbers of annotators.
The lowest line is for the naive 50% majority voting rule. (This is equivalent to the model under uniform priors and equal accu-racies across workers and labels.)
"Each point is the data set’s accuracy against the gold labels, averaged across resamplings each of which obtains k annota-tions per example."
"RTE has an average +4.0% ac- curacy increase, averaged across 2 through 10 anno-tators."
We find a +3.4% gain on event annotation.
"Finally, we experimented with a similar calibration method for numeric data, using a Gaussian noise model for each worker: y w |x ∼ N(x + µ w ,σ w )."
"On the affect task, this yielded a small but consis-tent increases in Pearson correlation at all numbers of annotators, averaging a +0.6% gain."
In this section we train a supervised affect recogni-tion system with expert vs. non-expert annotations.
"For the purpose of this experiment we create a sim-ple bag-of-words unigram model for predicting af-fect and valence, similar to the SWAT system[REF_CITE], one of the top-performing systems on the SemEval Affective Text task. [Footnote_7]"
"7 Unlike the SWAT system we perform no lemmatization, synonym expansion, or any other preprocessing of the tokens; we simply use whitespace-separated tokens within each head-line."
"For each token t in our training set, we assign t a weight for each emotion e equal to the average emotion score ob-served in each headline H that t participates in. i.e., if H t is the set of headlines containing the token t, then:"
"Score(e, t) ="
"P H∈H t Score(e, H) |H t |"
"With these weights of the individual tokens we may then compute the score for an emotion e of a new headline H as the average score over the set of tokens t ∈ H that we’ve observed in the training set (ignoring those tokens not in the training set), i.e.:"
"Score(e, t) Score(e, H) = X t∈H |H|"
"Where |H| is simply the number of tokens in headline H, ignoring tokens not observed in the training set."
"Since we are fortunate to have the six separate ex-pert annotations in this task, we can perform an ex-tended systematic comparison of the performance of the classifier trained with expert vs. non-expert data."
For this evaluation we compare the performance of systems trained on expert and non-expert annota-tions.
"For each expert annotator we train a system using only the judgments provided by that annota-tor, and then create a gold standard test set using the average of the responses of the remaining five label-ers on that set."
"In this way we create six indepen-dent expert-trained systems and compute the aver-age across their performance, calculated as Pearson correlation to the gold standard; this is reported in the “1-Expert” column of Table 4."
"Next we train systems using non-expert labels; for each possible subset of n annotators, for n ∈ {1, 2, . . . , 10} we train a system, and evaluate by calculating Pearson correlation with the same set of gold standard datasets used in the expert-trained sys-tem evaluation."
Averaging the results of these stud-ies yields the results in Table 4.
"As in Table 2 we calculate the minimum number of non-expert annotations per example k required on average to achieve similar performance to the ex-pert annotations; surprisingly we find that for five of the seven tasks, the average system trained with a single set of non-expert annotations outperforms the average system trained with the labels from a sin-gle expert."
"One possible hypothesis for the cause of this non-intuitive result is that individual labelers (including experts) tend to have a strong bias, and since multiple non-expert labelers may contribute to a single set of non-expert annotations, the annotator diversity within the single set of labels may have the effect of reducing annotator bias and thus increasing system performance."
We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
Our evaluation of non-expert la-beler data vs. expert annotations for five tasks found that for many tasks only a small number of non-expert annotations per item are necessary to equal the performance of an expert annotator.
In a detailed study of expert and non-expert agreement for an af-fect recognition task we find that we require an av-erage of 4 non-expert labels per item in order to em-ulate expert-level label quality.
"Finally, we demon-strate significant improvement by controlling for la-beler bias."
"Compared to the telephone, email based cus-tomer care is increasingly becoming the pre-ferred channel of communication for corpora-tions and customers."
Most email-based cus-tomer care management systems provide a method to include template texts in order to re-duce the handling time for a customer’s email.
The text in a template is suitably modified into a response by a customer care agent.
"In this paper, we present two techniques to im-prove the effectiveness of a template by pro-viding tools for the template authors."
"First, we present a tool to track and visualize the ed-its made by agents to a template which serves as a vital feedback to the template authors."
"Second, we present a novel method that au-tomatically extracts potential templates from responses authored by agents."
These meth-ods are investigated in the context of an email customer care analysis tool that handles over a million emails a year.
Email based customer care is increasingly becom-ing the preferred channel of communication for cor-porations and customers compared to the conven-tional telephone-based customer care.
"For cus-tomers, email channel offers several advantages – there are no tedious menus to navigate, there is no waiting time to reach an operator, the request can be formulated at the customer’s pace and additional material supporting the case can be attached to the email."
There is also a record of the service re-quest for the customer unlike the telephone-based customer care.
"However, there are also limitations of the email channel."
The most significant one is that the customer-agent interaction could be drawn out over successive emails spanning over several days as opposed to being resolved in one or two telephone calls.
"For corporations, the asynchronous nature of email-based customer care offers signifi-cant opportunities to reduce operations cost by ef-fective load balancing compared to telephone-based customer care."
It is quite common for an email cus-tomer care agent to work on several cases simulta-neously over a period of a few hours.
"Email chan-nel also offers higher bandwidth for corporations to send additional information in the form of web links, images and video or audio instructions."
The effectiveness of customer care in the email channel is measured using two competing metrics: Average Handling Time (AHT) and Customer Ex-perience Evaluation (CEE).
AHT measures the time taken from when a customer email is opened to the time when the response is sent out.
This time is typ-ically averaged over a period of a week or a month for reporting purposes.
CEE measures customer sat-isfaction through a survey of a random subset of cus-tomers who have interacted with the email customer care center.
These surveys typically involve qual-itative and quantitative questions and measure the quality of the interactions along a number of differ-ent dimensions.
As is the case in many surveys the population responding to such questionnaires is typ-ically small and very often quite biased.
We do not use the CEE metric for the work we report in this paper.
"As is evident from the definitions of AHT and CEE, it is in the interest of a corporation to minimize AHT while maximizing CEE."
"In order to reduce AHT, most email customer care systems[REF_CITE]provide a mechanism for an agent to respond to a customer’s email by selecting a predefined template text that can be quickly cus-tomized to serve as the response."
The template text is usually associated with a problem category it is in-tended to address and might even be suggested to the agent automatically using classification techniques applied to the customer’s email.
"Once the template is selected, the agent edits the template text to per-sonalize as well as add case specific details as part of composing a response."
Each of the text edits con-tributes to the handling time of the email.
"Hence, it is in the interest of the template designer to mini-mize the number of edits of the template in order to lower AHT."
"Although most email management systems pro-vide a mechanism to author the template text, there is typically no mechanism to monitor and track how these templates are modified by the agents when they compose a response."
This information is vital to the template authors when creating new versions of the templates that reduce the number of edits and consequently reduce AHT.
"In this paper, we present two methods for improv-ing the templates in a principled manner."
"After de-scribing the related work in Section 2, we present a brief description of the email tracking tool we have developed in Section 3."
"In Section 4, we present a tool called HotSpots that helps visualize the edits being made by the customer care agents to the tem-plates."
This tool provides a visual feedback to the template authors and suggests means of improving the template text based on the edits made by agents.
"In Section 5, we present a new approach to automat-ically identify emerging templates – texts that are repeatedly created by agents and are similar to each other but distinct from the current template text."
We use AHT as the metric to minimize for automatic identification of emerging templates.
We discuss some of the issues concerning this work in Section 6 and conclude in Section 7.
There are few threads of research that are relevant to the work presented in this paper.
"First, the topic of email response generation in the context of customer care has been investigated[REF_CITE]."
"In[REF_CITE], the authors model multi-sentence generation of response letters to customer com-plaints in French."
"The generation model is carefully crafted for the domain using domain-specific rules for conceptual planning, rhetorical relations and sur-face word order operators."
They show that their approach performs better than predefined templates and slightly worse than human generated responses.
"In[REF_CITE], the authors ex-plore three different approaches based on classifica-tion, case-based reasoning and question-answering to compose responses to queries in an email cus-tomer care application for the telecommunication in-dustry."
The case-based reasoning approach is the most similar to the template approach we follow.
"In[REF_CITE], the authors investi-gate an approach to assembling a response by first predicting the clusters of sentences to be included in the response text and then applying multi-document summarization techniques to collate the representa-tive sentences into a single response."
"In contrast, in this paper, due to constraints from the deployment environment, we rely on a template-based approach to response generation."
We focus on providing tools for investigating how the templates are modified and suggest techniques for evolving more effective tem-plates based on quantitative criteria.
Another thread of relevant research are methods for visualizing texts.
There are several methods that have been proposed to provide a visual map of a set of text documents with the focus of illustrating the relatedness of these texts[REF_CITE].
"Us-ing a metric for comparing texts (e.g. n-gram over-lap) , the texts are clustered and the resulting clus-ters are visualized as two or three dimensional color maps."
These approaches are useful to depict similar-ities in a static repository of documents or the return results of a search query.
These maps are primar-ily designed for exploration and navigation through the document space.
"While the underlying algorithm we use to illustrate the text edits is similar to the one used in text map visualizations, our focus in this pa-per is to provide a mechanism for template designers to quickly identify the variants of a template sen-tence created by the agents."
"A third thread is in the context of human-assisted machine translation, where a human trans-lator post-edits the output of a machine translation system[REF_CITE]."
"In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the translation model."
"While the ap- proach we follow is partly motivated by the post-editing paradigm, there are significant differences in the context we apply this approach."
"In the context of this paper, the template designer is presented a summary of the set of variants created by each agent for each sentence of the template."
The task of the template designer is to use this tool to select (or con-struct) a new variant for the template sentence with the aim of minimizing the need for editing that sen-tence in future uses of the template.
"Typically, a large email customer care management center receives over 100,000 emails a month."
These centers typically use a customer care management system that offer not only logging and tracking of emails but also tools for improving the efficiency of agents responding to emails.
"Usually, an incom-ing customer email is categorized into a set of few topics/issues."
The categorization might be done au-tomatically based on regular expressions involving keywords in the email or using weighted classifiers that are trained on data.
"In order for an agent to re-spond to an incoming email, these systems provide a text box which allows the agent to author a response from scratch."
"However, most email customer care systems offer the ability to store a prefabricated re-sponse (also called templates), instead of agents hav-ing to author a response from scratch."
These tem-plates are typically associated with a problem cate-gory or an issue that they are intended to address.
A template helps an agent compose a well-formed response quickly.
It contains hints for information that the agent should enter as well as indications of where that information should be entered in the tem-plate.
The template might also contain helpful infor-mation to the customer in addition to legal verbiage that the customer needs to be aware of.
"An agent receives a customer email and after comprehending the issues and consulting the cus-tomer records in the database, selects one of the pre-defined set of templates that best addresses the is-sues raised in the email."
"Less frequently, she might even select more than one template to compose the response."
She then proceeds to edit and personal-ize the chosen templates to better suit the customer’s email.
An example of a ‘generic’ template – not as- sociated with a specific problem category is shown in Figure 1.
The process of selecting an appropriate template that addresses the customer’s inquiries could be quite tedious when there are hundreds of templates.
Email management systems offer tools that suggest appropriate template to use based on the content of the customer’s email.
These tools are trained using classification techniques on previous email interac-tions.
"As mentioned earlier, there are two metrics that are typically used to measure the effectiveness and efficiency of email responses."
Customers are sur-veyed after their email interaction to assess their level of satisfaction for the service they received.
"This is usually called the Customer Experience Evaluation (CEE) and includes an evaluation of the customer’s total interaction experience with the cor-poration, not just the last email interaction."
A small subset of customers who had an interaction with the email center is randomly chosen (typically in the or-der of about 10% of customers) and are invited to take part in the follow-up survey.
"Typically, only a small percent (about 10%) of the customers who receive these invitations respond to the survey; ef-fectively about 1% of the total emails have customer survey scores."
A second metric that is also used to measure the efficiency of an operation is called the average han-dling time (AHT) which measures the average of times taken by agents to respond to emails.
"The handling time includes the time to comprehend the email, the time for database lookup and the time for response composition."
It is in the interest of the email customer care operation to minimize AHT and maximize CEE scores.
We have designed and developed an Email Customer Care Analysis Tool (ECAT) to help analyze the op-erations of the email care center.
It provides an end-to-end view from the activities involved in answer-ing emails to the results of subsequent customer care surveys.
"In addition, ECAT also provides insights into how the agents are editing templates as well as guides template authors in designing more effective templates."
"ECAT is a web-based tool and offers a birds-eye summary of the operations aggregated by region, the template used, and the customer satisfaction sur-vey results."
"Using this tool, analysts can drill down through a series of views until they are eventually presented with the results of a single survey or a sin-gle email interaction."
One of the most useful functions of the tool is that it shows the extent to which agents edit the tem-plates in the process of creating responses to cus-tomer emails.
The degree to which a template is edited is based on Levenshtein string edit distance metric[REF_CITE].
"This metric measures the number of edits (substitution, deletion and inser-tions) of words that are needed to transform a tem-plate into a response."
The number of edits is normal-ized by the number of words in the template.
These morphing scores can be viewed for a single email or averaged per agent or per template used.
"The scores range from 100 to 0, with 100 representing a tem-plate which hadn’t been edited at all."
"The tool also allows the morphing score to be viewed alongside the handling time for an email, in other words the amount of time that the agent spends gathering data and actually composing a response."
Handling time is an important metric since it is di-rectly related to cost of operating the email customer care center.
"The more editing an agent does, the more time they take to respond to a customer."
"So, the number of templates, their precise wording and the ease with which agents can distinguish them ob- viously have significant influences on overall han-dling time."
"Beyond the confines of the email centers them-selves, the CEE is the most important elements in gauging the effectiveness of the agent."
The survey asks customers to rate their overall satisfaction with the email reply to their question.
Five is the high-est score which equates with ‘Extremely Satisfied’ while a one equals ‘Extremely Dissatisfied.’
"Cus-tomers are also asked to rank the email in terms of it’s content, clarity, professionalism and the length of time it took to receive a reply."
"The customer is also allowed to enter some free text so that they can say how satisfied they were, or not, with how an in-quiry or problem was dealt with."
Customers can also say whether they called the company using a tele-phone channel before turning to the email channel.
"The survey files, all of which can be accessed in their entirety from within the ECAT tool, also con-tain information on what templates were used when replying to the customer."
They also tell the analyst who the replying agent was and whether this was the first or a subsequent email in communications between the customer and the company.
The ECAT tool juxtaposes this CEE score with the template morphing score to show correlations between customer satisfaction and the degree to which the template had been edited.
This data is graphed so that the analyst can immediately see if heavy editing of a template is leading to higher CEE.
Heavy editing with a low customer rating could mean that the template is not helping the agent to respond correctly to the customer.
We designed the HotSpots tool that provides in-sights to the template authors on how templates are being edited by the agents when creating responses.
It suggests methods for improving the next version of the template so as to reduce edits by agents and hence reduce the handling time for an email.
"In this section, we discuss the algorithm and the visualiza-tion of the information that aids template authors in improving the efficacy of the templates."
The HotSpots algorithm proceeds in two steps as shown in Algorithm 1.
The first step creates an alignment between the template string and the re-
"Algorithm 1 Compute HotSpots for a template T given a response set R 1: EdEv = φ 2: T = s 1 s 2 . . . s n 3: T s = {s i |1 ≤ i ≤ n} 4: R s = {r ji |R j ∈ R, R j = r 1j r 2j . . . r mj j , 1 ≤ i ≤ m j } 5: Index : {T s ∪ R s } → I 6: T in = Index(s 1 )Index(s 2 ) . . ."
Index(s n ) 7: for all R ∈ R do 8: R = r 1 r 2 . . . r n R 9: R in = Index(r 1 )
Index(r 2 ) . . .
"Index(r n R ) // compute distance with sentences as tokens and return the alignment and score 10: (alignment, score) = IndDist(T in , R in ) // for each of the sentences in T, update its map 11: for all s i ∈ T do 12: in = Index(s i ) 13: if (s i , ) ∈ alignment then 14: EdEv[in].map = EdEv[in].map ∪ {∗delete∗} 15: else // (s i , r j ) ∈ alignment 16: EdEv[in].map = EdEv[in].map ∪ {r j } 17: end if 18: end for 19: end for // Cluster the response sentences aligned for each template sentence 20: for all s i ∈ T do 21: in = Index(s i ) 22: Cl = KmedianCl(EdEv[in].map, ncl) 23: end for"
Algorithm 2 KmedianCl: Compute k centroids for a set of strings S using k-median clustering algorithm sponse string.
"For the purposes of this paper, we consider the alignments between the template text and the response text with sentences as tokens in-stead of a word-based alignment."
The rationale for this tokenization is that for template develop-ers the visualization of the edits is expected to be more meaningful when aggregated at the sentence level rather than at the word level.
"In the second step, using the sentence-level alignment we compute the edit events (insertion, deletion and substitution) of the template sentences in order to create the re-sponse."
All the edits events associated with a tem-plate sentence are then clustered into k clusters and the centroids of the k clusters are displayed as the potential changes to that sentence.
We next describe these two steps in detail as illustrated in Algorithm 1 and Algorithm 2.
"Given a set of responses R that agents create us-ing a template T, Algorithm 1 proceeds as follows."
Each of the sentences in the template and the set of responses are mapped into an integer index (Line 1).
The template T and each of the responses in R are split into sentences and mapped into index sequences (Line 6 and Line 9).
The alignment be-tween the two index strings is computed[REF_CITE].
"This is a dynamic programming algorithm sim-ilar to computing Levenshtein distance between two strings, except the cost function used to compute the match between tokens is as shown below."
"From the alignment that maps s i to r j , we collect the set of response sentences associated with each template sentence ([REF_CITE]-16)."
These sentences are then clustered using k-median clustering method (il-lustrated in Algorithm 2)[REF_CITE].
"In Algorithm 2, we illustrate the method of clus-tering we use to summarize the set of sentences we have collected for each template sentence after the alignment step."
"The algorithm is similar to the k-means algorithm[REF_CITE], however, given that we are clustering strings instead of real num-bers (as is typical in applications of k-means), we re-strict the centroid of a cluster to be one of the mem-bers of the set being clustered, hence the name k-median algorithm[REF_CITE]."
The distance function to measure the closeness of two strings is instantiated to be an n-gram overlap between the two strings. 1
The algorithm iterates over three steps until the data is partitioned into k clusters (Line 5).
The first step (Lines 6-10) is the initialization of a centroid for a new cluster.
"Initially when the data is not parti-tioned into any cluster, the median string of the data set is used as the initial centroid."
"For subsequent iterations, the farthest point from all the centroids computed thus far is used as the centroid for the new cluster."
"In the second step ([REF_CITE]-16), each mem-ber of the data set is assigned to the nearest clus-ter based on its distance to that cluster’s centroid."
"Finally, in the third step ([REF_CITE]-20), the cluster centroids are recomputed based on the new cluster memberships."
Steps two and three are repeated until there are no changes in the cluster memberships and cluster centroids.
This completes the introduction of a new cluster for the data.
"For the purposes of our task, we use up to a four-gram overlap to measure distance between two strings and use k = 5 for clustering the data."
The HotSpots page was created within the ECAT tool to surgically dissect the way in which templates were being morphed.
"For a given template, as shown in Figure 2, the analyst is presented with a copy of the texts from the current and previous versions of that template."
Each sentence in the two versions of the template are color coded to show how fre-quently the agents have changed that sentence.
"This involved running the HotSpots algorithm against ap-proximately [Footnote_1],000 emails per template version."
"1 We have also experimented with a symmetric version of Levenshtein distance, but we prefer the n-gram overlap score due to its linear run time complexity."
A sentence that is colored red is one that was changed in over 50% of the emails that were responded to using that template.
"An orange sentence is one that was edited in between 30% and 50%, green is be-tween 10% to 30% and blue is between 0% and 10%."
The more often a sentence is edited the ‘hotter’ the color.
The analyst can see the typical substitutions for a sentence by hovering the mouse over that sentence.
The typical sentences computed as the centroids of the clusters created using Algorithm 2 are them-selves color coded using the same identification sys- tem.
A typical sentence that occurred in over 50% of the emails is colored red.
A typical sentence that occurred in 30% to 50% of the emails was orange and so on.
"In seeing the two versions side by side, the an-alyst can visually inspect the agents’ edits on the current version of a template relative to the previ-ous version."
"If the previous version of the template is a ‘hotter’ document (with more red sentences), it means that the changes made to the template by the author had led to less editing by agents thus speeding up the process of creating a customer response."
"If the current template looks hotter, it suggests that the changes made to the template were increasing the agents’ edits and probably the email handling time."
The goal of the template author is to minimize the number of edits done to a template and thus in-directly lowering the handling time for an email.
"In the preceding section, we discussed a tool that aids the template authors to identify sentences where changes are most often made by agents to a tem-plate."
This information could be used by the tem- plate authors to create a new version of the template that achieve the goal.
"In this section, we investigate a technique that au-tomatically identifies a possible template with the potential of directly minimizing the average han-dling time for an email."
We use the set of responses created by the agents using a given template and se-lect one of the responses to be generalized and stored as a new template.
The response to be converted into a template is chosen so as to directly minimize the average handling time.
"In essence, we seek to partition the set of responses R generated from tem-plate T into two clusters R 1 and R 2 ."
These clus-ters have centroids T (current template) and
"T 0 (new template) such that constraint shown in 1 holds. (∀ r∈R 1 AHT (T, r) &lt; AHT (T 0 , r)) ∧ (∀ r∈R 2 AHT (T 0 , r) &lt; AHT (T, r)) (1)"
"Now, the quantity AHT(T, r) is logged as part of the email management system and corresponds to the time taken to respond to a customer’s email. [Footnote_2]"
2 Although typically this time includes the time to look up a
"However, we do not have access to AHT (T 0 , r) for any T 0 6= T ."
We propose a model to estimate this in the next section.
We model AHT as a linear combination of sev-eral factors which we believe would influence the handling time for an email.
"These factors in-clude the length in words of the customer’s input email (inplen), the length in words of the template (templatelen), the length in words of the response (resplen), the total number of edits between the template and the response (edit), the normalized edit score (nedit), the number of individual events of the edit distance – substitution (sub), insertion (ins), deletion (del) and identity (id), the number of block (contiguous) substitution (blksub), block insertion (blkins) and block deletion (blkdel)."
"Using these in-dependent variables, we fit a linear regression model using the AHT values for 6175 responses created from one particular template (say G)."
The result of the regression fit is shown in Equation 2 and the data and error statistics are shown in Table 2.
"It must be noted that the coefficients for the variables are not necessarily reflective of the importance of the vari-ables, since they compensate for the different ranges in variable values."
"We have also tried several differ-the customer’s account etc., we assume that time is quite similar for all responses created from the same template. ent regression fits with fewer variables, but find that this fit gives us the best correlation with the data."
"Based on the goodness statistics of the regression fit, it is clear the AHT model could be improved further."
"However, we acknowledge that AHT does not depend solely on the editing of a template to a response but involves several other components in-cluding the user interface, the complexity of cus-tomer’s email, the database retrieval to access the customer’s account and so forth."
"Nevertheless, we use this model to cluster a new set of 2005 responses originating from the same template (G), as shown in Equation 1."
"Using the k-median clustering as described earlier, we parti-tion the responses into two clusters."
We restrict the first cluster centroid to be the template and search for the best centroid for the second cluster.
The re-sults are shown in Table 1.
The centroid for clus-ter 1 with 1799 members is the template itself while the centroid for cluster 2 with 206 members is a re-sponse that could be suitably generalized to serve as a template.
"The overall AHT for the 2005 responses using the template was 989.2 seconds, while the av-erage AHT for the members of cluster 1 and 2 was 971.9 seconds and 1140 seconds, indicating that the template had to be edited considerably to create the members of cluster 2."
"For the purposes of this paper, it is assumed that AHT is the same as or correlates well with the time to compose a response for an email."
"However, in most cases the email care agent might have to per-form several verification, validation, and problem resolution phases by consulting the specifics of a customer account before formulating and compos-ing a response."
The time taken for each of these phases typically varies depending on the customer’s account and the problem category.
"Nevertheless, we assume that the times for these phases is mostly a constant for a given problem category, and hence the results presented in this paper need to be interpreted on a per problem category basis."
A second limitation of the approach presented in this paper is that the metric used to measure the sim-ilarity between strings (n-gram overlap) is only a crude approximation of an ideal semantic similarity metric.
There are however other similarity metrics (e.g. BLEU[REF_CITE]) which could be used equally well.
The purpose of this paper is to il-lustrate the possibility of analysis of responses using one particular instantiation of the similarity metric.
"In spite of the several directions that this work can be improved, the system and algorithms described in this paper have been deployed in an operational customer care center."
The qualitative feedback we have received are extremely positive and analysts have greatly improved the efficiency of the opera-tion using this tool.
"In this paper, we have presented two approaches that help template authors in designing effective tem-plates for email customer care agents."
"In the first ap-proach, we have presented details of a graphical tool that provides vital feedback to the template authors on how their templates are being modified by agents when creating responses."
The template authors can accommodate this information when designing the next version of the template.
We also presented a novel technique for identifying responses that can potentially serve as templates and reduce AHT.
"To-wards this end, we discussed a method to model AHT based on the characteristics of the customer’s email, the template text and the response text."
"We would like to thanks Mazin Gilbert, Junlan Feng, Narendra Gupta and Wenling Hsu for the discus-sions during the course of this work."
We also thank the members who generously offered to their sup-port to provide us with data used in this study with-out which this work would not have been possible.
We thank the anonymous reviewers for their useful suggestions in improving the quality of this paper.
"This paper describes a language-independent, scalable system for both challenges of cross-document co-reference: name variation and entity disambiguation."
We provide system re-sults from the[REF_CITE]evaluation in both English and Arabic.
Our English system’s ac-curacy is 8.4% relative better than an exact match baseline (and 14.2% relative better over entities mentioned in more than one docu-ment).
"Unlike previous evaluations,[REF_CITE]evaluated both name variation and entity disambiguation over naturally occurring named mentions."
An information extraction engine finds document entities in text.
We de-scribe how our architecture designed for the 10K document ACE task is scalable to an even larger corpus.
Our cross-document ap-proach uses the names of entities to find an initial set of document entities that could refer to the same real world entity and then uses an agglomerative clustering algorithm to disam-biguate the potentially co-referent document entities.
We analyze how different aspects of our system affect performance using ablation studies over the English evaluation set.
"In ad-dition to evaluating cross-document co-reference performance, we used the results of the cross-document system to improve the ac-curacy of within-document extraction, and measured the impact in the[REF_CITE]within-document evaluation."
Cross-document entity co-reference is the problem of identifying whether mentions from different documents refer to the same or distinct entities.
There are two principal challenges: the same entity can be referred to by more than one name string (e.g. Mahmoud Abbas and Abu Mazen) and the same name string can be shared by more than one entity (e.g. John Smith).
"Algorithms for solving the cross-document co-reference problem are neces-sary for systems that build knowledge bases from text, question answering systems, and watch list applications."
There are several challenges in evaluating and developing systems for the cross-document co-reference task. (1) The annotation process required for evaluation and for training is expensive; an an-notator must cluster a large number of entities across a large number of documents.
"The annotator must read the context around each instance of an entity to make reliable judgments. (2) On randomly selected text, a baseline of exact string match will do quite well, making it difficult to evaluate pro-gress. (3) For a machine, there can easily be a scal-ability challenge since the system must cluster a large number of entities."
"Because of the annotation challenges, many previous studies in cross-document co-reference have focused on only the entity disambiguation problem (where one can use string retrieval to col-lect many documents that contain same name); or have used artificially ambiguated data."
"Section 2 describes related work; section 3 in-troduces ACE, where the work was evaluated; sec-tion 4 describes the underlying information extraction engine; sections 5 and 6 address the challenges of coping with name variation and dis-ambiguating entities; sections 7, 8, and 9 present empirical results, improvement of entity extraction within documents using cross-document corefer-ence, and a difference in performance on person versus organization entities."
Person disambiguation given a person name string.
"Their work presented a vector space model for the problem of entity disambiguation, clustering 197 articles that contained the name ‘John Smith’."
Participants in the 2007 Sem-Eval Web People Search(WEPS) task clustered 100-document sets based on which person a name string of interest referenced.
WEPS document sets were collected by selecting the top 100 web search results to que-ries about a name string[REF_CITE].
Clustering different variants of the same name.
Lloyd et. al (2006) use a combination of ‘morpho-logical similarity’ and ‘contextual similarity’ to cluster name variants that refer to the same entity.
Clustering and disambiguation.
"The John[REF_CITE]Summer Workshop produced a cross-document annotated version of the[REF_CITE]cor-pus (18K document entities, 599 documents) con-sisting of 5 entity types (Day, et. al, 2007)."
There was little ambiguity or variation in the corpus.
Par-ticipants demonstrated that disambiguation im-provements could be achieved with a Metropolis-Hastings clustering algorithm.
The study assumed human markup of document-level entities.
The work reported in this paper ad-dresses both entity clustering and name variation for both persons and organizations in a corpus of 10K naturally occurring documents selected to be far richer than the[REF_CITE]data by NIST and LDC.
"We investigated a new approach in both English and Arabic, and evaluated on document-level entities detected by information extraction."
"NIST’s ACE evaluation measures system perform-ance on a predetermined set of entities, relations, and events."
1 NIST’s evaluation of cross-document co-reference.
The GEDR task was run over both English and Arabic documents.
Partici-pants processed over 10K documents for each lan-guage.
References were produced for about 400 documents per language[REF_CITE].
The evalua-tion set included documents from several genres over a 10 year time period.
Document counts are provided in Table 1.
This evaluation differed from previous community cross-document coreference evaluations in that it (a) covered both organizations and people; (b) required processing a relatively large data set; (c) evaluated entity disambiguation and name variation simultaneously; and (d) meas-ured cross-document co-reference over system-detected document-level entities and mentions.
The evaluation set was selected to include in-teresting cases for cross-document co-reference (e.g cases with spelling variation and entities with shared names).
This is necessary because annota-tion is difficult to produce and naturally sampled data has a high percentage of entities resolvable with string match.
The selection techniques were unknown to ACE participants.
"Our cross-document co-reference system relies on SERIF, a state-of-the-art information extraction (IE) system (Ramshaw, et. al, 2001) for document-level information extraction."
"The IE system uses statistically trained models to detect and classify mentions, link mentions into entities, and detect and classify relations and events."
"English and Ara-bic SERIF share the same general models, al-though there are differences in the specific features used by the models."
Arabic SERIF does not per-form event detection.
"While Arabic SERIF does make use of some morphological features, the cross-document co-reference system, which fo-cused specifically on entity names, does not use these features."
Figure 1 and Figure 2 illustrate the architecture and algorithms of the cross-document co-reference system respectively.
Our system separately ad-dresses two aspects of the cross-document co-reference problem: name variation (Section 5) and entity disambiguation (Section 6).
This leads to a scalable solution as described[REF_CITE].
"The features used by the cross-document co-reference system can be divided into four classes: World Knowledge (W), String Similarity (S), Pre-dictions about Document Context (C), and Meta-data (M)."
Name variation (V) features operate over unique corpus name strings.
Entity disambiguation features (D) operate over document-level entity instances.
"During disambiguation, the agglomera-tive clustering algorithm merges two clusters when conditions based on the features are met."
"For ex-ample, two clusters are merged when they share at least half the frequently occurring nouns that de-scribe an entity (e.g. president)."
"As shown in Table 2, features from the same class were often used in both variation and disambiguation."
All classes of features were used in both English and Arabic.
"Because very little training data was avail-able, both the name variation system and the dis-ambiguation system use manually tuned heuristics to combine the features."
"Tuning was done using the[REF_CITE]pilot data[REF_CITE], documents from the SemEval WEPS task[REF_CITE], and some internally annotated documents."
Internal annotation was similar in style to the WEPS annotation and did not include full ACE annotation.
Annotators simply clustered documents based on potentially confusing entities.
Internal annotation was done for ~100 names in both Eng-lish and Arabic.
The name variation component (Block 1 of Figure 1) collects all name strings that appear in the document set and provides a measure of similarity between each pair of name strings. [Footnote_2] Regions (A) and (B) of Figure 2 illustrate the input and output of the name variation component.
"2 For the majority of pairs, this similarity score will be 0."
"This component was initially developed for question answering applications, where when asked the question ‘Who is George Bush?’ relevant answers can refer to both George W and George HW (the question is ambiguous)."
However when asked ‘Who leads al Qaeda?’ the QA system must be able to identify spelling variants for the name al Qaeda.
"For the cross-document co-reference prob-lem, separating the name variation component from the disambiguation component improves the scalability of the system (described[REF_CITE])."
"The name variation component makes use of a variety of features including web-mined alias lists, aliases mined from the corpus (e.g ‘John aka J’), statistics about the relations and co-reference deci-sions predicted by SERIF, character-based edit distance, and token subset trees."
The token subset trees algorithm measures similarity using word overlap by building tree-like structures from the unique corpus names based on overlapping tokens.
Translation dictionaries (pulled from machine translation training and cross-language links in Wikipedia) account for names that have a canoni-cal form in one language but may appear in many forms in another language.
The features are combined with hand-tuned weights resulting in a unidirectional similarity score for each pair of names.
The similarity be-tween two name strings is also influenced by the similarity between the contexts in which the two names appear (for example the modifiers or titles that precede a name).
"This information allows the system to be more lenient with edit distance when the strings appear in a highly similar context, for example increasing the similarity score between ‘Iranian President Ahmadinejad’ and ‘Iranian President Nejad.’"
We use a complete link agglomerative cluster-ing algorithm for entity disambiguation.
"To make agglomerative clustering feasible over a 10K document corpus, rather than clustering all docu-ment-level entities together, we run agglomerative clustering over subsets of the corpus entities."
"For each name string, we select the set of names that the variation component chose as valid variants."
"In Figure 2 region C, we have selected Mahmoud Abbas and 3 variants."
We then run a three stage agglomerative clus-tering algorithm over the set of document entities that include any of the name string variants or the original name.
Figure 2 region D illustrates three document-level entities.
"The name variation links are not transitive, and therefore a name string can be associated with more than one clustering instance."
Furthermore document-level entities can include more than one name string.
"However once a document-level en-tity has been clustered, it remains linked to entities that were a part of that initial clustering."
"Because of this, the order in which the algorithm selects name strings is important."
We sort the name strings so that those names about which we have the most information and believe are less likely to be am-biguous are clustered first.
Name strings that are more ambiguous or about which less information is available are clustered later.
"The clustering procedure starts by initializing singleton clusters for each document entity, except those document entities that have already partici-pated in an agglomerative clustering process."
"For those entities that have already been clustered, the clustering algorithm retrieves the existing clusters."
The merging decisions are based on the similar-ity between two clusters as calculated through fea-ture matches.
Many features are designed to capture the context of the document in which enti-ties appear.
"These features include the document topics (as predicted by the unsupervised topic de-tection system[REF_CITE], the publication date and source of a document, and the other names that appear in the document (as predicted by SERIF)."
Other features are designed to provide information about the specific context in which an entity appears for example: the noun phrases that refer to an entity and the relationships and events in which an entity participates (as predicted by SERIF).
"Finally some features, such as the uniqueness of a name in Wikipedia are designed to provide the disambiguation component with world knowledge about the entity."
"Since each cluster represents a global entity, as clusters grow through merges, the features associated with the clusters expand."
"For example, the set of associated docu-ment topics the global entity participates in grows."
"While we have experimented with statistically learning the threshold for merging, because of the small amount of available training data, this threshold was set manually for the evaluation."
Clustering over these subsets of similar strings has the additional benefit of limiting the number of global decisions that are affected by a mistake in the within-document entity linking.
"For example, if in one document, the system linked Hillary Clinton to Bill Clinton; assuming that the two names are not chosen as similar variants, we are likely to end up with a cluster made largely of mentions of Hillary with one spurious mention of Bill and a separate cluster that contains all other mentions of Bill."
"In this situation, an agglomerative clustering algorithm that linked over the full set of document-level entities is more likely to be led astray and create a single ‘Bill and Hillary’ entity."
"Table 3 and Table [Footnote_4] include preliminary ACE results 3 for the highest, lowest, and average system in the local and cross-document tasks respectively."
"4 There was a swap in rank between metrics, so the low num-bers reflect two different systems."
"While a single participant could submit more than one entry, these numbers reflect only the primary submissions."
The ACE scorer maps system pro-duced entities to reference entities and produces several metrics.
"For the within-document task, metrics include ACE Value, B[Footnote_3], and a variant of B3 weighted to reflect ACE value weightings."
3 Results in this paper use v2.1 of the references and v17 of the ACE scorer. Final results will be posted[URL_CITE]
"For the cross-document task, the B3 metric is replaced with F[REF_CITE]."
ACE value has traditionally been the official metric of the ACE evaluation.
"It puts a higher cost on certain classes of entities (e.g. people are more important than facilities), certain classes of mentions (e.g. names are more important than pronouns), and penalizes systems for mistakes in type and subtype detection as well as linking mistakes."
Assigning a mention to the wrong entity is very costly in terms of value score.
"If the men-tion is a name, a system is penalized 1.0 for the missed mention and an additional 0.75 for a men-tion false alarm."
We will report ACE Value and value weighted B3/F. Scores on the local task are not directly comparable to scores on the global task.
The local entity detection and recognition task (LEDR) includes entity detection for five (rather than two) classes of entities and includes pronoun and nominal (e.g. ‘the group’) mentions in addition to names.
Our cross-document co-reference system used BBN-A-edropt as input.
BBN-B-st-mg is the result of using cross-document co-reference to improve local results (Section 9).
"For cross-document co-reference, our primary submission, BBN-B-med, was slightly outperformed by an alternate system BBN-B-low."
The two submissions differed only in a parameter setting for the topic detection system (BBN-B-low requires more documents to predict a ‘topic’).
BBN-A-st-mg-fix and BBN-B-med-fix are the result of post-processing the BBN output to account for a discrepancy between the training and evaluation material. [Footnote_5]
5 There were discrepancies between the ACE evaluation and training material with respect to the portions of text that should be processed. Therefore our initial system included a number of spurious entities. NIST has accepted revised output that removes these entities. Experiments in this paper reflect the corrected system.
"In addition to releasing results, NIST also re-leased the references."
Table 5 includes the ACE score for our submitted English system and the score when the system was run over only the 415 documents with references.
The system performs slightly better when operating over the full docu-ment set.
This suggests that the system is using information from the corpus even when it is not directly scored.
We have run a series of ablation experiments over the 415 files in the English test set to evaluate the effectiveness of different feature classes.
These experiments were run using only the annotated files (and not the full 10K document set).
We ran two simple baselines.
"The first baseline (‘No Link’) does not perform any cross-document co-reference, all document entities are independent global entities."
The second baseline (‘Exact Match’) links document-level entities using exact string match.
We ran 6 variations of our system: o Configuration 1 is the most limited system.
"It uses topics and IE system output for disambigua-tion, and aliases mined from the documents for the name variation component. o Configuration 2 includes Configuration 1 fea-tures with the addition of string similarity (edit distance, token subset trees) algorithms for the name variation stage. o Configuration 3 includes Configuration 2 fea-tures and adds context-based features (e.g. titles and premodifiers) for name variation. o Configuration 4 adds information from docu-ment metadata to the disambiguation component. o Configuration 5 adds web-mined information (alias lists, Wikipedia, etc.) to both the variation and disambiguation components."
This is the con-figuration that was used for our NIST submission. o
Configuration 5a is identical to Configuration 5 except that the string-based edit distance was removed from the name variation component.
"As noted previously, the ACE collection was selected to include challenging entities."
The selec-tion criteria of the corpus (which are not known by ACE participants) can affect the importance of fea-tures.
"For example, a corpus that included very few transliterated names would make less use of fea-tures based on edit distance."
Figure 3 and Figure 4 show performance (with value weighted F) on the eight conditions over sys-tem predicted within-document extraction and ref-erence within-document extraction respectively.
Figure 3 also includes configuration 5 run over all 10K documents.
We provide two sets of results.
The first evaluates system performance over all entities.
The relatively high score of the ‘No Link’ baseline indicates that a high percentage of the document-level entities in the corpus are only men-tioned in one document.
The second set of num-bers measures system performance on those entities appearing in more than one reference document.
While this metric does not give a com-plete picture of the cross-document co-reference task (sometimes a singleton entity must be disam-biguated from a large entity that shares the same name); it does provide useful insights given the frequency of singleton entities.
Overall system performance improved as fea-tures were added.
"Configuration 1, which disam-biguated entities with a small set of features, performed worse than a more aggressive exact string match strategy."
The nature of our agglom-erative clustering algorithm leads to entity merges only when there is sufficient evidence for the merge.
"The relatively high performance of the ex-act match strategy suggests that in the ACE corpus, most entities that shared a name string referred to the same entity, and therefore aggressive merging leads to better performance."
"As additional features are added, our system becomes more confident and merges more document-level entities."
With the addition of string similarity measures (Configuration 2) our system outperforms the exact match baseline.
The submitted results on system entities (Configuration 5) provide a 8.4% relative reduction in error over the exact match baseline.
"If scored only on entities that occur in more than one document, Configuration 5 gives a 14.2% relative redution in error over the exact match baseline."
The context based features (Configuration 3) al-low for more aggressive edit-distance-based name variation when two name strings frequently occur in the same context.
"In Configuration 3, ‘Sheik Hassan Nasrallah’ was a valid variant of ‘Hassan Nasrallah’ because both name strings were com-monly preceded by ‘Hezbollah leader’."
"Similarly, ‘Dick Cheney’ became a valid variant of ‘Richard Bruce Cheney’ because both names were preceded by ‘vice president’."
In Configuration 2 the entities included in both sets of name strings had remained unmerged because the strings were not considered valid variants.
"With the addition of contextual in-formation (Configuration 3), the clustering algo-rithm created a single global entity."
"For the ‘Dick Cheney’ cluster, this was correct. ‘Sheik Hassan Nassrallah’ was a more complex instance, in some cases linking was correct, in others it was not."
The impact of the metadata features (Configu-ration 4) was both positive and negative.
An article about the ‘Arab League Secretary General Amru Moussa’ was published on the same day in the same source as an article about ‘Intifada Fatah movement leader Abu Moussa’.
"With the addition of metadata features, these two distinct global enti-ties were merged."
"However, the addition of meta-data features correctly led to the merging of three instances of the name ‘Peter’ in ABC news text (all referring ABC’s Peter Jennings)."
Web-mined information (Configuration 5) pro-vides several variation and disambiguation fea-tures.
"As we observed, the exact match baseline has fairly high accuracy but is obviously also too aggressive of a strategy."
"However, for certain very famous global entities, any reference to the name (especially in corpora made of primarily news text) is likely to be a reference to a single global entity."
"Because these people/organizations are famous, and commonly mentioned, many of the topic and extraction based features will provide insufficient evidence for merging."
The same famous person will be mentioned in many different contexts.
We use Wikipedia as a resource for such entities.
"If a name is unambiguous in Wikipedia, then we merge all instances of this name string."
"In the evaluation corpus, this led to the merging of many different instances of ‘Osama Bin Laden’ into a single en-tity."
Web-mined information is also a resource for aliases and acronyms.
"These alias lists, allowed us to merge ‘Abu Muktar’ with ‘Khadafi Montanio’ and ‘National Liberation Army’ with ‘ELN’."
"Interestingly, removing the string edit distance algorithm (System 5a), is a slight improvement over System 5."
"Initial error analysis has shown that while the string edit distance algorithm did im-prove accuracy on some entities (e.g linking ‘Sam Alito’ with ‘Sam Elito’ and linking ‘Andres Pas-trana’ with ‘Andreas Pastrana’); in other cases, the algorithm allowed the system to overlink two entities, for example linking ‘Megawati Soekar-noputri’ and her sister ‘Rachmawati Sukarnoputri’."
"In addition to evaluating the cross-document sys-tem performance on the GEDR task, we ran a pre-liminary set of experiments using the cross-document co-reference system to improve within-document extraction."
Global output modified within-document extraction in two ways.
"First, the cross-document co-reference system was used to modify the within-document system’s subtype classification."
"In addition to evaluating entity links and type classification, the ACE task measures subtype classification."
"For example, for organization entities, systems distinguish between Media and Entertainment organizations."
The IE system uses all mentions in a given entity to assign a subtype.
"The cross-document co-reference sys-tem has merged several document-level entities, and therefore has even more information with which to assign subtypes."
The cross-document sys-tem also has access to a set of manual labels that have been assigned to Wikipedia categories.
"Secondly, we used the cross-document co-reference system’s linking decisions to merge within-document entities."
"If the cross-document co-reference system merged two entities in the same document, then those entities were merged in the within-document output."
"Table 6 includes results for our within-document IE system, the IE system with improved subtypes, and the IE system with improved sub-types and merged entities."
"While these preliminary experiments yield rela-tively small improvements in accuracy, an analysis of the system’s output suggests that the merging approach is quite promising."
The output that has been corrected with global merges includes the linking entities with ‘World Knowledge’ acronyms (e.g. linking ‘FARC’ with ‘Armed Revolutionary Forces of Colombia’); linking entities despite document-level extraction mistakes (e.g. ‘Lady Thatcher’ with ‘Margaret Thatcher’); and linking entities despite spelling mistakes in a document (e.g linking ‘Avenajado’ with ‘Robert Aventa-jado’).
"However, as we have already seen, the cross-document co-reference system does make mistakes and these mistakes can propagate to the within-document output."
"In particular, we have noticed that the cross-document system has a tendency to link person names with the same last name when both names appear in a single document."
"As we think about the set of features used for entity disambiguation, we can see why this would be true."
These names may have enough similarity to be considered equivalent names.
"Because they appear in the same document, they will have the same publication date, document source, and document topics."
Adjusting the cross-document system to either use a slightly different approach to cluster document-level entities from the same document or at the very least to be more conservative in applying merges that are the result primarily of document metadata and context to the within-document output could improve accuracy.
"Unlike previous evaluations of cross-document co-reference performance, the[REF_CITE]evaluation included both person and organization entities."
We have noticed that the performance of the cross-document co-reference system on organizations lags behind the performance of the system on peo-ple.
"In contrast, for LEDR, the extraction system’s performance is quite similar between the two entity classes."
"Furthermore, the difference between global organization and person accuracy in the GEDR is smaller when the GEDR task performed with perfect document-level extraction."
Scores are shown in Table 7.
These differences suggest that part of the reason for the low performance on or-ganizations in GEDR is within-document accuracy.
"The LEDR task evaluates names, nominals, and pronouns."
"GEDR, however only evaluates over name strings."
"To see if this was a part of the differ-ence in accuracy, we removed all pronoun and nominal mentions from both the IE system’s local output and the reference set."
"As shown in Table 8, the gap in performance between organizations and people is much larger in this setting."
"Because the GEDR task focuses exclusively on names and excludes nominals and pronouns, mis-takes in mention type labeling (e.g. labeling a name as a nominal) become misses and false alarms rather than type substitutions."
"As the task is currently defined, type substitutions are much less costly than a missing or false alarm entity."
"Intuitively, correctly labeling the name of a per-son as a name and not a nominal is simple."
The distinction for organizations may be fuzzier.
"For example the string ‘the US Department of Justice’ could conceivably contain one name, two names, or a name and a nominal."
"The ACE guidelines[REF_CITE]suggest that this distinction can be difficult to make, and in fact have a lengthy set of rules for classifying such cases."
"However, these rules can seem unintuitive, and may be difficult for machines to learn."
For example ‘Justice Depart-ment’ is not a name but ‘Department of Justice’ is.
"In some sense, this is an artificial distinction en-forced by the task definition, but the accuracy numbers suggest that the distinction has a negative effect on system evaluation."
One of the challenges for systems participating in the ACE task was the need to process a rela-tively large document set (10K documents).
"In question answering applications, our name varia-tion algorithms have been applied to even larger corpora (up to 1M documents)."
There are two fac-tors that make our solution scalable.
"First, much of the name variation work is highly parallelizable."
Most of the time spent in this algorithm is spent in the name string edit distance calculation.
This is also the only algorithm in the name variation component that scales quadratically with the number of name strings.
"However, each calculation is independent, and could be done si-multaneously (with enough machines)."
"Second, the disambiguation algorithm clusters subsets of document-level entities, rather than run-ning the clustering over all entities in the document set."
"In the English ACE corpus, the IE system found more than 135K document-level entities that were candidates for global entity resolution."
"As described in Section 6, a document entity is only clustered one time."
Even the largest clustering instance contained only 1.4% of the document-level enti-ties.
The vast majority of agglomerative clustering instances disambiguated a small number of docu-ment-level entities and ran quickly. 99.7% of the agglomerative clustering runs took less than 1 sec-ond. 99.9% took 90 seconds or less.
"A small number of clustering instances in-cluded a large number of document entities, and took significant time."
"The largest clustering in-stance, initialized with the name string ‘Xinhua,’ contained 1848 document-level entities (1.4% of the document-level entities in the corpus)."
This instance took 2.6 hours (27% of the total time spent running agglomerative clustering).
Another frequent entity ‘George Bush’ took 1.2 hours.
"As described in Section 6, the clustering proce-dure can combine unresolved document-level enti-ties into existing global entities."
"For large cluster sets (e.g entities referred to by the string ‘Xinhua’), speed would be improved by running many smaller clustering instances on subsets of the document-level entities and then merging the results."
"We have presented a cross-document co-reference clustering algorithm for linking entities across a corpus of documents that • addresses both the challenges of name varia-tion and entity disambiguation. • is language-independent, • is scalable"
As measured[REF_CITE]for English our sys-tem produced an .8.4% relative reduction in error over a baseline that used exact match of name strings.
"When measured on only entities that ap-peared in more than one document, the system gave a 14.2% relative reduction in error."
"For the Arabic task, our system produced a 7% reduction in error over exact match (12.4% when scored over entities that appear in more than one document)."
We have shown how a variety of features are im-portant for addressing different aspects of the cross-document co-reference problem.
Our current features are merged with hand-tuned weights.
"As additional development data becomes available, we believe it would be feasible to statistically learn the weights."
"With statistically learned weights, a larger feature set could improve accuracy even further."
Global information from the cross-document co-reference system improved within-document information extraction.
This suggests both that a document-level IE system operating over a large corpus text can improve its accuracy with informa-tion that it learns from the corpus; and also that integrating an IE system more closely with a source of world knowledge (e.g. a knowledge base) could improve extraction accuracy.
The Named Entity Recognition (NER) task has been garnering significant attention in NLP as it helps improve the performance of many natural language processing applica-tions.
"In this paper, we investigate the im-pact of using different sets of features in two discriminative machine learning frameworks, namely, Support Vector Machines and Condi-tional Random Fields using Arabic data."
"We explore lexical, contextual and morphological features on eight standardized data-sets of dif-ferent genres."
"We measure the impact of the different features in isolation, rank them ac-cording to their impact for each named entity class and incrementally combine them in or-der to infer the optimal machine learning ap-proach and feature set."
Our system yields a performance of F β=[URL_CITE] -measure=83.5[REF_CITE]Broadcast News data.
Named Entity Recognition (NER) is the process by which named entities are identified and classified in an open-domain text.
NER is one of the most im-portant sub-tasks in Information Extraction.
"Thanks to standard evaluation test beds such as the Auto-matic Content Extraction (ACE) 1 , the task of NER has garnered significant attention within the natu-ral language processing (NLP) community."
ACE has facilitated evaluation for different languages cre-ating standardized test sets and evaluation metrics.
NER systems are typically enabling sub-tasks within large NLP systems.
The quality of the NER sys-tem has a direct impact on the quality of the overall NLP system.
"Evidence abound in the literature in areas such as Question Answering, Machine Trans-lation, and Information Retrieval[REF_CITE]."
The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classify-ing them according to a set of designed features into one of a predefined set of classes[REF_CITE].
The number of classes differ depending on the data set.
"To our knowledge, to date, the ap-proach is always to model the problem with a sin-gle set of features for all the classes simultaneously."
"This research, diverges from this view."
We recog-nize that different classes are sensitive to differing features.
"Hence, in this study, we aspire to discover the optimum feature set per NE class."
We approach the NER task from a multi-classification perspec-tive.
"We create a classifier for each NE class inde-pendently based on an optimal feature set, then com-bine the different classifiers for a global NER sys-tem."
"For creating the different classifiers per class, we adopt two discriminative approaches: Support Vector Machines (SVM)[REF_CITE], and Condi-tional Random Fields (CRF)[REF_CITE]."
"We comprehensively investigate many sets of fea-tures for each class of NEs: contextual, lexical, mor-phological and shallow syntactic features."
We ex-plore the feature sets in isolation first.
"Then, we employ the Fuzzy Borda Voting Scheme (FBVS)[REF_CITE]in or-der to rank the features according to their perfor- mance per class."
The incremental approach to fea-ture selection leads to an interpretable system where we have a better understanding of the resulting er-rors.
The paper is structured as follows: Section 2 gives a general overview of the state-of-the-art NER approaches with a particular emphasis on Ara-bic NER; Section 3 describes relevant character-istics of the Arabic language illustrating the chal-lenges posed to NER; in Section 4.1 we describe the Support Vector Machines and Conditional Ran-dom Fields Modeling approaches.
We discuss de-tails about our feature-set in 4.[Footnote_2] and describe the Fuzzy Borda Voting Scheme in Section 4.[Footnote_3].
2 We use the Buckwalter transliteration scheme to show ro-manized Arabic[REF_CITE].
3 It is worth noting that each vowelized form could still be ambiguous as in the English homograph/homophone ‘bank’ case.
"Sec-tion 5 describes the experiments and shows the re-sults obtained; Withing Section 5, Section 5.1 gives details about the data-sets which we use; finally, we discuss the results and some of our insights in Sec-tion 6 and draw some conclusions in 7."
"To date, the most successful language independent approaches to English NER are systems that employ Maximum Entropy (ME) techniques in a supervised setting[REF_CITE].[REF_CITE]show that using a Sup-port Vector Machine (SVM) approach outperforms (F β=1 =87.75) using CRF (F β=1 =86.48) on the NER task in Vietnamese."
"For Arabic NER,[REF_CITE]show that using a basic ME approach yields F β=1 =55.23."
"Then they followed up with fur-ther work[REF_CITE], where they model the problem as a two step classification ap-proach applying ME, separating the NE boundary detection from the NE classification."
That mod-ification showed an improvement in performance yielding an F β=1 =65.91.
"None of these studies in-cluded Arabic specific features, all the features used were language independent."
"In a later study,[REF_CITE]report using lexical and morphological features in a single step model us-ing CRF which resulted in significant improvement over state of the art to date for Arabic NER, yield-ing F β=1 =79.21."
"However, the data that was used in these evaluation sets were not standard sets."
"Most recently,[REF_CITE]have explored using a structured perceptron based model that employs Arabic morphological features."
Their system ben- efits from the basic POS tag (15 tags) information and the corresponding capitalization information on the gloss corresponding to the Arabic word.
Exploit-ing this information yields a significant improve-ment in recall of 7% and an overall F β=1 =69.6 on the[REF_CITE]data set.
The authors note the lack of improvement in the system’s performance when us-ing other Arabic morphological information.
"The Arabic language is a language of significant in-terest in the NLP community mainly due to its po-litical and economic significance, but also due to its interesting characteristics."
Arabic is a Semitic lan-guage.
"It is known for its templatic morphology where words are made up of roots, patterns, and af-fixes."
Clitics agglutinate to words.
"For instance, the surface word ÑîEA. ð wbHsnAthm 2 ‘and by their virtues[fem.]’, can be split into the conjunction w ‘and’, preposition b ‘by’, the stem HsnAt ‘virtues [fem.]’, and possessive pronoun hm ‘their’."
"With respect to the NER task, Arabic poses sev-eral major challenges: Absence of capital letters in the orthography: English like many other Latin script based languages has a specific marker in the orthography, namely capitalization of the initial letter, indicating that a word or sequence of words is a named entity."
Arabic has no such special signal rendering the detection of NEs more challenging.
Absence of short vowels: The absence of short vowels renders the lexical items a lot more ambigu-ous than in other languages exacerbating the homog-raphy problem.
The average polysemy for surface unvowelized words[REF_CITE]possible vow-elized forms and when the inflections are removed the average is 4 possible vowelized forms. 3
"For in-stance, words such as X@QK. brAd can be read both as ‘refrigerator’ or ‘Brad’,respectively, where the for-mer is a common noun and the latter is an NE."
"The Arabic language is highly inflectional: As we mentioned earlier, Arabic language uses an ag-glutinative strategy to form surface tokens."
"As seen in the example above, a surface Arabic word may be translated as a phrase in English."
"Consequently, the Arabic data in its raw surface form (from a statistical viewpoint) is much more sparse which decreases the efficiency of training significantly."
We approach the problem of NER from a per NE class based perspective.
The intuition is that features that are discriminative for one NE class might not be for another class.
"In the process, we decide on an op-timal set of features for each NE class."
Finally we combine the different classifiers to create a global NER system.
"Hence, we identify a set of features for NER and proceed to investigate them individually."
Then we use an automatic ranking system to pick the optimal set of features per NE class.
"To that end, we use the Fuzzy Borda Voting Scheme (FBVS)."
We employ two discriminative classification techniques: Support Vector Machines (SVM) and Conditional Random Fields (CRF).
"Even though some previous studies seem to point to the superiority of SVM over CRF for NER[REF_CITE], it is hard to draw a definitive conclusion since their assessment was based on comparing the average F-measure. [Footnote_4] More-over, the best system to date on Arabic NER reports results using CRF[REF_CITE]."
4 The authors did not report any per class comparison be-tween SVM and CRF.
We adopt an IOB2 annotation scheme for classification.
"For each NE class, we have two types of class labels: B-Class, marking the beginning of a Class chunk, and I-Class marking the inside of a class chunk."
"Fi-nally, we mark words not participating in an NE as O, meaning they are outside some NE class label."
SVM approach is based on Neural Networks[REF_CITE].
"The goal is to find, in the training phase, the best decision function which allows us to obtain the class c for each set of features f. SVM are robust to noise and have powerful generalization ability, especially in the presence of a large number of features."
"Moreover, SVM have been used suc- cessfully in many NLP areas of research in general[REF_CITE], and for the NER task in partic-ular[REF_CITE]."
"We use a sequence model Yamcha toolkit, [URL_CITE] which is defined over SVM."
CRF are a generalization of Hidden Markov Mod-els oriented toward segmenting and labeling se-quence data[REF_CITE].
CRF are undi-rected graphical models.
During the training phase the conditional likelihood of the classes are maxi-mized.
The training is discriminative.
They have been used successfully for Arabic NER (see sec-tion 2).
We have used CRF++ [URL_CITE] for our experiments.
One of the most challenging aspects in machine learning approaches to NLP problems is deciding on the optimal feature sets.
"In this work, we investigate a large space of features which are characterized as follows:"
Contextual (CXT): defined as a window of +/− n tokens from the NE of interest
Lexical (LEX i ): defined as the lexical ortho-graphic nature of the tokens in the text.
It is a representation of the character n-grams in a token.
We define the lexical features fo-cusing on the first three and last three char-acter n-grams in a token.
"Accordingly, for a token C 1 C 2 C 3 ...C n−1 Cn, then the lexical fea-tures for this token are LEX 1 =C 1 , LEX 2 =C 1 C 2 , LEX 3 =C 1 C 2 C 3 , LEX4=C n , LEX 5 = C n−1 C n , LEX 6 = C n−2 C n−1 C n ."
Gazetteers (GAZ): These include hand-crafted dictionaries/gazetteers listing predefined NEs.
"We use three gazetteers for person names, locations and organization names. [URL_CITE] We semi-automatically enriched the location gazetteer using the Arabic Wikipedia [URL_CITE] as well as other web sources."
"This en-richment consisted of: (i) taking the page labeled “Countries of the world” ( ÕËAªË@ ÈðX , dwl AlEAlm) as a starting point to crawl into Wikipedia and re-trieve location names; (ii) we automatically filter the data removing stop words; (iii) finally, the resulting list goes through a manual validation step to ensure quality."
"On the training and test data, we tag only the entities which exist entirely in the gazetteer, e.g. if the entity ‘United States of America’ exists in our gazetteer, we would not tag ‘United States’ on the data as a location."
Exception is made for person names.
We augment our dictionary by converting the multiword names to their singleton counterparts in addition to keeping the multiword names in the list.
We tag them on the evaluation data separately.
"Accordingly, the name ‘Bill Clinton’ and ‘Michael Johnson’ as two entries in our dictionary, are further broken down to ‘Bill’, ‘Clinton’, ‘Michael’, ‘John-son’."
The intuition is that the system will be able to identify names such as ‘Bill Johnson’ and ‘Clin-ton’ as person names.
"This is always true for person names, however this assumption does not hold for location or organization names."
"Part-Of-Speech (POS) tags and Base Phrase Chunks (BPC): To derive part of speech tags (POS) and base phrase chunks (BPC) for Arabic, we employ the AMIRA-1.0 system [URL_CITE] described[REF_CITE]."
"The POS tagger has a reported accu-racy of 96.2% (25 tags) and the BPC system per-forms at a reported F β=1 =96.33%, assuming gold tokenization and POS tagging."
Nationality (NAT): The input is checked against a manually created list of nationalities.
Morphological features (MORPH):
This feature set is based on exploiting the characteristic rich mor-phological features of the Arabic language.
"We rely on the MADA system for morphological dis-ambiguati[REF_CITE], to ex-tract relevant morphological information."
MADA disambiguates words along 14 different morphologi-cal dimensions.
"It typically operates on untokenized texts (surface words as they naturally occur), hence, several of the features indicate whether there are clitics of different types."
"We use MADA for the preprocessing step of clitic tokenization (which ad-dresses one of the challenges we note in Section 3, namely the impact different morphological surface forms have on sparseness)."
"Recognizing the varying importance of the different morphological features and heeding the reported MADA performance per feature, we carefully engineered the choice of the relevant morphological features and their associated value representations."
We selected 5 morphological features to include in this study. 1.
Aspect (M ASP ) :
"In Arabic, a verb maybe im-perfective, perfective or imperative."
"However since none of the NEs is verbal, we decided to turn this feature into a binary feature, namely indicating if a token is marked for Aspect (APP, for applicable) or not (NA, for not applicable). 2."
Person (M PER ) :
"In Arabic, verbs, nouns, and pronouns typically indicate person information."
"The possible values are first, second or third person."
"Again, similar to aspect, the applicability of this fea-ture to the NEs is more relevant than the actual value of first versus second, etc."
"Hence, we converted the values to APP and NA, where APP applies if the per-son feature is rendered as first, second or third. 3."
Definiteness (M DEF ) : MADA indicates whether a token is definite or not.
All the NEs by definition are definite.
"The possible values are DEF, INDEF or NA. 4."
Gender (M GEN ) : All nominals in Arabic bear gender information.
"According to MADA, the pos-sible values for this feature are masculine (MASC), feminine (FEM), and neuter (or not applicable NA), which is the case where gender is not applicable for instance in some of the closed class tokens such as prepositions, or in the case of verbs."
"We use the three possible values MASC, FEM and NA, for this feature."
"The intuition is that since we are using a sequence model, we are likely to see agreement in gender information in participants in the same NE. 5."
Number (M NUM ) :
"For almost all the tokens categories (verbs, nouns, adjectives, etc.)"
MADA provides the grammatical number.
"In Arabic, the possible values are singular (SG), dual (DU) and plural (PL)."
The correlation of the SG value with most of the NEs classes is very high.
"Heeding the underlying agreement of words in Arabic when they are part of the same NE, the values for this feature are SG, DU, PL and NA (for cases where number is not applicable such as closed class function words)."
Corresponding English Capitalization (CAP): MADA provides the English translation for the words it morphologically disambiguates as it is based on an underlying bilingual lexicon.
"The in-tuition is that if the translation begins with a capital letter, then it is most probably a NE."
This feature is an attempt to overcome the lack of capitalization for NEs in Arabic (see Section 3).
This is similar to the GlossCAP feature used[REF_CITE].
Fuzzy Borda Voting Scheme (FBVS) is useful when several possible candidates (c n ) are ranked by differ-ent experts (e m ) and we need to infer a single rank-ing[REF_CITE].
It is based on the Borda count method which was introduced by Jean-Charles de[REF_CITE].
"In FBVS, each expert provides the ranking of the can-didates with a weight [Footnote_10] (w mn ) assigned to each of them."
10 weights are not required for classical Borda count.
"Thereafter, for each expert e i , we generate a square matrix such as e i = (r 1i,1 . . . r in,n ) where: w ij r ij,k = (1) w ji + w ik"
"Given each expert matrix, we calculate for each row r j0i = P k r ji,k ; r ji,k &gt; α where α is a certain threshold."
"Accordingly, for each candidate, we sum up the weights obtained from the different experts in order to obtain a final weight for each candidate (r 00j ="
P i r j0i ).
"Finally, we rank them according to r 00j ."
"In our experiments, the candidates we rank are the features."
"The FBVS ranking is calculated per ML technique and class of NEs across all the data sets according to the features’ performances F β=1 , i.e. the weights."
The F β=1 ranges from 0−1.
"We use α = 0.5, thereby taking into consideration only the features which have shown a significant difference in performance."
"We report the results of our experiments on the stan-dard sets[REF_CITE]data sets. [URL_CITE] The ACE data (see Table 1) is anno-tated for many tasks: Entity Detection and Track-ing (EDT), Relation Detection and Recognition (RDR), Event Detection and Recognition (EDR)."
All the data sets comprise Broadcast News (BN) and Newswire (NW) genres.
"We create a dev, test and train set for each of the collections."
Table 1 gives the relevant statis-tics.
It is worth noting that the standard training sets have 4 folds that are typically used for training.
"We used one of the folds as dev data for tuning pur-poses, rendering our training data less for our exper-iments."
"For data preprocessing, we remove all anno-tations which are not oriented to the EDR task."
"Also, we remove all the ‘nominal’ and ‘pronominal’ men-tions of the entities and keep only the ‘named’ ones."
"Hence, all the listed characteristics for this corpus pertain to the portions of the data that are relevant to NER only."
"Whereas[REF_CITE]and 2005, two NE classes are added to the[REF_CITE]tag-set: Vehicles (e.g. Rotterdam Ship) and Weapons (e.g. Kalashnikof)."
"In order to overcome the sparseness issues resulting , we clitic tokenize the text using the MADA system."
We use the ATB style clitic tokenization standard.
"Finally, we con-vert the data from the ACE format into the IOB2 an-notation scheme (Tjong[REF_CITE])."
Our objective is to find the optimum set of features per NE class and then combine the outcome in a global NER system for Arabic.
"We set the context window to be of size −1/+1 for all the experiments, as it empirically yields the best performance."
"We use the CoNLL evaluation metrics of precision, recall, and F β=1 measures."
The CoNLL metrics are geared to the chunk level yielding results as they pertain to the entire NE (Tjong[REF_CITE]).
Our experiments are presented as follows: 1. Training per individual NE class: We train for an individual class by turning off the other an-notations for the other classes in the training set.
We experimented with two settings: 1.
"Setting all the other NE classes to O, similar to non-NE words, thereby yielding a 3-way classification, namely, B-NE and I-NE for the class of interest, and O for the rest including the rest of the NEs and other words and punctuation; 2."
The second setting discrimi-nated between the other NE classes that are not of interest and the rest of the words.
The intuition in this case is that NE class words will naturally be-have differently than the rest of the words in the data.
"Thereby, this setting yields a 4-way classifi-cation: B-NE and I-NE for class of interest, NE for the other NE classes, and O for the other words and punctuation in the data."
"In order to contrast the 3-way vs the 4-way classification, we run experiments and evaluate using the[REF_CITE]data set with no features apart from ‘CXT’ and ‘current word’ using SVM."
Table 2 illustrates the yielded results.
For all the NE classes we note that the 4-way classification yields the best results.
"Moreover, we counted the number of ‘conflicts’ obtained for each NE classifi-cation."
A ‘conflict’ arises when the same token is classified as a different NE class by more than one classification system.
"Our findings are summarized as follows: (i). 3 classes: 16 conflicts (8 conflicts in BN and 8 in NW). 10 of these conflicts are between GPE and PER, and 6 of them are between GPE and ORG. (ii). 4 classes: 10 conflicts (3 conflicts in BN and 7 in NW). 9 of these conflicts are between GPE and ORG, and only one of them is between GPE and FAC."
"An example of a conflict observed using the 3-way classification that disappeared when we ap-ply the 4-way classification is in the following sen-tence: @QKQ Öß á¢J@ð é®J HQå n$rt SHyfp WA$nTn tAyms tqryrA, which is translated as ‘The Washington Times newspaper published a report’."
"When trained using a 3-way classifier, ‘Washington’ is assigned the tag GPE by the GPE classifier sys-tem and as an ORG by the ORG classifier system."
"However, when trained using the 4-way classifier, this conflict is resolved as an ORG in the ORG clas-sifier system and an NE in the GPE classifier sys-tem."
Thereby confirming our intuition that a 4-way classification is better suited for the individual NE classification systems.
"Accordingly, for the rest of the experiments in this paper reporting on individual NE classifiers systems, we use a 4-way classification approach. 2."
Measuring the impact of Individual features per class : An experiment is run for each fold of the data.
"We train on data annotated for one NE class, one Machine Learning (ML) method (i.e. SVM or CRF), and one feature."
"For each experiment we use the tuning set for evaluation, i.e. obtaining the F β=1 performance value. 3. FBVS Ranking : After obtaining the F-measures for all the individual features on all the data genres and using the two ML techniques, we rank the features (in a decreasing order) according to their impact (F-measure obtained) using FBVS (see 4.3)."
This results in a ranked list of features for each ML approach and data genre per class.
"Once the features are ranked, we incrementally experi-ment with the features in the order of the ranking, i.e. train with the first feature and measure the perfor-mance on the tuning data, then train with the second together with the first feature, i.e. the first two fea-tures and measure performance, then the first three features and so on. 4. Feature set/class generalization : Finally, we pick the first n features that yield the best converging performance (after which additional features do not impact performance or cause it to deteriorate)."
We use the top n features to tag the test data and compare the results against the system when it is trained on the whole feature set.
"After running experiments using each feature indi-vidually, each result is considered an expert (the ob-tained F-measure is the weight in this framework)."
Our goal is to find a general ranking of the fea-tures for each ML approach and each class.
Table 3 shows the obtained rankings of the features for each class using SVM.
It is worth noting that the obtained CRF rankings are very similar to those yielded by using SVM.
We note that there are no specific fea-tures that have proven to be useless for all classes and ML approaches.
We combine the features per NE class incrementally.
"Since the total number of features is 16, each ML classifier is trained and evaluated on the tuning data 16 times for each genre."
A best number of features per class per genre per ML technique is determined based on the highest yielded F β=1 .
"Finally, the last step is combining the outputs of the different clas- sifiers for all the classes."
"In case of conflicts, where the same token is tagged as two different NE classes, we use a simple heuristic based on the classifier pre-cision for that specific tag, favoring the tag with the highest precision."
Table 4 illustrates the obtained results.
For each data set and each genre it shows the F-measure ob-tained using the best feature set and
We show results for both the dev and test data using the optimal number of features Best Feat-Set/ML contrasted against the system when using all 16 fea-tures per class All Feats/ML.
The table also illus-trates three baseline results on the test data only.
"FreqBaseline: For this baseline, we assign a test token the most frequent tag observed for it in the training data, if a test token is not observed in the training data, it is assigned the most frequent tag which is the O tag."
"In this baseline setting, we train an NER system with the full 16 features for all the NE classes at once."
We use the two different ML approaches yielding two baselines: MLBaseline SV M and MLBaseline CRF .
It is important to note the difference between the All Feats/ML setting and the MLBaseline setting.
"In the former, All Feats/ML, all 16 features are used per class in a 4-way classifier system and then the classifications are combined and the conflicts are re-solved using our simple heuristic while in the lat-ter case of MLBaseline the classes are trained to-gether with all 16 features for all classes in one sys-tem."
"Since different feature-sets and different ML approaches are used and combined for each experi-ment, it is not possible to present the number of fea-tures used in each experiment in Table 4."
"However, Table 5 shows the number of features and the ML approach used for each genre and NE class."
"As illustrated in Table 5, SVM outperformed CRF on most of the classes."
"Interestingly, CRF tends to model the ORG and FAC entities better than SVM."
"Hence, it is not possible to give a final word on the superiority of SVM or CRF in the NER task, and it is necessary to conduct a per class study, as the one we present in this paper, in order to determine the right ML approach and features to use for each class."
"Therefore, our best global NER system combined the results obtained from both ML approaches."
"Table 4, shows that our Best Feat-set/ML set-ting outperforms the baselines and the All Feats {SVM/CRF} settings for all the data genres and sets forthe test data."
"Moreover, the Best Feat-set/ML setting outperforms both All Feats {SVM/CRF} settings for the dev data for all genres except[REF_CITE]NW, where the difference is very small."
The results yielded from the ML baselines are comparable across all the data genres and the two ML approaches.
"Comparing the global ML baseline systems against the All Feature Setting, we see that the All Feats setting consistently outperforms the MLBase-line settings except[REF_CITE]NW data set."
This suggests that training separate systems for the differ-ent NEs has some benefit over training in one global system.
Comparing the performance per genre across the different data sets.
We note better performance across the board for BN data over NW per year.
The worst results are yielded[REF_CITE]data for both BN and NW genres.
There is no definitive con-clusion that a specific ML approach is better suited for a specific data genre.
We observe slightly bet-ter performance for the CRF ML approach in the MLBaseline CRF condition for both BN and NW.
The worst performance is yielded for the WL data.
This may be attributed to the small amount of training data available for this genre.
"Moreover the quality of the performance of the different fea-ture extraction tools such as AMIRA (for POS tag-ging and BPC) and MADA (for the morphological features) are optimized for NW data genres, thereby yielding suboptimal performance on the WL genre, leading to more noise than signal for training."
"How-ever, comparing relative performance on this genre, we see a significant jump from the most frequent baseline FreqBaseline (F β=1 =27.66) to the best baseline MLBaseline CRF (F β=1 =55.32)."
We see a further significant improvement when the Best Feat-set/ML setting is applied yielding an F β=1 =57.3.
"Interestingly, however the MLBaseline CRF yields a much better performance (F β=1 =55.32) than All Feats CRF with an F β=1 =27.36."
This may indi-cate that a global system that trains all classes at once using CRF for sparse data is better than train-ing separate classifiers and then combining the out- puts.
"It is worth noting the difference between MLBaseline SV M and All Feats SVM, F β=1 =54.68 and F β=1 =55.58, respectively."
This result suggests that SVM are more robust to less training data as il-lustrated in the case of the individual classifiers in the latter setting.
"Comparing dev and test performance, we note that the overall results on the dev data are better than those obtained on the test data, which is expected given that the weights for the FBVS ranking are de-rived based on the dev data used as a tuning set."
"The only counter example for this trend is with the WL data genre, where the test data yields a significantly higher performance for all the conditions except for All Feats CRF."
"As observed in Table 3, the ranking of the indi-vidual features could be very different for two NE classes."
"For instance, the BPC is ranked 4th for the PER class and is ranked 13th for GPE and ORG classes."
The disparity in ranking for the same indi-vidual features strongly suggests that using the same features for all the classes cannot lead to a global op-timal classifier.
"With regards to morphological fea-tures, we note in Table 3, that Definiteness, M DEF , is helpful for all the NE classification systems, by virtue of being included for all optimal systems for all NE classification systems."
"Aspect, M ASP , is use-ful for all classes except PER."
"Moreover, M GEN and M NUM , corresponding to Gender and Number, re-spectively, contributed significantly to the increase in recall for PER and GPE classes."
"Finally, the Per-son feature, M PER contributed mostly to improv-ing the classification of ORG and FAC classes."
"Ac-cordingly, observing these results, contrary to pre-vious results[REF_CITE], our results strongly suggest the significant impact morpholog-ical features have on Arabic NER, if applied at the right level of granularity."
Inconsistencies in the data lead to many of the ob-served errors.
The problem is that the ACE data is annotated primarily for a mention detection task which leads to the same exact words not being anno-tated consistently.
"For instance, the word ’Palestini-ans’ would sometimes be annotated as a GPE class while in similar other contexts it is not annotated as a named entity at all."
"Since we did not manually cor-rect these cases, the classifiers are left with mixed signals."
The VEH and WEA classes both exhibit a uniform ranking for all the features and yield a very low performance.
This is mainly attributed to the fact that they appear very rarely in the training data.
"For instance, in the[REF_CITE]BN genre, there are 1707 instances of the class[REF_CITE]of FAC and only 4[REF_CITE]for VEH."
We described the performance yielded using language-dependent and language independent fea-tures in SVM and CRF for the NER task on differ-ent standard Arabic data-sets comprising different genres.
"We have measured the impact of each fea-ture individually on each class, we ranked them ac-cording to their impact using the Fuzzy Borda Vot-ing Scheme, and then performed an incremental fea-tures’ selection considering each time the N best features."
We reported the importance of each feature for each class and then the performance obtained when the best feature-set is used.
Our experiments yield state of the art performance significantly outper-forming the baseline.
Our best results achieve an F β=1 score of 83.5 for the[REF_CITE]BN data.
It is worth noting that these obtained results are trained on less data since we train only on 3 folds vs the standard 4 folds.
Our results show that the SVM and CRF have very sim-ilar behaviors.
"However, SVM showed more robust performance in our system using data with very ran-dom contexts, namely for the WL data, i.e. We-blogs."
We definitively illustrate that correctly ex-ploiting morphological features for languages with rich morphological structures yields state of the art performance.
"For future work, we intend to investi-gate the use of automatic feature selection methods on the same data."
The authors would like to thank the reviewers for their detailed constructive comments.
We would like to thank[REF_CITE]-15265-C06-04 and PCI-AECI A/010317/07 research projects for par-tially funding this work.
Mona Diab would like to ac-knowledge DARPA GALE Grant Contract No.
"In recent years there has been substantial work on the important problem of coreference res-olution, most of which has concentrated on the development of new models and algo-rithmic techniques."
These works often show that complex models improve over a weak pairwise baseline.
"However, less attention has been given to the importance of selecting strong features to support learning a corefer-ence model."
"This paper describes a rather simple pair-wise classification model for coreference res-olution, developed with a well-designed set of features."
We show that this produces a state-of-the-art system that outperforms sys-tems built with complex models.
We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more ro-bust set of features is used.
The paper also presents an ablation study and discusses the relative contributions of various features.
Coreference resolution is the task of grouping all the mentions of entities 1 in a document into equivalence classes so that all the mentions in a given class refer to the same discourse entity.
"For example, given the sentence (where the head noun of each mention is subscripted)"
"An American 1 official 2 announced that American 1 President 3 Bill Clinton 3 met his 3 Russian 4 counterpart 5 , Vladimir Putin 5 , today. the task is to group the mentions so that those refer-ring to the same entity are placed together into an equivalence class."
"Many NLP tasks detect attributes, actions, and relations between discourse entities."
"In order to discover all information about a given entity, tex-tual mentions of that entity must be grouped to-gether."
"Thus coreference is an important prerequi-site to such tasks as textual entailment and informa-tion extraction, among others."
"Although coreference resolution has received much attention, that attention has not focused on the relative impact of high-quality features."
"Thus, while many structural innovations in the modeling ap-proach have been made, those innovations have gen-erally been tested on systems with features whose strength has not been established, and compared to weak pairwise baselines."
"As a result, it is possible that some modeling innovations may have less im-pact or applicability when applied to a stronger base-line system."
"This paper introduces a rather simple but state-of-the-art system, which we intend to be used as a strong baseline to evaluate the impact of structural innovations."
"To this end, we combine an effective coreference classification model with a strong set of features, and present an ablation study to show the relative impact of a variety of features."
"As we show, this combination of a pairwise model and strong features produces a [Footnote_1].5 percent- age point increase in B-Cubed F-Score over a com-plex model in the state-of-the-art system[REF_CITE], although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions."
"1 We follow the ACE[REF_CITE]terminology: A noun phrase referring to a discourse entity is called a mention, and an equivalence class is called an entity."
"Given a document and a set of mentions, corefer-ence resolution is the task of grouping the mentions into equivalence classes, so that each equivalence class contains exactly those mentions that refer to the same discourse entity."
"The number of equiv-alence classes is not specified in advance, but is bounded by the number of mentions."
"In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, generate a set of edges such that any two mentions that belong in the same equiva-lence class are connected by some path in the graph."
"We construct this entity-mention graph by learning to decide for each mention which preceding men-tion, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coref-erence model[REF_CITE]."
"To decide whether two mentions should be linked in the graph, we learn a pairwise coreference function pc that produces a value indicating the probability that the two men-tions should be placed in the same equivalence class."
The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function.
"Given a document d and a pairwise coreference scor-ing function pc that maps an ordered pair of men-tions to a value indicating the probability that they are coreferential (see Section 2.2), we generate a coreference graph G d according to the Best-Link de-cision model[REF_CITE]as follows:"
"For each mention m in document d, let B m be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmax(pc(b, m)). b∈B m"
"If pc(a, m) is above a threshold chosen as described in Section 4.4, we add the edge (a, m) to the coref-erence graph G d ."
"The resulting graph contains connected compo-nents, each representing one equivalence class, with all the mentions in the component referring to the same entity."
"This technique permits us to learn to detect some links between mentions while being ag-nostic about whether other mentions are linked, and yet via the transitive closure of all links we can still determine the equivalence classes."
"We also require that no non-pronoun can refer back to a pronoun: If m is not a pronoun, we do not consider pronouns as candidate antecedents."
"For pairwise models, it is common to choose the best antecedent for a given mention (thereby impos-ing the constraint that each mention has at most one antecedent); however, the method of deciding which is the best antecedent varies."
Best-Link was shown to out-perform Closest-Link in an experiment[REF_CITE].
"Our model differs from that of Ng and Cardie in that we impose the constraint that non-pronouns cannot refer back to pronouns, and in that we use as training examples all ordered pairs of mentions, subject to the constraint above."
"Since the number of possi-ble classes is exponential in the number of mentions, they use heuristics to select training examples."
Our method does not require determining which equiva-lence classes should be considered as examples.
Learning the pairwise scoring function pc is a cru-cial issue for the pairwise coreference model.
"We apply machine learning techniques to learn from ex-amples a function pc that takes as input an ordered pair of mentions (a, m) such that a precedes m in the document, and produces as output a value that is interpreted as the conditional probability that m and a belong in the same equivalence class."
The ACE training data provides the equivalence classes for mentions.
"However, for some pairs of mentions from an equivalence class, there is little or no direct evidence in the text that the mentions are coreferential."
"Therefore, training pc on all pairs of mentions within an equivalence class may not lead to a good predictor."
"Thus, for each mention m we select from m’s equivalence class the closest pre-ceding mention a and present the pair (a,m) as a positive training example, under the assumption that there is more direct evidence in the text for the ex-istence of this edge than for other edges."
This is similar to the technique[REF_CITE].
"For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class."
Note that in doing so we generate more negative examples than positive ones.
"Since we never apply pc to a pair where the first mention is a pronoun and the second is not a pro-noun, we do not train on examples of this form."
We learn the pairwise coreference function using an averaged perceptron learning algorithm[REF_CITE]– we use the regularized version in Learning Based Java [Footnote_2][REF_CITE].
2 LBJ code is available[URL_CITE]
The performance of the document-level coreference model depends on the quality of the pairwise coref-erence function pc.
"Beyond the training paradigm described earlier, the quality of pc depends on the features used."
"We divide the features into categories, based on their function."
A full list of features and their cat-egories is given in Table 2.
"In addition to these boolean features, we also use the conjunctions of all pairs of features. [Footnote_3]"
3 The package of all features used is available[URL_CITE]php?skey=LBJ#features.
"In the following description, the term head means the head noun phrase of a mention; the extent is the largest noun phrase headed by the head noun phrase."
"The type of a mention indicates whether it is a proper noun, a common noun, or a pronoun."
"This feature, when conjoined with others, allows us to give dif-ferent weight to a feature depending on whether it is being applied to a proper name or a pronoun."
"For our experiments in Section 5, we use gold mention types as is done[REF_CITE]and[REF_CITE]."
Note that in the experiments described in Sec-tion 6 we predict the mention types as described there and do not use any gold data.
The mention type feature is used in all experiments.
"String relation features indicate whether two strings share some property, such as one being the substring of another or both sharing a modifier word."
Features are listed in Table 1.
Modifiers are limited to those occurring before the head.
Another class of features captures the semantic re-lation between two words.
"Specifically, we check whether gender or number match, or whether the mentions are synonyms, antonyms, or hypernyms."
We also check the relationship of modifiers that share a hypernym.
Descriptions of the methods for computing these features are described next.
"We determine the gender (male, female, or neuter) of the two phrases, and report whether they match (true, false, or unknown)."
"For a proper name, gender is determined by the exis-tence of mr, ms, mrs, or the gender of the first name."
"If only a last name is found, the phrase is consid-ered to refer to a person."
"If the name is found in a comprehensive list of cities or countries, or ends with an organization ending such as inc, then the gender is neuter."
"In the case of a common noun phrase, the phrase is looked up in WordNet[REF_CITE], and it is assigned a gender according to whether male, female, person, artifact, location, or group (the last three correspond to neuter) is found in the hypernym tree."
The gender of a pronoun is looked up in a table.
Number Match Number is determined as fol-lows:
"Phrases starting with the words a, an, or this are singular; those, these, or some indicate plural."
Names not containing and are singular.
Common nouns are checked against extensive lists of singular and plural nouns – words found in neither or both lists have unknown number.
"Finally, if the num-ber is unknown yet the two mentions have the same spelling, they are assumed to have the same number."
"We check whether any sense of one head noun phrase is a synonym, antonym, or hypernym of any sense of the other."
"We also check whether any sense of the phrases share a hypernym, after dropping entity, abstraction, physical entity, object, whole, artifact, and group from the senses, since they are close to the root of the hypernym tree."
Modifiers Match Determines whether the text be-fore the head of a mention matches the head or the text before the head of the other mention.
Both Mentions Speak True if both mentions ap-pear within two words of a verb meaning to say.
Be-ing in a window of size two is an approximation to being a syntactic subject of such a verb.
This feature is a proxy for having similar semantic types.
Additional evidence is derived from the relative lo-cation of the two mentions.
"We thus measure dis-tance (quantized as multiple boolean features of the form [distance ≥ i]) for all i up to the distance and less than some maximum, using units of compatible mentions, and whether the mentions are in the same sentence."
We also detect apposition (mentions sepa-rated by a comma).
"For details, see Table 3."
"Modifier Names If the mentions are both mod-ified by other proper names, use a basic corefer-ence classifier to determine whether the modifiers are coreferential."
"This basic classifier is trained using Mention Types, String Relations, Semantic Features, Apposition, Relative Pronoun, and Both Speak."
"For each mention m, examples are generated with the closest antecedent a to form a positive ex-ample, and every mention between a and m to form negative examples."
"Thus, we learn a separate classifier to detect whether a mention is anaphoric (that is, whether it is not the first mention in its equivalence class), and use that classifier’s output as a feature for the coref-erence model."
"Features for the anaphoricity classi-fier include the mention type, whether the mention appears in a quotation, the text of the first word of the extent, the text of the first word after the head (if that word is part of the extent), whether there is a longer mention preceding this mention and having the same head text, whether any preceding mention has the same extent text, and whether any preceding mention has the same text from beginning of the ex-tent to end of the head."
Conjunctions of all pairs of these features are also used.
This classifier predicts anaphoricity with about 82% accuracy.
We determine the relationship of any pair of modi-fiers that share a hypernym.
"Each aligned pair may have one of the following relations: match, sub-string, synonyms, hypernyms, antonyms, or mis-match."
Mismatch is defined as none of the above.
We restrict modifiers to single nouns and adjectives occurring before the head noun phrase.
We allow our system to learn which pairs of nouns tend to be used to mention the same entity.
"For ex-ample, President and he often refer to Bush but she and Prime Minister rarely do, if ever."
"To enable the system to learn such patterns, we treat the presence or absence of each pair of final head nouns, one from each mention of an example, as a feature."
"We predict the entity type (person, organization, geo-political entity, location, facility, weapon, or ve-hicle) as follows: If a proper name, we check a list of personal first names, and a short list of honorary ti-tles (e.g. mr) to determine if the mention is a person."
"Otherwise we look in lists of personal last names drawn from US census data, and in lists of cities, states, countries, organizations, corporations, sports teams, universities, political parties, and organiza-tion endings (e.g. inc or corp)."
"If found in exactly one list, we return the appropriate type."
We return unknown if found in multiple lists because the lists are quite comprehensive and may have significant overlap.
"For common nouns, we look at the hypernym tree for one of the following: person, political unit, loca-tion, organization, weapon, vehicle, industrial plant, and facility."
"If any is found, we return the appropri-ate type."
"If multiple are found, we sort as in the above list."
"For personal pronouns, we recognize the entity as a person; otherwise we specify unknown."
This computation is used as part of the following two features.
Entity Type Match
This feature checks to see whether the predicted entity types match.
"The result is true if the types are identical, false if they are dif-ferent, and unknown if at least one type is unknown."
"Entity Type Conjunctions This feature indicates the presence of the pair of predicted entity types for the two mentions, except that if either word is a pro-noun, the word token replaces the type in the pair."
"Since we do this replacement for entity types, we also add a similar feature for mention types here."
"These features are boolean: For any given pair, a feature is active if that pair describes the example."
Many of our features are similar to those described[REF_CITE].
"This includes Mention Types, String Relation Features, Gender and Num-ber Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak."
The implementations of those features may vary from those of other systems.
"Anaphoricity has been pro-posed as a part of the model in several systems, in-cluding[REF_CITE], but we are not aware of it being used as a feature for a learning algorithm."
Distances have been used in e.g.[REF_CITE].
"However, we are not aware of any system using the number of compatible mentions as a distance."
We use the official[REF_CITE]English training data[REF_CITE].
"Much work has been done on coreference in several languages, but for this work we focus on English text."
"We split the corpus into three sets: Train, Dev, and Test."
Our test set contains the same 107 documents[REF_CITE].
Our training set is a random 80% of the 336 doc-uments in their training set and our Dev set is the remaining 20%.
"For our ablation study, we further randomly split our development set into two evenly sized parts, Dev-Tune and Dev-Eval."
"For each experiment, we set the parameters of our algorithm to optimize B-Cubed F-Score using Dev-Tune, and use those pa-rameters to evaluate on the Dev-Eval data."
"For the experiments in Section 5, following[REF_CITE], to make experiments more compara-ble across systems, we assume that perfect mention boundaries and mention type labels are given."
We do not use any other gold annotated input at evalu-ation time.
In Section 6 experiments we do not use any gold annotated input and do not assume mention types or boundaries are given.
In all experiments we automatically split words and sentences using our preprocessing tools. [Footnote_4]
4 The code is available[URL_CITE]
"B-Cubed F-Score We evaluate over the com-monly used B-Cubed F-Score[REF_CITE], which is a measure of the overlap of predicted clusters and true clusters."
"It is computed as the har-monic mean of precision (P ),   1 X X c m P = p m  ,  (1) N d∈D m∈d and recall (R),   1 X X c m R = (2) t m  ,  N d∈D m∈d where c m is the number of mentions appearing both in m’s predicted cluster and in m’s true clus-ter, p m is the size of the predicted cluster containing m, and t m is the size of m’s true cluster."
"Finally, d represents a document from the set D, and N is the total number of mentions in D."
"B-Cubed F-Score has the advantage of being able to measure the impact of singleton entities, and of giving more weight to the splitting or merging of larger entities."
It also gives equal weight to all types of entities and mentions.
"For these reasons, we re-port our results using B-Cubed F-Score."
MUC F-Score We also provide results using the official MUC scoring algorithm[REF_CITE].
The MUC F-score is also the harmonic mean of precision and recall.
"However, the MUC precision counts precision errors by computing the minimum number of links that must be added to ensure that all mentions referring to a given entity are connected in the graph."
Recall errors are the number of links that must be removed to ensure that no two men-tions referring to different entities are connected in the graph.
We train a regularized average perceptron using ex-amples selected as described in Section 2.2.1.
The learning rate is 0.1 and the regularization parameter (separator thickness) is 3.5.
"At training time, we use a threshold of 0.0, but when evaluating, we select pa-rameters to optimize B-Cubed F-Score on a held-out development set."
We sample all even integer thresh-olds from -16 to 8.
"We choose the number of rounds of training similarly, allowing any number from one to twenty."
"In Table 4, we compare our performance against a system that is comparable to ours: Both use gold mention boundaries and types, evaluate using B-Cubed F-Score, and have the same training and test data split."
Our results show that a pairwise model with strong features outperforms a state-of-the-art system with a more complex model.
MUC Score We evaluate the performance of our system using the official MUC score in Table 5.
In Table 6 we show the relative impact of various features.
"We report data on Dev-Eval, to avoid the possibility of overfitting by feature selection."
The parameters of the algorithm are chosen to maximize the BCubed F-Score on the Dev-Tune data.
"Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable[REF_CITE]."
"For comparable results, see Table 4 and the discussion above."
"Our ablation study shows the impact of various classes of features, indicating that almost all the fea-tures help, although some more than others."
"It also illustrates that some features contribute more to pre-cision, others more to recall."
"For example, aligned modifiers contribute primarily to precision, whereas our learned features and our apposition features con-tribute to recall."
"This information can be useful when designing a coreference system in an applica-tion where recall is more important than precision, or vice versa."
"We examine the effect of some important features, selecting those that provide a substantial improve-ment in precision, recall, or both."
"For each such feature we examine the rate of coreference amongst mention pairs for which the feature is active, com-pared with the overall rate of coreference."
We also show examples on which the coreference systems differ depending on the presence or absence of a fea-ture.
"Apposition This feature checks whether two men-tions are separated by only a comma, and it in-creases B-Cubed F-Score by about one percentage point."
"We hypothesize that proper names and com-mon noun phrases link primarily through apposition, and that apposition is thus a significant feature for good coreference resolution."
"When this feature is active 36% of the examples are coreferential, whereas only 6% of all examples are coreferential."
"Looking at some examples our system begins to get right when apposition is added, we find the phrase"
"Israel’s Deputy Defense Minister, Ephraim Sneh."
"Upon adding apposition, our system begins to cor-rectly associate Israel’s Deputy Defense Minister with Ephraim Sneh."
Likewise in the phrase
"The court president, Ronald Sutherland, the system correctly associates The court president with Ronald Sutherland when they appear in an ap-positive relation in the text."
"In addition, our system begins correctly associating relative pronouns such as who with their referents in phrases like"
"Sheikh Abbad, who died 500 years ago. although an explicit relative pronoun feature is added only later."
"Although this feature may lead the system to link comma separated lists of entities due to misinter-pretation of the comma, for example Wyoming and western South Dakota in a list of locations, we be-lieve this can be avoided by refining the apposition feature to ignore lists."
Next we investigate the relative pronoun feature.
"With this feature active, 93% of examples were positive, indicating the precision of this feature."
"Looking to examples, we find who in the official, who wished to remain anony-mous is properly linked, as is that in nuclear warheads that can be fitted to mis-siles."
"Distances Our distance features measure separa-tion of two mentions in number of compatible men-tions (quantized), and whether the mentions are in the same sentence."
"Distance features are important for a system that makes links based on the best pair-wise coreference value rather than implicitly incor-porating distance by linking only the closest pair whose score is above a threshold, as done by e.g.[REF_CITE]."
"Looking at examples, we find that adding dis-tances allows the system to associate the pronoun it with this missile not separated by any mentions, rather than Tehran, which is separated from it by many mentions."
"Predicted Entity Types Since no two mentions can have different entity types (person, organization, geo-political entity, etc.) and be coreferential, this feature has strong discriminative power."
"When the entity types match, 13% of examples are positive compared to only 6% of examples in general."
"Qual-itatively, the entity type prediction correctly recog-nizes the Gulf region as a geo-political entity, and He as a person, and thus prevents linking the two."
"Likewise, the system discerns Baghdad from am-bassador due to the entity type."
"However, in some cases an identity type match can cause the system to be overly confident in a bad match, as in the case of a palestinian state identified with holy Jerusalem on the basis of proximity and shared entity type."
This type of example may require some additional world knowledge or deeper comprehension of the docu-ment.
The ultimate goal for a coreference system is to process unannotated text.
We use the term end-to-end coreference for a system capable of determin-ing coreference on plain text.
"We describe the chal-lenges associated with an end-to-end system, de-scribe our approach, and report results below."
"Developing an end-to-end system requires detecting and classifying mentions, which may degrade coref-erence results."
One challenge in detecting mentions is that they are often heavily nested.
"Additionally, there are issues with evaluating an end-to-end sys-tem against a gold standard corpus, resulting from the possibility of mismatches in mention boundaries, missing mentions, and additional mentions detected, along with the need to align detected mentions to their counterparts in the annotated data."
We resolve coreference on unannotated text as fol-lows: First we detect mention heads following a state of the art chunking approach[REF_CITE]using standard features.
This results in a 90% F 1 head detector.
"Next, we detect the extent boundaries for each head using a learned classifier."
"This is followed by determining whether a mention is a proper name, common noun phrase, prenominal modifier, or pronoun using a learned mention type classifier that."
"Finally, we apply our coreference al-gorithm described above."
"To evaluate, we align the heads of the detected men-tions to the gold standard heads greedily based on number of overlapping words."
We choose not to impute errors to the coreference system for men-tions that were not detected or for spuriously de-tected mentions (following[REF_CITE]and oth-ers).
"Although this evaluation is lenient, given that the mention detection component performs at over 90% F 1 , we believe it provides a realistic measure for the performance of the end-to-end system and fo-cuses the evaluation on the coreference component."
The results of our end-to-end coreference system are shown in Table 7.
We described and evaluated a state-of-the-art coref-erence system based on a pairwise model and strong features.
"While previous work showed the impact of complex models on a weak pairwise baseline, the applicability and impact of such models on a strong baseline system such as ours remains uncertain."
"We also studied and demonstrated the relative value of various types of features, showing in particular the importance of distance and apposition features, and showing which features impact precision or recall more."
"Finally, we showed an end-to-end system ca-pable of determining coreference in a plain text doc-ument."
"Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summa-rization."
"In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: k-means and Expectation Maximization (EM), for computing relative importance of the sen-tences."
"However, the performance of these ap-proaches depends entirely on the feature set used and the weighting of these features."
"We extracted different kinds of features (i.e. lex-ical, lexical semantic, cosine similarity, ba-sic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query."
We used a local search technique to learn the weights of the features.
"For all our methods of generating summaries, we have shown the effects of syn-tactic and shallow-semantic features over the bag of words (BOW) features."
"After having made substantial headway in factoid and list questions, researchers have turned their at-tention to more complex information needs that can-not be answered by simply extracting named enti-ties (persons, organizations, locations, dates, etc.) from documents."
"For example, the question: “De-scribe the after-effects of cyclone[REF_CITE]in Bangladesh” requires inferencing and synthesizing information from multiple documents."
"This infor-mation synthesis in NLP can be seen as a kind of topic-oriented, informative multi-document summa-rization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information."
"In this paper, we experimented with one em-pirical and two well-known unsupervised statisti-cal machine learning techniques: k-means and EM and evaluated their performance in generating topic-oriented summaries."
"However, the performance of these approaches depends entirely on the feature set used and the weighting of these features."
"We ex-tracted different kinds of features (i.e. lexical, lexi-cal semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query."
We have used a gradient descent local search technique to learn the weights of the features.
"Traditionally, information extraction techniques are based on the BOW approach augmented by language modeling."
"But when the task requires the use of more com-plex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis."
"Some improvements on BOW are given by the use of dependency trees and syntactic parse trees[REF_CITE],[REF_CITE],[REF_CITE], but these, too are not ade-quate when dealing with complex questions whose answers are expressed by long and articulated sen-tences or even paragraphs."
"Shallow semantic rep-resentations, bearing a more compact information, could prevent the sparseness of deep structural ap-proaches and the weakness of BOW models[REF_CITE]."
"Attempting an application of syntactic and semantic information to complex QA hence seems natural, as pinpointing the answer to a question relies on a deep understanding of the se-mantics of both."
"In more complex tasks such as computing the relatedness between the query sen-tences and the document sentences in order to gen-erate query-focused summaries (or answers to com-plex questions), to our knowledge no study uses tree kernel functions to encode syntactic/semantic infor-mation."
"For all our methods of generating sum-maries (i.e. empirical, k-means and EM), we have shown the effects of syntactic and shallow-semantic features over the BOW features."
"This paper is organized as follows: Section 2 fo-cuses on the related work, Section 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we re-move the redundant sentences before adding them to the summary, Section 6 describes our experimen-tal study."
We conclude and give future directions in Section 7.
Researchers all over the world working on query-based summarization are trying different direc-tions to see which methods provide the best re-sults.
The LexRank method addressed[REF_CITE]was very successful in generic multi-document summarization.
A topic-sensitive LexRank is proposed[REF_CITE].
"As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences."
Then the system ranked the sentences according to a ran-dom walk model defined in terms of both the inter-sentence similarities and the similarities of the sen-tences to the topic description or question.
"The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words[REF_CITE]."
"Then using WordNet, the systems find the semantic similarity between the nouns and compound nouns."
"After that, lexical chains are built in two steps: 1) Building single document strong chains while dis-ambiguating the senses of the words and, 2) build-ing multi-chain by merging the strongest chains of the single documents into one chain."
"The systems rank sentences using a formula that involves a) the lexical chain, b) keywords from query and c) named entities.[REF_CITE]introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) fac-toid QA techniques; and (c) Multi-Document Sum-marization (MDS) techniques."
"The question decom-position procedure operates on a Marcov chain, by following a random walk with mixture model on a bipartite graph of relations established between con-cepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations."
Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS sys-tem.
They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions.
There are approaches that are based on probabilis-tic models[REF_CITE].[REF_CITE]rank the sentences based on a mixture model where each component of the model is a statistical model:
"Score(s) = α×QIScore(s)+(1−α)×QFocus(s, Q)"
"Where, Score(s) is the score for sentence s. Query-independent score (QIScore) and query-dependent score (QFocus) are calculated based on probabilistic models.[REF_CITE]learns a log-linear sentence rank-ing model by maximizing three metrics of sentence good-ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency."
The scoring function is learned by fit-ting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion.
The scoring function is fur-ther adapted to apply to summaries rather than sentences and to take into account redundancy among sentences.
"There are approaches in “Recognizing Textual Entail-ment”, “Sentence Alignment” and “Question Answering” that use syntactic and/or semantic information in order to measure the similarity between two textual units.[REF_CITE]use typed dependency graphs (same as dependency trees) to represent the text and the hypoth-esis."
Then they try to find a good partial alignment be-tween the typed dependency graphs representing the hy-pothesis and the text in a search space of O((m + 1)n) where hypothesis graph contains n nodes and a text graph contains m nodes.[REF_CITE]represent the sen-tences using Dependency Tree Path (DTP) to incorporate syntactic information.
They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences.
They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs.[REF_CITE]use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment.
"According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. dele-tion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold.[REF_CITE]represent the ques-tion and the sentence containing answer with their depen-dency trees."
"They add semantic information (i.e. named entity, synonyms and other related words) in the depen-dency trees."
They apply the approximate tree matching in order to decide how similar any given pair of trees are.
They also use the edit distance as the matching criteria in the approximate tree matching.
All these methods show the improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-dency tree to extract the BEs (i.e. head-modifier-relation) and ranked the BEs based on their log-likelihood ratios.
"For syntactic feature, we extracted the syntactic trees for the sentence as well as for the query using the Charniak parser and measured the similarity between the two trees using the tree kernel function."
We used the ASSERT se-mantic role labeler system to parse the sentence as well as the query semantically and used the shallow seman-tic tree kernel to measure the similarity between the two shallow-semantic trees.
The sentences in the document collection are analyzed in various levels and each of the document-sentences is represented as a vector of feature-values.
The features can be divided into several categories:
N-gram overlap measures the overlapping word se-quences between the candidate sentence and the query sentence.
"With the view to measure the N-gram (N=[Footnote_1],2,3,4) overlap scores, a query pool and a sentence pool are created."
"1 hence forth important words are the nouns, verbs, adverbs and adjectives"
"In order to create the query (or sentence) pool, we took the query (or document) sentence and cre-ated a set of related sentences by replacing its important words 1 by their first-sense synonyms."
"For example given a stemmed document-sentence: “John write a poem”, the sentence pool contains: “John compose a poem”, “John write a verse form” along with the given sentence."
"We measured the recall based n-gram scores for a sentence P using the following formula: n-gramScore(P) = max i (max j N-gram(s i , q j ))"
"P Count match (gram n ) N-gram(S,Q) ="
P gramn∈S Count(gram n ) gramn∈S
"Where, n stands for the length of the n-gram (n = 1, 2, 3, 4) and Count match (gram n ) is the number of n-grams co-occurring in the query and the candi-date sentence, q j is the j th sentence in the query pool and s i is the i th sentence in the sentence pool of sentence P ."
"A sequence W = [w 1 ,w 2 ,...,w n ] is a subse-quence of another sequence X = [x 1 , x 2 , ..., x m ], if there exists a strict increasing sequence [i 1 , i 2 , ..., i k ] of indices of X such that for all j = 1, 2, ..., k we have x i j = w j ."
"Given two sequences, S 1 and S 2 , the Longest Common Subsequence (LCS) of S 1 and S 2 is a common subsequence with maximum length."
"The longer the LCS of two sen-tences is, the more similar the two sentences are."
The basic LCS has a problem that it does not dif-ferentiate LCSes of different spatial relations within their embedding sequences[REF_CITE].
"To improve the basic LCS method, we can remember the length of consecutive matches encountered so far to a reg-ular two dimensional dynamic program table com-puting LCS."
We call this weighted LCS (WLCS) and use k to indicate the length of the current con-secutive matches ending at words x i and y j .
"Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic pro-gramming procedure as stated[REF_CITE]."
We computed the LCS and WLCS-based F-measure fol-lowing[REF_CITE]using both the query pool and the sentence pool as in the previous section.
"Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps."
Skip-bigram mea-sures the overlap of skip-bigrams between a candi-date sentence and a query sentence.
"Following[REF_CITE], we computed the skip bi-gram score using both the sentence pool and the query pool."
The number of head words common in between two sentences can indicate how much they are rel-evant to each other.
"In order to extract the heads from the sentence (or query), the sentence (or query) is parsed by Minipar [URL_CITE] and from the dependency tree we extract the heads which we call exact head words."
"For example, the head word of the sentence: “John eats rice” is “eat”."
"We take the synonyms, hyponyms and hyper-nyms [Footnote_3] of both the query-head words and the sentence-head words and form a set of words which we call head-related words."
3 hypernym and hyponym levels are restricted to 2 and 3 re-spectively
We measured the exact head score and the head-related score as follows:
P Count match (w 1 ) ExactHeadScore = w P 1∈HeadSet Count(w 1 ) w1∈HeadSet P Count match (w 1 ) w1∈HeadRelSet
HeadRelatedScore = P Count(w 1 ) w1∈HeadRelSet
Where HeadSet is the set of head words in the sen-tence and Count match is the number of matches between the HeadSet of the query and the sen-tence.
"HeadRelSet is the set of synonyms, hy-ponyms and hypernyms of head words in the sen-tence and Count match is the number of matches between the head-related words of the query and the sentence."
"We form a set of words which we call QueryRe-latedWords by taking the important words from the query, their first-sense synonyms, the nouns’ hy-pernyms/hyponyms and important words from the nouns’ gloss definitions."
Synonym overlap measure is the overlap be-tween the list of synonyms of the important words extracted from the candidate sentence and the QueryRelatedWords.
"Hypernym/hyponym overlap measure is the overlap between the list of hypernyms and hyponyms of the nouns extracted from the sen-tence and the QueryRelatedWords, and gloss overlap measure is the overlap between the list of important words that are extracted from the gloss definitions of the nouns of the sentence and the QueryRelated-Words."
Statistical similarity measures are based on the co-occurance of similar words in a corpus.
We have used two statistical similarity measures: 1.
Dependency-based similarity measure and 2.
Proximity-based similarity measure.
Dependency-based similarity measure uses the dependency relations among words in order to mea-sure the similarity.
It extracts the dependency triples then uses statistical approach to measure the similar-ity.
Proximity-based similarity measure is computed based on the linear proximity relationship between words only.
It uses the information theoretic defini-tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin [URL_CITE] .
"Using the data, one can retrieve most similar words for a given word."
The similar words are grouped into clusters.
"Note that, for a word there can be more than one cluster."
Each cluster represents the sense of the word and its similar words for that sense.
"For each query word, we extract all of its clus-ters from the data."
"Now, in order to determine the right cluster for a query word, we measure the over-lap score between the QueryRelatedWords and the clusters of words."
"The hypothesis is that, the cluster that has more words common with the QueryRelat-edWords is the right cluster."
We chose the cluster for a word which has the highest overlap score.
"Once we get the clusters for the query words, we measured the overlap between the cluster words and the sentence words as follows:"
P Count match (w 1 ) Measure = w P 1∈SenWords Count(w 1 ) w1∈SenWords
"Where, SenWords is the set of important words ex-tracted from the sentence and Count match is the number of matches between the sentence words and the clusters of similar words of the query words."
"constructed, the sentences are then ranked according to their eigenvector centrality."
"To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed[REF_CITE]."
We followed a similar approach in order to calculate this feature.
The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similar-ity of the sentence to other high-scoring sentences.
"So far, we have included the features of type Bag of Words (BOW)."
"The task like query-based summarization that requires the use of more complex syntactic and se-mantics, the approaches with only BOW are often inade-quate to perform fine-level textual analysis."
We extracted three features that incorporate syntactic/semantic infor-mation.
"The “head-modifier-relation” triples, extracted from the dependency trees are considered as BEs in our exper-iment."
The triples encode some syntactic/semantic infor-mation and one can quite easily decide whether any two units match or not- considerably more easily than with longer units[REF_CITE].
We used the BE package distributed by ISI [Footnote_5] to extract the BEs for the sentences.
5 BE website[URL_CITE]cyl/BE
"Once we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following[REF_CITE]."
Sorting BEs according to their LR scores pro-duced a BE-ranked list.
Our goal is to generate a sum-mary that will answer the user questions.
The ranked list of BEs in this way contains important BEs at the top which may or may not be relevant to the user questions.
We filter those BEs by checking whether they contain any word which is a query word or a QueryRelatedWords (de-fined in Section 3.2).
The score of a sentence is the sum of its BE scores divided by the number of BEs in the sen-tence.
Encoding syntactic structure is easier and straight for-ward.
"Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the tree kernel defined[REF_CITE]."
"Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models[REF_CITE]."
Initiatives such as PropBank (PB)[REF_CITE]have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT[REF_CITE].
"For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency]"
"Such annotation can be used to design a shallow se-mantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency]"
"In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Se-mantic Tree (ST)."
"In the semantic tree, arguments are re-placed with the most important word-often referred to as the semantic head."
The sentences may contain one or more subordinate clauses.
"For example the sentence, “the Vatican, located wholly within Italy uses the Italian lira as their currency.” gives the STs as in Figure 2."
"As we can see in Fig-ure 2(A), when an argument node corresponds to an en-tire subordinate clause, we label its leaf with ST, e.g. the leaf of ARG0."
Such ST node is actually the root of the subordinate clause in Figure 2(B).
"If taken separately, such STs do not express the whole meaning of the sen-tence, hence it is more accurate to define a single struc-ture encoding the dependency between the two predicates as in Figure 2(C)."
We refer to this kind of nested STs as STNs.
"Note that, the tree kernel (TK) function defined[REF_CITE]computes the number of com-mon subtrees between two trees."
Such subtrees are sub-ject to the constraint that their nodes are taken with all or none of the children they have in the original tree.
"Though, this definition of subtrees makes the TK func-tion appropriate for syntactic trees but at the same time makes it not well suited for the semantic trees (ST) de-fined above."
"For instance, although the two STs of Fig-ure 1 share most of the subtrees rooted in the ST node, the kernel defined above computes only one match (ST ARG0 TARGET ARG1 ARG2) which is not useful."
The critical aspect of the TK function is that the pro-ductions of two evaluated nodes have to be identical to allow the match of further descendants.
This means that common substructures cannot be composed by a node with only some of its children as an effective ST represen-tation would require.[REF_CITE]solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST.
We fol-lowed the similar approach to compute the SSTK.
"In this section, we describe the scoring techniques in de-tail."
"In order to fine-tune the weights of the features, we used a local search technique with simulated annealing to find the global maximum."
"Initially, we set all the feature-weights, w 1 , · · · , w n , as equal values (i.e. 0.5) (see Al-gorithm 1)."
Based on the current weights we score the sentences and generate summaries accordingly.
We eval-uate the summaries using the automatic evaluation tool ROUGE[REF_CITE](described in Section 6) and the ROUGE value works as the feedback to our learning loop.
Our learning system tries to maximize the ROUGE score in every step by changing the weights individually by a specific step size (i.e. 0.01).
"That means, to learn weight w i , we change the value of w i keeping all other weight values (w j ∀ j6=i ) stagnant."
"For each weight w i , the algorithm achieves the local maximum of ROUGE value."
In order to find the global maximum we ran this algorithm multiple times with different random choices of initial values (i.e. simulated annealing).
"Input: Stepsize l, Weight Initial Value v Output: A vector w~ of learned weights Initialize the weight values w i to v. for i ← 1 to n do"
"Once we have learned the feature-weights, our empir-ical method computes the final scores for the sentences using the formula: score i = x~ i .w~ (1)"
"Where, x~ i is the feature vector for i-th sentence, w~ is the weight vector and score i is the score of i-th sentence."
We start with a set of initial cluster centers and go through several iterations of assigning each object to the cluster whose center is closest.
"After all objects have been as-signed, we recompute the center of each cluster as the centroid or mean (µ) of its members."
"Once we have learned the means of the clusters using the k-means algorithm, our next task is to rank the sen-tences according to a probability model."
We have used Bayesian model in order to do so.
"Bayes’ law says: p(x~|q k , Θ)P(q k |Θ)"
"P(q k |x~, Θ) = (2) P K p(~x|q k , Θ)p(q k |Θ) k=1 where q k is a class, x~ is a feature vector repre-senting a sentence and Θ is the parameter set of all class models."
We set the weights of the clusters as equiprobable (i.e. P(q k |Θ) = 1/K).
"We calculated p(x|q k ,Θ) using the gaussian probability distribu-tion."
"The gaussian probability density function (pdf) for the d-dimensional random variable ~x is given by: −1 (x~−µ) T Σ −1 (x~−µ) e 2 p (µ,Σ) (x~) = √ d (3) 2π pdet(Σ) where µ, the mean vector and Σ, the covariance matrix are the parameters of the gaussian distribu-tion."
We get the means (µ) from the k-means algo-rithm and we calculate the covariance matrix using the unbiased covariance estimation:
N Σ̂ = 1 X(x j − µ j )(x i − µ i )
T (4) N − 1 i=1
EM is an iterative two step procedure: 1. Expectation-step and 2.
"In the expectation step, we compute expected values for the hidden variables h i,j which are cluster mem-bership probabilities."
"Given the current parameters, we compute how likely an object belongs to any of the clusters."
The maximization step computes the most likely parameters of the model given the cluster membership probabilities.
The data-points are considered to be generated by a mixture model of k-gaussians of the form: k P(~x) =
"X P(C = i)P(x~|µ i , Σ i ) (5) i=1"
"Where the total likelihood of model Θ with k components given the observed data points, X = x 1 , · · · , x n is: n k L(Θ|X) ="
"Y X P(C = j)P(x i |Θ j ) i=1 j=1 n k = Y X w j P(x i |µ j , Σ j ) i=1 j=1 n k ⇔ X log X w j P(x i |µ j , Σ j ) i=1 j=1 where P is the probability density function (i.e. eq 3). µ j and Σ j are the mean and covariance ma-trix of component j, respectively."
"Each component contributes a proportion, w j , of the total population, such that: P Kj=1 w j = 1."
"However, a significant problem with the EM al-gorithm is that it converges to a local maximum of the likelihood function and hence the quality of the result depends on the initialization."
"In order to get good results from using random starting val-ues, we can run the EM algorithm several times and choose the initial configuration for which we get the maximum log likelihood among all con-figurations."
Choosing the best one among several runs is very computer intensive process.
"So, to im-prove the outcome of the EM algorithm on gaus-sian mixture models it is necessary to find a better method of estimating initial means for the compo-nents."
To achieve this aim we explored the widely used “k-means” algorithm as a cluster (means) find-ing method.
"That means, the means found by k-means clustering above will be utilized as the initial means for EM and we calculate the initial covari-ance matrices using the unbiased covariance estima-tion procedure (eq:4)."
"Once the sentences are clustered by EM al-gorithm, we filter out the sentences which are not query-relevant by checking their probabilities, P(q r |x i , Θ) where, q r denotes the cluster “query-relevant”."
"If for a sentence x i , P(q r |x i ,Θ) &gt; 0.5 then x i is considered to be query-relevant."
Our next task is to rank the query-relevant sen-tences in order to include them in the summary.
This can be done easily by multiplying the feature vector x~ i with the weight vector w~ that we learned by the local search technique (eq:1).
"When many of the competing sentences are included in the summary, the issue of information overlap be-tween parts of the output comes up, and a mecha-nism for addressing redundancy is needed."
"There-fore, our summarization systems employ a final level of analysis: before being added to the final output, the sentences deemed to be important are compared to each other and only those that are not too simi-lar to other candidates are included in the final an-swer or summary."
"Following[REF_CITE], we modeled this by BE overlap between an intermedi-ate summary and a to-be-added candidate summary sentence."
"We call this overlap ratio R, where R is between 0 and 1 inclusively."
"Setting R = 0.7 means that a candidate summary sentence, s, can be added to an intermediate summary, S, if the sentence has a BE overlap ratio less than or equal to 0.7."
We used the main task of Document Understanding Conference (DUC) 2007 for evaluation.
"The task was: “Given a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word sum-mary of the documents that answers the question(s) in the topic.”"
NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each topic.
Each topic and its document cluster were given to 4 different NIST as-sessors.
The assessor created a 250-word summary of the document cluster that satisfies the information need expressed in the topic statement.
These multi-ple “reference summaries” are used in the evaluation of summary content.
"We carried out automatic evaluation of our sum-maries using ROUGE[REF_CITE]toolkit, which has been widely adopted by DUC for automatic summarization evaluation."
"It measures summary quality by counting overlapping units such as the n-grams (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary."
ROUGE parameters were set as the same[REF_CITE]evaluation setup.
One purpose of our experiments is to study the impact of different features for complex question answering task.
"To accomplish this, we generated summaries for the topics[REF_CITE]by each of our seven systems defined as below:"
"The LEX system generates summaries based on only lexical features: n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym."
"The LSEM system considers only lexical semantic features: synonym, hypernym/hyponym, gloss, dependency-based and proximity-based similarity."
The COS system generates summary based on the graph-based method.
"The SYS1 system considers all the features except the BE, syntactic and seman-tic features."
The SYS2 system considers all the fea-tures except the syntactic and semantic features.
The SYS3 considers all the features except the semantic and the ALL [Footnote_6] system generates summaries taking all the features into account.
"6 SYS2, SYS3 and ALL systems show the impact of BE, syntactic and semantic features respectively"
"Table 1 7 to Table 3, Table 4 to Table 6 and Table 7 to Table 9 show the evaluation measures for k-means, EM and empirical approaches respectively."
"As Ta-ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3 gets 4-32% and ALL gets 3-36% improvement in ROUGE-2 scores over the SYS1 system."
We get best ROUGE-W (Table 2) scores for SYS2 (i.e. includ-ing BE) but SYS3 and ALL do not perform well in this case.
SYS2 improves the ROUGE-W F-score by 1% over SYS1.
We do not get any improvement in ROUGE-SU (Table 3) scores when we include any kind of syntactic/semantic structures.
The case is different for EM and empirical ap-proaches.
"Here, in every case we get a significant amount of improvement when we include the syn-tactic and/or semantic features."
"For EM (Table 4 to Table 6), the ratio of improvement in F-scores over SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2- 24% for ALL."
"In our empirical approach (Table 7 to Table 9), SYS2, SYS3 and ALL improve the F-scores by 3-11%, [Footnote_7]-15% and 8-19% over SYS1 re-spectively."
"7 R stands for Recall, P stands for Precision and F stands for F-score"
These results clearly indicate the positive impact of the syntactic/semantic features for com-plex question answering task.
The baseline system gener- ates summaries by returning all the leading sen-tences (up to 250 words) in the hTEXTi field of the most recent document(s).
It shows that the em-pirical approach outperforms the other two learning techniques and EM performs better than k-means al-gorithm.
EM improves the F-scores over k-means by 0.7-22.5%.
Empirical approach improves the F-scores over k-means and EM by 5.9-20.2% and 3.5- 6.5% respectively.
Comparing with the[REF_CITE]participants our systems achieve top scores and for some ROUGE measures there is no statistically sig-nificant difference between our system and the best[REF_CITE]system.
"Our experiments show the following: (a) our ap-proaches achieve promising results, (b) empirical approach outperforms the other two learning and EM performs better than the k-means algorithm for this particular task, and (c) our systems achieve bet-ter results when we include BE, syntactic and se-mantic features."
"In future, we have the plan to decompose the com-plex questions into several simple questions before measuring the similarity between the document sen-tence and the query sentence."
"We expect that by de-composing complex questions into the sets of sub-questions that they entail, systems can improve the average quality of answers returned and achieve bet-ter coverage for the question as a whole."
We describe the first tractable Gibbs sam-pling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment.
"We propose and evalu-ate two nonparametric priors that successfully avoid the degenerate behavior noted in previ-ous work, where overly large phrases mem-orize the training data."
Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems.
"In phrase-based translation, statistical knowledge of translation equivalence is primarily captured by counts of how frequently various phrase pairs occur in training bitexts."
"Since bitexts do not come seg-mented and aligned into phrase pairs, these counts are typically gathered by fixing a word alignment and applying phrase extraction heuristics to this word-aligned training corpus."
"Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges."
"In this paper, we address the two most signifi-cant challenges in phrase alignment modeling."
The first challenge is with inference: computing align-ment expectations under general phrase models is #P-hard[REF_CITE].
"Previous phrase alignment work has sacrificed consistency for effi-ciency, employing greedy hill-climbing algorithms and constraining inference with word alignments[REF_CITE]."
"We describe a Gibbs sampler that con-sistently and efficiently approximates expectations, using only polynomial-time computable operators."
"Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expecta-tions are guaranteed to converge to the true poste-rior distributions under the model (in theory) and do converge to effective values (in practice)."
"The second challenge in learning phrase align-ments is avoiding a degenerate behavior of the gen-eral model class: as with many models which can choose between large and small structures, the larger structures win out in maximum likelihood estima-tion."
"Indeed, the maximum likelihood estimate of a joint phrase alignment model analyzes each sen-tence pair as one large phrase with no internal struc-ture[REF_CITE]."
We describe two non-parametric priors that empirically avoid this degen-erate solution.
"Fixed word alignments are used in virtually ev-ery statistical machine translation system, if not to extract phrase pairs or rules directly, then at least to constrain the inference procedure for higher-level models."
"We estimate phrase translation features consistently using an inference procedure that is not constrained by word alignments, or any other heuris-tic."
"Despite this substantial change in approach, we report translation improvements over the standard word-alignment-based heuristic estimates of phrase table weights."
We view this result as an important step toward building fully model-based translation systems that rely on fewer procedural heuristics.
"While state-of-the-art phrase-based translation sys-tems include an increasing number of features, translation behavior is largely driven by the phrase pair count ratios φ(e|f) and φ(f|e)."
"These features are typically estimated heuristically using the counts c(he, fi) of all phrase pairs in a training corpus that are licensed by word alignments: c(he, fi) . φ(e|f) = P e 0 c(he 0 , fi)"
"In contrast, a generative model that explicitly aligns pairs of phrases he, fi gives us well-founded alternatives for estimating phrase pair scores."
"For instance, we could use the model’s parameters as translation features."
"In this paper, we compute the expected counts of phrase pairs in the training data according to our model, and derive features from these expected counts."
This approach endows phrase pair scores with well-defined semantics relative to a probabilistic model.
"Practically, phrase models can discover high-quality phrase pairs that often elude heuristics, as in Figure 1."
"In addition, the model-based approach fits neatly into the framework of sta-tistical learning theory for unsupervised problems."
"We first describe the symmetric joint model[REF_CITE], which we will extend."
"A two-step generative process constructs an ordered set of English phrases e 1:m , an ordered set of for-eign phrases f 1:n , and a phrase-to-phrase alignment between them, a = {(j, k)} indicating that he j , f k i is an aligned pair. 1. Choose a number of components ` and generate each of ` phrase pairs independently. [Footnote_2]. Choose an ordering for the phrases in the for-eign language; the ordering for English is fixed by the generation order. [Footnote_1]"
"2 Parameters were chosen by hand during development on a small training corpus. p $ = 0.1, b = 0.85 in experiments."
1 We choose the foreign to reorder without loss of generality.
"In this process, m = n = |a|; all phrases in both sentences are aligned one-to-one."
"We parameterize the choice of ` using a geometric distribution, denoted P G , with stop parameter p $ :"
P (`) = P G (`; p $ ) = p $ · (1 − p $ ) `−1 .
"Each aligned phrase pair he,fi is drawn from a multinomial distribution θ J which is unknown."
"We fix a simple distortion model, setting the probability of a permutation of the foreign phrases proportional to the product of position-based distortion penalties for each phrase:"
"P (a|{he, fi}) ∝ Y δ(a) a∈a δ(a = (j, k)) = b |pos(e j )−pos(f k )·s| , where pos(·) denotes the word position of the start of a phrase, and s the ratio of the length of the En-glish to the length of the foreign sentence."
This po-sitional distortion model was deemed to work best[REF_CITE].
We can now state the joint probability for a phrase-aligned sentence consisting of ` phrase pairs:
"P({he, fi}, a) = P G (`; p $ )P(a|{he, fi})"
"Yθ J (he, fi) . he,fi"
"While this model has several free parameters in ad-dition to θ J , we fix them to reasonable values to fo-cus learning on the phrase pair distribution. 2"
"Sentence pairs do not always contain equal informa-tion on both sides, and so we revise the generative story to include unaligned phrases in both sentences."
"When generating each component of a sentence pair, we first decide whether to generate an aligned phrase pair or, with probability p ø , an unaligned phrase. [Footnote_3] Then, we either generate an aligned phrase pair from θ J or an unaligned phrase from θ N , where θ N is a multinomial over phrases."
3 We strongly discouraged unaligned phrases in order to align as much of the corpus as possible: p ø = 10 −10 in ex-periments.
"Now, when generating e 1:m , f 1:n and alignment a, the number of phrases m + n can be greater than 2 · |a|."
"To unify notation, we denote unaligned phrases as phrase pairs with one side equal to null: he, nulli or hnull, fi."
"Then, the revised model takes the form:"
"P({he, fi}, a) = P G (`; p $ )P(a|{he, fi})YP M (he, fi) he,fi P M (he, fi) = p ø θ N (he, fi) + (1 − p ø )θ J (he, fi) ."
"In this definition, the distribution θ N gives non-zero weight only to unaligned phrases of the form he,nulli or hnull,fi, while θ J gives non-zero weight only to aligned phrase pairs."
"Our model involves observed sentence pairs, which in aggregate we can call x, latent phrase segmenta-tions and alignments, which we can call z, and pa-rameters θ J and θ N , which together we can call θ."
"A model such as ours could be used either for the learning of the key phrase pair parameters in θ, or to compute expected counts of phrase pairs in our data."
"These two uses are very closely related, but we focus on the computation of phrase pair expecta-tions."
"For exposition purposes, we describe a Gibbs sampling algorithm for computing expected counts of phrases under P(z|x,θ) for fixed θ."
"Such ex-pectations would be used, for example, to compute maximum likelihood estimates in the E-step of EM."
"In Section 4, we instead compute expectations under P (z|x), with θ marginalized out entirely."
"In a Gibbs sampler, we start with a complete phrase segmentation and alignment, state z 0 , which sets all latent variables to some initial configuration."
"We then produce a sequence of sample states z i , each of which differs from the last by some small local change."
"The samples z i are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later)."
"Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model."
Nor-malizing these expected counts yields estimates for the features φ(e|f) and φ(f|e).
Gibbs sampling is not new to the natural language processing community[REF_CITE].
"However, it is usually used as a search pro-cedure akin to simulated annealing, rather than for approximating expectations[REF_CITE]."
"Our application is also atypical for an NLP application in that we use an approxi-mate sampler not only to include Bayesian prior in-formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem[REF_CITE]."
"That is, we could not run EM exactly, even if we wanted maxi-mum likelihood estimates."
"Expected phrase pair counts under P(z|x, θ) have been approximated before in order to run EM."
"Subsequent work has relied heav-ily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps[REF_CITE]."
"None of these approximations are consistent, and they offer no method of measuring their biases."
"Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conve-niently (section 4)."
"Of course, sampling has liabili-ties as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.”"
"However, the basic sampling step they propose – resampling all segmentations and alignments for a sequence at once – requires a #P-hard computation."
"While this asymptotic com-plexity was apparently not prohibitive in the case of morphological alignment, where the sequences are short, it is prohibitive in phrase alignment, where the sentences are often very long."
Our Gibbs sampler repeatedly applies each of five operators to each position in each training sentence pair.
"Each operator freezes all of the current state z i except a small local region, determines all the ways that region can be reconfigured, and then chooses a (possibly) slightly different z i+1 from among those outcomes according to the conditional probability of each, given the frozen remainder of the state."
"This frozen region of the state is called a Markov blanket (denoted m), and plays a critical role in proving the correctness of the sampler."
"The first operator we consider is S WAP , which changes alignments but not segmentations."
"It freezes the set of phrases, then picks two English phrases e 1 and e 2 (or two foreign phrases, but we focus on the English case)."
"All alignments are frozen except the phrase pairs he 1 , f 1 i and he 2 , f 2 i. S WAP chooses be-tween keeping he 1 , f 1 i and he 2 , f 2 i aligned as they are (outcome o 0 ), or swapping their alignments to create he 1 , f 2 i and he 2 , f 1 i (outcome o 1 )."
"S WAP chooses stochastically in proportion to each outcome’s posterior probability: P (o 0 |m, x, θ) and P(o 1 |m, x, θ)."
"Each phrase pair in each out-come contributes to these posteriors the probability of adding a new pair, deciding whether it is null, and generating the phrase pair along with its contribu-tion to the distortion probability."
"This is all captured in a succinct potential function ψ(he, fi) = ( (1−p $ ) (1−p ø ) θ J (he, fi) δ(he, fi) e &amp; f non-null (1−p $ ) · p ø · θ N (he, fi) otherwise ."
"Thus, outcome o 0 is chosen with probability P (o 0 |m, x, θ) = ψ(he 1 , f 1 i)ψ(he 2 , f 2 i) . ψ(he 1 , f 1 i)ψ(he 2 , f 2 i) + ψ(he 1 , f 2 i)ψ(he 2 , f 1 i)"
Operators in a Gibbs sampler require certain con-ditions to guarantee the correctness of the sampler.
"First, they must choose among all possible configu-rations of the unfrozen local state."
"Second, imme-diately re-applying the operator from any outcome must yield the same set of outcome options as be-fore. [Footnote_4] If these conditions are not met, the sampler may no longer be guaranteed to yield consistent ap-proximations of the posterior distribution."
4 These are two sufficient conditions to guarantee that the Metropolis-Hastings acceptance ratio of the sampling step is 1.
"A subtle issue arises with S WAP as defined: should it also consider an outcome o 2 of he 1 , nulli and he 2 ,nulli that removes alignments?"
"No part of the frozen state is changed by removing these alignments, so the first Gibbs condition dictates that we must include o 2 ."
"However, after choosing o 2 , when we reapply the operator to positions e 1 and e 2 , we freeze all alignments except he 1 ,nulli and he 2 , nulli, which prevents us from returning to o 0 ."
"Thus, we fail to satisfy the second condition."
"This point is worth emphasizing because some prior work has treated Gibbs sampling as randomized search and, intentionally or otherwise, proposed inconsis-tent operators."
"Luckily, the problem is not with S WAP , but with our justification of it: we can salvage S WAP by aug-menting its Markov blanket."
"Given that we have se-lected he 1 , f 1 i and he 2 , f 2 i, we not only freeze all other alignments and phrase boundaries, but also the number of aligned phrase pairs."
"With this count held invariant, o 2 is not among the possible outcomes of S WAP given m. Moreover, regardless of the out-come chosen, S WAP can immediately be reapplied at the same location with the same set of outcomes."
All the possible starting configurations and out-come sets for S WAP appear in Figure 2(a).
"S WAP can arbitrarily shuffle alignments, but we need a second operator to change the actual phrase boundaries."
The F LIP operator changes the status of a single segmentation position [Footnote_5] to be either a phrase boundary or not.
5 A segmentation position is a position between two words that is also potentially a boundary between two phrases in an aligned sentence pair.
In this sense F LIP is a bilingual analog of the segmentation boundary flipping oper-ator[REF_CITE].
Figure 3 diagrams the operator and its Markov blanket.
"First, F LIP chooses any between-word po-sition in either sentence."
"The outcome sets for F LIP vary based on the current segmentation and adjacent alignments, and are depicted in Figure 2."
"Again, for F LIP to satisfy the Gibbs conditions, we must augment its Markov blanket to freeze not only all other segmentation points and alignments, but also the number of aligned phrase pairs."
"Oth-erwise, we end up allowing outcomes from which we cannot return to the original state by reapply-ing F LIP ."
"Consequently, when a position is already segmented and both adjacent phrases are currently aligned, F LIP cannot unsegment the point because it can’t create two aligned phrase pairs with the one larger phrase that results (see bottom of Figure 2(b))."
Both S WAP and F LIP freeze the number of align-ments in a sentence.
"The T OGGLE operator, on the other hand, can add or remove individual alignment links."
"In T OGGLE , we first choose an e 1 and f 1 ."
"If he 1 ,f 1 i ∈ a or both e 1 and f 1 are null, we freeze all segmentations and the rest of the alignments, and choose between including he 1 , f 1 i in the alignment or leaving both e 1 and f 1 unaligned."
"If only one of e 1 and f 1 are aligned, or they are not aligned to each other, then T OGGLE does nothing."
"Together, F LIP , S WAP and T OGGLE constitute a complete Gibbs sampler that consistently samples from the posterior P(z|x,θ)."
"Not only are these operators valid Gibbs steps, but they also can form a path of positive probability from any source state to any target state in the space of phrase alignments (formally, the induced Markov chain is irreducible)."
"Such a path can at worst be constructed by unalign-ing all phrases in the source state with T OGGLE , composing applications of F LIP to match the target phrase boundaries, then applying T OGGLE to match the target alignments."
We include two more local operators to speed up the rate at which the sampler explores the hypothesis space.
"In short, F LIP T WO simultaneously flips an English and a foreign segmentation point (to make a large phrase out of two smaller ones or vice versa), while M OVE shifts an aligned phrase boundary to the left or right."
We omit details for lack of space.
"With our sampling procedure in place, we can now estimate the expected number of times a given phrase pair occurs in our data, for fixed θ, using a Monte-Carlo average,"
"N 1 X count he,fi (x, z i ) −→ E count he,fi (x, ·) . a.s. N i=1"
The left hand side is simple to compute; we count aligned phrase pairs in each sample we generate.
"In practice, we only count phrase pairs after apply-ing every operator to every position in every sen-tence (one iteration). [Footnote_6] Appropriate normalizations of these expected counts can be used either in an M-step as maximum likelihood estimates, or to com-pute values for features φ(f|e) and φ(e|f)."
"6 For experiments, we ran the sampler for 100 iterations."
The Gibbs sampler we presented addresses the infer-ence challenges of learning phrase alignment mod-els.
"With slight modifications, it also enables us to include prior information into the model."
"In this sec-tion, we treat θ as a random variable and shape its prior distribution in order to correct the well-known degenerate behavior of the model."
The structure of our joint model penalizes explana-tions that use many small phrase pairs.
Each phrase pair token incurs the additional expense of genera-tion and distortion.
"In fact, the maximum likelihood estimate of the model puts mass on he, fi pairs that span entire sentences, explaining the training corpus with one phrase pair per sentence."
"Previous phrase alignment work has primarily mitigated this tendency by constraining the in-ference procedure, for example with word align-ments and linguistic features[REF_CITE], or by disallowing large phrase pairs using a non-compositional constraint[REF_CITE]."
"However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure."
"Model-based solutions appear in the literature as well, though typically combined with word align-ment constraints on inference."
"A sparse Dirichlet prior coupled with variational EM was explored[REF_CITE], but it did not avoid the degen-erate solution."
"We control this degenerate behavior by placing a Dirichlet process (DP) prior over θ J , the distribution over aligned phrase pairs[REF_CITE]."
"If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior."
"A draw from a K-dimensional Dirichlet distribution is a list of K real numbers in [0, 1] that sum to one, which can be in-terpreted as a distribution over K phrase pair types."
"However, since the event space of possible phrase pairs is in principle unbounded, we instead use a Dirichlet process."
"A draw from a DP is a countably infinite list of real numbers in [0, 1] that sum to one, which we interpret as a distribution over a countably infinite list of phrase pair types. [Footnote_7]"
"7 Technical note: to simplify exposition, we restrict the dis-cussion to settings such as ours where the base measure of the DP has countable support."
The Dirichlet distribution and the DP distribution have similar parameterizations.
"A K-dimensional Dirichlet can be parameterized with a concentration parameter α &gt; 0 and a base distribution M 0 = (µ 1 , . . . , µ K−1 ), with µ i ∈ (0, 1). [Footnote_8]"
"8 This parametrization is equivalent to the standard pseudo-counts parametrization of K positive real numbers. The bi-jection is given by α = P iK=1 α̃ i and µ i = α̃ i /α, where (α̃ 1 , . . . , α̃ K ) are the pseudo-counts."
"This parameteri-zation has an intuitive interpretation: under these pa-rameters, the average of independent samples from the Dirichlet will converge to M 0 ."
"That is, the aver-age of the ith element of the samples will converge to µ i ."
"Hence, the base distribution M 0 characterizes the sample mean."
The concentration parameter α only affects the variance of the draws.
"Similarly, we can parameterize the Dirichlet pro-cess with a concentration parameter α (that affects only the variance) and a base distribution M 0 that determines the mean of the samples."
"Just as in the finite Dirichlet case, M 0 is simply a probability dis-tribution, but now with countably infinite support: all possible phrase pairs in our case."
"In practice, we can use an unnormalized M 0 (a base measure) by appropriately rescaling α."
"In our model, we select a base measure that strongly prefers shorter phrases, encouraging the model to use large phrases only when it has suffi-cient evidence for them."
We continue the model:
P WA is the IBM model 1 likelihood of one phrase conditioned on the other[REF_CITE].
"P f and P e are uniform over types for each phrase length: the constants n f and n e denote the vocab-ulary size of the foreign and English languages, re-spectively, and P G is a geometric distribution."
"Above, θ J is drawn from a DP centered on the ge-ometric mean of two joint distributions over phrase pairs, each of which is composed of a monolingual unigram model and a lexical translation component."
This prior has two advantages.
"First, we pressure the model to use smaller phrases by increasing p s (p s = 0.8 in experiments)."
"Second, we encour-age good phrase pairs by incorporating IBM Model 1 distributions."
"This use of word alignment distri-butions is notably different from lexical weighting or word alignment constraints: we are supplying prior knowledge that phrases will generally follow word alignments, though with enough corpus evi-dence they need not (and often do not) do so in the posterior samples."
"The model proved largely insen-sitive to changes in the sparsity parameter α, which we set to 100 for experiments."
Introducing unaligned phrases invites further degen-erate megaphrase behavior: a sentence pair can be generated cheaply as two unaligned phrases that each span an entire sentence.
"We attempted to place a similar DP prior over θ N , but surprisingly, this modeling choice invoked yet another degenerate be-havior."
"The DP prior imposes a rich-get-richer prop-erty over the phrase pair distribution, strongly en-couraging the model to reuse existing pairs rather than generate new ones."
"As a result, common words consistently aligned to null, even while suit-able translations were present, simply because each null alignment reinforced the next."
"For instance, the was always unaligned."
"Instead, we fix θ N to a simple unigram model that is uniform over word types."
"This way, we discour-age unaligned phrases while focusing learning on θ J ."
"For simplicity, we reuse P f (f) and P e (e) from the prior over θ J . ( 1 · P e (e) if f = null θ N (he, fi) = 21 2 · P f (f) if e = null ."
"Our entire model now has the general form P(x, z, θ J ); all other model parameters have been fixed."
"Instead of searching for a suitable θ J , [Footnote_9] we sample from the posterior distribution P(z|x) with θ J marginalized out."
"9 For instance, using approximate MAP EM."
"To this end, we convert our Gibbs sampler into a collapsed[REF_CITE]using the Chinese Restaurant Process (CRP) representation of the DP[REF_CITE]."
"With the CRP, we avoid the prob-lem of explicitely representing samples from the DP."
"CRP-based samplers have served the commu-nity well in related language tasks, such as word seg-mentation and coreference resoluti[REF_CITE]."
"Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θ J ."
Let z m be the set of aligned phrase pair tokens ob-served in the rest of the corpus.
"Then, when he, fi is aligned (that is, neither e nor f are null), the condi-tional probability for a pair he, fi takes the form: count he,fi (z m ) + α · M 0 (he, fi) τ(he, fi|z m ) = , |z m | + α where count he,fi (z m ) is the number of times that he, fi appears in z m ."
We can write this expression thanks to the exchangeability of the model.
"For fur-ther exposition of this collapsed sampler posterior, 75 see[REF_CITE]. 11"
Figure 4 shows a histogram of phrase pair sizes in the distribution of expected counts under the model.
"As reference, we show the size distribution of both minimal and all phrase pairs extracted from word alignments using the standard heuristic."
"Our model tends to select minimal phrases, only using larger phrases when well motivated. [Footnote_12]"
12 The largest phrase pair found was 13 English words by 7 Spanish words.
This result alone is important: a model-based solution with no inference constraint has yielded a non-degenerate distribution over phrase lengths.
"Note that our sampler does find the degenerate solu-tion quickly under a uniform prior, confirming that the model, and not the inference procedure, is select-ing these small phrases."
"We also evaluate a hierarchical Dirichlet process (HDP) prior over θ J , which draws monolingual dis-tributions θ E and θ F from a DP and θ J from their cross-product: θ J ∼ DP(M 00 , α) 1 M 00 (he, fi) = [θ F (f)P WA (e|f) · θ E (e)P WA (f|e)] 2 θ F ∼ DP(P f , α 0 ) θ E ∼ DP(P e ,α 0 ) ."
"This prior 3+ x encourages 3+ novel phrase pairs to be com-posed of 2 x phrases 3, 3 x 2 that have been used before."
"In the sampler 2 , x we 2 approximate table counts for θ E and θ F with [Footnote_11] their xx 21, 2 expectations x 1 , which can be computed from phrase pair counts (see the appendix[REF_CITE]for details)."
"11 Note that the expression for τ changes slightly under con-ditions where two phrase pairs being changed simultaneously coincidentally share the same lexical content. Details of these fringe conditions have been omitted for space, but were in-cluded in our implementation."
The HDP prior gives a similar distribution over phrase sizes.
We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Ma-chine Translation Workshop shared task.
"We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the grow-diag-final-and combination heuristic[REF_CITE], which performed better than any alternative combination heuristic. [Footnote_13] The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment."
13 Sampling iteration time scales quadratically with sentence length. Short sentences were chosen to speed up our experiment cycle.
"We used a bidirectional lexicalized dis-tortion model that conditions on both foreign and English phrases, along with their orientations."
Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing.
"We report results with and without lexical weighting, denoted lex."
We tuned and tested on development corpora for the 2006 translation workshop.
The parameters for each phrase table were tuned separately using min-imum error rate training[REF_CITE].
"Results are scored with lowercased, tokenized NIST BLEU, and exact match METEOR[REF_CITE]."
"The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1."
"For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex."
We initialized the sampler with a configuration de-rived from the word alignments generated by the baseline.
We greedily constructed a phrase align-ment from the word alignment by identifying min-imal phrase pairs consistent with the word align-ment in each region of the sentence.
We then ran the sampler for 100 iterations through the training data.
"Each iteration required 12 minutes under the DP prior, and 30 minutes under the HDP prior."
Total running time for the HDP model neared two days on an eight-processor machine with 16 Gb of RAM.
"Estimating phrase counts under the DP prior de-creases[REF_CITE].8, or 29.1 under the HDP prior."
This gap is not surprising: heuristic extraction dis-covers many more phrase pairs than sampling.
Note that sacrificing only 0.7 BLEU while shrinking the phrase table by 92% is an appealing trade-off in resource-constrained settings.
The estimates DP-composed and HDP-composed in Table 1 take expectations of a more liberal count function.
"While sampling, we count not only aligned phrase pairs, but also larger ones composed of two or more contiguous aligned pairs."
"This count function is similar to the phrase pair extraction heuristic, but never includes unaligned phrases in any way."
"Expec-tations of these composite phrases still have a proba-bilistic interpretation, but they are not the structures we are directly modeling."
"Notably, these estimates outperform the baseline by 0.3 BLEU without ever extracting phrases from word alignments, and per-formance increases despite a reduction in table size."
We can instead increase coverage by smooth-ing the learned estimates with the heuristic counts.
"The estimates DP-smooth and HDP-smooth add counts extracted from word alignments to the sam-pler’s running totals, which improves performance by 0.4 BLEU over the baseline."
This smoothing bal-ances the lower-bias sampler counts with the lower-variance heuristics ones.
"Our novel Gibbs sampler and nonparametric pri-ors together address two open problems in learn-ing phrase alignment models, approximating infer-ence consistently and efficiently while avoiding de-generate solutions."
"While improvements are mod-est relative to the highly developed word-alignment-centered baseline, we show for the first time com-petitive results from a system that uses word align-ments only for model initialization and smoothing, rather than inference and estimation."
"We view this milestone as critical to eventually developing a clean probabilistic approach to machine translation that unifies model structure across both estimation and decoding, and decreases the use of heuristics."
"In recent years, with the development of Chi-nese semantically annotated corpus, such as Chinese Proposition Bank and Normalization Bank, the Chinese semantic role labeling (SRL) task has been boosted."
"Similar to Eng-lish, the Chinese SRL can be divided into two tasks: semantic role identification (SRI) and classification (SRC)."
Many features were in-troduced into these tasks and promising re-sults were achieved.
"In this paper, we mainly focus on the second task: SRC."
"After exploit-ing the linguistic discrepancy between num-bered arguments and ARGMs, we built a se-mantic role classifier based on a hierarchical feature selection strategy."
"Different from the previous SRC systems, we divided SRC into three sub tasks in sequence and trained models for each sub task."
"Under the hierarchical ar-chitecture, each argument should first be de-termined whether it is a numbered argument or an ARGM, and then be classified into fine-gained categories."
"Finally, we integrated the idea of exploiting argument interdependence into our system and further improved the per-formance."
"With the novel method, the classi-fication precision of our system is 94.68%, which outperforms the strong baseline signifi-cantly."
It is also the state-of-the-art on Chi-nese SRC.
Semantic Role labeling (SRL) was first defined[REF_CITE].
The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.
The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.
"Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc."
"Since the arguments can provide useful semantic information, the SRL is crucial to many natural language proc-essing tasks, such as Question and Answering[REF_CITE], Information Extrac-ti[REF_CITE], and Machine Transla-ti[REF_CITE]."
"With the efforts of many re-searchers ([REF_CITE]), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."
"Compared to the research on English, the re-search on Chinese SRL is still in its infancy stage."
"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such[REF_CITE],[REF_CITE]and[REF_CITE]."
"They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chi-nese sentences, and used Support Vector Machines to identify and classify the arguments."
This paper made the first attempt on Chinese SRL and pro-duced promising results.
"After the PropBank[REF_CITE]was built,[REF_CITE]and[REF_CITE]have produced more complete and systematic research on Chinese SRL."
"However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierar-chical system is to simplify the classification proc-ess to make it less time consuming."
"So the hierar-chical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient."
"They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification."
"For semantic analysis, developing features that capture the right kind of information is crucial."
Experiments on Chinese SRL ([REF_CITE]) reassured these findings.
"In this paper, we mainly focus on the semantic role classification (SRC) process."
"With the find-ings about the linguistic discrepancy of different semantic role groups, we try to build a 2-step se-mantic role classifier with hierarchical feature se-lection strategy."
"That means, for different sub tasks, different models will be trained with different fea-"
The purpose of this strategy is to capture the right kind of information of different semantic role groups.
"It is hard to do manual selection of features since there are too many feature templates which has been proven to be useful in SRC; so, we de-signed a simple feature selection algorithm to se-lect useful features automatically from a large set of feature templates."
"With this hierarchical feature selection architecture, our system can outperform previous systems."
The selected feature templates for each process of SRC can in turn reassure the existence of the linguistic discrepancy.
"At last, we also integrate the idea of exploiting argument in-terdependence to make our system perform better."
The rest of the paper is organized as follows.
"In section 2, the semantically annotated corpus - Chi-nese Propbank is discussed."
The architecture of our method is described in section 3.
The feature selec-tion strategy is discussed in section 4.
The settings of experiments can be found in section 5.
"The re-sults of the experiments can be found in section 6, where we will try to make some linguistic explana-tions of the selected features."
Section 7 is conclu-sions and future work.
The Chinese PropBank has labeled the predicate-argument structures of sentences from the Chinese
It is constituted of two parts.
"One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank."
The other is a dictionary which lists the frames of all the labeled predicates.
Figure 1 is an example from the PropBank [Footnote_1] .
"1 This sentence is extracted from chtb_082.fid of Chinese PropBank 1.0, and we made some simplifications on it."
We put the word-by-word translation and the translation of the whole sentence below the example.
"It is quite a complex sentence, as there are many semantic roles in it."
"In this sentence, all the seman-tic roles of the verb 提供 (provide) are presented in the syntactic tree."
We can separate the semantic roles into two groups.
"The first group of semantic roles can be called the core arguments, which capture the core rela-tions."
"In this sentence, there are three arguments of verb 提供 (provide) in this sentence. 保险公司 (the insurance company) is labeled as ARG0, which is the proto-agent of the verb."
"Specifically to the verb 提供 (provide), it is the provider. 保险服 务 (insurance services) is the direct object of the verb, and it is the proto-patient, which is labeled as ARG1."
"Specifically to the verb 提供 (provide), it represents things provided. 为三峡工程 (for the Sanxia Project) is another kind of argument, which is labeled as ARG1, and it represents the receiver."
The other group of semantic roles is called ad-juncts.
They are always used to reveal the periph-eral information.
"There are two adjuncts of the tar-get verb in this sentence: 截止目前 (until recently) and 已 (has), both of which are labeled as ARGM."
"However, the two ARGMs reveal information of different aspects."
"Besides the ARGM tags, the sec-ondary tags “TMP” and “ADV” are assigned to the two semantic roles respectively. “TMP” indicates that 截止目前 (until recently) is a modifier repre-senting the temporal information, and “ADV” in-dicates that 已 (has) is an adverbial modifier."
"In the Chinese PropBank, the difference of the two groups is obvious."
"The core arguments are all labeled with numbers, and they are also called the numbered arguments."
The numbers range from 0 to 4 in Chinese PropBank.
The adjuncts are labeled with “ARGM”.
"In this section, we will discuss the linguistic fun-daments of the construction of a hierarchical se- mantic role classifier."
We use “hierarchical” to dis-tinguish our classifier from the previous “flat” ones.
The purpose of the SRC task is to assign a tag to all the semantic roles which have been identified.
"The tags include ARG0-4, and 17 kinds of ARGMs (with functional tags)."
"Previous SRC sys-tems treat all the tags equally, and view the SRC as a multi-category classification task."
"However, we have different opinions of the traditional architec-ture."
"Due to the discussions in section 2, we noticed that the semantic roles can be divided into two groups naturally according to the different kinds of semantic information represented by them."
Here we will make some linguistic analysis of the two semantic role groups.
"Conversely to the process of the syntactic realization of semantic roles, we want to find out what linguistic features make a con-stituent ARG0 instead of ARG1, or another con-stituent ARGM-TMP instead of ARGM-ADV, i.e. what features capture the most crucial information of the two groups."
"As what we have assumed, the linguistic fea-tures which made a syntactic constituent labeled as either one of the core arguments or one of the ad-juncts varies greatly."
"Take the sentence in section 2 as an example, even if the only information we have about the phrase 截止目前 (until now) is that it is an adjunct of the verb, we can almost confirm, no matter where this node takes place in the pars-ing tree, this constituent will be labeled as ARGM-TMP. 已 (has) is also the same."
"According to its meaning, the only category can be assigned to it is ARGM-ADV."
"But, things are quite different to the core argument."
"In the same sentence, 保险公司 (the insurance company) is a good example."
"If we limit our observation to the phrase itself, we can hardly assert that it is the ARG0 of the target verb."
"Only when we extend our observation to the syn-tactic structure level, find out it is the subject of this sentence, and the voice of the sentence is ac-tive, the semantic type of 保险公司 (the insurance company) is finally confirmed."
"If we have another sentence in which 保险公司 (the insurance com-pany) is not the subject, but rather the object, and the target verb is 开办 (set up), then it will proba-bly be labeled as ARG1."
"Due to the analysis above, we can conclude the linguistic discrepancy of the two semantic role groups as follows."
Core arguments and adjuncts share different kinds of inner linguistic consistency respectively.
"For the core arguments, the specific type cannot be determined with the information of the arguments only."
"At this level, the core argu-ments are dependent on other information except the information about themselves."
"For example, the information of syntactic structures is crucial to the determination of the types of core arguments, and trivial differences of the syntactic structures will lead to the different output."
"Because of this, we can say that the core arguments are sensitive to the syntactic structures."
"Compared to the core argu-ments, adjuncts are the opposite."
"They are rela-tively independent on other information, since most of the adjuncts can be easily classified just based on the information about themselves [Footnote_2] ."
"2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of self-descriptive features, e.g. phrase type, are limited."
"And although the positions of the adjuncts in the syntac-tic structure can vary, the types of the adjuncts are fixed."
"In this sense, the adjuncts are insensitive to the syntactic structures."
"After we made the linguistic discrepancy of the two semantic role groups, we can make a bold as-sumption that the differences of the two groups can be reflected in the capability of different kinds of features to capture the crucial information for the two groups."
"For example, the “voice” features seems to be crucial to the core arguments but use-less to the adjuncts."
This assumption provided us with the idea of a hierarchical feature selection sys-tem.
"In this system, we first classify the constituents into two classes: core arguments and adjuncts."
"And then, the system classifies core arguments and ad-juncts separately."
For different subtasks we only select the most useful features and discard the less pertinent ones.
We hope to take utilization of the most crucial features to improve semantic role classification.
Previous semantic role classifiers always did the classification problem in one-step.
"However, in this paper, we did SRC in two steps."
"The architec-tures of hierarchical semantic role classifiers can be found in figure 2, which is similar with that[REF_CITE]."
"As what has been shown in figure 2, a semantic role will first be determined whether it is a num-bered argument or an ARGM by a binary-category classifier."
"And, then if the semantic role is a num-bered argument, it will be determined by a 5-category classifier designed for ARGX, i.e. the numbered arguments."
"If it is an ARGM, the func-tional tag will be assigned by a 17-category classi-fier built for ARGMs."
"Accordingly, with this hier-archical architecture, the SRC problem is divided into 3 sub tasks, each of which has an independent classifier."
It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.
Se-mantic context features indicates the features ex-tracted from the arguments around the current one.
We can use window size to represent the scope of the context.
"Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu-ments will be utilized for the classification of cur-rent semantic role."
"There are two kinds of argu-ment sequences[REF_CITE], and we only test the linear sequence."
Take the sentence in fig-ure 1 as an example.
"The linear sequence of the arguments in this sentence is: 截止目前(until then), 保险公司 (the insurance company), 已 (has), 为 三峡工程 (for the Sanxia Project), 保险服务 (in-surance services)."
"For the argument 已 (has), if the semantic context window size is [-1,2], the seman-tic context features e.g. headword, phrase type and etc. of 保险公司 (the insurance company), 为三 峡工程 (for the Sanxia Project) and 保险服务 (insurance services) will be utilized to serve the classification task of 已 (has)."
"While their paper has improved the SRC per-formance on English, it also has one potential dis-advantage, which is that they didn’t separate the core arguments and ARGMs."
The influence and explanations of this defect are presented in Section 6.
"But in our hierarchical system, this problem can be solved."
"Since in the first step, we have separated the numbered arguments and ARGMs."
"We suppose that with the separation of the two semantic role groups, the system performance will be further im-proved."
"Due to what we have discussed in the section 3.1, we need to select different features for the three sub task of SRC."
"In this paper, we did not make the selection manually; however, we make a simple greedy strategy for feature selection to do it auto-matically."
"Although the best solution may not be found, automatic selection of features can try far more combinations of feature templates than man-ual selection."
"Because of this, this strategy possibly can produce a better local optional solution."
"First, we built a pool of feature templates which has proven to be useful on the SRC."
"Most of the feature templates are standard, so only the new ones will be explained."
The candidate feature tem-plates include: Voice[REF_CITE].
"Head word POS, Head Word of Prepositional Phrases, Constituent tree distance, from Pradhan etc. (2004)."
"Position, subcat frame, phrase type, first word, last word, subcat frame+, predicate, path, head word and its POS, predicate + head word, predi-cate + phrase type, path to BA and BEI, verb class [Footnote_3] , verb class + head word, verb class + phrase type,[REF_CITE]. predicate POS, first word + last word, phrase type of the sibling to the left, phrase type of the sibling to the right, verb + subcate frame+, verb POS + subcat frame+, the amount of VPs in path, phrase type + phrase type of parent node, which can be easily understood by name. voice position, indicates whether the voice marker (BA, BEI) is before or after the constituent in focus. subcat frame*, the rule that expands the parent node of the constituent in focus. subcat frame@, the rule that expands the con-stituent in focus. layer of the constituent in focus, the number of constituents in the ascending part of the path sub-tracted by the number of those in the descending part of path, e.g. if the path is PP-BNF↑VP↓VP ↓VV, the feature extracted by this template will be -1."
"3 It is a bit different[REF_CITE], since we didn’t use the syntactic alternation information."
"SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word."
"The semantic categories of verbs and other words are extracted from the Se-mantic Knowledge-base of Contemporary Chinese[REF_CITE]. verb AllFrameSets, the combination of all the framesets of a predicate. verb class + verb AllFrameSets, verb AllFra-meSets + head word, verb AllFrameSets + phrase type."
"There are more than 40 feature templates, and it is quite difficult to traverse all the possible combi-nations and get the best one."
So we use a greedy algorithm to get an approximate optimal solution.
The feature selection algorithm is as follows.
Each time we choose one of the feature templates and add it into the system.
"The one, after which is added, the performance is the highest, will be cho-sen."
Then we continue to choose feature templates until there are no one left.
"In the end, there are a series of feature sets, which recorded the process of feature selection."
Then we choose the feature set which can perform the best on development set.
The code of feature selection algorithm is designed in Figure 3.
"To make a comparison, we also built a tradi-tional 1-step semantic role classifier based on this feature selection strategy."
We will take this classi-fier as the baseline system.
"In our SRL system, we use a Maximum Entropy toolkit with tunable Gaussian Prior and L-BFGS parameter estimation, which is implemented by Zhang Le."
This toolkit is available[URL_CITE]
It can well handle the multi-category classification problem and it is quite efficient.
We use Chinese PropBank 1.0 (LDC number:[REF_CITE]) in our experiments.
"PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1."
"For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set."
"The development set includes 40 files, from chtb_041.fid to chtb_080.fid."
"The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931."
"We use the same data setting[REF_CITE], however a bit different[REF_CITE]."
The results of the feature selection are presented in table1.
"In this table, “Baseline” indicates the 1-step architecture, and “Hierarchical” indicates the “hi-erarchical feature selection architecture” imple-mented in this paper. “X_M”, “ARGX” and “ARGM” indicate the three sub-procedures of the hierarchical architecture, which are “ARGX and ARGM separation”, “ARGX classification”, “ARGM classification” respectively. “Y” in the table indicates that the feature template has been selected for the sub task."
"According to table 1, we can find some interest-ing facts, which in turn prove what we found about semantic role groups in section 3.1."
"In table 1, feature templates related to the syn-tactic structure includes: voice-related group (voice, voice information, path to BA and BEI), frame-related group (verb class, verb class + head word, verb class + phrase type, all frames of verb, verb class + all frames of verb), the layer of argument, position and 4 kinds of subcat frames."
"As we as-sumed before, these features are crucial to core arguments but of little use to adjuncts."
The results have proven this assumption.
"Of the entire 14 syn-tactic structure-related feature templates, 8 were selected by the ARGX process but only 2 was se-lected by the ARGM process."
"The two exceptions should be viewed as the result of random impact, which cannot be avoided in automatic feature se-lection."
"Compared with the different features selected by these tasks, we can find other interesting results."
"Few of the features selected by the X_M process also have related with the verb or the syntactic structures, which is quite similar with the ARGM process."
"This is probably because most of ARGMs are easy to be identified without syntactic structure information, which makes the opposite of ARGMs, i.e. the ARGXs easy to be filtered."
"Besides, the features selected by the baseline system have much in common with those selected by the ARGX process."
"This can be explained by the fact that both in the development and test set, the amount of core arguments outperforms that of adjuncts."
"The pro-portions between core arguments and adjuncts are 1.79:1 on the development set, and 1.63:1 on the test set."
"Because of the bias, the baseline system will tend to choose more syntactic structure-related features to label core arguments precisely."
"With this new architecture, we have achieved improvement on the performance of the semantic role classification, which can be found in table 2."
Our classifier performs better both on the devel-opment and the test set.
"The labeled precision on the development set is from 95.15% to 95.94%, and the test set is from 93.38% to 94.31%, with an ERR (error reduction rate) of 14.05%."
Both of the improvements are statistically significant (χ2 test with p= 0.05).
"The errors of SRC have three ori-gins, which are correspondent to the three sub tasks of the hierarchical architecture."
"Detailed in-formation of the comparison between the two sys-tems can be found in table 3, which can tell us where the improvements come from."
"In table 3, the percentages are calculated the way that the number of the errors was divided by the number of the arguments in the test set."
"ARGX/ARGM errors represent the errors that the semantic roles are classified into wrong group, e.g. ARGXs are labeled as ARGMs and vice versa."
"The inner errors represent the errors in a group, e.g. ARG0 are labeled as ARG1."
"From this table, we can find that ARGX is the most difficult task."
X-M and ARGM are less challenging.
"Besides the rela-tively little error reduction in the ARGM process, the greatest part of improvement comes from the process of the most difficult sub task: the ARGX sub task."
It is a bit surprising that the first step of the X_M in the hierarchical system process did not perform better than that in the baseline system.
Table 4 presented the labeled precision of each type of semantic role.
"It demonstrates that with respect to ARGMs and ARGXs, the hierarchical system outperforms the baseline system."
"Further-more, the improvement on ARGXs is greater than that of ARGMs."
"All types of numbered arguments get improvement in the hierarchical architecture, especially ARG1, ARG[Footnote_4] and ARG3."
"4 From the name, TBERR possibly indicates the labeled errors in Chinese PropBank. However, we did not find any explana-tions, so we just put it here and group it to ARGM."
"Although the performances of some types of the ARGMs de-creased, the performances of all types of the ARGMs which occurs more than 100 times in-creased, including ADV (adverbials), LOC (loca-tives), MNR (manner markers) and TMP (temporal markers)."
"After the hierarchical system was built, we tried to integrate the idea of exploiting argument inter-dependence into our system."
"We extract the seman-tic context features in a linear order, with the win-dow size from [0,0] to [-3,3]."
Larger window sizes are of little value since too few arguments have more than 6 other arguments in context.
"The re-sults are presented in table 5. “Base” stands for the hierarchical system built above, without semantic context features. “+window selection” indicates the new system which has utilized the idea of exploiting argument interdependence."
"The best window sizes for the baseline system, ARGX and ARGM processes in the hierarchical system are [0,0], [-1,1], [0,0] re-spectively, which were determined by testing on the development set."
"We can find that only for the ARGX process, the semantic context features are useful."
"For the baseline system and the ARGM process, exploiting argument interdependence does not help improve the performance."
"This conclusion is different[REF_CITE], but it can be explained in the following way."
"In fact, the interdependence only exists among core arguments."
"For ARGMs, it is a different thing."
An ARGM cannot provide any information about the type of the arguments close to it and the seman-tic context features does not help the classification of ARGMs.
"Also, take the sentence in section 2 as an example, the fact that 截止目前 (until now) is ARGM-TMP cannot raise the probability that 保险 公司 (the insurance company) is ARG0 or 已 (has) is ARGM-ADV and vice versa."
"However, if we know that 保险公司 (the insurance company) is ARG0, at least the phrase 保险服务 (insurance services) can never be ARG0."
"The semantic con-text features extracted from or for ARGMs will do harm to the improvement of the system, since they are irrelative information."
"Because of the same rea-son, the performance of base system also decreased when semantic context features were extracted, since the core arguments and the ARGMs are mixed together in the baseline system."
"But for the ARGX sub task of our hierarchical system, since we have separated the numbered ar-guments and ARGMs first, the influences of ARGMs can be eliminated."
This made the interde-pendence of core arguments can be directly ex-plored from the extraction of semantic context fea-tures.
So the ARGX sub task is improved.
"To prove that our method is effective, we also make a comparison between the performances of our system and[REF_CITE],[REF_CITE]."
The results are presented in Table 6.
X &amp; P (2005)[REF_CITE]
Comparison with previous systems
We have to point out that all the three systems are based on Gold standard parsing.
"From the table 6, we can find that our system is better than both of the related systems."
Our system has outperformed[REF_CITE]with a relative error reduction rate of 9.8%.
"In this paper, we have divided all the semantic roles into two groups according to their semantic relations with the verb."
"After the grouping of the semantic roles was made, we designed a hierarchi-cal semantic role classifier."
"To capture the accurate information of different semantic role groups, we designed a simple feature selection algorithm to calibrate features for each sub task of SRC."
It was very encouraging that the hierarchical SRC system outperformed the strong baseline built with tradi-tional methods.
"And the selected features could be explained, which in turn proves that the linguistic discrepancy of semantic role groups not only exists but also can be captured."
Then we integrated the idea of exploiting argument interdependence to further improve the performance of our system and explained linguistically why the results of our sys-tem were different from the ones in previous re-search.
"Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained."
"However, the impact of this idea is limited due to that the amount of the research tar-get, ARG2, is few in PropBank."
What if we could extend the idea of hierarchical architecture to the single semantic role level?
Would that help the improvement of SRC?
This paper describes a novel Bayesian ap-proach to unsupervised topic segmentation.
Unsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution.
We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial lan-guage model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmenta-tion.
"This contrasts with previous approaches, which relied on hand-crafted cohesion met-rics."
"The Bayesian framework provides a prin-cipled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previ-ously used in unsupervised segmentation sys-tems."
Our model yields consistent improve-ments over an array of state-of-the-art systems on both text and speech datasets.
We also show that both an entropy-based analysis and a well-known previous technique can be de-rived as special cases of the Bayesian frame-work. [Footnote_1]
1 Code and materials for this work are available[URL_CITE]
"Topic segmentation is one of the fundamental prob-lems in discourse analysis, where the task is to divide a text into a linear sequence of topically-coherent segments."
"Hearst’s T EXT T[REF_CITE]introduced the idea that unsupervised segmentation can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distri-butions within each topic segment."
"Lexical cohesion has provided the inspiration for several successful systems (e.g.,[REF_CITE]), and is cur-rently the dominant approach to unsupervised topic segmentation."
"But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems."
"For example, consider cue phrases, which are ex-plicit discourse markers such as “now” or “how-ever”[REF_CITE]."
"Cue phrases have been shown to be a useful feature for supervised topic segmentati[REF_CITE], but cannot be incorporated by current unsupervised models."
"One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohe-sion, such as weighted cosine similarity[REF_CITE]."
"Without su-pervision, it is not possible to combine such met-rics with additional sources of information."
"More-over, such hand-crafted metrics may not general-ize well across multiple datasets, and often include parameters which must be tuned on development sets[REF_CITE]."
"In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of in-formation to be incorporated without the need for labeled data."
We formalize lexical cohesion in a generative model in which the text for each seg- ment is produced by a distinct lexical distribution.
Lexically-consistent segments are favored by this model because probability mass is conserved for a narrow subset of words.
"Thus, lexical cohesion arises naturally through the generative process, and other sources of information – such as cue words – can easily be incorporated as emissions from the segment boundaries."
"More formally, we treat the words in each sen-tence as draws from a language model associated with the topic segment."
This is related to topic-modeling methods such as latent Dirichlet allocation (LDA;
This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation.
"We con-sider two approaches to handling the language mod-els: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial dis-tribution (also known as the multivariate Polya dis-tribution)."
We model cue phrases as generated from a sep-arate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases.
"The addition of cue phrases renders our dynamic programming-based inference inapplicable, so we design a sampling-based inference technique."
"This algorithm can learn in a completely unsupervised fashion, but it also provides a principled mechanism to improve search through the addition of declara-tive linguistic knowledge."
"This is achieved by bias-ing the selection of samples towards boundaries with known cue phrases; this does not change the under-lying probabilistic model, but guides search in the direction of linguistically-plausible segmentations."
"We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset[REF_CITE]and a new tex-tual corpus constructed from the contents of a med-ical textbook."
In both cases our model achieves per-formance surpassing multiple state-of-the-art base-lines.
"Moreover, we demonstrate that the addition of cue phrases can further improve segmentation per-formance over cohesion-based methods."
"In addition to the practical advantages demon-strated by these experimental results, our model re-veals interesting theoretical properties."
"Other re- searchers have observed relationships between dis-course structure and entropy (e.g.,[REF_CITE])."
"We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment."
This finding demonstrates that a re-lationship between discourse segmentation and en-tropy is a natural consequence of modeling topic structure in a generative Bayesian framework.
"In addition, we show that the benchmark segmentation system[REF_CITE]can be viewed as another special case of our Bayesian model."
Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique.
"In their unsu-pervised model, inference is performed by selecting segmentation points at the local maxima of the cohe-sion function."
"Most similar to our work is the approach[REF_CITE], who search for segmentations with compact language models; as shown in Sec-tion 3.1.1, this can be viewed as a special case of our model."
Both of these last two systems use dynamic programming to search the space of segmentations.
An alternative Bayesian approach to segmentation was proposed[REF_CITE].
They assume a set of documents that is characterized by some num-ber of hidden topics that are shared across multiple documents.
They then build a linear segmentation by adding a switching variable to indicate whether the topic distribution for each sentence is identical to that of its predecessor.
"Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually."
"Addi-tionally, the inference procedure of Purver et al. re-quires sampling multiple layers of hidden variables."
"In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points."
"The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see[REF_CITE]."
"More recently, cue phrases have been applied to topic segmentation in the supervised setting."
"In a supervised system that is distinct from the unsupervised model described above,[REF_CITE]automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation."
"Unlike these ap-proaches, we identify candidate cue phrases auto-matically from unlabeled data and incorporate them in the topic segmentation task without supervision."
The core idea of lexical cohesion is that topically-coherent segments demonstrate compact and con-sistent lexical distributions[REF_CITE].
Lexical cohesion can be placed in a prob-abilistic context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment.
"Formally, if sen-tence t is in segment j, then the bag of words x t is drawn from the multinomial language model θ j ."
"This is similar in spirit to hidden topic models such as latent Dirichlet allocati[REF_CITE], but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmenta-tion of the document."
"We will assume that topic breaks occur at sen-tence boundaries, and write z t to indicate the topic assignment for sentence t."
"The observation likeli-hood is,"
"T p(X|z, Θ) ="
"Y p(x t |θ z t ), (1) t where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and Θ is the set of all K language models. [Footnote_2] A linear segmen-tation is ensured by the additional constraint that z t must be equal to either z t−1 (the previous sentence’s segment) or z t−1 + 1 (the next segment)."
"2 Our experiments will assume that the number of topics K is known. This is common practice for this task, as the desired number of segments may be determined by the user[REF_CITE]."
"To obtain a high likelihood, the language mod-els associated with each segment should concentrate their probability mass on a compact subset of words."
Language models that spread their probability mass over a broad set of words will induce a lower likeli-hood.
This is consistent with the principle of lexical cohesion.
"Thus far, we have described a segmentation in terms of two parameters: the segment indices z, and the set of language models Θ."
"For the task of seg-menting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well."
"We consider two alternatives: taking point estimates of the language models (Section 3.1), and analytically marginalizing them out (Section 3.2)."
One way to handle the language models is to choose a single point estimate for each set of segmenta-tion points z.
Suppose that each language model is drawn from a symmetric Dirichlet prior: θ j ∼ Dir(θ 0 ).
"Let n j be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: n j,i ="
"P {t:z t =j} m t,i , where m t,i is the count of word i in sentence t."
"Assuming that each x t ∼ θ j , then the posterior distribution for θ j is Dirichlet with vector parameter n j +θ 0[REF_CITE]."
"The expected value of this distri-bution is the multinomial distribution θ̂ j , where, n j,i + θ 0 θ̂ j,i = (2)."
"P W n j,i + W θ 0i"
"In this equation, W indicates the number of words in the vocabulary."
"Having obtained an estimate for the language model θ̂ j , the observed data likelihood for segment j is a product over each sentence in the segment, = Y θ̂ jn,i j,i . (5) i"
"By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting con-nections with prior work on segmentation and infor-mation theory."
"In this section, we explain how our model gen-eralizes the well-known method of Utiyama and Isahara (2001; hereafter U&amp;I)."
"As in our work, Utiyama and Isahara propose a probabilistic frame-work based on maximizing the compactness of the language models induced for each segment."
Their likelihood equation is identical to our equations 3-5.
"They then define the language models for each seg-ment as θ̂ j,i = n j,i +1 , without rigorous justifi-"
"W+P Wi n j,i cation."
"This form is equivalent to Laplacian smooth-ing[REF_CITE], and is a special case of our equation 2, with θ 0 = 1."
"Thus, the lan-guage models in U&amp;I can be viewed as the expec-tation of the posterior distribution p(θ j |{x t : z t = j},θ 0 ), in the special case that θ 0 = 1."
Our ap-proach generalizes U&amp;I and provides a Bayesian justification for the language models that they ap-ply.
"The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases."
We empirically demonstrate that these extensions substantially improve perfor-mance.
"Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework."
Equation 1 defines the objec-tive function as a product across sentences; using equations 3-5 we can decompose this across seg-ments instead.
"Working in logarithms, = X X log p(x t |θ̂ j ) j {t:z t =j} K W = X X n j,i log θ̂ j,i (6) j i"
The last line substitutes in the logarithm of equa-tion 5.
"Setting θ 0 = 0 and rearranging equation 2, we obtain n j,i = N j θ̂ j,i , with N j = P Wi n j,i , the total number of words in segment j. Substituting this into equation 6, we obtain"
"K log p(X|z, Θ̂) = X N j X θ̂ j,i log θ̂ j,i j i K = X N j H(θ̂ j ), j where H(θ̂ j ) is the negative entropy of the multino-mial θ̂ j ."
"Thus, with θ 0 = 0, the log conditional prob-ability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths."
"This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g.,[REF_CITE])."
The previous subsection uses point estimates of the language models to reveal connections to en-tropy and prior work on segmentation.
"However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible lan- guage models:"
"K p(X|z, θ 0 ) ="
"Y Y p(x t |θ 0 ) j {t:z t =j} K Z = Y dθ j Y p(x t |θ j )p(θ j |θ 0 ) j {t:z t =j} K = Y p dcm ({x t : z t = j}|θ 0 ), (7) j where p dcm refers to the Dirichlet compound multi-nomial distribution (DCM), also known as the multi-variate Polya distributi[REF_CITE]."
"The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior θ 0 ."
"When θ 0 is a symmetric Dirichlet prior, p dcm ({x t : z t = j}|θ 0 )"
"W Y Γ(n j,i + W θ 0 ), Γ(W θ 0 ) ="
Γ(N j + W θ 0 )
"Γ(θ 0 ) i where n j,i is the count of word i in segment j, and N j = P Wi n j,i , the total number of words in the segment."
"The symbol Γ refers to the Gamma func-tion, an extension of the factorial function to real numbers."
"Using the DCM distribution, we can com-pute the data likelihood for each segment from the lexical counts over the entire segment."
The overall observation likelihood is a product across the likeli-hoods for each segment.
"The optimal segmentation maximizes the joint prob-ability, p(X, z|θ 0 ) = p(X|z, θ 0 )p(z)."
"We assume that p(z) is a uniform distribution over valid segmentations, and assigns no probability mass to invalid segmentations."
The data likelihood is defined for point estimate language models in equation 5 and for marginalized language models in equation 7.
Note that equation 7 is written as a product over segments.
"The point estimates for the language models depend only on the counts within each segment, so the overall likelihood for the point-estimate version also decomposes across segments."
Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming.
We define B(t) as the value of the objective function for the optimal seg-mentation up to sentence t.
"The contribution to the objective function from a single segment between sentences t 0 and t is written, b(t 0 , t) = p({x t 0 . . . x t }|z t 0 ...t = j)"
"The maximum value of the objective function is then given by the recurrence relation, B(t) = max t 0 &lt;t B(t 0 )b(t 0 +1, t), with the base case B(0) = 1."
"These values can be stored in a table of size T (equal to the number of sentences); this admits a dy-namic program that performs inference in polyno-mial time. [Footnote_3] If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size T K."
"3 This assumes that the objective function for individual seg-ments can also be computed efficiently. In our case, we need only keep vectors of counts for each segment, and evaluate probability density functions over the counts."
"The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ 0 ."
We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization frame-work[REF_CITE].
"In the E-step, we estimate a segmentation ẑ of the dataset, as de-scribed in Section 3.3."
"In the M-step, we maxi-mize p(θ 0 |X,ẑ) ∝ p(X|θ 0 ,ẑ)p(θ 0 )."
"Assuming a non-informative hyperprior p(θ 0 ), we maximize the likelihood in Equation 7 across all documents."
The maximization is performed using a gradient-based search; the gradients are dervied[REF_CITE].
This procedure is iterated until convergence or a maximum of twenty iterations.
"One of the key advantages of a Bayesian framework for topic segmentation is that it permits the prin-cipled combination of multiple data sources, even without labeled data."
"We are especially interested in cue phrases, which are explicit markers for dis-course structure, such as “now” or “first”[REF_CITE]."
"Cue phrases have previously been used in supervised topic segmentation (e.g.,[REF_CITE]); we show how they can be used in an unsupervised setting."
The previous section modeled lexical cohesion by treating the bag of words in each sentence as a se-ries of draws from a multinomial language model indexed by the topic segment.
"To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure."
"This idea is imple-mented by drawing the text at each topic boundary from a special language model φ, which is shared across all topics and all documents in the dataset."
"For sentences that are not at segment bound-aries, the likelihood is as before: p(x t |z,Θ,φ) ="
"Q i∈x t θ z t ,i ."
"For sentences that immediately follow segment boundaries, we draw the first ` words from φ instead."
"Writing x (t`) for the ` cue words in x t , and x̃ t for the remaining words, the likelihood for a segment-initial sentence is, p(x t |z t =6 z t−1 , Θ, φ) ="
"Y φ i Y θ z t ,i . i∈x (t`) i∈x̃ t"
We draw φ from a symmetric Dirichlet prior φ 0 .
"Fol-lowing prior work[REF_CITE], we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments."
"To estimate or marginalize the language models Θ and φ, it is necessary to maintain lexical counts for each segment and for the segment boundaries."
"The counts for φ are summed across every segment in the entire dataset, so shifting a boundary will af-fect the probability of every segment, not only the adjacent segments as before."
"Thus, the factoriza-tion that enabled dynamic programming inference in Section 3.3 is no longer applicable."
"Instead, we must resort to approximate inference."
Sampling-based inference is frequently used in related Bayesian models.
Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model.
"The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the condi-tional distribution of each hidden variable[REF_CITE]."
"However, Gibbs sampling is slow to con-verge to a stationary distribution when the hidden variables are tightly coupled."
"This is the case in linear topic segmentation, due to the constraint that z t ∈ {z t−1 , z t−1 + 1} (see Section 3)."
"For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sam-pling arbitrary transformations of the latent vari-ables."
"In our framework, such transformations cor-respond to moves through the space of possible seg-mentations."
A new segmentation z 0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z 0 |z). [Footnote_4]
"4 Because the cue phrase language model φ is used across the entire dataset, transformations affect the likelihood of all documents in the corpus. For clarity, our exposition will focus on the single-document case."
"The probability of accepting a proposed transformation depends on the ratio of the joint probabilities and a correction term for asymmetries in the proposal distribution: 0 |θ , φ ) q(z|z 0 ) p accept (z → z 0 ) = min 1, p(X, z 0 0 p(X, z|θ 0 , φ 0 ) q(z 0 |z) ."
"The Metropolis-Hastings algorithm guarantees that by accepting samples at this ratio, our sampling procedure will converge to the stationary distribu-tion for the hidden variables z."
"When cue phrases are included, the observation likelihood is written:"
"Y p(X|z, Θ, φ) ="
"Y φ i Y θ z t ,i {t:z t =6 z t−1 }i∈x (t`) i∈x̃ t Y × Y θ z t ,i . {t:z t =z t−1 } i∈x t"
"As in Section 3.2, we can marginalize over the language models."
"We obtain a product of DCM dis-tributions: one for each segment, and one for all cue phrases in the dataset."
Metropolis-Hastings requires a proposal distribution to sample new configurations.
The proposal distri- bution does not affect the underlying probabilistic model – Metropolis-Hastings will converge to the same underlying distribution for any non-degenerate proposal.
"However, a well-chosen proposal distribu-tion can substantially speed convergence."
"Our basic proposal distribution selects an existing segmentation point with uniform probability, and considers a set of local moves."
"The proposal is con-structed so that no probability mass is allocated to moves that change the order of segment boundaries, or merge two segments; one consequence of this re-striction is that moves cannot add or remove seg-ments. [Footnote_5] We set the proposal distribution to decrease exponentially with the move distance, thus favoring incremental transformations to the segmentation."
5 Permitting moves to change the number of segments would substantially complicate inference.
"More formally, let d(z → z 0 ) &gt; 0 equal the dis-tance that the selected segmentation point is moved when we transform the segmentation from z to z 0 ."
"We can write the proposal distribution q(z 0 | z) ∝ c(z → z 0 )d(z → z 0 ) λ , where λ &lt; 0 sets the rate of exponential decay and c is an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points. [Footnote_6]"
"6 We set λ = − max-move1 , where max-move is the maximum move-length, set to 5 in our experiments. These parameters af-fect the rate of convergence but are unrelated to the underly-ing probability model. In the limit of enough samples, all non-pathological settings will yield the same segmentation results."
We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers.
"We multiply the unnormalized chance of proposing a move to location z → z 0 by a term equal to one plus the number of candidate cue phrases in the segment-initial sentences in the new configuration z 0 , written num-cue(z 0 )."
"Formally, q ling (z 0 | z 0 ) ∝ (1 + num-cue(z 0 ))q(z 0 | z)."
We use a list of cue phrases identified[REF_CITE].
We evaluate our model with both the basic and linguistically-enhanced proposal distribu-tions.
"As in section 3.4, we set the priors θ 0 and φ 0 us-ing gradient-based search."
"In this case, we perform gradient-based optimization after epochs of 1000"
Interleaving sampling-based inference with direct optimization of param-eters can be considered a form of Monte Carlo Expectation-Maximization (MCEM;[REF_CITE]).
Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text.
"For multi-speaker meetings, we use the ICSI cor-pus of meeting transcripts[REF_CITE], which is becoming a standard for speech segmentation (e.g.,[REF_CITE])."
"This dataset includes transcripts of 75 multi-party meet-ings, of which 25 are annotated for segment bound-aries."
"For text, we introduce a dataset in which each document is a chapter selected from a medical text-book[REF_CITE]. [Footnote_7] The task is to divide each chapter into the sections indicated by the au-thor."
7 The full text of this book is available for free download[URL_CITE]
"This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter)."
"Each chapter contains an average of 140 sentences, giv-ing an average of 28 sentences per segment."
Metrics All experiments are evaluated in terms of the commonly-used P k[REF_CITE]and WindowDiff (WD)[REF_CITE]scores.
"Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other."
"WindowDiff is stricter in that it requires that the number of intervening seg-ments between the two sentences be identical in the hypothesized and the reference segmentations, while P k only asks whether the two sentences are in the same segment or not."
"P k and WindowDiff are penalties, so lower values indicate better segmenta-tions."
We use the evaluation source code provided[REF_CITE].
System configuration We evaluate our Bayesian approach both with and without cue phrases.
"With-out cue phrases, we use the dynamic programming inference described in section 3.3."
This system is referred to as B AYES S EG in Table 1.
"When adding cue phrases, we use the Metropolis-Hastings model described in 4.1."
Both basic and linguistically-motivated proposal distributions are evaluated (see Section 4.2); these are referred to as B AYES S EG - CUE and B AYES S EG - CUE - PROP in the table.
"For the sampling-based systems, results are av-eraged over five runs."
"The initial configuration is obtained from the dynamic programming inference, and then 100,000 sampling iterations are performed."
"The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero."
"The use of annealing to obtain a maximum a posteri-ori (MAP) configuration from sampling-based in-ference is common (e.g.,[REF_CITE])."
The total running time of our system is on the order of three minutes per document.
"Due to mem-ory constraints, we divide the textbook dataset into ten parts, and perform inference in each part sepa-rately."
"We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all docu-ments."
Baselines We compare against three com-petitive alternative systems from the literature: U&amp;I[REF_CITE]; LCS EG[REF_CITE]; M CS[REF_CITE].
All three systems are described in the related work (Section 2).
"In all cases, we use the publicly avail-able executables provided by the authors."
"Parameter settings For LCS EG , we use the pa-rameter values specified in the paper[REF_CITE]."
M CS requires parameter settings to be tuned on a development set.
"Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described[REF_CITE]."
Our system does not require pa-rameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3.
"U&amp;I requires no parameter tuning, and is used “out of the box.”"
"In all exper-iments, we assume that the number of desired seg-ments is provided."
Preprocessing Standard preprocessing techniques are applied to the text for all comparisons.
"A set of stop-words is also removed, using the same list originally em-ployed by several competitive systems ([REF_CITE];"
Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems.
"As shown in the ta-ble, the Bayesian models achieve the best results on both metrics for both corpora."
"On the medical text-book corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all base-lines on both metrics."
"On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the P k metric, and achieve smaller improvement on the WindowDiff metric."
"The results on the meeting corpus also compare favorably with the topic-modeling method[REF_CITE], who report a P k of .289 and a WindowDiff of .329."
Another observation from Table 1 is that the con-tribution of cue phrases depends on the dataset.
"Cue phrases improve performance on the meeting cor-pus, but not on the textbook corpus."
The effective-ness of cue phrases as a feature depends on whether the writer or speaker uses them consistently.
"At the same time, the addition of cue phrases prevents the use of exact inference techniques, which may ex-plain the decline in results for the meetings dataset."
"To investigate the quality of the cue phrases that our model extracts, we list its top ten cue phrases for each dataset in Table 2."
"Cue phrases are ranked by their chi-squared value, which is computed based on the number of occurrences for each word at the beginning of a hypothesized segment, as compared to the expectation."
"For cue phrases listed in bold, the chi-squared value is statistically significant at the level of p &lt; .01, indicating that the frequency with which the cue phrase appears at the beginning of segments is unlikely to be a chance phenomenon."
"As shown in the left column of the table, our model has identified several strong cue phrases from the meeting dataset which appear to be linguistically plausible."
Four of the ten cue phrases identified by our system overlap with their analysis; these are indicated with asterisks.
"In con-trast to our model’s success at extracting cue phrases from the meeting dataset, only very common words are selected for the textbook dataset."
"This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook."
This paper presents a novel Bayesian approach to unsupervised topic segmentation.
"Our algorithm is capable of incorporating both lexical cohesion and cue phrase features in a principled manner, and out-performs state-of-the-art baselines on text and tran-scribed speech corpora."
"We have developed exact and sampling-based inference techniques, both of which search only over the space of segmentations and marginalize out the associated language mod-els."
"Finally, we have shown that our model provides a theoretical framework with connections to infor-mation theory, while also generalizing and justify-ing prior work."
"In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information."
There is growing interest in applying Bayesian techniques to NLP problems.
"There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on."
This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with var-ious numbers of hidden states on data sets of different sizes.
"Recent papers have given con-tradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM."
"We invesigate a variety of samplers for HMMs, including some that these earlier pa-pers did not study."
"We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers."
"In terms of times of conver-gence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were gener-ally faster than their collapsed counterparts on large data sets."
Probabilistic models now play a central role in com-putational linguistics.
These models define a prob-ability distribution P(x) over structures or analyses x.
"For example, in the part-of-speech (POS) tag-ging application described in this paper, which in- volves predicting the part-of-speech tag t i of each word w i in the sentence w = (w 1 ,... ,w n ), the structure x = (w,t) consists of the words w in a sentence together with their corresponding parts-of-speech t = (t 1 , . .. , t n )."
In general the probabilistic models used in com-putational linguistics have adjustable parameters θ which determine the distribution P(x | θ).
In this paper we focus on bitag Hidden Markov Models (HMMs).
"Since our goal here is to compare algo-rithms rather than achieve the best performance, we keep the models simple by ignoring morphology and capitalization (two very strong cues in English) and treat each word as an atomic entity."
This means that the model parameters θ consist of the HMM state-to-state transition probabilities and the state-to-word emission probabilities.
In virtually all statistical approaches the parame-ters θ are chosen or estimated on the basis of training data d.
"This paper studies unsupervised estimation, so d = w = (w 1 , . . . , w n ) consists of a sequence of words w i containing all of the words of training corpus appended into a single string, as explained below."
Maximum Likelihood (ML) is the most common estimation method in computational linguistics.
A Maximum Likelihood estimator sets the parameters to the value θ̂ that makes the likelihood L d of the data d as large as possible:
L d (θ) =
P(d | θ) θ̂ = arg max L d (θ) θ
"In this paper we use the Inside-Outside algo-rithm, which is a specialized form of Expectation-"
"Maximization, to find HMM parameters which (at least locally) maximize the likelihood function L d ."
"Recently there is increasing interest in Bayesian methods in computational linguistics, and the pri-mary goal of this paper is to compare the perfor-mance of various Bayesian estimators with each other and with EM."
A Bayesian approach uses Bayes theorem to fac-torize the posterior distribution P(θ | d) into the likelihood P(d | θ) and the prior P(θ).
P(θ | d) ∝ P(d | θ) P(θ)
Priors can be useful because they can express pref-erences for certain types of models.
"To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well)."
"One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words."
An appro-priate Dirichlet prior can express this preference.
"While it is possible to use Bayesian inference to find a single model, such as the Maximum A Pos-teriori or MAP value of θ which maximizes the posterior P(θ | d), this is not necessarily the best approach[REF_CITE]."
"Instead, rather than commiting to a single value for the pa-rameters θ many Bayesians often prefer to work with the full posterior distribution P(θ | d), as this naturally reflects the uncertainty in θ’s value."
In all but the simplest models there is no known closed form for the posterior distribution.
"However, the Bayesian literature describes a number of meth-ods for approximating the posterior P(θ | d)."
Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsu-pervised HMM POS taggers[REF_CITE].
"These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t | w) of POS tags (i.e., HMM hidden states) t given words w."
This recent literature reports contradictory results about these Bayesian inference methods.
"John- son (2007) compared two Bayesian inference algo-rithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to con-verge and produced a worse solution than EM."
"On the other hand,[REF_CITE]re-ported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task."
One of the primary motivations for this paper was to understand and resolve the dif-ference in these results.
We replicate the results of both papers and show that the difference in their re-sults stems from differences in the sizes of the train-ing data and numbers of states in their models.
It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.
"This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsuper-vised POS tagging problems of various sizes."
Our goal here is to try to learn how the performance of these different estimators varies as we change the number of hidden states in the HMMs and the size of the training data.
"In theory, the Gibbs samplers produce streams of samples that eventually converge on the true posterior distribution, while the Variational Bayes (VB) estimator only produces an approximation to the posterior."
"However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes in-creasingly peaked, so one would expect this varia-tional approximation to become increasingly accu-rate."
"Further the Gibbs samplers used in this paper should exhibit reduced mobility as the size of train-ing data increases, so as the size of the training data increases eventually the Variational Bayes estimator should prove to be superior."
"However the two point-wise Gibbs samplers in-vestigated here, which resample the label of each word conditioned on the labels of its neighbours (amongst other things) only require O(m) steps per sample (where m is the number of HMM states), while EM, VB and the sentence-blocked Gibbs sam-plers require O(m 2 ) steps per sample."
"Thus for HMMs with many states it is possible to perform one or two orders of magnitude more iterations of the point-wise Gibbs samplers in the same run-time as the other samplers, so it is plausible that they would yield better results."
"There are a number of excellent textbook presen-tations of Hidden Markov Models[REF_CITE], so we do not present them in detail here."
"Conceptually, a Hidden Markov Model uses a Markov model to generate the se-quence of states t = (t 1 , . . . , t n ) (which will be in-terpreted as POS tags), and then generates each word w i conditioned on the corresponding state t i ."
"We insert endmarkers at the beginning and end of the corpus and between sentence boundaries, and constrain the estimators to associate endmarkers with a special HMM state that never appears else-where in the corpus (we ignore these endmarkers during evaluation)."
"This means that we can formally treat the training corpus as one long string, yet each sentence can be processed independently by a first-order HMM."
"In more detail, the HMM is specified by a pair of multinomials θ t and φ t associated with each state t, where θ t specifies the distribution over states t 0 fol-lowing t and φ t specifies the distribution over words w given state t. t i | t i−1 = t ∼ Multi(θ t ) (1) w i | t i = t ∼ Multi(φ t )"
The Bayesian model we consider here puts a fixed uniform Dirichlet prior on these multinomials.
"Be-cause Dirichlets are conjugate to multinomials, this greatly simplifies inference. θ t | α ∼ Dir(α) φ t | α 0 ∼ Dir(α 0 )"
A multinomial θ is distributed according to the Dirichlet distribution Dir(α) iff: m α j −1 P(θ | α) ∝ Y θ j j=1
"In our experiments we set α and α 0 to the uniform values (i.e., all components have the same value α or α 0 ), but it is possible to estimate these as well[REF_CITE]."
"Informally, α controls the sparsity of the state-to-state transition probabil-ities while α 0 controls the sparsity of the state-to-word emission probabilities."
"As α 0 approaches zero the prior strongly prefers models in which each state emits as few words as possible, capturing the intu-ition that most word types only belong to one POS mentioned earlier."
"Expectation-Maximization is a procedure that iter-atively re-estimates the model parameters (θ,φ), converging on a local maximum of the likelihood."
"Specifically, if the parameter estimate at iteration ` is (θ (`) , φ (`) ), then the re-estimated parameters at it-eration ` + 1 are: θ t( 0 `|+t 1) ="
"E[n t 0 ,t ]/E[n t ] (2) φ w(`+|t 1) ="
"E[n 0w,t ]/E[n t ] where n 0w,t is the number of times word w occurs with state t, n t 0 ,t is the number of times state t 0 fol-lows t and n t is the number of occurences of state t; all expectations are taken with respect to the model (θ (`) , φ (`) )."
"The experiments below used the Forward-Backward algorithm[REF_CITE], which is a dy-namic programming algorithm for calculating the likelihood and the expectations in (2) in O(nm 2 ) time, where n is the number of words in the train-ing corpus and m is the number of HMM states."
"Variational Bayesian inference attempts to find a function Q(t, θ, φ) that minimizes an upper bound (3) to the negative log likelihood. − log P(w) = − log Z Q(t, θ, φ)P(Qw(t,,tθ, ,θφ, φ) ) dt dθ dφ ≤ − Z Q(t, θ, φ) log PQ(w(t,,tθ, ,θφ, φ) ) dt dθ dφ (3)"
The upper bound (3) is called the Variational Free Energy.
"We make a “mean-field” assumption that the posterior can be well approximated by a factor-ized model Q in which the state sequence t does not covary with the model parameters θ, φ:"
"P(t, θ, φ | w) ≈ Q(t, θ, φ) = Q 1 (t)Q 2 (θ, φ)"
The calculus of variations is used to minimize the KL divergence between the desired posterior distri-bution and the factorized approximation.
"It turns out that if the likelihood and conjugate prior be-long to exponential families then the optimal Q 1 and Q 2 do too, and there is an EM-like iterative pro-cedure that finds locally-optimal model parameters[REF_CITE]."
"This procedure is especially attractive for HMM inference, since it involves only a minor modifica-tion to the M-step of the Forward-Backward algo-rithm."
"In general, the E-step for VB inference for HMMs is the same as in EM, while the M-step is as follows: θ̃ t( 0 `|+t 1) = f(E[n t 0 ,t ] + α)/f(E[n t ] + mα) (4) φ̃ (w`|+t 1) = f(E[n 0w,t ] + α 0 )/f(E[n t ] + m 0 α 0 ) f(v) = exp(Ψ(v)) where m 0 and m are the number of word types and states respectively, Ψ is the digamma function and the remaining quantities are as in (2)."
"This means that a single iteration can be performed in O(nm 2 ) time, just as for the EM algorithm."
"The goal of Markov Chain Monte Carlo (MCMC) algorithms is to produce a stream of samples from the posterior distribution P(t | w, α)."
A Gibbs sampler is a simple kind of MCMC algorithm that is well-suited to sampling high-dimensional spaces.
"A Gibbs sampler for P(z) where z = (z 1 , . . . , z n ) proceeds by sampling and updating each z i in turn from P(z i | z −i ), where z −i = (z 1 , . . . , z i−1 , z i+1 , . . . , z n ), i.e., all of the z except z i[REF_CITE]."
"We evaluate four different Gibbs samplers in this paper, which vary along two dimensions."
"First, the sampler can either be pointwise or blocked."
"A point-wise sampler resamples a single state t i (labeling a single word w i ) at each step, while a blocked sam-pler resamples the labels for all of the words in a sentence at a single step using a dynamic program-ming algorithm based on the Forward-Backward al-gorithm. (In principle it is possible to use block sizes other than the sentence, but we did not explore this here)."
"A pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm 2 ) time per iteration, where m is the number of HMM states and n is the length of the training corpus."
"Second, the sampler can either be explicit or col-lapsed."
"An explicit sampler represents and sam-ples the HMM parameters θ and φ in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled."
The difference between explicit and collapsed samplers corresponds exactly to the dif-ference between the two PCFG sampling algorithms presented[REF_CITE].
"An iteration of the pointwise explicit Gibbs sam-pler consists of resampling θ and φ given the state-to-state transition counts n and state-to-word emis-sion counts n 0 using (5), and then resampling each state t i given the corresponding word w i and the neighboring states t i−1 and t i+1 using (6). θ t | n t , α ∼ Dir(n t + α) (5) φ t | n 0t , α 0 ∼ Dir(n 0t + α 0 )"
"P(t i | w i , t −i , θ, φ) ∝ θ t i |t i−1 φ w i |t i θ t i+1 |t i (6) The Dirichlet distributions in (5) are non-uniform; n t is the vector of state-to-state transition counts in t leaving state t in the current state vector t, while n 0t is the vector of state-to-word emission counts for state t."
The samplers th[REF_CITE]and[REF_CITE]describe are pointwise collapsed Gibbs samplers.
Figure 1 gives the sampling distri-bution for this sampler.
The blocked Gibbs samplers differ from the point-wise Gibbs samplers in that they resample the POS tags for an entire sentence at a time.
"At each iteration the explicit blocked Gibbs sam-pler resamples θ and φ using (5), just as the explicit pointwise sampler does."
Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned.
This can be done in parallel for each sentence in the training corpus.
"The collapsed blocked Gibbs sampler is a straight-forward application of the Metropolis-within-Gibbs approach proposed[REF_CITE]for PCFGs, so we only sketch it here."
"We iterate through the sentences of the training data, re-sampling the states for each sentence conditioned on the state-to-state transition counts n and state-to-word emission counts n 0 for the other sentences in the corpus."
"This is done by first computing the parameters θ ? and φ ? of a proposal HMM using (7). n t 0 ,t + α θ t? 0 |t = n t + mα (7) n 0w,t + α 0 φ w? |t = n t + m 0 α"
Then we use the dynamic programming sampler de-scribed above to produce a proposal state sequence t ? for the words in the sentence.
"Finally, we use a Metropolis-Hastings accept-reject step to decide whether to update the current state sequence for the sentence with the proposal t ? , or whether to keep the current state sequence."
"In practice, with all but the very smallest training corpora the acceptance rate is very high; the acceptance rate for all of our collapsed blocked Gibbs samplers was over 99%."
The previous section described six different unsu-pervised estimators for HMMs.
In this section we compare their performance for English part-of-speech tagging.
One of the difficulties in evalu-ating unsupervised taggers such as these is map-ping the system’s states to the gold-standard parts-of-speech.
"How-ever as Goldwater (p.c.) points out, this may not be an ideal evaluation measure; e.g., a tagger which as-signs all words the same single part-of-speech tag does disturbingly well under Variation of Informa-tion, suggesting that a poor tagger may score well under VI."
In order to avoid this problem we focus here on evaluation measures that construct an explicit map-ping between the gold-standard part-of-speech tags and the HMM’s states.
"Perhaps the most straight-forward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state se-quence t to a sequence of part-of-speech tags."
"But[REF_CITE]observes, this approach has several de-fects."
If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.
We can partially address this by cross-validation.
"We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most fre-quently, and use that mapping to map the states of the second part of the corpus to parts-of-speech."
We call the accuracy of the resulting tagging the cross-validation accuracy.
"Finally, following[REF_CITE]and[REF_CITE]we can instead insist that at most one HMM state can be mapped to any part-of-speech tag."
"Following these authors, we used a greedy algo-rithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 accuracy."
The studies presented[REF_CITE]and[REF_CITE]differed in the number of states that they used.
We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).
"Also, the studies differed in the size of the corpora used."
"The largest corpus th[REF_CITE]studied contained 96,000 words, while[REF_CITE]used all of the [Footnote_1],173,766 words in the full Penn WSJ treebank."
"1 We found that on some data sets the results are sensitive to the values of the hyperparameters. So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others. Un-fortunately, we do not know any efficient way of searching the optimal hyperparameters in a much wider and more fine-grained space. We leave it to future work."
"For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank."
"We ran each estimator with the eight different combinations of values for the hyperparameters α and α 0 listed below, which include the optimal values for the hyperparameters found[REF_CITE], and report results for the best combination for each estimator below 1 . α α 0 1 1 1 0.5 0.5 1 0.5 0.5 0.1 0.1 0.1 0.0001 0.0001 0.1 0.0001 0.0001"
"Further, we ran each setting of each estimator at least 10 times (from randomly jittered initial start-ing points) for at least [Footnote_1],000 iterations,[REF_CITE]showed that some estimators require many it-erations to converge."
"1 We found that on some data sets the results are sensitive to the values of the hyperparameters. So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others. Un-fortunately, we do not know any efficient way of searching the optimal hyperparameters in a much wider and more fine-grained space. We leave it to future work."
The results of our experiments are summarized in Figures 2–5.
"As might be expected, our evaluation measures dis-agree somewhat, but the following broad tendancies seem clear."
"On small data sets all of the Bayesian estimators strongly outperform EM (and, to a lesser extent, VB) with respect to all of our evaluation measures, confirming the results reported[REF_CITE]."
"This is perhaps not too surprising, as the Bayesian prior plays a compara-tively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets."
"But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used."
"Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, con-firming the results reported[REF_CITE]."
Variational Bayes converges faster than all of the other estimators we examined here.
"We found that the speed of convergence of our samplers depends to a large degree upon the values of the hyperparam-eters α and α 0 , with larger values leading to much faster convergence."
"This is not surprising, as the α and α 0 specify how likely the samplers are to con-sider novel tags, and therefore directly influence the sampler’s mobility."
"However, in our experiments the best results are obtained in most settings with small values for α and α 0 , usually between 0.1 and 0.0001."
"In terms of time to convergence, on larger data sets we found that the blocked samplers were gen-erally faster than the pointwise samplers, and that the explicit samplers (which represented and sam-pled θ and φ) were faster than the collapsed sam-plers, largely because the time saved in not com-puting probabilities on the fly overwhelmed the time spent resampling the parameters."
Of course these experiments only scratch the sur-face of what is possible.
"Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers."
"Inspired by this, one can devise hybrid strategies that inter-leave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here."
This paper introduces a new method for iden-tifying named-entity (NE) transliterations in bilingual corpora.
"Recent works have shown the advantage of discriminative approaches to transliteration: given two strings (w s ,w t ) in the source and target language, a classifier is trained to determine if w t is the translitera-tion of w s ."
This paper shows that the translit-eration problem can be formulated as a con-strained optimization problem and thus take into account contextual dependencies and con-straints among character bi-grams in the two strings.
We further explore several methods for learning the objective function of the opti-mization problem and show the advantage of learning it discriminately.
Our experiments show that the new framework results in over 50% improvement in translating English NEs to Hebrew.
Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity be-tween the entities.
"Identifying transliteration pairs is an important component in many linguistic appli-cations which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval[REF_CITE]."
"It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward task; however in many cases a consistent deter-ministic mapping between characters does not ex-ist; rather, the mapping depends on the context the characters appear in and on transliteration conven-tions which may change across domains."
"Figure 1 exhibits two examples of NE transliterations in En-glish and Hebrew, with the correct mapping across the two scripts."
"Although the two Hebrew names share a common prefix [Footnote_1] , this prefix can be mapped into a single English character or into two differ-ent characters depending on the context it appears in."
1 In all our example the Hebrew script is shown left-to-right to simplify the visualization of the transliteration mapping.
"Similarly, depending on the context it appears in, the English character a can be mapped into different characters or to an “empty” character."
"In recent years, as it became clear that solutions that are based on linguistics rules are not satisfac-tory, machine learning approaches have been de-veloped to address this problem."
The common ap-proach adopted is therefore to view this problem as a classification problem[REF_CITE]and train a discriminative classifier.
"That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that deter-mines if one is a transliteration of the other."
Sev-eral papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the dis-criminative classifier[REF_CITE].
"While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropri-ately."
"This has been shown to be difficult for lan-guage pairs which are very different, such as English and Hebrew[REF_CITE]."
"In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem."
We formalize it as an optimization problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legit-imacy constraints.
We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem.
"Our technical approach follows a large body of work developed over the last few years, following[REF_CITE]that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods ([REF_CITE]; Clarke and Lapata, ;[REF_CITE])."
"We investigate several ways to train our objective function, which is represented as a dot product be-tween a set of features chosen to represent a pair (w s ,w t ), and a vector of initial weights."
"Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter- mine initial weights."
We then use a method simi-lar[REF_CITE]in order to discriminatively train a better weight vector for the objective function.
Our key contribution is that we use a constrained optimization approach also to determine a better fea-ture representation for a given pair.[REF_CITE]attempted a related approach to re-stricting the set of features representing a transliter-ation candidate.
"However, rather than directly align-ing the two strings as done there, we exploit the ex-pressiveness of the ILP formulation and constraints to generate a better representation of a pair."
This is the representation we then use to discriminatively learn a better weight vector for the objective func-tion used in our final model.
"Our experiments focus on Hebrew-English transliteration, which were shown to be very dif-ficult in a previous work[REF_CITE]."
"We show very significant improvements over existing work with the same data set, proving the advantage of viewing the transliteration decision as a global inference problem."
"Furthermore, we show the importance of using a discriminatively trained objective function."
The rest of the paper is organized as follows.
The main algorithmic contribution of this paper is de-scribed in Sec. 2.
Our experimental study is de-scribes in Sec. 3 and Sec. 4 concludes.
"In this section we present our transliteration decision framework, which is based on solving a constrained optimization problem with an objective function that is discriminatively learned."
Our framework consists of three key elements: 1.
"Decision Model When presented with a NE in the source language w s and a set of candi-dates {w t } k1 in the target language, the decision model ranks the candidate pairs (w s ,w t ) and selects the “best” candidate pair."
"This is framed as an optimization problem w ∗t = argmax i {w · F (w s , w ti )}, (1) where F is a feature vector representation of the pair (w s , w it ) and w is a vector of weights assigned to each feature. 2."
"Representation A pair s = (w s , w t ) of source and target NEs is represented as a vector of fea-tures, each of which is a pair of character n-grams, from w s and w t , resp."
"Starting with a baseline representation introduced[REF_CITE], denoted here AF(s), we refine this representation to take into ac-count dependencies among the individual n-gram pairs."
This refinement process is framed as a constrained optimization problem:
"F (s) ∗ = argmax F⊆AF {w · AF (s)}, (2) subject to a set C of linear constraints."
"Here AF is the initial representation (All−Features), w is a vector of weights assigned to each fea-ture and C is a set of constraints accounting for interdependencies among features. 3."
Weight Vector Each pairwise n-gram feature is associated with a weight; this weigh vector is used in both optimization formulations above.
The weight vector is determined by considering the whole training corpus.
"The initial weight vector is obtained generatively, by counting the relative occurrence of substring pairs in posi-tive examples."
The representation is refined by discriminatively training a classifier to maxi-mize transliteration performance on the train-ing data.
"In doing that, each example is rep-resented using the feature vector representation described above."
"The three key operations described above are be-ing used in several stages, with different parameters (weight vectors and representations) as described in Alg. 1."
In each stage a different element is re-fined.
"The input to this process is a training corpus Tr=(D S ,D T ) consisting of NE transliteration pairs s = (w s ,w t ), where w s ,w t are NEs in the source and target language, respectively."
"Each such sam-ple point is initially represented as a feature vector AF (s) (for All−Features), where features are pairs of substrings from the two words (following[REF_CITE])."
"Given the set of feature vectors generated by ap-plying AF to Tr, we assign initial weights W to the features ((1) in Alg. 1)."
"These weights form the initial objective function used to construct a new feature based representation, Informative−Features, IF W (s) ((2) in Alg. 1)."
"Specifically, for an instance s, IF W (s) is the solution of the optimization prob-lem in Eq. 2, with W as the weight vector, AF(s) as the representation, and a set of constraints ensur-ing the “legitimacy” of the selected set of features (Sec. 2.2.1)."
"Input: Training Corpora Tr=(D S ,D T )"
Output: Transliteration model M 1.
"Initial Representation and Weights For each sample s ∈ Tr, use AF to generate a feature vector {(f s , f t ) 1 , (f s , f t ) 2 , . . . , (f s , f t ) n } ∈ {0, 1} n ."
"Define W:f → R s.t. foreach feature f =(f s , f t )"
"W(f) = ##(f( s f,f t ) × ##(f( s f,f t ) s ) t ) 2."
Inferring Informative Representation (W) Modify the initial representation by solving the following constrained optimization problem:
IF W (s) ∗ = argmax IF(s)⊆(AF(s))
"W · AF(s), subject to constraints C. 3."
"Train a discriminative model on Tr, using {IF(s)} s∈Tr ."
Let W D be the new weight vector obtained by discriminative training. 4.
Inferring Informative Representation (W D )
Modify the initial representation by solving the following constrained optimization problem.
"This time, the objective function is determined by the discriminatively trained weight vector W D ."
IF W D (s) ∗ = argmax IF(s)⊆(AF(s))
"W D · AF(s), subject to constraints C. 5."
"Given a word w s and a list of candidates w 1t , w 2t , . . . w tk , the chosen transliteration is w t ∗ , determined by: t ∗ = argmax i {W D ·"
"IF W D ((w s , w ti ))} Algorithm 1: Transliteration Framework."
The new feature extraction operator IF W (s) is now used to construct a new representation of the training corpus.
"With this representation, we train discriminately a new weight vector W D ."
"This weight vector, now defines a new objective function for the optimization problem in Eq. 2; W D is the weight vector and AF (s) the representation."
We de- note by IF W D (s) the solution of this optimization problem for an instance s.
"Given a representation and a weight vector, the optimization problem in Eq. 1 is used to find the transliteration of w s ."
Our best decision model makes use of Eq. 1 using W D as the feature vector and IF W D (s) as the feature representation of s.
The rest of this section provides details on the op-erations and how we use them in different stages.
"The feature space we consider consists of n po-tential features, each feature f = (f s ,f t ) repre-sents a pairing of character level n-grams, where f s ∈ {Source-Language ∪ empty-string } and f t ∈ {Target-Language ∪ empty-string}."
"A given sample (w s , w t ) consisting of a pair of NEs is represented as a features vector s ∈ {0, 1} n ."
"We say that a fea-ture f i is active if f i = 1 and that s 1 ⊂ s 2 , ⇐⇒ {f i } {f i = 1 in s 1 } ⊂ {f i } {f i =1 in s 2 } ."
"We represent the active features corresponding to a pair as a bipar-tite graph G = (V, E), in which each vertex v ∈ V either represents the empty string, a single character or a bi-gram."
"V S , V T denote the vertices represent-ing source and target language n-grams respectively."
"Each of these sets is composed of two disjoint sub-sets: V S = V US ∪ V BS , V T = V UT ∪ V BT consisting of vertices representing the uni-gram and bi-gram strings."
"Given a vertex v, degree(v, V 0 )denotes the degree of v in a subgraph of G, consisting only of V 0 ⊂ V ; index(v) is the index of the substring rep-resented by v in the original string."
Edges in the bipartite graph represent active fea-tures.
The only deviation is that the vertex represent-ing the empty string can be connected to any other (non-empty) vertex.
"Our initial feature extraction method follows the one presented[REF_CITE], in which the feature space consists of n-gram pairs from the two languages."
"Given a pair, each word is decomposed into a set of character substrings of up to a given length (including the empty string)."
"Features are generated by pairing substrings from the two sets whose relative positions in the original words differ by k or less places, or formally:"
"E = {e = (v i , v j ) | (v i ∈ V S ∧ v j ∈ V T ) ⇒ (index(v j ) + k ≥ index(v i ) ≥ index(v j ) − k) ∧ (v i 6= v empty−string ∨ v j 6= v empty−string )}."
"In our experiments we used k=1 which tested em-pirically, achieved the best performance."
Figure 2 exhibits the active features in the exam-ple using the graph representation.
"We refer to this feature extraction method as All-Features (AF), and define it formally as an operator AF : s → {(f s , f t ) i } that maps a sample point s = (w s , w t ) to a set of active features."
The initial sample representation generates fea-tures by coupling substrings from the two terms without considering the dependencies between the possible consistent combinations.
"Ideally, given a positive sample, it is desirable that paired sub-strings would encode phonetic similarity or a dis-tinctive context in which the two substrings corre-late."
"However, AF simply pairs substrings from the two words, resulting in a noisy representation of the sample point."
"Given enough positive samples, we assume that features appearing with distinctive fre-quency will encode the desired relation."
"We use this observation, and construct a weight vector, associ-ating each feature with a positive number indicating its relative occurrence frequency in the training data representation formed by AF."
This weight is com-puted as follows:
Definition 1 (Initial Feature Weights Vector) Let
"W :f → R s.t. for each feature f={f s , f t },"
"W (f) = #(f s , f t ) × #(f s , f t ), #(f s ) #(f t ) where #(f s , f t ) is the number of occurrences of that feature in the positive sample set, and #(f L ), L = {s, t} is the number of occurrences of an individual substring, in any of the features extracted from pos-itive samples in the training set."
"These weights transform every example into a weighted graph, where each edge is associated by W with the weight assigned to the feature it represents."
"As we empirically tested, this initialization assigns high weights to features that preserve the phonetic correlation between the two languages."
The top part of figure 5 presents several examples of weights as-signed by W to features composed of different En-glish and Hebrew substrings combinations.
It can be observed that combination which are phonetically similar are associated with a higher weight.
"How-ever, as it turns out, transliteration mappings do not consist of “clean” and consistent mappings of pho-netically similar substrings."
In the following section we explain how to use these weights to generate a more compact representation of samples.
In this section we suggest a new feature extraction method for determining the representation of a given word pair.
"We use the strength of the active features computed above, along with legitimacy constraints on mappings between source and target strings to find an optimal set of consistent active features that represents a pair."
"This problem can be naturally en-coded as a linear optimization problem, which seeks to maximize a linear objective function determined by W, over a set of variables representing the ac-tive features selection, subject to a set of linear con-straints representing the dependencies between se-lections."
"We follow the formulation given[REF_CITE], and define it as an Integer Linear Programming (ILP) optimization problem, in which each integer variable a (j,k) , defined over {0, 1}, rep-resents whether a feature pairing an n-gram j ∈ S with an n-gram k ∈ T , is active."
"Although using ILP is in general NP-hard, it has been used efficiently in many natural language (see section 1)."
Our experi-ence as well has been that this process is very effi-cient due to the sparsity of the constraints used.
"To limit the selection of active features in each sample we require that each element in the decom-position of w s into bi-grams should be paired with an element in w t , and the vice-versa."
"We restrict the possible pairs by allowing only a single n-gram to be matched to any other n-gram, with one excep- tion - we allow every bi-gram to be mapped into an empty string."
"Viewed as a bipartite graph, we allow each node (with the exception of the empty string) to have only one connected edge."
"These constraints, given the right objective function, should enforce an alignment of bi-grams according to phonetic simi-larity; for example, the word pairs described in Fig-ure 1, depicts a character level alignment between the words, where in some cases a bi-gram is mapped into a single character and in other cases single char-acters are mapped to each other, based on phonetic similarity encoded by the two scripts."
"However, im-posing these constraints over the entire set of candi-date features would be too restrictive; it is unlikely that one can consistently represent a single “correct” phonetic mapping."
We wish to represent both the character level and bi-gram mapping between names as both represent informative features on the corre-spondence between the names over the two scripts.
"To allow this, we decompose the problem into two disjoint sets of constraints imposing 1-1 mappings, one over the set of single character substrings and the other over the bi-gram substrings."
"Given the bi-partite graph generated by AF, we impose the fol-lowing constraints: Definition 2 (Transliteration Constraints) Let C be the set of constraints, consisting of the following predicates: ∀v ∈ V S , degree(v,V S ∪V UT ) ≤1 ∧ ∀v ∈ V S , degree(v,V S ∪V BT ) ≤1 ∧ ∀v ∈ V T , degree(v,V T ∪V US ) ≤1 ∧ ∀v ∈ V T , degree(v,V T ∪V BS ) ≤1"
"For example, Figure 2 shows the graph of all pos-sible candidates produced by AF."
"In Figure 3, the graph is decomposed into two graphs, each depict-ing possible matches between the character level uni-gram or bi-gram substrings. the ILP constraints ensure that in each graph, every node (with the ex-ception of the empty string) has a degree of one ."
Figure 4 gives the results of the ILP process – a unified graph in which every node has only a single edge associated with it.
Definition 3 (Informative Feature Extraction (IF))
"We define the Informative-Features(IF ) feature extraction operator, IF : s → {(f s ,f t ) i } as the solution to the ILP problem in Eq. 2."
"IF (s) ∗ = argmax IF(s)⊆(AF(s)) w · AF (s), subject to constraints C. We will use this operator with w = W, defined above, and denote it IF W , and also use it with a different weight vector, trained discriminatively, as described next."
"Using the IF W operator, we generate a better rep-resentation of the training data, which is now used to train a discriminative model."
"We use a linear classifier trained with a regularized average percep-tron update rule[REF_CITE]as imple-mented in SNoW,[REF_CITE]."
"This learning al-gorithm provides a simple and general linear clas-sifier that has been demonstrated to work well in other NLP classification tasks, e.g.[REF_CITE], and allows us to incorporate extensions such as strength of features naturally into the train-ing algorithm."
"We augment each sample in the train- ing data with feature weights; given a sample, the learner is presented with a real-valued feature vec-tor instead of a binary vector."
"This can be viewed as providing a better starting point for the learner, which improves the learning rate[REF_CITE]."
The weight vector learned by the discriminative training is denoted W D .
"Given the new weight vec-tor, we can define a new feature extraction opera-tor, that we get by applying the objective function in Eq. 2 with W D instead of W ."
"Given a sample s, the feature representation generated by this new infor-mation extraction operator is denoted IF W D (s)."
"The key difference between W and W D is that the latter was trained over a corpora containing both negative and positive examples, and as a result W D contains negative weights."
To increase the impact of training we multiplied the negative weights by 2.
"Figure 5 presents some examples of the benefit of discriminately learning the objective function; the weighted edges in the top figure show the values as-signed to features by W, while the bottom figure shows the weights assigned by W D ."
"In all cases, phonetically similar characters were assigned higher scores by W D , and character pairs not phonetically similar were typically assigned negative weights."
It is also interesting to note a special phenomena oc-curring in English-Hebrew transliterations.
"The En-glish vowels will be paired to almost any Hebrew character when generating pairs using AF, since vowels in most cases are omitted in Hebrew, there is no distinctive context in which English vowels appear."
"We can see for example, in the top graph presented in Figure 5 an edge matching a vowel to a Hebrew character with a high weight, the bottom graph showing the results of the discriminative train-ing process show that this edge is associated with a zero weight score."
"This section defines several transliteration decision models given a word w s and a list of candidates w 1t , w t2 , . . . w kt ."
"The models are used to identify the correct transliteration pair from the set of candidates {s i = (w s , w ti )} i=1... k ."
"In all cases, the decision is formulated as in Eq. 1, where different models differ by the representations and weight vectors used."
Decision Model 1 Ranking the transliteration can-didates is done by evaluating s ∗ = argmax i W ·
"AF(s i ), which selects the transliteration pair which maxi-mizes the objective function based on the genera-tively computed weight vector."
Decision Model 2 Ranking the transliteration can-didates is done by evaluating: s ∗ = argmax i W D ·
AF(s i )).
"This decision model is essentially equivalent to the transliteration models used[REF_CITE], in which a linear transliteration model was trained using a fea-ture extraction method equivalent to AF."
Decision Model 3 Ranking the transliteration can-didates is done by evaluating: s ∗ = argmax i W ·
"IF W (s i ), which maximizes the objective function with the generatively computed weight vector and the infor-mative feature representation derived based on it."
"Decision Model 4 Ranking the transliteration can-didates is done by evaluating: s ∗ = argmax i W D · IF W (s i )), which conceptually resembles the transliteration model presented[REF_CITE], in that a discriminative classifier was trained and used over a pruned feature set."
"Decision Model 5 Ranking the transliteration can-didates is done by evaluating: s ∗ = argmax i W D · IF W D (s i ), which maximize the objective function with the dis-criminately derived weight vector and the informa-tive features inferred based on it."
This decision model is the only model that incorporates discrim-inative weights as part of the feature extraction pro-cess; W D is used as the objective function used when inferring IF W D .
"We evaluated our approach over a corpus of 300 English-Hebrew transliteration pairs, and used an-other 250 different samples for training the models."
We constructed the test set by pairing each English name with all Hebrew names in the corpus.
The sys-tem was evaluated on its ability to correctly iden-tify the 300 transliteration pairs out of all the pos-sible transliteration candidates.
We measured per-formance using the Mean Reciprocal Rank (MRR) measure.
"This measure, originally introduced in the field of information retrieval, is used to evaluate systems that rank several options according to their probability of correctness."
"MRR is a natural mea-sure in our settings and has been used previously for evaluating transliteration systems, for example[REF_CITE]."
"Given a set Q of queries and their respective responses ranked according to the system’s confi-dence, we denote the rank of the correct response to a query q i ∈ Q as rank(q i )."
"MRR is then de-fined as the average of the multiplicative inverse of the rank of the correct answer, that is: 1 X 1 MRR = |Q| . i=1... |Q| rank(q i )"
"In our experiments we solved an ILP problem for every transliteration candidate pairs, and computed MRR with respect to the confidence of our decision model across the candidates."
"Although this required solving thousands of ILP instances, it posed no com-putational burden as these instances typically con-tained a small number of variables and constraints."
The entire test set is solved in less than 20 minutes using the publicly available GLPK package[URL_CITE]//[URL_CITE].
"The performance of the different models is sum-marized in table 1, these results are based on a train-ing set of 250 samples used to train the discrimi-native transliteration models and also to construct the initial weight vector W ."
Figure 6 shows perfor-mance over different number of training examples.
"Our evaluation is concerns with the core transliter-ation and decision models presented here and does not consider any data set optimizations that were in-troduced in previous works, which we view as or-thogonal additions, hence the difference with the re-sults published[REF_CITE]."
"The results clearly show that our final model, model 5, outperform other models."
"Interestingly, model 1, a simplistic model, significantly outper-forms the discriminative model presented[REF_CITE]."
We believe that this is due to two reasons.
"It shows that discriminative training over the representation obtained using AF is not efficient; moreover, this phenomenon is ac-centuated given that we train over a very small data set, which favors generative estimation of weights."
"This is also clear when comparing the performance of model 1 to model 4, which shows that learning over the representation obtained using constrained optimization (IF) results in a very significant perfor-mance improvement."
The improvement of using IF W is not automatic.
"Model 3, which uses IF W , and model 1, which uses AF, converge to nearly the same result."
"Both these models use generative weights to make the translit-eration decision, and this highlights the importance of discriminative training."
Both model 4 and model 5 use discriminatively trained weights and signifi-cantly outperform model 3.
These results indicate that using constraint optimization to generate the ex-amples’ representation in itself may not help; the ob-jective function used in this inference has a signifi-cant role in improved performance.
"The benefit of discriminatively training the objec-tive function becomes even clearer when compar-ing the performance of model 5 to that of model 4, which uses the original weight vector when inferring the sample representation."
"It can be assumed that this algorithm can bene-fit from further iterations – generating a new feature representations, training a model on it, and using the resulting model as a new objective function."
"How-ever, it turns out that after a single round, improved weights due to additional training do not change the feature representation; the inference process does not yield a different outcome."
Formulating the transliteration decision as an op-timization problem also allows us to naturally en-code other considerations into our objective func-tion. in this case we give preference to matching short words.
We encode this preference as a normal-ization factor for the objective function.
"When eval-uating on pair (w s , w t ), we divide the weight vector length of the shorter word; our decision model now becomes:"
"Ranking the transliteration candidates is done by evaluating: s ∗ = argmax i W D · IF W D (s i )/min(|w s |, |w t |)"
"As described in table 2 and figure 7, using length normalization significantly improves the re-sults."
"This can be attributed to the fact that typically Hebrew names are shorter and therefore every pair (w s ,w t ) considered by our model will be effected differently by this normalization factor."
"We introduced a new approach for identifying NE transliteration, viewing the transliteration decision as a global inference problem."
We explored sev-eral methods for combining discriminative learning in a global constraint optimization framework and showed that discriminatively learning the objective function improves performance significantly.
"From an algorithmic perspective, our key contri-bution is the introduction of a new method, in which learning and inference are used in an integrated way."
"We use learning to generate an objective function for the inference process; use the inference process to generate a better representation for the learning pro-cess, and iterate these stages."
"From the transliteration perspective, our key con-tribution is in deriving and showing the significance of a good representation for a pair of NEs."
"Our representation captures both phonetic similarity and distinctive occurrence patterns across character level matchings of the two input strings, while enforcing the constraints induced by the interdependencies of the individual matchings."
"As we show, this represen-tation serves to improve the ability of a discrimina-tive learning algorithm to weigh features appropri-ately and results in significantly better transliteration models."
This representation can be viewed as a com-promise between models that do not consider depen-dencies between local decisions and those that try to align the two strings.
Achieving this compromise is one of the advantages of the flexibility allowed by the constrained optimization framework we use.
"We plan to investigate using more constraints within this framework, such as soft constraints which can pe-nalize unlikely local decisions while not completely eliminating the entire solution."
How can the development of ideas in a sci-entific field be studied over time?
We ap-ply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational[REF_CITE]to 2006.
"We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time."
"Our methods find trends in the field including the rise of prob-abilistic methods starting in 1988, a steady in-crease in applications, and a sharp decline of research in semantics and understanding be-tween 1978 and 2001, possibly rising again after 2001."
"We also introduce a model of the diversity of ideas, topic entropy, using it to show that COLING is a more diverse confer-ence than ACL, but that both conferences as well as EMNLP are becoming broader over time."
"Finally, we apply Jensen-Shannon di-vergence of topic distributions to show that all three conferences are converging in the topics they cover."
"How can we identify and study the exploration of ideas in a scientific field over time, noting periods of gradual development, major ruptures, and the wax-ing and waning of both topic areas and connections with applied topics and nearby fields?"
One im-portant method is to make use of citation graphs[REF_CITE].
"This enables the use of graph-based algorithms like PageRank for determining re-searcher or paper centrality, and examining whether their influence grows or diminishes over time."
"However, because we are particularly interested in the change of ideas in a field over time, we have chosen a different method, following[REF_CITE]."
"In Kuhn’s model of scientific change, science pro-ceeds by shifting from one paradigm to another."
"Because researchers’ ideas and vocabulary are con-strained by their paradigm, successive incommensu-rate paradigms will naturally have different vocabu-lary and framing."
Kuhn’s model is intended to apply only to very large shifts in scientific thought rather than at the micro level of trends in research foci.
"Nonetheless, we propose to apply Kuhn’s insight that vocabulary and vocabulary shift is a crucial indicator of ideas and shifts in ideas."
Our operationalization of this in-sight is based on the unsupervised topic model La-tent Dirichlet Allocation (LDA;[REF_CITE]).
"For many fields, doing this kind of historical study would be very difficult."
"Computational linguistics has an advantage, however: the ACL Anthology, a public repository of all papers in the Computational Linguistics journal and the conferences and work-shops associated with the ACL, COLING, EMNLP, and so on."
"The ACL Anthology[REF_CITE], and comprises over 14,000 documents from conferences and the journal, beginning as early as 1965 through 2008, indexed by conference and year."
"This re-source has already been the basis of citation anal-ysis work, for example, in the ACL Anthology Net-work[REF_CITE]."
"We apply LDA to the text of the papers in the ACL Anthology to induce topics, and use the trends in these topics over time and over conference venues to address ques-tions about the development of the field."
"Despite the relative youth of our field, computa-tional linguistics has witnessed a number of research trends and shifts in focus."
"While some trends are obvious (such as the rise in machine learning meth-ods), others may be more subtle."
Has the field got-ten more theoretical over the years or has there been an increase in applications?
"What topics have de-clined over the years, and which ones have remained roughly constant?"
How have fields like Dialogue or Machine Translation changed over the years?
"Are there differences among the conferences, for exam-ple between COLING and ACL, in their interests and breadth of focus?"
"As our field matures, it is im-portant to go beyond anecdotal description to give grounded answers to these questions."
"Such answers could also help give formal metrics to model the dif-ferences between the many conferences and venues in our field, which could influence how we think about reviewing, about choosing conference topics, and about long range planning in our field."
"The analyses in this paper are based on a text-only version of the Anthology that comprises some 12,500 papers."
The distribution of the Anthology data is shown in Table 1.
Our experiments employ Latent Dirichlet Allocation (LDA;
"Each document is char- acterized by a multinomial distribution over topics, and each topic is in turn characterized by a multino-mial distribution over words."
We perform parame-ter estimation using collapsed Gibbs sampling[REF_CITE].
"Possible extensions to this model would be to in-tegrate topic modelling with citations (e.g.,[REF_CITE],[REF_CITE], and[REF_CITE])."
"Another option is the use of more fine-grained or hi-erarchical model (e.g.,[REF_CITE], and[REF_CITE])."
All our studies measure change in various as-pects of the ACL Anthology over time.
"LDA, how-ever, does not explicitly model temporal relation-ships."
One way to model temporal relationships is to employ an extension to LDA.
"The Dynamic Topic Model[REF_CITE], for example, rep-resents each year’s documents as generated from a normal distribution centroid over topics, with the following year’s centroid generated from the pre-ceding year’s."
The Topics over Time Model[REF_CITE]assumes that each document chooses its own time stamp based on a topic-specific beta distribution.
"Both of these models, however, impose con-straints on the time periods."
The Dynamic Topic Model penalizes large changes from year to year while the beta distributions in Topics over Time are relatively inflexible.
We chose instead to perform post hoc calculations based on the observed proba-bility of each topic given the current year.
"We define p̂(z|y) as the empirical probability that an arbitrary paper d written in year y was about topic z: p̂(z|y) = X p̂(z|d)p̂(d|y) d:t d =y = 1 X p̂(z|d) C (1) d:t d =y = 1 X X I(z 0i = z) C d:t d =y z i0 ∈d where I is the indicator function, t d is the date docu-ment d was written, p̂(d|y) is set to a constant 1/C."
"We first ran[REF_CITE]topics, and took 36 that we found to be relevant."
We then hand-selected seed words for 10 more topics to improve coverage of the field.
The top ten most frequent words for 43 of the topics along with hand-assigned labels are listed in Table 2.
Topics deriving from manual seeds are marked with an asterisk.
"Given the space of possible topics defined in the pre-vious section, we now examine the history of these in the entire ACL[REF_CITE]until 2006."
"To visualize some trends, we show the probability mass associated with various topics over time, plot-ted as (a smoothed version of) p̂(z|y)."
Figure 1 shows topics that have become more promi-nent more recently.
"Of these new topics, the rise in probabilistic mod-els and classification/tagging is unsurprising."
"In or-der to distinguish these two topics, we show 20 of the strongly weighted words:"
Some of the papers with the highest weights for the probabilistic models class include:
Some of the papers with the highest weights for the classification/tagging class include: (Workshop On Speech And[REF_CITE])
"As Figure 1 shows, probabilistic models seem to have arrived significantly before classifiers."
"The probabilistic model topic increases around 1988, which seems to have been an important year for probabilistic models, including high-impact papers[REF_CITE]below."
The ten papers from 1988 with the highest weights for the proba-bilistic model and classifier topics were the follow-ing:
Speech Recognition and the Frequency of Recently Used Words (COLING)[REF_CITE]
Grammatical Category Disambiguation by Statistical Optimization. (CL Journal)
What do these early papers tell us about how probabilistic models and classifiers entered the field?
"First, not surprisingly, we note that the vast majority (9 of 10) of the papers appeared in con-ference proceedings rather than the journal, con-firming that in general new ideas appear in confer-ences."
"Second, of the 9 conference papers, most of them appeared in the COLING conference (5) or the ANLP workshop (3) compared to only 1 in the ACL conference."
"This suggests that COLING may have been more receptive than ACL to new ideas at the time, a point we return to in Section 6."
"Fi-nally, we examined the background of the authors of these papers."
"Six of the 10 papers either focus on speech ([REF_CITE]) or were written by authors who had previously published on speech recognition topics, including the influential IBM (Brown et al.) and AT&amp;T (Church) labs (C88- 1016,[REF_CITE])."
Speech recognition is historically an electrical engineering field which made quite early use of probabilistic and statistical methodologies.
This suggests that researchers work-ing on spoken language processing were an impor-tant conduit for the borrowing of statistical method-ologies into computational linguistics.
Figure 2 shows several topics that were more promi-nent at the beginning of the ACL but which have shown the most precipitous decline.
Papers strongly associated with the plan-based dialogue topic in-clude:
Papers strongly associated with the computational semantics topic include:
Papers strongly associated with the conceptual se-mantics/story understanding topic include:
The declines in both computational semantics and conceptual semantics/story understanding suggests that it is possible that the entire field of natural lan-guage understanding and computational semantics broadly construed has fallen out of favor.
"To see if this was in fact the case we created a metatopic called semantics in which we combined various se-mantics topics (not including pragmatic topics like anaphora resolution or discourse coherence) includ-ing: lexical semantics, conceptual semantics/story understanding, computational semantics, WordNet, word sense disambiguation, semantic role labeling, RTE and paraphrase, MUC information extraction, and events/temporal."
"We then plotted p̂(z ∈ S|y), the sum of the proportions per year for these top-ics, as shown in Figure 3."
The steep decrease in se-mantics is readily apparent.
"The last few years has shown a levelling off of the decline, and possibly a revival of this topic; this possibility will need to be confirmed as we add data from 2007 and 2008."
"We next chose two fields, Dialogue and Machine Translation, in which it seemed to us that the topics discovered by LDA suggested a shift in paradigms in these fields."
"Figure 4 shows the shift in translation, while Figure 5 shows the change in dialogue."
"The shift toward statistical machine translation is well known, at least anecdotally."
"The shift in di-alogue seems to be a move toward more applied, speech-oriented, or commercial dialogue systems and away from more theoretical models."
"Finally, Figure 6 shows the history of several top-ics that peaked at intermediate points throughout the history of the field."
"We can see the peak of unifica-tion around 1990, of syntactic structure around 1985 of automata in 1985 and again in 1997, and of word sense disambiguation around 1998."
"We don’t know whether our field is becoming more applied, or whether perhaps there is a trend to-wards new but unapplied theories."
"We therefore looked at trends over time for the following appli-cations: Machine Translation, Spelling Correction, Dialogue Systems, Information Retrieval, Call Rout-ing, Speech Recognition, and Biomedical applica-tions."
Figure 7 shows a clear trend toward an increase in applications over time.
The figure also shows an interesting bump near 1990.
Why was there such a sharp temporary increase in applications at that time?
"Figure 8 shows details for each application, making it clear that the bump is caused by a tempo-rary spike in the Speech Recognition topic."
"In order to understand why we see this temporary spike, Figure 9 shows the unsmoothed values of the Speech Recognition topic prominence over time."
Figure 9 clearly shows a huge spike for the years 1989–1994.
"These years correspond exactly to the DARPA Speech and Natural Language Workshop, held at different locations from 1989–1994."
"That workshop contained a significant amount of speech until its last year (1994), and then it was revived in 2001 as the Human Language Technology work-shop with a much smaller emphasis on speech pro-cessing."
"It is clear from Figure 9 that there is still some speech research appearing in the[REF_CITE]certainly more than the period before 1989, but it’s equally clear that speech recognition is not an application that the ACL community has been successful at attracting."
"The computational linguistics community has two distinct conferences, COLING and ACL, with dif-ferent histories, organizing bodies, and philoso-phies."
"Traditionally, COLING was larger, with par-allel sessions and presumably a wide variety of top-ics, while ACL had single sessions and a more nar-row scope."
"In recent years, however, ACL has moved to parallel sessions, and the conferences are of similar size."
Has the distinction in breadth of top-ics also been blurred?
What are the differences and similarities in topics and trends between these two conferences?
"More recently, the EMNLP conference grew out of the Workshop on Very Large Corpora, sponsored by the Special Interest Group on Linguistic Data and corpus-based approaches to NLP (SIGDAT)."
"EMNLP started as a much smaller and narrower conference but more recently, while still smaller than both COLING and ACL, it has grown large enough to be considered with them."
How does the breadth of its topics compare with the others?
"Our hypothesis, based on our intuitions as con-ference attendees, is that ACL is still more narrow in scope than COLING, but has broadened consid-erably."
"Similarly, our hypothesis is that EMNLP has begun to broaden considerably as well, although not to the extent of the other two."
"In addition, we’re interested in whether the topics of these conferences are converging or not."
Are the probabilistic and machine learning trends that are dominant in ACL becoming dominant in COLING as well?
Is EMNLP adopting some of the topics that are popular at COLING?
"To investigate both of these questions, we need a model of the topic distribution for each conference."
"We define the empirical distribution of a topic z at a conference c, denoted by p̂(z|c) by: p̂(z|c) = X p̂(z|d)p̂(d|c) d:c d =c = 1 X p̂(z|d) C (2) d:c d =c = 1 X X I(z 0i = z) C d:c d =c z 0i ∈d"
"We also condition on the year for each conference, giving us p̂(z|y, c)."
We propose to measure the breadth of a confer-ence by using what we call topic entropy: the condi-tional entropy of this conference topic distribution.
Entropy measures the average amount of informa-tion expressed by each assignment to a random vari-able.
"If a conference has higher topic entropy, then it more evenly divides its probability mass across the generated topics."
"If it has lower, it has a far more narrow focus on just a couple of topics."
We there-fore measured topic entropy:
"We removed from the ACL and COLING lines the years when ACL and COLING are colocated (1984, 1998, 2006), and marked those colocated years as points separate from either plot."
"As expected, COLING has been historically the broadest of the three conferences, though perhaps slightly less so in recent years."
"ACL started with a fairly narrow focus, but became nearly as broad as COLING during the 1990’s."
"However, in the past 8 years it has become more narrow again, with a steeper decline in breadth than COLING."
"EMNLP, true to its status as a “Special Interest” con-ference, began as a very narrowly focused confer-ence, but now it seems to be catching up to at least ACL in terms of the breadth of its focus."
"Since the three major conferences seem to be con-verging in terms of breadth, we investigated whether or not the topic distributions of the conferences were also converging."
"To do this, we plotted the Jensen-Shannon (JS) divergence between each pair of con-ferences."
The Jensen-Shannon divergence is a sym-metric measure of the similarity of two pairs of dis-tributions.
The measure is 0 only for identical dis-tributions and approaches infinity as the two differ more and more.
"Formally, it is defined as the aver-age of the KL divergence of each distribution to the average of the two distributions:"
"Note that EMNLP and COLING have historically met very infre-quently in the same year, so those similarity scores are plotted as points and not smoothed."
"The trend across all three conferences is clear: each confer-ence is not only increasing in breadth, but also in similarity."
"In particular, EMNLP and ACL’s differ-ences, once significant, are nearly erased."
"Our method discovers a number of trends in the field, such as the general increase in applications, the steady decline in semantics, and its possible re-versal."
"We also showed a convergence over time in topic coverage of ACL, COLING, and EMNLP as well an expansion of topic diversity."
"This growth and convergence of the three conferences, perhaps influenced by the need to increase recall[REF_CITE]seems to be leading toward a tripartite real-ization of a single new “latent” conference."
This paper describes a lexical trigger model for statistical machine translation.
We present various methods using triplets incorporating long-distance dependencies that can go be-yond the local context of phrases or n-gram based language models.
We evaluate the pre-sented methods on two translation tasks in a reranking framework and compare it to the re-lated IBM model 1.
We show slightly im-proved translation quality in terms of BLEU and TER and address various constraints to speed up the training based on Expectation-Maximization and to lower the overall num-ber of triplets without loss in translation per-formance.
Data-driven methods have been applied very suc-cessfully within the machine translation domain since the early 90s.
"Starting from single-word-based translation approaches, significant improve-ments have been made through advances in mod-eling, availability of larger corpora and more pow-erful computers."
"Thus, substantial progress made in the past enables today’s MT systems to achieve acceptable results in terms of translation quality for specific language pairs such as Arabic-English."
"If sufficient amounts of parallel data are available, sta-tistical MT systems can be trained on millions of sentence pairs and use an extended level of context based on bilingual groups of words which denote the building blocks of state-of-the-art phrase-based SMT systems."
"Due to data sparseness, statistical models are of-ten trained on local context only."
Language mod-els are derived from n-grams with n ≤ 5 and bilin-gual phrase pairs are extracted with lengths up to 10 words on the target side.
This captures the local dependencies of the data in detail and is responsi-ble for the success of data-driven phrase-based ap-proaches.
"In this work, we will introduce a new statistical model based on lexicalized triplets (f, e, e 0 ) which we will also refer to as cross-lingual triggers of the form (e,e 0 → f)."
This can be understood as two words in one language triggering one word in another language.
"These triplets, modeled by p(f|e,e 0 ), are closely related to lexical translation probabilities based on the IBM model 1, i.e. p(f|e)."
"Several constraints and setups will be described later on in more detail, but as an introduction one can think of the following interpretation which is de-picted in Figure 1: Using a phrase-based MT ap-proach, a source word f is triggered by its trans-lation e which is part of the phrase being consid-ered, whereas another target word e 0 outside this phrase serves as an additional trigger in order to al-low for more fine-grained distinction of a specific word sense."
"Thus, this cross-lingual trigger model can be seen as a combination of a lexicon model (i.e. f and e) and a model similar to monolingual long-range (i.e. distant bigram) trigger models (i.e. e and e 0 , although these dependencies are reflected indi-rectly via e 0 → f) which uses both local (in-phrase) and global (in-sentence) information for the scoring."
The motivation behind this approach is to get non-local information outside the current context (i.e. the currently considered bilingual phrase pair) into the translation process.
"The triplets are trained via the EM algorithm, as will be shown later in more detail."
"In the past, a significant number of methods has been presented that try to capture long-distance de-pendencies, i.e. use dependencies in the data that reach beyond the local context of n-grams or phrase pairs."
"In language modeling, monolingual trigger approaches have been presented[REF_CITE]as well as syntactical meth-ods that parse the input and model long-range de-pendencies on the syntactic level by conditioning on the predecessing words and their corresponding par-ent nodes[REF_CITE]."
The latter approach was shown to reduce perplex-ities and improve the WER in speech recognition systems.
One drawback is that the parsing process might slow down the system significantly and the approach is complicated to be integrated directly in the search process.
"Thus, the effect is often shown offline in reranking experiments using n-best lists."
One of the simplest models that can be seen in the context of lexical triggers is the IBM model 1[REF_CITE]which captures lexical depen-dencies between source and target words.
It can be seen as a lexicon containing correspondents of trans-lations of source and target words in a very broad sense since the pairs are trained on the full sentence level.
"The model presented in this work is very close to the initial IBM model [Footnote_1] and can be seen as taking another word into the conditioning part, i.e. the trig-gering items. [Footnote_1]"
"1 Thus, instead of p(f|e) we model p(f|e, e 0 ) with different additional constraints as explained later on."
"1 Thus, instead of p(f|e) we model p(f|e, e 0 ) with different additional constraints as explained later on."
"Furthermore, since the second trig-ger can come from any part of the sentence, we also have a link to long-range monolingual triggers as presented above."
A long-range trigram model is presented[REF_CITE]where it is shown how to derive a probabilistic link grammar in order to cap-ture long-range dependencies in English using the EM algorithm.
Expectation-Maximization is used in the presented triplet model as well which is de-scribed in more detail in Section 3.
"Instead of deriv-ing a grammar automatically (based on POS tags of the words), we rely on a fully lexicalized approach, i.e. the training is taking place at the word level."
Related work in the context of fine-tuning lan-guage models by using cross-lingual lexical triggers is presented[REF_CITE].
The authors show how to use cross-lingual triggers on a document level in order to extract translation lexi-cons and domain-specific language models using a mutual information criterion.
"Recently, word-sense disambiguation (WSD) methods have been shown to improve translation quality[REF_CITE]."
They use local collocations based on sur-rounding words left and right of an ambiguous word including the corresponding parts-of-speech.
"Al-though no long-range dependencies are modeled, the approach yields an improvement of +0.6% BLEU on the NIST Chinese-English task."
"Although the baseline is significantly lower than in the work of Chan et al., this setup reaches an improvement of 0.5% BLEU on the NIST CE task and up to 1.1% BLEU on the"
"As an extension to commonly used lexical word pair probabilities p(f|e) as introduced[REF_CITE], we define our model to operate on word triplets."
"A triplet (f, e, e 0 ) is assigned a value α(f|e, e 0 ) ≥ 0 with the constraint such that ∀e, e 0 : X α(f|e, e 0 ) = 1. f"
"Throughout this paper, e and e 0 will be referred to as the first and the second trigger, respectively."
In view of its triggers f will be termed the effect.
"For a given bilingual sentence pair (f 1J ,e I1 ), the probability of a source word f j given the whole tar- get sentence e I1 for the triplet model is defined as:"
"I I 1 X X α(f j |e i ,e k ), p all (f j |e I1 ) ="
"Z (1) i=1 k=i+1 where Z denotes a normalization factor based on the corresponding target sentence length, i.e."
I(I − 1) Z = . (2) 2
The introduction of a second trigger (i.e. e k in Eq. 1) enables the model to combine local (i.e. word or phrase level) and global (i.e. sentence level) infor-mation.
"In the following, we will describe the training pro-cedure of the model via maximum likelihood esti-mation for the unconstrained case."
"The goal of the training procedure is to maximize the log-likelihood F all of the triplet model for a given bilingual training corpus {(f 1J , e I1 )} N1 consisting of N sentence pairs:"
"N J n F all := X X log p all (f j |e I1 n ), n=1 j=1 where J n and I n are the lengths of the n th source and target sentences, respectively."
"As there is no closed form solution for the maximum likelihood es-timate, we resort to iterative training via the EM al-gorithm[REF_CITE]."
"We define the aux-iliary function Q(µ; µ̄) based on F all where µ̄ is the new estimate within an iteration which is to be de-rived from the current estimate µ. Here, µ stands for the entire set of model parameters to be estimated, i.e. the set of all {α(f|e, e 0 )}."
"Thus, we obtain"
"Q {α(f|e, e 0 )}; {ᾱ(f|e, e 0 )} = N J n I n I n X X X X Z n−1 α(f j |e i , e k ) · (3) n=1 j=1 i=1 k=i+1 p all (f j |e I1 n ) log Z n−1 ᾱ(f j |e i ,e k ) , where Z n is defined as in Eq. 2."
"Using the method of Lagrangian multipliers for the normaliza-tion constraint, we take the derivative with respect to ᾱ(f|e, e 0 ) and obtain:"
"A(f, e, e 0 ) ᾱ(f|e, e 0 ) = (4) P A(f 0 , e, e 0 ) f 0 where A(f,e,e 0 ) is a relative weight accumulator over the parallel corpus:"
"A(f, e, e 0 ) = N J n −1 α(f|e, e 0 ) X X δ(f, f j )Z n C n (e, e 0 ) (5) n=1 j=1 p all (f j |e I1 n ) and"
"I n I n C n (e, e 0 ) = X X δ(e, e i )δ(e 0 , e k ). i=1 k=i+1"
"The function δ(·,·) denotes the Kronecker delta."
The resulting training procedure is analogous to the one presented[REF_CITE]and[REF_CITE].
The next section presents variants of the ba-sic unconstrained model by putting restrictions on the valid regions of triggers (in-phrase vs. out-of-phrase) and using alignments obtained from either GIZA++ training or forced alignments in order to reduce the model size and to incorporate knowledge already obtained in previous training steps.
"Based on the unconstrained triplet model presented in Section 3, we introduce additional constraints, namely the phrase-bounded and the path-aligned triplet model in the following."
The former reduces the number of possible triplets by posing constraints on the position of where valid triggers may originate from.
"In order to obtain phrase boundaries on the training data, we use forced alignments, i.e. translate the whole training data by constraining the transla-tion hypotheses to the target sentences of the training corpus."
Path-aligned triplets use an alignment constraint from the word alignments that are trained with GIZA++.
"Here, we restrict the first trigger pair (f, e) to the alignment path as based on the alignment ma-trix produced by IBM model 4."
"These variants require information in addition to the bilingual sentence pair (f 1J , e I1 ), namely a corre-sponding phrase segmentation Π = {π ij } with 1 ∃ a phrase pair that covers e i and f j π ij = 0 otherwise for the phrase-bounded method and, similarly, a word alignment A = {a ij } where a ij = 1 if e i is aligned to f j . 0 otherwise"
"The phrase-bounded triplet model (referred to as p phr in the following), restricts the first trigger e to the same phrase as f, whereas the second trigger e 0 is set outside the phrase, resulting in p phr (f j |e I1 , Π) ="
"I I 1 X X π ij (1 − π kj )α(f j |e i , e k ). (6) Z j i=1 k=1"
"The path-aligned triplet model (denoted by p align in the following), restricts the scope of e to words aligned to f by A, yielding: p align (f j |e I1 , A) ="
"I I 1 X X a ij α(f j |e i , e k ) (7) Z j i=1 k=1 where the Z j are, again, the appropriate normaliza-tion terms."
"Also, to account for non-aligned words (analo-gously to the IBM models), the empty word e 0 is considered in all three model variations."
We show the effect of the empty word in the experiments (Sec-tion 4).
"Furthermore, we can train the presented models in the inverse direction, i.e. p(e|f, f 0 ), and combine the two directions in the rescoring frame-work."
The next section presents a set of experiments that evaluate the performance of the presented triplet model and its variations.
"In this section, we describe the system setup used in this work, including the translation tasks and the cor-responding training corpora."
The experiments are based on an n-best list reranking framework.
The experiments were carried out using a state-of-the-art phrase-based SMT system.
The dynamic programming beam search decoder uses several models during decoding by combining them log-linearly.
"We incorporate phrase translation and word lexicon models in both directions, a language model, as well as phrase and word penalties including a distortion model for the reordering."
"While gener-ating the hypotheses, a word graph is created which compactly represents the most likely translation hy-potheses."
"Out of this word graph, we generate n-best lists and use them to test the different setups as described in Section 3."
"In the experiments, we use 10,000-best lists con-taining unique translation hypotheses, i.e. duplicates generated due to different phrase segmentations are reduced to one single entry."
The advantage of this reranking approach is that we can directly test the obtained models since we already have fully gener-ated translations.
"Thus, we can apply the triplet lex-icon model based on p(f|e, e 0 ) and its inverse coun-terpart p(e|f, f 0 ) directly."
"During decoding, since e 0 could be from anywhere outside the current phrase, i.e. even from a part which lies beyond the current context which has not yet been generated, we would have to apply additional constraints during training (i.e. make further restrictions such as i 0 &lt; i for a trigger pair (e i , e i 0 ))."
Optimization of the model scaling factors is car-ried out using minimum error rate training (MERT) on the development sets.
The optimization criterion is 100-BLEU since we want to maximize the BLEU score.
"For the first part of the experiments, we use the corpora that were released for the IWSLT’07 evaluation campaign."
"The training corpus con-sists of approximately 43K Chinese-English sen-tence pairs, mainly coming from the BTEC cor-pus (Basic Travel Expression Corpus)."
"This is a multilingual speech corpus which contains tourism-related material, such as transcribed conversations about making reservations, asking for directions or conversations as taking place in restaurants."
"For the experiments, we use the clean data track, i.e. tran-scriptions of read speech."
"As the development set which is used for tuning the parameters of the base-line system and the reranking framework, we use the IWSLT’04 evaluation set (500 sentence pairs)."
The two blind test sets which are used to evaluate the final performance of the models are the official evaluation sets from IWSLT’05 (506 sentences) and IWSLT’07 (489 sentences).
The average sentence length of the training cor-pus is 10 words.
"Thus, the task is somewhat lim-ited and very domain-specific."
One of the advan-tages of this setting is that preliminary experiments can be carried out quickly in order to analyze the ef-fects of the different models in detail.
This and the small vocabulary size (12K entries) makes the cor-pus ideal for first “rapid application development”-style setups without having to care about possible constraints due to memory requirements or CPU time restrictions.
"Furthermore, additional experiments are based on the EPPS corpus (European Parliament Plenary Ses-sions) as used within the FTE (Final Text Edition) track of the TC-STAR evaluations."
"The corpus con-tains speeches held by politicians at plenary sessions of the European Parliament that have been tran-scribed, “corrected” to make up valid written texts and translated into several target languages."
The lan-guage pairs considered in the experiments here are Spanish-English and English-Spanish.
The training corpus consists of roughly 1.3M sen-tence pairs with 35.5M running words on the En-glish side.
"The vocabulary sizes are considerably larger than for the IWSLT task, namely around 170K on the target side."
"As development set, we use the development data issued for the 2006 evaluation (1122 sentences), whereas the two blind test sets are the official evaluation data from 2006 (TC-Star’06, 1117 sentences) and 2007 (TC-Star’07, 1130 sen-tences)."
One of the first questions that arises is how many EM iterations should be carried out during training of the triplet model.
"Since the IWSLT task is small, we can quickly run the experiments on a full uncon-strained triplet model without any cutoff or further constraints."
Figure 2 shows the rescoring perfor-mance for different numbers of EM iterations.
"After that, there are no big changes."
The performance even de-grades a little bit after 30 iterations.
"For the IWSLT task, we therefore set a fixed number of 20 EM iter-ations for the following experiments since it shows a good performance in terms of both BLEU and TER score."
"The oracle TER scores of the 10k-best lists are 14.18% for IWSLT’04, 11.36% for IWSLT’05 and 18.85% for IWSLT’07, respectively."
"The next chain of experiments on the IWSLT task investigates the impact of changes to the setup of training an unconstrained triplet model, such as the addition of the empty word and the inclusion of sin-gletons (i.e. triplets that were only seen once in the training data)."
"This might show the importance of rare events in order to derive strategies when mov-ing to larger tasks where it is not feasible to train all possible triplets, such as e.g. on the EPPS task (as shown later) or the Chinese-English NIST task."
"The results for the unconstrained model are shown in Ta-ble 1, beginning with a full triplet model in reverse direction, p all (e|f,f 0 ), that contains no singletons and no empty words for the triggering side."
"In this setting, singletons seem to help on dev but there is no clear improvement on one of the test sets, whereas empty words do not make a significant difference but can be used since they do not harm either."
The base-line can be improved by +0.6% BLEU and around -0.5% in TER on the IWSLT’04 set.
"For the vari-ous setups, there are no big differences in the TER score which might be an effect of optimization on BLEU."
"Therefore, for further experiments using the constraints from Section 3.2, we use both singletons and empty words as the default."
"Adding the other direction p(f|e, e 0 ) results in an-other increase, with a total of +0.8% BLEU and -0.8% TER, which shows that the combination of both directions helps overall translation quality."
The results on the two test sets are shown in Table 2.
"As can be seen, we arrive at similar improvements, namely +0.6% BLEU and -0.3% TER on IWSLT’05 and +0.8% BLEU and -0.4% TER on IWSLT’07, re-spectively."
"The constrained models, i.e. the phrase-bounded (p phr ) and path-aligned (p align ) triplets are outperformed by the full unconstrained case, al-though on IWSLT’07 both unconstrained and path-aligned models are close."
"For a fair comparison, we added a classical IBM model 1 in the rescoring framework."
It can be seen that the presented triplet models slightly outperform the simple IBM model 1.
Note that IBM model 1 is a special case of the triplet lexicon model if the second trigger is the empty word.
"Since EPPS is a considerably harder task (larger vocabulary and longer sentences), the training of a full unconstrained triplet model cannot be done due to memory restrictions."
"One possibility to reduce the number of extracted triplets is to apply a max-imum distance constraint in the training procedure, i.e. only trigger pairs are considered where the dis-tance between first and second trigger is below or equal to the specified maximum."
Table 3 shows the effect of a maximum distance constraint for the Spanish-English direction.
"Due to the large amount of triplets (we extract roughly two billion triplets 2 for the EPPS data), we drop all triplets that occur less than 3 times which results in 640 million triplets."
"Also, due to time restrictions 3 , we only train 4 iterations and compare it to 4 itera-tions of the same setting with the maximum distance set to 10."
The training with the maximum distance constraints ends with a total of 380 million triplets.
"As can be seen (Table 3), the performance is compa-rable while cutting down the computation time from 9.[Footnote_2] to 3.1 hours."
2 Extraction can be easily done in parallel by splitting the corpus and merging identical triplets iteratively in a separate step for two chunks at a time.
The experiments were carried out on a 2.2GHz Opteron machine with 16 GB of mem-ory.
The overall gain is +0.4–0.6% BLEU and up to -0.4% in TER.
We even observe a slight increase in BLEU for the TC-Star’07 set which might be a ran-dom effect due to optimization on the development set where the behavior is the same as for TC-Star’06.
Results on EPPS English-Spanish for the phrase-bounded triplet model are presented in Table 4.
"Since the number of triplets is less than for the un-constrained model, we can lower the cutoff from 3 to 2 (denoted in the table by occ 3 and occ 2 , respec-tively)."
"There is a small additional gain on the TC-Star’07 test set by this step, with a total of +0.7% BLEU for TC-Star’06 and +0.8% BLEU for TC-Star’07."
"Table 5 shows results for a variation of the path-aligned triplet model p align that restricts the first trig-ger to the best aligned word as estimated in the IBM model 1, thus using a maximum-approximation of the given word alignment."
"The model was trained on two word alignments, firstly the one contained in the forced alignments on the training data, and sec-ondly on an IBM-4 word alignment generated using GIZA++."
For this second model we also demon-strate the improvement obtained when increasing the triplet lexicon size by using less trimming.
Another experiment was carried out to investigate the effect of immediate neighboring words used as triggers within the p align setting.
This is equivalent to using a “maximum distance of 1” constraint.
"We obtained worse results, namely a 0.2-0.3% drop in BLEU and a 0.[Footnote_3]-0.4% raise in TER (cf."
3 One iteration needs more than 12 hours for the uncon-strained case.
"Table 5, last row), although the training is significantly faster with this setup, namely roughly 30 minutes per it- eration using less than 2 GB of memory."
"However, this shows that triggers outside the immediate con-text help overall translation quality."
"Additionally, it supports the claim that the presented methods are a complementary alternative to the WSD approaches mentioned in Section 2 which only consider the im-mediate context of a single word."
"Finally, we compare the constrained models to an unconstrained setting and, again, to a standard IBM model 1."
Table 6 shows that the p align model con-strained on using the IBM-4 word alignments yields +0.7% in BLEU on TC-Star’06 which is +0.2% more than with a standard IBM model 1.
TER de-creases by -0.3% when compared to model 1.
"For the TC-Star’07 set, the observations are similar."
"The oracle TER scores of the development n-best list are 25.16% for English-[REF_CITE].0% for Spanish-English, respectively."
"From the results of our reranking experiments, we can conclude that the presented triplet lexicon model outperforms the baseline single-best hypotheses of the decoder."
"When comparing to a standard IBM model 1, the improvements are significantly smaller though measurable."
"So far, since IBM model 1 is considered one of the stronger rescoring mod-els, these results look promising."
"An unconstrained triplet model has the best performance if training is feasible since it also needs the most memory and time to be trained, at least for larger tasks."
"In order to cut down computational requirements, we can apply phrase-bounded and path-aligned training constraints that restrict the possibilities of selecting triplet candidates (in addition to simple thresholding)."
"Although no clear effect could be observed for adding empty words on the trigger-ing side, it does not harm and, thus, we get a sim-ilar functionality to IBM model 1 being “integrated” in the triplet lexicon model."
The phrase-bounded training variant uses forced alignments computed on the whole training data (i.e. search constrained to producing the target sentences of the bilingual corpus) but could not outperform the path-aligned model which reuses the alignment path information obtained in regular GIZA++ training.
"Additionally, we observe a positive impact from triggers lying outside the immediate context of one predecessor or successor word."
"Table 7 shows an excerpt of the top entries for (e, e 0 ) = (taxpayer, bill) and compares it to the top entries of a lexicon based on IBM model 1."
We ob-serve a triggering effect since the Spanish word pa-gar (to pay) is triggered at top position by the two English words taxpayer and bill.
The average dis-tance of taxpayer and bill is 5.4 words.
The models presented in this work try to capture this property and apply it in the scoring of hypotheses in order to allow for better lexical choice in specific contexts.
"In Table 8, we show an example translation where rescoring with the triplet model achieves higher n-gram coverage on the reference translation than the variant based on IBM model 1 rescoring."
The differ-ing phrases are highlighted.
We have presented a new lexicon model based on triplets extracted on a sentence level and trained it-eratively using the EM algorithm.
The motivation of this approach is to add an additional second trigger to a translation lexicon component which can come from a more global context (on a sentence level) and allow for a more fine-grained lexical choice given a specific context.
"Thus, the method is related to word sense disambiguation approaches."
We showed improvements by rescoring n-best lists of the IWSLT Chinese-English and EPPS Spanish-English/English-Spanish task.
"In total, we achieve up to +1% BLEU for some of the test sets in comparison to the decoder baseline and up to +0.3% BLEU compared to IBM model 1."
Future work will address an integration into the decoder since the performance of the current rescor-ing framework is limited by the quality of the n-best lists.
"For the inverse model, p(e|f,f 0 ), an in-tegration into the search is directly possible."
"Further experiments will be conducted, especially on large tasks such as the NIST Chinese-English and Arabic-English task."
Training on these huge databases will only be possible with an appropriate selection of promising triplets.
In this paper we present a textual dialogue system that uses word associations retrieved from the Web to create propositions.
We also show experiment results for the role of modal-ity generation.
The proposed system automat-ically extracts sets of words related to a con-versation topic set freely by a user.
"After the extraction process, it generates an utterance, adds a modality and verifies the semantic re-liability of the proposed sentence."
"We evalu-ate word associations extracted form the Web, and the results of adding modality."
Adding modality improved the system significantly for all evaluation criteria.
We also show how our system can be used as a simple and expandable platform for almost any kind of experiment with human-computer textual conversation in Japanese.
Two exam-ples with affect analysis and humor generation are given.
Many task-oriented dialogue systems[REF_CITE]have been developped.
"Research on non-task-oriented dialogue systems like casual conversation dialogue systems (”chatbots”) is on the other hand not very common, perhaps due to the many amateurs who try to build naturally talking systems using sometimes very clever, but rather un-scientific methods although there are systems with chatting abilities[REF_CITE]but concentrate on applying strategies to casual con-versation rather than their automatic generation of those conversations."
"However, we believe that the main reason is that an unrestricted domain is dispro-portionately difficult compared to the possible use such a system could have."
"It is for example very hard to predict the contents and topics of user utterances, and therefore it is almost impossible to prepare con-versational scenarios."
"Furthermore, scenarios need more or less specific goals to be useful."
"However in our opinion, sooner or later non-task-oriented di-alogue systems will have to be combined with task oriented systems and used after recognizing that the user’s utterance does not belong to a given task."
This would lead to more natural interfaces for e.g. infor-mation kiosks or automatic guides placed in public places where anyone can talk to them about anything[REF_CITE]re-gardless of the role the developers intended.
For this reason we have also started implementing emotive-ness recognition and joke generation modules that are presented later in the paper.
Well-known examples of non-task-oriented dia-logue systems are ELIZA[REF_CITE]and
"A.L.I.C.E [Footnote_1] , though the former was built to parody a Rogerian therapist which can be regarded as a task."
"1 Wallace, R. The Anatomy of A.L.I.C.E.[URL_CITE]"
Both systems and their countless imitators [Footnote_2] use a lot of rules coded by hand.
2 Many of them have been quite successful in the Loeb-ner Prize and the Chatterbox Challenge (competitions only for English-speaking bots) but explanations of their algorithms are not available.
"ELIZA is able to make a response to any input, but these responses are only information requests without providing any new in-formation to the user."
"In the case of A.L.I.C.E, the knowledge resource is limited to the existing database."
Creating such databases is costly and a programmer must learn the AIML mark-up lan-guage to build it.
"Although there have been attempts at updating AIML databases automatically[REF_CITE], the scale was rather limited."
"As mentioned above, these examples and many other ”chatbots” need hand-crafted rules, and are thus often ignored by computer scientists and rarely become a research topic."
"However, they have proved to be useful for e-learning[REF_CITE]and machine learning[REF_CITE]support."
"Building a system using automatic methods, like we do, seems to be the most realistic way for unre-stricted domains."
"Considering the large cost of de-veloping a program that can talk about any topic, it is appealing to turn to the huge and cheap textual source that is the Internet."
In this very moment millions of people[REF_CITE]are updating their blogs and writing articles on every possible topic.
"These are available on the Web which we can access any time, and in a faster and faster manner, the search engines grow more and more efficient."
"Thus, the Web is well suited to ex-tracting word associations triggered by words from user utterances made in a topic-free dialogue sys-tem.."
We present a system making use of this type of information.
"It automatically extracts word asso-ciation lists using all keywords in a given utterance without choosing a specific one (which most other systems that ignore the context do) then generates a reply using the only one strongest association from the nouns, verbs and adjectives association groups."
"Modality is then added to the reply, and then it is output."
Our system is built upon the idea that human utter-ances consist of a proposition and a modality[REF_CITE].
In this paper we present an algorithm for extracting word associations from the Web and a method for adding modality to statements.
We evaluate both the word associations and the use of modality.
We also suggest some future possible ex-tensions of the system and show a small experiment with adding humor to the system.
"In this paper, the system described works for Japanese and uses text as input and output."
Though the final goal of our research is to help developing freely talking car navigation systems that by their chatting abilities can help to avoid drowsiness while driving and so on. in this part of the development we concentrate on proposition generation and modality processing.
"Therefore, we work only with text now."
We plan to combine this project with research on in car voice recognition and generation.
"In this chapter, we present a method for automatic extraction of word associations based on keywords from user utterances."
"We use the Google [Footnote_3] search engine snippets to extract word associations in real time without using earlier prepared resources, such as off-line databases."
"In the first step, the system analyzed user utterances using the morphological analyzer MeCab [Footnote_4] in order to spot query keywords for extracting word associ-ations lists."
"4 MeCab: Yet Another Part-of-Speech and Morphological Analyzer,[URL_CITE]"
"We define nouns, verbs, adjectives, and unknown words as query keywords."
"The reason we chose these word classes is that these word classes can be treated as important and, to some extent, de-scribe the context."
We define a noun as the longest set of nouns in a compound noun.
"For example, the compound noun shizen gengo shori [Footnote_5] (natural language processing) is treated by MeCab as three words: (shizen - natural), (gengo - language) and (shori - processing)."
5 All Japanese transcriptions will be written in italics.
"Our system, however, threats it as one noun."
"In the next step, the system uses these keywords as query words for the Google search engine."
The system extracts the nouns from the search results and sorts them in frequency order.
This process is based on the idea that words which co-occur frequently with the input words are of high relevance to them.
The number of extracted snippets is 500.
"This value was set experimentally, taking the processing time and output quality into account."
"The top ten words of a list are treated as word associations, see Table 1 for an example."
We asked volunteers to use our system and to eval-uate the correctness of word lists generated by the system.
"First, a participant freely inputs an utter-ance, for which the system retrieves ten association words."
"Next, a participant rated these words using a scale of one to three with 3 meaning ”perfectly cor-rect”, 2 -”partially correct” and 1 - ”incorrect”."
In this experiment we consider words that receive a 2 or 3 as usable.
The reason associations rated 2 or 3 are considered as usable is that the definition of what makes a good word association here is difficult to specify.
When it comes to topic-free conversations we have observed that associations have an effect on a certain context.
"Three volunteers repeated the experiment ten times, so the final amount of evalu-ated words was 300."
"Table 2 shows the results of the top 10 words, sorted by the frequency of appearance."
Table 3 shows the results of the top 5 words.
What constitutes a correct word association was left to each volunteer to decide subjectively since in a casual conversation setting associations are hard to define strictly.
As shown in Table 2 approximately 77% of the word associations were judged as usable but there were individual differences between the evaluators.
This shows that the definition of word associations is different for each participant.
Table 3 shows that approximately 80% of the word associations were judged as usable.
It is thus highly likely that the top words from the frequency lists are correct associa-tions.
The results show that automatic extracting of word associations using a Web search engine is fea-sible.
"The main reason for extracting word associa-tions from the Web is that thanks to this method, the system can handle new information, proper names, technical terms and so on. by using only the snip-pets from the search engine."
The word association extraction takes no more than few seconds.
"For the evaluation we used only nouns but we expect al-though verbs and adjectives are often more abstract than nouns, the word associations for them will im-prove the results."
The system generates replies in the following way: • extraction of keywords from user utterance • extraction of word associations from the Web • generation of sentence proposition using the extracted associations • addition of modality to the sentence proposi-tion
The system applies morphological analysis to the use utterances in the same way as described in sec-tion 2.1 and extracts keywords based on part of speech.
The system performs a Google search using the ex-tracted keywords as a query.
The system sorts the results obtained from the query by their frequency as in section 2.1.
In section 2.1 only nouns were extracted but here we also extract verbs and adjec-tives.
"After sorting all words in adjective, verb and noun lists the system uses the ones with the highest frequency as word associations."
"Using the associations, the system generates the proposition of a sentence to be used as a reply to the user input."
A proposition is an expression rep-resenting an objective statement.
The proposition is generated by applying associations to a proposition template like [(noun) (topic indicating particle wa) (adjective)].
We prepared 8 proposition templates manually (see Table 4).
The templates were cho-sen subjectively after examining statistics from IRC [Footnote_6] chat logs.
"6 Internet Relay Chat Protocol,[URL_CITE]"
"Our criteria for choosing templates from the chat logs was that they should belong to the 20 most frequent modality patterns and to be flexible enough to fit a range of grammatical constructions, for example in English, ”isn’t it” cannot follow verbs while ”I guess” can follow nouns, adjectives, and verbs."
"The proposition templates are applied in a predetermined order: for example, first a template ”(noun) (wa) (adjective)” is used; next a template ”(noun) (ga) (adjective)” is used."
"However, since the generated proposition is not always a natural state-ment, the system uses exact matching searches of the whole phrases in a search engine to check the naturalness of each proposition."
"If the frequency of occurrence of the proposition is low, it is defined as unnatural and deleted."
This processing is based on the idea that the phrases existing on the Web in large numbers are most probably correct grammat-ically and semantically.
"If an unnatural proposition is generated, the system generates another proposi-tion in the same way."
"In this experiment the sys-tem used propositions for which the hit number ex-ceeded 1,000 hits using Google."
"Thus, the process-ing proceeds as follows."
"The system first selects the top noun, top verb, and top adjective word associa-tions."
These are applied to the templates in a prede-termined order.
"If a generated proposition is judged as valid (using Google, occurrence on the web indi-cates validity), it is used."
"If not, another template is tried until a valid proposition is found."
The reason for not trying every possible combination of associ-ated words is prohibitively long processing time.
"Finally, the system adds modality to the generated proposition."
By modality we mean a set of grammat-ical and pragmatic rules to express subjective judg-ments and attitudes.
"In our system, modality is real-ized through adverbs at the end of a sentence which is common in Japanese[REF_CITE]."
"In our system, a pair of sentence head and sentence end auxiliary verb are defined as ”modality”."
There is no standard definition of what consti-tutes modality in Japanese.
In this paper modality of casual conversation is classified into questions and informative expressions.
Questions are expressions that request information from the user.
Informative expressions are expressions that transmit informa-tion to the user.
"Patterns for these modalities are ex-tracted automatically from IRC chat logs (100,000 utterances) in advance."
"Modality patterns are ex-tracted in these ways: • pairs of grammatical particles and an auxiliary verbs placed at the end of sentences are defined as ending patterns • sentences with question marks are defined as questions • adverbs, emotive words, and connectives at the beginning of sentences are defined as informa-tive expressions • candidate patterns thus obtained are sorted by frequency"
First the system extracts sentence ending patterns from IRC chat logs.
"If an expression contains ques-tion marks, it is classified as a question."
"Next, the system extracts adverbs, emotive words, and con-nectives from the beginning and end of sentences from the IRC logs."
These pairs (beginning and end) of expressions are classified as ”informative expres-sions”.
For example question expression ”desu-ka?” is extracted from a human utterance like ”Kyou-wa samui desu-ka?” (Is it cold today?).
"An informative expression ”maa *** kedo” is extracted from a hu-man utterance as ”Maa sore-wa ureshii kedo” (Well, I’m glad, but you know...). 685 patterns were obtained for informative ex-pressions. 550 of these informative expression pat-terns were considered by authors as correct (80%)."
We sorted these candidates in frequency order.
"The words appearing at the top of the list were correct, but even the ones appearing only once were still deemed as usable."
"For example, the question expression ”janakatta deshita-kke?” is a correct expression, but appeared only once in the 100,000 utterances."
"Hence, we confirmed that chat logs include various modality expressions, and only a few of them are incorrect."
Tables 5 and 6 show some examples of modality patterns.
The system adds the modality from section 3.4.1 to the proposition from section 3.3 to generate the system output.
This process is based on the idea that human utterance consists of proposition and modal-ity.
A modality pattern is selected randomly.
"For ex-ample, if the system generates the proposition ”fuyu wa samui (Winter is cold.)” and selects the modal-ity ”iyaa ... desu-yo (Ooh ... isn’t it?)”, the gen- erated output will be ”iyaa, fuyu-wa samui desu-yo (Winter is cold, you know)”."
"However, there is a possibility that the system generates unnatural out-put like ”fuyu-wa samui dayo-ne (Winter is cold, arent’t it?)”, depending on the pair of proposition and modality."
"To this problem, the system uses the Google search engine to filter out unnatural output."
The system performs a phrase search on the end of the sentence.
"If the number of search hits is higher than threshold, the output is judged as correct."
"If the number of a search hits is lower than the threshold, the output is judged as incorrect and discarded, and a new reply is generated."
"Here, we experimentally set the threshold to 100 hits."
"We used system α, generating only the proposi-tion, and system β, generating both proposition and modality. 5 participants used each systems for con-versations of 10 turns and evaluated the conversa-tions on a 5-point scale."
"Evaluation criteria were ”will to continue the conversation” (A), ”grammati-cal naturalness of dialogues” (B), ”semantical nat-uralness of dialogues” (C), ”vocabulary richness” (D), ”knowledge richness” (E), and ”humanity of the system” (F)."
Table 7 shows average scores for the evaluations of each system.
System β that uses modality scored much higher than system α.
Table 8 shows examples of actual dialogue.
"In the eval-uation, the participants expressed the opinion that an utterance like (xx ha yy) is unnatural and using a modality like (maa)(”well”), (moo)(”anyway”) is very natural."
Thus we can say that the modality ex-pressions make the utterances of the system seem more natural.
"The simplicity, real-time processing capabilities and promising results showing that users do not get bored so quickly encouraged us to perform trials with other ongoing projects and experiment with the system working as a platform for adding various modules and algorithms."
By using our system it is possible to perform tests to see if a new idea will support or improve human-computer interaction or not.
Here we will briefly describe two such trials - one on guessing emotive values of utterances and one on improving the system’s overall evaluation by adding a pun generator.
Ptaszynski et al.[REF_CITE]have devel-oped a method for affect analysis of Japanese text.
Their method is based on cross-referencing lexical emotive elements with emotive expressions appear-ing in text.
In the process of analysis first a gen-eral emotive context is determined and then the spe-cific types of emotional states conveyed in an utter-ance are extracted.
They support this method with a
Web-mining technique to improve the performance of the emotional state type extraction.
"A system constructed on the basis of their method achieved human level performance in determining the emo-tiveness of utterances, and 65% of human level per-formance in extracting the specific types of emo-tions."
"Also, the supporting Web mining technique improved the performance of the emotional state type extraction to 85% of the human level[REF_CITE]."
As these are very promising figures we are currently in the phase of implementing their ideas in our system and testing how emotion recognition can influence speech act analysis and the automatic choice of proper modality.
"In this trial, an experiment showing that humor can improve a non-task oriented conversational system’s overall performance was conducted."
"By using a simplified version of Dybala’s PUNDA system[REF_CITE], a pun-generation was added to our baseline system."
The PUNDA algorithm consists of two parts: A Can-didate Selection Algorithm and a Sentence Integra-tion Engine.
"The former generates a candidate for a pun analyzing an input utterance and selects words or phrases that could be transformed into a pun by one of four generation patterns: homophony, ini-tial mora addition, internal mora addition or final mora addition."
The latter part generates a sentence including the candidate extracted in the previous step.
"To make the system’s response more related to the user’s input, each sentence that included a joke started with the pattern ”[base phrase] to ieba” (”Speaking of [base phrase]”)."
The remaining part of the sentence was extracted from the Web and the candidate was used as a query word and the list of sentences including this word was retrieved.
Then the shortest sentence with an exclamation mark is se-lected as most jokes convey some emotions.
"When the candidate list was empty, the system selected one random pun from a pun database."
"In the first experiment, 5 participants were asked to perform a 10-turn dialogue with two systems."
"After using both systems (baseline and humor-equipped), users were asked to evaluate both sys-tems’s performances by answering the following questions: A) Do you want to continue the dia-logue?; B) Was the system’s utterances grammati-cally natural?; C) Was the system’s utterances se-mantically natural?; D) Was the system’s vocabu-lary rich?; E) Did you get an impression that the system possesses any knowledge?; F) Did you get an impression that the system was human-like?; G)"
Do you think the system tried to make the dialogue more funny and interesting? and H) Did you find the system’s utterances interesting and funny?
An-swers were given on a 5-point scale and the results are shown in Table 9.
A third-person evaluation experiment was also performed and again the humor-equipped system scored higher than the non-humor one.
The ques-tion asked in this evaluation was: ”Which dialogue do you find most interesting and funny?”.
"Evalu-ators could choose between 3 options: Dialogue 1 (Baseline system first 3 turns), Dialogue 2 (Humor-equipped system, first 3 turns with system’s third re-sponse replaced by pun generator’s output) and"
Dia- logue 3 (the first 3 turns of the baseline system with joking ability).
Dialogue 1 and Dialogue 2 have the same input.
This means that each of humor equipped dialogues received evalu-ations two times higher than non-humor dialogue.
Our system can be also disassembled into a set of flexible tools which help students to experiment with dialogue processing.
"By using simple web-mining techniques we described, this dialogue engine is ca-pable of automatic retrieval of associations which can be used to produce a whole range of utterances - for example by using the bottom, not the top of the associations list, one can examine how interesting or provocative the dialogue becomes."
"As the sys-tem has a cgi interface, the experiments are easy and any new feature (for instance a speech act choice menu) can be easily added."
Such toolkit gives stu-dents an opportunity to experiment on a given aspect of dialogue processing without the need of build-ing a conversation system from the scratch.
"There is also no need of laborious knowledge input and, as such open-domain oriented system generates new ”on topic” utterances, experiment subjects do not get bored quickly, which is always a problem while col-lecting conversation logs of human-machine inter-action."
"A programmer also can freely choose be-tween thousands of IRC logs utterances and Internet resources for the statistical trials, grammar patterns retrieval, speech acts analysis."
In this research we investigated if word associations extracted automatically from the Web are reasonable (semantically on topic) and if they can be success-fully used in non-task-oriented dialogue systems.
We also implemented such a system extraction mod-ule.
It is able to automatically generate in real-time responses to user utterances by generating a propo-sition and adding modality retrieved from IRC chat logs.
We conducted evaluation experiments on the overall influence of the modality usage and it im-proved the system.
Therefore we showed that it is possible to construct a dialogue system that au-tomatically generates understandable on-topic utter-ances without the need of creating vast amounts of rules and data beforehand.
"We also confirmed that our system can be used as a experimental platform which can be easily used by other researchers to test their algorithms with a more unpredictible (and less boring) ”chatbot”, an important factor for long tir-ing sessions of human-computer conversation."
Cur-rently there are several projects which use the sys-tem described here as a platform for experiments and we introduced two of them - on joke generation and affect analysis.
There is still a lot of work left to be done.
"It is necessary for a non-task-oriented dialogue system to obtain not only word associations, but also different kinds of knowledge - of user’s preferences or of di-alogue itself - for example conversational strategies."
At this moment the system generates utterances by applying word associations to the proposition tem-plates and adding modality.
"We also need to more deeply consider semantics, speech acts and context to create a more advanced system."
"Finally, the sys-tem needs to recognize not only keywords, but also user’s modality."
We assume that the affect recog-nition mentioned above will help us to achieve this goal in near future and this is our next step.
By opening the system’s code and giving others the op-portunity of adding their own modules and changes we hope to solve remaining problems.
In this pa-per we focus on the impact of adding modality to a system.
Comparing the system to Japanese versions of ELIZA (already available) and ALICE (not avail-able in Japanese yet) is also one of our next steps.
Foreign name translations typically include multiple spelling variants.
"These variants cause data sparseness problems, increase Out-of-Vocabulary (OOV) rate, and present challenges for machine translation, information extraction and other NLP tasks."
This paper aims to identify name spelling variants in the target language using the source name as an anchor.
"Based on word-to-word translation and transliteration probabilities, as well as the string edit distance metric, target name translations with similar spellings are clustered."
With this approach tens of thousands of high precision name translation spelling variants are extracted from sentence-aligned bilingual corpora.
"When these name spelling variants are applied to Machine Translation and Information Extraction tasks, improvements over strong baseline systems are observed in both cases."
"Foreign names typically have multiple spelling variants after translation, as seen in the following examples:"
"He confirmed that &quot;al-Kharroub province is at the top of our priorities.&quot; …for the Socialist Progressive Party in upper Shuf and the Al-Kharrub region,… …during his tour of a number of villages in the region of Al-Kharub,… …Beirut and its suburbs and Iqlim al-Khurub,…"
"Such name spelling variants also frequently appear in other languages, such as (bushi) /12 (bushu) / (buxi) (for Bush) in Chinese, and1234 123456789789 (sbrngfyld) /12345 789 (sbryngfyld) / 13 14 (sbrynjfyld) (for Springfield) in Arabic."
"These spelling variants present challenges for many NLP tasks, increasing vocabulary size and OOV rate, exacerbating the data sparseness problem and reducing the readability of MT output when different spelling variants are generated for the same name in one document."
We address this problem by replacing each spelling variant with its corresponding canonical form.
"Such text normalization could potentially benefit many NLP tasks including information retrieval, information extraction, question answering, speech recognition and machine translation."
"Research on name spelling variants has been studied mostly in Information Retrieval research, especially in query expansion and cross-lingual IR."
Both approaches use a named entity extraction system to automatically identify names.
"For multi-lingual name spelling variants,[REF_CITE]proposed to use a general edit distance metric with a weighted FST to find technical term translations (which were referred to as “cross-lingual spelling variants”)."
These variants are typically translated words with similar stems in another language.
Toivonen and colleagues (2005) proposed a two-step fuzzy translation technique to solve similar problems.
This paper aims to identify mono-lingual name spelling variants using cross-lingual information.
"Instead of using a named entity tagger to identify name spelling variants, we treat names in one language as the anchor of spelling variants in another language."
From sentence-aligned bilingual corpora we collect word co-occurrence statistics and calculate word translation [Footnote_1] probabilities.
"1 In this paper, the translation cost measures the semantic difference between source and target names, which are estimated from their co-occurrence statistics. The transliteration cost measures their phonetic distance and are estimated based on a character transliteration model."
"For each source word, we group its target translations into clusters according to string edit distances, then calculate the transliteration cost between the source word and each target translation cluster."
"Word pairs with small transliteration costs are considered as name translations, and the target cluster contains multiple spelling variants corresponding to the source name."
We apply this approach to extract name transliteration spelling variants from bilingual corpora.
We obtained tens of thousands of high precision name translation pairs.
"We further apply these spelling variants to Machine Translation (MT) and Information Extraction (IE) tasks, and observed statistically significant improvement on the IE task, and close to oracle improvement on the MT task."
The rest of the paper is organized as follows.
In section 2 we describe the technique to identify name spelling variants from bilingual data.
In section 3 and 4 we address their application to MT and IE respectively.
We present our experiment results and detailed analysis in section 5.
Section 6 concludes this paper with future work.
"Starting from sentence-aligned parallel data, we run HMM alignment (Vogel et. al. 1996 &amp;[REF_CITE]) to obtain a word translation model."
For each source word this model generates target candidate translations as well as their translation probabilities.
A typical entry is shown in Table 1.
"It can be observed that the Arabic name’s translations include several English words with similar spellings, all of which are correct translations."
"However, because the lexical translation probabilities are distributed among these variants, none of them has the highest probability."
"As a result, the incorrect translation, iqlim, is assigned the highest probability and often selected in MT output."
"To fix this problem, it is desirable to identify and group these target spelling variants, convert them into a canonical form and merge their translation probabilities."
"For each source word in the word translation model, we cluster its target translations based on string edit distances using group average agglomerative clustering algorithm[REF_CITE]."
Initially each target word is a single word cluster.
"We calculate the average editing distance between any two clusters, and merge them if the distance is smaller than a certain threshold."
This process repeats until the minimum distance between any two clusters is above a threshold.
"In the above example, al-kharrub, al-kharub, al-khurub and al-kharroub are grouped into a single cluster, and each of the ungrouped words remains in its single word cluster."
Note that the source word may not be a name while its translations may still have similar spellings.
"An example is the Arabic word  which is aligned to English words brief, briefing, briefed and briefings."
"To detect whether a source word is a name, we calculate the transliteration cost between the source word and its target translation cluster, which is defined as the average transliteration cost between the source word and each target word in the cluster."
"As many names are translated based on their pronunciations, the source and target names have similar phonetic features and lower transliteration costs."
Word pairs whose transliteration cost is lower than an empirically selected threshold are considered as name translations.
The transliteration cost measures the phonetic similarity between a source word and a target word.
"It is calculated based on the character transliteration model, which can be trained from bilingual name translation pairs."
"We segment the source and target names into characters, then run monotone 2 HMM alignment on the source and target character pairs."
"After the training, character transliteration probabilities can be estimated from the relevant frequencies of character alignments."
"Suppose the source word f contains m characters, f 1 , f 2 , …, f m , and the target word e contains n characters, e 1 , e 2 , …, e n ."
"For j=1, [Footnote_2],…, n, letter e j is aligned to character f a according j to the HMM aligner."
"2 As name are typically phonetically translated, the character alignment are often monotone. There is no cross-link in character alignments."
"Under the assumption that character alignments are independent, the word transliteration probability is calculated as n P(e | f ) = ∏ p(e | f ) (2.1) j a j j=1 where p(e j | f a ) is the character transliteration j probability."
"Note that in the above configuration one target character can be aligned to only one source character, and one source character can be aligned to multiple target characters."
An example of the trained A-E character transliteration model is shown in Figure 1.
The Arabic character is aligned with high probabilities to English letters with similar pronunciation.
"Because Arabic words typically omit vowels, English vowels are also aligned to Arabic characters."
"Given this model, the characters within a Romanized Arabic name and its English translation are aligned as shown in Figure 1."
The transliteration units are typically characters.
"The Arabic alphabet includes 32 characters, and the English alphbet includes 56 letters [Footnote_3] ."
"3 Uppercase and lowercase letters plus some special symbols such as ‘_’, ‘-“."
"However, Chinese has about 4000 frequent characters."
The imbalance of Chinese and English vocabulary sizes results in suboptimal transliteration model estimation.
"Each Chinese character also has a pinyin, the Romanized representation of its pronunciation."
"Segmenting the Chinese pinyin into sequence of Roman letters, we now have comparable vocabulary sizes for both Chinese and English."
"We build a pinyin transliteration model using Chinese-English name translation pairs, and compare its performance with a character transliteration model in Experiment section 5.1."
We applied the extracted name translation spelling variants to the machine translation task.
"Given the name spelling variants, we updated both the translation and the language model, adding variants’ probabilities to the canonical form."
Our baseline MT decoder is a phrase-based decoder as described[REF_CITE].
"Given a source sentence, the decoder tries to find the translation hypothesis with minimum translation cost, which is defined as the log-linear combination of different feature functions, such as translation model cost, language model cost, distortion cost and sentence length cost."
The translation cost includes word translation probability and phrase translation probability.
"Given target name spelling variants { t 1 ,t 2 ,...,t m } for a source name s, here t 1 ,t 2 ,...,t m are sorted based on their lexical translation probabilities, p(t 1 | s) ≥ p(t 2 | s) ≥ ... ≥ p(t m | s)."
"We select t 1 as the canonical spelling, and merge other spellings’ translation probabilities with this one: m p(t 1 | s) = ∑ p(t | s). m j=1"
Other spelling variants get zero probability.
Table 2 shows the updated word translation probabilities for “|Alxwrb”.
"Compared with Figure 1, the translation probabilities from several spelling variants are merged with the canonical form, al-kharrub, which now has the highest probability in the new model. 



"
"The phrase translation table includes source phrases, their target phrase translations and the frequencies of the bilingual phrase pair alignment."
"The phrase translation probabilities are calculated based on their alignment frequencies, which are collected from word aligned parallel data."
"To update the phrase translation table, for each phrase pair including a source name and its spelling variant in the target phrase, we replace the target name with its canonical spelling."
"After the mapping, two target phrases differing only in target names may end up with the identical target phrase, and their alignment frequencies are added."
Phrase translation probabilities are re-estimated with the updated frequencies.
The machine translation decoder uses a language model as a measure of a well-formedness of the output sentence.
"Since the updated translation model can produce only the canonical form of a group of spelling variants, the language model should be updated in that all m-grams ( 1≤ m ≤ N ) that are spelling variants of each other are merged (and their counts added), resulting in the canonical form of the m-gram."
"Two m-grams are considered spelling variants of each other if they contain words t 1i , t 2i ( t 1i ≠ t i2 ) at the same position i in the m-gram, and that t 1i and t 2i belong to the same spelling variant group."
"An easy way to achieve this update is to replace every spelling variant in the original language model training data with its corresponding canonical form, and then build the language model again."
"However, since we do not want to replace words that are not names we need to have a mechanism for detecting names."
"For simplicity, in our experiments we assumed a word is a name if it is capitalized, and we replaced spelling variants with their canonical forms only for words that start with a capital letter."
"Information extraction is a crucial step toward understanding a text, as it identifies the important conceptual objects in a discourse."
"We address here one important and basic task of information extraction: mention detection [Footnote_4] : we call instances of textual references to objects mentions, which can be either named (e.g. John Smith), nominal (the president) or pronominal (e.g. he, she)."
4 We adopt here the ACE[REF_CITE]nomenclature.
"For instance, in the sentence • President John Smith said he has no comments . there are two mentions: John Smith and he."
"Similar to many classical NLP tasks, we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions."
Good performance in many natural language processing tasks has been shown to depend heavily on integrating many sources of informati[REF_CITE].
"We select an exponential classifier, the Maximum Entropy (MaxEnt henceforth) classifier that can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classificati[REF_CITE]."
"In this paper, the MaxEnt model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique[REF_CITE], and it uses a Gaussian prior for regularizati[REF_CITE]."
"In ACE, there are seven possible mention types: person, organization, location, facility, geopolitical entity (GPE), weapon, and vehicle."
Experiments are run on Arabic and English.
Our baseline system achieved very competitive result among systems participating in the[REF_CITE]evaluation.
"It uses a large range of features, including lexical, syntactic, and the output of other information extraction models."
"These features were described in ([REF_CITE]&amp;[REF_CITE]), and are not discussed here."
In this paper we focus on examining the effectiveness of name spelling variants in improving mention detection systems.
"We add a new feature that for each token xi to process we fire its canonical form (class label) C(xi) , representative of name spelling variants of xi ."
"This name spelling variant feature is also used in conjunction with the lexical (e.g., words and morphs in a 3-word window, prefixes and suffixes of length up to 4, stems in a 4-word window for Arabic) and syntactic (POS tags, text chunks) features."
We extracted Arabic-English and English-Arabic name translation variants from sentence-aligned parallel corpora released by LDC.
The accuracy of the extracted name translation spelling variants are judged by proficient Arabic and Chinese speakers.
"The Arabic-English parallel corpora include 5.6M sentence pairs, 845K unique Arabic words and 403K unique English words."
"We trained a word translation model by running HMM alignment on the parallel data, grouped target translation with similar spellings and computed the average transliteration cost between the Arabic word and each English word in the translation clusters according to Formula 2.1."
"We sorted the name translation groups according to their transliteration costs, and selected 300 samples at different ranking position for evaluation (20 samples at each ranking position)."
"The quality of the name translation variants are judged as follows: for each candidate name translation group {t 1 ,t 2 ,...,t m | s} , if the source word s is a name and all the target spelling variants are correct translations, it gets a credit of 1."
"If s is not a name, the credit is 0."
"If s is a name but only part of the target spelling variants are correct, it gets partial credit n/m, where n is the number of correct target translations."
We evaluate only the precision of the extracted spelling variants [Footnote_5] .
"5 Evaluating recall requires one to manually look through the space of all possible transliterations (hundreds of thousands of entries), which is impractical."
"As seen in Figure 2, the precision of the top 22K A-E name translations is 96.9%."
The precision gets lower and lower when more non-name Arabic words are included.
"On average, each Arabic name has 2.47 English spelling variants, although there are some names with more than 10 spelling variants."
"Switching the source and target languages, we obtained English-Arabic name spelling variants, i.e., one English name with multiple Arabic spellings."
"As seen in Figure 3, top 20K E-A name pairs are obtained with a precision above 87.9%, and each English name has 3.3 Arabic spellings on average."
"Table 3 shows some A-E and E-A name spelling variants, where Arabic words are represented in their Romanized form."
"We conduct a similar experiment on the Chinese-English language pair, extracting Chinese-English and English-Chinese name spelling variants from 8.7M Chinese-English sentence pairs."
"After word segmentation, the Chinese vocabulary size is 1.5M words, and English vocabulary size is 1.4M words."
"Chinese pinyin transliteration model, we extract 64K C-E name spelling variants with 93.6% precision."
Figure 4 also shows the precision curve of the Chinese character transliteration model.
On average the pinyin transliteration model has about 6% higher precision than the character transliteration model.
"The pinyin transliteration model is particularly better on the tail of the curve, extracting more C-E transliteration variants."
"Figure 5 shows the precision curve for E-C name spelling variants, where 20K name pairs are extracted using letter-to-character transliteration model, and obtaining a precision of 74.3%."
Table 4 shows some C-E and E-C name spelling variants.
We observed errors due to word segmentation.
"For example, the last two Chinese words corresponding to “drenica” have additional Chinese characters, meaning “drenica region” and “drenica river”."
"Similarly for tenet, the last two Chinese words also have segmentation errors due to missing or spurious characters."
"Note that in the C-E spelling variants, the source word “ ” has 14 spelling567 variants."
"Judge solely from the spelling, it is hard to tell whether they are the same person name with different spellings."
We apply the Arabic-English name spelling variants on the machine translation task.
"Our baseline system is trained with 5.6M Arabic-English sentence pairs, the same training data used to extract A-E spelling variants."
The language model is a modified Kneser-Ney 5-gram model trained on roughly 3.5 billion words.
"After pruning (using count cutoffs), it contains a total of 935 million N-grams."
We updated the translation models and the language model with the name spelling variant class.
"Table 5 shows a Romanized Arabic sentence, the translation output from the baseline system and the output from the updated models."
"In the baseline system output, the Arabic name “Alxrwb” was incorrectly translated into “regional”."
"This error was fixed in the updated model, where both translation and language models assign higher probabilities to the correct translation “al-kharroub” after spelling variant normalization."
We also evaluated the updated MT models on a MT test set.
The test set includes 70 documents selected[REF_CITE]Development set.
MT results are evaluated against one reference human translation using BLEU (Papineni et. al. 2001) and TER (Snover et. al. 2006) scores.
The results using the baseline decoder and the updated models are shown in Table 6.
Applying the updated language model (ULM) and the translation model (UTM) lead to a small reduction in TER.
"After we apply similar name spelling normalization on the reference translation, we observed some additional improvements."
"Overall, the BLEU score is increased by 0.1 BLEU point and TER is reduced by 0.26."
Although the significance of correct name translation can not be fully represented by
"BLEU and TER scores 6 , we still want to understand the reason of the relatively small improvement."
"After some error analysis, we found that in the testset only 2.5% of Arabic words are names with English spelling variants."
"Because the MT system is trained on the same bilingual data from which the name spelling variants are extracted, some of these Arabic names are already correctly translated in the baseline system."
So the room of improvement is small.
"We did an oracle experiment, manually correcting the name translation errors in the first 10 documents (89 sentences with 2545 words)."
"With only [Footnote_6] name translation errors corrected, this reduced the[REF_CITE].83 to 48.65."
"6 These scores treat information bearing words, like names, the same as any other words, like punctuations."
Mention detection system experiments are conducted on the[REF_CITE]data sets in Arabic and English.
"Since the evaluation test set is not publicly available, we have split the publicly available training corpus into an 85%/15% data split."
"To facilitate future comparisons with work presented here, and to simulate a realistic scenario, the splits are created based on article dates: the test data is selected as the latest 15% of the data in chronological order."
"This way, the documents in the training and test data sets do not overlap in time, and the content of the test data is more recent than the training data."
"For English we use 499 documents for training and 100 documents for testing, while for Arabic we use 323 documents for training and 56 documents for testing."
"English and Arabic mention detection systems are using a large range of features, including lexical (e.g., words and morphs in a 3-word window, prefixes and suffixes of length up to 4, stems in a 4-word window for Arabic), syntactic (POS tags, text chunks), and the output of other information extraction models."
These features were described in ([REF_CITE]&amp;[REF_CITE]) with more details.
Our goal here is to investigate the effectiveness of name spelling variants information in improving mention detection system performance.
"Results in Table 7 show that the use of name spelling variants (NSV) improves mention detection systems performance, especially for English; an interesting improvement is obtained in recall – which is to be expected, given the method –, but also in precision, leading to systems with better performance in terms of F-measure (82.4 vs. 82.7)."
This improvement in performance is statistically significant according to the stratified bootstrap re-sampling approach[REF_CITE].
This approach is used in the named entity recognition shared task[REF_CITE]7 .
"However, the small improvement obtained for Arabic is not statistically significant based on the approach described earlier."
One hypothesis is that Arabic name spelling variants are not rich enough and that a better tuning of the alignment score is required to improve precision.
We proposed a cross-lingual name spelling variants extraction technique.
We extracted tens of thousands of high precision bilingual name translation spelling variants.
"We applied the spelling variants to the IE task, observing statistically significant improvements over a strong baseline system."
"We also applied the spelling variants to MT task and even though the overall improvement is relatively small, it achieves performance close to the one observed in an oracle experiment."
This paper introduces a new kernel which computes similarity between two natural lan-guage sentences as the number of paths shared by their dependency trees.
The paper gives a very efficient algorithm to compute it.
This kernel is also an improvement over the word subsequence kernel because it only counts linguistically meaningful word subsequences which are based on word dependencies.
It overcomes some of the difficulties encoun-tered by syntactic tree kernels as well.
Ex-perimental results demonstrate the advantage of this kernel over word subsequence and syn-tactic tree kernels.
Kernel-based learning methods[REF_CITE]are becoming increasingly popular in natural language processing (NLP) because they allow one to work with potentially infinite number of features with-out explicitly constructing or manipulating them.
"In most NLP problems, the data is present in structured forms, like strings or trees, and this structural infor-mation can be effectively passed to a kernel-based learning algorithm using an appropriate kernel, like a string kernel[REF_CITE]or a tree kernel[REF_CITE]."
"In contrast, feature-based methods require reducing the data to a pre-defined set of features often leading to some loss of the use-ful structural information present in the data."
A kernel is a measure of similarity between ev-ery pair of examples in the data and a kernel-based machine learning algorithm accesses the data only through these kernel values.
"For example, the string kernel[REF_CITE]computes the similarity between two natural lan-guage strings as the number of common word sub-sequences between them."
A subsequence allows gaps between the common words which are penal-ized according to a parameter.
Each word subse-quence hence becomes an implicit feature used by the kernel-based machine learning algorithm.
A problem with this kernel is that many of these word subsequences common between two strings may not be semantically expressive or linguistically mean-ingful [Footnote_1] .
1[REF_CITE]use character subsequences instead of word subsequences which are even less meaningful.
"Another problem with this kernel is that if there are long-range dependencies between the words in a common word subsequence, then they will unfairly get heavily penalized because of the presence of word gaps."
The syntactic tree kernel presented[REF_CITE]captures the structural similarity be-tween two syntactic trees as the number of syntac-tic subtrees common between them.
"However, of-ten syntactic parse trees may share syntactic sub-trees which correspond to very different semantics based on what words they represent in the sentence."
"On the other hand, some subtrees may differ syn-tactically but may represent similar underlying se-mantics."
These differences can become particularly problematic if the tree kernel is to be used for tasks which require semantic processing.
This paper presents a new kernel which computes similarity between two sentences as the the number of paths common between their dependency trees. (a) A fat cat was chased by a dog. (b) A cat with a red collar was chased two days ago by a fat dog.
It improves over the word subsequence kernel be-cause it only counts the word subsequences which are linked by dependencies.
It also circumvents some of the difficulties encountered with the syntac-tic tree kernel when applied for semantic processing tasks.
"Although several dependency-tree-based kernels and modifications to syntactic tree kernels have been proposed which we briefly discuss in the Related Work section, to our best knowledge no previous work has presented a kernel based on dependency paths which offers some unique advantages."
We also give a very efficient algorithm to compute this ker-nel.
We present experimental results on the task of domain-specific semantic parsing demonstrating the advantage of this kernel over word subsequence and syntactic tree kernels.
The following section gives some background on string and tree kernels.
Section 3 then introduces the dependency-based word subsequence kernel and gives an efficient algorithm to compute it.
"Some of the related work is discussed next, followed by ex-periments, future work and conclusions."
A kernel between two sentences measures the simi-larity between them.
This was extended[REF_CITE]to the number of common word subsequences be-tween them.
We will refer to this kernel as the word subsequence kernel.
Consider the two sentences shown in Figure 1.
"Some common word subsequences between them are “a cat”, “was chased by”, “by a dog”, “a cat chased by a dog”, etc."
Note that the subsequence “was chased by” is present in the second sentence but it requires skipping the words “two days ago” or has a gap of three words present in it.
"The kernel downweights the presence of gaps by a decay fac-tor λǫ(0, 1]."
"If g 1 and g 2 are the sum totals of gaps for a subsequence present in the two sentences re-spectively, then the contribution of this subsequence towards the kernel value will be λ g 1 +g 2 ."
"The ker-nel can be normalized to have values in the range of [0,1] to remove any bias due to different sen-tence lengths."
"Subsequence kernels have been used with success in NLP for text classificati[REF_CITE], informa-tion extracti[REF_CITE]and semantic parsing[REF_CITE]."
"There are, however, some shortcomings of this word subsequence kernel as a measure of similarity between two sentences."
"Firstly, since it considers all possible common subsequences, it is not sensitive to whether the subsequence is linguistically mean-ingful or not."
"For example, the meaningless sub-sequences “cat was by” and “a was a” will also be considered common between the two sentences by this kernel."
"Since these subsequences will be used as implicit features by the kernel-based machine learn-ing algorithm, their presence can only hurt the per-formance."
"Secondly, if there are long distance de-pendencies between the words of the subsequence present in a sentence then the subsequence will get unfairly penalized."
"For example, the most important word subsequence shared between the two sentences shown in Figure 1 is “a cat was chased by a dog” which will get penalized by total gap of eight words coming from the second sentence and a gap of one word from the first sentence."
"Finally, the kernel is not sensitive to the relations between the words, for example, the kernel will consider “a fat dog” as a common subsequence although in the first sentence “a fat” relates to the cat and not to the dog."
Syntactic tree kernels were first introduced[REF_CITE]and were also used by
They define a kernel between two trees as the number of subtrees shared between them.
"A subtree is defined as any subgraph of the tree which includes more than one node, with the restriction that entire productions must be included at every node."
The kernel defined this way captures most of the structural information present in the syntac-tic parse trees in the form of tree fragments which the kernelized learning algorithms can then implic-itly use as features.
"The kernel can be computed in O(|N 1 ||N 2 |) time, where |N 1 | and |N 2 | are the number of nodes of the two trees."
An efficient al-gorithm to compute tree kernels was given[REF_CITE]which runs in close to linear time in the size of the input trees.
"One drawback of this tree kernel, though, partic-ularly when used for any task requiring semantic processing, is that it may match syntactic subtrees between two trees even though they represent very dissimilar things in the sentence."
"For example, be-tween the syntactic parse trees shown in Figures 2 and 3 for the two sentences shown in Figure 1, the syntactic tree kernel will find (NP (DT a) JJ NN) as a common subtree but in the first sentence it represents “cat” while in the second it represents “collar” and “dog”."
It will also find “(NP (DT a) (JJ fat) NN)” as a common subtree which again refers to “cat” in the first sentence and “dog” in the second sentence.
"As another example, consider two simple sentences: (S (NP Chip) (VP (V saw) (NP Dale))) and (S (NP Mary) (VP (V heard) (NP Sally)))."
"Even though se-mantically nothing is similar between them, the syn-tactic tree kernel will still find common subtrees (S NP VP), (VP N NP) and (S NP (VP V NP))."
"The underlying problem is that the syntactic tree kernel tends to overlook the words of the sentences which, in fact, carry the essential semantics."
"On the other hand, although (NP (DT a) (NN cat)) and (NP (DT a) (JJ fat) (NN cat)) represent very similar concepts but the kernel will not capture this high level sim-ilarity between the two constituents, and will only find (DT a) and (NN cat) as the common substruc-tures."
"Finally, the most important similarity between the two sentences is “a cat was chased by a dog” which will not be captured by this kernel because there is no common subtree which covers it."
The Related Work section discusses some modifications that have been proposed to the syntactic tree kernel.
A dependency tree encodes functional relationships between the words in a sentence[REF_CITE].
The words of the sentence are the nodes and if a word complements or modifies another word then there is a child to parent edge from the first word to the second word.
Every word in a dependency tree has exactly one parent except for the root word.
Fig-ures 4 and 5 show dependency trees for the two sen-tences shown in Figure 1.
There has been a lot of progress in learning dependency tree parsers[REF_CITE].
They can also be obtained indirectly from syntactic parse trees utilizing the head words of the constituents.
"We introduce a new kernel which takes the words into account like the word-subsequence kernel and also takes the syntactic relations between them into account like the syntactic tree kernel, however, it does not have the shortcomings of the two kernels pointed out in the previous section."
This kernel counts the number of common paths between the de-pendency trees of the two sentences.
Another way to look at this kernel is that it counts all the common word subsequences which are linked by dependen-cies.
Hence we will call it a dependency-based word subsequence kernel.
"Since the implicit features it uses are dependency paths which are enumerable, it is a well defined kernel."
"In other words, an example gets implicitly mapped to the feature space in which each dependency path is a dimension."
"The dependency-based word subsequence kernel will find the common paths ‘a → cat’, ‘cat → was ← chased’, ‘chased ← by ← dog’ among many oth-ers between the dependency trees shown in Figures 4 and 5."
The arrows are always shown from child node to the parent node.
A common path takes into ac-count the direction between the words as well.
Also note that it will find the important subsequence ‘a → cat → was ← chased ← by ← dog ← a’ as a common path.
It can be seen that the word subsequences this kernel considers as common paths are linguistically meaningful.
It is also not affected by long-range de-pendencies between words because those words are always directly linked in a dependency tree.
There is no need to allow gaps in this kernel either because related words are always linked.
It also won’t find ‘a fat’ as a common path because in the first tree “cat” is between the two words and in the second sentence “dog” is between them.
Thus it does not have the shortcomings of the word subsequence ker-nel.
It also avoids the shortcomings of the syntac-tic tree kernel because the common paths are words themselves and syntactic labels do not interfere in capturing the similarity between the two sentences.
It will not find anything common between depen-dency trees for the sentences “Chip saw Dale” and “Mary heard Sally”.
But it will find ‘a → cat’ as a common path between “a cat” and “a fat cat”.
"We however note that this kernel does not use general syntactic categories, unlike the syntactic tree kernel, which will limit its applicability to the tasks which depend on the syntactic categories, like re-ranking syntactic parse trees."
We now give an efficient algorithm to compute all the common paths between two trees.
"To our best knowledge, no previous work has considered this problem."
The key observation for this algo-rithm is that a path in a tree always has a structure in which nodes (possibly none) go up to a highest node followed by nodes (possibly none) coming down.
Based on this observation we compute two quanti-ties for every pair of nodes between the two trees.
"We call the first quantity common downward paths (CDP ) between two nodes, one from each tree, and it counts the number of common paths between the two trees which originate from those two nodes and which always go downward."
"For example, the com-mon downward paths between the ‘chased’ node of the tree in Figure 4 and the ‘chased’ node of the tree in Figure 5 are ‘chased ← by’, ‘chased ← by ← dog’ and ‘chased ← by ← dog ← a’."
"Hence CDP(chased,chased) = 3."
A word may occur multiple times in a sentence so the CDP values will be computed separately for each occurrence.
We will shortly give a fast recursive algorithm to com-pute CDP values.
"Once these CDP values are known, using these the second quantity is computed which we call com-mon peak paths (CPP) between every two nodes, one from each tree."
"This counts the number of com-mon paths between the two trees which peak at those two nodes, i.e. these nodes are the highest nodes in those paths."
"For example, ‘was’ is the peak for the path ‘a → cat → was ← chased’."
"Since every com-mon path between the two trees has a unique highest node, once these CP P values have been computed, the number of common paths between the two trees is simply the sum of all these CP P values."
We now describe how all these values are effi-ciently computed.
"The CDP values between every two nodes n 1 and n 2 of the trees T 1 and T 2 respec-tively, is recursively computed as follows:"
"CDP (n 1 , n 2 ) = 0 if n 1 .w 6= n 2 .w otherwise,"
"CDP (n 1 , n 2 ) = X (1 + CDP (c 1 , c 2 )) c 1 ǫC(n 1 ) c 2 ǫC(n 2 ) c 1 .w = c 2 .w"
"In the first equation, n.w stands for the word at the node n. If the words are not equal then there cannot be any common downward paths originating from the nodes."
"In the second equation, C(n) rep-resents the set of children nodes of the node n in a tree."
"If the words at two children nodes are the same, then the number of common downward paths from the parent will include all the common downward paths at the two children nodes incremented with the link from the parent to the children."
In addition the path from parent to the child node is also a common downward path.
"For example, in the trees shown in Figures 4 and 5, the nodes with word ‘was’ have ‘chased’ as a common child."
"Hence all the common downward paths originating from ‘chased’ (namely ‘chased ← by’, ‘chased ← by ← dog’ and ‘chased ← by ← dog ← a’) when incremented with ‘was ← chased’ become common downward paths orig-inating from ‘was’."
"In addition, the path ‘was ← chased’ itself is a common downward path."
"Since ‘cat’ is also a common child at ‘was’, it’s common downward paths will also be added."
The CDP values thus computed are then used to compute the CP P values as follows:
"CP P (n 1 , n 2 ) = 0 if n 1 .w 6= n 2 .w otherwise,"
"CP P (n 1 , n 2 ) = CDP (n 1 , n 2 ) + X ( 1 +CDPCDP(c(c 1 ,c2) +"
"CDP(cˆ 1 ,cˆ 2 )+ ) 1 ,c 2 ) ∗ CDP(cˆ 1 ,cˆ 2 ) c 1 , cˆ 1 ǫC(n 1 ) c 2 , cˆ 2 ǫC(n 2 ) c 1 .w = c 2 .w cˆ 1 .w = cˆ 2 .w"
If the two nodes are not equal then the number of common paths that peak at them will be zero.
"If the nodes are equal, then all the common downward paths between them will also be the paths that peak at them, hence it is the first term in the above equa-tion."
"Next, the remaining paths that peak at them can be counted by considering every pair of common children nodes represented by c 1 &amp; c 2 and cˆ 1 &amp; cˆ 2 ."
"For example, for the common node ‘was’ in Figures 4 and 5, the children nodes ‘cat’ and ‘chased’ are common."
"The path ‘cat → was ← chased’ is a path that peaks at ‘was’, hence 1 is added in the second term."
All the downward paths from ‘cat’ when in-cremented up to ‘was’ and down to ‘chased’ are also the paths that peak at ‘was’ (namely ‘a → cat → was ← chased’).
"Similarly, all the downward paths from ‘chased’ when incremented up to ‘was’ and down to ‘cat’ are also paths that peak at ‘was’ (‘cat → was ← chased ← by’, ‘cat → was ← chased ← by ← dog’, etc.)."
Hence the next two terms are present in the equation.
"Finally, all the downward paths from ‘cat’ when incremented up to ‘was’ and down to ev-ery downward path from ‘chased’ are also the paths that peak at ‘was’ (‘a → cat → was ← chased ← by’, ‘a → cat → was ← chased ← by ← dog’ etc.)."
Hence there is the product term present in the equa-tion.
It is important not to re-count a path from the opposite direction hence the two pairs of common children are considered only once (i.e. not reconsid-ered symmetrically).
The dependency word subsequence kernel be-tween two dependency trees T 1 and T 2 is then sim-ply:
"K(T 1 , T 2 ) ="
"X (1 + CP P (n 1 , n 2 )) n 1 ǫT 1 n 2 ǫT 2 n 1 .w = n 2 .w"
"We also want to count the number of common words between the two trees in addition to the num-ber of common paths, hence 1 is added in the equa-tion."
The kernel is normalized to remove any bias due to different tree sizes:
"K(T 1 , T 2 ) K normalized (T 1 , T 2 ) = p K(T 1 , T 1 ) ∗ K(T 2 , T 2 )"
"Since for any long path common between two trees, there will be many shorter paths within it which will be also common between the two trees, it is reasonable to downweight the contribution of long paths."
"We do this by introducing a parameter αǫ(0, 1] and by downweighting a path of length l by α l ."
A similar mechanism was also used in the syn-tactic tree kernel[REF_CITE].
The equations for computing CDP and CPP are accordingly modified as follows to accommodate this downweighting.
"CDP (n 1 , n 2 ) = 0 if n 1 .w 6= n 2 .w otherwise,"
"CDP (n 1 , n 2 ) = X (α + α ∗ CDP (c 1 , c 2 )) c 1 ǫC(n 1 ) c 2 ǫC(n 2 ) c 1 .w = c 2 .w"
"CP P (n 1 , n 2 ) = 0 if n 1 .w =6 n 2 .w otherwise,"
"CP P (n 1 , n 2 ) = CDP (n 1 , n 2 ) + α 2 + α ∗ CDP(c 1 ,c[Footnote_2])+ X ( ) α ∗ CDP(cˆ 1 ,cˆ 2 )+ α 2 ∗ CDP(c 1 ,c 2 ) ∗ CDP(cˆ 1 , cˆ 2 ) c 1 , cˆ 1 ǫC(n 1 ) c 2 , cˆ 2 ǫC(n 2 ) c 1 .w = c 2 .w cˆ 1 .w = cˆ 2 .w"
2 This analysis uses the fact that any node in a tree on average has O(1) number of children.
"This algorithm to compute all the common paths between two trees has worst time complexity of O(|T 1 ||T 2 |), where |T 1 | and |T 2 | are the number of nodes of the two trees T 1 and T 2 respectively."
This is because CDP computations are needed for every pairs of nodes between the two trees and is recur-sively computed.
Using dynamic programming their recomputations can be easily avoided.
The CPP computations then simply add the CDP values 2 .
If the nodes common between the two trees are sparse then the algorithm will run much faster.
"Since the algorithm only needs to store the CDP values, its space complexity is O(|T 1 ||T 2 |)."
Also note that this algorithm computes the number of common paths of all lengths unlike the word subsequence kernel in which the maximum subsequence length needs to be specified and the time complexity then depends on this length.
Several modifications to the syntactic tree kernels have been proposed to overcome the type of prob-lems pointed out in Subsection 2.2.
"For example, their kernel will be able to match the subtrees (NP (DT a) (NN cat)) and (NP (DT a ) (JJ fat) (NN cat)) with some penalty."
We however note that the paths in this kernel link predicates and their arguments and are very different from general paths in a tree that our dependency-based word subsequence kernel uses.
A few kernels based on dependency trees have also been proposed.
This tree kernel was slightly generalized[REF_CITE]to com-pute similarity between two dependency trees.
"In addition to the words, this kernel also incorporates word classes into the kernel."
The kernel is based on counting matching subsequences of children of matching nodes.
"But as was also noted[REF_CITE], this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either."
"In contrast, our dependency-based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths."
"Their kernel is also very time consuming and in their more general sparse setting it requires O(mn 3 ) time and O(mn 2 ) space, where m and n are the number of nodes of the two trees (m &gt;= n)[REF_CITE]."
"Their ker-nel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sen-tences."
This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.
Their kernel also uses word classes in addition to the words them-selves.
We show that the new dependency-based word sub-sequence kernel performs better than word subse-quence kernel and syntactic tree kernel on the task of domain-specific semantic parsing.
"Semantic parsing is the task of converting natu-ral language sentences into their domain-specific complete formal meaning representations which an application can execute, for example, to answer database queries or to control a robot."
A learn-ing system for semantic parsing induces a seman-tic parser from the training data of natural language sentences paired with their respective meaning rep-resentations.
K RISP[REF_CITE]is a semantic parser learning system which uses word subsequence kernel based SVM[REF_CITE]classifiers and was shown to be robust to noise compared to other semantic parser learners.
The system learns an SVM classi-fier for every production of the meaning representa-tion grammar which tells the probability with which a substring of the sentence represents the semantic concept of the production.
Using these classifiers a complete meaning representation of an input sen-tence is obtained by finding the most probable parse which covers the whole sentence.
For details please refer[REF_CITE].
The key operation in K RISP is to find the sim-ilarity between any two substrings of two natural language sentences.
Word subsequence kernel was employed[REF_CITE]to compute the similarity between two substrings.
We modi-fied K RISP so that the similarity between two sub-strings can also be computed using the syntactic tree kernel and the dependency-based word subsequence kernel.
"For applying the syntactic tree kernel, the syntactic subtree over a substring of a sentence is de-termined from the syntactic tree of the sentence by finding the lowest common ancestor of the words in this substring and then considering the smallest sub-tree rooted at this node which includes all the words of the substring."
"For applying the dependency-based word subsequence kernel to two substrings of a sen-tence, the kernel computation was suitably modified so that the common paths between the two depen- dency trees always begin and end with the words present in the substrings."
This is achieved by in-cluding only those downward paths in computations of CDP which end with words within the given substrings.
These paths relate the words within the substrings perhaps using words outside of these sub-strings.
We measure the performance of K RISP obtained us-ing the three types of kernels on the G EOQUERY corpus which has been used previously by several semantic parsing learning systems.
"Since the purpose of the experiments is to compare different kernels and not different seman-tic parsers, we do not compare the performance with other semantic parser learning systems."
The train-ing and testing was done using standard 10-fold cross-validation and the performance was measured in terms of precision (the percentage of generated meaning representations that were correct) and re-call (the percentage of all sentences for which cor-rect meaning representations were obtained).
"Since K RISP assigns confidences to the meaning represen-tations it outputs, an entire range of precision-recall trade-off can be obtained."
We measure the best F-measure (harmonic mean of precision and recall) ob-tained when the system is trained using increasing amounts of training data.
"Since we were not interested in the accuracy of dependency trees or syntactic trees but in the com-parison between various kernels, we worked with gold-standard syntactic trees."
We did not have gold-standard dependency trees available for this cor-pus so we obtained them indirectly from the gold-standard syntactic trees using the head-rules[REF_CITE].
We however note that accurate syn-tactic trees can be obtained by training a syntac-tic parser on WSJ treebank and gold-standard parse trees of some domain-specific sentences[REF_CITE].
"In the experiments, the α parameter of the dependency-based word subsequence kernel was set to 0.25, the λ parameter of the word subsequence kernel was fixed to 0.75 and the downweighting pa- rameter for the syntactic tree kernel was fixed to 0.4."
These were determined through pilot experiments with a smaller portion of the data set.
"The maxi-mum length of subsequences required by the word subsequence kernel was fixed to 3, a longer length was not found to improve the performance and was only increasing the running time."
Table 1 shows the results.
The dependency-based word subsequence kernel always performs better than the syntactic tree kernel.
All the numbers under the dependency kernel were found statisti-cally significant (p &lt; 0.05) over the correspond-ing numbers under the syntactic tree kernel based on paired t-tests.
"The improvement of the dependency-based word subsequence kernel over the word sub-sequence kernel is greater with less training data, showing that the dependency information is more useful when the training data is limited."
The per-formance converges with higher amounts of training data.
The numbers shown in bold were found statis-tically significant over the corresponding numbers under the word subsequence kernel.
It may be noted that syntactic tree kernel is mostly doing worse than the word subsequence kernel.
We believe this is because of the shortcomings of the syntactic tree kernel pointed out in Subsection 2.2.
"Since this is a semantic processing task, the words play an important role and the generalized syntactic categories are not very helpful."
"In future, the dependency-based word subsequence kernel could be extended to incorporate word classes like the kernels presented[REF_CITE]."
It should be possible to achieve this by incorporating matches between word classes in addition to the exact word matches in the kernel computations similar to the way in which the word subsequence kernel was extended to incorpo-rate word classes[REF_CITE].
This will generalize the kernel and make it more ro-bust to data sparsity.
"The dependency-based word subsequence kernel could be tested on other tasks which require comput-ing similarity between sentences or texts, like text classification, paraphrasing, summarization etc."
We believe this kernel will help improve performance on those tasks.
We introduced a new kernel which finds similarity between two sentences as the number of common paths shared between their dependency trees.
This kernel can also be looked upon as an improved word subsequence kernels which only counts the common word subsequences which are related by dependen-cies.
We also gave an efficient algorithm to compute this kernel.
The kernel was shown to out-perform the word subsequence kernel and the syntactic tree kernel on the task of semantic parsing.
"Journal of Machine Learning Research,"
ing ltag based features in parse reranking.
In Proc. of
Lexical gaps between queries and questions (documents) have been a major issue in ques-tion retrieval on large online question and answer (Q&amp;A) collections.
Previous stud-ies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques.
"However, since it is possible for unimpor-tant words (e.g., non-topical words, common words) to be included in the translation mod-els, a lack of noise control on the models can cause degradation of retrieval performance."
This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation mod-els for retrieval purposes.
Experiments con-ducted on a real world Q&amp;A collection show that substantial improvements in retrieval per-formance can be achieved by using compact translation models.
"Community-driven question answering services, such as Yahoo!"
"Answers [URL_CITE] and Live Search QnA [URL_CITE] , have been rapidly gaining popularity among Web users interested in sharing information online."
"By inducing users to collaboratively submit questions and answer questions posed by other users, large amounts of information have been collected in the form of question and answer (Q&amp;A) pairs in recent years."
"This user-generated information is a valu-able resource for many information seekers, because users can acquire information straightforwardly by searching through answered questions that satisfy their information need."
Retrieval models for such Q&amp;A collections should manage to handle the lexical gaps or word mismatches between user questions (queries) and answered questions in the collection.
Consider the two following examples of questions that are seman-tically similar to each other: • “Where can I get cheap airplane tickets?” • “Any travel website for low airfares?”
"Conventional word-based retrieval models would fail to capture the similarity between the two, be-cause they have no words in common."
"To bridge the query-question gap, prior work on Q&amp;A retrieval[REF_CITE]implicitly expands queries with the use of pre-constructed translation models, which lets you generate query words not in a question by trans-lation to alternate words that are related."
"In prac-tice, these translation models are often constructed using statistical machine translation techniques that primarily rely on word co-occurrence statistics ob-tained from parallel strings (e.g., question-answer pairs)."
A critical issue of the translation-based ap-proaches is the quality of translation models con-structed in advance.
"If no noise control is conducted during the construction, it is possible for translation models to contain “unnecessary” translations (i.e., translating a word into an unimportant word, such as a non-topical or common word)."
"In the query expan-sion viewpoint, an attempt to identify and decrease the proportion of unnecessary translations in a trans-lation model may produce an effect of “selective” implicit query expansion and result in improved re-trieval."
"However, prior work on translation-based Q&amp;A retrieval does not recognize this issue and uses the translation model as it is; essentially no attention seems to have been paid to improving the perfor-mance of the translation-based approach by enhanc-ing the quality of translation models."
"In this paper, we explore a number of empiri-cal methods for selecting and eliminating unimpor-tant words from parallel strings to avoid unnecessary translations from being learned in translation models built for retrieval purposes."
"We use the term compact translation models to refer to the resulting models, since the total number of parameters for modeling translations would be minimized naturally."
We also present experiments in which compact translation models are used in Q&amp;A retrieval.
The main goal of our study is to investigate if and how compact trans-lation models can improve the performance of Q&amp;A retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-trieval model and accompanying techniques used to retrieve query-relevant questions.
Section 3 presents a number of empirical ways to select and eliminate unimportant words from parallel strings for training compact translation models.
Section 4 summarizes the compact translation models we built for retrieval experiments.
Section 5 presents and discusses the results of retrieval experiments.
Section 6 presents related works.
"Finally, the last section concludes the paper and discusses future directions."
This section introduces the translation-based lan-guage modeling approach to retrieval that has been used to bridge the lexical gap between queries and already-answered questions in this paper.
"In the basic language modeling framework for re-trieval[REF_CITE], the similarity be-tween a query Q and a document D for ranking may be modeled as the probability of the document lan-guage model M D built from D generating Q: sim(Q, D) ≈ P (Q|M D ) (1)"
"Assuming that query words occur independently given a particular document language model, the query-likelihood P (Q|M D ) is calculated as:"
Y P (Q|M D ) =
P (q|M D ) (2) q∈Q where q represents a query word.
"To avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribu-tion estimated from the entire document collection is widely used in practice:"
Y h P (Q|M D ) = (1 − λ) · P (q|M D ) q∈Q i +λ · P (q|M C ) ([Footnote_3]) where 0 &lt; λ &lt; 1 and M C represents a language model built from the entire collection.
3 The formulation of our retrieval model is basically equiva-lent to the approach[REF_CITE].
The probabil-ities P (w|M D ) and P (w|M C ) are calculated using maximum likelihood estimation.
The basic language modeling framework does not address the issue of lexical gaps between queries and question.
"Assuming that a translation model can be represented by a condi-tional probability distribution of translation T(·|·) between words, we can model P(q|M D ) in Equa-tion 3 as:"
X P (q|M D ) =
T (q|w)P (w|M D ) (4) w∈D where w represents a document word. 3
"The translation probability T(q|w) virtually rep-resents the degree of relationship between query word q and document word w captured in a differ-ent, machine translation setting."
"Then, in the tra-ditional information retrieval viewpoint, the use of translation models produce an implicit query expan-sion effect, since query words not in a document are mapped to related words in the document."
This im-plies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions.
"Obviously, we need to build a translation model in advance."
"Usually the IBM Model 1, developed in the statistical machine translation field[REF_CITE], is used to construct translation models for retrieval purposes in practice."
"Specifically, given a number of parallel strings, the IBM Model 1 learns the translation probability from a source word s to a target word t as:"
"X N T (t|s) = λ s−1 c(t|s; J i ) (5) i where λ s is a normalization factor to make the sum of translation probabilities for the word s equal to 1, N is the number of parallel string pairs, and J i is the ith parallel string pair. c(t|s; J i ) is calculated as: µ ¶ P (t|s) c(t|s; J i ) ="
"P (t|s 1 ) + · · · + P (t|s n ) ×freq t,"
"J i × freq s,J i (6) where {s 1 , . .. , s n } are words in the source text in J i . freq t,J i and freq s,J i are the number of times that t and s occur in J i , respectively."
"Given the initial values of T(t|s), Equations (5) and (6) are used to update T(t|s) repeatedly until the probabilities converge, in an EM-based manner."
Note that the IBM Model 1 solely relies on word co-occurrence statistics obtained from paral-lel strings in order to learn translation probabilities.
"This implies that if parallel strings have unimportant words, a resulted translation model based on IBM Model 1 may contain unimportant words with non-zero translation probabilities."
"We alleviate this drawback by eliminating unim-portant words from parallel strings, avoiding them from being included in the conditional translation probability distribution."
This naturally induces the construction of compact translation models.
The construction of statistical translation models previously discussed requires a corpus consisting of parallel strings.
"Since monolingual parallel texts are generally not available in real world, one must arti-ficially generate a “synthetic” parallel corpus."
"Question and answer as parallel pairs: The simplest approach is to directly employ questions and their answers in the collections by setting ei-ther as source strings and the other as target strings, with the assumption that a question and its cor-responding answer are naturally parallel to each other."
"Formally, if we have a Q&amp;A collection as C = {D 1 , D 2 , . . . , D n }, where D i refers to an ith Q&amp;A data consisting of a question q i and its an-swer a i , we can construct a parallel corpus C 0 as {(q 1 , a 1 ), . . . , (q n , a n )}∪{(a 1 , q 1 ), . . . , (a n , q n )} = C 0 where each element (s, t) refers to a parallel pair consisting of source string s and target string t."
The number of parallel string samples would eventually be twice the size of the collections.
Similar questions as parallel pairs:[REF_CITE]proposed an alternative way of auto-matically collecting a relatively larger set of par-allel strings from Q&amp;A collections.
"Motivated by the observation that many semantically identi-cal questions can be found in typical Q&amp;A collec-tions, they used similarities between answers cal-culated by conventional word-based retrieval mod-els to automatically group questions in a Q&amp;A col-lection as pairs."
"Formally, two question strings q i and q j would be included in a parallel corpus C 0 as {(q i ,q j ),(q j ,q i )} ⊂"
C 0 only if their answer strings a i and a j have a similarity higher than a pre-defined threshold value.
"The similarity is cal-culated as the reverse of the harmonic mean of ranks as sim(a i , a j ) = 12 ( r1 j + 1r i ), where r j and r i refer to the rank of the a j and a i when a i and a j are given as queries, respectively."
"This approach may artificially produce much more parallel string pairs for training the IBM Model 1 than the former approach, depend-ing on the threshold value. [Footnote_4]"
4 We have empirically set the threshold (0.05) for our exper-iments.
"To our knowledge, there has not been any study comparing the effectiveness of the two approaches yet."
"In this paper, we try both approaches and com-pare the effectiveness in retrieval performance."
"We adopt a term weight ranking approach to iden-tify and eliminate unimportant words from parallel strings, assuming that a word in a string is unim- portant if it holds a relatively low significance in the document (Q&amp;A pair) of which the string is origi-nally taken from."
Some issues may arise: • How to assign a weight to each word in a doc-ument for term ranking? • How much to remove as unimportant words from the ranked list?
The following subsections discuss strategies we use to handle each of the issues above.
"In this section, the two different term weighting strategies are introduced. tf-idf:"
The use of tf-idf weighting on evaluating how unimportant a word is to a document seems to be a good idea to begin with.
"We have used the fol-lowing formulas to calculate the weight of word w in document D: tf-idf w,D = tf w,D × idf w (7) tf w,D = freq w,D , idf w = log |C| |D| df w where freq w,D refers to the number of times w oc-curs in D, |D| refers to the size of D (in words), |C| refers to the size of the document collection, and df w refers to the number of documents where w appears."
"Eventually, words with low tf-idf weights may be considered as unimportant."
"The task of term weighting, in fact, has been often applied to the keyword extraction task in natural language processing studies."
"As an alternative term weighting approach, we have used a variant[REF_CITE]’s Tex-tRank, a graph-based ranking model for keyword extraction which achieves state-of-the-art accuracy without the need of deep linguistic knowledge or domain-specific corpora."
"Specifically, the ranking algorithm proceeds as follows."
"First, words in a given document are added as vertices in a graph G."
"Then, edges are added be-tween words (vertices) if the words co-occur in a fixed-sized window."
The number of co-occurrences becomes the weight of an edge.
"When the graph is constructed, the score of each vertex is initialized as 1, and the PageRank-based ranking algorithm is run on the graph iteratively until convergence."
The TextRank score of a word w in document D at kth iteration is defined as follows:
"X R wk ,D = (1 − d) + d · P e i,j R k−1w,D ∀j:(i,j)∈G ∀l:(j,l)∈G e j,l (8) where d is a damping factor usually set to 0.85, and e i,j is an edge weight between i and j."
The assumption behind the use of the variant of TextRank is that a word is likely to be an important word in a document if it co-occurs frequently with other important words in the document.
"Eventually, words with low TextRank scores may be considered as unimportant."
The main differences of TextRank compared to tf-idf is that it utilizes the context infor-mation of words to assign term weights.
Figure 1 demonstrates that term weighting results of TextRank and tf-idf are greatly different.
Notice that TextRank assigns low scores to words that co- occur only with stopwords.
"This implies that Tex-tRank weighs terms more “strictly” than the tf-idf approach, with use of contexts of words."
"Once a final score (either tf-idf or TextRank score) is obtained for each word, we create a list of words ranked in decreasing order of their scores and elim-inate the ones at lower ranks as unimportant words."
The question here is how to decide the proportion or quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-proach we have used is to decide the number of unimportant words based on the size of the original string.
"For our experiments, we manually vary the proportion to be removed as 25%, 50%, and 75%."
"For instance, if the proportion is set to 50% and an original string consists of ten words, at most five words would be remained as important words."
Using average score as threshold: We also have used an alternate approach to deciding the quantity.
"Instead of eliminating a fixed proportion, words are removed if their score is lower than the average score of all words in a document."
This approach decides the proportion to be removed more flexibly than the former approach.
"We have initially built two parallel corpora from a Q&amp;A collection [Footnote_5] , denoted as (QkA) corpus and (QkQ) corpus henceforth, by varying the methods in which parallel strings are gathered (described in Section 2.2)."
5 Details on this data will be introduced in the next section.
"The (QkA) corpus consists of 85,938 parallel string pairs, and the (QkQ) corpus contains 575,649 parallel string pairs."
"In order to build compact translation models, we have preprocessed the parallel corpus using differ-ent word elimination strategies so that unimpor-tant words would be removed from parallel strings."
We have also used a stoplist [URL_CITE] consisting of 429 words to remove stopwords.
The out-of-the-box GIZA++ [URL_CITE][REF_CITE]has been used to learn translation models using the pre-processed par-allel corpus for our retrieval experiments.
"We have also trained initial translation models, using a par-allel corpus from which only the stopwords are re-moved, to compare with the compact translation models."
"Eventually, the number of parameters needed for modeling translations would be minimized if unimportant words are eliminated with different ap- proaches."
"Table 1 and 2 shows the impact of various word elimination strategies on the construction of compact translation models using the (QkA) corpus and the (QkQ) corpus, respectively."
"The two tables report the size of the vocabulary contained and the average number of translations per word in the re-sulting compact translation models, along with per-centage decreases with respect to the initial transla-tion models in which only stopwords are removed."
We make these observations: • The translation models learned from the (QkQ) corpus have less vocabularies but more aver-age translations per word than the ones learned from the (QkA) corpus.
"This result implies that a large amount of noise may have been cre-ated inevitably when a large number of parallel strings (pairs of similar questions) were artifi-cially gathered from the Q&amp;A collection. • The TextRank strategy tends to eliminate larger sets of words as unimportant words than the tf-idf strategy when a fixed proportion is re-moved, regardless of the corpus type."
"Recall that the TextRank approach assigns weights to words more strictly by using contexts of words. • The approach to remove words according to the average weight of a document (denoted as Avg.Score) tends to eliminate relatively larger portions of words as unimportant words than any of the fixed-proportion strategies, regard-less of either the corpus type or the ranking strategy."
Experiments have been conducted on a real world Q&amp;A collection to demonstrate the effectiveness of compact translation models on Q&amp;A retrieval.
"In this section, four experimental settings for the Q&amp;A retrieval experiments are described in detail."
"Data: For the experiments, Q&amp;A data have been collected from the Science domain of Yahoo!"
"An-swers, one of the most popular community-based question answering service on the Web."
"We have obtained a total of 43,001 questions with a best an-swer (selected either by the questioner or by votes of other users) by recursively traversing subcategories of the Science domain, with up to 1,000 question pages retrieved. [Footnote_8]"
8 Yahoo! Answers did not expose additional question pages to external requests at the time of collecting the data.
"Among the obtained Q&amp;A pairs, 32 Q&amp;A pairs have been randomly selected as the test set, and the remaining 42,969 questions have been the reference set to be retrieved."
"Each Q&amp;A pair has three text fields: question title, question content, and answer. [Footnote_9]"
"9 When collecting parallel strings from the Q&amp;A collection, we have put together the question title and the question content as one question string."
"The fields of each Q&amp;A pair in the test set are con-sidered as various test queries; the question title, the question content, and the answer are regarded as a short query, a long query, and a supplementary query, respectively."
We have used long queries and supplementary queries only in the relevance judg-ment procedure.
All retrieval experiments have been conducted using short queries only.
"Relevance judgments: To find relevant Q&amp;A pairs given a short query, we have employed a pool-ing technique used in the TREC conference series."
"We have pooled the top 40 Q&amp;A pairs from each retrieval results generated by varying the retrieval algorithms, the search field, and the query type."
"Popular word-based models, including the[REF_CITE]query-likelihood language model, and pre-vious translation-based models[REF_CITE], have been used. [Footnote_10]"
10 The retrieval model using compact translation models has not been used in the pooling procedure.
Relevance judgments have been done by two stu-dent volunteers (both fluent in English).
"Since many community-based question answering ser-vices present their search results in a hierarchical fashion (i.e. a list of relevant questions is shown first, and then the user chooses a specific question from the list to see its answers), a Q&amp;A pair has been judged as relevant if its question is semantically sim-ilar to the query; neither quality nor rightness of the answer has not been considered."
"When a disagree-ment has been made between two volunteers, one of the authors has made the final judgment."
"As a result, 177 relevant Q&amp;A pairs have been found in total for the 32 short queries."
Baseline retrieval models: The proposed ap- proach to Q&amp;A retrieval using compact translation models (denoted as CTLM henceforth) is compared to three baselines:
"Query-likelihood language model for re-trieval (equivalent to Equation 3, without use of translation models)."
This model represents word-based retrieval models widely used in practice.
TLM(QkQ): Translation-based language model for question retrieval[REF_CITE].
This model uses IBM Model 1 learned from the (QkQ) corpus of which stopwords are removed.
TLM(QkA): A variant of the translation-based ap-proach.
This model uses IBM model 1 learned from the (QkA) corpus.
Evaluation metrics: We have reported the re-trieval performance in terms of Mean Average Pre-cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the precision at each relevant document in the ranking.
"Mean Average Precision is defined as the mean of the Average Precision values across the set of all queries: 1 X 1 X m q MAP (Q) = |Q| Precision(R k ) (9) q∈Q m q k=1 where Q is the set of test queries, m q is the number of relevant documents for a query q, R k is the set of ranked retrieval results from the top until rank posi-tion k, and P recision(R k ) is the fraction of relevant documents in R k[REF_CITE]."
R-Precision is defined as the precision after R documents have been retrieved where R is the number of relevant documents for the current query[REF_CITE].
Mean R-Precision is the mean of the R-Precisions across the set of all queries.
We take MAP as our primary evaluation metric.
Preliminary retrieval experiments have been con-ducted using the baseline QLM and different fields of Q&amp;A data as retrieval unit.
Table 3 shows the effectiveness of each field.
The results imply that the question title field is the most important field in our Yahoo!
Answers collec-tion; this also supports the observation presented by
"Based on the preliminary obser-vations, all retrieval models tested in this paper have ranked Q&amp;A pairs according to the similarity scores between queries and question titles."
Table 4 presents the comparison results of three baseline retrieval models and the proposed CTLMs.
"For each method, the best performance after empir-ical λ parameter tuning according to MAP is pre-sented."
Notice that both the TLMs and CTLMs have out-performed the word-based QLM.
"This implies that word-based models that do not address the issue of lexical gaps between queries and questions often fail to retrieve relevant Q&amp;A data that have little word overlap with queries, as noted[REF_CITE]."
"Moreover, notice that the proposed CTLMs have achieved significantly better performances in all evaluation metrics than both QLM and TLMs, regard-less of the parallel corpus in which the incorporated translation models are trained from."
This is a clear indication that the use of compact translation models built with appropriate word elimination strategies is effective in closing the query-question lexical gaps for improving the performance of question retrieval in the context of language modeling framework.
Note that the retrieval performance varies by the type of training corpus; CTLM(QkA) has outper-formed CTLM(QkQ) significantly.
This proves the statement we made earlier that the (QkQ) corpus would contain much noise since the translation mod-els learned from the (QkQ) corpus tend to have smaller vocabulary sizes but significantly more aver-age translations per word than the ones learned from the (QkA) corpus.
"Table 5 and 6 show the effect of various word elimination strategies on the retrieval performance of CTLMs in which the incorporated compact trans-lation models are trained from the (QkQ) corpus and the (QkA) corpus, respectively."
It is interesting to note that the importance of modifications in word elimination strategies also varies by the type of train-ing corpus.
"The retrieval results indicate that when the trans-lation model is trained from the “less noisy” (QkA) corpus, eliminating a relatively large proportions of words may hurt the retrieval performance of CTLM."
"In the case when the translation model is trained from the “noisy” (QkQ) corpus, a better retrieval performance may be achieved if words are elimi-nated appropriately to a certain extent."
"In terms of weighting scheme, the TextRank ap-proach, which is more “strict” than tf-idf in elim-inating unimportant words, has led comparatively higher retrieval performances on all levels of re-moval quantity when the translation model has been trained from the “noisy” (QkQ) corpus."
"On the con-trary, the “less strict” tf-idf approach has led better performances when the translation model has been trained from the “less noisy” (QkA) corpus."
"In summary, the results imply that the perfor-mance of translation-based retrieval models can be significantly improved when strategies for building of compact translation models are chosen properly, regarding the expected noise level of the parallel cor-pus for training the translation models."
"In a case where a noisy parallel corpus is given for training of translation models, it is better to get rid of noise as much as possible by using “strict” term weight-ing algorithms; when a less noisy parallel corpus is given for building the translation models, a tolerant approach would yield better retrieval performance."
"Our work is most closely related[REF_CITE]’s work, which addresses the issue of word mismatch between queries and questions in large online Q&amp;A collections by using translation-based methods."
"Apart from their work, there have been some related works on applying translation-based methods for retrieving FAQ data."
"Although all of these translation-based approaches are based on the statistical translation models, including the IBM Model 1, none of them focus on addressing the noise issues in translation models."
Bridging the query-question gap has been a major is-sue in retrieval models for large online Q&amp;A collec-tions.
"In this paper, we have shown that the perfor-mance of translation-based retrieval on real online Q&amp;A collections can be significantly improved by using compact translation models of which the noise (unimportant word translations) is properly reduced."
We have also observed that the performance en-hancement may be achieved by choosing the appro-priate strategies regarding the strictness of various term weighting algorithms and the expected noise level of the parallel data for learning such transla-tion models.
Future work will focus on testing the effective-ness of the proposed method on a larger set of Q&amp;A collections with broader domains.
"Since the pro-posed approach cannot handle many-to-one or one-to-many word transformations, we also plan to in-vestigate the effectiveness of phrase-based transla-tion models in closing gaps between queries and questions for further enhancement of Q&amp;A retrieval."
This paper explores the challenge of scaling up language processing algorithms to increas-ingly large datasets.
"While cluster comput-ing has been available in commercial environ-ments for several years, academic researchers have fallen behind in their ability to work on large datasets."
I discuss two barriers contribut-ing to this problem: lack of a suitable pro-gramming model for managing concurrency and difficulty in obtaining access to hardware.
"Hadoop, an open-source implementation of Google’s MapReduce framework, provides a compelling solution to both issues."
"Its simple programming model hides system-level de-tails from the developer, and its ability to run on commodity hardware puts cluster comput-ing within the reach of many academic re-search groups."
This paper illustrates these points with a case study in building word co-occurrence matrices from large corpora.
I con-clude with an analysis of an alternative com-puting model based on renting instead of buy-ing computer clusters.
"Over the past couple of decades, the field of compu-tational linguistics (and more broadly, human lan-guage technologies) has seen the emergence and later dominance of empirical techniques and data-driven research."
Concomitant with this trend is a coherent research thread that focuses on exploiting increasingly-large datasets.
"In fact, they argued that size of training set was perhaps more important than the choice of ma-chine learning algorithm itself."
"Similarly, exper-iments in question answering have shown the ef-fectiveness of simple pattern-matching techniques when applied to large quantities of data[REF_CITE]."
"More recently, this line of argumentation has been echoed in experi-ments with Web-scale language models."
Challenges in scaling algorithms to increasingly-large datasets have become a serious issue for re-searchers.
It is clear that datasets readily available today and the types of analyses that researchers wish to conduct have outgrown the capabilities of individ-ual computers.
"The only practical recourse is to dis-tribute the computation across multiple cores, pro-cessors, or machines."
"The consequences of failing to scale include misleading generalizations on arti-ficially small datasets and limited practical applica-bility in real-world contexts, both undesirable."
This paper focuses on two barriers to develop-ing scalable language processing algorithms: chal-lenges associated with parallel programming and access to hardware.
"Google’s MapReduce frame-work[REF_CITE]provides an at-tractive programming model for developing scal-able algorithms, and with the release of Hadoop, an open-source implementation of MapReduce lead by Yahoo, cost-effective cluster computing is within the reach of most academic research groups."
It is emphasized that this work focuses on large-data algorithms from the perspective of academia— colleagues in commercial environments have long enjoyed the advantages of cluster computing.
"How-ever, it is only recently that such capabilities have become practical for academic research groups."
"These points are illustrated by a case study in build-ing large word co-occurrence matrices, a simple task that underlies many NLP algorithms."
The remainder of the paper is organized as fol-lows: the next section overviews the MapReduce framework and why it provides a compelling solu-tion to the issues sketched above.
"Section 3 intro-duces the task of building word co-occurrence ma-trices, which provides an illustrative case study."
Two separate algorithms are presented in Section 4.
"The experimental setup is described in Section 5, fol-lowed by presentation of results in Section 6."
Im-plications and generalizations are discussed follow-ing that.
"Before concluding, I explore an alternative model of computing based on renting instead of buy-ing hardware, which makes cluster computing prac-tical for everyone."
"The only practical solution to large-data challenges today is to distribute the computation across mul-tiple cores, processors, or machines."
The de-velopment of parallel algorithms involves a num-ber of tradeoffs.
"First is that of cost: a decision must be made between “exotic” hardware (e.g., large shared memory machines, InfiniBand inter-connect) and commodity hardware."
"There is signif-icant evidence[REF_CITE]that solutions based on the latter are more cost effective—and for resource-constrained academic NLP groups, com-modity hardware is often the only practical route."
"Given appropriate hardware, researchers must still contend with the challenge of developing soft-ware."
"Quite simply, parallel programming is diffi-cult."
"Due to communication and synchronization issues, concurrent operations are notoriously chal-lenging to reason about."
Reliability and fault tol-erance become important design considerations on clusters containing large numbers of unreliable com- modity parts.
"With traditional parallel programming models (e.g., MPI), the developer shoulders the bur-den of explicitly managing concurrency."
"As a result, a significant amount of the programmer’s attention is devoted to system-level details, leaving less time for focusing on the actual problem."
"Recently, MapReduce[REF_CITE]has emerged as an attractive alternative to existing parallel programming models."
"The Map-Reduce abstraction shields the programmer from having to explicitly worry about system-level is-sues such as synchronization, inter-process commu-nication, and fault tolerance."
The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics.
This frees the programmer to focus on solving the problem at hand.
"MapReduce builds on the observation that many information processing tasks have the same basic structure: a computation is applied over a large num-ber of records (e.g., Web pages, bitext pairs, or nodes in a graph) to generate partial results, which are then aggregated in some fashion."
"Naturally, the per-record computation and aggregation function vary according to task, but the basic structure remains fixed."
"Taking inspiration from higher-order func-tions in functional programming, MapReduce pro-vides an abstraction at the point of these two opera-tions."
"Specifically, the programmer defines a “map-per” and a “reducer” with the following signatures: map: (k 1 , v 1 ) → [(k 2 , v 2 )] reduce: (k 2 , [v 2 ]) → [(k 3 , v 3 )]"
Key-value pairs form the basic data structure in MapReduce.
The mapper is applied to every input key-value pair to generate an arbitrary number of in-termediate key-value pairs ([. . .] is used to denote a list).
The reducer is applied to all values associated with the same intermediate key to generate output key-value pairs.
This two-stage processing structure is illustrated in Figure 1.
"Under the framework, a programmer needs only to provide implementations of the mapper and re-ducer."
"On top of a distributed file system[REF_CITE], the runtime transparently han-dles all other aspects of execution, on clusters rang-ing from a few to a few thousand nodes."
"The run-time is responsible for scheduling map and reduce workers on commodity hardware assumed to be un-reliable, and thus is tolerant to various faults through a number of error recovery mechanisms."
"In the dis-tributed file system, data blocks are stored on the local disks of machines in the cluster—the Map-Reduce runtime handles the scheduling of mappers on machines where the necessary data resides."
It also manages the potentially very large sorting prob-lem between the map and reduce phases whereby in-termediate key-value pairs must be grouped by key.
"As an optimization, MapReduce supports the use of “combiners”, which are similar to reducers except that they operate directly on the output of mappers (in memory, before intermediate output is written to disk)."
Combiners operate in isolation on each node in the cluster and cannot use partial results from other nodes.
"Since the output of mappers (i.e., the key-value pairs) must ultimately be shuffled to the appropriate reducer over a network, combiners al-low a programmer to aggregate partial results, thus reducing network traffic."
"In cases where an opera-tion is both associative and commutative, reducers can directly serve as combiners."
Google’s proprietary implementation of Map-Reduce is in C++ and not available to the public.
"However, the existence of Hadoop, an open-source implementation in Java spearheaded by Yahoo, al-lows anyone to take advantage of MapReduce."
"The growing popularity of this technology has stimu-lated a flurry of recent work, on applications in ma-chine learning[REF_CITE], machine transla-ti[REF_CITE], and document retrieval[REF_CITE]."
"To illustrate the arguments outlined above, I present a case study using MapReduce to build word co-occurrence matrices from large corpora, a common task in natural language processing."
"Formally, the co-occurrence matrix of a corpus is a square N × N matrix where N corresponds to the number of unique words in the corpus."
A cell m ij contains the number of times word w i co-occurs with word w j within a specific context—a natural unit such as a sentence or a certain window of m words (where m is an application-dependent parameter).
Note that the upper and lower triangles of the matrix are iden-tical since co-occurrence is a symmetric relation.
"This task is quite common in corpus linguistics and provides the starting point to many other algo-rithms, e.g., for computing statistics such as point-wise mutual informati[REF_CITE], for unsupervised sense clustering[REF_CITE], and more generally, a large body of work in lexi-cal semantics based on distributional profiles, dat-ing back[REF_CITE]and[REF_CITE]."
"The task also has applications in information retrieval, e.g.,[REF_CITE], and other related fields as well."
"More gen-erally, this problem relates to the task of estimating distributions of discrete events from a large number of observations (more on this in Section 7)."
"It is obvious that the space requirement for this problem is O(N 2 ), where N is the size of the vocab-ulary, which for real-world English corpora can be hundreds of thousands of words."
"The computation of the word co-occurrence matrix is quite simple if the entire matrix fits into memory—however, in the case where the matrix is too big to fit in memory, a naive implementation can be very slow as mem-ory is paged to disk."
"For large corpora, one needs to optimize disk access and avoid costly seeks."
"As illustrated in the next section, MapReduce handles exactly these issues transparently, allowing the pro-grammer to express the algorithm in a straightfor-ward manner."
"A bit more discussion of the task before mov-ing on: in many applications, researchers have discovered that building the complete word co-occurrence matrix may not be necessary."
"For ex-ample,[REF_CITE]discusses feature selection techniques in defining context vectors;[REF_CITE]present evidence that conceptual distance is better captured via distributional profiles mediated by thesaurus categories."
"These objections, however, miss the point—the focus of this paper is on practical cluster computing for academic re-searchers; this particular task serves merely as an illustrative example."
"In addition, for rapid proto-typing, it may be useful to start with the complete co-occurrence matrix (especially if it can be built ef-ficiently), and then explore how algorithms can be optimized for specific applications and tasks."
This section presents two MapReduce algorithms for building word co-occurrence matrices for large corpora.
"The goal is to illustrate how the prob-lem can be concisely captured in the MapReduce programming model, and how the runtime hides many of the system-level details associated with dis-tributed computing."
"Pseudo-code for the first, more straightforward, algorithm is shown in Figure 2."
Unique document ids and the corresponding texts make up the input key-value pairs.
The mapper takes each input doc-ument and emits intermediate key-value pairs with each co-occurring word pair as the key and the inte-ger one as the value.
"In the pseudo-code, E MIT de-notes the creation of an intermediate key-value pair that is collected (and appropriately sorted) by the MapReduce runtime."
"The reducer simply sums up all the values associated with the same co-occurring word pair, arriving at the absolute counts of the joint event in the corpus (corresponding to each cell in the co-occurrence matrix)."
"For convenience, I refer to this algorithm as the “pairs” approach."
"Since co-occurrence is a symmet-ric relation, it suffices to compute half of the matrix."
"However, for conceptual clarity and to generalize to instances where the relation may not be symmetric, the algorithm computes the entire matrix."
The Java implementation of this algorithm is quite concise—less than fifty lines long.
Notice the Map-Reduce runtime guarantees that all values associated with the same key will be gathered together at the re-duce stage.
"Thus, the programmer does not need to explicitly manage the collection and distribution of partial results across a cluster."
"In addition, the pro-grammer does not need to explicitly partition the in-put data and schedule workers."
"This example shows the extent to which distributed processing can be dominated by system issues, and how an appropriate abstraction can significantly simplify development."
It is immediately obvious that Algorithm 1 gen-erates an immense number of key-value pairs.
"Al-though this can be mitigated with the use of a com-biner (since addition is commutative and associa-tive), the approach still results in a large amount of network traffic."
"An alternative approach is presented in Figure 3, first reported[REF_CITE]."
The major difference is that counts of co-occurring words are first stored in an associative array (H).
The output of the mapper is a number of key-value pairs with words as keys and the corresponding asso-ciative arrays as the values.
"The reducer performs an element-wise sum of all associative arrays with the same key (denoted by the function M ERGE ), thus ac- cumulating counts that correspond to the same cell in the co-occurrence matrix."
"Once again, a com-biner can be used to cut down on the network traffic by merging partial results."
"In the final output, each key-value pair corresponds to a row in the word co-occurrence matrix."
"For convenience, I refer to this as the “stripes” approach."
"Compared to the “pairs” approach, the “stripes” approach results in far fewer intermediate key-value pairs, although each is significantly larger (and there is overhead in serializing and deserializing associa-tive arrays)."
"A critical assumption of the “stripes” approach is that at any point in time, each associa-tive array is small enough to fit into memory (other-wise, memory paging may result in a serious loss of efficiency)."
"This is true for most corpora, since the size of the associative array is bounded by the vo-cabulary size."
Section 6 compares the efficiency of both algorithms. [Footnote_1]
"1 Implementations of both algorithms are included in Cloud 9 , an open source Hadoop library that I have been de-veloping to support research and education, available from my homepage."
"Work reported in this paper used the English Gi-gaword corpus (version 3), 2 which consists of newswire documents from six separate sources, to-taling 7.15 million documents (6.8[REF_CITE].4 GB uncompressed)."
"Some experiments used only documents from the Associated Press World-stream (APW), which contains [Footnote_2].27 million docu-ments (1.8 GB compressed, 5.7 GB uncompressed)."
2 LDC catalog number[REF_CITE]
"By LDC’s count, the entire collection contains ap-proximately [Footnote_2].97 billion words."
2 LDC catalog number[REF_CITE]
"Prior to working with Hadoop, the corpus was first preprocessed."
"All XML markup was removed, followed by tokenization and stopword removal us-ing standard tools from the Lucene search engine."
All tokens were replaced with unique integers for a more efficient encoding.
The data was then packed into a Hadoop-specific binary file format.
"The entire Gigaword corpus took up 4.69 GB in this format; the APW sub-corpus, 1.32 GB."
"Initial experiments used Hadoop version 0.16.0 running on a 20-machine cluster (1 master, 19 slaves)."
This cluster was made available to the Uni- versity of Maryland as part of the Google/IBM Aca-demic Cloud Computing Initiative.
"Each machine has two single-core processors (running at either 2.4 GHz or 2.8 GHz), 4 GB memory."
The cluster has an aggregate storage capacity of 1.7 TB.
"Hadoop ran on top of a virtualization layer, which has a small but measurable impact on performance; see[REF_CITE]."
Section 6 reports experimental results using this cluster; Section 8 explores an alternative model of computing based on “renting cycles”.
"First, I compared the running time of the “pairs” and “stripes” approaches discussed in Section 4."
Run-ning times on the 20-machine cluster are shown in Figure 4 for the APW section of the Gigaword corpus: the x-axis shows different percentages of the sub-corpus (arbitrarily selected) and the y-axis shows running time in seconds.
"For these experi-ments, the co-occurrence window was set to two, i.e., w i is said to co-occur with w j if they are no more than two words apart (after tokenization and stopword removal)."
Results demonstrate that the stripes approach is far more efficient than the pairs approach: 666 sec-onds (11m 6s) compared to 3758 seconds (62m 38s) for the entire APW sub-corpus (improvement by a factor of 5.7).
"On the entire sub-corpus, the map-pers in the pairs approach generated 2.6 billion in-termediate key-value pairs totally 31.2 GB."
"After the combiners, this was reduced to 1.1 billion key-value pairs, which roughly quantifies the amount of data involved in the shuffling and sorting of the keys."
"On the other hand, the mappers in the stripes approach generated 653 million intermediate key-value pairs totally 48.1 GB; after the combiners, only 28.8 mil-lion key-value pairs were left."
"The stripes approach provides more opportunities for combiners to aggre-gate intermediate results, thus greatly reducing net-work traffic in the sort and shuffle phase."
Figure 4 also shows that both algorithms exhibit highly desirable scaling characteristics—linear in the corpus size.
"This is confirmed by a linear regres-sion applied to the running time data, which yields R 2 values close to one."
"Given that the stripes algo-rithm is more efficient, it is used in the remainder of the experiments."
"With a window size of two, computing the word co-occurrence matrix for the entire Gigaword corpus (7.15 million documents) takes 37m 11s on the 20-machine cluster."
Figure 5 shows the running time as a function of window size.
"With a window of six words, running time on the complete Gigaword corpus rises to 1h 23m 45s."
"Once again, the stripes algorithm exhibits the highly desirable characteris-tic of linear scaling in terms of window size, as con-firmed by the linear regression with an R 2 value very close to one."
The elegance of the programming model and good scaling characteristics of resulting implementations make MapReduce a compelling tool for a variety of natural language processing tasks.
"In fact, Map-Reduce excels at a large class of problems in NLP that involves estimating probability distributions of discrete events from a large number of observations according to the maximum likelihood criterion: c(A, B) c(A, B) P MLE (B|A) = = (1) c(A)"
"P c(A, B 0 )"
"In practice, it matters little whether these events are words, syntactic categories, word alignment links, or any construct of interest to researchers."
Ab-solute counts in the stripes algorithm presented in Section 4 can be easily converted into conditional probabilities by a final normalization step.
"Recently,[REF_CITE]used this approach for word align-ment and phrase extraction in statistical machine translation."
"Of course, many applications require smoothing of the estimated distributions—this prob-lem also has known solutions in MapReduce[REF_CITE]."
Synchronization is perhaps the single largest bot-tleneck in distributed computing.
"In MapReduce, this is handled in the shuffling and sorting of key-value pairs between the map and reduce phases."
De-velopment of efficient MapReduce algorithms criti-cally depends on careful control of intermediate out-put.
"Since the network link between different nodes in a cluster is by far the component with the largest latency, any reduction in the size of intermediate output or a reduction in the number of key-value pairs will have significant impact on efficiency."
The central theme of this paper is practical clus-ter computing for NLP researchers in the academic environment.
I have identified two key aspects of what it means to be “practical”: the first is an appro-priate programming model for simplifying concur-rency management; the second is access to hardware resources.
The Hadoop implementation of Map-Reduce addresses the first point and to a large ex-tent the second point as well.
The cluster used for experiments in Section 6 is modest by today’s stan-dards and within the capabilities of many academic research groups.
It is not even a requirement for the computers to be rack-mounted units in a machine room (although that is clearly preferable); there are plenty of descriptions on the Web about Hadoop clusters built from a handful of desktop machines connected by gigabit Ethernet.
"Even without access to hardware, cluster comput-ing remains within the reach of resource-constrained academics. “Utility computing” is an emerging con-cept whereby anyone can provision clusters on de-mand from a third-party provider."
"Instead of up-front capital investment to acquire a cluster and re-occurring maintenance and administration costs, one could “rent” computing cycles as they are needed— this is not a new idea[REF_CITE]."
"One such ser-vice is provided by Amazon, called Elastic Compute Cloud (EC2). [URL_CITE] With EC2, researchers could dynam-ically create a Hadoop cluster on-the-fly and tear down the cluster once experiments are complete."
"To demonstrate the use of this technology, I replicated some of the previous experiments on EC2 to provide a case study of this emerging model of computing."
Virtualized computation units in EC2 are called instances.
"At the time of these experiments, the ba-sic instance offers, according to Amazon, 1.7 GB of memory, 1 EC2 Compute Unit (1 virtual core with 1 EC2 Compute Unit), and 160 GB of instance storage."
Each instance-hour costs $0.10 (all prices given in USD).
"Computational resources are simply charged by the instance-hour, so that a ten-instance cluster for ten hours costs the same as a hundred-instance cluster for one hour (both $10)—the Ama-zon infrastructure allows one to dynamically provi-sion and release resources as necessary."
"This is at- tractive for researchers, who could on a limited basis allocate clusters much larger than they could other-wise afford if forced to purchase the hardware out-right."
"Through virtualization technology, Amazon is able to parcel out allotments of processor cycles while maintaining high overall utilization across a data center and exploiting economies of scale."
"Using EC2, I built word co-occurrence matrices from the entire English Gigaword corpus (window of two) on clusters of various sizes, ranging from 20 slave instances all the way up to 80 slave in-stances."
The entire cluster consists of the slave in-stances plus a master controller instance that serves as the job submission queue; the clusters ran Hadoop version 0.17.0 (the latest release at the time these experiments were conducted).
"Running times are shown in Figure 6 (solid squares), with varying clus-ter sizes on the x-axis."
"Each data point is anno-tated with the cost of running the complete experi-ment. [Footnote_4] Results show that computing the complete word co-occurrence matrix costs, quite literally, a couple of dollars—certainly affordable by any aca-demic researcher without access to hardware."
4 Note that Amazon bills in whole instance-hour increments; these figures assume fractional accounting.
"For reference, Figure 6 also plots the running time of the same experiment on the 20-machine cluster used in Section 6 (which contains 38 worker cores, each roughly comparable to an instance)."
The alternate set of axes in Figure 6 shows the scaling characteristics of various cluster sizes.
"The circles plot the relative size and speedup of the EC2 experiments, with respect to the 20-slave clus-ter."
The results show highly desirable linear scaling characteristics.
The above figures include only the cost of running the instances.
One must additionally pay for band-width when transferring data in and out of EC2.
"At the time these experiments were conducted, Ama-zon charged $0.10 per GB for data transferred in and $0.17 per GB for data transferred out."
"To comple-ment EC2, Amazon offers persistent storage via the Simple Storage Service (S3), [URL_CITE] at a cost of $0.15 per GB per month."
There is no charge for data transfers between EC2 and S3.
The availability of this service means that one can choose between paying for data transfer or paying for persistent storage on a cyclic basis—the tradeoff naturally depends on the amount of data and its permanence.
The cost analysis presented above assumes optimally-efficient use of Amazon’s services; end-to-end cost might better quantify real-world usage conditions.
"In total, the experiments reported in this section resulted in a bill of approximately thirty dol-lars."
The figure includes all costs associated with in-stance usage and data transfer costs.
It also includes time taken to learn the Amazon tools (I previously had no experience with either EC2 or S3) and to run preliminary experiments on smaller datasets (be-fore scaling up to the complete corpus).
"The lack of fractional accounting on instance-hours contributed to the larger-than-expected costs, but such wastage would naturally be reduced with more experiments and higher sustained use."
"Overall, these cost appear to be very reasonable, considering that the largest cluster in these experiments (1 master + 80 slave in-stances) might be too expensive for most academic research groups to own and maintain."
Consider another example that illustrates the pos-sibilities of utility computing.
"Their paper reported experiments on a corpus con-taining 31 billion tokens (about an order of magni-tude larger than the English Gigaword): on 400 ma-chines, the model estimation took 8 hours. [Footnote_6] With EC2, such an experiment would cost a few hundred dollars—sufficiently affordable that availability of data becomes the limiting factor, not computational resources themselves."
"6 Brants et al. were affiliated with Google, so access to hard-ware was not an issue."
The availability of “computing-on-demand” ser-vices and Hadoop make cluster computing practi-cal for academic researchers.
"Although Amazon is currently the most prominent provider of such ser-vices, they are not the sole player in an emerging market—in the future there will be a vibrant market with many competing providers."
"Considering the tradeoffs between “buying” and “renting”, I would recommend the following model for an academic re-search group: purchase a modest cluster for devel-opment and for running smaller experiments; use a computing-on-demand service for scaling up and for running larger experiments (since it would be more difficult to economically justify a large cluster if it does not receive high sustained utilization)."
"If the concept of utility computing takes hold, it would have a significant impact on computer sci-ence research in general: the natural implication is that algorithms should not only be analyzed in tradi-tional terms such as asymptotic complexity, but also in terms of monetary costs, in relationship to dataset and cluster size."
One can argue that cost is a more di-rect and practical measure of algorithmic efficiency.
This paper address two challenges faced by aca-demic research groups in scaling up natural lan-guage processing algorithms to large corpora: the lack of an appropriate programming model for ex-pressing the problem and the difficulty in getting ac-cess to hardware.
"With this case study in building word co-occurrence matrices from large corpora, I demonstrate that MapReduce, via the open source Hadoop implementation, provides a compelling so-lution."
"A large class of algorithms in computa-tional linguistics can be readily expressed in Map-Reduce, and the resulting code can be transparently distributed across commodity clusters."
"Finally, the “cycle-renting” model of computing makes access to large clusters affordable to researchers with lim-ited resources."
"Together, these developments dra-matically lower the entry barrier for academic re-searchers who wish to explore large-data issues."
We propose a novel lexicon acquirer that works in concert with the morphological ana-lyzer and has the ability to run in online mode.
"Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage."
"When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis."
We use the constraints of Japanese morphology and effectively reduce the num-ber of examples required to acquire a mor-pheme.
Experiments show that unknown mor-phemes were acquired with high accuracy and improved the quality of morphological analy-sis.
Morphological analysis is the first step for most nat-ural language processing applications.
"In Japanese morphological analysis, segmentation is processed simultaneously with the assignment of a part of speech (POS) tag to each morpheme."
Segmentation is a nontrivial task in Japanese because it does not delimit words by white-space.
Japanese morphological analysis has successfully adopted dictionary-based approaches[REF_CITE].
"In these approaches, a sentence is trans-formed into a lattice of morphemes by searching a pre-defined dictionary, and an optimal path in the lattice is selected."
"This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99%[REF_CITE]."
"When applied to web texts, however, more errors are made due to unknown morphemes."
"In previous studies, exper-iments were performed on newspaper articles, but web texts include slang words, informal spelling al-ternates[REF_CITE]and technical terms."
"For example, the verb “ ググる ” (gugu-ru, to google) is erroneously segmented into “ ググ ” (gugu) and “ る ” (ru)."
One solution to this problem is to augment the lexicon of the morphological analyzer by extracting unknown morphemes from texts[REF_CITE].
"In the previous method, a morpheme extrac-tion module worked independently of the morpho-logical analyzer and ran in off-line (batch) mode."
It is inefficient because almost all high-frequency morphemes have already been registered to the pre-defined dictionary.
"Moreover, it is inconvenient when applied to web texts because the web corpus is huge and diverse compared to newspaper corpora."
It is not necessarily easy to build subcorpora before lexicon acquisition.
Suppose that we want to ana-lyze whaling-related documents.
It is unnecessary and probably harmful to acquire morphemes that are irrelevant to the topic.
A whaling-related subcorpus should be extracted from the whole corpus but it is not clear how large it must be.
We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode.
"As shown in Fig-ure 1, every time a sentence is analyzed, the lexicon acquirer detects unknown morphemes, enumerates candidates and selects the best candidates by com-paring multiple examples kept in the storage."
"When a morpheme is unambiguously selected, the lexicon acquirer updates the automatically constructed dic-tionary, and it will be used in subsequent analysis."
The proposed method is flexible and gives the sys-tem more control over the process.
We do not have to limit the target corpus beforehand and the system can stop whenever appropriate.
We use the constraints of Japanese morphology that have already been coded in the morphological analyzer.
These constraints effectively reduce the number of examples required to acquire an unknown morpheme.
Experiments show that unknown mor-phemes were acquired with high accuracy and im-proved the quality of morphological analysis.
"In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [URL_CITE]"
"We explain Japanese mor-phemes in Section 2.1, morphological constraints in Section 2.2, and unknown morpheme processing in Section 2.3."
"In JUMAN, the POS tagset consists of four ele-ments: class, subclass, conjugation type and con-jugation form."
"The classes are noun, verb, adjec-tive and others."
"Noun has subclasses such as com-mon noun, sa-group noun, proper noun, organiza- tion, place, personal name."
Verb and adjective have no subclasses.
"Verbs and adjectives among others change their form according to the morphemes that occur after them, which is called conjugation."
"Conjugable mor-phemes are grouped by conjugation types such as vowel verb, ra-row verb, i-type adjective and na-type adjective."
Each conjugable morpheme takes one of conjugation forms in texts.
It has an invari-ant stem and an ending which changes according to conjugation type and conjugation form.
"In this paper, the tuple of class, subclass and con-jugation type is referred to as a POS tag."
"For sim-plicity, POS tags for nouns are called by their sub-classes and those for verbs and adjectives by their conjugation types."
"There are two types of morphemes: abstract dic-tionary entries, and examples or actual occurrences in texts."
"An entry consists of a stem and a POS tag while an example consists of a stem, a POS tag and a conjugation form."
"For example, the entry of the ra-row verb “ 走る ” (hashi-ru, to run) can be repre-sented as (“ 走 ” (hashi), ra-row verb), and their examples “ 走ら ” (hashi-ra) and “ 走り ” (hashi-ri) as (“ 走 ” (hashi), ra-row verb, imperfective), and (“ 走 ” (hashi), ra-row verb, plain continu- ative) respectively."
"As nouns do not conjugate, the entry of the sa-group noun “ 希望 ” (kibou, hope) can be represented as (“ 希望 ” (kibou), sa-group noun) and its sole example form is (“ 希望 ” (kibou), sa-group noun, NIL)."
Japanese is an agglutinative language.
"Depending on its grammatical roles, a morpheme is followed by a sequence of grammatical suffixes, auxiliary verbs and particles, and the connectivity of these elements is bound by morphological constraints."
"For exam-ple, the particle “ を ” (wo, accusative case) can fol-low a verb with the conjugation form of plain contin-uative, as in “ 走りを ” (hashi-ri-wo, running- ACC ), but it cannot follow an imperfective verb (“* 走らを ” (*hashi-ra-wo))."
These constraints are used by JUMAN to reduce the ambiguity.
They can be also used in lexicon ac-quisition.
"Given a sentence, JUMAN builds a lattice of mor-phemes by searching a pre-defined dictionary, and then selects an optimal path in the lattice."
"To han-dle morphemes that cannot be found in the dictio-nary, JUMAN enumerates unknown morpheme can-didates using character type-based heuristics, and adds them to the morpheme lattice."
"Unknown mor-phemes are given the special POS tag “undefined,” which is treated as noun."
"Character type-based heuristics are based on the fact that Japanese is written with several different character types such as kanji, hiragana and katakana, and that the choice of character types gives some clues on morpheme boundaries."
"For example, a se-quence of katakana characters are considered as an unknown morpheme candidate, as in “ グーグル ” (gûguru, Google) out of “ グーグルが ” (gûguru-ga, Google- NOM )."
"Kanji characters are segmented per character, which is sometimes wrong but prevents error propagation."
"These heuristics are simple and effective, but far from perfect."
"They cannot identify mixed-character morphemes, verbs and adjectives correctly."
"For ex-ample, the verb “ ググる ” (gugu-ru, to google) is wrongly divided into the katakana unknown mor-pheme ” ググ ” (gugu) and the hiragana suffix “ る ” (ru)."
The task of lexicon acquisition is to generate dictio-nary entries inductively from their examples in texts.
"Since the morphological analyzer provides a basic lexicon, the morphemes to be acquired are limited to those unknown to the analyzer."
"In order to generate an entry, its stem and POS tag need to be identified."
Determining the stem of an example is to draw the front and rear boundaries in a character sequence in texts which corresponds to the stem.
The POS tag is selected from the tagset given by the morphological analyzer.
Figure 1 shows the system architecture.
Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [URL_CITE] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary.
KNP is used to form a phrasal unit called bunsetsu by chunking morphemes.
"Every time a sentence is analyzed, the lexicon acquirer receives the analysis."
It detects examples of unknown morphemes and keeps them in storage.
"When an entry is unambiguously selected, the lex-icon acquirer updates the automatically constructed dictionary, and it will be used in subsequent analy-sis."
"The process of lexicon acquisition has four phases: detection, candidate enumeration, aggregation and selection."
First the analysis is scanned to detect ex-amples of unknown morphemes.
"For each exam-ple, one or more candidates for dictionary entries are enumerated."
"It is added to the storage, and multiple examples in the storage that share the candidates are aggregated."
They are compared and the best candi-date is selected from it.
Take the ra-row verb “ ググる ” (gugu-ru) for ex-ample.
"Its example “ ググってみた。 ” (gugu-tte-mi-ta, to have tried to google) can be interpreted in many ways as shown in Figure 2."
"Similarly, multi-ple candidates are enumerated for another example “ ググるのは ” (gugu-ru-no-ha, to google- TOPIC )."
"If these examples are compared, we can see that the ra-row verb “ ググる ” (gugu-ru) can explain them."
Morphological constraints are used for candidate enumeration.
"Since they are coded in JUMAN, we first transform them into a set of strings called suf-fixes."
A suffix is created by concatenating the end-ing of a morpheme (if any) and subsequent ancillary morphemes.
"Each POS tag is associated with a set of suffixes, as shown in Table 1."
This means that a stem can be followed by one of the suffixes specified by its POS tag and cannot be followed by any other suffix.
"In preparation for lexicon acquisition, suffixes are acquired from a corpus."
We used a web corpus that was compiled through the procedures proposed[REF_CITE].
Suffixes were ex-tracted from examples of registered morphemes and were aggregated per POS tag.
We found that the number of suffixes did not con-verge even in this large-scale corpus.
It was because ancillary morphemes included the wide variety of auxiliary verbs and formal nouns.
"Alternatively, we used the first five characters as a suffix."
"In the exper-iments, we obtained 500 thousand unique suffixes from 100 million pages."
The number of POS tags that corresponded to a suffix was 1.33 on average.
The first step of lexicon acquisition is unknown mor-pheme detection.
"Every time the analysis of a sen-tence was given, the sequence of morphemes are scanned, and suspicious points that probably repre- sent unknown morphemes are detected."
"Currently, we use the POS tag “undefined” to de-tect unknown morphemes."
"For example, the exam-ple “ ググってみた。 ” is detected because “ ググ ” is given “undefined.”"
This simple method cannot detect unknown morphemes if they are falsely seg-mented into combinations of registered morphemes.
We leave the comprehensive detection of unknown morphemes to future work.
"For each example, one or more candidates for the dictionary entry are enumerated."
Each candidate is represented by a combination of a front boundary and the pair of a rear boundary and a POS tag.
"The search range for enumeration is based on bun-setsu phrases, which is created by chunking mor-phemes."
"The range is at most the corresponding bunsetsu and the two immediately preceding and succeeding bunsetsu, which we found wide enough to contain correct candidates."
The candidates for the rear boundary and the POS tag are enumerated by string matching of suffixes as shown in Figure 2.
"If a suffix matches, the start-ing position of the suffix becomes a candidate for the rear boundary and the suffix is mapped to one or more corresponding POS tags."
"In addition, the candidates for the front and rear boundaries are enumerated by scanning the se-quence of morphemes."
"The boundary markers we use are • punctuations, • grammatical prefixes such as “ 御 ” (go-, hon-orific prefix), for front boundaries, • grammatical suffixes such as “ 様 ” (-sama, hon-orific title), for rear boundaries, and • bunsetsu boundaries given by KNP."
Each rear boundary candidate whose correspond-ing POS tag is not decided is given the special tag “EOB” (end-of-bunsetsu).
This means that no suf-fix is attached to the candidate.
"Since nouns, vowel verbs and na-type adjectives can appear in isolation, it will be expanded to these POS tags when selecting the best POS tag."
Selection of the best candidate is done by compar-ing multiple examples.
"Each example is added to the storage, and then examples that possibly repre-sent the same entry with it are extracted from the storage."
Examples aggregated at this phase share the front boundary but may be unrelated to the example in question.
They are pruned in the next phase.
"In order to manage examples efficiently, we im-plement a trie."
The example is added to the trie for each front boundary candidate.
The key is the char-acter sequence determined by the front boundary and the leftmost rear boundary.
"To retrieve examples that share the front boundary with it, we check every node in the path from the root to the node where it is stored, and collect examples stored in each node."
"The best candidate is selected by identifying the front boundary, the rear boundary and the POS tag in this order."
"Starting from the rightmost front boundary candidate, multiple rear boundary candi-dates that share the front boundary are compared and some are dropped."
"Then starting from the leftmost surviving rear boundary candidate, the best POS tag is selected from the examples that share the stem."
"If the selected candidate satisfies simple termination conditions, it is added to the dictionary and the ex-amples are removed from the storage."
"For each front boundary candidate, some inappro-priate rear boundary candidates are dropped by ex-amining the inclusion relation between the examples of a pair of candidates."
The assumption behind this is that an appropriate candidate can interpret more examples than incorrect ones.
"Let p and q be a pair of the candidates for the rear boundary, and R p and"
R q be the sets of examples for which p and q are enumerated.
"If p is a prefix of q and p is the correct stem, then R q must be contained in R p ."
"In practice we loosen this condition, considering possible errors in candidate enumeration"
"For each stem candidate, the appropriate POS tag is identified."
"Similarly to rear boundary identifica-tion, POS identification is done by checking inclu-sion relation."
"If the POS tag is successfully disambiguated, sim-ple termination conditions is checked to prevent the accidental acquisition of erroneous candidates."
The first condition is that the number of unique conjuga-tion forms that appear in the examples should be 3 or more.
"If the candidate is a noun, it is substituted with the number of the unique base forms of their imme-diate ancillary morphemes."
The second condition is that the front boundaries of some examples are de-cided by clear boundary markers such as punctua-tions and the beginning of sentence.
This prevents oversegmentation.
"For example, the stem candidate “* 撰組 ” (*sengumi) is always enumerated for exam-ples of “ 新撰組 ” (Shingengumi, a historical organi-zation) since “ 新 ” (shin-, new) is a prefix."
This can-didate is not acquired because “* 撰組 ” (*sengumi) does not occur alone and is always accompanied by “ 新 ” (shin-).
Thresholds are chosen empirically.
"Since a morpheme is extracted from a small num-ber of examples, it is inherently possible that the ac-quired morpheme actually consists of two or more morphemes."
"For example, the noun phrase “ 顆粒 タイプ ” (karyuu-taipu, granular type) may be ac-quired as a morpheme before “ 顆粒 ” (karyuu, gran-ule) is extracted."
"To handle this phenomenon, it is checked at the time of acquisition whether the new morpheme (kairyuu) can decompose registered morphemes (kairyuu-taipu)."
"If found, a composite “morpheme” is removed from the dictionary."
Currently we leave the decompositionality check to the morphological analyzer.
Possible compounds are enumerated by string matching and temporar-ily removed from the dictionary.
Each candidate is analyzed by the morphological analyzer and it is checked whether the candidate is divided into a com-bination of registered morphemes.
"If not, the candi-date is restored to the dictionary."
We used the default dictionary of the morphological analyzer JUMAN as the initial lexicon.
"If spelling variants were expanded and proper nouns were counted, the total number of morphemes was 120 thousands."
We used domain-specific corpora as target texts because efficient acquisition was expected.
"If target texts shared a topic, relevant unknown morphemes were used frequently."
"In the experiments, we used search engine TSUBAKI[REF_CITE]and casted the search results as domain-specific corpora."
"For each query, our system sequentially read pages from the top of the result and acquired morphemes."
We terminated the acquisition at the 1000th page and analyzed the same 1000 pages with the aug-mented lexicon.
"The queries used were “ 捕鯨問 題 ” (whaling issue), “ 赤ちゃんポスト ” (baby hatch), “ ジャスラック ” (JASRAC, a copyright collective), “ ツンデレ ” (tsundere, a slang word) and “ アガリク ス ” (agaricus)."
The proposed method is evaluated by measuring the accuracy of acquired morphemes and their contri-bution to the improvement of morphological analy-sis.
A morpheme is considered accurate if both seg-mentation and the POS tag are correct.
Note that segmentation is a nontrivial problem for evaluation.
"In fact, the disagreement over segmentation criteria was considered one of the main reasons for reported errors[REF_CITE]and[REF_CITE]."
It is difficult to judge whether a compound term should be divided because there is no definite stan-dard for morpheme boundaries in Japanese.
"For ex-ample, “ ミンク鯨 ” (minku-kujira, minke whale) can be extracted as a single morpheme or decomposed into “ ミンク ” and “ 鯨 .”"
"While segmentation is an open question in Japanese morphological analysis, “correct” segmentation is not necessarily important for applications using morphological analysis."
"Even if a noun is split into two or more morphemes in morphological analysis, they are chunked to form a phrasal unit called bunsetsu in dependency pars-ing, and to extract a keyword[REF_CITE]."
"To avoid the decompositionality problem, we adopted manual evaluation."
We analyzed the tar-get texts with both the initial lexicon and the aug-mented lexicon.
Then we checked differences be-tween the two analyses and extracted sentences that were affected by the augmentation.
"Among these sentences, we evaluated randomly selected 50 sen-tences per query."
"We checked the accuracy of seg-mentation and POS tagging of each “diff” block, which is illustrated in Figure 3."
The segmentation of a block was judged correct unless morpheme bound-aries were clearly wrong.
"In the evaluation of POS tagging, we did not dis-tinguish subclasses of noun [Footnote_3] such as common noun and proper noun."
"3 In the experiments, we regarded demonstrative pronouns as"
The special POS tag “undefined” given by JUMAN was treated as noun.
Table 2 summarizes statistical information per query.
The number of sentences affected by the augmentation varied considerably (1.04%–15.4%).
The initial lexicon of the morphological analyzer lacked morphemes that appeared frequently in some corpora because morphological analysis had been tested mainly with newspaper articles.
"The precision of acquired morphemes was high (97.4%–99.3%), and the number of examples used for acquisition was as little as 4–9."
These results are astonishing considering th[REF_CITE]ignored candidates that appeared less than 10 times (because they were unreliable).
Table 3 shows some acquired morphemes.
"As expected, the overwhelming majority were nouns (93.0%–100%) and katakana morphemes (80.7%– 91.6%)."
"Some were mixed-character morphemes (“ ソフ倫 ” and “ シャ乱 Q”), which cannot be recog-nized by character-type based heuristics, and slang words (“ 腐女子 ,” “ ヲタ ,” etc.) which did not ap-pear in newspaper articles."
Some morphemes were spelling variants of those in the pre-defined dictio-nary.
Uncommon kanji characters were used in ba-sic words (“ 棄てる ” for “ 捨てる ” and “ 訊く ” for “ 聞く ”) and katakana was used to change nuances (“ モテる ” for “ もてる ” and “ ダンナ ” for “ 旦那 ”).
Table 4 shows the results of manual evaluation of “diff” blocks.
The overwhelming majority of blocks were correctly analyzed with the augmented lexicon (E → C and C → C).
"On the other hand, adverse effects were observed only in a few blocks (C → E)."
"In conclusion, acquired morphemes improve the quality of morphological analysis."
Some short katakana morphemes oversegmented other katakana nouns.
"For example, “ サーバー ” (sâbâ, server) was wrongly segmented by newly-acquired “ サー ” (sâ, sir) and preregistered “ バー ” (bâ, bar)."
Neither the morphological analyzer and the lexicon acquirer could detect this semantic mis-match.
"Curiously, one example of “ サー ” (sâ) was actuallly part of “ サーバー ” (sâbâ), which was erro- neously segmented when extracting sentences from HTML."
"The katakana adjective “ イイ ” (i-i, good), a spelling variant of the basic morpheme “ いい ,” was falsely identified as a noun because its ending “ イ ” was written in katakana."
"The morphological ana-lyzer, and hence the lexicon acquirer, assume that the ending of a verb or adjective is written in hi-ragana."
"This assumption is reasonable for stan-dard Japanese, but does not always hold when we analyze web texts."
"In order to recognize uncon-ventional spellings that are widely used in web texts[REF_CITE], more flexible analysis is needed."
"It is too costly or impractical to calculate the re-call of acquisition, or the ratio of the number of ac-quired morphemes against the total number of un-known morphemes because it requires human judges to find undetected unknown morphemes from a large amount of raw texts."
"Alternatively, we examined the ratio against the number of detected unknown morphemes."
Figure 4 shows the process of online acquisition for the query “JASRAC.”
The monotonic increase of the num-bers of acquired morphemes and stored examples suggests that the vocabulary size did not converge.
The number of occurrences of acquired morphemes in re-analysis was approximately the same with the number of examples kept in the storage during ac-quisition.
"This means that, in terms of frequency of occurrence, about half of unknown morphemes were acquired."
Most unknown morphemes belong to the “long tail” and the proposed method seems to have seized a “head” of the long tail.
"Although some previous studies emphasized cor-rect identification of low frequency terms[REF_CITE], it is no longer necessary because very large scale web texts are available today."
"If a small set of texts needs to be analyzed with high accuracy, we can incorporate similar texts retrieved from the web, to increase the number of examples of unknown morphemes."
The proposed method can be modified to check if un-known morphemes detected in the initial set are ac-quired and to terminate whenever sufficient acquisi-tion coverage is achieved.
"Since most languages delimit words by white-space, morphological analysis in these languages is to seg-ment words into morphemes."
"For example, Mor-pho[REF_CITE]was eval-uations of unsupervised segmentation for English, Finnish, German and Turkish."
"While Japanese is an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages."
"Among them, Chinese has been a subject of intensive research."
They detected new words by comput-ing segment confidence and re-analyzed the inputs with detected words as features.
The Japanese language is unique in that it is writ-ten with several different character types.
Heuris-tics widely used in unknown morpheme process-ing are based on character types.
They were also used as important clues in statistical methods.
The appropriateness of a word candidate was measured by the distance between probability distributions of the candidate and a model.
"In this method, mor-phological constraints were indirectly represented by distributions."
This setting is impractical in Japanese because POS tagging is inseparable from segmentation.
We propose a novel method that augments the lexi-con of a Japanese morphological analyzer by acquir-ing unknown morphemes from texts in online mode.
Unknown morphemes are acquired with high accu-racy and improve the quality of morphological anal-ysis.
Unknown morphemes are one of the main sources of error in morphological analysis when we analyze web texts.
"The proposed method has the potential to overcome the unknown morpheme problem, but it cannot be achieved without recognizing or being robust over various phenomena such as unconven-tional spellings and typos."
These phenomena are not observed in newspaper articles but cannot be ignored in web texts.
"In the future, we will work on these phenomena."
Morphological analysis is now very mature.
It is widely applied as preprocessing for NLP appli-cations such as parsing and information retrieval.
"Hence in the future, we aim to use the proposed method to improve the quality of these applications."
We investigate the problem of binary text clas-sification in the domain of legal docket entries.
This work presents an illustrative instance of a domain-specific problem where the state-of-the-art Machine Learning (ML) classifiers such as SVMs are inadequate.
Our investiga-tion into the reasons for the failure of these classifiers revealed two types of prominent er-rors which we call conjunctive and disjunctive errors.
We developed simple heuristics to ad-dress one of these error types and improve the performance of the SVMs.
"Based on the in-tuition gained from our experiments, we also developed a simple propositional logic based classifier using hand-labeled features, that ad-dresses both types of errors simultaneously."
"We show that this new, but simple, approach outperforms all existing state-of-the-art ML models, with statistically significant gains."
"We hope this work serves as a motivating example of the need to build more expressive classifiers beyond the standard model classes, and to ad-dress text classification problems in such non-traditional domains."
"Text Classification is a widely researched area, with publications spanning more than a decade[REF_CITE]."
"Although earlier models used logic based rules[REF_CITE]and decision trees[REF_CITE], recently the emphasis has been on statistical classifiers such as the naive Bayes model[REF_CITE], logis-tic regressi[REF_CITE]and support vector machines[REF_CITE]."
"Although several complex features were considered for classification, eventually researchers have settled down to simple bag-of-words features such as unigrams and some times bigrams[REF_CITE], thereby com-pletely ignoring the grammar and other semantic in-formation in the text."
"Despite this fact, the state-of-the-art performance is close to or above 90% on F1 scores on most standard test collections such[REF_CITE]newsgroups, etc.[REF_CITE]."
"As such, most researchers and practitioners believe text classification technology has reached a mature state, where it is suitable for deployment in real life applications."
"In this work, we present a text classification prob-lem from the legal domain which challenges some of our understanding of text classification problems."
"In the new domain, we found that the standard ML approaches using bag-of-words features perform rel-atively poorly."
"Not only that, we noticed that the linear form (or even polynomial form) used by these classifiers is inadequate to capture the semantics of the text."
"Our investigation into the shortcomings of the traditional models such as SVMs, lead us to build a simple propositional logic based classifier using hand-labeled features that outperforms these strong baselines."
"Although the new model by itself is interesting, the main objective of our work is to present the text classification community with an interesting prob-lem where the current models are found inadequate."
"Our hope is that the new problem will encourage researchers to continue to build more sophisticated models to solve classification problems in diverse, non-traditional domains."
The rest of the paper is organized as follows.
"In section 2, we introduce the problem of legal docket entry classification and describe the data with some representative examples."
"In section 3, we describe the experiments performed with SVMs and several of its variants."
We also identify the shortcomings of the current classifiers in this section.
"In section 3.2, we present results from using human selected features for the classification problem and motivate their application for the docket entry classification using propositional logic in subsection 3.3."
We also show that simple propositional logic using human selected features and their labels outperforms the state-of-the-art classifiers.
"We conclude the discus-sion in section 4, where we argue the case for more sophisticated classifiers for specialized domains."
"In this section, we introduce the problem of legal docket entry classification."
"In any US district court of law, information on the chronological events in a case is usually entered in a document called the case docket."
"Each entry in a docket lists an event that occured on a specific date such as pleading, appeal, order, jury trial, judgment, etc."
The entries are brief descriptions of the events in natural language.
"Sometimes, a single docket entry can list multiple events that take place on the same day."
Table 1 displays a sample docket for a case.
Identifying various events in a court case is a cru-cial first step to automatically understanding the pro-gression of a case and also in gathering aggregate statistics of court cases for further analysis.
"While some events such as “Complaint” may be easy to identify using regular expressions, others are much more complex and may require sophisticated mod-eling."
"In this work, we are primarily interested in iden-tifying one such complex event called “Order re: Summary Judgment”."
Summary Judgment is a le-gal term which means that a court has made a deter-mination (a judgment) without a full trial. [Footnote_1]
"1 See e.g., Wikipedia for more information[URL_CITE]judgment"
"Such a judgment may be issued as to the merits of an entire case, or of specific issues in that case."
"Typically, one of the parties (plaintiff or defendant) involved in the case moves a motion for summary judgment, (usu-ally) in an attempt to eliminate the risk of losing a trial."
"In an “Order re: Summary Judgment” event, the court may grant or deny a motion for summary judgment upon inspecting all the evidence and facts in the case."
"The task then, is to identify all docket entries in a set of cases that list occurrences of “Or-der re: Summary Judgment” events."
We will call them OSJ events in short.
A few typical positive and negative docket entries for the OSJ event from various cases are shown in table 2.
The examples require some explanation.
"Firstly, all orders granting, denying or amending motions for full or partial summary judgment are considered OSJs."
"However, if the motion is denied as moot or denied without prejudice, it is not an OSJ event, as shown in the negative examples [Footnote_1] and 2 in table 2."
"1 See e.g., Wikipedia for more information[URL_CITE]judgment"
"This is because in such cases, no de-cision was made on substantive issues of the case."
"Also, there are other kinds of orders that are issued with reference to a summary judgment motion that do not fall into the category of OSJ, such as negative examples 3 through 9."
"To elaborate further, negative example 3 is about amending the deadline for fil-ing a summary judgment motion, but not a summary judgment motion itself."
"Likewise, in negative exam-ple 4, the judge denies a motion to shorten time on a motion to vacate the order on summary judgment, but not the motion on summary judgment itself."
The other negative examples are very similar in spirit and we leave it as an exercise to the reader to interpret why they are negatively labeled.
"On first glance, it appears that a standard classifier may do a good job on this data, since the classifica-tion seems to depend mostly on certain key words such as ‘granting’, ‘denying’, ‘moot’, etc."
"Also no-tice that some of the docket entries contain multiple events, but as long as it contains the ‘order re: sum-mary judgment’ event, it falls into the positive class."
"This seems very similar to the standard case, where a document may belong to multiple topics, but it is still identified as on-topic by a binary classifier on the corresponding topic."
"Hence, as a first step, we attempted using a stan-dard SVM classifier."
"We have collected [Footnote_5],595 docket entries from several court cases on intellectual property litigation, that are related to orders pertaining to summary judg-ment, and hand labeled them into OSJ or not OSJ categories. [Footnote_2] The hand-labeling was done by a sin-gle legal expert, who practised law for a number of years."
"5 The variants include “sum jgm”, “S/J”, “summary adjudi-cation”, “summary jgm”, etc."
2 The data can be made available free of cost upon request. Please email the first author for more information.
"In all, 1,848 of these docket entries fall into the OSJ category."
"In all our experiments, we split the entire data ran-domly into 20 disjoint subsets, where each set has the same proportion of positive-to-negative exam-ples as the original complete set."
"For all the clas-sifiers we used in this work, we performed 20-fold cross validation."
We compute F1 scores on the held-out data of each run and report overall F1 score as the single point performance measure.
We also per-form statistical significance tests using the results from the 20 cross-validation runs.
"Before we ran our classifiers, we removed all punc-tuation, did casefolding, removed stopwords and stemmed the words using the Porter stemmer."
We used unigrams and bigrams as our basic features. [Footnote_3] We considered all the words and bigrams as bi-nary features and did not use any TF-IDF weight-ing.
"3 In our preliminary experiments, we found that a combina-tion of unigrams and bigrams works better than unigrams alone."
Our justification for this decision is as fol-lows: the docket text is typically very short and it is usually rare to see the same feature occurring mul-tiple times in a docket entry.
"In addition, unlike in standard text classification, some of the features that are highly frequent across docket entries such as ‘denying’,‘granting’, etc., are also the ones that are highly discriminative."
"In such a case, down-weighting these features using IDF weights might actually hurt performance."
Besides[REF_CITE]found that using binary features works as well as using TF-IDF weights.
"In addition, we also built a domain specific sen-tence boundary detector using regular expressions. [Footnote_4]"
"4 It works well in most cases but is far from perfect, due to the noisy nature of the data."
"For constructing the features of a docket entry, we only consider those sentences in the entry that con-tain the phrase “summary judgment” and its vari-ants. 5 Our preliminary experiments found that this helps the classifier focus on the relevant features, helping it to improve precision while not altering its recall noticeably."
First we implemented the standard linear SVM [Footnote_6] on this problem with only word-based features (uni-grams and bigrams) as the input.
6 All our SVM experiments were performed us-ing the libsvm implementation downloadable[URL_CITE]
"Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5."
"On inspection, we no-"
"REPRESENTATIVE POSITIVE EXAMPLES 1. ORDER denying [36-1] motion for summary judgment on dfts Ranbaxy invalidity defenses by pltfs. (signed by Judge Garrett E. Brown, Jr.) 2. ORDER GRANTING IN PART AND DENYING IN PART DEFENDANTS’"
"MOTION FOR SUMMARY JUDGMENT 3.[REF_CITE]MOTION to Amend/Correct Motion for Summary Judgment and supporting documents, filed by Defendant Synergetics USA, Inc. ; ORDERED GRANTED. 4."
"MEMORANDUM AND ORDER re: 495 Third MOTION for Partial Summary Judgment Dismissing Mon-santo’s Defenses Related to Dr. Barnes filed by Bayer BioScience N.V., motion is GRANTED IN PART AND DENIED IN PART. 5. ORDER GRANTING IN PART PLTF S/J MOT; GRANTING IN PART PLTF MOT/CLARIFY; GRANTING DEFT MOT/CLARIFY; PRTL S/J STAYED. 6. ORDER by Chief Judge Joe B. McDade."
Court is granting in part and denying in part Deere’s motion for reconsideration and clarification [42-2]; granting Toro’s motion for summary judgment of non-infringement [45- 1]; denying Deere’s motion for summary judgment [58-1]; 7. ORDER GRANTING DEFT.
MOTION FOR S/J AND DENYING PLTF.
MOTIONS FOR S/J AND TO SUP-PLEMENT.
REPRESENTATIVE NEGATIVE EXAMPLES ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.
"As a natural solution to this problem, we selected the top 100 features [Footnote_7] using the standard information gain metric[REF_CITE]and ran the SVM on the pruned feature set."
"7 We tried other numbers as well, but top 100 features achieves the best performance."
"As one would ex-pect, the performance of the SVM improved signif-icantly to reach an F1 score of 83.08% as shown in entry 2 of the same table."
"However, it is still a far cry from the typical results on standard test beds where the performance is above 90% F1."
"We suspected that training data was probably insufficient, but a learning curve plotting performance of the SVM as a function of the amount of training data reached a plateau with the amount of training data we had, so this problem was ruled out."
"To understand the reasons for its inferior perfor-mance, we studied the features that are assigned the highest weights by the classifier."
"Although the SVM is able to assign high weights to several dis-criminative features such as ‘denied’, and ‘granted’, it also assigns high weights to features such as ‘opinion’, ‘memorandum’, ‘order’, ‘judgment’, etc., which have high co-occurrence rates with the posi-tive class, but are not very discriminative in terms of the actual classification."
"This is indicative of the problems associated with standard feature selection algorithms such as infor-mation gain in these domains, where high correla-tion with the label does not necessarily imply high discriminative power of the feature."
"Traditional clas-sification tasks usually fall into what we call the ‘topical classification’ domain, where the distribu-tion of words in the documents is a highly discrimi-native feature."
"On such tasks, feature selection algo-rithms based on feature-class correlation have been very successful."
"In contrast, in the current problem, which we call ‘semantic classification’, there seem to be a fixed number of domain specific operative words such as ‘grant’, ‘deny’, ‘moot’, ‘strike’, etc., which, almost entirely decide the class of the docket entry, irrespective of the existence of other highly correlated features."
The information gain metric as well as the SVM are not able to fully capture such features in this problem.
"We leave the problem of accurate feature selec-tion to future work, but in this work, we address the issue by asking for human intervention, as we de-scribe in the next section."
One reason for seeking human assistance is that it will give us an estimate of upperbound performance of an automatic feature selection system.
"In addition, it will also offer us a hint as to whether the poor performance of the SVM is because of poor feature selection."
We will aim to answer this question in the next section.
Using human assistance for feature selection is a rel-atively new idea in the text classification domain.[REF_CITE]propose a framework in which the system asks the user to label documents and features alternatively.
They report that this re-sults in substantial improvement in performance es-pecially when the amount of labeled data is mea-gre.[REF_CITE]propose a new General-ized Expectation criterion that learns a classification function from labeled features alone (and no labeled documents).
"They showed that feature labeling can reduce annotation effort from humans compared to document labeling, while achieving almost the same performance."
"Following this literature, we asked our annotators to identify a minimal but definitive list of discrim-inative features from labeled data."
The annotators were specifically instructed to identify the features that are most critical in tagging a docket entry one way or the other.
"In addition, they were also asked to assign a polarity to each feature."
"In other words, the polarity tells us whether or not the features be-long to the positive class."
Table 3 lists the complete set of features identified by the annotators.
"As an obvious next step, we trained the SVM in the standard way, but using only the features from ta-ble 3 as the pruned set of features."
"Remarkably, the performance improves to 86.77% in F1, as shown in entry 3 of table 5."
"Again, this illustrates the unique-ness of this dataset, where a small number of hand selected features (&lt; 40) makes a huge difference in performance compared to a state-of-the-art SVM combined with automatic feature selection."
We be-lieve this calls for more future work in improving feature selection algorithms.
"Positive grant, deny, amend, reverse, adopt, correct, reconsider, dismiss"
"Negative strike, proposed, defer, adjourn, moot, exclude, change, extend, leave, exceed, premature, unseal, hearing, extend, permission, oral argument, schedule, ex parte, protective order, oppose, without prejudice, withdraw, response, suspend, request, case management order, to file, enlarge, reset, supplement placing under seal, show cause reallocate, taken under submission"
"Notice that despite using human assistance, the performance of the SVM is still not at a desirable level."
This clearly points to deficiencies in the model other than poor feature selection.
"To understand the problem, we examined the errors made by the SVM and found that there are essentially two types of er-rors: conjunctive and disjunctive."
Representative ex-amples for both kinds of errors are displayed in ta-ble 4.
"The first example in the table corresponds to a conjunctive error, where the SVM is unable to model the binary switch like behavior of features."
"In this example, although ‘deny’ is rightly assigned a positive weight and ‘moot’ is rightly assigned a negative weight, when both features co-occur in a docket entry (as in ‘deny as moot’), it makes the la-bel negative. [Footnote_8]"
8 This is very similar to the conjunction of two logical vari-ables where the conjunction of the variables is negative when at least one of them is negative. Hence the name conjunctive error.
"However, the combined weight of the linear SVM is positive since the absolute value of the weight assigned to ‘deny’ is higher than that of ‘moot’, resulting in a net positive score."
"The second example falls into the category of disjunctive errors, where the SVM fails to model disjunctive behav-ior of sentences."
"In this example, the first sentence contains an OSJ event, but the second and third sen-tences are negatives for OSJ."
"As we have discussed earlier, this docket entry belongs to the OSJ category since it contains at least one OSJ event."
"However, we see that the negative weights assigned by the SVM to the second and third sentences result in an overall negative classification."
"As a first attempt, we tried to reduce the conjunc-tive errors in our system."
"Towards this objective, we built a decision tree [Footnote_9] using the same features listed in table 3."
9 We used the publicly available implementation[URL_CITE]
"Our intuition was that a decision tree makes a categorical decision at each node in the tree, hence it could capture the binary-switch like behavior of features."
"However, the performance of the decision tree is found to be statistically indistin-guishable from the linear SVM as shown in entry 4 of table 5."
"As an alternative, we used an SVM with a quadratic kernel, since it can also capture such pairwise interactions of features."
"This resulted in a fractional improvement in performance, but is again statistically indistinguishable from the decision tree."
"We also tried higher order polynomial kernels and the RBF kernel, but the performance got no better. [Footnote_10]"
10 We also tried various parameter settings for these kernels with no success.
It is not easy to analyze the behavior of non-linear kernels since they operate in a higher kernel space.
"Our hypothesis is that polynomial functions capture higher order interactions between features, but they do not capture conjunctive behavior precisely."
"As an alternative, we considered the following heuristic: whenever two or more of the hand selected features occur in the same sentence, we merged them to form an n-gram."
"The intuition behind this heuristic is the following: using the same example as before, if words such as ‘deny’ and ‘moot’ oc-cur in the same sentence, we form the bigram ‘deny-moot’, forcing the SVM to consider the bigram as a separate feature."
We hope to capture the conjunctive behavior of some features using this heuristic.
"The result of this approach, as displayed in entry 6 of table 5, shows small but statistically significant im-provement over the quadratic SVM, confirming our theory."
"We also attempted a quadratic kernel using sentence level n-grams, but it did not show any im-provement."
"Note that all the models and heuristics we used above only address conjunctive errors, but not dis-junctive errors."
"From the discussion above, we sus-pect the reader already has a good picture of what an appropriate model for this data might look like."
The next section introduces this new model devel-oped using the intuition gained above.
"So far, the classifiers we considered received a per-formance boost by piggybacking on the human se-lected features."
"However, they did not take into ac-count the polarity of these features."
A logical next step would be to exploit this information as well.
"An appropriate model would be the generalized expec-tation criterion model[REF_CITE]which learns by matching model specific label expectations conditioned on each feature, with the corresponding empirical expectations."
"However, the base model they use is a logistic regression model, which is a log-linear model, and hence would suffer from the same limitations as the linear SVM."
"There is also other work on combining SVMs with labeled fea-tures using transduction on unlabeled examples, that are soft-labeled using labeled features[REF_CITE], but we believe it will again suffer from the same limitations as the SVM on this domain."
"In order to address the conjunctive and disjunc-tive errors simultaneously, we propose a new, but simple approach using propositional logic."
"We con-sider each labeled feature as a propositional variable, where true or false corresponds to whether the la-bel of the feature is positive or negative respectively."
"Given a docket entry, we first extract its sentences, and for each sentence, we extract its labeled features, if present."
"Then, we construct a sentence-level for-mula formed by the conjunction of the variables rep- resenting the labeled features."
The final classifier is a disjunction of the formulas of all sentences in the docket entry.
"Formally, the propositional logic based classifier can be expressed as follows:"
"C(D) = ∨ Ni=(1D) (∧ Mj=1 L(f ij )) (1) i where D is the docket entry, N(D) is its number of sentences, M i is the number of labeled features in the i th sentence, f ij is the j th labeled feature in the i th sentence and L() is a mapping from a fea-ture to its label, and C(D) is the classification func-tion where ‘true’ implies the docket entry contains an OSJ event."
The propositional logic model is designed to ad-dress the within-sentence conjunctive errors and without-sentence disjunctive errors simultaneously.
"Clearly, the within-sentence conjunctive behavior of the labeled features is captured by applying logical conjunctions to the labeled features within a sen-tence."
"Similarly, the disjunctive behavior of sen-tences is captured by applying disjunctions to the sentence-level clauses."
"This model requires no train-ing, but for reasons of fairness in comparison, at test-ing time, we used only those human features (and their labels) that exist in the training set in each cross-validation run."
"The performance of this new approach, listed in table 5 as entry 7, is slightly bet-ter than the best performing SVM in entry 6."
"The difference in performance in this case is statistically significant, as measured by a paired, 2-tailed t-test at 95% confidence level (p-value = 0.007)."
"Although the improvement for this model is sta-tistically significant, it does not entirely match our expectations."
"Our data analysis showed a variety of errors caused mostly due to the following issues: • Imperfect sentence boundary detection: since the propositional logic model considers sen-tences as strong conjunctions, it is more sen-sitive to errors in sentence boundary detection than SVMs."
Any errors would cause the model to form conjunctions with features in neighbor-ing sentences and deliver an incorrect labeling. • Incomplete feature set: Some errors are caused because the feature set is not complete.
"For ex-ample, negative example 4 in table 2 is tagged as positive by the new model."
This error could have been avoided if the word ‘shorten’ had been identified as a negative feature. • Relevant but bipolar features:
"Although our model assumes that the selected features ex-hibit binary nature, this may not always be true."
"For example the word allow is sometimes used as a synonym for ‘grant’ which is a positive fea-ture, but other times, as in negative example 8 in table 2, it exhibits negative polarity."
Hence it is not always possible to encode all relevant features into the logic based model. • Limitations in expressiveness: Some natural language sentences such as negative example 5 in table 2 are simply beyond the scope of the conjunctive and disjunctive formulations.
"Clearly, there is a significant amount of work to be done to further improve the performance of the propositional logic based classifier."
One obvious line of work is towards better feature selection in this domain.
"One plausible technique would be to use shallow natural language processing techniques to extract the operative verbs acting on the phrase “summary judgment”, and use them as the pruned feature set."
Another potential direction would be to extend the SVM-based system to model disjunctive behavior of sentences. [Footnote_11] One way to accomplish this would be to classify each sentence individually and then to combine the outcomes using a disjunction.
11 Recall that the heuristics we presented for SVMs only ad-dress the conjunctive errors.
"But for this to be implemented, we would also need labels at the sentence level during training time."
"One could procure these labels from annotators, but as an alter-native, one could learn the sentence-level labels in an unsupervised fashion using a latent variable at the sentence level, but a supervised model at the docket-entry level."
"Such models may also be appropriate for traditional document classification where each doc-ument could be multi-labeled, and it is something we would like attempt in the future."
"In addition, instead of manually constructing the logic based system, one could also automatically learn the rules by using ideas from earlier work on ILP[REF_CITE], FOIL[REF_CITE], etc."
"To summarize, we believe it is remarkable that a simple logic-based classifier could outperform an SVM that is already boosted by hand picked fea-tures and heuristics such as sentence level n-grams."
"This work clearly exposes some of the limitations of the state-of-the-art models in capturing the intrica-cies of natural language, and suggests that there is more work to be done in improving the performance of text based classifiers in specialized domains."
"As such, we hope our work motivates other researchers towards building better classifiers for this and other related problems."
"String transformation, which maps a source string s into its desirable form t ∗ , is related to various applications including stemming, lemmatization, and spelling correction."
The essential and important step for string trans-formation is to generate candidates to which the given string s is likely to be transformed.
This paper presents a discriminative approach for generating candidate strings.
We use sub-string substitution rules as features and score them using an L 1 -regularized logistic regres-sion model.
We also propose a procedure to generate negative instances that affect the de-cision boundary of the model.
The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm be-cause the processes of string transformation are tractable in the model.
We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations.
String transformation maps a source string s into its destination string t ∗ .
"In the broad sense, string trans-formation can include labeling tasks such as part-of-speech tagging and shallow parsing[REF_CITE]."
"However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring."
"Typical applications of this task include stemming, lemmatization, spelling correcti[REF_CITE], OCR error correc-ti[REF_CITE], approximate string matching[REF_CITE], and duplicate record de-tecti[REF_CITE]."
"Recent studies have formalized the task in the dis-criminative framework[REF_CITE], t ∗ = argmax P (t|s). (1) t∈gen(s)"
"Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s."
The scorer was modeled by a noisy-channel model[REF_CITE]and max-imum entropy framework[REF_CITE].
The candidate generator gen(s) also affects the accuracy of the string transformation.
"Previous stud-ies of spelling correction mostly defined gen(s), gen(s) = {t | dist(s, t) &lt; δ}. (2)"
"Here, the function dist(s,t) denotes the weighted Levenshtein distance[REF_CITE]between strings s and t. Furthermore, the threshold δ requires the distance between the source string s and a can-didate string t to be less than δ."
"The choice of dist(s, t) and δ involves a tradeoff between the precision, recall, and training/tagging speed of the scorer."
"A less restrictive design of these factors broadens the search space, but it also in-creases the number of confusing candidates, amount of feature space, and computational cost for the scorer."
"Moreover, the choice is highly dependent on the target task."
"It might be sufficient for a spelling correction program to gather candidates from known words, but a stemmer must handle unseen words ap-propriately."
The number of candidates can be huge when we consider transformations from and to un-seen strings.
This paper addresses these challenges by explor-ing the discriminative training of candidate genera-tors.
"More specifically, we build a binary classifier that, when given a source string s, decides whether a candidate t should be included in the candidate set or not."
"This approach appears straightforward, but it must resolve two practical issues."
"First, the task of the classifier is not only to make a binary decision for the two strings s and t, but also to enumerate a set of positive strings for the string s, gen(s) = {t | predict(s, t) = 1}. (3)"
"In other words, an efficient algorithm is necessary to find a set of strings with which the classifier predict(s, t) yields positive labels for the string s."
Another issue arises when we prepare a training set.
A discriminative model requires a training set in which each instance (pair of strings) is annotated with a positive or negative label.
"Even though some existing resources (e.g., inflection table and query log) are available for positive instances, such re-sources rarely contain negative instances."
"Therefore, we must generate negative instances that are effec-tive for discriminative training."
"To address the first issue, we design features that express transformations from a source string s to its destination string t. Feature selection and weight-ing are performed using an L 1 -regularized logistic regression model, which can find a sparse solution to the classification model."
We also present an al-gorithm that utilizes the feature weights to enumer-ate candidates of destination strings efficiently.
We deal with the second issue by generating negative instances from unlabeled instances.
We describe a procedure to choose negative instances that affect the decision boundary of the classifier.
This paper is organized as follows.
Section 2 for-malizes the task of the candidate generator as a bi-nary classification modeled by logistic regression.
Features for the classifier are designed using the rules of substring substitution.
"Therefore, we can obtain, efficiently, candidates of destination strings and negative instances for training."
"Section 3 re-ports the remarkable performance of the proposed method in various applications including lemmati-zation, spelling normalization, and noun derivation."
"We briefly review previous work in Section 4, and conclude this paper in Section 5."
"In this section, we first introduce a binary classifier that yields a label y ∈ {0,1} indicating whether a candidate t should be included in the candidate set (1) or not (0), given a source string s."
"We express the conditional probability P (y|s, t) using a logistic regression model, 1 1 + exp (−Λ T F (s, t)),"
"P (1|s, t) = (4) P (0|s, t) = 1 − P (1|s, t). (5) In these equations, F = {f 1 , ..., f K } denotes a vec-tor of the Boolean feature functions; K is the num-ber of feature functions; and Λ = {λ 1 ,...,λ K } presents a weight vector of the feature functions."
"We obtain the following decision rule to choose the most probable label y ∗ for a given pair hs, ti, (1 Λ T F (s, t) &gt; 0 y ∗ = argmax P (y|s, t) = . y∈{0,1} 0 (otherwise) (6)"
"Finally, given a source string s, the generator func-tion gen(s) is defined to collect all strings to which the classifier assigns positive labels: gen(s) = {t | P (1|s, t) &gt; P (0|s, t)} = {t | Λ T F (s, t) &gt; 0}. (7)"
The binary classifier can include any arbitrary fea-ture.
"This is exemplified by the Levenshtein dis-tance and distributional similarity[REF_CITE]be-tween two strings s and t. These features can im-prove the classification accuracy, but it is unrealistic to compute these features for every possible string, as in equation 7."
"For that reason, we specifically examine substitution rules, with which the process of transforming a source string s into its destination form t is tractable."
"In this study, we assume that every string has a prefix ‘ˆ’ and postfix ‘$’, which indicate the head and tail of a string."
"A substitution rule r = (α,β) replaces every occurrence of the substring α in a source string into the substring β."
"Assuming that a string s can be transformed into another string t with a single substitution operation, substitution rules ex-press the different portion between strings s and t."
"Equation 8 defines a binary feature function with a substitution rule between two strings s and t, ([Footnote_1] (rule r k can convert s into t) f k (s, t) = . 0 (otherwise) (8)"
"1 The number of letters for a substitution rule r = (α, β) is defined as the sum of the quantities of letters in α and β, i.e., |α| + |β|. We determined the threshold θ = 12 experimentally."
We allow multiple substitution rules for a given pair of strings.
"For instance, substitution rules (‘a’, ‘’), (‘na’, ‘n’), (‘ae’, ‘e’), (‘nae’, ‘ne’), etc. form feature functions that yield 1 for strings s = ‘ˆanaemia$’ and t = ‘ˆanemia$’."
"Equation 6 produces a decision based on the sum of feature weights, or scores of substitution rules, representing the different portions between s and t."
Substitution rules for the given two strings s and t are obtained as follows.
"Let l denote the longest common prefix between strings s and t, and r the longest common postfix."
"We define c s as the sub-string in s that is not covered by the longest common prefix l and postfix r, and define c t for t analogously."
"In other words, strings s and t are divided into three regions, lc s r and lc t r, respectively."
"For strings s = ‘ˆanaemia$’ and t = ‘ˆanemia$’ in Figure 1 (2), we obtain c s = ‘a’ and c t = ‘’ because l = ‘ˆan’ and r = ‘emia$’."
"Because substrings c s and c t express different portions between strings s and t, we obtain the mini- mum substitution rule (c s , c t ), which can convert the string s into t by replacing substrings c s in s with c t ; the minimum substitution rule for the same ex-ample is (‘a’, ‘’)."
"However, replacing letters ‘a’ in ‘ˆanaemia$’ into empty letters does not pro-duce the correct string ‘ˆanemia$’ but ‘ˆnemi$’."
"Furthermore, the rule might be inappropriate for ex-pressing string transformation because it always re-moves the letter ‘a’ from every string."
"Therefore, we also obtain expanded substitution rules, which insert postfixes of l to the head of min-imum substitution rules, and/or append prefixes of r to the rules."
"For example, we find an expanded substitution rule (‘na’, ‘n’), by inserting a postfix of l = ‘ˆan’ to the head of the minimum substitu-tion rule (‘a’, ‘’); similarly, we obtain an expanded substitution rule (‘ae’, ‘e’), by appending a prefix of r = ‘emia$’ to the tail of the rule (‘a’, ‘’)."
Figure 1 displays examples of substitution rules (the right side) for three pairs of strings (the left side).
"Letters in blue, green, and red respectively represent the longest common prefixes, longest com-mon postfixes, and different portions."
"In this study, we expand substitution rules such that the number of letters in rules is does not pass a threshold θ 1 ."
"Given a training set that consists of N instances, D = (s (1) , t (1) , y (1) ), ..., (s (N) , t (N) , y (N) ) , we optimize the feature weights in the logistic regres-sion model by maximizing the log-likelihood of the conditional probability distribution,"
"N L Λ = X log P (y (i) |s (i) , t (i) ). (9) i=1"
"The partial derivative of the log-likelihood with re-spect to a feature weight λ k is given as equation 10,"
"N ∂L Λ = X ny (i) − P (1|s (i) , t (i) ) o f k (s (i) , t (i) ). ∂λ k i=1 (10)"
The maximum likelihood estimation (MLE) is known to suffer from overfitting the training set.
"The common approach for addressing this issue is to use the maximum a posteriori (MAP) estimation, intro-ducing a regularization term of the feature weights Λ, i.e., a penalty on large feature weights."
"In addi-tion, the generation algorithm of substitution rules might produce inappropriate rules that transform a string incorrectly, or overly specific rules that are used scarcely."
"Removing unnecessary substitution rules not only speeds up the classifier but also the algorithm for candidate generation, as presented in Section 2.4."
"In recent years, L 1 regularization has received in-creasing attention because it produces a sparse so-lution of feature weights in which numerous fea-ture weights are zero[REF_CITE]."
"Therefore, we regularize the log-likelihood with the L 1 norm of the weight vector Λ and define the final form the objective function to be minimized as |Λ| E Λ = −L Λ + . (11) σ"
"Here, σ is a parameter to control the effect of L 1 regularization; the smaller the value we set to σ, the more features the MAP estimation assigns zero weights to: it removes a number of features from the model."
The advantage of our feature design is that we can enumerate strings to which the classifier is likely to assign positive labels.
"We start by observing the nec-essary condition for t in equation 7,"
"Λ T F (s, t) &gt; 0 ⇒ ∃k : f k (s, t) = 1 ∧ λ k &gt; 0. (12)"
The classifier might assign a positive label to strings s and t when at least one feature function whose weight is positive can transform s to t.
Let R + be a set of substitution rules to which MAP estimation has assigned positive feature weights.
"Because each feature corresponds to a sub-stitution rule, we can obtain gen(s) for a given string s by application of every substitution rule r ∈ R + , gen(s) = {r(s) | r ∈ R + ∧ Λ T F (s, r(s)) &gt; 0}. (13)"
"Input: s = (s 1 ,...,s l ): an input string s (series of letters)"
Input: D: a trie dictionary containing positive features
"Output: T: gen(s) 1 T = {}; 2 U = {}; 3 foreach i ∈ (1,...,|s|) do 4 F ← D.prefix search(s,i); 5 foreach f ∈ F do 6 if f ∈/ U then 7 t ← f.apply(s); 8 if classify(s,t) = 1 then 9 add t to T; 10 end 11 add f to U; 12 end 13 end 14 end 15 return T;"
Algorithm 1 : A pseudo-code for gen(s).
"Here, r(s) presents the string to which the substitu-tion rule r transforms the source string s. We can compute gen(s) with a small computational cost if the MAP estimation with L 1 regularization reduces the number of active features."
Algorithm 1 represents a pseudo-code for obtain-ing gen(s).
"To search for positive substitution rules efficiently, the code stores a set of rules in a trie structure."
"In line 4, the code obtains a set of positive substitution rules F that can rewrite substrings start-ing at offset #i in the source string s. For each rule f ∈ F , we obtain a candidate string t by application of the substitution rule f to the source string s (line 7)."
The candidate string t is qualified to be included in gen(s) when the classifier assigns a positive label to strings s and t (lines 8 and 9).
Lines 6 and 11 pre-vent the algorithm from repeating evaluation of the same substitution rule.
The parameter estimation requires a training set D in which each instance (pair of strings) is annotated with a positive or negative label.
"Negative instances (counter examples) are essential for penalizing in-appropriate substitution rules, e.g. (‘a’, ‘’)."
"Even though some existing resources (e.g. verb inflection table) are available for positive instances, such re-sources rarely contain negative instances."
A common approach for handling this situation is to assume that every pair of strings in a resource
"Algorithm 2 : Generating negative instances. is a negative instance; however, negative instances amount to ca."
"V (V − 1)/[Footnote_2], where V represents the total number of strings."
2 UMLS SPECIALIST Lexicon[URL_CITE]
"Moreover, substitution rules expressing negative instances are innumerable and sparse because the different portions are peculiar to individual negative instances."
"For instance, the min-imum substitution rule for unrelated words anaemia and around is (‘naemia’, ‘round’), but the rule cannot be too specific to generalize the conditions for other negative instances."
"In this study, we generate negative instances so that they can penalize inappropriate rules and settle the decision boundary of the classifier."
This strat-egy is summarized as follows.
We consider every pair of strings as candidates for negative instances.
"We obtain substitution rules for the pair using the same algorithm as that described in Section 2.2 if a string pair is not included in the dictionary (i.e., not in positive instances)."
The pair is used as a nega-tive instance only when any substitution rule gener-ated from the pair also exists in the substitution rules generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-ments the strategy for generating negative instances efficiently.
"First, we presume that we have positive instances D + = [(s 1 , t 1 ), ..., (s l , t l )] and unlabeled strings V ."
"For example, positive instance D + repre-sent orthographic variants, and unlabeled strings V include all possible words (vocabulary)."
"We insert the vocabulary into a suffix array, which is used to locate every occurrence of substrings in V ."
The algorithm first generates substitution rules R only from positive instances
D + (lines 3 to 7).
"For each substitution rule r ∈ R, we enumerate known strings S that contain the source substring r.src (line 9)."
We apply the substitution rule to each string s ∈ S and obtain its destination string t (line 11).
"If the pair of strings hs, ti is not included in D + (line 12), and if the destination string t is known (line 13), the substitution rule r might associate incorrect strings s and t, which do not exist in D + ."
"Therefore, we insert the pair to the negative set D − (line 14)."
"We evaluated the candidate generator using three different tasks: normalization of orthographic vari-ants, noun derivation, and lemmatization."
"The datasets for these tasks were obtained from the UMLS SPECIALIST Lexicon 2 , a large lexicon that includes both commonly occurring English words and biomedical vocabulary."
Table 1 displays the list of tables in the SPECIALIST Lexicon that were used in our experiments.
"We prepared three datasets, Or-thography, Derivation, and Inflection."
"The Orthography dataset includes spelling vari-ants (e.g., color and colour) in the LRSPL table."
We chose entries as positive instances in which spelling variants are caused by (case-insensitive) alphanu-meric changes 3 .
"The Derivation dataset was built di-rectly from the LRNOM table, which includes noun derivations such as abandon → abandonment."
"The LRAGR table includes base forms and their inflec-tional variants of nouns (singular and plural forms), verbs (infinitive, third singular, past, past participle forms, etc), and adjectives/adverbs (positive, com-parative, and superlative forms)."
"For the Inflection dataset, we extracted the entries in which inflec-tional forms differ from their base forms [Footnote_4] , e.g., study → studies."
"4 LRAGR table also provides agreement information even when word forms do not change. For example, the table con-tains an entry indicating that the first-singular present form of the verb study is study, which might be readily apparent to En-glish speakers."
"For each dataset, we applied the algorithm de-scribed in Section 2.5 to generate substitution rules and negative instances."
"Table 2 shows the number of positive instances (# +), negative instances (# -), and substitution rules (# Rules)."
We evaluated the per-formance of the proposed method in two different goals of the tasks: classification (Section 3.2) and normalization (Section 3.[Footnote_3]).
"3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.)."
"In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels."
We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset [Footnote_5] .
5 We determined the regularization parameter σ = 5 experi-mentally. Refer to Figure 2 for the performance change.
"Eight baseline systems were prepared for comparison: Levenshtein distance (LD), nor-malized Levenshtein distance (NLD), Dice coef-ficient on letter bigrams (DICE)[REF_CITE], Longest Common Substring Ra-tio (LCSR)[REF_CITE], Longest Common Prefix Ratio (PREFIX)[REF_CITE], Porter’s stemmer[REF_CITE], Morpha[REF_CITE], and CST’s lemmatiser (Dalianis and Jonge- jan, 2006) [Footnote_6] ."
6 We used CST’s lemmatiser version 2.13[URL_CITE]index.html
"The five systems LD, NLD, DICE, LCSR, and PREFIX employ corresponding metrics of string distance or similarity."
"Each system assigns a posi-tive label to a given pair of strings hs, ti if the dis-tance/similarity of strings s and t is smaller/larger than the threshold δ (refer to equation 2 for distance metrics)."
The threshold of each system was chosen so that the system achieves the best F1 score.
The remaining three systems assign a positive la-bel only if the system transforms the strings s and t into the identical string.
"For example, a pair of two words studies and study is classified as positive by Porter’s stemmer, which yields the identical stem studi for these words."
We trained CST’s lemmatiser for each dataset to obtain flex patterns that are used for normalizing word inflections.
"To examine the performance of the L 1 -regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM)."
These SVM classifiers were implemented by the SVM perf[Footnote_7] on a linear kernel [Footnote_8] .
7 SVM for Multivariate Performance Measures (SVM perf )[URL_CITE]
8 We determined the parameter C = 500 experimentally; it controls the tradeoff between training error and margin.
An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L 1 -regularized logistic regression and the linear-kernel SVM.
Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed[REF_CITE].
"Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct de-cisions for positive instances."
"The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively."
"Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms."
"CST’s lemmatizer suf-fered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) → (clone, clo)."
Morpha and CST’s lemma- tizer were not designed for orthographic variants and noun derivations.
"Levenshtein distance (δ = 1) did not work for the Derivation set because noun derivations often append two or more letters (e.g., happy → happi-ness)."
No string similarity/distance metrics yielded satisfactory results.
Some metrics obtained the best F1 scores with extreme thresholds only to classify every instance as positive.
These results imply the difficulty of the string metrics for the tasks.
The L 1 -regularized logistic regression was com-parable to the SVM with linear kernel in this exper-iment.
"However, the presented model presents the advantage that it can reduce the number of active features (features with non-zero weights assigned); the L 1 regularization can remove 74%, 48%, and 82% of substitution rules in each dataset."
The performance improvements by incorporating string metrics as features were very subtle (less than 0.7%).
"What is worse, the distance/similarity metrics do not specifically derive destination strings to which the classifier is likely to assign positive labels."
"There-fore, we can no longer use the efficient algorithm as a candidate generator (in Section 2.4) with these features."
Table 4 demonstrates the ability of our approach to obtain effective features; the table shows the top 10 features with high weights assigned for the Or-thography data.
An interesting aspect of the pro-posed method is that the process of the orthographic variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for the Inflection data when we change the number of active features (x-axis) by controlling the regular-ization parameter σ from 0.001 to 100.
"The larger the value we set for σ, the better the classifier per-forms, generally, with more active features."
"In ex-treme cases, the number of active features drops to 97 with σ = 0.01; nonetheless, the classifier still achieves 0.961 of the F1 score."
The result suggests that a small set of substitution rules can accommo-date most cases of inflectional variations.
The second experiment examined the performance of the string normalization tasks formalized in equa-tion 1.
"In this task, a system was given a string s and was required to yield either its transformed form t ∗ (s =6 t ∗ ) or the string s itself when the transforma-tion is unnecessary for s."
The conditional probabil-ity distribution (scorer) in equation 1 was modeled by the maximum entropy framework.
"Features for the maximum entropy model consist of: substitution rules between strings s and t, letter bigrams and tri-grams in s, and letter bigrams and trigrams in t."
"We prepared four datasets, Orthography, Deriva-tion, Inflection, and XTAG morphology."
"Each dataset is a list of string pairs hs,ti that indicate the transformation of the string s into t. A source string s is identical to its destination string t when string s should not be changed."
These instances correspond to the case where string s has already been lemmatized.
"For each string pair (s, t) in LR-SPL [Footnote_9] , LRNOM, and LRAGR tables, we generated two instances hs, ti and ht, ti."
9 We define that s precedes t in dictionary order.
"Consequently, a sys-tem is expected to leave the string t unchanged."
We also used[REF_CITE]to perform a cross-domain evaluation of the lemmatizer trained on the[REF_CITE].
"The entries in XTAG morphol- ogy that also appear in the Inflection dataset were 39,130 out of 317,322 (12.3 %)."
We evaluated the proposed method and CST’s lemmatizer by per-forming ten-fold cross validation.
Table 5 reports the performance based on the number of correct transformations.
The proposed method again outperformed the baseline systems with a wide margin.
It is noteworthy that the pro-posed method can accommodate morphological in-flections in the XTAG morphology corpus with no manual tuning or adaptation.
"Although we introduced no assumptions about target tasks (e.g. a known vocabulary), the aver-age number of positive substitution rules relevant to source strings was as small as 23.9 (in XTAG morphology data)."
"Therefore, the candidate gen-erator performed 23.9 substitution operations for a given string."
"It applied the decision rules (equa-tion 7) 21.3 times, and generated 1.67 candidate strings per source string."
The experimental results described herein demonstrated that the candidate generator was modeled successfully by the discrim-inative framework.
The task of string transformation has a long history in natural language processing and information re-trieval.
"As described in Section 1, this task is re-lated closely to various applications."
"Therefore, we specifically examine several prior studies that are relevant to this paper in terms of technical aspects."
Some researchers have reported the effectiveness of the discriminative framework of string similarity.
They extracted features from substring pairs that are consistent to a character-based align-ment of two strings.
"However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discrimina-tive models."
"Although they pro-posed a method to obtain suffix rules from a training data, the method did not use counter-examples (neg-atives) for reducing incorrect string transformations."
"However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dic-tionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destina-tion strings that were specified by the training data."
We have presented a discriminative approach for generating candidates for string transformation.
"Unlike conventional spelling-correction tasks, this study did not assume a fixed set of destination strings (e.g. correct words), but could even generate unseen candidate strings."
We used an L 1 -regularized logistic regression model with substring-substitution features so that candidate strings for a given string can be enumerated using the efficient algorithm.
"The results of experiments described herein showed re-markable improvements and usefulness of the pro-posed approach in three tasks: normalization of or-thographic variants, noun derivation, and lemmati-zation."
The method presented in this paper allows only one region of change in string transformation.
"A natural extension of this study is to handle mul-tiple regions of changes for morphologically rich languages (e.g. German) and to handle changes at the phrase/term level (e.g., “estrogen receptor” and “receptor of oestrogen”)."
Another direction would be to incorporate the methodologies for semi-supervised machine learning to accommodate situa- tions in which positive instances and/or unlabeled strings are insufficient.
Most attempts to integrate FrameNet in NLP systems have so far failed because of its lim-ited coverage.
"In this paper, we investigate the applicability of distributional and WordNet-based models on the task of lexical unit induc-tion, i.e. the expansion of FrameNet with new lexical units."
"Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined."
Most inference-based NLP tasks require a large amount of semantic knowledge at the predicate-argument level.
"This type of knowledge allows to identify meaning-preserving transformations, such as active/passive, verb alternations and nominal-izations, which are crucial in several linguistic in-ferences."
"Recently, the integration of NLP sys-tems with manually-built resources at the predi-cate argument-level, such as FrameNet[REF_CITE]and PropBank[REF_CITE]has received growing interest."
"For example,[REF_CITE]show the potential improvement that FrameNet can bring on the performance of a Ques-tion Answering (QA) system."
"Similarly, several other studies (e.g.[REF_CITE]) indicate that frame semantics plays a central role in Recognizing Textual Entailment (RTE)."
"Un-fortunately, most attempts to integrate FrameNet or similar resources in QA and RTE systems have so far failed, as reviewed respectively[REF_CITE]and[REF_CITE]."
These studies indicate limited coverage as the main reason of insuccess.
"Indeed, the FrameNet database only contains 10,000 lexical units (LUs), far less than the 210,000 entries in WordNet 3.0."
"Also, frames are based on more complex information than word senses, so that their manual development is much more demanding[REF_CITE]."
"Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed[REF_CITE]."
"Un-fortunately, research in this area is still somehow limited and fragmentary."
"The aim of our study is to pioneer in this field by proposing two unsuper-vised models for LU induction, one based on dis-tributional techniques and one using WordNet as a support; and a combined model which mixes the two."
"The goal is to investigate to what extent distri-butional and WordNet-based models can be used to induce frame semantic knowledge in order to safely extend FrameNet, thus limiting the high costs of manual annotation."
In Section 2 we introduce the LU induction task and present related work.
"In Sections 3, 4 and 5 we present our distributional, WordNet-based and com-bined models."
"Then, in Section 6 we report experi-mental results and comparative evaluations."
"Finally, in Section 7 we draw final conclusions and outline future work."
"As defined[REF_CITE], a frame is a con-ceptual structure modeling a prototypical situation, evoked in texts through the occurrence of its lex-ical units."
A lexical unit (LU) is a predicate that linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-guments.
"For example the frame K ILLING has lex-ical units such as assassin, assassinate, blood-bath, fatal, murderer, kill, suicide that share semantic ar-guments such as KILLER , INSTRUMENT , CAUSE , VICTIM ."
"Building on this frame-semantic model, the Berkeley FrameNet project[REF_CITE]has been developing a frame-semantic lexicon for the core vocabulary[REF_CITE]."
"The current FrameNet release contains 795 frames and about 10,000 LUs."
"Part of FrameNet is also a cor-pus of 135,000 annotated example sentences from the British National Corpus (BNC)."
LU induction is a fairly new task.
"Formally, it can be defined as the task of assigning a generic lexical unit not yet present in the FrameNet database (hereafter called unknown LU) to the cor-rect frame(s)."
As the number of frames is very large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then be mapped to more than one frame (for example the word tea could map both to F OOD and S O - CIAL E VENT ).
"Also, even unambiguous words can be assigned to more than one frame – e.g. child maps to both K INSHIP and P EOPLE BY AGE ."
"LU induction is relevant to many NLP tasks, such as the semi-automatic creation of new FrameNets, and semantic role labelling."
"LU induction has been integrated[REF_CITE]as part of the Frame Se-mantic Structure Extraction shared task[REF_CITE], where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet."
"Tested on the FrameNet gold standard, the method achieves an accuracy of 0.78, at the cost of a low coverage of 31% (i.e. many LUs are not assigned)."
"Burchardt and colleagues (2005) present Detour, a rule-based sys-tem using words in a WordNet relation with the un-known LU to find the correct frame."
The system achieves an accuracy of 0.39 and a coverage of 87%.
"Unfortunately this algorithm requires the LU to be previously disambiguated, either by hand or using contextual information."
"In a departure from previous work, our first model leverages distributional properties to induce LUs, in-stead of relying on pre-existing lexical resources as WordNet."
This guarantees two main advantages.
"First, it can predict a frame for any unknown LU, while WordNet based approaches can be applied only to words having a WordNet entry."
"Second, it allows to induce LUs in languages for which Word-Net is not available or has limited coverage."
"Our second WordNet-based model uses sense informa-tion to characterize the frame membership for un-known LU, by adopting a semantic similarity mea-sure which is sensitive to all the known LUs of a frame."
"The basic idea behind the distributional approach is to induce new LUs by modelling existing frames and unknown LUs in a semantic space, where they are represented as distributional co-occurrence vectors computed over a corpus."
Semantic spaces are widely used in NLP for rep-resenting the meaning of words or other lexical en-tities.
"They have been successfully applied in sev-eral tasks, such as information retrieval[REF_CITE]and harvesting thesauri[REF_CITE]."
"The intu-ition is that the meaning of a word can be described by the set of textual contexts in which it appears (Distributional Hypothesis[REF_CITE]), and that words with similar vectors are semantically related."
"In our setting, the goal is to find a semantic space model able to capture the notion of frame – i.e. the property of “being characteristic of a frame”."
"In such a model, an unknown LU is induced by first computing the similarity between its vector and the vectors of the existing frames, and then assigning the LU to the frame with the highest similarity."
"In our model, a LU l is represented by a vector ~l whose dimensions represent the set of contexts C of the semantic space."
"The value of each dimen-sion is given by the co-occurrence value of the LU with a contextual feature c ∈ C, computed over a large corpus using an association measure."
We ex-periment with two different association measures: normalized frequency and pointwise mutual infor-mation.
"We approximate these measures by using Maximum Likelihood Estimation, as follows:"
"F(l, c) = MLE ||∗l,, ∗c|| (1) MI(l, c) = MLE ||l∗,,cc||||∗l,, ∗∗|| where |l,c| denotes the co-occurrence counts of the pair (l, c) P in the corpus, |∗, c| ="
"P P l∈L |l, c|, |l, ∗| = c∈C |l, c| and finally |∗, ∗| = l∈L,c∈C |l, c|."
"A frame f is modeled by a vector f~, representing the distributional profile of the frame in the seman-tic space."
We here assume that a frame can be fully described by the set of its lexical units F .
"We imple-ment this intuition by computing f~ as the weighted centroid of the set F , as follows: f~ = X w lf ∗ ~l (2) l∈F where w lf is a weighting factor, accounting for the relevance of a given lexical unit with respect to the frame, estimated as: w lf = X |l| (3) |l| l∈F where |l| denotes the counts of l in the corpus."
"From a more cognitive perspective, the vector f~ rep-resents the prototypical lexical unit of the frame."
"Given the set of all frames N and an unknown lex-ical unit ul, we assign ul to the frame fmax ul which is distributionally most similar – i.e. we intuitively map an unknown lexical unit to the frame whose prototypical lexical unit f~ has the highest similarity with ul~ : fmax ul = argmax f∈N sim D (ul~ , f~) (4)"
"In our model, we used the traditional cosine simi-larity: sim cos (ul, f) = ul~ · f~ (5) |ul~ | ∗ |f~|"
Different types of contexts C define spaces with dif-ferent semantic properties.
We are here looking for a space able to capture the properties which charac-terise a frame.
The most relevant of these properties is that LUs in the same frame tend to be either co-occurring or substitutional words (e.g. assassin/kill or assassinate/kill) – i.e. they are either in paradig-matic and syntagmatic relation.
"In an ideal space, a high similarity value sim D would be then given both to assassinate/kill and to assassin/kill."
We ex-plore three spaces which seem to capture the above property well:
Contexts are words appear-ing in a n-window of the lexical unit.
Such spaces model a generic notion of semantic relatedness.
"Two LUs close in the space are likely to be re-lated by some type of generic semantic relation, either paradigmatic (e.g. synonymy, hyperonymy, antonymy) or syntagmatic (e.g. meronymy, concep-tual and phrasal association). [Footnote_1]"
1 See[REF_CITE]for an in depth analysis.
"Contexts are syntactic re-lations (e.g. X-VSubj-man where X is the LU), as described[REF_CITE]."
These spaces are good at modeling semantic similarity.
"Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in a is-a hierarchy[REF_CITE]."
"Indeed, as con-texts are syntactic relations, targets with the same part of speech are much closer than targets of differ-ent types."
"Mixed space: In a combination of the two above spaces, contexts are words connected to the LU by a dependency path of at most length n. Unlike word-based spaces, contexts are selected in a more princi-pled way: only syntactically related words are con-texts, while other (possibly noisy) material is filtered out."
"Unlike syntax-based spaces, the context c does not explicitly state the type of syntactic relation with the LU: this usually allows to capture both paradig-matic and syntagmatic relations."
"In a departure from previous work, our WordNet-based model does not rely on standard WordNet sim-ilarity measures[REF_CITE], as these measures can only be applied to pairs of words, while we here need to capture the meaning of whole frames, which typically consist of larger sets of LUs."
"Our intuition is that senses able to evoke a frame can be detected via WordNet, by jointly considering the WordNet synsets activated by all LUs of the frame."
"We implement this intuition in a weakly-supervised model, where each frame f is repre-sented as a set of specific sub-graphs of the WordNet hyponymy hierarchy."
"As different parts of speech have different WordNet hierarchies, we build a sub-graph for each of them: S nf for nouns, S fv for verbs and S fa for adjectives. [Footnote_2]"
"2 Our WordNet model does not cover the limited number of LUs which are not nouns, verbs or adjectives."
These sub-graphs repre-sent the lexical semantic properties characterizing the frame.
"An unknown LU ul of a given part of speech is assigned to the frame whose correspond-ing sub-graph is semantically most similar to one of the senses of ul: fmax ul = argmax f∈N sim WN (ul, f) (6) where sim WN is a WordNet-based similarity measure."
In the following subsections we will de-scribe how we build sub-graphs and model the sim-ilarity measure for the different part of speech.
"Figure 1 reports an excerpt of the noun sub-graph for the frame P EOPLE BY A GE , cover-ing the suitable senses of its nominal LUs {adult, baby, boy, kid, youngster, youth}."
"The relevant senses (e.g. sense 1 of youth out of the 6 potential ones) are generally selected, as they share the most specific generalizations in WordNet with the other words."
"To compute similarity for nouns we adopt conceptual density (cd)[REF_CITE], a semantic similarity model previously applied to word sense disambiguation tasks."
"Given a frame f and its set of nominal lexical units F n , the nominal subgraph S fn is built as fol-lows."
All senses of all words in F n are activated in WordNet.
All hypernyms H fn of these senses are then retrieved.
"Every synset σ ∈ H fn is given a cd score, representing the density of the WordNet sub-hierarchy rooted at σ in representing the set of nouns F n ."
"The intuition behind this model is that the larger the number of LUs in F n that are generalized by σ is, the better it captures the lexical semantics intended by the frame f. Broader generalizations are penal-ized as they give rise to bigger hierarchies, not well correlated with the full set of targets F n ."
"To build the final sub-graph S fn , we apply the greedy algorithm proposed by Basili and colleagues (2004)."
"It first computes the set of WordNet synsets that generalize at least two LUs in F n , and then se-lects the subset of most dense ones S fn ⊂ H fn that cover F n ."
"If a LU has no common hypernym with other members of F n , it is not represented in S fn , and its similarity is set to 0 ."
S nf disambiguates words in F n as only the lexical senses with at least one hyper-nym in S fn are considered.
Figure 1 shows the nominal sub-graph automati-cally derived using conceptual density for the frame P EOPLE BY A GE .
"The word boy is successfully dis-ambiguated, as its only hypernym in the sub-graph refers to its third sense (a male human offspring) which correctly maps to the given frame."
Notice that this model departs from the first sense heuris-tics largely successful in word sense disambigua-tion: most frames in fact are characterized by non predominant senses.
The only questionable disam-biguation is for the word adult: the wrong sense (adult mammal) is selected.
"However, even in these cases, the cd values are very low (about 10 −4 ), so that they do not impact much on the quality of the resulting inference."
"Using this model, LU induction is performed as follows."
"Given an unknown lexical unit ul, for each frame f ∈ N we first build the sub-graph S fn from the set F n ∪ {ul}."
"We then compute sim WN (f, ul) as the maximal cd of any synset σ ∈ S fn that gener-alizes one of the lexical senses of ul."
"In the example baby would receive a score of 0.117 according to its first sense in WordNet 2.0 (“baby,babe,infant”)."
"In a final step, we assign the LU to the most similar frame, according to Eq. 6 Verbs and Adjectives."
"As the conceptual density algorithm can be used only for nouns, we apply dif-ferent similarity measures for verbs and adjectives."
"For verbs we exploit the co-hyponymy relation: the sub-graph S vf is given by all hyponyms of all verbs F v in the frame f. Similarity sim WN (f,ul) is computed as follows:   1 iff ∃K ⊂ F such that |K| &gt; τ AND sim WN (ul, f) =  ∀l ∈ K, l is a co-hyponym of ul  ² otherwise (7)"
"As for adjectives, WordNet does not provide a hy-ponymy hierarchy."
"We then compute similarity sim-ply on the basis of the synonymy relation, as fol-lows:   1 iff ∃l ∈ F such that l is a synonym of ul sim WN (ul, f) =  (8) ² otherwise"
The methods presented so far use two independent information sources to induce LUs: distributional similarity sim D and WordNet similarity sim WN .
"We also build a joint model, leveraging both ap-proaches: we expect the combination of different information to raise the overall performance."
"We here choose to combine the two approaches using a simple back-off model, that uses the WordNet-based model as a default and backs-off to the distributional one when no frame is proposed by the former."
"The intuition is that WordNet should guarantee the high-est precision in the assignment, while distributional similarity should recover cases of low coverage."
"In this section we present a comparative evaluation of our models on the task of inducing LUs, in a leave-one-out setting over a reference gold standard."
"Our gold standard is the FrameNet 1.3 database, containing 795 frames and a set L of 7,522 unique LUs (in all there are 10,196 LUs possibly assigned to more than one frame)."
"Given a lexical unit l ∈ L, we simulate the induction task by executing a leave-one-out procedure, similarly to Burchardt and col- leagues (2005)."
"First, we remove l from all its origi-nal frames."
"Then, we ask our models to reassign it to the most similar frame(s) f, according to the simi-larity measure [Footnote_3] ."
"3 In the distributional model, we recompute the centroids for each frame f in which the LU appeared, applying Eq. 2 to the set F − {l}."
We repeat this procedure for all lex-ical units.
"Though our experiment is not completely realistic (we test over LUs already in FrameNet), it has the advantage of a reliable gold standard pro-duced by expert annotators."
"A second, more re-alistic, small-scale experiment is described in Sec-tion 6.2."
We compute accuracy as the fraction of LUs in L that are correctly re-assigned to the original frame.
"Accuracy is computed at different levels k: a LU l is correctly assigned if its gold standard frame appears among the best-k frames f ranked by the model us-ing the sim(l, f) measure."
"As LUs can have more than one correct frame, we deem as correct an as-signment for which at least one of the correct frames is among the best-k."
"We also measure coverage, intended as the per-centage of LUs that have been assigned to at least one frame by the model."
"Notice that when no sense preference can be found above the threshold ², the WordNet-based model cannot predict any frame, thus decreasing coverage."
We present results for the following models and parametrizations (further parametrizations have re-vealed comparable performance).
Dist-word : the word-based space described in Section 3.
"Contextual features correspond to the set of the [Footnote_4],000 most frequent words in the BNC. 4 The association measure between LUs and contexts is the pointwise mutual information."
"4 We didn’t use the FrameNet corpus directly, as it is too small to obtain reliable statistics."
Valid contexts for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described in Section 3.
"Context features are the 10,000 most frequent syntactic relations in the BNC [Footnote_5] ."
"5 Specifically, we use the minimum context selection func-tion and the plain path value function described[REF_CITE]."
As associ-ation measure we apply log-likelihood ratio[REF_CITE]to normalized frequency.
Syntactic rela-tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec- tion 3.
"As for the Dist-word model, contextual fea-tures are 4,000 and pointwise mutual information is the association measure."
The maximal dependency path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described in Section 4.
WNet-bsense : this model is computed as WNet-full but using only the most frequent sense for each LU as defined in WordNet.
Combined : the combined method presented in Section 5.
"Specifically, it uses WNet-full as a default and Dist-word as back-off."
"Baseline-rnd : a baseline model, randomly as-signing LUs to frames."
Baseline-mostfreq : a model predicting as best-k frames the most likely ones in FrameNet – i.e. those containing the highest number of LUs.
"Table 1 reports accuracy and coverage results for the different models, considering only 6792 LUs with frequency higher than 5 in the BNC, and frames with more than 2 lexical units (to allow better gen-eralizations in all models)."
"Results show that all our models largely outperform both baselines, achieving a good level of accuracy and high coverage."
"In particular, accuracy for the best-10 frames is high enoungh to support tasks such as the semi-automatic creation of new FrameNets."
"This claim is supported by a further task-driven experiment, in which we asked 3 annotators to assign 60 unknown LUs (from the Detour system log) to frames, with and without the support of the Dist-word model’s predictions as suggestions [Footnote_6] ."
"6 For this purpose, the dataset is evenly split in two parts."
We verified that our model guarantee an annotation speed-up of 25% – i.e. in average an annotator saves 25% of annotation time by using the system’s suggestions.
Distributional vs. WordNet-based models.
"WordNet-based models are significantly better than distributional ones, for several reasons."
"First, distri-butional models acquire information only from the contexts in the corpus."
"As we do not use a FrameNet annotated corpus, there is no guarantee that the us-age of a LU in the texts reflects exactly the semantic properties of the LU in FrameNet."
"In the extreme cases of polysemous LUs, it may happen that the textual contexts refer to senses which are not ac-counted for in FrameNet."
"In our study, we explicitly ignore the issue of polisemy, which is a notoriously hard task to solve in semantics spaces (see[REF_CITE]), as the occurrences of different word senses need to be clustered separately."
We will approach the problem in future work.
"The WordNet-based model suffers from the problem of polisemy to a much lesser extent, as all senses are explicitly rep-resented and separated in WordNet, including those related to the FrameNet gold standard."
A second issue regards data sparseness.
"The vec-torial representation of LUs with few occurrences in the corpus is likely to be semantically incomplete, as not enough statistical evidence is available."
Par-ticularly skewed distributions can be found when some frames are very rarely represented in the cor-pus.
A more in-depth descussion on these two issues is given later in this section.
"Regarding the WordNet-based models, WNet-full in most cases outperforms WNet-bsense."
"The first sense heuristic does not seem to be as effective as in other tasks, such as Word Sense Disambigua-tion."
"Although sense preferences (or predominance) across two general purpose resources, such as Word-Net and FrameNet, should be a useful hint, the con-ceptual density algorithm seems to produce better distributions (i.e. higher accuracy), especially when several solutions are considered."
"Indeed, for many LUs the first WordNet sense is not the one repre-sented in the FrameNet database."
"As for distributional models, results show that the Dist-word model performs best."
"In general, syntac-tic relations (Dist-syntax model) do not help to cap-ture frame semantic properties better than a simple window-based approach."
"This seems to indicate that LUs in a same frame are related both by paradig-matic and syntagmatic relations, in accordance to the definition given in Section 3.2 – i.e. they are mostly semantically related, but not similar."
Distributional models show a coverage 15% higher than WordNet-based ones.
"Indeed, as far as corpus evidence is available (i.e. the unknown LU appears in the corpus), distributional methods are al-ways able to predict a frame."
WordNet-based mod- els cannot make predictions in two specific cases.
"First, when the LU is not present in WordNet."
"Sec-ond, when the function sim WN does not has suffi-cient relational information to find a similar frame."
"This second factor is particularly evident for adjec-tives, as Eq. 8 assigns a frame only when a synonym of the unknown LU is found."
It is then not surpris-ing that 68% of the missed assignment are indeed adjectives.
"Results for the Combined model suggest that the integration of distributional and WordNet-based methods can offer a viable solution to the cover-age problem, as it achieves an accuracy comparable to the pure WordNet approaches, while keeping the coverage high."
A major issue when using dis-tributional approaches is that words with low fre-quency tend to have a very sparse non-meaningful representation in the vector space.
This highly im-pacts on the accuracy of the models.
"To measure the impact of data sparseness, we computed the ac- curacy at different frequency cuts – i.e. we exclude LUs below a given frequency threshold from cen-troid computation and evaluation."
"Figure 2 reports the results for best-10 assignment at different cuts, for the Dist-word model."
"As expected, accuracy im-proves by excluding infrequent LUs."
"Only at a fre-quency cut of 200 performance becomes stable, as statistical evidence is enough for a reliable predic-tion."
"Yet, in a real setting the improvement in accu-racy implies a lower coverage, as the system would not classify LUs below the threshold."
"For example, by discarding LUs occurring less than 200 times in the corpus, we obtain a +0.12 improvement in accu-racy, but the coverage decreases to 57%."
"However, uncovered LUs are also the most rare ones and their relevance in an application may be negligible."
"Lexical Semantics, Ambiguity and Plausible As-signments."
"The overall accuracies achieved by our methods are “pessimistic”, in the sense that they should be intended as lower-bounds."
"Indeed, a qual-itative analysis of erroneous predictions reveals that in many cases the frame assignments produced by the models are semantically plausible, even if they are considered incorrect in the leave-one-out test."
"Consider for example the LU guerrilla, assigned in FrameNet to the frame P EOPLE BY V OCATION ."
"Our mixed model proposes as two most similar frames M ILITARY and T ERRORISM , which could still be considered plausible assignment."
"The same holds for the LU caravan, for which the most similar frame is V EHICLE , while in FrameNet the LU is as-signed only to the frame B UILDINGS ."
"These cases are due to the low FrameNet coverage, i.e LUs are not fully annotated and they appear only in a subset of their potential frames."
The real accuracy of our models is therefore expected to be higher.
"To explore the issue, we carried out a qualita-tive analysis of 5 words (i.e. abandon.v, accuse.v, body.n, charge.v and partner.n)."
"For each of them, we randomly picked 60 sentences from the BNC corpus, and asked two human annotators to assign to the correct frame the occurrence of the word in the given sentence."
"For 2 out of 5 words, no frame could be found for most of the sentences, suggesting that the most frequent frames for these words were missing from FrameNet 7 ."
"We can then conclude that 100% accuracy cannot be considered as the upper-bound of our experiment, as word usage in texts is not well reflected in the FrameNet modelling."
We also tested our models on a realistic gold-standard set of 24 unknown LUs extracted from the[REF_CITE]corpus[REF_CITE].
These are words not present in FrameNet 1.3 which have been assigned by human annotators to an existing frame [Footnote_8] .
8 The set does not contain 4 LUs which have no frame in FrameNet.
"WNet-full achieves an accu-racy of 0.25 for best-1 and 0.69 for best-10, with a coverage of 67%."
A qualitative analysis showed that the lower performance wrt to our main experiment is due to higher ambiguity of the LUs (e.g. we assign tea to S OCIAL EVENT instead of F OOD ).
Comparison to other approaches.
We compare our models to the system presented[REF_CITE]and Burchardt and col-leagues (2005).
7 Note that the need of new frames to account for seman-tic phenomena in free texts has been also demonstrated by the[REF_CITE]competition.
They measure accuracy at different coverage levels.
"These results confirm that WordNet-based approaches, while being highly accurate wrt dis-tributional ones, present strong weaknesses as far as coverage is concerned."
"Furthermore,[REF_CITE]show that their machine learn- ing approach outperforms a simple approach based on WordNet similarity: thus, our results indirectly prove that our WordNet-based method is more ef-fective than the application of the similarity measure presented[REF_CITE]."
We also compare our results to those reported by Burchardt and colleagues (2005) for Detour.
"Though the experimental setting is slightly different (LU assignment is done at the text-level), they use the same gold standard and leave-one-out technique, reporting a best-1 accuracy of 0.38 and a coverage of 87%."
"Our WordNet-based models significantly outperform Detour on best-1 accuracy, at the cost of lower coverage."
"Yet,our combined model is signifi-cantly better both on accuracy (+5%) and coverage (+8%)."
"Also, in most cases Detour cannot predict more than one frame (best-1), while our accuracies can be improved by relaxing to any best-k level."
In this paper we presented an original approach for FrameNet LU induction.
"Results show that mod-els combining distributional and WordNet informa-tion offer the most viable solution to model the no-tion of frame, as they allow to achieve a reasonable trade-off between accuracy and coverage."
"We also showed that in contrast to previous work, simple se-mantic spaces are more helpful than complex syn-tactic ones."
Results are accurate enough to support the creation and the development of new FrameNets.
"As future work, we will evaluate new types of spaces (e.g. dimensionality reduction methods) to improve the generalization capabilities of the space models."
"We will also address the data sparseness is-sue, by testing smoothing techniques to better model low frequency LUs."
"Finally, we will implement the presented models in a complex architecture for semi-supervised FrameNets development, both for specializing the existing English FrameNet in spe-cific domains, and for creating new FrameNets in other languages."
We investigate the combination of several sources of information for the purpose of sub-jectivity recognition and polarity classification in meetings.
"We focus on features from two modalities, transcribed words and acoustics, and we compare the performance of three dif-ferent textual representations: words, charac-ters, and phonemes."
"Our experiments show that character-level features outperform word-level features for these tasks, and that a care-ful fusion of all features yields the best perfor-mance. [Footnote_1]"
"1 This work was supported by the Dutch BSIK-project Mul-timediaN, and the European IST Programme Project FP6- 0033812. This paper only reflects the authors’ views and fund-ing agencies are not liable for any use that may be made of the information contained herein."
"Opinions, sentiments and other types of subjective content are an important part of any meeting."
"Meet-ing participants express pros and cons about ideas, they support or oppose decisions, and they make suggestions that may or may not be adopted."
"When recorded and archived, meetings become a part of the organizational knowledge, but their value is lim-ited by the ability of tools to search and summa-rize meeting content, including subjective content."
"While progress has been made on recognizing pri-marily objective meeting content, for example, in-formation about the topics that are discussed[REF_CITE]and who is assigned to work on given tasks[REF_CITE], there has been fairly little work specifically directed toward recog-nizing subjective content."
"In contrast, there has been a wealth of research over the past several years on automatic subjectiv-ity and sentiment analysis in text, including on-line media."
"Partly inspired by the rapid growth of so-cial media, such as blogs, as well as on-line news and reviews, researchers are now actively address-ing a wide variety of new tasks, ranging from blog mining (e.g., finding opinion leaders in an on-line community), to reputation management (e.g. find-ing negative opinions about a company on the web), to opinion-oriented summarization and question an-swering."
"Yet many challenges remain, including how best to represent and combine linguistic infor-mation for subjectivity analysis."
"With the additional modalities that are present when working with face-to-face spoken communication, these challenges are even more pronounced."
The work in this paper focuses on two tasks: (1) recognizing subjective utterances and (2) discrimi-nating between positive and negative subjective ut-terances.
"An utterance may be subjective because the speaker is expressing an opinion, because the speaker is discussing someone else’s opinion, or be-cause the speaker is eliciting the opinion of someone else with a question."
"We approach the above tasks as supervised ma-chine learning problems, with the specific goal of finding answers to the following research questions: • Given a variety of information sources, such as text arising from (transcribed) speech, phoneme representations of the words in an ut-terance, and acoustic features extracted from the audio layer, which of these sources are par-ticularly valuable for subjectivity analysis in multiparty conversation? • Does the combination of these sources lead to further improvement? • What are the optimal representations of these information sources in terms of feature design for a machine learning component?"
"A central tenet of our approach is that subword representations, such as character and phoneme n-grams, are beneficial for the tasks at hand."
"Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information for various text classification tasks."
"An example of character n-grams is the set of 3-grams {#se, sen, ent, nti, tim, ime, men, ent, nt#, t#a, #an, ana, nal, aly, lys, ysi, sis, is#} for the two-word phrase sentiment analysis."
The special symbol # represents a word boundary.
"While it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully been used for fine-grained classification tasks, such as named-entity recogniti[REF_CITE]and subjective sentence recogniti[REF_CITE], as well as a variety of document-level tasks[REF_CITE]."
The informativeness of these low-level features comes in part from a form of attenuati[REF_CITE]: a slight abstraction of the underlying data that leads to the formation of string equivalence classes.
"For instance, words in a sentence will in-variably share many character n-grams."
"Since ev-ery unique character n-gram in an utterance consti-tutes a separate feature, this leads to the formation of string classes, which is a form of abstraction."
"For example,[REF_CITE]investigate similar subword representations, called key substring group features."
"By compressing substrings in a corpus in a trie (a prefix tree), and labeling entire sets of distri-butionally equivalent substrings with one group la- bel, an attenuation effect is obtained that proves very beneficial for a number of text classification tasks."
"Aside from attenuation effects, character n-grams, especially those that represent word bound-aries, have additional benefits."
Treating word boundaries as characters captures micro-phrasal in-formation: short strings that express the transition of one word to another.
"Stemming occurs naturally within the set of initial character n-grams of a word, where the suffix is left out."
"Also, some part-of-speech information is captured."
"For example, the modals could, would, should can be represented by the 4-gram, ould, and the set of adverbs ending in -ly can be represented by the 3-gram ly#."
"A challenging thought is to extend the use of n-grams to the level of phonemes, which comprise the first symbolic level in the process of sound to grapheme conversion."
"If n-grams of phonemes com-pare favorably to word n-grams for the purpose of sentiment classification, then significant speedups can be obtained for online sentiment classification, since tokenization of the raw speech signal can make a halt at the phoneme level."
For this work we use 13 meetings from the AMI Meeting Corpus[REF_CITE].
Each meet-ing has four participants and is approximately 30 minutes long.
"The participants play specific roles (e.g., Project Manager, Marketing Expert) and to-gether function as a design team."
"Within the set of 13 meetings, there are a total of 20 participants, with each participant taking part in two or three meet-ings as part of the same design team."
"Meetings with the same set of participants represent different stages in the design process (e.g., Conceptual Design, De-tailed Design)."
The meetings used in the experiments have been annotated for subjective content using the AMIDA annotation scheme[REF_CITE].
Table 1 lists the types of annotations that are marked in the data.
"There are three main categories of annotations, sub-jective utterances, subjective questions, and objec-tive polar utterances."
A subjective utterance is a span of words (or possibly sounds) where a pri-vate state is being expressed either through choice of words or prosody.
"A private state[REF_CITE]is an internal mental or emotional state, including opinions, beliefs, sentiments, emotions, evaluations, uncertainties, and speculations, among others."
"Al-though typically when a private state is expressed it is the private state of the speaker, as in example (1) below, an utterance may also be subjective be-cause the speaker is talking about the private state of someone else."
"For example, in (2) the negative opinion attributed to the company is what makes the utterance subjective. (1) Finding them is really a pain, you know (2) The company’s decided that teletext is out-dated"
Subjective questions are questions in which the speaker is eliciting the private state of someone else.
"In other words, the speaker is asking about what someone else thinks, feels, wants, likes, etc., and the speaker is expecting a response in which the other person expresses what he or she thinks, feels, wants, or likes."
"For example, both (3) and (4) below are subjective questions. (3) Do you like the large buttons? (4) What do you think about the large buttons?"
Objective polar utterances are statements or phrases that describe positive or negative factual information about something without conveying a private state.
"The sentence The camera broke the first time I used it gives an example of negative factual information; generally, something breaking the first time it is used is not good."
"For the work in this paper, we focus on recog-nizing subjectivity in general and distinguishing be-tween positive and negative subjective utterances."
"Positive subjective utterances are those in which any of the following types of private states are expressed: agreements, positive sentiments, positive sugges-tions, arguing for something, beliefs from which positive sentiments can be inferred, and positive re-sponses to subjective questions."
"Negative subjective utterances express private states that are the oppo-site of those represented by the positive subjective category: disagreements, negative sentiments, nega-tive suggestions, arguing against something, beliefs from which negative sentiments can be inferred, and negative responses to subjective questions."
Example (5) below contains two positive subjective utterances and one negative subjective utterance.
Each annota-tion is indicated by a pair of angle brackets. (5)
Um hPOS-SUBJ it’s very easy to usei.
Um hNEG-SUBJ but unfortunately it does lack the advanced functionsi hPOS-SUBJ which I I quite like having on the controlsi.
The positive and negative subjective category is for marking cases of positive and negative subjectivity that are so closely interconnected that it is difficult or impossible to separate the two.
"For example, (6) below is marked as both positive and negative sub-jective. (6)"
"Um hPOS-AND-NEG-SUBJ they’ve also suggested that we um we only use the remote control to control the television, not the VCR, DVD or anything elsei."
"In[REF_CITE], agreement is measured for each class separately at the level of dialogue act segments."
"If a dialogue act overlaps with an annotation of a particular type, then the segment is considered to be labelled with that type."
"Table 2 gives the Kappa[REF_CITE]and % agreement for subjective seg-ments, positive and negative subjective segments, [Footnote_2] and subjective questions."
2 A positive subjective segment is any dialogue act segment that overlaps with a positive subjective utterance or a positive-and-negative subjective utterance. The negative subjective seg-ments are defined similarly.
We conduct two sets of classification experiments.
"For the first set of experiments (Task 1), we auto-matically distinguish between subjective and non-subjective utterances."
"For the second set of ex-periments (Task 2), we focus on distinguishing be-tween positive and negative subjective utterances."
"For both tasks, we use the manual dialogue act seg-ments available as part of the AMI Corpus as the unit of classification."
"For Task 1, a segment is considered subjective if it overlaps with either a subjective utter-ance or subjective question annotation."
"For Task 2, the segments being classified are those that overlap with positive or negative subjective utterances."
"For this task, we exclude segments that are both positive and negative."
"Although limiting the set of segments to be classified to just those that are positive or nega-tive makes the task somewhat artificial, it also allows us to focus in on the performance of features specifi-cally for this task. [Footnote_3][REF_CITE]subjective and 8707 non-subjective dialog acts for Task 1 (with an aver-age duration of 1.9s, standard deviation of 2.0s), and 3157 positive subjective and 1052 negative subjec-tive dialog acts for Task 2 (average duration of 2.6s, standard deviation of 2.3s)."
"3 In practice, this excludes about 7% of the positive/negative segments."
The experiments are performed using 13-fold cross validation.
"Each meeting constitutes a separate fold for testing, e.g., all the segments from meeting 1 make up the test set for fold 1."
"Then, for a given fold, the segments from the remaining 12 meetings are used for training and parameter tuning, with roughly a 85%, 7%, and 8% split between training, tuning, and testing sets for each fold."
"The assignment to training versus tuning set was random, with the only constraint being that a segment could only be in the tuning set for one fold of the data."
The experiments we perform involve two steps.
"First, we train and optimize a classifier for each type of feature using BoosTexter[REF_CITE]"
"Then, we investigate the per-formance of all possible combinations of features using linear combinations of the individual feature classifiers."
"The two modalities that are investigated, prosodic, and textual, are represented by four different sets of features: prosody (PROS), word n-grams (WORDS), character n-grams (CHARS), and phoneme n-grams (PHONES)."
"Based on previous research on prosody modelling in a meeting context[REF_CITE]and on the literature in emotion research[REF_CITE]we extract PROS features that are mainly based on pitch, energy and the distribution of energy in the long-term averaged spectrum (LTAS) (see Table 3)."
These features are extracted at the word level and aggregated to the dialogue-act level by taking the average over the words per dialogue act.
We then normalize the features per speaker per meeting by converting the raw feature values to z-scores (z = (x − µ)/σ).
"The textual features, WORDS and CHARS, and the PHONES features are based on a manual tran-scription of the speech."
The PHONES were pro-duced through dictionary lookup on the words in the reference transcription.
Both CHARS and PHONES representations include word boundaries as informa-tive tokens.
The textual features for a given seg-ment are simply all the WORDS/CHARS/PHONES in that segment.
Selection of n-grams is performed by the learning algorithm.
"We train four single source classifiers using BoosT-exter, one for each type of feature."
"For the WORDS, CHARS, and PHONES, we optimize the classi-fier by performing a grid search over the parame-ter space, varying the number of rounds of boosting (100, 500, 1000, 2000, 5000), the length of the n-gram (1, 2, 3, 4, 5), and the type of n-gram."
"Boos-Texter can be run with three different n-gram con-figurations: n-gram, s-gram, and f-gram."
"For the default configuration (n-gram), BoosTexter searches for n-grams up to length n."
"For example, if n = 3, BoosTexter will consider 1-grams, 2-grams, and 3-grams."
"For the s-gram configuration, BoosTexter will in addition consider sparse n-grams (i.e., n-grams containing wildcards), such as the * idea."
"For the f-gram configuration, BoosTexter will only con-sider n-grams of a maximum fixed length, e.g., if n = 3 BoosTexter will only consider 3-grams."
"For the PROS classifier, only the number of rounds of boosting was varied."
"The parameters are selected for each fold separately; the parameter set that pro-duces the highest subjective F 1 score on the tuning set for Task 1, and the highest positive subjective F 1 score for Task 2, is used to train the final classifier for that fold."
"After the single source classifiers have been trained, they have to be combined into an aggregate classi-fier."
"To this end, we decided to apply a simple linear interpolation strategy."
"Linear interpolation of mod-els is the weighted combination of simple models to form complex models, and has its roots in generative language models[REF_CITE].[REF_CITE]has demonstrated its use for discrim-inative machine learning."
"In the present binary class setting, BoosTexter produces two decision values, one for every class."
"For every individual single-source classifier (i.e., PROS, WORDS, CHARS and PHONES), separate weights are estimated that are applied to the decision values for the two classes produced by these classi-fiers."
These weights express the relative importance of the single-source classifiers.
The prediction of an aggregate classifier for a class c is then simply the sum of all weights for all participating single-source classifiers applied to the decision values these classifiers produce for this class.
"The class with the maximum score wins, just as in the simple non-aggregate case."
"Formally, then, this linear interpolation strategy finds for n single-source classifiers n interpolation weights λ 1 , . . . λ n that minimize the empirical loss (measured by a loss function L), with λ j the weight of classifier j (λ ∈ [0, 1]), and C cj (x i ) the decision value of class c produced by classifier j for datum x i (a feature vector)."
"The two classes are denoted with 0, 1."
The true class for datum x i is denoted with x̂ i .
The loss function is in our case based on subjective F-measure (Task 1) or positive subjective F-measure (Task 2) measured on heldout development training and test data.
"The aggregate prediction x̃ i for datum x i on the basis of n single-source classifiers then becomes x̃ i = arg max( X n λ j · C jc=0 ( x i ), X λ n c j · C cj=1 (x i )) j=1 j=1 (1) and the lambdas are defined as λ nj = arg min X k L(x̂ i , x̃ i ; λ j , . . . , λ n ) (2) λ nj ⊂[0,1] i"
The search process for these weights can easily be implemented with a simple grid search over admis-sible ranges.
"In the experiments described below, we investi-gate all possible combinations of the four differ-ent sets of features (PROS, WORDS, CHARS, and PHONES) to determine which combination yields the best performance for subjectivity and subjective polarity recognition."
Results for the two tasks are given in Tables 4 and 5 and in Figures 1 and 2.
"We use two baselines, listed at the top of each table."
The bullets in a given row indicate the features that are being evaluated for a given experiment.
"In Table 4, subjective F 1 , recall, and precision are reported as well as overall accu-racy."
"In Table 4, the F 1 , recall, and precision scores are for the positive subjective class."
All values in the tables are averages over the 13 folds.
"It is quite obvious that the combination of differ-ent sources of information is beneficial, and in gen-eral, the more information the better the results."
"The best performing classifier for Task 1 uses all the fea-tures, achieving a subjective F 1 of 67.1."
"For Task 2, the best performing classifier also uses all the fea-tures, although it does not perform significantly bet-ter than the classifier using only WORDS, CHARS, and PHONES. [Footnote_4] This classifier achieves a positive-subjective F 1 of 89.9."
"4 We measured significance with the non-parametric Wilcoxon signed rank test, p &lt; 0.05."
We measured the effects of adding more infor-mation to the single source classifiers.
These re-sults are listed in Table 6.
"Of the various feature types, prosody seems to be the least informative for both subjectivity and polarity classification."
"In ad-dition to producing the single-source classifier with the lowest performance for both tasks, Table 6 shows that when prosody is added, of all the features it is least likely to yield significant improvements."
"Throughout the experiments, adding an additional type of textual feature always yields higher results."
"In all cases but two, these improvements are sig-nificant."
The best performing of the features are the character n-grams.
"Of the single-source exper-iments, the character n-grams achieve the best per-formance, with significant improvements in F 1 over the other single-source classifiers for both Task 1 and Task 2."
"Also, adding character n-grams to other feature combinations always gives significant im-provements in performance."
An obvious question that remains is what the ef-fect is of classifier interpolation on the results.
"To answer this question, we conducted two additional experiments for both tasks."
"First, we investigated the performance of an uninterpolated combination of the four single-source classifiers."
"In essence, this combines the separate feature spaces without explic-itly weighting them."
"Second, we investigated the re-sults of training a single BoosTexter model using all the features, essentially merging all feature spaces into one agglomerate feature space."
"The results for these experiments are given in Table 7, along with the results from the all-feature interpolated classifi-cation for comparison."
The results in Table 7 show that interpolation outperforms both the unweighted and single-model combinations for both tasks.
"For Task 1, the ef-fect of interpolation compared to a single model is marginal (a .03 point difference in F 1 )."
"However, compared to the uninterpolated combination, inter-polation gives a clear 3.1 points improvement of F 1 ."
"For Task 2, interpolation outperforms both the unin-terpolated and single-model classifiers, with 2 and 3 points improvements in F 1 , respectively."
"Previous work has demonstrated that textual units below the word level, such as character n-grams, are valuable sources of information."
"Character-level models have successfully been used for named-entity recogniti[REF_CITE], predicting authorship[REF_CITE], text categorizati[REF_CITE], web page genre identificati[REF_CITE], and sentence-level subjectivity recogniti[REF_CITE]"
"In spoken-language data,[REF_CITE]achieves good results using chains of phonemes to automatically segment meetings ac-cording to topic."
"However, to the best of our knowl-edge there has been no investigation to date on the combination of character-level, phoneme-level, and word-level models for any natural language classifi-cation tasks."
"In text, there has been a significant amount of research on subjectivity and sentiment recognition, ranging from work at the phrase level to work on classifying sentences and documents."
"Sentence-level subjectivity classification (e.g.,[REF_CITE]) and sentiment classification (e.g.,[REF_CITE]) is the research in text most closely related to our work."
"Of the sentence-level research, the most similar is work[REF_CITE]comparing word-spanning character n-grams to word-internal char-acter n-grams for subjectivity classification in news data."
They found that character n-grams spanning words perform the best.
"Research on recognizing subjective content in multiparty conversation includes work[REF_CITE]on recognizing sentiments and arguing in meetings, work by Neiberg el al. (2006) on recognizing positive, negative, and neutral emo-tions in meetings, work on recognizing agreements and disagreements in meetings[REF_CITE], and work[REF_CITE]on recognizing meet-ing hotspots."
Somasundaran et al. use lexical and discourse features to recognize sentences and turns where meeting participants express sentiments or ar-guing.
"They also use the AMI corpus in their work; however, the use of different annotations and task definitions makes it impossible to directly compare their results and ours."
Neiberg et al. use acoustic– prosodic features (Mel-frequency Cepstral Coeffi- cients (MFCCs) and pitch features) and lexical n-grams for recognizing emotions in the ISL Meeting Corpus[REF_CITE].
Agreements and disagreements are a subset of the private states represented by the positive and neg-ative subjective categories used in this work.
"To recognise agreements and disagreements automati-cally, Hillard et al. train 3-way decision tree clas-sifiers (agreement, disagreement, other) using both word-based and prosodic features."
"Galley et al. model this task as a sequence tagging problem, and investigate whether features capturing speaker inter-actions are useful for recognizing agreements and disagreements."
"Hahn et al. investigate the use of contrast classifiers[REF_CITE]for the task, using only lexical features."
Hotspots are places in a meeting in which the par-ticipants are highly involved in the discussion.
"Al-though high involvement does not necessarily equate subjective content, in practice, we expect more sen-timents, opinions, and arguments to be expressed when participants are highly involved in the discus-sion."
"In their work on recognizing meeting hotspots, Wrede and Shriberg focus on evaluating the contri-bution of various prosodic features, ignoring lexi-cal features completely."
The results of their study helped to inform our choice of prosodic features for the experiments in this paper.
"In this paper, we investigated the use of prosodic features, word n-grams, character n-grams, and phoneme n-grams for subjectivity recognition and polarity classification of dialog acts in multiparty conversation."
"We show that character n-grams outperform prosodic features, word n-grams and phoneme n-grams in subjectiviy recognition and po-larity classification."
Combining these features sig-nificantly improves performance.
"Comparing the additive value of the four information sources avail-able, prosodic information seem to be least in-formative while character-level information indeed proves to be a very valuable source."
"For subjectiv-ity recognition, a combination of prosodic, word-level, character-level, and phoneme-level informa-tion yields the best performance."
"For polarity clas-sification, the best performance is achieved with a combination of words, characters and phonemes."
"Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue."
"In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by us-ing manually-annotated training data at the POS and lexical category levels only."
This ap-proach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain.
We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.
"Most state-of-the-art wide-coverage parsers are based on the Penn Treebank[REF_CITE], making such parsers highly tuned to newspaper text."
"A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages."
"A re-lated question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions."
Questions are particularly important since a question parser is a component in most Question Answering ( QA ) sys-tems[REF_CITE].
"In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a parser based on Combinatory Categorial Grammar ( CCG )[REF_CITE]."
"A key property of CCG is that it is lexicalized, meaning that each word in a sentence is associated with an elementary syntactic structure."
In the case of CCG this is a lexical cate-gory expressing subcategorization information.
"We exploit this property of CCG by performing manual annotation in the new domain, but only up to this level of representation, where the annotation can be carried out relatively quickly."
"Since CCG lexical cat-egories are so expressive, many of the syntactic char-acteristics of a domain are captured at this level."
The two domains we consider are the biomedical domain and questions for a QA system.
"We use the term “domain” somewhat loosely here, since ques-tions are best described as a particular set of syn-tactic constructions, rather than a set of documents about a particular topic."
"However, we consider ques-tion data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank ( PTB ) and so PTB parsers typically perform poorly on them; 2) questions form a fairly homogeneous set with re-spect to the syntactic constructions employed, and it is an interesting question how easy it is to adapt a parser to such data; and 3) QA is becoming an impor-tant example of NLP technology, and question pars-ing is an important task for QA systems."
"The CCG parser we use[REF_CITE]makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical cat-egories, which are assigned to words by a CCG su- pertagger; and three, a hierarchical level consisting of CCG derivations."
"A key idea in this paper, follow-ing a pilot study[REF_CITE], is to perform manual annotation only at the first two levels."
"Since the lexical category level consists of sequences of tags, rather than hierarchical derivations, the anno-tation can be performed relatively quickly."
"For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences, respectively, with CCG lexical categories."
"We also created a gold standard set of grammati-cal relations ( GR ) in the Stanford format (de[REF_CITE]), using 500 of the questions."
"For the biomedical domain we used the BioInfer corpus[REF_CITE], an existing gold-standard GR resource also in the Stanford format."
We evaluated the parser on both lexical category assignment and recovery of GR s.
"The results show that the domain adaptation ap-proach used here is successful in two very different domains, achieving parsing accuracy comparable to state-of-the-art accuracy for newspaper text."
"The re-sults also show, however, that the two domains have different profiles with regard to the levels of repre-sentation used by the parser."
"We find that simply re-training the POS tagger used by the parser leads to a large improvement in performance for the biomed-ical domain, and that retraining the CCG supertag-ger on the annotated biomedical data improves the performance further."
"For the question data, retrain-ing just the POS tagger also improves parser perfor-mance, but retraining the supertagger has a much greater effect."
We perform some analysis of the two datasets in order to explain the different behaviours with regard to porting the CCG parser.
The CCG parser is described in detail[REF_CITE]and so we provide only a brief de-scription.
The stages in the CCG parsing pipeline are as follows.
"First, a maximum entropy POS tagger assigns a single POS tag to each word in a sentence."
"POS tags are fairly coarse-grained grammatical la-bels indicating part-of-speech; the Penn Treebank set, used here, contains approximately 50 labels."
"Second, a maximum entropy supertagger assigns CCG lexical categories to the words in the sentence."
"Lexical categories can be thought of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word."
"Supertagging was originally developed for Lexicalized Tree Adjoining Grammar[REF_CITE], but has been particularly suc-cessful for wide-coverage CCG parsing[REF_CITE]."
"Rather than assign a single category to each word, the supertagger operates as a multi-tagger, sometimes assigning more than one category if the context is not sufficiently discriminating to suggest a single tag[REF_CITE]."
"Since the taggers have linear time complexity, the first two stages can be performed extremely quickly."
"Finally, the parsing stage combines the lexical cat-egories, using a small set of combinatory rules that are part of the grammar of CCG , and builds a packed chart representation containing all the derivations which can be built from the lexical categories."
"The Viterbi algorithm efficiently finds the highest scor-ing derivation from the packed chart, using a log-linear model to score the derivations."
"The grammar and training data for the newspaper version of the CCG parser are obtained from CCGbank[REF_CITE], a CCG version of the Penn Treebank."
The aspect of the pipeline which is most relevant to this paper is the supertagging phase.
"Figure 1 gives an example sentence from each target domain, with the CCG lexical category assigned to each word shown below the word, and the POS tag to the right."
"Note that the categories contain a significant amount of grammatical information, in particular subcatego-rization information."
"The verb acts in the biomedi-cal sentence, for example, looks for a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the resulting category a declarative sentence (S[dcl])."
The CCG supertagger is not able to assign a single category to each word with extremely high accuracy — hence the need for it to operate as a multi-tagger — but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser[REF_CITE].
"The parser has been evaluated on DepBank[REF_CITE], using the GR scheme[REF_CITE], and it scores 82.4% labelled precision and 81.2% labelled recall overall[REF_CITE]."
Section 4.4 describes how the CCG depen-dencies can be mapped into the Stanford GR scheme (de[REF_CITE]) and gives the results of evaluating the parser on biomedical and question GR resources.
The CCG parser is particularly well suited to the biomedical and question domains.
"First, use of CCG allows recovery of long-distance dependencies."
"In the sentence What does target heart rate mean?, the word What is an underlying object of the verb mean."
The parser recovers this information despite the dis-tance between the two words.
"This capability is crucial for question parsing, and also useful in the biomedical field for extraction of relationships be-tween biological entities."
"Additionally, the speed of the parser (tens of sentences per second) is useful for the large volumes of biomedical data that require processing for biomedical text mining."
"Our approach to domain adaptation is to target the coarser-grained, less syntactically complex, levels of representation used by the parser, and to train new models with manually annotated data at these levels."
The motivation for this approach is twofold.
"First, accuracy at each stage of the pipeline depends on ac-curacy at the earlier stages."
"If the POS tagger assigns incorrect tags, it is unlikely that the supertagger will be able to recover and produce the correct lexical categories, since it relies heavily on POS tags as fea-tures."
"Without the correct categories, the parser in turn will be unable to find a correct parse."
"In the sentence What year did the Vietnam War end?, the newspaper-trained POS tagger incorrectly assigns the POS tag NN (common noun) to the verb end, since verb-final sentences are atypical for the PTB ."
"As a result, the supertagger is virtually cer-tain (greater than 99% probability) that the correct CCG lexical category for end is N (noun)."
"The parser then assigns the Vietnam War end the structure of a noun phrase, and chooses an unusual subcategoriza-tion frame for did in which it takes three arguments: What, year, and the Vietnam War end."
"In the sentence How many siblings does she have?, on the other hand, the supertag-ger assigns an incorrect category to the word How despite it having the correct POS tag (WRB for wh-adverb)."
"The correct category is ((S[wq]/(S[q]/NP))/N )/(NP/N ), which takes many (category NP/N ) and siblings (category N ) as arguments."
"Instead it is tagged as S[wq]/S[q], the category for a sentential adverb (i.e. the man-ner reading of how), which prevents a correct parse."
"Our intention was that creating new training data at the lower levels of representation would improve the accuracy of the POS tagger and supertagger in the target domains, thereby improving the accuracy of later stages in the pipeline as well."
The second motivation for our approach is to re-duce annotation overhead.
Full syntactic deriva-tions are costly to produce by hand.
"POS tags, how-ever, are relatively easy to annotate; even an out-of-domain tagger will provide a good starting point, and manual correction is quick, especially in a do-main without much unfamiliar vocabulary."
"CCG lex-ical categories require more expertise, but our ex-perience shows that an out-of-domain supertagger can again provide a starting point for correction, and since the annotation is flat rather than hierarchical, we hypothesize that it is not as difficult or time-consuming as annotation of full derivations."
Our adaptation approach has been partially ex-plored in previous work which targets one or another of the different levels of representation.
"The parser improvement was due solely to the new POS tagger, without retraining the parser model."
"Since the Charniak parser does not use a lexicalized grammar with an intermediate level of representation, any further improvements would have to come from the parser model itself."
"Because a question resource annotated with GR s was not available, they did not perform a parser evaluation, and the effects of the POS tagging level were not compared to the lexi-cal category level."
"In this paper, we extend the pi-lot experiments performed[REF_CITE]in four ways."
"First, we use a larger corpus of TREC questions covering additional question types, thus extending the experiments to the question domain more broadly, as well as to the biomedical domain."
"Second, we create a gold standard GR resource en-abling a full parser evaluation on question data."
"Third, we show that the POS level is important for adaptation, reinforcing the work[REF_CITE]."
A key finding of the present paper is that the combination of retraining at the POS tag and lexical category levels provides additional improve-ments beyond those gained by retraining at a single level.
"Finally, we provide analysis comparing the adaptation methodology for question and biomedi-cal data."
"Enju is based on HPSG , a lex-icalized grammar formalism."
They obtained an im-provement in parsing accuracy in the biomedical do-main by training a new probabilistic model of lexi-cal entry assignments on a combination of newspa-per and biomedical data without changing the orig-inal newspaper-trained parsing model.
"The lexical category data[REF_CITE]was de- rived from a gold standard treebank, while the an-notation of lexical categories in this paper was per-formed without reference to gold standard syntactic derivations."
Our approach differs in retraining only at the levels of representation below parse trees.
"We have used a combination of existing resources and new, manually annotated data."
"The baseline POS tagger, supertagger, and parser are trained[REF_CITE]-21 of CCGbank."
"The baseline perfor-mance at each level of representation is on WSJ[REF_CITE]of CCGbank, which contains 1913 sentences and approximately 45,000 words."
"For the biomedical domain, we trained the POS tagger on gold-standard POS tags from GENIA[REF_CITE], a corpus of 2,000 MEDLINE abstracts containing a total of approximately 18,500 sentences and 440,000 words."
"We also annotated the first 1,000 sentences of GENIA with CCG lexical cate-gories."
"This set of 1,000 sentences, containing ap-proximately 27,000 words, was used for POS tagger evaluation and for development and evaluation of a new supertagger model."
"For parser evaluation, we used BioInfer[REF_CITE], a corpus of MEDLINE abstracts (on a different topic from those in GENIA ) containing 1,100 sentences, and with syn-tactic dependencies encoded as grammatical rela-tions in the Stanford GR format."
"We used the same evaluation set of 500 sentences as[REF_CITE], and the remaining 600 for development of the mapping to Stanford format."
"Two parsers have already been evaluated on BioInfer, which makes it a useful resource for comparative evaluation."
"For the question domain, we extended the dataset described[REF_CITE]."
"That dataset con-tained 1,171 questions beginning with the word What, from the TREC 9-12 competitions (2000- 2003), manually POS tagged and annotated with"
CCG lexical categories.
"We annotated all the addi-tional TREC question types and improved the exist-ing annotation, for a total of 1,828 sentences."
We ad-ditionally annotated a random subset of 500 of these with GR s in the Stanford format.
This subset served as our evaluation set at all levels of representation.
"It contains approximately 4,000 words, fewer than the other domains because of the significantly shorter sentence lengths of typical questions."
"The remain-ing [Footnote_1],328 sentences were used as training data."
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
A set of about a dozen sentences from the evaluation and training sets were used to develop the mapping to Stanford format for lexical categories not occur-ring in the biomedical data.
We began by training new models at the POS tag level of representation.
All datasets use the PTB tagset.
"As a baseline, we used the original[REF_CITE]- 21 model on the biomedical and question datasets."
For comparison we also evaluated[REF_CITE]us-ing the WSJ -trained model.
"For the question data, the new POS tagger was trained[REF_CITE]-21 plus ten copies of the [Footnote_1],328 training sentences."
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
"The WSJ data pro-vides additional robustness and wide grammatical coverage, and the weighting factor of ten was chosen in preliminary experiments to prevent the newspaper data from “overwhelming” the question data."
"For the biomedical data, the new POS tagger was trained on the full GENIA corpus, minus the first [Footnote_1],000 sen-tences."
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
GENIA is large enough that combination with the newspaper data was not needed.
Table 1 gives the results.
For both of the new do-mains the performance of the WSJ model decreased compared[REF_CITE]but the retrained model per-formed at least as well as the WSJ model did on 00. [Footnote_1]
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
"Improving the POS tagger performance has a posi-tive effect on the performance of the supertagger and parser, which will be discussed in Sections 4.3-4.4."
We next trained new models at the CCG lexical cat-egory level.
"The training data consisted of manu-ally annotated biomedical and question sentences; specifically, lexical categories were automatically assigned by the original parsing pipeline and then manually corrected."
"Whenever possible we used categories from the parser’s original set of 425, al-though occasionally it was necessary to use a new category for a syntactic construction not occurring in CCG bank[REF_CITE]-21. (The parser can be con-figured to recognize additional categories.)"
Question data in particular requires the use of categories that are rare or unseen in CCGbank.
"For the questions, the new supertagger model, like the POS tagger, was trained[REF_CITE]-21 plus ten copies of the [Footnote_1],328 training sentences."
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
"For the biomedical data, a ten-fold cross-validation was per-formed, training each supertagger model[REF_CITE]- 21 plus ten copies of 90% of the [Footnote_1],000 annotated sentences."
"1 Since GENIA does not use the proper noun tag, NNP, for names of genes and other biomedical entities, all figures in this paper collapse the NNP-NN distinction where relevant for biomedical data. The question data uses NNP and the distinc-tion is not collapsed."
Table 2 gives the supertagger accuracy with and without the retrained POS and supertagger models.
The figure for the retrained biomedical su-pertagger is the average of the ten-fold split.
"The results show an improvement in accuracy of lexical category assignment solely from retraining the POS tagger, and an additional improvement from retraining the supertagger."
"Supertagger accuracy for the two domains with a retrained supertagger was comparable, and in both cases was at least as high as for the original pipeline[REF_CITE]."
"The ques-tion data started from a much lower baseline figure, however."
We evaluated the parser on the 500 questions anno-tated with Stanford GR s and on the 500 evaluation sentences from the BioInfer corpus.
"We used the original newspaper pipeline, a pipeline with a re-trained POS tagger, and a pipeline with both a re-trained POS tagger and supertagger."
In order to perform these evaluations we devel-ooped a mapping from the parser’s native CCG syn-tactic dependencies to GR s in the Stanford format.
The mapping was based on the same principles as the mapping that produces GR output in the style[REF_CITE].
"These principles are dis-cussed in detail[REF_CITE]; in summary, the argument slots in the CCG dependen-cies are mapped to argument slots in Stanford GR s, a fairly complex, many-to-many mapping."
An ad-ditional post-processing script applies some manu-ally developed rules to bring the output closer to the Stanford format.
"Figure 2 gives an example of Stan-ford GR s, where the label of the relation is followed by two arguments, head and dependent."
"Table 3 gives the results of the parser evaluation on GR s. Since the parser model was not retrained, the improvements in accuracy are due solely to the new POS and supertaggers."
The results are given as an F-score over labelled GR s. 2
The F-scores given in Table 3 are only for sen-tences for which a parse was found.
"However, there were also improvements in coverage with the re-trained models."
"For the question data, parser cov- erage was 94% for the original pipeline and the pipeline with just the retrained POS tagger, and 99.6% with the retrained POS and supertaggers."
"For the biomedical data, coverage was 97.[Footnote_2]% for the original pipeline, 99.0% for the pipeline with the re-trained POS tagger, and 99.8% for the pipeline with the retrained POS and supertaggers."
2 Only GR s at the lowest level of the Stanford hierarchy were considered in the evaluation; more generic relations such as de-pendent were not considered.
"The final accuracy for both domains is in the same range as that of the original parser on newspaper data (81.8%)[REF_CITE], although the results are not directly comparable, since the newspaper resource uses a different GR scheme."
"For the BioInfer corpus, the final accuracy is also in line with results reported in the literature for other parsers[REF_CITE]. (No comparable GR results are available for questions.)"
"A score in this range is thought to be near the upper bound when evaluating a CCG parser on GR s, since some loss is inherent in the mapping to GR s[REF_CITE]."
"Although domain adaptation was successful for both of our target domains, the impact of the different levels of representation on parsing accuracy was not uniform."
"Table 3 shows that retraining the POS tag-ger accounted for a greater proportion of the im-provement on biomedical data, while retraining the supertagger accounted for a much greater proportion on questions."
"In this section we discuss some of the differences between the domains which may have contributed to their behaviour in this regard, with the intention of highlighting attributes that may be relevant for domain adaptation in general."
"Informally, we believe that the main difference between newspaper and biomedical text is vocabu-lary, and that their syntactic structures are essentially similar (with some isolated exceptions, such as more frequent use of parentheses and comma-separated lists in biomedical text)."
"Once the POS tagger had been retrained for biomedical text, accounting for unfamiliar vocabulary, the original supertagger al-ready performed well."
"The main difference between newspaper and question data, on the other hand, is syntactic."
Retraining the POS tagger for questions therefore had less effect; even with the correct POS tags the supertagger was unable to assign the correct lexical categories.
"Since lexical categories encode syntactic information, the domain with the more di-vergent syntax is likely to benefit most from new training data at the lexical category level."
Table 1 showed that the accuracy of the newspaper-trained POS tagger was in the same range for both biomedical and question data.
"However, the distri-bution of errors was different."
"Table 4 shows the tags with the most frequent errors, accounting for about 75% of all POS tag errors in each domain, and the tags that they were most frequently confused with."
"For the question data, the most frequent error was tagging a wh-determiner (WDT) as a wh-pronoun (WP)."
"A determiner combines with a noun to form a noun phrase, as in the sentence What Liverpool club spawned the Beatles?."
"A pronoun, on the other hand, is a noun phrase in its own right, as in What are the colors of the German flag?."
This tagger er-ror arises from the fact that the word What occurs only once[REF_CITE]-21 with a WDT tag.
"The sec-ond most common error was on bare verbs (VB), be-cause the newspaper model gives a low probability of bare verbs occurring in sentence-final position, or not directly following an auxiliary."
"For the biomedical data, the most frequent errors by far were confusions of noun (NN) and adjective (JJ)."
"This is most likely due to the prevalence of long noun phrases in the biomedical data, such as major histocompatibility complex class II molecules."
"Al-though the words preceding the head noun are rec-ognized as nominal modifiers, the classification into noun and adjective is difficult, especially when the word is previously unseen."
"There were also prob-lems distinguishing verbal past participles (VBN) from adjectives (JJ) and identifying foreign words (FW), for example the phrase in vitro."
"The fact that the newspaper-trained POS tagger performed comparably in the two target domains (Table 1) is surprising, since their lexical profiles are quite different."
"However, the unknown word rate compared[REF_CITE]-21 is much higher for the biomedical data than for the question data, as seen in Table 5. (The unknown word rate for the question data is still higher than that[REF_CITE]which may be due to the high proportion of proper nouns in the question data.)"
"Some POS tagging errors can be attributed, not to an unknown word, but to the use of a known word with an unfamiliar tag (as in the WDT exam-ple above)."
"However, it is not the case that the ques-tion data contains many known words with unknown tags, since the rate of unknown word-tag pairs is also much higher for biomedical than for question data, as seen in the rightmost column of Table 5."
We do know that the newspaper-trained POS tag-ger performs better on unknown words for biomedi-cal (84.7%) than for question data (80.4%).
"We hy-pothesize that the syntactic context of the biomed-ical data, being more similar to newspaper data, provides more information for the POS tagger in biomedical than in question data."
Syntactic differ-ences are discussed in the next section.
"To quantify the syntactic distance between domains, we propose using the unknown POS n-gram rate compared[REF_CITE]-21."
"In the absence of parse trees, POS n-grams can serve as a rough proxy for the syntactic characteristics of a domain, reflect-ing local word order configurations."
"POS n-grams have been used in document modeling for text cate-gorizati[REF_CITE], but we believe our proposed use of the unknown POS n-gram rate is novel."
The leftmost column of Table 6 gives the un-known POS trigram and 5-gram rates compared[REF_CITE]-21.
The rates for the biomedical data are quite similar to those[REF_CITE].
"The question data, however, shows higher rates of un-known POS n-grams."
"For both biomedical and question data, adding in-domain data to the training set makes its syntactic profile more like that of the evaluation set."
"The right-most column of Table 6 shows the unknown POS n-gram rates compared to the datasets used for training the new supertagger models, consisting[REF_CITE]- 21 plus annotated question or biomedical data. (For the biomedical data, the figures are averages of the same ten-fold split used for evaluation)."
It can be seen that adding in-domain data reduces the rate of unknown POS n-grams to about the same level ob-served for newspaper text.
The unknown POS n-gram rate requires POS tagged data for a new domain and thus cannot be used with unlabelled data.
"However, since POS tag-ging is relatively inexpensive, it might be possible to use this rate as one measure of syntactic distance be-tween a training corpus and a target domain, prior to undertaking parser domain adaptation."
"The measure does not capture all aspects of syntactic distance, however."
"As pointed out by an anonymous reviewer, if the syntactic tree structures are similar across do-mains but lexical distributions are different – e.g. a large number of words with unfamiliar categories in the new domain – this measure will not be sensitive to the difference."
Another useful measure for comparing domain adaptation in the biomedical and question domains is frequent POS n-grams.
Table 7 shows how many of the 20 most frequent POS n-grams in each dataset overlap with the 20 most frequent POS n-grams[REF_CITE]-21.
"It can be seen that the overlap is the highest[REF_CITE]but much lower for the ques-tion data than for the biomedical data, again demon-strating that the question data makes frequent use of syntactic constructions which are rare in the PTB ."
"Table 8 shows the four most frequent POS tri-grams[REF_CITE]-21, [Footnote_3] and the four most frequent POS trigrams in the biomedical and ques-tion data that are not among the 20 most frequent[REF_CITE]-21."
3 Collapsing the NNP-NN distinction yields a slightly differ-ent set.
"The frequent question trigrams in-clude two sentence-initial question words as well as the pattern — WP VBZ, occurring in sentences be-ginning with e.g. What is or Who is."
"Though not among the top four, the pattern VB . —, represent-ing a sentence-final bare verb, is also frequent."
"The most frequent biomedical POS trigrams are not dra-matically different from the newspaper trigrams, but do appear to reflect the prevalence of NPs and PPs in the data."
One final measure of syntactic distance is the frequency with which CCG lexical categories that are rare or unseen in CCG bank are used in a do-main.
"It is typical to use a few such categories, even for in-domain data, for unusual syntactic con-structions, but each one is usually used only a hand-ful of times."
The question data is unique in the frequency with which previously rare or unseen categories are required.
"For example, the unseen category (S[wq]/S[q])/N, representing the word What in a question such as What day did[REF_CITE]come out? is used 11 times in the evaluation set; the rare category (S[wq]/(S[dcl]\NP))/N, used in subject questions like Which river runs through Dublin?, is used 61 times; and the rare cat-egory (S[q]/(S[pss]\NP))/NP, representing pas-sive verbs in sentences like What is Jane Goodall known for?, is used 59 times."
"We have targeted lower levels of representation in order to adapt a lexicalized-grammar parser to two new domains, biomedical text and questions."
"Al-though each of the lower levels has been targeted in-dependently in previous work, this is the first study that examines both levels together to determine how they affect parsing accuracy."
"We achieved an accu-racy on grammatical relations in the same range as that of the original parser for newspaper text, with-out requiring costly annotation of full parse trees."
Both biomedical and question data are domains in which there is an immediate need for accurate pars-ing.
"The question dataset is in some ways an ex-treme example for domain adaptation, since the sen-tences are syntactically uniform; on the other hand, it is of interest as a set of constructions where the parser initially performed poorly, and is a realistic parsing challenge in the context of QA systems."
"Interestingly, although an increase in accuracy at each stage of the pipeline did yield an increase at the following stage, these increases were not uni-form across the two domains."
"The new POS tagger model was responsible for most of the improvement in parsing for the biomedical domain, while the new supertagger model was necessary to see a large im-provement in the question domain."
We attribute this to the fact that question syntax is significantly differ-ent from newspaper syntax.
We expect these consid-erations to apply to any lexicalized-grammar parser.
"Of course, it would be useful to have a way of predicting which level of annotation would be most effective for adapting to a new domain before the an-notation begins."
"The utility of measures such as un-known word rate (which can be performed with un-labelled data) and unknown POS n-gram rate (which can be performed with only POS tags) is not yet suffi-ciently clear to rely on them as predictive measures, but it seems a fruitful avenue for future work to in-vestigate the importance of such measures for parser domain adaptation."
"We would like to thank Marie-Catherine de Marn-effe for advice on the use of the Stanford GR for-mat, Sampo Pyysalo for sharing information about the BioInfer corpus, and Mark Steedman for advice on encoding question data in CCG ."
We would also like to thank three anonymous reviewers for their suggestions.
This work was supported by EPSRC grant EP/[REF_CITE]/1: Accurate and Efficient Parsing of Biomedical Text.
"Although Machine Translation (MT) is a very active research field which is receiving an in-creasing amount of attention from the research community, the results that current MT sys-tems are capable of producing are still quite far away from perfection."
"Because of this, and in order to build systems that yield correct translations, human knowledge must be inte-grated into the translation process, which will be carried out in our case in an Interactive-Predictive (IP) framework."
"In this paper, we show that considering Mouse Actions as a sig-nificant information source for the underly-ing system improves the productivity of the human translator involved."
"In addition, we also show that the initial translations that the MT system provides can be quickly improved by an expert by only performing additional Mouse Actions."
"In this work, we will be using word graphs as an efficient interface between a phrase-based MT system and the IP engine."
Information technology advances in modern society have led to the need of more efficient methods of translation.
It is important to remark that current MT systems are not able to produce ready-to-use texts[REF_CITE].
"Indeed, MT systems are usually limited to specific semantic domains and the translations provided re- quire human post-editing in order to achieve a cor-rect high-quality translation."
"A way of taking advantage of MT systems is to combine them with the knowledge of a human trans-lator, constituting the so-called Computer-Assisted Translation (CAT) paradigm."
CAT offers different approaches in order to benefit from the synergy be-tween humans and MT systems.
An important contribution to interactive CAT technology was carried out around the TransType (TT) project[REF_CITE].
"This project entailed an interesting focus shift in which interac-tion directly aimed at the production of the target text, rather than at the disambiguation of the source text, as in former interactive systems."
The idea proposed was to embed data driven MT techniques within the interactive translation environment.
"Following these TT ideas, (Barrachina and oth-ers, 2008) propose the usage of fully-fledged statis-tical MT (SMT) systems to produce full target sen-tence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator."
"Each partial correct text segment is then used by the SMT system as additional infor-mation to achieve further, hopefully improved sug-gestions."
"In this paper, we also focus on the inter-active and predictive, statistical MT (IMT) approach to CAT."
"The IMT paradigm fits well within the In-teractive Pattern Recognition framework introduced in (Vidal and others, 2007)."
Figure 1 illustrates a typical IMT session.
"Ini-tially, the user is given an input sentence x to be translated."
The reference y provided is the trans-lation that the user would like to achieve at the end of the IMT session.
"At iteration 0, the user does not supply any correct text prefix to the system, for this reason p is shown as empty."
"Therefore, the IMT sys-tem has to provide an initial complete translation s h , as it were a conventional SMT system."
"At the next iteration, the user validates a prefix p as correct by positioning the cursor in a certain position of s h ."
"In this case, after the words “To print a”."
"Implicitly, he is also marking the rest of the sentence, the suffix s l , as potentially incorrect."
"Next, he introduces a new word k, which is assumed to be different from the first word s l 1 in the suffix s l which was not validated, k =6 s l 1 ."
"This being done, the system suggests a new suffix hypothesis ŝ h , subject to ŝ h 1 = k."
"Again, the user validates a new prefix, introduces a new word and so forth."
The process continues until the whole sentence is correct that is validated introducing the special word “#”.
"As the reader could devise from the IMT session described above, IMT aims at reducing the effort and increasing the productivity of translators, while preserving high-quality translation."
"For instance, in Figure 1, only three interactions were necessary in order to achieve the reference translation."
"In this paper, we will show how Mouse Actions performed by the human expert can be taken advan-tage of in order to further reduce this effort."
In this section we will briefly describe the statistical framework of IMT.
"IMT can be seen as an evolution of the SMT framework, which has proved to be an efficient framework for building state-of-the-art MT systems with little human effort, whenever adequate corpora are available[REF_CITE]."
The fundamental equation of the statistical approach to MT is ŷ = argmax P r(y | x) (1) y = argmax P r(x | y) P r(y) (2) y where P r(x | y) is the translation model modelling the correlation between source and target sentence and Pr(y) is the language model representing the well-formedness of the candidate translation y.
"In practise, the direct modelling of the posterior probability Pr(y|x) has been widely adopted."
"To this purpose, different authors[REF_CITE]propose the use of the so-called log-linear models, where the decision rule is given by the expression ŷ = argmax X λ"
"M m h m (x, y) (3) y m=1 where h m (x, y) is a score function representing an important feature for the translation of x into y, M is the number of models (or features) and λ m are the weights of the log-linear combination."
One of the most popular instantiations of log-linear models is that including phrase-based (PB) models[REF_CITE].
Phrase-based models allow to capture contextual in-formation to learn translations for whole phrases in-stead of single words.
"The basic idea of phrase-based translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the trans-lated target phrases in order to compose the tar-get sentence."
Phrase-based models were employed throughout this work.
"In log-linear models, the maximisation problem stated in Eq. 3 is solved by means of the beam search algorithm 1 which was initially introduced[REF_CITE]for its application in the field of speech recognition."
"The beam search algorithm attempts to generate partial solutions, called hypotheses, until a complete sentence is found; these hypotheses are stored in a stack and ordered by their score."
Such a score is given by the log-linear combination of fea-ture functions.
"However, Eq. 1 needs to be modified according to the IMT scenario in order to take into account part of the target sentence that is already translated, that is p and k ŝ h = argmax P r(s h |x, p, k) (4) s h where the maximisation problem is defined over the suffix s h ."
"This allows us to rewrite Eq. 4, by decom-posing the right side appropriately and eliminating constant terms, achieving the equivalent criterion ŝ h = argmax"
"P r(p, k, s h |x). (5) s h"
An example of the intuition behind these variables can be seen in Figure 1.
"Note that, since (p k s h ) = y, Eq. 5 is very simi-lar to Eq. 1."
The main difference is that the argmax search is now performed over the set of suffixes s h that complete (pk) instead of complete sentences (y in Eq. [Footnote_1]).
1 Also known as stack decoding algorithm.
"This implies that we can use the same models if the search procedures are adequately mod-ified (Barrachina and others, 2008)."
The phrase-based approach presented above can be easily adapted for its use in an IMT scenario.
The most important modification is to rely on a word graph that represents possible translations of the given source sentence.
"The use of word graphs in IMT has been studied in (Barrachina and oth-ers, 2008) in combination with two different trans-lation techniques, namely, the Alignment Templates technique[REF_CITE], and the Stochastic Finite State Transducers tech-nique[REF_CITE]."
"A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see[REF_CITE]for more details)."
"In[REF_CITE], the use of a word graph is proposed as interface between an alignment-template SMT model and the IMT en-gine."
"Analogously, in this work we will be using a word graph built during the search procedure per-formed on a PB SMT model."
"During the search process performed by the above mentioned beam search algorithm, it is possible to create a segment graph."
"In such a graph, each node represents a state of the SMT model, and each edge a weighted transition between states labelled with a sequence of target words."
"Whenever a hypothesis is extended, we add a new edge connecting the state of that hypothesis with the state of the extended hy-pothesis."
The new edge is labelled with the sequence of target words that has been incorporated to the ex-tended hypothesis and is weighted appropriately by means of the score given by the SMT model.
"Once the segment graph is generated, it can be easily converted into a word graph by the introduc-tion of artificial states for the words that compose the target phrases associated to the edges."
"During the process of IMT for a given source sen-tence, the system makes use of the word graph gen-erated for that sentence in order to complete the pre-fixes accepted by the human translator."
"Specifically, the system finds the best path in the word graph as-sociated with a given prefix so that it is able to com-plete the target sentence, being capable of providing several completion suggestions for each prefix."
"A common problem in IMT arises when the user sets a prefix which cannot be found in the word graph, since in such a situation the system is un-able to find a path through the word graph and pro-vide an appropriate suffix."
The common procedure to face this problem is to perform a tolerant search in the word graph.
This tolerant search uses the well known concept of Levenshtein distance in order to obtain the most similar string for the given prefix (see[REF_CITE]for more details).
"Although the IMT paradigm has proved to offer in-teresting benefits to potential users, one aspect that has not been reconsidered as of yet is the user– machine interface."
"Hence, in traditional IMT the system only received feedback whenever the user typed in a new word."
"In this work, we show how to enrich user–machine interaction by introducing Mouse Actions (MA) as an additional information source for the system."
"By doing so, we will consider two types of MAs, i.e. non-explicit (or positioning) MAs and interaction-explicit MAs."
"Before typing in a new word in order to correct a hy-pothesis, the user needs to position the cursor in the place where he wants to type such a word."
"In this work, we will assume that this is done by perform-ing a MA, although the same idea presented can also be applied when this is done by some other means."
"It is important to point out that, by doing so, the user is already providing some very useful information to the system: he is validating a prefix up to the posi-tion where he positioned the cursor, and, in addition, he is signalling that whatever word is located after the cursor is to be considered incorrect."
"Hence, the system can already capture this fact and provide a new translation hypothesis, in which the prefix re-mains unchanged and the suffix is replaced by a new one in which the first word is different to the first word of the previous suffix."
"We are aware that this does not mean that the new suffix will be correct, but given that we know that the first word in the previ-ous suffix was incorrect, the worst thing which can happen is that the the first word of the new suffix is incorrect as well."
"However, if the new suffix hap-pens to be correct, the user will happily find that he does not need to correct that word any more."
An example of such behaviour can be seen in Figure 2.
"In this example, the SMT system first provides a translation which the user does not like."
"Hence, he positions the cursor before word “postscript”, with the purpose of typing in “lists”."
"By doing so, he is validating the prefix “To print a”, and signalling that he wants “postscript” to be replaced."
"Before typing in anything, the system re-alises that he is going to change the word located after the cursor, and replaces the suffix by another one, which is the one the user had in mind in the first place."
"Finally, the user only has to accept the final translation."
"We are naming this kind of MA non-explicit be-cause it does not require any additional action from the user: he has already performed a MA in order to position the cursor at the place he wants, and we are taking advantage of this fact to suggest a new suffix hypothesis."
"Since the user needs to position the cursor before typing in a new word, it is important to point out that any improvement achieved by introducing non-explicit MAs does not require any further effort from the user, and hence is considered to have no cost."
"Hence, we are now considering two different situ-ations: the first one, the traditional IMT framework, in which the system needs to find a suffix according to Eq. 5, and a new one, in which the system needs to find a suffix in which the first word does not need to be a given k, but needs to be different to a given s l1 ."
"This constraint can be expressed by the follow-ing equation: ŝ h = argmax Pr(p, s h |x, s l ) (6) s h :s h1 6=s l1 where s l is the suffix generated in the previous iter-ation, already discarded by the user, and s l 1 is the first word in s l . k is omitted in this formula because the user did not type any word at all."
"If the system is efficient and provides suggestions which are good enough, one could easily picture a situation in which the expert would ask the system to replace a given suffix, without typing in any word."
"We will be modelling this as another kind of MA, interaction-explicit MA, since the user needs to in-dicate explicitly that he wants a given suffix to be replaced, in contrast to the non-explicit positioning MA."
"However, if the underlying MT engine provid-ing the suffixes is powerful enough, the user would quickly realise that performing a MA is less costly that introducing a whole new word, and would take advantage of this fact by systematically clicking be-fore introducing any new word."
"In this case, as well, we assume that the user clicks before an in-correct word, hence demanding a new suffix whose first word is different, but by doing so he is adopting a more participative and interactive attitude, which was not demanded in the case of non-explicit posi-tioning MAs."
An example of such an explicit MA correcting an error can be seen in Figure 3
"In this case, however, there is a cost associated to this kind of MAs, since the user does need to per-form additional actions, which may or may not be beneficial."
"It is very possible that, even after asking for several new hypothesis, the user will even though need to introduce the word he had in mind, hence wasting the additional MAs he had performed."
"If we allow the user to perform n MAs before in-troducing a word, this problem can be formalised in an analogous way as in the case of non-explicit MAs as follows: ŝ h = argmax P r(p, s h |x, s 1l , s l2 , . . . , s nl ) (7) s h :s h1 =6 s il1 ∀i∈{1..n} where s il1 is the first word of the i-th suffix dis-carded and s 1l , s 2l , . .. , s nl is the set of all n suffixes discarded."
"Note that this kind of MA could also be imple-mented with some other kind of interface, e.g. by typing some special key such as F1 or Tab."
"How-ever, the experimental results would not differ, and in our user interface we found it more intuitive to implement it as a MA."
Automatic evaluation of results is a difficult problem in MT.
"In fact, it has evolved to a research field with own identity."
"This is due to the fact that, given an input sentence, a large amount of correct and differ-ent output sentences may exist."
"Hence, there is no sentence which can be considered ground truth, as is the case in speech or text recognition."
"By extension, this problem is also applicable to IMT."
"In this paper, we will be reporting our results as measured by Word Stroke Ratio (WSR) (Barrachina and others, 2008), which is computed as the quotient between the number of word-strokes a user would need to perform in order to achieve the translation he has in mind and the total number of words in the sentence."
"In this context, a word-stroke is in-terpreted as a single action, in which the user types a complete word, and is assumed to have constant cost."
"Moreover, each word-stroke also takes into ac-count the cost incurred by the user when reading the new suffix provided by the system."
"In the present work, we decided to use WSR in-stead of Key Stroke Ratio (KSR), which is used in other works on IMT such[REF_CITE]."
"The reason for this is that KSR is clearly an optimistic measure, since in such a scenario the user is often overwhelmed by receiving a great amount of trans-lation options, as much as one per key stroke, and it is not taken into account the time the user would need to read all those hypotheses."
"In addition, and because we are also introducing MAs as a new action, we will also present results in terms of Mouse Action Ratio (MAR), which is the quotient between the amount of explicit MAs per- formed and the number of words of the final trans-lation."
"Hence, the purpose is to elicit the number of times the user needed to request a new translation (i.e. performed a MA), on a per word basis."
"Lastly, we will also present results in terms of uMAR (useful MAR), which indicates the amount of MAs which were useful, i.e. the MAs that actu-ally produced a change in the first word of the suffix and such word was accepted."
"Formally, uMAR is defined as follows:"
"MAC − n · W SC uMAR = (8) MAC where MAC stands for “Mouse Action Count”, WSC for “Word Stroke Count” and n is the max-imum amount of MAs allowed before the user types in a word."
Note that MAC −n·W SC is the amount of MAs that were useful since WSC is the amount of word-strokes the user performed even though he had already performed n MAs.
"Since we will only use single-reference WSR and MAR, the results presented here are clearly pes-simistic."
"In fact, it is relatively common to have the underlying SMT system provide a perfectly correct translation, which is ”corrected” by the IMT proce-dure into another equivalent translation, increasing WSR and MAR significantly by doing so."
"Our experiments were carried out on the Eu-roparl[REF_CITE]corpus, which is a corpus widely used in SMT and that has been used in sev-eral MT evaluation campaigns."
"Moreover, we per-formed our experiments on the partition established for the Workshop on Statistical Machine Translation of the[REF_CITE]."
The Europarl corpus[REF_CITE]is built from the pro-ceedings of the European Parliament.
"Here, we will focus on the German–English, Spanish–English and French–English tasks, since these were the language pairs selected for the cited workshop."
"The corpus is divided into three separate sets: one for training, one for development, and one for test."
The characteris-tics of the corpus can be seen in Table 1.
"As a first step, we built a SMT system for each of the language pairs cited in the previous subsection."
"This was done by means of the Moses toolkit (Koehn and others, 2007), which is a complete system for building Phrase-Based SMT models."
"This toolkit in-volves the estimation from the training set of four different translation models, which are in turn com- bined in a log-linear fashion by adjusting a weight for each of them by means of the MERT[REF_CITE]procedure, optimising the BLEU[REF_CITE]score obtained on the development partition."
"This being done, word graphs were generated for the IMT system."
"For this purpose, we used a multi-stack phrase-based decoder which will be dis-tributed in the near future together with the Thot toolkit[REF_CITE]."
We discarded the use of the Moses decoder because preliminary experiments performed with it revealed that the de-coder[REF_CITE]performs clearly better when used to generate word graphs for use in IMT.
"In addition, we performed an ex-perimental comparison in regular SMT with the Eu-roparl corpus, and found that the performance dif-ference was negligible."
"The decoder was set to only consider monotonic translation, since in real IMT scenarios considering non-monotonic transla-tion leads to excessive waiting time for the user."
"Finally, the word graphs obtained were used within the IMT procedure to produce the reference translation contained in the test set, measuring WSR and MAR."
The results of such a setup can be seen in Table 2.
"As a baseline system, we report the tradi-tional IMT framework, in which no MA is taken into account."
"Then, we introduced non-explicit MAs, ob-taining an average improvement in WSR of about 3.2% (4.9% relative)."
The table also shows the confidence intervals at a confidence level of 95%.
These intervals were computed following the boot-strap technique described[REF_CITE].
"Since the confidence intervals do not overlap, it can be stated that the improvements obtained are statisti-cally significant."
"Once the non-explicit MAs were considered and introduced into the system, we analysed the effect of performing up to a maximum of 5 explicit MAs."
"Here, we modelled the user in such a way that, in case a given word is considered incorrect, he will always ask for another translation hypothesis until he has asked for as many different suffixes as MAs considered."
The results of this setup can be seen in Figure 4.
This yielded a further average improve-ment in WSR of about 16% (25% relative improve-ment) when considering a maximum of 5 explicit MAs.
"However, relative improvement in WSR and uMAR increase drop significantly when increasing the maximum allowed amount of explicit MAs from 1 to 5."
"For this reason, it is difficult to imagine that a user would perform more than two or three MAs before actually typing in a new word."
"Nevertheless, just by asking twice for a new suffix before typing in the word he has in mind, the user might be saving about 15% of word-strokes."
"Although the results in Figure 4 are only for the translation direction “foreign”→English, the experiments in the opposite direction (i.e. English→“foreign”) were also performed."
"How-ever, the results were very similar to the ones dis-played here."
"Because of this, and for clarity pur-poses, we decided to omit them and only display the direction “foreign”→English."
"In this paper, we have considered new input sources for IMT."
"By considering Mouse Actions, we have shown that a significant benefit can be obtained, in terms of word-stroke reduction, both when consid-ering only non-explicit MAs and when considering MAs as a way of offering the user several suffix hy-potheses."
"In addition, we have applied these ideas on a state-of-the-art SMT baseline, such as phrase-based models."
"To achieve this, we have first ob-tained a word graph for each sentence which is to be translated."
Experiments were carried out on a refer-ence corpus in SMT.
"Note that there are other systems (Esteban and others, 2004) that, for a given prefix, provide n-best lists of suffixes."
"However, the functionality of our system is slightly (but fundamentally) different, since the suggestions are demanded to be different in their first word, which implies that the n-best list is scanned deeper, going directly to those hypothe-ses that may be of interest to the user."
"In addition, this can be done “on demand”, which implies that the system’s response is faster and that the user is not confronted with a large list of hypotheses, which often results overwhelming."
"As future work, we are planning on performing a human evaluation that assesses the appropriateness of the improvements described."
"In this paper, we first introduce a new archi-tecture for parsing, bidirectional incremental parsing."
"We propose a novel algorithm for in-cremental construction, which can be applied to many structure learning problems in NLP."
"We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set."
"The phrase “Bidirectional Incremental” may appear self-contradictory at first sight, since incremental parsing usually means left-to-right parsing in the context of conventional parsing."
"In this paper, we will extend the meaning of incremental parsing."
The idea of bidirectional parsing is related to the bidirectional sequential classification method de-scribed[REF_CITE].
"In that paper, a tagger assigns labels to words of highest confidence first, and then these labels in turn serve as the context of later labelling operations."
The bidirectional tagger obtained the best results in literature on POS tagging on the standard PTB dataset.
"We extend this method from labelling to structure learning, The search space of structure learning is much larger, so that it is appropriate to exploit con-fidence scores in search."
"In this paper, we are interested in LTAG depen-dency parsing because TAG parsing is a well known problem of high computational complexity in reg-ular parsing."
"In order to get a focus for the learn-ing algorithm, we work on a variant of LTAG based parsing in which we learn the word dependency re-lations encoded in LTAG derivations instead of the full-fledged trees."
"Two types of parsing strategies are popular in nat-ural language parsing, which are chart parsing and incremental parsing."
Suppose the input sentence is w 1 w 2 ...w n .
"Let cell [i, j] represent w i w i+1 ...w j , a substring of the sen-tence."
"As far as CFG parsing is concerned, a chart parser computes the possible structures over all pos-sible cells [i,j], where 1 ≤ i ≤ j ≤ n."
"The order of computing on these n(n + 1)/2 cells is based on some partial order , such that [p 1 , p 2 ] [q 1 , q 2 ] if q 1 ≤ p 1 ≤ p 2 ≤ q 2 ."
"In order to employ dynamic programming, one can only use a fragment of a hy-pothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence as-sumption."
It is well known that richer context rep-resentation gives rise to better parsing performance[REF_CITE].
"However, the need for tractability does not allow much internal information to be used to represent a hypothesis."
"The designs of hypothe-ses[REF_CITE]show a del-icate balance between expressiveness and tractabil-ity, which play an important role in natural language parsing."
Some recent work on incremental parsing[REF_CITE]showed another way to handle this problem.
"In these incremental parsers, tree structures are used to represent the left context."
"In this way, one can access the whole tree to collect rich context in-formation at the expense of being limited to beam search, which only maintains k-best results at each step."
"Compared to chart parsing, incremental pars-ing searches for the analyses for only 2n − 1 cells, [1, 1], [2, 2], [1, 2], .., [i, i], [1, i], .., [1, n], incremen-tally, while complex structures are used for the anal-yses for each cell, which satisfy conditional inde-pendence under a much weaker assumption."
"In this paper, we call this particular approach left-to-right incremental parsing, since one can also search from right to left incrementally in a similar way."
A major problem of the left-to-right approach is that one can only utilize the structural information on the left side but not the right side.
"A natural way to handle this problem is to employ bidirectional search, which means we can dynami-cally search the space in two directions."
So we ex-pand the idea of incremental parsing by introducing greedy search.
"Specifically, we look for the hypothe-ses over the cell [1,n] by building analyses over 2n − 1 cells [a i,1 , a i,2 ], i = 1, .., 2n − 1 step by step, where [a 2n−1,1 , a 2n−1,2 ] = [1, n]."
"Furthermore, for any [a i,1 , a i,2 ] • a i,1 = a i,2 , or • ∃j, k, such that [a i,1 , a i,2 ] = [a j,1 , a k,2 ], where j &lt; i, k &lt; i and a j,2 + 1 = a k,1 ."
"It is easy to show that the set {[a i,1 ,a i,2 ] | 1 ≤ i ≤ 2n − 1} forms a tree relation, which means that each cell except the last one will be used to build an-other cell just once."
"In this framework, we can begin with several starting points in a sentence and search in any direction."
So left-to-right parsing is only a special case of incremental parsing defined in this way.
"We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as[REF_CITE]."
"Furthermore, we can utilize the rich context on both sides of the partial results."
"Similar to bidirectional labelling[REF_CITE], there are two learning tasking in this model."
"First, we need to learn which cell we should choose."
"At each step, we can select only one path."
"Sec-ondly, we need to learn which operation we should take for a given cell."
"We maintain k-best candidates for each cell instead of only one, which differenti-ates this model from normal greedy search."
So our model is more robust.
"Furthermore, we need to find an effective way to iterate between these two tasks."
"Instead of giving an algorithm specially designed for parsing, we generalize the problem for graphs."
A sentence can be viewed as a graph in which words are viewed as vertices and neighboring words are connected with an arc.
"In Sections 2 and 3, we will propose decoding and training algorithms re-spectively for graph-based incremental construction, which can be applied to many structure learning problems in NLP."
We will apply this algorithm to dependency pars-ing of Lexicalized Tree Adjoining Grammar[REF_CITE].
"Specifically, we will train and evaluate an LTAG dependency parser over the LTAG treebank described[REF_CITE]."
We report the experimental results[REF_CITE]of the LTAG treebank.
"The accuracy on LTAG dependency is 90.5%, which is 1.2 points over 89.3%, the previ-ous best result[REF_CITE]on the same data set."
"It should be noted that PTB-based bracketed la-belling is not an appropriate evaluation metric here, since the experiments are on an LTAG treebank."
The derived trees in the LTAG treebank are different from the CFG trees in PTB.
"Hence, we do not use metrics such as labeled precision and labeled recall for evaluation."
Now we define the problem formally.
We will use dependency parsing as an example to illustrate the idea.
"We are given a connected graph G(V, E) whose hidden structure is U, where V = {v i }, E ⊆ V × V is a symmetric relation, and U = {u k } is composed of a set of elements that vary with ap-plications."
"As far as dependency parsing is con-cerned, the input graph is simply a chain of ver-tices, where E(v i−1 , v i ), and its hidden structure is {u k = (v s k , v e k , b k )}, where vertex v e k depends on vertex v s k with label b k ."
A graph-based incremental construction algo-rithm looks for the hidden structure in a bottom-up style.
"Let x i and x j be two sets of connected vertexes in V , where x i ∩ x j = φ and they are directly con-nected via an edge in E. Let y xi be a hypothesized hidden structure of x i , and y xj a hypothesized hid-den structure of x j ."
Suppose we choose to combine y xi and y xj with an operation r to build a hypothesized hidden struc-ture for x k = x i ∪ x j .
"We say the process of con-struction is incremental if the output of the opera-tion, y xk = r(x i ,x j ,y xi ,y xj ) ⊇ y xi ∪ y xj for all the possible x i , x j , y xi , y xj and operation r. As far as dependency parsing is concerned, incrementality means that we cannot remove any links coming from the substructures."
"Once y xk is built, we can no longer use y xi or y xj as a building block."
It is easy to see that left to right incremental construction is a special case of our approach.
So the question is how we decide the order of construction as well as the type of operation r.
"For example, in the very first step of dependency parsing, we need to decide which two words are to be combined as well as the dependency label to be used."
"This problem is solved statistically, based on the features defined on the substructures involved in the operation and their context."
"Suppose we are given the weights of these features, we will show in the next section how these parameters guide us to build a set of hypothesized hidden structures with beam search."
"In Section 3, we will present a Perceptron like algorithm[REF_CITE]to obtain the parameters."
Now we introduce the data structure to be used in our algorithms.
"A fragment is a connected sub-graph of G(V, E)."
"Each fragment x is associated with a set of hypothe-sized hidden structures, or fragment hypotheses for short: Y x = {y 1x , ..., y xk }."
Each y x is a possible frag-ment hypothesis of x.
"It is easy to see that an operation to combine two fragments may depend on the fragments in the con-text, i.e. fragments directly connected to one of the operands."
So we introduce the dependency relation over fragments.
"Suppose there is a dependency re-lation D ⊆ F × F, where F ⊆ 2 V is the set of all fragments in graph G. D(x i , x j ) means that any op-eration on a fragment hypothesis of x i depends on the features in the fragment hypothesis of x j , and vice versa."
We are especially interested in the following two dependency relations. • level-0 dependency:
"D 0 (x i ,x j ) ⇐⇒ i = j. • level-1 dependency: D 1 (x i ,x j ) ⇐⇒ x i and x j are directly connected in G."
Level-0 dependency means that the features of a hypothesis for a vertex x i do not depend on the hypotheses for other vertices.
Level-1 dependency means that the features depend on the hypotheses of nearby vertices only.
The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described[REF_CITE].
Level-1 depen-dency requires more data structures to maintain the hypotheses with dependency relations among them.
"However, we do not get into the details of level-1 formalism in this papers for two reasons."
One is the limit of page space and depth of a conference pa-per.
"On the other hand, our experiments show that the parsing performance with level-1 dependency is close to what level-0 dependency could provides."
Interested readers could refer[REF_CITE]for detailed description of the learning algorithms for level-1 dependency.
"Algorithm 1 shows the procedure of building hy-potheses incrementally on a given graph G(V,E)."
Parameter k is used to set the beam width of search.
Weight vector w is used to compute score of an op-eration.
"We have two sets, H and Q, to maintain hypothe-ses."
"Hypotheses in H are selected in beam search, and hypotheses in Q are candidate hypotheses for the next step of search in various directions."
"We first initiate the hypotheses for each vertex, and put them into set H. For example, in depen-dency parsing, the initial value is a set of possible POS tags for each single word."
Then we use a queue Q to collect all the possible hypotheses over the ini-tial hypotheses H.
"Whenever Q is not empty, we search for the hy-pothesis with the highest score according to a given weight vector w. Suppose we find (x, y)."
"Algorithm 1 Incremental Construction Require: graph G(V, E); Require: beam width k; Require: weight vector w; 1: H ← init H (); 2: Q ← init Q (H); 3: repeat 4: (x ′ , y ′ ) ← arg max (x,y)∈Q score(y); 5: H ← update H (H, x ′ ); 6: Q ← update Q (Q, H, x ′ ); 7: until (Q = φ) top k-best hypotheses for segment x from Q and use them to update H. Then we remove from Q all the hypotheses for segments that have overlap with seg-ment x. In the end, we build new candidate hypothe-ses with the updated selected hypothesis set H, and add them to Q."
We use an example of dependency parsing to illus-trate the incremental construction algorithm first.
Suppose the input sentence is the student will take four courses.
We are also given the candidate POS tags for each word.
So the graph is just a linear struc-ture in this case.
We use level-0 dependency and set beam width to two.
We use boxes to represent fragments.
The depen-dency links are from the parent to the child.
Figure 1 shows the result after initialization.
"Fig-ure 2 shows the result after the first step, combining the fragments of four and courses."
"Figure 3 shows the result after the second step, combining the and student, and figure 4 shows the result after the third step, combining take and four courses."
"Due to lim-ited space, we skip the rest operations."
"Now we will explain the functions in Algorithm 1 one by one. • update H (H, x) is used to H. First, we remove from whose corresponding Then, we add into H the segment x. • update Q (Q, H, x) is also two tasks."
"First, we hypotheses whose overlap with segment x. candidate hypotheses"
"Algorithm 2 Parameter Optimization 1: w ← 0; 2: for (round r = 0; r &lt; R; r++) do 3: load graph G r (V, E), gold standard H r ; 4: initiate H and Q; 5: repeat 6: (x ′ , y ′ ) ← arg max (x,y)∈Q score(y); 7: if (y ′ is compatible with H r ) then 8: update H and Q; 9: else 10: ỹ ← positive(Q, x ′ ); 11: promote(w, ỹ); 12: demote(w, y ′ ); 13: update Q with w; 14: end if 15: until (Q = φ) 16: end for similar to the init Q (H) function."
"For each seg-ment, we maintain the top k candidates for each segment."
"In the previous section, we described an algorithm for graph-based incremental construction for a given weight vector w. In Algorithm 2, we present a Per-ceptron like algorithm to obtain the weight vector for the training data."
"For each given training sample (G r ,H r ), where H r is the gold standard hidden structure of graph G r , we first initiate cut T, hypotheses H T and can-didate queue Q by calling init H and init Q as in Al-gorithm 1."
Then we use the gold standard H r to guide the search.
"We select candidate (x ′ ,y ′ ) which has the highest operation score in Q."
"If y ′ is compatible with H r , we update H and Q by calling update H and update Q as in Algorithm 1."
"If y ′ is incompatible with H r , we treat y ′ as a negative sample, and search for a positive sample ỹ in Q with positive(Q, x ′ )."
"If there exists a hypothesis ỹ x ′ for fragment x ′ which is compatible with H r , then positive(Q, x ′ ) returns ỹ x ′ ."
"Otherwise positive(Q,x ′ ) returns the candidate hypothesis which is compatible with H r and has the highest operation score in Q."
Then we update the weight vector w with ỹ and y ′ .
"At the end, we update the candidate Q by using the new weights w."
"In order to improve the performance, we use Per-ceptron with margin in the training[REF_CITE]."
The margin is proportional to the loss of the hypothesis.
"Furthermore, we use aver-aged weights[REF_CITE]in Algorithm 1."
We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank[REF_CITE]extracted from Penn Treebank[REF_CITE]and Proposition Bank[REF_CITE].
Penn Treebank was previously used to train and evalu-ate various dependency parsers[REF_CITE].
"In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context."
The dependency relation encoded in the LTAG Treebank reveals deeper information for the follow-ing two reasons.
"First, the LTAG architecture itself reveals deeper dependency."
"Furthermore, the PTB was reconciled with the Propbank in the LTAG Tree-bank extracti[REF_CITE]."
"We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunc-tion and predicate coordination."
They are used to encode dependency relations which are unavailable in other approaches.
"On the other hand, these struc-tures turn out to be a big problem for the general rep-resentation of dependency relations, including ad-junction and coordination."
We will show that the algorithm proposed here provides a nice solution for this problem.
"In the LTAG Treebank[REF_CITE], each word is associated with a spinal template, which repre-sents the projection from the lexical item to the root."
Templates are linked together to form a derivation tree.
"The topology of the derivation tree shows a type of dependency relation, which we call LTAG dependency here."
"There are three types of operations in the LTAG Treebank, which are attachment, adjunction, and co-ordination."
Attachment is used to represent both substitution and sister adjunction in the traditional LTAG.
So it is similar to the dependency relation in other approaches.
The LTAG dependency can be a non-projective relation thanks to the operation of adjunction.
"In the LTAG Treebank, raising verbs and passive ECM verbs are represented as auxiliary trees to be ad-joined."
"In addition, adjunction is used to handle many cases of discontinuous arguments in Prop-bank."
"For example, in the following sentence, ARG1 of says in Propbank is discontinuous, which is First Union now has packages for seven customer groups. • First Union, he says, now has packages for seven customer groups."
"In the LTAG Treebank, the subtree for he says ad-joins onto the node of has, which is the root of the derivation tree, as shown in Figure 5."
Another special aspect of the LTAG Treebank is the representation of predicate coordination.
Figure 6 is the representation of the following sentence. • I couldn’t resist rearing up on my soggy loafers and saluting.
"The coordination between rearing and saluting is represented explicitly with a coord-structure, and this coord-structure attaches to resist."
"It is shown[REF_CITE]that coord-structures could en-code the ambiguity of argument sharing, which can be non-projective also."
We build LTAG derivation trees incrementally.
A hypothesis of a fragment is represented with a par-tial derivation tree.
"When the fragment hypotheses of two nearby fragments combine, the partial deriva-tion trees are combined into one."
It is trivial to combine two partial derivation trees with attachment.
We simply attach the root of one tree to some node on the other tree which is visible to this root node.
"Adjunction is similar to attachment, except that an adjoined subtree may be visible from the other side of the derivation tree."
"For example, in sentence • The stock of UAL Corp. continued to be pounded amid signs that British Airways ... continued adjoins onto pounded, and amid attaches to continued from the other side of the derivation tree (pounded is between continued and amid), as shown in Figure 7."
The predicate coordination is decomposed into a set of operations to meet the need for incremen-tal processing.
Suppose a coordinated structure at-taches to the parent node on the left side.
We build this structure incrementally by attaching the first conjunct to the parent and conjoining other con-juncts to first one.
"In this way, we do not need to force the coordination to be built before the attach-ment."
Either can be executed first.
A sample is shown in Figure 8.
"In this section, we will describe the features used in LTAG dependency parsing."
"An operation is repre-sented by a 4-tuple • op = (type, dir, pos left , pos right ), where type ∈ {attach,adjoin,conjoin} and dir is used to represent the direction of the operation. pos left and pos right are the POS tags of the two operands."
Features are defined on POS tags and lexical items of the nodes in the context.
"In order to represent the features, we use m for the main-node of the oper-ation, s for the sub-node, m r for the parent of the main-node, m 1 ..m i for the children of m, and s 1 ..s j for the children of s, as shown in Figure 9."
The in-dex always starts from the side where the operation takes place.
We use the Gorn addresses to represent the nodes in the subtrees rooted on m and s.
"Furthermore, we use l k and r k to represent the nodes in the left and right context of the flat sen-tence."
We use h l and h r to represent the head of the hypothesis trees on the left and right context respec-tively.
Let x be a node.
"We use x.p to represent the POS tag of node x, and x.w to represent the lexical item of node x."
Table 1 show the features used in LTAG depen-dency parsing.
There are seven classes of features.
"The first three classes of features are those defined on only one operand, on both operands, and on the siblings respectively."
"If gold standard POS tags are used as input, we define features on the POS tags in the context."
"If level-1 dependency is used, we define features on the root node of the hypothesis partial derivation trees in the neighborhood."
Half check and full check features are designed for grammatical check.
"For example, in Figure 9, node s attaches onto node m from left."
Then nothing can attach onto s from the right side.
"The children of the right side of s are fixed, so we use the half check features to check the completeness of the children of the right half for s."
"Furthermore, we notice that all the rightmost descendants of s and the leftmost descendants of m at each level become unavailable for any further operation."
So their children are fixed after this operation.
All these nodes are in the form of m 1.1...1 or s 1.1...1 .
We use full check features to check the children from both sides for these nodes.
"In the discussion above, we ignored adjunction and conjunction."
We need to slightly refine the con-ditions of checking.
"Due to the limit of space, we skip these cases."
We use the same data set as[REF_CITE].
"We use Sec. 2-21 of the LTAG Treebank for training, Sec. 22 for feature selection, and Sec. 23 for test."
Table 2 shows the comparison of different models.
Beam size is set to five in our experiments.
"With level-0 dependency, our system achieves an ac-curacy of 90.3% at the speed of 4.25 sentences a sec-ond on a Xeon 3G Hz processor with JDK 1.5."
"With level-1 dependency, the parser achieves 90.5% at 3.59 sentences a second."
Level-1 dependency does not provide much improvement due to the fact that level-0 features provide most of the useful informa-tion for this specific application.
It is interesting to compare our system with other dependency parsers.
"The accuracy on LTAG depen- dency is comparable to the numbers of the previ-ous best systems on dependency extracted from PTB with Magerman’s rules, for example, 90.3%[REF_CITE]and 90.9%[REF_CITE]."
"However, their experiments are on the PTB, while ours is on the LTAG corpus."
It should be noted that it is more difficult to learn LTAG dependencies.
"Theoretically, the LTAG de-pendencies reveal deeper relations."
"Adjunction can lead to non-projective dependencies, and the depen-dencies defined on predicate adjunction are linguis-tically more motivated, as shown in the examples in Figure 5 and 7."
The explicit representation of predi-cate coordination also provides deeper relations.
"For example, in Figure 6, the LTAG dependency con-tains resist → rearing and resist → saluting, while the Magerman’s dependency only contains resist → rearing."
The explicit representation of predicate coordination will help to solve for the de-pendencies for shared arguments.
"In our approach, each fragment in the graph is asso-ciated with a hidden structure, which means that we cannot reduce it to a labelling task."
"Therefore, the problem of interest to us is different from previous work on graphical models, such as CRF[REF_CITE]and MMMN[REF_CITE]."
"However, adjunction, prediction coordination, and long dis-tance dependencies in LTAG dependency parsing make it difficult to implement."
Our approach pro-vides a novel alternative to CFD.
Our learning algorithm stems from Perceptron training[REF_CITE].
"Variants of this method have been successfully used in many NLP tasks, like shallow processing[REF_CITE], parsing ([REF_CITE];"
Theoret-ical justification for those algorithms can be applied to our training algorithm in a similar way.
"In our algorithm, dependency is defined on com-plicated hidden structures instead of on a graph."
"Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations."
The search strategy of our bidirectional depen-dency parser is similar to that of the bidirectional CFG parser in ([REF_CITE];
A unique contribu-tion of this paper is that selection of path and deci-sions about action are trained simultaneously with discriminative learning.
"In this way, we can employ context information more effectively."
"In this paper, we introduced bidirectional incremen-tal parsing, a new architecture of parsing."
"We pro-posed a novel algorithm for graph-based incremen-tal construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn."
We evaluated the parser on an LTAG Tree-bank.
Experimental results showed significant im-provement over the previous best system.
"Incre-mental construction can be applied to other structure learning problems of high computational complex-ity, for example, such as machine translation and se-mantic parsing."
Parallel web pages are important source of training data for statistical machine translation.
"In this paper, we present a new approach to sentence alignment on parallel web pages."
Parallel web pages tend to have parallel structures，and the structural correspondence can be indica-tive information for identifying parallel sentences.
"In our approach, the web page is represented as a tree, and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment."
Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as “Hansard”.
"With improved sentence alignment performance, web mining sys-tems are able to acquire parallel sentences of higher quality from the web."
"Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translati[REF_CITE], and many other multi-lingual natural language processing applica-tions."
The task of aligning parallel sentences has received considerable attention since the renais-sance of data driven machine translation in late 1980s.
"During the past decades, a number of methods have been proposed to address the sentence align-ment problem."
"Although excellent performance was reported on clean corpora, they are less robust with presence of noise."
A recent study[REF_CITE]completed a systematic evalua-tion on different sentence aligners under various conditions.
"Their experiments showed that the per-formance of sentence aligners are sensitive to properties of the text, such as format complexity (presence of elements other than text), structural distance (a scale from literal to free translation), the amount of noise (text deletions or preprocess-ing errors) and typological distance between lan-guages."
Their performance varies on different type of texts and they all demonstrate marked perfor-mance degradation over noisy data.
"The results suggest that there is currently no universal solution to sentence alignment under all conditions, and different methods should be applied to different types of texts."
"In this paper, we specifically address sentence alignment on parallel web pages."
It has come to attention with the increasing trend of acquiring large-scale parallel data from the web.
"Currently, large-scale parallel data are not readily available for most language pairs and domains."
"But due to a sharply increasing number of bilingual web sites, web mining shows great promise as a solution to this knowledge bottleneck problem."
"Many systems[REF_CITE]have been developed to discover paral-lel web pages, and sentence aligners are used to extract parallel sentences from the mined web cor-pora."
"Sentence alignment performance on parallel web pages, therefore, becomes an increasingly im-portant issue for large-scale high-quality parallel data acquisition."
"Compared with clean parallel corpora such as &quot;Hansard&quot;[REF_CITE], which consists of"
"French-English translations of political debates in the Canadian parliament, texts from the web are far more diverse and noisy."
They are from many dif-ferent domains and of various genres.
Their trans-lation may be non-literal or written in disparate language pairs.
"Noise is abundant with frequent insertions, deletions or non-translations."
And there are many very short sentences of 1-3 words.
"Due to the characteristics of web corpora, direct appli-cation of conventional alignment methods without exploiting additional web document information acts as useful information to constrain the scope of search for parallel sentences."
"The paper is organized as follows: In section 2, we briefly survey previous approaches to sentence alignment."
"In section 3, we present the stochastic tree alignment model, including parameter estima-tion and decoding."
"Then in section 4, we describe how to use the tree alignment model in sentence alignment."
"Benchmarks are shown in section 5, and the paper is concluded in section 6. yields unsatisfactory alignment results."
Our approach to this problem is to make use of the structural parallelism between parallel web pages.
"Structural parallelism is the phenomenon, that when representing the same content in two different languages, authors have a very strong tendency to use the same document structure."
"As is shown in Figure 1, sentences located in similar position on both pages are more likely to be trans-lations."
"Hence, correspondence in the web page structure is an informative indication of parallel sentences."
"In our approach, the web page is represented as a tree, and a stochastic tree align-ment model is used to find the most probable alignment of the tree pair based on their structure and the texts in tree nodes."
The tree alignment then
"Sentence alignment methods can be categorized into three major categories: the length-based, lex-icon-based and hybrid method which combines the length-based model and lexicon-based model as complement to each other."
"The length model was based on the intuition that the length of a translated sentence is likely to be similar to that of the source sentence. (Brown et. at. 1991) used word count as the sentence length, where[REF_CITE]used character count."
Dynamic programming is used to search the optimal sentence alignment.
Both algorithms have achieved remarkably good results for language pairs like English-French and English-German with an error rate of 4% on average.
"But they are not robust with respect to non-literal translations, deletions and disparate language pairs."
"Unlike the length-based model, which totally ignores word identity, lexicon-based methods use lexical information to align parallel sentences."
Kay’s[REF_CITE]approach is based on the idea that words that are translations of each other will have similar distribution in source and target texts.
"By adopting the IBM model 1,[REF_CITE]used word translation probabilities, which he showed gives better accuracy than the sentence length based method."
Melamed[REF_CITE]rather used word correspondence from a different perspective as geometric corres-pondence for sentence alignment.
"The hybrid method combines the length model with the lexical method.[REF_CITE]used a two-pass approach, where the first pass performs length-based alignment at the cha-racter level as[REF_CITE]and the second pass uses IBM Model 1, following[REF_CITE]."
Moore’s[REF_CITE]approach is similar to Simard’s.
"The difference is that Moore used the data obtained in the first pass to train the IBM model in the second pass, so that his approach does not require a priori knowledge about the language pair."
"Instead of using a two-pass approach,[REF_CITE]combines the length model and the IBM model 1 in a unified framework under a maximum likelihood criterion."
"To make it more robust on noisy text, they developed a background model to handle text deletions."
"To further improve sentence alignment accuracy and robustness, methods that make use of addi-tional language or corpus specific information were developed."
"In Brown and Church’s length-based aligner, they assume prior alignment on some corpus specific anchor points to constrain and keep the Viterbi search on track.[REF_CITE]implemented a length-based model for Chinese-English with language specific lexical clues to im-prove accuracy.[REF_CITE]used cognates, which only exists in closely related language pairs.[REF_CITE]exploited the statistically ordered matching of punctuation marks in two lan-guages to achieve high accuracy sentence align-ment."
"In their web parallel data mining system,[REF_CITE]used HTML tags in the same way as cognates[REF_CITE]for align-ing Chinese-English parallel sentences."
Tree based alignment models have been successfully applied in machine translation ([REF_CITE]Yamada &amp;[REF_CITE]).
"The structure of the HTML document is recursive, with HTML markup tags embedded within other markup tags."
"While converting an HTML docu-ment into the tree representation, such hierarchical order is maintained."
"Each node of the tree is la-beled with their corresponding HTML tag ( e.g. body, title, i mg etc. ) and in labeling tree nodes, only markup tags are used and attribute value pairs are dropped."
"Among all markup tags in the HTML file, those of our most interest are tags containing content text, which is what we want to align."
These tags are those surrounding a text chunk or have the attribute of “ALT”.
"Comments, scripts and style specifications are not regarded as content text and hence are eliminated."
Figure 2 illustrates the tree representation of an example HTML document.
"Given two trees, the tree alignment is the non-directional alignments of their nodes."
A node in one tree can be aligned with at most one node in the other tree.
It is valid for a node to be aligned with nothing ( NULL ) and such case is regarded as node deletion in tree alignment.
"To comply with the tree hierarchical structure, we constrain that the alignments keep the tree hierarchy invariant i.e. if node A is aligned with node B , then the children of A are either deleted or aligned with the children of B ."
"Besides, to simplify the model training and de-coding, the tree alignment model also keeps the sequential order invariant, i.e. if node A is aligned with node B , then the left sibling nodes of A cannot be aligned with the right sibling nodes of B ."
"The stochastic tree alignment model assigns probabilities to tree alignments, based on the par-ticular configuration of the alignment and model parameters."
"Then, the decoder is able to find the most probable (optimal) alignment of two trees."
"To facilitate the presentation of the tree alignment model, the following symbols are introduced: giv-en a HTML document D, 𝑇 𝐷 denotes the corres-ponding tree; 𝑁 𝑖𝐷 denotes the i th node of 𝑇 𝐷 , and 𝑇 𝑖𝐷 denotes the sub-tree rooted at 𝑁 𝑖𝐷 ."
"Espe-cially, 𝑇 1𝐷 is the root of the tree 𝑇 𝐷 ."
"𝑇 [𝐷𝑖,𝑗] denotes the forest consisting of the sub-trees rooted at sibl-ing nodes from 𝑇 𝑖𝐷 to 𝑇 𝑗𝐷 ."
"𝑁 𝑖𝐷 . 𝑡 denotes the text in the node𝑁 𝑖𝐷 , and 𝑁 𝑖𝐷 .𝑡 denotes the label ( i.e. HTML tag) of the node 𝑁 𝑖𝐷 ;"
𝑁 𝑖𝐷 .
𝐶 𝑖 denotes the j th child of the node 𝑁 𝑖𝐷 ;
𝑁 𝑖𝐷 .
"𝐶 [𝑚,𝑛] denotes the con-secutive sequence of 𝑁 𝑖𝐷 ’s children nodes from 𝑁 𝑖𝐷 ."
𝐶 𝑚 to 𝑁 𝑖𝐷 .
𝐶 𝑛 ; the sub-tree rooted at 𝑁 𝑖𝐷 .
𝐶 𝑖 is represented as 𝑁 𝑖𝐷 .𝐶𝑇 𝑖 and the forest of the sub-trees rooted at 𝑁 𝑖𝐷 ’s children is represented as 𝑁 𝑖𝐷 .
"𝐶𝐹. To accommodate node deletion, NULL is introduced to denote the empty node."
"Finally, the tree alignment is referred as A ."
"Given two HTML documents F (in French) and E (in English) represented as trees 𝑇 𝐹 and 𝑇 𝐸 , the tree alignment task is defined as finding the align-ment A that maximizes the conditional probability Pr(𝐴|𝑇 𝐹 ,𝑇 𝐸 )."
"Based on the Bayes’ Rule, Pr(𝐴|𝑇 𝐹 ,𝑇 𝐸 ) ∝ Pr(𝑇 𝐹 ,𝑇 𝐸 |𝐴)Pr(𝐴) , where Pr(𝑇 𝐹 ,𝑇 𝐸 |𝐴) is the probability of synchronously generating 𝑇 𝐹 and 𝑇 𝐸 given the alignment A, and Pr(𝐴) is the prior knowledge of the tree alignment."
"To simplify computation, we assume a uniform prior probability Pr(𝐴)."
"Hence, the tree alignment task is to find the A that maximizes the synchron-ous probability Pr(𝑇 𝐹 , 𝑇 𝐸 |𝐴)."
"Based on the hierarchical structure of the tree, in order to facilitate the presentation and computation of the tree alignment probabilistic model, the fol-lowing alignment probabilities are defined in a hie-rarchically recursive manner:"
"Pr(𝑇 𝑚𝐹 ,𝑇 𝑖𝐸 |𝐴):"
"The probability of synchronously generating sub-tree pair {𝑇 𝑚𝐹 ,𝑇 𝑖𝐸 } given the align-ment A ; Pr(𝑁 𝑚𝐹 ,𝑁 𝑖𝐸 |𝐴): The probability of synchronously generating node pair {𝑁 𝑚𝐹 , 𝑁 𝑖𝐸 };"
"Pr(𝑇 [𝐹𝑚,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] |𝐴): The probability of synchron-ously generating forest pairs {𝑇 [𝐹𝑚,𝑛] ,𝑇 [𝐸𝑖,𝑗] } given the alignment A ."
"From the definition, the tree pair generative probability Pr(𝑇 𝐹 ,𝑇 𝐸 |𝐴) equals to the root sub-tree pair generative probability Pr(𝑇 1𝐹 , 𝑇 1𝐸 |𝐴)."
"The alignment of the sub-tree pair 𝑇 𝑗𝐹 and 𝑇 𝑖𝐸 may have the following configurations, based on which the tree pair generative probability Pr(𝑇 𝑗𝐹 ,𝑇 𝑖𝐸 |𝐴) can be calculated: (1) If 𝑁 𝑚𝐹 is aligned with 𝑁 𝑖𝐸 , and the children of 𝑁 𝑚𝐹 are aligned with children of 𝑁 𝑖𝐸 (as is shown in Fig. 3a), then we have"
"Pr.𝑇 𝑚𝐹 ,𝑇 𝑖𝐸 /𝐴0 = Pr(𝑁 𝑚𝐹 ,𝑁 𝑖𝐸 )"
"Pr(𝑁 𝑚𝐹 .𝐶𝐹, 𝑁 𝑖𝐸 .𝐶𝐹|𝐴) (2) If 𝑁 𝑚𝐹 is deleted, and the children of 𝑁 𝑚𝐹 is aligned with 𝑁 𝑖𝐸 (as shown in Fig. 3b), then we have"
"Pr.𝑇 𝑚𝐹 ,𝑇 𝑖𝐸 /𝐴0 ="
"Pr(𝑁 𝑚𝐹 |𝑁𝑈𝐿𝐿)Pr(𝑁 𝑚𝐹 .𝐶𝐹, 𝑇 𝑖𝐸 |𝐴) (3) If 𝑁 𝑖𝐸 is deleted, and 𝑁 𝑚𝐹 is aligned with child-ren of 𝑁 𝑖𝐸 (as shown in Fig. 3c), then we have"
The above equations involve forest pair generative probabilities.
"The alignment of the forest 𝑇 [𝐹𝑚,𝑛] and 𝑇 [𝐸𝑖,𝑗] may have the following configura-tions, based on which their forest pair generative probability Pr(𝑇 [𝐹𝑚,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] |𝐴) can be calculated: (4) If 𝑇 𝑚𝐹 is aligned with 𝑇 𝑖𝐸 , and 𝑇 [𝐹𝑚+1,𝑛] is aligned with 𝑇 [𝐸𝑖+1,𝑗] (as is shown in Fig. 4a), then"
"Pr.𝑇 [𝐹𝑚,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] /𝐴0 = Pr(𝑇 𝑚𝐹 , 𝑇 𝑖𝐸 |𝐴)Pr(𝑇 [𝐹𝑚+1,𝑛] ,"
"𝑇 [𝐸𝑖+1,𝑗] |𝐴) (5) If 𝑁 𝑚𝐹 is deleted, and the forest rooted at 𝑁 𝑚𝐹 ’s children 𝑁 𝑚𝐹 ."
"𝐶𝐹 is combined with 𝑇 [𝐹𝑚+1,𝑛] for alignment with 𝑇 [𝐸𝑖,𝑗] , then"
"Pr.𝑇 [𝐹𝑚,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] /𝐴0 = Pr(𝑁 𝑚𝐹 |𝑁𝑈𝐿𝐿)Pr(𝑁 𝑚𝐹 ."
"𝐶𝐹 𝑇 [𝐹𝑚+1,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] |𝐴) (6) If 𝑁 𝑖𝐸 is deleted, and the forest rooted at 𝑁 𝑖𝐸 ’s children"
"𝑁 𝑖𝐸 .𝐶𝐹 is combined with 𝑇 [𝐸𝑖,𝑗] for alignment with 𝑇 [𝐹𝑚,𝑛] , then"
"Pr.𝑇 [𝐹𝑚,𝑛] ,"
"𝑇 [𝐸𝑖,𝑗] /𝐴0 = Pr(𝑁 𝑖𝐸 |𝑁𝑈𝐿𝐿)Pr(𝑇 [𝐹𝑚,𝑛] , 𝑁 𝑚𝐹 ."
"𝐶𝐹 𝑇 [𝐸𝑖+1,𝑗] |𝐴)"
"Τ mF T [Fm1,n] T [Ei1,j] Τ iE (a)"
"Τ Fm NULL T [mF 1,n] T [Ei,j]"
"T mF .CF (b) T [mF ,n] NULL Τ Ei T [Ei1,j] (c) T iE .CF"
"Finally, the node pair probability is modeled as Pr(𝑁 𝑚𝐹 , 𝑁 𝑖𝐸 ) ="
"Pr(𝑁 𝑚𝐹 . 𝑡, 𝑁 𝑖𝐸 . 𝑡)Pr(𝑁 𝑚𝐹 . 𝑙, 𝑁 𝑖𝐸 . 𝑙) , where Pr(𝑁 𝑚𝐹 . 𝑡, 𝑁 𝑖𝐸 . 𝑡) is the generative probability of the translationally equivalent text chunks in 𝑁 𝑚𝐹 and 𝑁 𝑖𝐸 , and Pr(𝑁 𝑚𝐹 . 𝑙, 𝑁 𝑖𝐸 . 𝑙) is their HTML tag pair probability."
"The text chunk generative probability Pr(𝑁 𝑚𝐹 .𝑡,𝑁 𝑖𝐸 .𝑡) can be modeled in a variety of ways."
"The conventional length-based, lexicon-based or hybrid methods used for sentence alignment can be applied here."
"In the next sub- section, we focus on how to estimate the tag pair probability Pr(𝑁 𝑚𝐹 .𝑙,𝑁 𝑖𝐸 .𝑙) from a set of parallel web pages."
We expect pairs of the same or similar HTML tags to have high probabilities and the probabilities for pairs of disparate tags to be low.
"One way to estimate the tag pair generative proba-bility Pr(𝑙,𝑙′) is to manually align nodes between parallel trees, and use the manually aligned trees as the training data for maximum likelihood estima-tion."
"However, this is a time-consuming and error-prone procedure."
"Instead, the Expectation Maximi-zation (EM) (Dempster,[REF_CITE]) algorithm is used to estimate the parameters Pr(𝑙,𝑙′) on 5615 manually verified parallel web page pairs from 45 different bilingual web sites."
The parameter estimation proceeds as follows: 1. Start with initial parameter values. 2.
"Expectation: estimate :count::::::(𝑙,𝑙 ′ ) which is the expectation of aligning tag l with l &apos; . 3."
"Maximization: update the parameters based to maximum likelihood estimation Pr  l , l &apos;    countcount  l  , ll , l and & apos;  &apos;  l &apos; Pr( l NULL ) count  NULL , l   count  l , NULL    [ count "
"NULL , l &apos;   count  l &apos; , NULL  ] l &apos; 4. Repeat step 2 and 3 until the parameters stabilize"
"In step 2, :count::::::(𝑙,𝑙 ′ ) is the expected count of l being aligned with l &apos; in the training corpus."
"By definition, :count::::::(𝑙, 𝑙 ′ ) is calculated as :::::::count(𝑙, 𝑙 ′ ) = ; Pr(𝐴|𝑇 𝐹 , 𝑇 𝐸 )count(𝑙, 𝑙 ′ ) 𝐴 where count(𝑙, 𝑙 ′ ) is the number of occurrence of l being aligned with l ’ in the tree alignment A ."
"To efficiently compute :count::::::(𝑙, 𝑙 ′ ) without enumerating the exponential number of A’s in the above equation, we extended the inside-outside algorithm presented[REF_CITE]."
"The inside probability 𝛼(𝑁 𝑗𝐹 , 𝑁 𝑖𝐸 ) is defined as the probability of generating sub-tree pair {𝑇 𝑗𝐹 ,𝑇 𝑖𝐸 } when 𝑁 𝑖𝐸 is aligned with 𝑁 𝑗𝐹 ."
"It is estimated as:   N m F , N iE   Pr  N m F , N iE    N m F ."
"CF , N iE ."
CF  where 𝛼(𝑁 𝑚𝐹 .
"𝐶𝐹, 𝑁 𝑖𝐸 ."
𝐶𝐹) is the inside probability for the forest pair (𝑁 𝑚𝐹 .
"𝐶𝐹, 𝑁 𝑖𝐸 ."
𝐶𝐹)   N m F .
"CF , N iE ."
CF    Pr  N m F .
"CF , N iE ."
CF A  .
The inside probability can be estimated recursively according to the various alignment configurations presented in Figure 3 and Figure 4.
"The outside probability 𝛽(𝑁 𝑗𝐹 , 𝑁 𝑖𝐸 ) is defined as the probability of generating the part of 𝑇 𝐸 and 𝑇 𝐹 excluding the sub-trees 𝑇 𝑗𝐹 and 𝑇 𝑖𝐸 , when 𝑁 𝑖𝐸 is aligned with 𝑁 𝑗𝐹 ."
"It is estimated as:   N m F , N iE    [   a m F , a E  , q i , p p , q    a m F , q ."
"LC F  N m F  , a iE , p ."
"LC F  N iE     a m F , q ."
"RC F  N m F  , a iE , p ."
"RC F  N iE    Pr( a m F , k | NULL )  Pr( a iE , k | NULL )] k  q k  p where 𝑎 𝐹𝑚,𝑞 is the q th ancestor of 𝑁 𝑚𝐹 , and 𝑎 𝑖𝐸,𝑞 is the p th ancestor of 𝑁 𝑖𝐸 . 𝑎 𝑚𝐹 ,𝑘 (𝑘 &lt; 𝑞) is an ancestor of 𝑁 𝑚𝐹 and a decedent of 𝑎 𝑚𝐹 ,𝑞 ."
"Similarly 𝑎 𝑖𝐸,𝑘 (𝑘 &lt; 𝑝 is an ancestor of 𝑁𝑖𝐸 , and a decedent of 𝑎𝑖,𝑝𝐸 . a.LCF(N) is the forest rooted at a and to the left of N , and a.RCF(N). a.RCF(N) is the forest rooted as a and to the right of N ."
"Once inside and outside probabilities are computed, the expected counts can be calculated as count  l , l &apos;       N , N  N , N E   m F E  F m i i  T F , T E  N iE . l  l &apos; Pr( T F , T E ) N m F . l  l where Pr(𝑇 𝐹 ,𝑇 𝐸 ) is the generative probability of the tree pair {𝑇 𝐹 ,𝑇 𝐸 } over all possible alignment configurations."
"Pr(𝑇 𝐹 , 𝑇 𝐸 ) can be estimated using dynamic programming techniques that will be pre-sented in the next sub-section."
"Furthermore, the expected count of tag deletion is estimated as: count(𝑙,𝑁𝑈𝐿𝐿) = ; count(𝑙, 𝑙 ′ ) − ; :count::::::(𝑙, 𝑙 ′ )::::::: 𝑖 𝑖≠𝑁𝑈𝐿𝐿 :::::::count(𝑁𝑈𝐿𝐿, 𝑙) = ; count(𝑙 ′ , 𝑙) − ; :count::::::(𝑙 ′ , 𝑙) 𝑖 𝑖≠𝑁𝑈𝐿𝐿"
An intuitive way to find the optimal tree alignment is to enumerate all alignments and pick the one with the highest probability.
But it is intractable since the total number of alignments is exponential.
"Based on the observation that if two trees are op-timally aligned, the alignment of their sub-trees must also be optimal, dynamic programming can be applied to find the optimal tree alignment using that of the sub-trees in a bottom-up manner."
That is we first compute the optimal alignment probabili-ties of small trees and use them to compute that of the bigger tree by trying different alignment confi-gurations.
This procedure is recursive until the op-timal alignment probability of the whole tree is obtained.
The following is the pseudo-code of the bottom-up decoding algorithm: for i= | 𝑇 𝐸 | to 1 (bottom-up) { for j= | 𝑇 𝐹 | to 1 (bottom-up) { Select and store optimal alignments of their children fo-rests 𝑇 𝑚𝐹 .CF and 𝑇 𝑖𝐸 .CF by testing configurations 4-6; Select and store the optimal alignment of the sub-tree pair 𝑇 𝑚𝐹 and 𝑇 𝑖𝐸 by testing configurations 1-3; Store the optimal configuration}} where |𝑇 𝐹 | and |𝑇 𝐸 | are the number of nodes in 𝑇 𝐹 and 𝑇 𝐸 .
The decoding algorithm finds the op-timal alignment and its probability for every sub-trees and forests.
"By replacing the selection opera-tion with summing probabilities of all configura-tions, the sub-tree pair generative probability Pr(𝑇 𝐹 ,𝑇 𝐸 ) can be calculated along the way."
"The worst-case time complexity of the algorithm is 𝑂(|𝑇 𝐹 ||𝑇 𝐸 |.𝑑𝑒𝑔𝑟(𝑇 𝐹 ) + 𝑑𝑒𝑔𝑟(𝑇 𝐸 )0 2 ) , where the degree of a tree is defined as the largest degree of its nodes."
"Since the tree alignment model aligns parallel web pages at the tree node level instead of the sentence level, we integrate the tree alignment model with the sentence alignment model in a cascaded mode, in which the whole sentence alignment process is divided into two steps."
"In the first step, the tree alignment decoder finds the optimal alignment of the two trees."
Nodes having texts should be aligned with nodes containing their translations.
"Then in the second step, the conventional sentence aligner is used to align sentences within text chunks in the aligned nodes."
"In this step, various sentence align-ment models can be applied, including the length-based model, the lexicon-based model and the hy-brid model."
Language or corpus specific informa-tion may also be used to further improve sentence alignment accuracy.
The tree alignment acts as constraints that confine the scope of the search of sentence aligners.
"To evaluate the effectiveness of exploiting web page document structure with the tree alignment model for improving sentence alignment accuracy, we compared the performance of three types of sentence alignment methods on parallel web pages."
The first type is to simply discard web page layout information.
"Web pages are converted to plain texts, and HTML tags are removed prior to performing sentence alignment."
The second type is the baseline method of using web page document information.
"Instead of exploiting full HTML doc-ument structure, it follows Chen’s approach[REF_CITE]which uses HTML tags in the same way as cognates used[REF_CITE]."
The third type is the combination of tree alignment model and conventional sentence models. computer and literature.
"By manual annotation, 9,824 parallel sentence pairs are found."
"All sen-tence aligners run through the test parallel web pages, and each extracts a set of sentence pairs that it regards as parallel."
The output pairs are matched with the annotated parallel sentences from the test corpus.
Only exact matches of the sentence pairs are counted as correct.
"Our evaluation metrics are precision (P), recall (R) and F-measure (F) defined as:"
P  # of correctly aligned sentence pairs # of totaloutputpairs R  # of correctly aligned sentence pairs # of true parallel pairs F  2* P * R P  R
"Based on the results in table 1, we can see that both Type 2 and Type 3 aligners outperform con-ventional sentence alignment models."
Leveraging HTML document information can enhance sen-tence alignment quality.
"Especially, by using the tree alignment model, Type 3 aligners achieve a"
"Each type of the web page sentence aligner makes use of three conventional sentence align-ment models, one is the length based model fol-lowing[REF_CITE], one is the lexicon based model following[REF_CITE], and the other one is the hybrid model presented[REF_CITE]."
"To be fair in performance comparisons, the text genera-tive probability Pr(𝑁 𝐹 . 𝑡, 𝑁 𝐸 . 𝑡) in tree node alignment is modeled in accordance with that in the sentence alignment model."
"All these sentence aligners are implemented to handle sentence bead types of “1-0”, “0-1”,“1-1”, “1-2”,”1-3”,”2-1” and “3-1”."
"The test corpus is 150 parallel web page pairs randomly drawn from 20 Chinese-English bilin-gual web sites on topics related to politics, sports, significant increase of around 7% on both preci-sion and recall."
"Compared with the tree alignment model, the improvement by the Type 2 aligners is marginal."
"A reason for this is that the tree align-ment model not only exploits HTML tag similari-ties as in the Type 2 method, but also takes into account location of texts."
"In the tree alignment model, texts at similar locations in the tree hierar-chical structure are more probable to be transla-tions than those in disparate locations, even though they all have the same tag."
We also evaluate the performance of the tree aligner.
"Since sentence alignment is performed within the text chunks of aligned nodes, tree alignment accuracy is very important for correct sentence alignment."
We measure the alignment accuracy on all nodes as well as that specifically on text nodes on the test corpus.
The evaluation result is shown in table 2.
Benchmarks in Table 2 show that the tree alignment model yields very reliable results with high accuracy in aligning both text nodes and non-text nodes.
"After an analysis on text node align-ment errors, we find that 79.7% of them have texts of very short length (no more than 4 words), which may not contain sufficient information to be identi-fied as parallel."
"In this paper, we present a new approach to sen-tence alignment on parallel web pages."
"Due to the diversity and noisy nature of web corpora, a sto-chastic tree alignment model is employed to ex-ploit document structure in parallel web pages as useful information for identifying parallel sen-tences."
The tree alignment model can be combined with various conventional sentence alignment models to extract parallel sentences from parallel web pages.
Experimental results show that exploit-ing structural parallelism inherent in parallel web pages provides superior alignment performance over conventional sentence alignment methods and significant improvement (around 7% in both preci-sion and recall) is achieved by using the stochastic tree alignment model.
"With improved sentence alignment performance, web parallel data mining systems are able to acquire parallel sentences of higher quality and quantity from the web."
Previously topic models such as PLSI (Prob-abilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) were developed for modeling the contents of plain texts.
"Re-cently, topic models for processing hyper-texts such as web pages were also proposed."
The proposed hypertext models are generative models giving rise to both words and hyper-links.
This paper points out that to better rep-resent the contents of hypertexts it is more es-sential to assume that the hyperlinks are fixed and to define the topic model as that of gen-erating words only.
"The paper then proposes a new topic model for hypertext processing, referred to as Hypertext Topic Model (HTM)."
"HTM defines the distribution of words in a document (i.e., the content of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites."
"The topics are further characterized as distributions of words, as in the conventional topic models."
This paper fur-ther proposes a method for learning the HTM model.
Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets.
Topic models are probabilistic and generative mod-els representing contents of documents.
Examples of topic models include PLSI[REF_CITE]and LDA[REF_CITE].
"The key idea in topic mod-eling is to represent topics as distributions of words and define the distribution of words in document (i.e., the content of document) as a mixture over hid-den topics."
"Topic modeling technologies have been applied to natural language processing, text min-ing, and information retrieval, and their effective-ness have been verified."
"In this paper, we study the problem of topic mod-eling for hypertexts."
"There is no doubt that this is an important research issue, given the fact that more and more documents are available as hypertexts cur-rently (such as web pages)."
Traditional work mainly focused on development of topic models for plain texts.
"It is only recently several topic models for pro-cessing hypertexts were proposed, including Link-LDA and Link-PLSA-LDA[REF_CITE]."
We point out that existing models for hypertexts may not be suitable for characterizing contents of hypertext documents.
This is because all the models are assumed to generate both words and hyperlinks (outlinks) of documents.
"The generation of the latter type of data, however, may not be necessary for the tasks related to contents of documents."
"In this paper, we propose a new topic model for hypertexts called HTM (Hypertext Topic Model), within the Bayesian learning approach (it is simi-lar to LDA in that sense)."
"In HTM, the hyperlinks of hypertext documents are supposed to be given."
Each document is associated with one topic distribu-tion.
The word distribution of a document is defined as a mixture of latent topics of the document itself and latent topics of documents which the document cites.
The topics are further defined as distributions of words.
That means the content (topic distribu-tions for words) of a hypertext document is not only determined by the topics of itself but also the top-ics of documents it cites.
It is easy to see that HTM contains LDA as a special case.
"Although the idea of HTM is simple and straightforward, it appears that this is the first work which studies the model."
We further provide methods for learning and in-ference of HTM.
"Our experimental results on three web datasets show that HTM outperforms the base-line models of LDA, Link-LDA, and Link-PLSA-LDA, in the tasks of topic discovery and document classification."
The rest of the paper is organized as follows.
Sec-tion 2 introduces related work.
Section 3 describes the proposed HTM model and its learning and infer-ence methods.
Experimental results are presented in Section 4.
Conclusions are made in the last section.
There has been much work on topic modeling.
"Many models have been proposed including PLSI[REF_CITE], LDA[REF_CITE], and their extensions[REF_CITE]."
"Inference and learning methods have been developed, such as vari-ational inference[REF_CITE], expectation propagati[REF_CITE], and Gibbs sampling[REF_CITE]."
"Topic models have been uti-lized in topic discovery[REF_CITE], document retrieval[REF_CITE], docu-ment classificati[REF_CITE], citation analy-sis[REF_CITE], social network analysis[REF_CITE], and so on."
Most of the existing models are for processing plain texts.
"There are also models for processing hypertexts, for example,[REF_CITE], which are most relevant to our work."
"The model, which is a combination of PLSI and PHITS[REF_CITE], gives rise to both the words and hyperlinks (outlinks) of the document in the generative process."
The model is useful when the goal is to understand the distribution of links as well as the distribution of words.
We refer to the modified mode as Link-LDA and take it as a baseline in this paper.
Note that the above two models do not directly associate the top-ics of the citing document with the topics of the cited documents.
"Assuming that the cit-ing and cited documents share similar topics, they explicitly model the information flow from the cit-ing documents to the cited documents."
"In Link-PLSA-LDA, the link graph is converted into a bi-partite graph in which links are connected from cit-ing documents to cited documents."
"If a document has both inlinks and outlinks, it will be duplicated on both sides of the bipartite graph."
"The generative process for the citing documents is similar to that of Link-LDA, while the cited documents have a differ-ent generative process."
Their goal is to find topical influ-ence of publications in research communities.
They convert the citation graph (created from the publica-tions) into a bipartite graph as in Link-PLSA-LDA.
The content of a citing document is assumed to be generated by a mixture over the topic distribution of the citing document and the topic distributions of the cited documents.
"The differences between the topic distributions of citing and cited documents are measured, and the cited documents which have the strongest influence on the citing document are iden-tified."
"Note that in most existing models described above the hyperlinks are assumed to be generated and link prediction is an important task, while in the HTM model in this paper, the hyperlinks are assumed to be given in advance, and the key task is topic iden-tification."
"In the existing models for hypertexts, the content of a document (the word distribution of the document) are not decided by the other documents."
"In contrast, in HTM, the content of a document is determined by itself as well as its cited documents."
"Furthermore, HTM is a generative model which can generate the contents of all the hypertexts in a col-lection, given the link structure of the collection."
"Therefore, if the goal is to accurately learn and pre- dict contents of documents, the use of HTM seems more reasonable."
"In topic modeling, a probability distribution of words is employed for a given document."
"Specifi-cally, the probability distribution is defined as a mix-ture over latent topics, while each topic is future characterized by a distribution of words[REF_CITE]."
"In this paper, we introduce an extension of LDA model for hypertexts."
Table 1 gives the major notations and their explanations.
The graphic representation of conventional LDA is given in Figure 1(a).
The generative process of LDA has three steps.
"Specifically, in each document a topic distribution is sampled from a prior distribu-tion defined as Dirichlet distribution."
"Next, a topic is sampled from the topic distribution of the document, which is a multinominal distribution."
"Finally, a word is sampled according to the word distribution of the topic, which also forms a multinormal distribution."
The graphic representation of HTM is given in Figure 1(b).
The generative process of HTM is de-scribed in Algorithm 1.
"First, a topic distribution is sampled for each document according to Dirich-let distribution."
"Next, for generating a word in a document, it is decided whether to use the current"
Draw a word w dn ∼
"P(w dn | z dn , β) end for document or documents which the document cites. (The weight between the citing document and cited documents is controlled by an adjustable hyper-parameter λ.)"
It is also determined which cited doc-ument to use (if it is to use cited documents).
"Then, a topic is sampled from the topic distribution of the se-lected document."
"Finally, a word is sampled accord-ing to the word distribution of the topic."
HTM natu-rally mimics the process of writing a hypertext docu-ment by humans (repeating the processes of writing native texts and anchor texts).
The formal definition of HTM is given be-low.
"Hypertext document d has N d words w d = w d1 · · · w dN d and L d cited documents I d = {i d1 , . .. , i dL d }."
"The topic distribution of d is θ d and topic distributions of the cited documents are θ i , i ∈ I d ."
"Given λ, θ, and β, the conditional proba-bility distribution of w d is defined as:"
"Y N d X X p(w d |λ, θ, β) = p(b dn |λ) p(c dn |ξ d ) n=1 b dn c dn"
"X p(z dn |θ d ) b dn p(z dn |θ i dcdn ) 1−b dn p(w dn |z dn , β). z dn"
"Here ξ d , b dn , c dn , and z dn are hidden vari-ables."
"When generating a word w dn , b dn determines whether it is from the citing document or the cited documents. c dn determines which cited document it is when b dn = 0."
"In this paper, for simplicity we as-sume that the cited documents are equally likely to be selected, i.e., ξ di = L1 d ."
Note that θ represents the topic distributions of all the documents.
"For any d, its word distribution is affected by both θ d and θ i , i ∈ I d ."
"There is a propagation of topics from the cited documents to the citing document through the use of θ i , i ∈ I d ."
For a hypertext document d that does not have cited documents.
The conditional probability dis-tribution degenerates to LDA:
"Y N d X p(w d |θ d , β) = p(z dn |θ d ) p(w dn |z dn , β). n=1 z dn"
"By taking the product of the marginal probabil-ities of hypertext documents, we obtain the condi-tional probability of the corpus D given the hyper-parameters λ, α θ , β, p(D|λ, α θ , β) = Z Y D Y N d X X p(θ d |α θ ) p(b dn |λ) p(c dn |ξ d ) d=1 n=1 b dn c dn X p(z dn |θ d ) b dn p(z dn |θ"
"I dcdn ) 1−b dn z dn p(w dn |z dn , β)dθ. (1)"
Note that the probability function (1) also covers the special cases in which documents do not have cited documents.
"In HTM, the content of a document is decided by the topics of the document as well as the topics of the documents which the document cites."
As a result contents of documents can be ‘propagated’ along the hyperlinks.
"For example, suppose web page A cites page B and page B cites page C, then the content of page A is influenced by that of page B, and the con-tent of page B is further influenced by the content of page C. Therefore, HTM is able to more accu-rately represent the contents of hypertexts, and thus is more useful for text processing such as topic dis-covery and document classification."
"An exact inference of the posterior probability of HTM may be intractable, we employ the mean field variational inference method[REF_CITE]to conduct approxi-mation."
Let I[·] be an indicator function.
We first define the following factorized variational posterior distribution q with respect to the corpus:
Y D q = q(θ d |γ d ) d=1
"Y N d µ ¶ I[L d &gt;0] q(x dn |ρ dn )(q(c dn |ψ dn ) q(z dn |φ dn ) , n=1 where γ, ψ, φ, and ρ denote free variational parame-ters."
Parameter γ is the posterior Dirichlet parameter corresponding to the representations of documents in the topic simplex.
"Parameters ψ, φ, and ρ cor-respond to the posterior distributions of their asso-ciated random variables."
We then minimize the KL divergence between q and the true posterior proba-bility of the corpus by taking derivatives of the loss function with respect to variational parameters.
The solution is listed as below.
"Let β iv be p(w dnv = 1|z i = 1) for the word v. If L d &gt; 0, we have E-step:"
X N d X D X L d0 γ d i = α θ i + ρ dn φ dni + I [i d 0 l = d] n=1 d 0 =1 l=1 X N d0 (1 − ρ d 0 n )ψ d 0 nl φ d 0 ni . n=1 µ ³ © X k ¡ ρ dn = 1 + exp (φ dni E q [log(θ di )|γ d ] i=1 X L d ¢ − ψ dnl φ dni E q [log(θ I dl i )|γ I dl ] l=1 ¡ ¢ª´ −1 ¶ −1 + log λ − log 1 − λ .
X k φ dni E q [log(θ I dl i )|γ I dl ]}. i=1
X N d γ d i = α θ i + φ dni + n=1 X D X L d0 X N d0 I [i d 0 l = d] (1 − ρ d 0 n )ψ d 0 nl φ d 0 ni . d 0 =1 l=1 n=1 © ª φ dni ∝ β iv exp E q [log (θ di ) |γ d ] .
From the first two equations we can see that the cited documents and the citing document jointly af-fect the distribution of the words in the citing docu-ment.
X D X N d β ij ∝ φ dni w jdn . d=1 n=1
"In order to cope with the data sparseness problem due to large vocabulary, we employ the same tech-nique as that[REF_CITE]."
"To be specific, we treat β as a K ∗ V random matrix, with each row being independently drawn from a Dirichlet distri-bution β i ∼ Dir(α β ) ."
Variational inference is modified appropriately.
"We compared the performances of HTM and three baseline models: LDA, Link-LDA, and Link-PLSA-LDA in topic discovery and document classification."
Note that LDA does not consider the use of link in-formation; we included it here for reference.
We made use of three datasets.
The documents in the datasets were processed by using the Lemur Took kit[URL_CITE]and the low fre-quency words in the datasets were removed.
The first dataset WebKB (available[URL_CITE]contains six subjects (categories).
"There are 3,921 documents and 7,359 links."
"The vocabulary size is 5,019."
"The second dataset Wikipedia (available[URL_CITE]contains four subjects (categories): Biology, Physics, Chem-istry, and Mathematics."
"There are 2,970 documents and 45,818 links."
"The vocabulary size is 3,287."
The third dataset is ODP composed of homepages of researchers and their first level outlinked pages (cited documents).
We randomly selected five sub-jects from the ODP archive.
"They are Cognitive Science (CogSci), Theory, NeuralNetwork (NN), Robotics, and Statistics."
"There are 3,679 pages and 2,872 links."
"The vocabulary size is 3,529."
WebKB and Wikipedia are public datasets widely used in topic model studies.
ODP was collected by us in this work.
"We created four topic models HTM, LDA, Link-LDA, and Link-PLSA-LDA using all the data in each of the three datasets, and evaluated the top-ics obtained in the models."
"We heuristically set the numbers of topics as 10[REF_CITE]for WebKB, and 8 for Wikipedia (i.e., two times of the number of true subjects)."
We found that overall HTM can construct more understandable topics than the other models.
Figure 2 shows the topics related to the subjects created by the four models from the ODP dataset.
"HTM model can more accurately extract the three topics: Theory, Statistic, and NN than the other models."
"Both LDA and Link-LDA had mixed topics, labeled as ‘Mixed’ in Figure 2."
Link-PLSA-LDA missed the topic of Statistics.
"Interestingly, all the four models split Cognitive Science into two top-ics (showed as CogSci-1 and CogSci-2), probably because the topic itself is diverse."
We applied the four models in the three datasets to document classification.
"Specifically, we used the word distributions of documents created by the mod-els as feature vectors of the documents and used the subjects in the datasets as categories."
"We further randomly divided each dataset into three parts (train-ing, validation, and test) and conducted 3-fold cross-validation experiments."
"In each trial, we trained an SVM classifier with the training data, chose param-eters with the validation data, and conducted evalu-ation on classification with the test data."
"For HTM, we chose the best λ value with the validation set in each trial."
Table 2 shows the classification accura-cies.
We can see that HTM performs better than the other models in all three datasets.
We conducted sign-tests on all the results of the datasets.
"In most cases HTM performs statistically significantly better than LDA, Link-LDA, and Link-PLSA-LDA (p-value &lt; 0.05)."
The test results are shown in Table 3.
We conducted analysis on the results to see why HTM can work better.
"Figure 3 shows an example homepage from the ODP dataset, where superscripts denote the indexes of outlinked pages."
"The home-page contains several topics, including Theory, Neu-ral network, Statistics, and others, while the cited pages contain detailed information about the topics."
Table 4 shows the topics identified by the four mod-els for the homepage.
We can see that HTM can really more accurately identify topics than the other models.
The major reason for the better performance by HTM seems to be that it can fully leverage the infor- mation from the cited documents.
We can see that the content of the example homepage is diverse and not very rich.
It might be hard for the other base-line models to identify topics accurately.
"In con-trast, HTM can accurately learn topics by the help of the cited documents."
"Specifically, if the content of a document is diverse, then words in the document are likely to be assigned into wrong topics by the existing approaches."
"In contrast, in HTM with prop-agation of topic distributions from cited documents, the words of a document can be more accurately as-signed into topics."
"Table 5 shows the first 15 words and the last 10 words for the homepage given by HTM, in ascending order of b dn , which measures the degree of influence from the cited documents on the words (the smaller the stronger)."
"The table also gives the values of c dn , indicating which cited docu-ments have the strongest influence."
"Furthermore, the topics having the largest posterior probabilities for the words are also shown."
"We can see that the words ’experiment’, ’sample’, ’parameter’, ’perform’, and ’energy’ are accurately classified."
Table 6 gives the most salient topics of cited documents.
It also shows the probabilities of the topics given by HTM.
We can see that there is a large agreement between the most salient topics in the cited documents and the topics which are affected the most in the citing document.
Parameter λ is the only parameter in HTM which needs to be tuned.
"We found that the performance of HTM is not very sensitive to the values of λ, which reflects the degree of influence from the cited doc-uments to the citing document."
HTM can perform well with different λ values.
Figure 4 shows the clas-sification accuracies of HTM with respect to differ-ent λ values for the three datasets.
"We can see that HTM works better than the other models in most of the cases (cf., Table 2)."
"In this paper, we have proposed a novel topic model for hypertexts called HTM."
Existing models for processing hypertexts were developed based on the assumption that both words and hyperlinks are stochastically generated by the model.
The gener-ation of latter type of data is actually unnecessary for representing contents of hypertexts.
"In the HTM model, it is assumed that the hyperlinks of hyper- texts are given and only the words of the hypertexts are stochastically generated."
"Furthermore, the word distribution of a document is determined not only by the topics of the document in question but also from the topics of the documents which the doc-ument cites."
"It can be regarded as ‘propagation’ of topics reversely along hyperlinks in hypertexts, which can lead to more accurate representations than the existing models."
"HTM can naturally mimic hu-man’s process of creating a document (i.e., by con-sidering using the topics of the document and at the same time the topics of the documents it cites)."
We also developed methods for learning and inferring an HTM model within the same framework as LDA (Latent Dirichlet Allocation).
"Experimental results show that the proposed HTM model outperforms the existing models of LDA, Link-LDA, and Link-PLSA-LDA on three datasets for topic discovery and document classification."
"As future work, we plan to compare the HTM model with other existing models, to develop learn-ing and inference methods for handling extremely large-scale data sets, and to combine the current method with a keyphrase extraction method for ex-tracting keyphrases from web pages."
We thank Eric Xing for his valuable comments on this work.
This paper describes a new automatic method for Japanese predicate argument structure analysis.
"The method learns relevant features to assign case roles to the argument of the tar-get predicate using the features of the words located closest to the target predicate under various constraints such as dependency types, words, semantic categories, parts of speech, functional words and predicate voices."
We constructed decision lists in which these fea-tures were sorted by their learned weights.
"Us-ing our method, we integrated the tasks of se-mantic role labeling and zero-pronoun iden-tification, and achieved a 17% improvement compared with a baseline method in a sen-tence level performance analysis."
"Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text pro-cessing tasks, such as machine translation, informa-tion extracti[REF_CITE], question answering[REF_CITE], and summarizati[REF_CITE]."
"In English predicate argument structure analysis, large corpora such as FrameNet[REF_CITE], PropBank[REF_CITE]and NomBank[REF_CITE]have been created and utilized."
"Recently, the GDA Corpus[REF_CITE], Kyoto Text Corpus Ver.4.0[REF_CITE]and NAIST Text Corpus[REF_CITE]were constructed in Japanese, and these corpora have become the target of an automatic Japanese predicate argument structure analysis system."
"We conducted Japanese predicate argument structure (PAS) analysis for the NAIST Text Corpus, which is the largest of these three corpora, and, as far as we know, this is the first time PAS analysis has been conducted for whole articles of the corpus."
"The NAIST Text Corpus has the following char-acteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three ma-jor case roles, [Footnote_1] namely the ga, wo and ni-cases in Japanese are annotated for the base form of pred-icates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated."
1 Kyoto Text Corpus has about 15 case roles.
"As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates[REF_CITE]."
"For exam-ple, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.”"
"As in this example, nouns sometimes have argument structures referring to an event."
Such nouns are called event nouns[REF_CITE]in the NAIST Text Corpus.
"At the same time, the problems related to compound nouns are also important."
"In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments."
"For example, the compound noun, ‘ 企業買収 (corporate buyout)’ contains an event noun ‘ 買収 (buyout)’ and its ac-cusative, ‘ 企業 (corporate).’"
"However, compound nouns provide no information about syntactic de-pendency or about case markers, so it is difficult to specify the predicate-argument structure."
Komachi et al. investigated the argument structure of event nouns using the co-occurrence of target nouns and their case roles in the same sentence[REF_CITE].
"In these approaches, predicates and event nouns are dealt with separately."
"Here, we try to unify these different argument structures using de-cision lists."
"As regards ii), for example, in the causative sen-tence, ‘ メアリーはトムに夕食を作らせる (Mary makes Tom fix dinner),’ the basic form of the causative verb, ‘ 作らせる (make fix)’ is ‘ 作る (fix),’ and its nominative is ‘ トム (Tom)’ and the ac-cusative case role (wo-case) is ‘ 夕食 (dinner),’ al-though the surface case particle is ni (dative)."
"We must deal with syntactic transformations in passive, causative, and benefactive constructions when ana-lyzing the corpus."
"As regards iii) and iv), in Japanese, zero pronouns often occur, especially when the argument has al-ready been mentioned in previous sentences."
There have been many studies of zero-pronoun identifica-ti[REF_CITE].
"In this paper, we present a general procedure for handling both the case role assignment of predicates and event nouns, and zero-pronoun identification."
"We use the decision list learning of rules to find the closest words with various constraints, because with decision lists the readability of learned lists is high and the learning is fast."
The rest of this paper is organized as follows.
"We describe the NAIST Text Corpus, which is our tar-get corpus in Section 2."
We describe our proposed method in Section 3.
The result of experiments us-ing the NAIST Text Corpus and our method are re-ported in Section 4 and our conclusions are provided in Section 5.
"In the NAIST Text Corpus, three major obligatory Japanese case roles are annotated, namely the ga-case (nominative or subjective case), the wo-case (accusative or direct object) and the ni-case (da-tive or in-direct object)."
"The NAIST Text Corpus is based on the Kyoto Text Corpus Ver. 3.0, which contains 38,384 sentences in 2,929 texts taken from news articles and editorials in a Japanese newspaper, the ‘Mainichi Shinbun’."
"We divided these case roles into four types by lo-cation in the article as[REF_CITE], i) the case role depends on the predicate or the predicate depends on the case role in the intra-sentence (‘de-pendency relations’), ii) the case role does not de-pend on the predicate and the predicate does not de-pend on the case role in the intra-sentence (‘zero-anaphoric (intra-sentential)’), iii) the case role is not in the sentence containing the predicate (‘zero-anaphoric (inter-sentential)’), and iv) the case role and the predicate are in the same phrase (‘in same phrase’)."
"Here, we do not deal with exophora."
We show the distribution of the above four types in test samples in our split of the NAIST Text Corpus in Tables 1 and 2.
"In predicates, the ‘dependency relations’ type in the wo-case and the ni-case occur frequently."
"In event nouns, the ‘zero-anaphoric (intra-sentential)’ and ‘zero-anaphoric (inter-sentential)’ types in the ga-case oc-cur frequently."
"With respect to the ‘in same phrase’ type, the wo-case occurs frequently."
"In this section, we describe our algorithm."
"In the algorithm, we used various constraints when search-ing for the words located closest to the target predi-cate."
"We described these constraints as features with the direct products of dependency types (ic, oc, ga c, wo c, ni c, sc, nc, fw and bw), generalization levels (words, semantic categories, parts of speech), func-tional words and voices."
"In Japanese, the functional words in a phrase (Bun-setsu in Japanese) and the interdependency of bun-setsu phrases are important for determining the predicate argument structure."
"In accordance with the character of the dependency between the case roles and the predicates or event nouns, we divided Japanese word dependency into the following seven types that cover all dependency types in Japanese."
"Additionally, we use two optional dependency types."
"With this type, the target case role is the head-word of a bunsetsu phrase and the case role phrase depends on the target predicate phrase (Figure 1)."
"With this type, the target case role is the headword of a phrase and a phrase containing a target predicate or event noun depends on the case role phrase (Fig-ure 2)."
"With this type, the target case role and the target predicate or event noun are in the same phrase (Fig-ure 3)."
"With these types, a phrase containing the target case role depends on a phrase containing another predetermined case role (Figure 4)."
"We use the terms ‘ga c’, ‘wo c’ and ‘ni c’ when the predetermined case roles are the ga-case, wo-case and ni-case, re-spectively."
"With this type, a phrase containing the target case role and a phrase containing the target predicate or event noun are in the same article, but these phrases do not depend on each other (Figure 5)."
"Type fw and bw stand for ‘forward’ and ‘back-ward’ types, respectively."
Type fw means the word located closest to the target predicate or event noun without considering functional words or voices.
"With fw, the word is located between the top of the article containing the target predicate and the target predicate or event noun."
"Similarly, type bw means the word located closest to the target predicate or noun, which is located between the targeted predi-cate or event noun, and the tail of the article con-taining the predicate."
"We used three levels of generalization for every case role candidate, that is, word, semantic category, and part of speech."
"Every word is annotated with a part of speech in the Kyoto Text Corpus, and we used these annotations."
"With regard to semantic cate-gories, we annotated every word with a semantic category based on a Japanese thesaurus, Nihongo Goi Taikei."
"The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve[REF_CITE]."
"We mainly used the semantic classes of the third level, and partly the fourth level, which are similar to semantic roles."
We show the top three lev-els of the Nihongo Goi Taikei common noun the-saurus in Figure 6.
We annotated the words with their semantic category by hand.
We used a functional word in the phrase containing the target case role and active and passive voices for the predicate as base features.
The training algorithm used for our method is shown in Figure 7.
"First, the algorithm constructs features that search for the words located closest to the tar-get predicate under various constraints."
"Next, the algorithm learns by using linear Support Vector Ma-chines (SVMs)[REF_CITE]."
SVMs learn effec-tive features by the one vs. rest method for every case role.
We used TinySVM [URL_CITE] as an SVM imple-mentation.
"Moreover, we construct decision lists sorted by weight from linear SVMs."
"Finally, the al-gorithm calculates the existing probabilities of case roles for every predicate or event noun."
This step produces the criterion that decides whether or not we will determine the case roles when there is no in-terdependency between the case role candidate and the predicate.
"Our split of the NAIST Text Corpus has only 62,264 training samples for 2,874 predicates, and we predict that there will be a shortage of training sam-ples when adopting traditional learning algorithms, such as learning algorithms using entropy."
"So, we used SVMs with a high generalization capability to learn the decision lists."
The test algorithm of our method is shown in Fig-ure 8.
"In the test phase, we analyzed test samples using decision lists and the existing probabilities of case roles learned in the training phase."
"In step 1, we determined case roles using a decision list consisting of features exhibiting case role and predicate inter-dependency, that is, ic, oc, ga c, wo c, and ni c."
This is because there are many cases in Japanese where the syntactic constraint is stronger than the seman-tic constraint when we determine the case roles.
"In step 2, we determined case roles using a decision list of sc (‘in same phrase’) for the case roles that were not determined in step 1."
This step was mainly for event nouns.
Japanese event nouns frequently form compound nouns that contain case roles.
"In step 3, we decided whether or not to proceed to the next step by using the existing probabilities of case roles."
"If the probability was less than a certain threshold (50%), then the algorithm stopped."
"In step 4, we de-termined case roles using a decision list of the fea-tures that have no interdependency, that is, nc, fw and bw."
"This step will be executed when the target case role is syntactically necessary and determined by the co-occurrence of the case roles and predicate or event noun without syntactic clues, such as de-pendency, functional words and voices."
We performed our experiments using the NAIST Text Corpus 1.4β[REF_CITE].
"And we used 19,501 predicate and 5,276 event nouns from articles dated[REF_CITE]th to 17th and editorials dated October to December as test ex-amples."
This is a typical way to split the data.
We used the annotations in the Kyoto Text Corpus as the interdependency of bunsetsu phrases.
We used both individual and multiple words as case roles.
"We used the phrase boundaries annotated in the NAIST Text Corpus in the training phase, and used those annotated automatically by our system using POSs and simple rules in the test phase."
The accuracy of the automatic annotation is about 90%.
"To evaluate our algorithm, we conducted experi-ments using a baseline method."
"With the method, we used only nouns that depended on predicates or event nouns as case role candidates."
"If the functional word (post-positional case) in the phrase is ‘ga’,‘wo’ and ‘ni’, we determined the ga-case, wo-case, or ni-case for the candidates."
"Next, as regards event nouns in compound nouns, if there was another word in a compound noun containing an event noun and it co-occurred with the event noun as a case role with a higher probability in the training samples, then the word was selected for the case role."
The conventional approach for making decision lists utilizes the entropy of samples selected by the rules[REF_CITE].
We per-formed comparative experiments using Yarowsky’s entropy algorithm[REF_CITE].
The overall results are shown in Table 7.
"Here, ‘en-tropy’ indicates Yarowsky’s algorithm, which uses entropy[REF_CITE]."
"Throughout the test data, the F-measure (%) of our method exceeded that of the baseline system and the ‘entropy’ system."
"With the ga-case (nominative) in particular, the F-measure increased 9 points."
Table 3 shows some examples of the existing probabilities of case roles for predicates or event nouns.
"When the probabilities are extreme values such as the ni-case (dative) of 交渉 (negotiation), the wo-case (accusative) of 参加 (participation), and the wo-case and ni-base of 基づく (based on), we can decide to fill the targeted case role or not with high precision."
"However, it is difficult to decide to fill the targeted case role or not when the probability is close to 50 percent as in the ga-case of 使う (use)."
"We show the learned decision list of the ic type (the case role depends on the predicate or event noun), sc type (in the same phrase) and the other types for event noun 交渉 (negotiation) in Tables 4, 5 and 6, respectively."
"Here, ‘word’ in the ‘level’ column means ‘base form of predicate’ and ‘sem’ means ’semantic category of predicate.’"
"In the ic and sc type decision lists, features with semantic categories, such as ‘REGION’, ’LOCATION’ and ‘EVENT’, occupy a higher order."
"In contrast, in the list of the other types, the features that occupy the higher order are the features of the word base form."
This means local knowledge of relations be-tween case roles and predicates or event nouns in the word level is more important than semantic level knowledge.
We show the results we obtained for predicates in Table 8.
The results reveal that our method is supe-rior to the baseline system.
Our algorithm is partic-ularly effective in the ga-case.
We show the results we obtained for event nouns in Table 9.
This also shows that our method is superior to the baseline system.
The precision with sc type is high and our method is effective as regards event nouns.
We presented a new method for Japanese automatic predicate argument structure analysis using deci-sion lists based on the features of the words located closest to the target predicate under various con-straints.
The method learns the relative weights of these different features for case roles and ranks them using decision lists.
"Using our method, we inte-grated the knowledge of case role determination and zero-pronoun identification, and generally achieved a high precision in Japanese PAS analysis."
"In par- ticular, we can extract knowledge at various levels from the corpus for event nouns."
"In future, we will use richer constraints and research better ways of distinguishing whether or not cases are obligatory."
Obtaining labeled data is a significant obstacle for many NLP tasks.
"Recently, online games have been proposed as a new way of obtain-ing labeled data; games attract users by be-ing fun to play."
"In this paper, we consider the application of this idea to collecting seman-tic relations between words, such as hyper-nym/hyponym relationships."
"We built three online games, inspired by the real-life games of Scattergories TM and Taboo TM ."
"As[REF_CITE]players have entered nearly 800,000 data instances, in two categories."
"The first type of data consists of category/answer pairs (“Types of vehicle”,“car”), while the second is essentially free association data (“subma-rine”,”underwater”)."
We analyze both types of data in detail and discuss potential uses of the data.
We show that we can extract from our data set a significant number of new hy-pernym/hyponym pairs not already found in WordNet.
One of the main difficulties in natural language pro-cessing is the lack of labeled data.
"Typically, obtain-ing labeled data requires hiring human annotators."
"Recently, building online games has been suggested an alternative to hiring annotators."
"For example, v[REF_CITE]built the ESP Game [URL_CITE] , an online game in which players tag images with words that describe them."
It is well known that there are large numbers of web users who will play online games.
"If a game is fun, there is a good chance that sufficiently many online users will play."
We have several objectives in this paper.
"The first is to discuss design decisions in building word games for collecting data, and the effects of these decisions."
The second is to describe the word games that we implemented and the kinds of data they are designed to collect.
"As[REF_CITE]our games have been online for nearly a year, and have col-lected nearly 800,000 data instances."
The third goal is to analyze the resulting data and demonstrate that the data collected from our games is potentially use-ful in linguistic applications.
"As an example appli-cation, we show that the data we have collected can be used to augment WordNet[REF_CITE]with a significant number of new hypernyms."
"Our primary goal is to produce a large amount of clean, useful data."
"Each of these three objectives (“large”, “clean”, and “useful”) has important im-plications for the design of our games."
"First, in order to collect large amounts of data, the game must be attractive to users."
"If the game is not fun, people will not play it."
This requirement is perhaps the most significant factor to take into ac-count when designing a game.
"For one thing, it tends to discourage extremely complicated labeling tasks, since these are more likely to be viewed as work."
"It would certainly be a challenge (although not neces-sarily impossible) to design a game that yields la-beled parse data, for example."
"In this paper, we assume that if people play a game in real life, there is a good chance they will play it online as well."
"To this end, we built on-line versions of two popular “real-world” games: Scattergories TM and Taboo TM ."
"Not only are these games fun, but there is also a preexisting demand for online versions of these games, driving search traffic to our site."
We will go into more detail about these games in the next section.
An important characteristic of these games is that they involve more than one player.
Interacting with another player increases the sense of fun.
Another important feature these games share is that they are timed.
Timing has several advantages.
"First, tim-ing helps make the games feel more “game-like”, by adding a sense of urgency."
"Without timing, it risks feeling more like a labeling task than a game."
The next requirement is that the data be clean.
"First, the players must be capable of producing high-quality annotations."
"Second, the game should en-courage users to enter relevant data."
"We award points as a motivating factor, but this can lead play-ers to enter irrelevant data, or collude with other players, in order to get a higher score."
"In particu-lar, collusion is more likely when players can freely communicate."
"An excellent technique for producing good data, used effectively in the ESP game, is to require the players to match on their inputs."
Requir-ing players to match their partner’s hidden answers discourages off-topic answers and makes it quite dif-ficult to collude (requiring outside communication).
We use this technique in all of our games.
"Finally, the data must be useful."
"Ideally, it would be directly applicable to an NLP task."
This require-ment can come into conflict with the other goals.
"There are certainly many kinds of data that would be useful for NLP tasks (such as labeled parses), but designing a game to collect this data that people will play and that produces clean data is difficult."
"In this paper, we focus on a particular kind of lin-guistic data: semantic relationships between pairs of words and/or phrases."
We do this for several rea-sons.
"First, this kind of data is relatively simple, leading to fun games which produce relatively clean data."
"Second, the real-world games we chose to emulate naturally produce this kind of data."
"Third, there are a number of recent works which focus on extracting these kinds of relationships, e.g. ([REF_CITE]; Nakov &amp;[REF_CITE])."
Our work presents an interesting new way of extracting this type of data.
"Finally, at least one of these kinds of relationships, the hypernym, or “X is a Y” relation, has proven to be useful for a variety of NLP tasks."
We now describe our three games in detail.
"Categorilla, inspired by Scattergories TM , asks play-ers to supply words or phrases which fit specific cat-egories, such as “Things that fly” or “Types of fish”."
"In addition, each game has a specific letter which all answers must begin with."
"Thus, if the current game has letter “b”, reasonable answers would be “bird” and “barracuda”, respectively."
"In each game, a ran-domly matched pair of players are given the same 10 categories; they receive points when they match with the other player for a particular category."
Play-ers are allowed to type as may answers for a given category as they wish (until a match is made for that category).
"After a match is made, the players get to see what word they matched on for that category."
"Each answer is supposed to fit into a specific cate-gory, so the data is automatically structured."
"Our system contains 8 types of categories, many of which were designed to correspond to linguistic resources used in NLP applications."
Table 1 de-scribes the category types.
"The purpose of the first three types of categories is to extract hypernym/hyponym pairs like those found in WordNet (e.g., “food” is a hypernym of “pizza”)."
"In fact, the categories were automatically generated from WordNet, as follows."
"First, we assigned counts C s to each synset s in WordNet using the Sem-Cor [Footnote_2] labeled data set of word senses."
Let desc(s) be the set of descendants of s in the hypernym hi-erarchy.
"Then for each pair of synsets s,d, where d ∈ desc(s), we computed a conditional distribu-"
C d tion P(d|s) = P d0∈desc(s)
"C d0 , the probability that we choose node d from among the descendants of s. Finally, we computed the entropy of each node s in WordNet, P d∈desc(s) P(d|s)logP(d|s)."
Synsets with many different descendants occurring in Sem-Cor will have higher entropies.
Each node with a sufficiently high entropy was chosen as a category.
We then turned each synset into a category by tak-ing the first word in that synset and plugging it into one of several set phrases.
"For nouns, we tried two variants (“Types of food” and “Foods”)."
"Depend-ing on the noun, either of these may be more natu-ral (consider “Cities” vs. “Types of city”). “Types of food” tends to produce more adjectival answers than “Foods”."
We tried only one variation for verbs (“Methods of paying”).
"This phrasing is not per-fect; in particular, it encourages non-verb answers like “credit card”."
"The second group of categories tries to capture se-lectional preferences of verbs – for example, “ba- nana” makes sense as the object of “eat” but not as the subject."
"Our goal with these categories was to produce data useful for automatically labeling se-mantic roles (Gildea &amp;[REF_CITE]), where selec-tional preferences play an important role."
"We tried three different types of categories, corresponding to subjects, objects, and prepositional objects."
"Exam-ples are “Things that eat”, “Things that are eaten”, and “Things that are eaten with”, to which good answers would be “animals”, “food”, and “forks”."
These categories were automatically generated us-ing the labeled parses in Penn Treebank[REF_CITE]and the labeled semantic roles of Prop-Bank[REF_CITE].
"To generate the object categories, for example, for each verb we then counted the number of times a core argument (ARG0-ARG5) appeared as the direct object of that verb (according to the gold-standard parses), and used all verbs with count at least 5."
This guaran-teed that all generated categories were grammati-cally correct and captured information about core arguments for that verb.
"Most of the prepositional object categories proved to be quite confusing (e.g., “Things that are acted as”), so we manually removed all but the most clear."
"Not surprisingly, the use of the Wall Street Journal had a noticeable effect on the types of categories extracted; they have a definite fi-nancial bias."
"The third group of categories only has one type, which consists of adjective categories such as “Things that are large”."
"While we did not have any specific task in mind for this category type, having a database of attributes/noun pairs seems potentially useful for various NLP tasks."
"To generate these categories, we simply took the most common ad-jectives in the SemCor data set."
"Again, the result-ing set of adjectives reflect the corpus; for example, “Things that are green” was not generated as a cate-gory, while “Things that are corporate” was."
The final group of categories were hand-written.
"This group was added to make sure that a sufficient number of “fun” categories were included, since some of the category types, particularly the verb categories, are somewhat confusing and difficult."
"Most of the hand-written categories are of the form “Things found at/in X”, where X is a location, such as “Japan” or “the ocean”."
The starting letter requirement also has important consequences for data collection.
"It was designed to increase the variety of obtained data; without this restriction, players might produce a smaller set of “obvious” answers."
"As we will see in the results, this restriction did indeed lead to a great diversity of answers, but at a severe cost to data quality."
"Categodzilla is a slightly modified version of Cat-egorilla, with the starting letter constraint relaxed."
The combination of difficult categories and rare let-ters often leads to bad answers in Categorilla.
"To in-crease data quality, in Categodzilla for each category there are three boxes."
In the first box you can type any word you want.
Answers in the second box must start with a given “easy” letter such as “c”.
"Answers in the third box must start with a given “hard” letter, such as “k”."
The boxes much be matched in order; guesses typed in the first box which match either of the other two boxes are automatically propagated.
"Free Association, inspired by Taboo TM , simply asks players to type words related to a given “seed” word."
"Players are not allowed to type any of several words on a “taboo” list, specific to the current seed word."
"As soon as a match is achieved, players move on to a new seed word."
The seed words came from two sources.
The first was the most common words in SemCor.
"The sec-ond was the Google unigram data, which lists the most common words on the web."
"In both cases, we filtered out stop words (including all prepositions)."
"Unlike Categorilla, we found that nearly all col-lected Free Association data was of good quality, due to the considerably easier nature of the task."
"Of course, we do lose the structure present in Catego-rilla."
"As the name suggests, the collected data is es-sentially free word association pairs."
We analyze the data in depth to see what kinds of relations we got.
Two notable word games already exist for collecting linguistic data.
The first is the Open Mind Common Sense system [URL_CITE][REF_CITE].
The second is Verbosity [URL_CITE] (v[REF_CITE]).
"Both these games are designed to extract common sense facts, and thus have a different focus than our games."
There may not always be enough players available online to match a human player with another human player.
"Therefore, one important part of designing an online game is building a bot which can func-tion in the place of a player."
The bots for all of our games are similar.
Each has a simple random model which determines how long to wait between guesses.
The bot’s guesses are drawn from past guesses made by human players for that category/seed word (plus starting letter in the case of Categorilla).
"Just as with a human player, as soon as one of the bot’s guesses matches one of the player’s, a match is made."
"If there are no past guesses, the bot instead makes “imaginary” guesses."
"For example, in Categorilla, we make the (obviously false) assumption that for every category and every starting letter there are ex-actly 20 possible answers, and that both the player’s guesses and the bot’s imaginary guesses are drawn from those 20 answers."
"Then, given the number of guesses made by the player and the number of imaginary guesses made by the bot, the probabil-ity of a match can be computed (assuming that all guesses are made independently)."
"Once this proba-bility passes a certain threshold, randomly generated for each category at the start of each game, the bot matches one of the player’s guesses, chosen at ran-dom."
The Free Association bot works similarly.
"For Free Association, the bot rarely has to resort to generating these imaginary guesses."
"In Catego-rilla, due to the starting letter requirement, the bot has to make imaginary guesses much more often."
"Imaginary guessing can encourage poor behavior on the part of players, since they see that matches can occur for obviously bad answers."
They may also re-alize that they are playing against a bot.
"An additional complication for Categorilla and Categodzilla is that the bot has to decide which cat-egories to make guesses for, and in what order."
Our current guessing model takes into account past diffi-culty of the category and the current guessing of the human player to determine where to guess next.
"Table 2 shows statistics of each of the games, as of late[REF_CITE]."
"While we have collected nearly 800,000 data instances, nearly all of the games were between a human and the bot."
"Over the course of a year, our site received between 40 and 100 vis-its a day; this was not enough to make it likely for human-human games to occur."
The fact that we still collected this amount of data suggests that our bot is a satisfactory substitue for a human teammate.
We have anecdotally found that most players do not re-alize they are playing against a bot.
"While most of the data comes from games between a human and a bot, our data set consists only of input by the human players."
Our main tool for attracting traffic to our site was Google.
"First, we obtained $1 a day in AdWords, which pays for between 7 to 10 clicks on our ad a day."
"Second, our site is in the top 10 results for many relevant searches, such as “free online scatter-gories”."
"Categorilla was the most popular of the games, with about 25% more games played than Free As-sociation."
"Taking the longer length of Categorilla games into account (see Table 2), this corresponds to almost 90% more play time."
This is despite the fact that Free Association is the first game listed on our home page.
"We hypothesize that this is because Scattergories TM is a more popular game in real life, and so many people come to our site specifically looking for an online Scattergories TM game."
Cat-egodzilla has been played signficantly less; it has been available for less time and is listed third on the site.
"Even for Categodzilla, the least played game, we have collected on average 24 guesses per cate-gory."
Several of our design decisions for the games were based on trying to increase the diversity of an-swers.
Categorilla has the highest answer diversity.
"For a given category, each answer occurred on aver-age only 1.15 times."
"In general, this average should increase with the amount of collected data."
"How-ever, Categodzilla and Free Association have col-lected significantly fewer answers per category than Categorilla, but still have a higher average, around 1.4."
The high answer diversity of Categorilla is a direct result of the initial letter constraint.
"For all three games, the majority of category/answer pairs occurred only once."
Figure 1 shows the distribution over users of the number of games played.
"Not surprisingly, it follows the standard Zipfian curve; there are a large number of users who have played only a few games, and a few users who have played a lot of games."
The mid-dle of the curve is quite thick; for both Categorilla and Free Association there are more than 100 play-ers who have played between 21 and 50 games.
Figure 2 shows the distribution of initial letters of collected answers for each game.
"Categorilla is nearly flat over all letters besides ’q’, ’x’, and ’z’ which are never chosen as the inital letter con-straint."
This means players make a similar number of guesses even for difficult initial letters.
"In con-trast, the distribution of initial letters for Free Asso-ciation data reflects the relatively frequency of initial letters in English."
"Even though Categodzilla does have letter constraints in the 2nd and 3rd columns, its statistics over initial letter are very similar to Free Association."
"In our analyses, we take ALL guesses made at any time, whether or not they actually produced a match."
"This greatly increases the amount of usable data, but also increases the amount of noise in the data."
The biggest question about the data collected from Categorilla and Categodzilla is the quality of the data.
"Many categories can be difficult or some-what confusing, and the initial letter constraint fur-ther increases the difficulty."
"To evaluate the quality of the data, we asked three volunteer labelers to label 1000 total cate-gory/answer pairs."
"Each labeler labeled every pair with one of three labels, ’y’, ’n’, or ’k’. ’ y’ means that the answer fit the category. ’n’ means that it does not fit. ’k’ means that it “kind of” fits."
This was mostly left up to the labelers; the only suggestion was that one use of ’k’ could be if the category was “Things that eat” and the answer was “sandwich.”
"Here, the answer is clearly related to the category, but doesn’t actually fit."
"The inter-annotator agreement was reasonable, with a Fleiss’ kappa score of .49."
The main differ-ence between annotators was how permissive they were; the percentage of answers labeled ’n’ ranged from 58% for the first annotator to 35% for the third.
The labeled pairs were divided into 5 subgroups of 200 pairs each (described below); Table 3 shows the number of each label for the Categorilla-Random subset.
"We aggregated the different annotations by taking a majority vote; if all three answers were dif-ferent, the item was labeled ’k’."
Table 3 also shows the statistics of the majority vote on the same subset.
Overall Data Quality.
"We compared results for three random subsets of answers, Control-Random, Categorilla-Random, and Categodzilla-Random."
Categorilla-Random was built by select-ing 200 random category/answer pairs from the Cat-egorilla data.
Note that category/answer pairs that occurred more than once were more likely to be se-lected.
Categodzilla-Random was built similarly.
"Control-Random was built by randomly selecting two sets of 200 category/answer pairs each (includ-ing data from both Categorilla and Categodzilla), and then combining the categories from the first set with the answers from the second to generate a set of random category/answer pairs."
Table 4 shows results for these three subsets.
The chance for a control answer to be labeled ’y’ was 15%.
"Categorilla produces data that is significantly better than control, with 38% of answers labeled ’y’."
"Categodzilla, which is more relaxed about initial let-ter restrictions, is significantly better than Catego-rilla, with 72% of answers labeled ’y’."
This relax-ation has an enormous impact on the quality of the data.
Note however that these statistics are not ad-justed for accuracy of individual players; it may be that only more accurate players play Categodzilla.
Effect of Category Type on Data Quality.
"Within each type of category (see Table 1), cer-tain categories appear much more often than oth-ers due to the way categories are selected (at least two “easy” categories are guaranteed every game)."
"To adjust for this, we built a subset of 200 cat-egory/answer pairs by selecting 25 different cate-gories randomly from each type of category."
We then selected an answer at random from among the answers submitted for that category.
"In addition, we built a control set using the same 200 categories but instead using answers selected at random from the entire Categorilla data set."
Results for Categorilla data are shown in Figure 3; we omit the correspond-ing graph for control for lack of space.
"For most categories, the Categorilla data is significantly bet-ter than the control."
"The hand-written category type, O, has the best data quality, which is not surpris-ing because these categories allow the most possible answers, and thus are easiest of think of answers for."
These categories also have the highest number of ’y’ labels for the control.
"Next best are the hypernym categories, NType."
NType is much higher than the other noun hypernym category NHyp because the “Type of” phrasing is generally more natural and al-lows for adjectival answers.
"The VPP category type, which tries to extract prepositional objects, contains the most number of ’k’ annotations; this is because players often put answers that are subjects or ob-jects of the verb, such as “pizza” for “Things that are eaten with”."
"The adjective category type, Adj, has the lowest increase over the control; this is likely due to the nature of the extracted adjectives."
Effect of Initial Letter on Data Quality.
"In general, we would expect common initial letters to yield better data since there are more possible an-swers to choose from."
We did not have enough la-beled data to do letter by letter statistics.
"Instead, we broke the letters into two groups, based on the em-pirical difficulty of obtaining matches when given that initial letter."
"The easy letters were ‘abcfhlmn-prst’, while the hard letters were ‘degijkouvwy’."
Ta-ble 5 shows the results on Categorilla-Random and Control-Random on these two subsets.
"First, note that the results on Control-Random are the same for hard letters and easy letters."
This means that words starting with common letters are not more likely to fit in a category.
"For both hard letters and easy let-ters, the accuracy is considerably better on the Cat-egorilla data."
"However, the increase in the number of ’y’ labels for easy letters is twice that for hard letters."
The quality of data for hard letters is consid-erably worse than that for easy letters.
"In contrast to Categorilla and even Categodzilla, we found that the Free Association data was quite clean."
"However, it is also not structured; we simply get pairs of related words."
"Thus, the essential question for this game is what kind of data we get."
"To analyze the types of relationships between words, the authors labeled 500 randomly extracted unique pairs with a rich set of word-word relations, described in Table 6."
This set of relations was de-signed to capture the observed relationships encoun-tered in the Free Association data.
"Unlike our Cat-egorilla labeled set, pairs that occurred more than once were NOT more likely to be selected than pairs that occurred once (i.e., the category/answer pairs were aggregated prior to sampling)."
Sampling in this way led to more diversity in the pairs extracted.
"To label each pair, the authors found a sequence of relationships which connected the two words."
"In many cases, this was a single link."
"For example, “dragon” and “wing” are connected by a single link, “wing” IS PART OF “dragon”."
"In others, multiple links were required."
"For the seed word “dispute” and answer “arbitrator”, we can connect using two links: “dispute” IS OBJECT OF “resolve”, “arbitrator” IS SUBJECT OF “resolve”."
There were two other pos-sible ways to label a pair.
"First, they might be totally unrelated (i.e., a bad answer)."
"Second, they might be related, but not connectable using our set of basic relations."
"For example, “echo” is clearly related to “valley”, but in a complicated way."
The quality of the data is considerably higher than Categorilla and Categodzilla; under 10% of words are unrelated.
The category Desc deserves some discussion.
"This category included both simple adjective de-scriptions, such as “creek” and “noisy”, and also qualifiers, such as “epidemic” and “typhoid”, where one word specifies what kind of thing the other is."
The distinction between Desc and Phrase was sim-ply based on to what extent the combination of the two words was a set phrase (such as “east” and “Ger-many”).
Schulte im[REF_CITE]address very sim-ilar issues to those discussed in this section.
"They built a free association data set containing about 200,000 German word pairs using a combination of online and offline volunteers (but not a game)."
"They then analyze the resulting associations by comparing the resulting pairs to a large-scale lexical resource, GermaNet (the German counterpart of WordNet)."
"Our data analysis was by hand, making it compar-atively small scale but more detailed."
It would be interesting to compare the data sets to see whether the use of a game affects the resulting data.
"In this section, we consider a simple heuristic for filtering bad data: only retaining answers that were guessed some minimum number of times."
Note that in this section all answers were stemmed in order to combine counts across plurals and verb tenses.
"For the Categorilla data, filtering out cate-gory/answer pairs that only occurred once from Categorilla-Random left a total of 64 answers (from an original 200), of which 36 were labeled ’y’ and 8 were labeled ’k’."
"The fraction of ’y’ labels in the reduced set is 56%, up from 38% in the original set."
This gain in quality comes at the cost of losing slightly over two-thirds of the data.
"For Categodzilla-Random, a similar filter left 88 (out of 200), with 79 labeled ’y’ and 7 labeled ’k’."
"For the hand-labeled Free Association data, apply-ing this filter yielded a total of 123 pairs (out of an original 500), with only 2 having no relation [Footnote_5] ."
"5 The higher fraction of lost pairs for Free Association is pri-marily due to the method of sampling pairs for evaluation, as discussed in Section 8."
"In these two games, this filter eliminates nearly all bad data while keeping a reasonable fraction of the data."
"Clearly, this filter is less effective for Catego-rilla than the other two games."
"One of the main reasons for this is that the letter constraints cause people to try to fit words starting with that letter into all categories that they even vaguely relate to, rather than thinking of words that really fit that cat-egory."
"Examples include {“Art supplies”,“jacket”}, {“Things found in Chicago”,“king”} and {“Things that are African”,“yak”}."
"Of course, we can further increase the quality of the data by making the fil-ter more restrictive, at the cost of losing more data."
"For example, removing answers occuring fewer than 5 times from Categorilla-Random leaves only 8 an-swers (out of 200), 7 labeled ’y’ and 1 labeled ’n’."
There are other ways we could filter the data.
"For example, suppose we are given an outside database of pairs of words which are known to be semanti-cally related."
"We could apply the following heuris-tic: if an answer to a particular category is similar to many other answers for that category, then that an-swer is likely to be a good one."
Preliminary experi-ments using distributional similarity of words as the similarity metric suggest that this heuristic captures complimentary information to the guess frequency heuristic.
We leave as future work a full integration of the two heuristics into a single improved filter.
Categorilla and Categodzilla produce structured data which is already in a usable or nearly usable form.
"For example, the NHyp and NType categories pro-duce lists of hypernyms, which could be used to aug-ment WordNet."
We looked at this particular applica-tion in some detail.
"First, in order to remove noisy data, we used only Categodzilla data and removed answers which occurred only once."
"We took all category/answer pairs where the category was of type either NHyp or NType, and where the answer was a noun."
This re-sulted in 1604 potential hypernym/hyponym pairs.
We then hand-labeled a random subset of 200 of the 871 to determine how many of them were real hyper-nym/hyponym pairs.
The results are shown in Ta-ble 7.
"Counting compound hyponyms, nearly two-thirds of the pairs are real hypernym/hyponym pairs."
These new pairs could directly augment WordNet.
"For example, for the word “crime”, WordNet has as hyponyms “burglary” and “fraud”."
"However, it doesn’t have “arson”, “homicide”, or “murder”, which are among the 871 new pairs."
"WordNet lists “wedding” as being an “event”, but not “birthday”."
"The verb subject, object, and prepositional object categories were designed to collect data about the selectional preferences of verbs."
These categories turned out to be problematic for several reasons.
"First, statistics about selectional preferences of verbs are not too difficult to extract from the web (although in some cases they might be somewhat noisy)."
"Thus, the motivation for extracting this data using a game is not as apparent."
"Second, providing arguments of verbs out of the context of a sentence may be too dif-ficult."
"For example, for the category “Things that are accumulated”, there a couple of obvious answers, such as “wealth” or “money”, but beyond these it becomes more difficult."
"In the context of an actual document, quite a lot of things can accumulate, but outside of that context it is difficult to think of them."
One solution to this problem would be to provide context.
"For example, the category “Things that ac-cumulate in your body” is both easier to think of answers for and probably collects more useful data."
"However, automatically creating categories with the right level of specificity is not a trivial task; our ini-tial experiments suggested that it is easy to gener-ate too much context, creating an uninteresting cat-egory."
"The Free Association game produces a lot of very clean data, but does not classify the relationships be-tween the words."
"While a web of relationships might be useful by itself, classifying the pairs by relation type would clearly be valuable."
"One issue with extract-ing new relations from text, for example meronyms (part-of relationships), is that they tend to occur fairly rarely."
"Thus, it is very easy to get a large num-ber of spurious pairs."
Using our data as a set of can-didate pairs for relation extraction could greatly re-duce the resulting noise.
"We believe that application of existing techniques to the data from the Free As-sociation game could lead to a clean, classified set of word-word relations, but leave this as future work."
One way to extend Categorilla and Categodzilla would be to add additional types of categories.
"For example, a meronym category type (e.g. “Parts of a car”) would work well."
"Further developing the verb categories (e.g., “Things that accumulate in your body”) is another challenging but interesting direc-tion; these categories would produce phrase-word relationships rather than word-word relationships."
Probably the most interesting direction for future work is trying to increase the complexity of the data collected from a game.
"There are two significant dif-ficulties: keeping the game fun, and making sure the collected data is not too noisy."
One interesting ques-tion for future research is whether different game ar-chitectures might be better suited to certain kinds of data.
"For example, a “telephone” style game, where players relay a phrase or sentence through some noisy channel, might be an interesting way to obtain paraphrase data."
We examine the problem of content selection in statistical novel sentence generation.
"Our approach models the processes performed by professional editors when incorporating ma-terial from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmen-tation."
We propose and evaluate a method called “Seed and Grow” for selecting such auxiliary information.
"Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sen-tence generation."
"Evaluation results are sup-portive, indicating that a schemata model sig-nificantly improves over the baseline."
"In the context of automatic text summarisation, we examine the problem of statistical novel sentence generation, with the aim of moving from the current state-of-the-art of sentence extraction to abstract-like summaries."
"In particular, we focus on the task of selecting content to include within a generated sen-tence."
Our approach to novel sentence generation is to model the processes underlying summarisation as performed by professional editors and abstractors.
An example of the target output of this kind of gen-eration is presented in Figure 1.
"In this example, the human authored summary sentence was taken verba-tim from the executive summary of a United Nations proposal for the provision of aid addressing a partic-ular humanitarian crisis."
Such documents typically exceed a hundred pages.
"Human-Authored Summary Sentence: Repeated [poor seasonal rains] [Footnote_1] [in 2004] 2 , culminating in [food insecurity] 3 , indicate [another year] 4 of crisis, the scale of which is larger than last year’s and is further [exacerbated by diminishing coping assets] 5 [in both rural and urban areas] 6 ."
1 This corpus is described in detail in Section 5.1.
Statistical text-to-text summarisation applications have borrowed much from the related field of statis-tical machine translation.
"In one of the first works to present summarisation as a noisy channel approach,[REF_CITE]presented a conditional model for learning the suitability of words from a news article for inclusion in headlines, or ‘ultra-summaries’."
"Inspired by this approach, and with the intention of designing a robust statistical gener-ation system, our work is also based on the noisy channel model."
"Into this, we incorporate our con-tent selection model, which includes Witbrock and Mittal’s model supplemented with schema-based in-formation."
"Roughly, text-to-text transformations fall into three categories: those in which information is com-pressed, conserved, and augmented."
We use these distinctions to organise this overview of the litera-ture.
"In Sentence Compression work, a single sentence undergoes pruning to shorten its length."
Previ-ous approaches have focused on statistical syntactic transformations[REF_CITE].
"For con-tent selection, discourse-level considerations were proposed[REF_CITE], who ex-plored the use of Rhetorical Structure Theory[REF_CITE]."
"More recently,[REF_CITE]use Centering Theory[REF_CITE]and Lexical Chains[REF_CITE]to identify which information to prune."
Our work is similar in incorporating discourse-level phenomena for content selection.
"However, we look at schema-like information as opposed to chains of references and focus on the sentence augmentation task."
The work[REF_CITE]on Sentence Fusion introduced the problem of convert-ing multiple sentences into a single summary sen- tence.
Each sentence set ideally tightly clusters around a single news event.
"Thus, there is one gen-eral proposition to be realised in the summary sen-tence, identified by finding the common elements in the input sentences."
We see this as an example of conservation.
"In our work, this general proposition is equivalent to the core information for the sum-mary sentence before the incorporation of supple-mentary material."
"In contrast to both compression and conservation work, we focus on augmenting the information in a key sentence."
"The closest work is that[REF_CITE]and[REF_CITE], in which multiple sentences are processed, with fragments within them being recycled to gener-ate the novel generated text."
"In both works, recyclable fragments are identified by automatic means."
"While similar in task, our models differ substan-tially in the nature of the phenomenon modelled."
"In this work, we focus on content-based considerations that model which words can be combined to build up a new sentence."
There exists related work from Natural Language Generation (NLG) in finding material to build up sentences.
"As mentioned above, our content selec-tion model is inspired by work on schemata from NLG[REF_CITE]."
"However, their work represents patterns at the sen-tence level, and is thus not directly comparable to our work, given our focus on sentence generation."
"In our system, what is required is a means to rank words for use in generation."
"Thus, we focus on com-monly occurring word co-occurrences, with the aim of encoding conventions in the texts we are trying to generate."
"In this respect, this is similar to work[REF_CITE], who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles."
"In contrast, our work focuses on word patterns found within a summary sentence, not between sentences."
"Additionally, our tasks dif-fer as we examine the statistical sentence generation instead of sentence ordering."
The “Seed and Grow” approach proposed in this pa-per divides the word-level content selection prob-lem into two underlying subproblems.
"We address these with two separate models, called the salience and schematic models."
The salience model chooses the key content for the summary sentence while the schematic model attempts to identify what else is typically mentioned given those salient pieces of in-formation.
"There are a variety of methods for determining the salient information in a text, and these underpin most work in automatic text summarisation."
"As an example of a salience model trained on corpus data,[REF_CITE]introduced a method for scoring summary words for inclusion within news headlines."
"In their model, headlines were treated as ‘ultra-summaries’."
"Their model learns which words are typically used in headlines and encodes, at least to some degree, which words are attention grabbing."
"In the domain of funding proposals, key words that grab attention may amount to domain-specific buzzwords."
"Intuitively, a reader, perhaps someone in charge of allocating donations, tends to look for certain types of key information matching donation criteria, and so human abstract authors will target their summaries for this purpose."
"We thus adapt the[REF_CITE]model to identify such domain specific buzzwords (BWM, for ‘buzzword model’)."
"For an aligned sen-tence tuple, the probability that a word is selected based on the salience of a word with respect to the domain is defined as: |summary w | probbwm(select = 1|w) = |source w | (1) where summary w is the set of aligned sentence tu-ples that contain the word w in the summary sen-tence and in the source sentences."
"The denomina-tor, source w , is the set of aligned sentence tuples that have the word w in either the key or an auxiliary sen-tence."
"As is implicit in this equation, we could just use this buzzword model to select content not only from the key sentence, but from the auxiliary sentences as well."
"While it is intended ultimately to find the key content of the summary, it can also serve as an alternative baseline for auxiliary content selection to compare against the “Seed and Grow” model."
To restate the problem at hand: the task is one of finding elements of secondary importance that schematically elaborate on the key information.
We do this by examining sample summary sentences for conventional juxtapositions of concepts.
"As men-tioned in Section 1, schemata are approximated here with patterns of word-pair co-occurrences."
"Using a corpus of human-authored summaries in the domain of our application, it is thus possible to learn what those common combinations of words are."
"Roughly, the process is as follows."
"To begin with, a seed set of words is chosen."
The purpose of the seed set is to represent the core proposition of the summary sentence.
"In this work, this core proposition is given by the key sentence and so the non-stopwords belonging to it are used to populate the seed set."
"In the “Seed and Grow” approach, we check to see which words from auxiliary sentences pair well with words in the seed set."
Each training case in the corpus contains a single human-authored summary sentence that can be used to learn which pairs of words conventionally occur in a summary.
"For each summary sentence, stop-words are removed."
"Then, each pairing of words in the sentence is used to update a pair-wise word co-occurrence frequency table."
"When looking up and storing a frequency, the order of words is ignored."
"Strength For any two words, w 1 from the seed set and w 2 from an auxiliary sentence, the word-pair co-occurrence probability is defined as follows: probco-oc(w 1 w ), 2 freq(w 1 w ), 2 = (2) freq(w 1 )+ freq(w 2 ) − freq(w 1 w ), 2 where freq(w 1 w ) is a lookup in the word-pair co-, 2 occurrence frequency table."
This table stores co-occurrence word pairs occurring in the summary sentence.
"Each auxiliary word now has a series of scores, one for each comparison with a seed word."
"To rank each auxiliary word, these need to be combined into a single score for sorting."
"When combining the set of co-occurrence scores, one might want to account for the fact that each pair-ing of a seed word with an auxiliary word might not contribute equally to the overall selection of that auxiliary word."
"Intuitively, a word in the seed set, derived from the key sentence, may only make a minor contribution to the core meaning of the sum-mary sentence."
"For example, words that are part of an adjunct phrase in the key sentence might not be good candidates to elaborate upon."
"Thus, one might want to weight these seed words lower, to reduce their influence on triggering schematically associ-ated words."
"To allow for this, a seed weight vector is main-tained, storing a weight per seed word."
Different weighting schemes are possible.
"For example, a scheme might indicate the salience of a word."
"In addition to the buzzword model (BWM) described earlier, one might employ a standard vector space approach[REF_CITE]from Informa-tion Retrieval, which uses term frequency scores weighted with an inverse document frequency fac-tor, or tf-idf."
"We also implement the case in which all seed words are treated equally using binary weights, where 1 indicates the presence of a seed word, and 0 indicates its absence."
"In the evaluations described in Section 5, we refer to these three seed weighting schemes as bwm and tf-idf, and binary respectively."
"To find the probability of selecting an auxiliary word using the schematic word-pair co-occurrence model (WCM), an averaged probability is found by normalising the sum of the weighted probabili-ties, where weights are provided by one of the three schemes above: probwcm(w i ) = 1 | seed |"
"Z × ∑ weights k ×probco-oc(w i w ) (3), kk=0 where seed is the set of seed words and w k is the k th word in that set."
"The vector, weights, stores the seed weights."
"The normalisation factor for the weighted average, Z, is the number of auxiliary words."
"Finally, since the WCM model only serves to se-lect words from the auxiliary sentences, words from the key sentence must be given scores as well."
"For these words, the scoring is as follows: 1 1 probwcm(w) = Z |seed| + probwcm(w) (4) where Z is a normalisation across the set of seed words."
"As mentioned above, the noisy channel approach is used for producing the augmented sentence."
"Al-though the focus of this paper is on Content Selec-tion, an overview of the end-to-end generation pro-cess is presented for completeness."
Sentence augmentation is essentially a text-to-text process: A key sentence and auxiliary material are transformed into a single summary sentence.
"Fol-lowing[REF_CITE], the task is to search for the string of words that maximises the probability prob(summary|source)."
Standardly re-formulating this probability using Bayes’ rule re-sults in the following: prob cm (source|summary) × prob lm (summary) (5)
"In this paper, we are concerned with the first factor, prob cm (source|summary), referred to as the channel model (CM), which combines both the buzzword (BWM) and word-pair co-occurrence (WCM) models."
An examination of differences be-tween the two approaches revealed only a 20% word overlap on the Jaccard metric.
"In order to combine multiple models, we intend to use machine learning approaches to combine the information in each model in a similar manner[REF_CITE]."
"We are currently exploring the use of logistic regression methods to learn a func-tion that would treat, as features, the probabilities defined by the salience and schematic content selec-tion models."
"Although generation is possible using each content selection model in isolation, evalua-tions of the combined model are on-going and are not presented in this paper."
"In this evaluation, the task is to select n words from the aligned source sentences for inclusion in a sum-mary."
"As a gold-standard for comparison, we sim-ply examine what words were actually chosen in the summary sentence of the aligned sentence tuple."
"We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case."
We evaluate against the set of open-class words in the human-authored summary sentence using re-call and precision metrics.
"Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words)."
"This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric[REF_CITE]used in the Document Understanding Conferences [URL_CITE] (DUC)."
Precision is the size of the intersection normalised by the number of words selected.
"We also report the F-measure, which is the harmonic mean of the recall and precision scores."
"Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sen-tence for a particular test case."
"For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter’s stem-ming algorithm."
"However, the results from stem-ming are not that different from exact token matches when examining performance on the entire data set and so, for simplicity, these are omitted in this dis-cussion."
"The corpus is made up of a number of humanitar-ian aid proposals called Consolidated Appeals Pro-cess (UN CAP) documents, which are archived at the United Nations website. [URL_CITE] 135 documents from the period 2002 to 2007 were downloaded by the au-thors."
A preprocessing stage extracted text from the PDF files and segmented the documents into execu-tive summary and source sections.
These were then automatically segmented further into sentences.
"Executive summary sentences were manually aligned by the authors to source key and auxiliary sentences, producing a corpus of 580 aligned sen-tence tuples referred to here as the UN CAP cor-pus."
Statistics for the training portion of the sentence augmentation set are provided in Table 1.
"In this paper, aligned sentence tuples are obtained via manual annotation."
Automatic construction of these sentence-level alignments is possible and has been explored[REF_CITE].
"We also envisage using tools for scoring sentence similarity (for example, see[REF_CITE]) for automatically constructing them; this is the focus of work[REF_CITE]."
"Three baselines were used in this work: the random, tf-idf and position baselines."
A random word selec-tor shows what performance might be achieved in the absence of any linguistic knowledge.
We also sorted all words in the aligned source sen-tences by their weighted tf-idf scores.
This baseline selects words in order until the desired word limit is reached.
This baseline is referred to as the tf-idf baseline.
"Finally, we selected words based on their sen-tence order, choosing first those words from the key sentence."
"When these are exhausted, auxiliary sen-tences are sorted by their sentence positions in the original document."
Words from the first auxiliary sentence are then chosen.
"This continues until ei-ther the desired number of words have been chosen, or no words remain."
This baseline is known as the position baseline.
We compare the three baselines to the two mod-els presented in Section 3.
These are the buzzword salience model (BWM) and the schematic word-pair co-occurrence model (WCM).
"We begin by presenting recall, precision and F-measure graphs when selecting from the aligned source sentences, comprising the key and auxiliary sentences."
Figure 3 shows the results for the two models against the three baselines.
"The two mod-els, the positional, and the tf-idf baselines perform better than the random baseline, as measured by a two-tailed Wilcoxon Matched Pairs Signed Ranks test ( α = 0 05).."
"The WCM consistently out-performs the BWM on all metrics, and the differences are statistically significant."
"In fact, the BWM also generally per-forms worse than the position and tf-idf baselines."
WCM and the position baseline both significantly outperform the tf-idf baseline on all metrics for longer sentence lengths.
"That the position baseline and WCM should per-form similarly is not really surprising since, in ef-fect, the position baseline first chooses words from the key sentence and then selects auxiliary words."
The difference essentially lies in how the auxiliary words are chosen.
The results of Figure 3 weakly support the hypothesis that using schematic word-pair co-occurrences helps improve performance over mod-els without discourse-related features.
The graphs show that WCM edges above the position base-line when the number of selected open-class words ranges from 10 to 15.
Note that the average num-ber of open-class words in a human authored sum-mary sentence is 16.
The only significant difference found was in the F-measure and precision scores for 19 selected open-class words.
"Nevertheless, a gen-eral trend can be observed in which WCM performs better than the position baseline."
"Ultimately, however, what we want to do is select auxiliary content to supplement the key sentence."
"To examine the effect of two best performing ap-proaches, WCM and the position baseline, on this task, were both modified so that the key sentence words were explicitly given a zero probability."
"Thus, the recall, precision and F-measure scores obtained are based solely on the ability of either to select aux-iliary words."
The F-measure scores are presented Figure 4.
WCM consistently outperforms the po-sition baseline for the selection of auxiliary words.
Differences are significant for 6 or more selected open-class words.
"The results show that even when considering only exact token matches, we can improve on the re-call of open-class words, and do so without penalty in precision."
Our working hypothesis is that such gains are possible because the corpus has a homo- geneous quality and key patterns are sufficiently re-peated even when the overall data set is of the or-der of hundreds of cases.
The benefit of using a model encoding some schematic information is fur-ther shown by the performance of WCM over the position baseline when selecting words from auxil-iary sentences.
"This is an interesting finding given that do-main independent methods are increasingly used on domain-specific corpora such as financial and biomedical texts, for which we may have access to only a limited amount of data."
"We anticipate that as we introduce methods to account for paraphrase and synonym differences, performance might rise fur-ther still."
We can also weight seed words in the “Seed and Grow” approach in a variety of ways.
"To test whether weighting schemes have any effect on con-tent selection performance, we examined the use of three schemes."
We were particularly interested in those schemes that indicate the contribution of a seed word to the core meaning of a sentence.
"These are the binary, tf-idf and buzzword weight-ing schemes described in Section 3."
We present the F-measure graph for these three variants of the schematic word-pair co-occurrence model (WCM) in Figure 5.
The graphs show that there is no discernible dif-ference between the seed weighting schemes.
No scheme significantly outperforms another.
"Thus, we conclude that the choice of these particular seed weighting schemes has no effect on performance."
"In future work, we intend to examine whether weight-ing schemes encoding syntactic information might fare better, since such information might more accu-rately represent the contribution of a substring to the main clause of the sentence."
"In this paper, we argued a case for sentence augmen-tation, a component that facilitates abstract-like text summarisation."
We showed that such a process can account for summary sentences as authored by pro-fessional editors.
"We proposed the use of schemata, as approximated with a word-pair co-occurrence model, and advocated a new schema-based “Seed and Grow” content selection model used for statisti-cal sentence generation."
"We also showed that domain-specific patterns, schematic word-pair co-occurrences in this case, can be acquired from a limited amount of data as indi-cated by modest performance gains for content se-lection using schemata information."
We postulate that this is particularly true when dealing with ho-mogeneous data.
"In future work, we intend to explore other string matches corresponding to variations due to para-phrases and synonymy."
We would also like to study the effects of corpus size when learning schematic patterns.
"Finally, we are currently investigating the use of machine learning methods to combine the best of the Salience and Schemata models in order to provide a single model for use in decoding."
We would like to thank the reviewers for their in-sightful comments.
This work was funded by the CSIRO ICT Centre and Centre for Language Tech-nology at Macquarie University.
It is a challenging task to identify sentiment polarity of Chinese reviews because the re-sources for Chinese sentiment analysis are limited.
"Instead of leveraging only monolin-gual Chinese knowledge, this study proposes a novel approach to leverage reliable English resources to improve Chinese sentiment analysis."
"Rather than simply projecting Eng-lish resources onto Chinese resources, our ap-proach first translates Chinese reviews into English reviews by machine translation ser-vices, and then identifies the sentiment polar-ity of English reviews by directly leveraging English resources."
"Furthermore, our approach performs sentiment analysis for both Chinese reviews and English reviews, and then uses ensemble methods to combine the individual analysis results."
Experimental results on a dataset of 886 Chinese product reviews dem-onstrate the effectiveness of the proposed ap-proach.
"The individual analysis of the translated English reviews outperforms the in-dividual analysis of the original Chinese re-views, and the combination of the individual analysis results further improves the perform-ance."
"In recent years, sentiment analysis (including sub-jective/objective analysis, polarity identification, opinion extraction, etc.) has drawn much attention in the NLP field."
"In this study, the objective of sen-timent analysis is to annotate a given text for polar-ity orientation (positive/negative)."
"Polarity orientation identification has many useful applica-tions, including opinion summarizati[REF_CITE]and sentiment retrieval[REF_CITE]."
"To date, most of the research focuses on English and a variety of reliable English resources for sen-timent analysis are available, including polarity lexicon, contextual valence shifters, etc."
"However, the resources for other languages are limited."
"In particular, few reliable resources are available for Chinese sentiment analysis [Footnote_1] and it is not a trivial task to manually label reliable Chinese sentiment resources."
1 This study focuses on Simplified Chinese.
"Instead of using only the limited Chinese knowl-edge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources."
"Generally speak-ing, there are two unsupervised scenarios for “bor-rowing” English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated[REF_CITE]; the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated."
"In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources."
"Furthermore, ensemble methods are employed to combine the individual analysis re-sults in each language (i.e. Chinese and English) in order to obtain improved results."
"Given machine translation services between the selected target language and English, the proposed approach can be applied to any other languages as well."
Experiments have been performed on a dataset of 886 Chinese product reviews.
Two commercial machine translation services (i.e. Google Translate and Yahoo Babel Fish) and a baseline dictionary-based system are used for translating Chinese re-views into English reviews.
Experimental results show that the analysis of English reviews trans-lated by the commercial translation services out-performs the analysis of original Chinese reviews.
"Moreover, the analysis performance can be further improved by combining the individual analysis results in different languages."
The results also demonstrate that our proposed approach is more effective than the approach that leverages gener-ated Chinese resources.
The rest of this paper is organized as follows: Section 2 introduces related work.
The proposed approach is described in detail in Section 3.
Sec-tion 4 shows the experimental results.
Lastly we conclude this paper in Section 5.
"Polarity identification can be performed on word level, sentence level or document level."
"Related work for word-level polarity identification includes ([REF_CITE]; Kim and Hovy. 2004;[REF_CITE]), and related work for sentence-level polarity identification in-cludes ([REF_CITE]; Kim and Hovy. 2004)"
Word-level or sentence-level senti-ment analysis is not the focus of this paper.
"Generally speaking, document-level polarity identification methods can be categorized into un-supervised and supervised."
Unsupervised methods involve deriving a senti-ment metric for text without training corpus.
Supervised methods consider the sentiment analysis task as a classification task and use la-beled corpus to train the classifier.
"Since the work[REF_CITE], various classification models and linguistic features have been proposed to im-prove the classification performance[REF_CITE]."
"Most recently,[REF_CITE]investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity."
Research work focusing on Chinese sentiment analysis includes[REF_CITE].
Such work represents heuristic extensions of the unsupervised or supervised methods for English sentiment analysis.
"To date, the most closely related work is[REF_CITE], which explores cross-lingual pro-jections to generate subjectivity analysis resources in Romanian by leveraging on the tools and re-sources available in English."
"They have investi-gated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections."
"In this study, we focus on unsupervised sentiment polarity iden-tification and we only investigate the lexicon-based approach in the experiments."
Other related work includes subjective/objective analysis[REF_CITE]and opinion mining and summa-rization ([REF_CITE]; Popescu and Etzioni. 2005;[REF_CITE]).
"The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources for sentiment analysis are limited or unreliable."
This study focuses on unsupervised polarity identi-fication of Chinese product reviews by using both the rich English knowledge and the limited Chi-nese knowledge.
The framework of our approach is illustrated in Figure 1.
"A Chinese review is translated into the corresponding English review using machine trans-lation services, and then the Chinese review and the English review are analyzed based on Chinese resources and English resources, respectively."
The analysis results are then combined to obtain more accurate results under the assumption that the indi-vidual sentiment analysis can complement each other.
"Note that in the framework, different ma-chine translation services can be used to obtain different English reviews, and the analysis of Eng-lish reviews translated by a specific machine trans-lation service is conducted separately."
"For simplicity, we consider the English reviews trans-lated by different machine translation services as reviews in different languages, despite the fact that in essence, they are still in English."
"Formally, give a review rev 0 in the target lan-guage (i.e. Chinese), the corresponding review rev i in the ith language is obtained by using a transla-tion function: rev i =f iTrans (rev 0 )， where 1≤i≤p and p is the total number of machine translation ser-vices."
"For each review rev k in the kth language (0≤k≤p), we employ the semantic oriented ap-proach to assign a semantic orientation value f kSO (rev k ) to the review, and the polarity orientation of the review can be simply predicated based on the value by using a threshold."
"Given a set of se-mantic orientation values F SO ={f kSO (rev k ) | 0≤k≤p}, the ensemble methods aim to derive a new seman-tic orientation value f SOEnsemble (rev 0 ) based on the values in F SO , which can be used to better classify the review as positive or negative."
"The steps of review translation, individual se-mantic orientation value computation and ensem-ble combination are described in details in the next sections, respectively."
Translation of a Chinese review into an English review is the first step of the proposed approach.
"Manual translation is time-consuming and labor-intensive, and it is not feasible to manually trans-late a large amount of Chinese product reviews in real applications."
"Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory."
A few commercial machine translation services can be publicly accessed.
"In this study, the following two commercial machine translation services and one baseline system are used to translate Chinese reviews into English re-views."
Google Translate [URL_CITE] (GoogleTrans): Google Translate is one of the state-of-the-art commercial machine translation systems used today.
Google Translate applies statistical learning techniques to build a translation model based on both monolin-gual text in the target language and aligned text consisting of examples of human translations be-tween the languages.
"Yahoo Babel Fish [URL_CITE] (YahooTrans): Different from Google Translate, Yaho Babel Fish uses SYSTRAN’s rule-based translation engine."
SYSTRAN was one of the earliest developers of machine translation software.
SYSTRAN applies complex sets of specific rules defined by linguists to analyze and then transfer the grammatical struc-ture of the source language into the target language.
Baseline Translate (DictTrans): We simply de-velop a translation method based only on one-to-one term translation in a large Chinese-to-English dictionary.
"Each term in a Chinese review is trans-lated by the first corresponding term in the Chi-nese-to-English dictionary, without any other processing steps."
"In this study, we use the LDC_CE_DIC2.0 [URL_CITE] constructed by LDC as the dic-tionary for translation, which contains 128366 Chinese terms and their corresponding English terms."
The Chinese-to-English translation perform-ances of the two commercial systems are deemed much better than the weak baseline system.
Google Translate has achieved very good results on the Chinese-to-English translation tracks of NIST open machine translation test (MT) [URL_CITE] and it ranks the first on most tracks.
"In the Chinese-to-English task[REF_CITE]the BLEU-4 score of Google Translate is 0.3531, and the BLEU-4 score of SYSTRAN is 0.1471."
"We can deduce that Google Translate is better than Yahoo Babel Fish, without considering the recent improvements of the two systems."
Here are two running example of Chinese re-views and the translated English reviews (Human-Trans refers to human translation):
"For any specific language, we employ the semantic orientated approach[REF_CITE]to compute the semantic orientation value of a re-view."
"The unsupervised approach is quite straight-forward and it makes use of the following sentiment lexicons: positive Lexicon (Posi-tive_Dic) including terms expressing positive po-larity, Negative Lexicon (Negative_Dic) including terms expressing negative polarity, Negation"
"Lexicon (Negation_Dic) including terms that are used to reverse the semantic polarity of a particular term, and Intensifier Lexicon (Intensifier_Dic) including terms that are used to change the degree to which a term is positive or negative."
"In this study, we conduct our experiments within two lan-guages, and we collect and use the following popu-lar and available Chinese and English sentiment lexicons [Footnote_6] , without any further filtering and labeling: 1) Chinese lexicons Positive_Dic cn : 3730 Chinese positive terms were collected from the Chinese Vocabulary for Sentiment Analysis (VSA) [URL_CITE] released by HOWNET."
"6 In this study, we focus on using a few popular resources in both Chinese and English for comparative study, instead of trying to collect and use all available resources."
Negative_Dic cn : 3116 Chinese negative terms were collected from Chinese Vocabulary for Sen-timent Analysis (VSA) released by HOWNET.
Negation_Dic cn : 13 negation terms were col-lected from related papers.
Intensifier_Dic cn : 148 intensifier terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET.
Positive_Dic en : 2718 English positive terms were collected from the feature file subjclueslen1-[REF_CITE].tff [URL_CITE] containing the subjectivity clues used in the work[REF_CITE].
The clues in this file were col-lected from a number of sources.
"Some were culled from manually developed resources, e.g. general inquirer [URL_CITE][REF_CITE]."
Others were identi-fied automatically using both annotated and unan-notated data.
A majority of the clues were collected as part of work reported[REF_CITE].
Negative_Dic en : 4910 English negative terms were collected from the same file described above.
Negation_Dic en : 88 negation terms were col-lected from the feature file valenceshifters.tff used in the work[REF_CITE].
Intensifier_Dic en : 244 intensifier terms were collected from the feature file intensifiers2.tff used in the work[REF_CITE].
"The semantic orientation value f kSO (rev k ) for rev k is computed by summing the polarity values of all words in the review, making use of both the word polarity defined in the positive and negative lexi-cons and the contextual valence shifters defined in the negation and intensifier lexicons."
The algo-rithm is illustrated in Figure 2.
"In the above algorithm, PosValue and Neg-Value are the polarity values for positive words and negative words respectively."
"We empirically set PosValue=1 and NegValue= –2 because nega-tive words usually contribute more to the overall semantic orientation of the review than positive words, according to our empirical analysis. ρ&gt;1 aims to intensify the polarity value and we simply set ρ=2. q is the parameter controlling the window size within which the negation terms and intensi-fier terms have influence on the polarity words and here q is set to 2 words."
"Note that the above pa-rameters are tuned only for Chinese sentiment analysis, and they are used for sentiment analysis in the English language without further tuning."
The tokenization of Chinese reviews involves Chinese word segmentation.
"Usually, if the semantic orientation value of a review is less than 0, the review is labeled as nega-tive, otherwise, the review is labeled as positive."
"After obtaining the set of semantic orientation val-ues F SO ={ f kSO (rev k ) | 0≤k≤p} by using the semantic oriented approach, where p is the number of Eng-lish translations for each Chinese review, we ex-ploit the following ensemble methods for deriving a new semantic orientation value f SOEnsemble (rev 0 ) : 1) Average"
It is the most intuitive combination method and the new value is the average of the values in F SO : p ∑ f SOk (rev k ) f SOEnsemble (rev 0 ) = k=0 p +1
"Note that after the new value of a review is ob-tained, the polarity tag of the review is assigned in the same way as described in Section 3.3."
"This combination method improves the average combination method by associating each individual value with a weight, indicating the relative confi-dence in the value. p f SOEnsemble (rev 0 ) = ∑ λ k f SOk (rev k ) k=0 where λ k ∈[0, 1] is the weight associated with f k"
SO (rev k ).
The weights can be set in the following two ways:
Weighting Scheme1: The weight of f kSO (rev k ) is set to the accuracy of the individual analysis in the kth language.
The weight of f kSO (rev k ) is set to be the maximal correlation coefficient be-tween the analysis results in the kth language and the analysis results in any other language.
"The underlying idea is that if the analysis results in one language are highly consistent with the analysis results in another language, the results are deemed to be more reliable."
"Given two lists of semantic values for all reviews, we use the Pearson’s corre-lation coefficient to measure the correlation be-tween them."
The weight associated with function f SO (rev k ) is then defined as the maximal Pearson’s k correlation coefficient between the reviews’ values in the kth language and the reviews’ values in any other language.
The new value is the maximum value in F SO : f SOEnsemble (rev 0 ) = max { f SOk (rev k ) | 0 ≤ k ≤ p } 4) Min
The new value is the minimum value in F SO : f SOEnsemble (rev 0 ) = min { f SOk (rev k ) | 0 ≤ k ≤ p } 5) Average Max&amp;Min
The new value is the average of the maximum value and the minimum value in F SO : f SOEnsemble (rev 0 ) = max { f (rev )|0 ≤ k ≤ p } + min { f (rev )|0 ≤ k ≤ p } k k k k SO
SO 2 6) Majority Voting
"This combination method relies on the final po-larity tags, instead of the semantic orientation val-ues."
A review can obtain p+1 polarity tags based on the individual analysis results in the p+1 lan-guages.
The polarity tag receiving more votes is chosen as the final polarity tag of the review.
"In order to assess the performance of the proposed approach, we collected 1000 product reviews from a popular Chinese IT product web site-[REF_CITE]."
"The reviews were posted by users and they focused on such products as mp3 players, mobile phones, digital camera and laptop computers."
Users usually selected for each review an icon indicating “pos-tive” or “negative”.
The reviews were first catego-rized into positive and negative classes according to the associated icon.
The polarity labels for the reviews were then checked by subjects.
"Finally, the dataset contained 886 product reviews with accu-rate polarity labels."
"We used the standard precision, recall and F-measure to measure the performance of positive and negative class, respectively, and used the Mac-roF measure and accuracy metric to measure the overall performance of the system."
The metrics are defined the same as in general text categorization.
"In this section, we investigate the following indi-vidual sentiment analysis results in each specified language:"
CN: This method uses only Chinese lexicons to analyze Chinese reviews;
This method uses only English lex-icons to analyze English reviews translated by GoogleTrans;
This method uses only English lex-icons to analyze English reviews translated by Ya-hooTrans;
DictEN: This method uses only English lexi-cons to analyze English reviews translated by DictTrans;
"In addition to the above methods for using English resources, the lexicon-based method inves-tigated[REF_CITE]can also use Eng-lish resources by directly projecting English lexicons into Chinese lexicons."
We use a large English-to-Chinese dictionary - LDC_EC_DIC2.0 [URL_CITE] with 110834 entries for pro-jecting English lexicons into Chinese lexicons via one-to-one translation.
"Based on the generated Chinese lexicons, two other individual methods are investigated in the experiments:"
This method uses only the generated Chinese Resources to analyze Chinese reviews.
This method combines the original Chi-nese lexicons and the generated Chinese lexicons and uses the extended lexicons to analyze Chinese reviews.
Table 1 provides the performance values of all the above individual methods.
"Seen from the table, the performances of GoogleEN and YahooEN are much better than the baseline CN method, and even the DictEN performs as well as CN."
The re-sults demonstrate that the use of English resources for sentiment analysis of translated English re-views is an effective way for Chinese sentiment analysis.
"We can also see that the English senti-ment analysis performance relies positively on the translation performance, and GoogleEN performs the best while DictEN performs the worst, which is consistent with the fact the GoogleTrans is deemed the best of the three machine translation systems, while DictTrans is the weakest one."
"Furthermore, the CN method outperforms the CN2 and CN3 methods, and the CN2 method per-forms the worst, which shows that the generated Chinese lexicons do not give any contributions to the performance of Chinese sentiment analysis."
We explain the results by the fact that the term-based one-to-one translation is inaccurate and the gener-ated Chinese lexicons are not reliable.
"Overall, the approach through cross-lingual lexicon translation does not work well for Chinese sentiment analysis in our experiments."
"In this section, we first use the simple average en-semble method to combine different individual analysis results."
Table 2 provides the performance values of the average ensemble results based on different individual methods.
"Seen from Tables 1 and 2, almost all of the av-erage ensembles outperforms the baseline CN method and the corresponding individual methods, which shows that each individual methods have their own evidences for sentiment analysis, and thus fusing the evidences together can improve performance."
"For the methods of CN+GoogleEN, CN+YahooEN and CN+DictEN, we can see the ensemble performance is not positively relying on the translation performance: CN+YahooEN per-forms better than CN+GoogleEN, and even CN+DictEN performs as well as CN+GoogleEN."
"The results show that the individual methods in the ensembles can complement each other, and even the combination of two weak individual methods can achieve good performance."
"However, the Dic-tEN method is not effective when the ensemble methods have already included GoogleEN and YahooEN."
"Overall, the performances of the en- semble methods rely on the performances of the most effective constituent individual methods: the methods including both GoogleEN and YahooEN perform much better than other methods, and CN+GoogleEN+YahooEN performs the best out of all the methods."
We further show the results of four typical av-erage ensembles by varying the combination weights.
"The combination weights are respectively specified as λ˙CN+(1-λ)˙GoogleEN, λ˙CN+(1- λ)˙YahooEN, λ˙CN+(1-λ)˙DictEN, λ 1 ˙CN+λ 2 ˙GoogleEN+(1-λ 1 -λ 2 )˙YahooEN ."
The results over the MacroF metric are shown in Figures 3 and 4 respectively.
We can see from the figures that GoogleEN and YahooEN are dominant factors in the ensemble methods.
"We then investigate to use other ensemble meth-ods introduced in Section 3.4 to combine the CN, GoogleEN and YahooEN methods."
Table 3 gives the comparison results.
"The methods of “Weighted Average1” and “Weighted Average2” are two weighted average ensembles using the two weigh-ing schemes, respectively."
"We can see that all the ensemble methods outperform the constituent indi-vidual method, while the two weighted average ensembles perform the best."
The results further demonstrate the good effectiveness of the ensem-ble combination of individual analysis results for Chinese sentiment analysis.
This paper proposes a novel approach to use Eng-lish sentiment resources for Chinese sentiment analysis by employing machine translation and ensemble techniques.
Chinese reviews are trans-lated into English reviews and the analysis results of both Chinese reviews and English reviews are combined to improve the overall accuracy.
Ex-perimental results demonstrate the encouraging performance of the proposed approach.
"In future work, more additional English re-sources will be used to further improve the results."
We will also apply the idea to supervised Chinese sentiment analysis.
"Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations."
We study both approaches under the framework of beam-search.
"By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods."
"More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for train-ing and decoding, showing that it outper-forms both the pure graph-based and the pure transition-based parsers."
"Testing on the En-glish and Chinese Penn Treebank data, the combined system gave state-of-the-art accura-cies of 92.1% and 86.2%, respectively."
Graph-based[REF_CITE]and transition-based[REF_CITE]parsing algorithms offer two dif-ferent approaches to data-driven dependency pars-ing.
"Given an input sentence, a graph-based algo-rithm finds the highest scoring parse tree from all possible outputs, scoring each complete tree, while a transition-based algorithm builds a parse by a se-quence of actions, scoring each action individually."
"The terms “graph-based” and “transition-based” were used[REF_CITE]to de-scribe the difference between MSTParser[REF_CITE], which is a graph-based parser with an exhaustive search decoder, and MaltParser[REF_CITE], which is a transition-based parser with a greedy search decoder."
"In this paper, we do not differentiate graph-based and transition-based parsers by their search algorithms: a graph-based parser can use an approximate decoder while a transition-based parser is not necessarily determin-istic."
"To make the concepts clear, we classify the two types of parser by the following two criteria: 1. whether or not the outputs are built by explicit transition-actions, such as ”Shift” and ”Reduce”; 2. whether it is dependency graphs or transition-actions that the parsing model assigns scores to."
"By this classification, beam-search can be applied to both graph-based and transition-based parsers."
"Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task[REF_CITE]."
"However, they make different types of errors, which can be seen as a reflection of their theoretical differ-ences[REF_CITE]."
"MSTParser has the strength of exact inference, but its choice of fea-tures is constrained by the requirement of efficient dynamic programming."
"MaltParser is deterministic, yet its comparatively larger feature range is an ad-vantage."
"By comparing the two, three interesting re-search questions arise: (1) how to increase the flex-ibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the two parsing approaches so that the strengths of each are utilized."
"In this paper, we study these questions under one framework: beam-search."
Beam-search has been successful in many NLP tasks ([REF_CITE];
"Inputs: training examples (x i , y i )"
Initialization: set w~ = 0
"Algorithm: // R training iterations; N examples for t = 1..R, i = 1..N: z i = arg max y∈GEN(x i )"
Φ(y) · w~ if z i 6= y i : w~ = w~ + Φ(y i ) − Φ(z i )
"Moreover, a beam-search decoder does not impose restrictions on the search problem in the way that an exact inference decoder typically does, such as requiring the “op-timal subproblem” property for dynamic program-ming, and therefore enables a comparatively wider range of features for a statistical system."
We develop three parsers.
"Firstly, using the same features as MSTParser, we develop a graph-based parser to examine the accuracy loss from beam-search compared to exact-search, and the accuracy gain from extra features that are hard to encode for exact inference."
Our conclusion is that beam-search is a competitive choice for graph-based pars-ing.
"Secondly, using the transition actions from MaltParser, we build a transition-based parser and show that search has a positive effect on its accuracy compared to deterministic parsing."
"Finally, we show that by using a beam-search decoder, we are able to combine graph-based and transition-based pars-ing into a single system, with the combined system significantly outperforming each individual system."
"In experiments with the English and Chinese Penn Treebank data, the combined parser gave 92.1% and 86.2% accuracy, respectively, which are comparable to the best parsing results for these data sets, while the Chinese accuracy outperforms the previous best reported by 1.8%."
"In line with previous work on de-pendency parsing using the Penn Treebank, we fo-cus on projective dependency parsing."
"Following MSTParser[REF_CITE], we define the graph-"
"Variables: agenda – the beam for state items item – partial parse tree output – a set of output items index, prev – word indexes"
Input: x – POS -tagged input sentence.
"Initialization: agenda = [“”] Algorithm: for index in 1..x.length(): clear output for item in agenda: // for all prev words that can be linked with // the current word at index prev = index − 1 while prev =6 0: // while prev is valid // add link making prev parent of index newitem = item // duplicate item newitem.link(prev, index) // modify output.append(newitem) // record // if prev does not have a parent word, // add link making index parent of prev if item.parent(prev) == 0: item.link(index, prev) // modify output.append(item) // record prev = the index of the first word before prev whose parent does not exist or is on its left; 0 if no match clear agenda put the best items from output to agenda Output: the best item in agenda based parsing problem as finding the highest scoring tree y from all possible outputs given an input x:"
"F (x) = arg max Score(y) y∈GEN(x) where GEN (x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the arg max to be part of the definition of graph-based parsing, only the fact that the dependency graph it-self is being scored, and factored into scores at-tached to the dependency links."
The score of an output parse y is given by a linear model:
Φ(y) · w~ where Φ(y) is the global feature vector from y and w~ is the weight vector of the model.
We use the discriminative perceptron learning al-gorithm[REF_CITE]to train the values of w~.
The algorithm is shown in Fig-ure 1.
"Averaging parameters is a way to reduce over-fitting for perceptron training[REF_CITE], and is applied to all our experiments."
"While the MSTParser uses exact-inference[REF_CITE], we apply beam-search to decoding."
This is done by extending the deterministic Covington algorithm for projective dependency parsing[REF_CITE].
"As shown in Figure 2, the decoder works incrementally, building a state item (i.e. par-tial parse tree) word by word."
"When each word is processed, links are added between the current word and its predecessors."
"Beam-search is applied by keeping the B best items in the agenda at each pro-cessing stage, while partial candidates are compared by scores from the graph-based model, according to partial graph up to the current word."
"Before decoding starts, the agenda contains an empty sentence."
"At each processing stage, existing partial candidates from the agenda are extended in all possible ways according to the Covington algo-rithm."
The top B newly generated candidates are then put to the agenda.
"After all input words are pro-cessed, the best candidate output from the agenda is taken as the final output."
The projectivity of the output dependency trees is guaranteed by the incremental Covington process.
"The time complexity of this algorithm is O(n 2 ), where n is the length of the input sentence."
"During training, the “early update” strategy[REF_CITE]is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item."
"The intuition is to improve learning by avoiding irrelevant informa-tion: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant be-cause the correct partial output no longer exists in the candidate ranking."
"Table 1 shows the feature templates from the MSTParser[REF_CITE], which are defined in terms of the context of a word, its parent and its sibling."
"To give more templates, fea-tures from templates 1 – 5 are also conjoined with the link direction and distance, while features from template 6 are also conjoined with the direction and distance between the child and its sibling."
Here “distance” refers to the difference between word in-dexes.
We apply all these feature templates to the graph-based parser.
"In addition, we define two extra feature templates (Table 2) that capture information about grandchildren and arity (i.e. the number of children to the left or right)."
These features are not conjoined with information about direction and dis-tance.
"They are difficult to include in an efficient dynamic programming decoder, but easy to include in a beam-search decoder."
"We develop our transition-based parser using the transition model of the MaltParser[REF_CITE], which is characterized by the use of a stack and four transition actions: Shift, ArcRight, ArcLeft and Reduce."
"An input sentence is processed from left to right, with an index maintained for the current word."
"Initially empty, the stack is used throughout the parsing process to store unfinished words, which are the words before the current word that may still be linked with the current or a future word."
The Shift action pushes the current word to the stack and moves the current index to the next word.
"The ArcRight action adds a dependency link from the stack top to the current word (i.e. the stack top becomes the parent of the current word), pushes the current word on to the stack, and moves the current index to the next word."
"The ArcLeft action adds a dependency link from the current word to the stack top, and pops the stack."
The Reduce action pops the stack.
"Among the four transition actions, Shift and ArcRight push a word on to the stack while ArcLeft and Reduce pop the stack; Shift and ArcRight read the next input word while ArcLeft and ArcRight add a link to the output."
"By repeated application of these actions, the parser reads through the input and builds a parse tree."
The MaltParser works deterministically.
"At each step, it makes a single decision and chooses one of the four transition actions according to the current context, including the next input words, the stack and the existing links."
"As illustrated in Figure 3, the contextual information consists of the top of stack ( ST ), the parent ( STP ) of ST , the leftmost ( STLC ) and rightmost child ( STRC ) of ST , the current word ( N0 ), the next three words from the input ( N1, N2, N3 ) and the leftmost child of N0 ( N0LC )."
"Given the context s, the next action T is decided as follows:"
"T (s) = arg max Score(T, s) T∈ACTION where ACTION = {Shift, ArcRight, ArcLeft, Reduce}."
"One drawback of deterministic parsing is error propagation, since once an incorrect action is made, the output parse will be incorrect regardless of the subsequent actions."
"To reduce such error propa-gation, a parser can keep track of multiple candi-date outputs and avoid making decisions too early."
"Suppose that the parser builds a set of candidates GEN (x) for the input x, the best output F(x) can be decided by considering all actions:"
F (x) = arg max P T 0 ∈act(y)
"Score(T 0 , s T 0 ) y∈GEN(x)"
"Here T 0 represents one action in the sequence (act(y)) by which y is built, and s T 0 represents the corresponding context when T 0 is taken."
"Our transition-based algorithm keeps B different sequences of actions in the agenda, and chooses the one having the overall best score as the final parse."
Pseudo code for the decoding algorithm is shown in Figure 4.
"Here each state item contains a partial parse tree as well as a stack configuration, and state items are built incrementally by transition actions."
"Initially the stack is empty, and the agenda contains an empty sentence."
"At each processing stage, one transition action is applied to existing state items as a step to build the final parse."
"Unlike the MaltParser, which makes a decision at each stage, our transition-based parser applies all possible actions to each ex-isting state item in the agenda to generate new items; then from all the newly generated items, it takes the B with the highest overall score and puts them onto the agenda."
"In this way, some ambiguity is retained for future resolution."
Note that the number of transition actions needed to build different parse trees can vary.
"For exam-ple, the three-word sentence “A B C” can be parsed by the sequence of three actions “Shift ArcRight ArcRight” (B modifies A; C modifies B) or the sequence of four actions “Shift ArcLeft Shift Ar-cRight” (both A and C modifies B)."
"To ensure that all final state items are built by the same number of transition actions, we require that the final state"
"Variables: agenda – the beam for state items item – (partial tree, stack config) output – a set of output items index – iteration index"
Input: x – POS -tagged input sentence.
"Initialization: agenda = [(“”, [])]"
"Algorithm: for index in 1 .. 2 × x.length() −1: clear output for item in agenda: // when all input words have been read, the // parse tree has been built; only pop. if item.length() == x.length(): else: if item.lastaction() =6 Reduce: newitem = item 5 POS bigram newitem."
Shift() 6 POS trigrams output.append(newitem) if item.stacksize() &gt; 0: newitem = item 7 N0 word newitem.
ArcRight() output.append(newitem) if (item.parent(item.stacktop())==0): newitem = item newitem.
ArcLeft() output.append(newitem) else: newitem = item newitem.
Reduce() output.append(newitem) items must 1) have fully-built parse trees; and 2) have only one root word left on the stack.
"In this way, popping actions should be made even after a complete parse tree is built, if the stack still contains more than one word."
"Now because each word excluding the root must be pushed to the stack once and popped off once during the parsing process, the number of actions needed to parse a sentence is always 2n − 1, where n is the length of the sentence."
"Therefore, the de-coder has linear time complexity, given a fixed beam size."
"Because the same transition actions as the MaltParser are used to build each item, the projec-tivity of the output dependency tree is ensured."
"We use a linear model to score each transition ac-tion, given a context:"
"Score(T, s) = Φ(T, s) · w~"
"Φ(T, s) is the feature vector extracted from the ac-tion T and the context s, and w~ is the weight vec-tor."
"Features are extracted according to the templates shown in Table 3, which are based on the context in Figure 3."
"Note that our feature definitions are sim-ilar to those used by MaltParser, but rather than us-ing a kernel function with simple features (e.g. STw ,"
"N0t , but not STwt or STwN0w ), we combine features manually."
"As with the graph-based parser, we use the dis-criminative perceptr[REF_CITE]to train the transition-based model (see Figure 5)."
"It is worth noticing that, in contrast to MaltParser, which trains each action decision individually, our training algo-rithm globally optimizes all action decisions for a parse."
"Again, “early update” and averaging parame-ters are applied to the training process."
The graph-based and transition-based approaches adopt very different views of dependency parsing.
"This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved."
The beam-search framework we have developed facilitates such a combination.
Our graph-based and transition-based parsers share many similarities.
"Both build a parse tree incrementally, keeping an agenda of comparable state items."
"Both rank state items by their current scores, and use the averaged perceptron with early update for training."
"The key differences are the scoring models and incremental parsing processes they use, which must be addressed when combining the parsers."
"Firstly, we combine the graph-based and the transition-based score models simply by summation."
This is possible because both models are global and linear.
"In particular, the transition-based model can be written as:"
Score T (y) =
P T 0 ∈act(y)
"Score(T 0 , s T 0 ) ="
"P T 0 ∈act(y) Φ(T 0 , s T 0 ) · w~ T = w~ T · P T 0 ∈act(y) Φ(T 0 , s T 0 )"
"If we take P T 0 ∈act(y) Φ(T 0 , s T 0 ) as the global fea-ture vector Φ T (y), we have:"
Score T (y) =
Φ T (y) · w~ T which has the same form as the graph-based model:
Score G (y) =
Φ G (y) · w~ G
We therefore combine the two models to give:
Score C (y) =
Score G (y) +
Score T (y) =
Φ G (y) · w~ G + Φ T (y) · w~ T
"Concatenating the feature vectors Φ G (y) and Φ T (y) to give a global feature vector Φ C (y), and the weight vectors w~ G and w~ T to give a weight vector w~ C , the combined model can be written as:"
Score C (y) =
"Φ C (y) · w~ C which is a linear model with exactly the same form as both sub-models, and can be trained with the per-ceptron algorithm in Figure 1."
"Because the global feature vectors from the sub models are concate-nated, the feature set for the combined model is the union of the sub model feature sets."
"Second, the transition-based decoder can be used for the combined system."
Both the graph-based de-coder in Figure 2 and the transition-based decoder in Figure 4 construct a parse tree incrementally.
"How-ever, the graph-based decoder works on a per-word basis, adding links without using transition actions, and so is not appropriate for the combined model."
"The transition-based algorithm, on the other hand, uses state items which contain partial parse trees, and so provides all the information needed by the graph-based parser (i.e. dependency graphs), and hence the combined system."
"In summary, we build the combined parser by using a global linear model, the union of feature templates and the decoder from the transition-based parser."
We evaluate the parsers using the English and Chi-nese Penn Treebank corpora.
The English data is prepared by following[REF_CITE].
"Bracketed sentences from the Penn Treebank ( PTB ) 3 are split into training, development and test sets as shown in Table 4, and then translated into depen-dency structures using the head-finding rules[REF_CITE]."
"Before parsing, POS tags are assigned to the in-put sentence using our reimplementation of the POS -tagger[REF_CITE]."
"Since the beam size affects all three parsers, we study its influence first; here we show the effect on the transition-based parser."
"Figure 6 shows different accuracy curves using the development data, each with a different beam size B. The X-axis represents the number of training iterations, and the Y-axis the precision of lexical heads."
"The parsing accuracy generally increases as the beam size increases, while the quantity of increase becomes very small when B becomes large enough."
"The decoding times after the first training iteration are 10.2s, 27.3s, 45.5s, 79.0s, 145.4s, 261.3s and 469.5s, respectively, when B = 1, 2, 4, 8, 16, 32, 64."
"In the rest of the experiments, we set B = 64 in order to obtain the highest possible accuracy."
"When B = 1, the transition-based parser be-comes a deterministic parser."
"By comparing the curves when B = 1 and B = 2, we can see that, while the use of search reduces the parsing speed, it improves the quality of the output parses."
"Therefore, beam-search is a reasonable choice for transition-based parsing."
"The test accuracies are shown in Table 5, where each row represents a parsing model."
"Rows “MSTParser 1/2” show the first-order (using feature templates 1 – 5 from Table 1)[REF_CITE]and second-order (using all feature templates from Table 1)[REF_CITE]MSTParsers, as re-ported by the corresponding papers."
"Rows “Graph [M]” and “Graph [MA]” represent our graph-based parser using features from Table 1 and Table 1 + Ta-ble 2, respectively; row “Transition” represents our transition-based parser; and rows “Combined [TM]” and “Combined [TMA]” represent our combined parser using features from Table 3 +"
"Table 1 and Ta-ble 3 + Table 1 + Table 2, respectively."
"Columns “Word” and “Complete” show the precision of lexi-cal heads and complete matches, respectively."
"As can be seen from the table, beam-search re-duced the head word accuracy from 91.5%/42.1% (“MSTParser 2”) to 91.2%/40.8% (“Graph [M]”) with the same features as exact-inference."
"How-ever, with only two extra feature templates from Table 2, which are not conjoined with direction or distance information, the accuracy is improved to 91.4%/42.5% (“Graph [MA]”)."
"This improvement can be seen as a benefit of beam-search, which al-lows the definition of more global features."
The combined parser is tested with various sets of features.
"Using only graph-based features in Ta-ble 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (“Graph [M]”)."
This can be explained by the difference between the decoders.
"In particu-lar, the graph-based model is unable to score the ac-tions “Reduce” and “Shift”, since they do not mod-ify the parse tree."
"Nevertheless, the score serves as a reference for the effect of additional features in the combined parser."
"Using both transition-based features and graph-based features from the MSTParser (“Combined [TM]”), the combined parser achieved 92.0% per-word accuracy, which is significantly higher than the pure graph-based and transition-based parsers."
"Ad-ditional graph-based features further improved the accuracy to 92.1%/45.5%, which is the best among all the parsers compared. [Footnote_1]"
"1 A recent paper,[REF_CITE]reported parent-prediction accuracy of 92.0% using a graph-based parser with a different (larger) set of features[REF_CITE]. By applying separate word cluster information,[REF_CITE]improved the accu-racy to 93.2%, which is the best known accuracy on the PTB data. We excluded these from Table 5 because our work is not concerned with the use of such additional knowledge."
We use the Penn Chinese Treebank ( CTB ) 5 for ex-perimental data.
"Most of the head-finding rules are[REF_CITE], while we added rules to handle NN and FRAG, and a default rule to use the rightmost node as the head for the constituent that are not listed."
"The parsing accuracy is eval-uated by the percentage of non-root words that have been assigned the correct head, the percentage of correctly identified root words, and the percentage of complete matches, all excluding punctuation."
The accuracies are shown in Table 7.
"Rows “Graph [MA]”, “Transition”, “Combined [TM]” and “Combined [TMA]” show our models in the same way as for the English experiments from Section 5.2."
"Row “[REF_CITE]” represents the transition-based model[REF_CITE], which applies beam-search to the deterministic model[REF_CITE], and achieved the previous best accuracy on the data."
Our observations on parsing Chinese are essen-tially the same as for English.
Our combined parser outperforms both the pure graph-based and the pure transition-based parsers.
It gave the best accuracy we are aware of for dependency parsing using CTB .
Our graph-based parser is derived from the work[REF_CITE].
"Instead of per-forming exact inference by dynamic programming, we incorporated the linear model and feature tem-plates[REF_CITE]into our beam-search framework, while adding new global features."
Our transition-based parser is derived from the deterministic parser[REF_CITE].
"We incorporated the transition process into our beam-search framework, in order to study the influence of search on this algorithm."
"Existing efforts to add search to deterministic parsing include[REF_CITE], which applied best-first search to constituent parsing, and[REF_CITE]and[REF_CITE], which applied beam-search to dependency parsing."
"All three methods es-timate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions."
"But different from our transition-based parser, which trains all transi-tions for a parse globally, these models train the probability of each action separately."
"Based on the work[REF_CITE],[REF_CITE]studied global training with an approximated large-margin algorithm."
"This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy."
Our combined parser makes the biggest contribu-tion of this paper.
"In contrast to the models above, it includes both graph-based and transition-based components."
"An existing method to combine mul-tiple parsing algorithms is the ensemble approach[REF_CITE], which was reported to be useful in improving dependency parsing[REF_CITE]."
A more recent approach[REF_CITE]combined MSTParser and MaltParser by using the output of one parser for features in the other.
"In contrast, our parser com-bines two components in a single model, in which all parameters are trained consistently."
"We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches."
"We then com-bined the two parsers into a single system, using dis-criminative perceptron training and beam-search de-coding."
"The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy."
"When tested using both English and Chinese dependency data, the combined parser was highly competitive com-pared to the best systems in the literature."
"The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other NLP tasks."
Syntactic word reordering is essential for translations across different grammar struc-tures between syntactically distant language-pairs.
"In this paper, we propose to em-bed local and non-local word reordering de-cisions in a synchronous context free gram-mar, and leverages the grammar in a chart-based decoder."
"Local word-reordering is ef-fectively encoded in Hiero-like rules; whereas non-local word-reordering, which allows for long-range movements of syntactic chunks, is represented in tree-based reordering rules, which contain variables correspond to source-side syntactic constituents."
We demonstrate how these rules are learned from parallel cor-pora.
Our proposed shallow Tree-to-String rules show significant improvements in trans-lation quality across different test sets.
One of the main issues that a translator (human or machine) must address during the translation pro-cess is how to match the different word orders be-tween the source language and the target language.
Different language-pairs require different levels of word reordering.
"For example, when we translate between English and Spanish (or other Romance languages), most of the word reordering needed is local because of the shared syntactical features (e.g., Spanish noun modifier constructs are written in English as modifier noun)."
"However, for syn-tactically distant language-pairs such as Chinese-English, long-range reordering is required where whole phrases are moved across the sentence."
"The idea of “syntactic cohesion”[REF_CITE]is characterized by its simplicity, which has attracted researchers for years."
Previous works include sev-eral approaches of incorporating syntactic informa-tion to preprocess the source sentences to make them more like the target language in structure.
"Later,[REF_CITE]presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English."
"Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple."
"The eleven rules described[REF_CITE]are ap-pealing, as they have rather simple structure, mod-eling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree (a special case of ITG[REF_CITE])."
It effectively en-hances the quality of the phrase-based translation of Chinese-to-English.
"One major weakness is that the reordering decisions were done in the preprocessing step, therefore rendering the decoding process un-able to recover the reordering errors from the rules if incorrectly applied to."
"Also the reordering decisions are made without the benefits of additional models (e.g., the language models) that are typically used during decoding."
"Another method to address the re-ordering prob-lem in translation is the Hiero model proposed[REF_CITE], in which a probabilistic synchronous context free grammar (PSCFG) was applied to guide the decoding."
"Hiero rules generalize phrase-pairs by introducing a single generic nonterminal (i.e., a variable) [X]."
The combination of variables and lex-icalized words in a Hiero rule nicely captures local word and phrase reordering (modeling an implicit reordering window of max-phrase length).
These rules are then applied in a CYK-style decoder.
"In Hiero rules, any nested phrase-pair can be general-ized as variables [X]."
"This usually leads to too many redundant translations, which worsens the spurious ambiguities[REF_CITE]problems for both de-coding and optimization (i.e., parameter tuning)."
We found thatvariables (nonterminal [X]) in Hiero rules offer a generalization too coarse to improve the ef-fectiveness of hierarchical models’ performance.
"We propose to enrich the variables in Hiero rules with additional source syntactic reordering informa-tion, in the form of shallow Tree-to-String syntactic structures."
"The syntactic information is represented by flat one-level sub-tree structures, with Hiero-like nonterminal variables at the leaf nodes."
"The syntac-tic rules, proposed in this paper, are composed of (possibly lexicalized) source treelets and target sur-face strings, with one or more variables that help capture local-reordering similar to the Hiero rules."
"Variables in a given rule are derived not only from the embedded aligned blocks (phrase-pairs), but also from the aligned source syntactic constituents."
"The aligned constituents, as in our empirical observa-tions for Chinese-English, tend to move together in translations."
The decoder is guided by these rules to reduce spurious derivations; the rules also constrain the exploration of the search space toward better translation quality and sometime improved speed by breaking long sentences into pieces.
"Overall, what we want is to enable the long-range reordering deci-sions to be local in a chart-based decoder."
"To be more specific, we think the simple shal-low syntactic structure is powerful enough for cap-turing the major structure-reordering patterns, such as NP, VP and LCP structures."
"We also use sim-ple frequency-based feature functions, similar to the blocks used in phrase-based decoder, to further im-prove the rules’ representation power."
"Overall, this enables us to avoid either a complex decoding pro-cess to generate the source parse tree, or difficult combinatorial optimizations for the feature func-tions associated with rules."
The combinatorial effects of the added feature functions can make the feature se-lection and optimization of the weights rather dif-ficult.
"Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work."
Word reordering can also be addressed via distortion models.
Work[REF_CITE]modeled the limited information available at phrase-boundaries.
Syntax-based ap-proaches such[REF_CITE]heavily rely on the parse-tree to constrain the search space by as-suming a strong mapping of structures across distant language-pairs.
"Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality."
"From our em-pirical observations, most of the time, however, the multi-level tree-structure is broken in the translation process, and POS tags are frequently distorted."
"In-deed, strictly following the source parse tree is usu-ally not necessary, and maybe too expensive for the translation process."
"The remainder of this paper is structured as fol-lows: in section § 2, we define the notations in our synchronous context free grammar, in section § 3, the rule extractions are illustrated in details, in sec-tion § 4, the decoding process of applying these rules is described."
Experiments in § 5 were carried out using[REF_CITE]datasets.
Improved translation qualities were obtained by applying the proposed Tree-to-String rules.
Conclusions and discussions are given in § 6.
Our proposed rules are in the form of probabilis-tic synchronous context free grammar (PSCFG).
We adopt the notations used[REF_CITE].
"Let N be a set of nonterminals, a rule has the following form:"
"X →&lt; `; γ; α; ∼; w̄ &gt;, (1) where X abstracts nonterminal symbols in N; γ ∈ [N,V S ] + is a sequence of one or more source [Footnote_1] words (as in the vocabulary of V S ) and nonterminal symbols in N; α ∈ [N, V T ] + is a sequence of one or more target words (in V T ) and nonterminals in N . ∼ is the one-to-one alignment of the nonterminals between γ and α; w̄ contains non-negative weights associated with each rule; ` is a label-symbol speci-fying the root node of the source span covering γ."
1 we use end-user terminologies for source and target.
"In our grammar, ` is one of the labels (e.g., NP) defined in the source treebank tagset (in our case UPenn Chinese tagset) indicating that the source span γ is rooted at `."
"Additionally, a NULL tag Ø in ` denotes a flat structure of γ, in which no constituent structure was found to cover the span, and we need to back off to the normal Hiero-style rules."
Our nonterminal symbols include the labels and the POS tags in the source parse trees.
"In the following, we will illustrate the Tree-to- String rules we are proposing."
"At the same time, we will describe the extraction algorithm, with which we derive our rules from the word-aligned source-parsed parallel text."
Our nonterminal set N is a re-duced set of the treebank tagset[REF_CITE].
It consists of 17 unique labels.
"The rules we extract belong to one of the follow-ing categories: • γ contains only words, and ` is NULL; this cor-responds to the general blocks used in phrase-based decoder[REF_CITE]; • γ contains words and variables of [X,0] and [X,1], and ` is NULL; this corresponds to the Hiero rules as[REF_CITE]; • γ contains words and variables in the form of [X,TAG [Footnote_2] ], in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams."
2 we index the tags for multiple occurrences in one rule
"If γ contains only variables from LDC tag set, this indicates all the constituents (children) in the subtree are aligned."
This is a superset of rules generalizing those[REF_CITE].
"If γ contains vari-ables from POS tags, this essentially produces a superset of the monolingual side POS-based reordering rules explored[REF_CITE]."
"We focus on the third category — a syntactic label ` over the span of γ, indicating the covered source words consist of a linguistically well-defined phrase. ` together with γ define a tree-like structure: the root node is `, and the aligned children are nonterminals in γ."
"The structure information is encoded in (`, γ) pair-wise connections, and the variables keep the generalizations over atomic translation-pairs similar to Hiero models."
"When the rule is applied during decoding time, the labels, the tree-structure and the lexical items need to be all matched."
A parser is assumed for the source language in the parallel data.
"In our case, a Chinese parser is applied for training and test data."
A word alignment model is used to align the source words with the target words.
Our rule extraction is a three-step process.
"First, tra-ditional blocks (phrase-pairs) extraction is carried out."
"Secondly, Tree-to-String rules, are then ex-tracted from the aligned blocks, of which the source side is covered by a complete subtree, with different permutations of the embedded aligned constituents, or partially lexicalized constituents."
"Otherwise, the Hiero-like rules will be extracted when there is no sub-tree structure identified, in our final step."
Fre-quencies of extracted rules were counted to compute feature functions.
Figure 1-(a) shows that a subtree (with root at VP) is aligned to the English string.
"Considering the huge quantity of all the permutations of the aligned constituents under the tree, only part of the Tree-to- String rules extracted are shown in Figure 1-(c)."
The variables incorporate linguistic information in the assigned tag by the parser.
"When there is no aligned constituent for further generalization, the variables, defined in our grammar, back off to the Hiero-like ones without any label-identity information."
"One such example is in the rule “在 [X,0] 前 [X,VP] → [X,VP] before the [X,0] ”, in which the Hiero-style variable [X,0] and the label-based variable [X,VP] co-exist in our proposed rule."
We illustrate several special cases of our extracted Tree-to-String rules in the following.
"We index the variables with their positions to indicate the align-ment ∼, and skip the feature function w̄ to simplify the notations."
"X →&lt; [X, IP]; [X, NP0] [X, V P0]; (2) [X, NP0] is [X, V P0] &gt; ."
"The rule in Eqn. 2 shows that a source tree rooted at IP, with two children of NP and VP generalized into variables [X,NP] and [X,VP]; they are rewritten into “[X,NP] is [X,VP]”, with the spontaneous word is inserted."
"Such rules are not allowed in Hiero-style models, as there is no lexical item between the two variables[REF_CITE]in the source side."
This rule will generate a spontaneous word “is” from the given subtree structure.
"Usually, it is very hard to align the spontaneous word correctly, and the rules we proposed indicate that spontaneous words are generated directly from the source sub-tree struc-ture, and they might not necessarily get aligned to some particular source words."
"A second example is shown in Eqn. 3, which is similar to the Hiero rules:"
"X →&lt; Ø; [X, 0] zhiyi; (3) one of the [X, 0] &gt; ."
"The rule in Eqn. 3 shows that when there is no linguistically-motivated root covering the span, ([X,NULL] is then assigned), we simply back off to the Hiero rules."
"In this case, the source span of [X,0] zhiyi is rewritten into the target “one of the [X, 0]”, without considering the map- ping of the root of the span."
"In this way, the repre-sentation power is kept in the variables in our rules, even if the source subtree is aligned to a discontin-uous sequence on the target side."
"This is important for Chinese-to-English, because the grammar struc-ture is so different that more than 40% of the subtree structures were not kept during the translation in our study on hand-aligned data."
Following strictly the source side syntax will derail from these informative translation patterns.
"X →&lt; [X, NP]; [X, NN1][X, NN2][X, NN3]; [X, NN3][X, NN1][X, NN2] &gt; . (4)"
Eqn. 4. is a POS-based rule — a special case in our proposed rules.
This rule shows the reorder-ing patterns for three adjacent NN’s.
"POS based rules can be very informative for some language-pairs such as Arabic-to-English, where the ADJ is usually moved before NN during the translations."
"As also shown in Eqn. 4 for POS sequences, in the UPenn treebank-style parse trees, a root usually have more than two variables."
"Our rule set for subtree, therefore, contain more than two variables: “X →&lt; [X, IP ]; [X, ADV P 0][X, NP 0][X, V P 0]; [X, NP 0] [X, ADV P 0][X, V P 0] &gt;”."
A CYK-style decoder has to rely on binarization to preprocess the grammar as did[REF_CITE]to handle multi-nonterminal rules.
"We adopt the so-called dotted-rule or dotted-production, similar to the Early-style algorithm[REF_CITE], to handle the multi-nonterminal rules in our chart-based decoder."
"As used in most of the SMT decoders for a phrase-pair, a set of standard feature functions are applied in our decoder, including IBM Model-1 like scores in both directions, relative frequencies in both direc-tions."
"In addition to these features, a counter is as-sociated to each rule to collect how many rules were applied so far to generate a hypothesis."
The stan-dard Minimum Error Rate training[REF_CITE]was applied to tune the weights for all feature types.
The number of extracted rules from the GALE data is generally large.
"We pruned the rules accord-ing to their frequencies, and only keep at most the top-50 frequent candidates for each source side."
"Given the source sentence, with constituent parse-trees, the decoder is to find the best derivation D ∗ which yield the English string e ∗ : e ∗ = arg max {φ(D)φ(e)φ(f|e)}, (5) D ∗ where φ(D) is the cost for each of the derivations that lead to e from a given source-parsed f; φ(e) is for cost functions from the standard n-gram lan-guage models; φ(f|e) is the cost for the standard translation models, including general blocks."
"We separate the costs for normal blocks and the general-ized rules explicitly here, because the blocks contain stronger lexical evidences observed directly from data, and we assign them with less cost penalties via a different weight factor visible for optimization, and prefer the lexical match over the derived paths during the decoding."
Our decoder is a chart-based parser with beam-search for each cell in a chart.
"Because the tree-structure can have more than two children, there-fore, the Tree-to-String rules extracted usually con-tain more than two variables."
"Slightly different from the decoder[REF_CITE], we implemented the dotted-rule in Early-style parser to handle rules containing more than two variables."
"Our cube-expansion, implemented the cube-pruning[REF_CITE], and integrated piece-wise cost computations for language models via LM states."
The intermedi-ate hypotheses were merged (recombined) accord-ing to their LM states and other cost model states.
We use MER[REF_CITE]to tune the decoder’s pa-rameters using a development data set.
Figure 2 shows an example of a tree-based rule fired at the subtree of VP covering the highlighted cell.
"When a rule is applied at a certain cell in the chart, the covered source ngram should match not only the lexical items in the rules, but also the tree-structures as well."
The two children under the sub-tree root VP are PP (“在当地”: in the local) and VP (“引发巨大震动”: triggered a huge shock ).
This rule triggered a swap of these children to generate the correct word order in the translation: “triggered a huge shock in the local”.
Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC hand-aligned corpus 1 .
The Chinese side of these two cor-pora were parsed using a constituency parser[REF_CITE].
The average labeled F-measure of the parser is 81.4%.
Parallel sentences were first word-aligned using a MaxEnt aligner[REF_CITE].
"Then, phrase-pairs that overlap with our develop-ment and test set were extracted from the word alignments (from both hand alignments and auto-matically aligned GALE corpora) based on the pro-jection principle[REF_CITE]."
"Besides the regu-lar phrase-pairs, we also extracted the Tree-to-String rules from the two corpora."
The detailed statistics are shown in Table 1.
Our re-implementation of Hi-ero system is the baseline.
"We integrated the eleven reordering rules described[REF_CITE], in our chart-based decoder."
"In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automat-ically aligned training data."
We also report the result of our translation quality in terms of both BLEU[REF_CITE]and TER[REF_CITE]against four human reference translations.
"Table 1 shows the statistics of our training, develop-ment and test data."
"As our word aligner[REF_CITE]can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set “[REF_CITE]K”, which consists of 16K sentence-pairs, to get relatively clean rules, free from align-ment errors."
"A much larger GALE data set, which consists of 10 million sentence-pairs, is used to in-vestigate the scalability of our proposed approach."
The text part[REF_CITE]Chinese-to-English Development set[REF_CITE]is used as our test set.
Both have four human reference trans-lations.
"From the hand-aligned data, the rules we extracted fall into three categories: regular blocks (phrase-pairs), Hiero-like rules, and Tree-to-String rules."
The statistics of the extracted rules are shown in Ta-ble 2
We focus on Tree-to-String rules.
Table 3 shows the detailed statistics of the Tree-to-String rules ex-tracted from the Chinese-to-English hand-aligned training data.
The following section provides a de-tailed analysis of the most frequent subtrees ob-served in our training data.
"The majority of Tree-to-String rules we extracted are rooted at the following labels: NP (46%), VP(22.8%), DNP (2.23%), and QP(2.94%)."
They apply these rules as a pre-processing step to reorder the input sentences with hard decisions.
"Our proposed Tree-to-String rules, on the contrary, are applied during the de-coding process which allows for considering many possible competing reordering options for the given sentences, and the decoder will choose the best one according to the cost functions."
Table 4 shows the statistics of reordering rules for subtrees rooted at VP.
The statistics suggest that it is impossible to come up with a reordering rule that is always applicable.
"For instance,[REF_CITE]will always swap the children of the sub-tree VP(PP,VP)."
"However, the statistics shown in Ta-ble 4 suggest that might not be best way."
"In fact, due to parser’s performance and word alignment ac- curacies, the statistics we collected from the GALE dataset, containing 10 million sentence-pairs, show that the children in the subtree VP(PP,VP) is trans-lated monotonically 126310 times, while reordered of only 22144 times."
"However, the hand-aligned data support the swap for 1245 times, and monotoni-cally for only 168 times."
"Part of this disagreement is due to the word segmentation errors, incorrect word alignments and unreliable parsing results."
Another observations through our extracted Tree-to-String rules is on the controlled insertion of the target spontaneous 2 (function) words.
"Instead of hy-pothesizing spontaneous words based only on the language model or only on observing in phrase-pairs, we make use of the Tree-to-String rules to get suggestion on the insertion of spontaneous words."
"In this way, we can make sure that the spontaneous words are generated from the structure information, as opposed to those from a pure hypothesis."
The ad-vantage of this method is shown in Table 4.
"For in-stance, the word “that” and the punctuation “,” were generated in the target side of the rule."
This proves that our model can provide a more principled way to generate spontaneous words needed for fluent trans-lations.
An interesting linguistic phenomenon that we in-vestigated is the Chinese word DE “的”. “的” is an informative lexical clue that indicates the need for long range phrasal movements.
Table 5 shows a few high-frequent reordering rules that contain the Chi-nese word “DE”.
The three type of rules handle “DE” differently.
A major difference is the structure in the source side.
"Hiero rules do not consider any structure, and ap-ply the rule of “[X,0] 的 [X,1]”."
"Tree-based rules, as described[REF_CITE]do not handle 的 directly; they are often implicitly taken care of when reordering DNPs instead."
"Our proposed Tree-to-String rules model 的 directly in a subtree con-taining DEG/DEC, which triggers word reordering within the structure."
"Our rule set includes all the above three rule-types with the associated frequen-cies, this enriched the reordering choices to be cho-sen by the chart-based decoder, guided by the statis-tics collected from the data and the language model costs."
"We tuned the decoding parameters using the[REF_CITE]data set, and applied the updated parameters to the GALE evaluation set."
"The eleven rules of VP, NP, and LCP (tree-based) improved the Hiero baseline [Footnote_3] from 32.43 to 33.02 on BLEU."
3 Hiero results are from our own re-implementation.
"The reason, the tree-reordering does not gain much over Hiero baseline, is probably that the reordering patterns covered by tree-reordering rules, are potentially handled in the standard Hiero grammar."
"A small but noticeable further improvement over tree-based rules, from 33.02 to 33.26, was ob-tained on applying Tree-to-String rules extracted from hand-aligned dataset."
"We think that the Tree-based rules covers major reordering patterns for Chinese-English, and our hand-aligned dataset is also too small to capture representative statistics and more reordering patterns."
A close check at the rules we learned from the hand-aligned data shows that the tree-based rules are simply the subset of the rules extracted.
"The Tree-to-String grammar im-proved the Hiero baseline from 32.43 to 33.26 on BLEU; considering the effects from the tree-based rules only, the additional information improved the BLEU scores from 33.02 to 33.26."
Similar pictures of improvements were observed for the two unseen tests of newswire and weblog in GALE data.
"When applying the rules extracted from the much larger GALE training set with about ten million sentence-pairs, we achieved significant improve-ments from both genres (newswire and web data)."
The improvements are significant in both BLEU and TER.
"BLEU improved from 32.44 to 33.51 on newswire, and from 25.88 to 27.91 on web data."
Similar improvements were found in TER as shown in the table.
"The gain came mostly from the richer extracted rule set, which not only presents robust statistics for reordering patterns, but also offers more target spontaneous words generated from the syntac-tic structures."
"Since the top-frequent rules extracted are NP, VP, and IP as shown in Table 3, our proposed rules will be able to win the correct word order with reliable statistics, as long as the parser shows accept-able performances on these structures."
"This is espe-cially important for weblog data, where the parser’s overall accuracy potentially might not be very good."
Table 7 shows the translations from different grammars for the same source sentence.
"Both Tree-based and Tree-to-String methods get the correct re-ordering, while the latter can suggest insertions of target spontaneous words like “a” to allow the trans-lation to run more fluently."
"In this paper, we proposed our approach to model both local and non-local word-reordering in one probabilistic synchronous CFG."
"Our current model incorporates source-side syntactic information, to model the observations that the source syntactic con-stituent tends to move together during translations."
"The proposed rule set generalizes over the variables in Hiero-rules, and we also showed the special cases of the Tree-based rules and the POS-based rules."
"Since the proposed rules has at most one-level tree structure, they can be easily applied in a chart-based decoder."
"We analyzed the statistics of our rules, qualitatively and quantitatively."
"Next, we compared our work with other research, especially with the work[REF_CITE]."
"Finally, we reported our empirical results on Chinese-English transla-tions."
Our Tree-to-String rules showed significant improvements over the Hiero baseline on the[REF_CITE]test set.
"Given the low accuracy of the parsers, and the po-tential errors from Chinese word-segmentations, and word-alignments, our rules learned are still noisy."
Exploring better cost functions associate each rule might lead to further improvement.
"Because of the relative high accuracy of English parsers, many works such[REF_CITE]and[REF_CITE]emphasize on using syntax in tar-get languages, to directly influence the fluency as-pect of the translation output."
"In future, we plan to incorporate features from target-side syntactic infor-mation, and connect them with the source informa-tion explored in this paper, to model long-distance reordering for better translation quality."
We present a graph-based semi-supervised la-bel propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and struc-tured text sources.
"This acquisition method significantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving com-parable precision."
"Users of large document collections can readily ac-quire information about the instances, classes, and relationships described in the documents."
"Such rela-tions play an important role in both natural language understanding and Web search, as illustrated by their prominence in both Web documents and among the search queries submitted most frequently by Web users[REF_CITE]."
These observations moti-vate our work on algorithms to extract instance-class information from Web documents.
"While work on named-entity recognition tradi-tionally focuses on the acquisition and identifica-tion of instances within a small set of coarse-grained classes, the distribution of instances within query logs indicates that Web search users are interested in a wider range of more fine-grained classes."
"De-pending on prior knowledge, personal interests and immediate needs, users submit for example medi-cal queries about the symptoms of leptospirosis or ∗ Contributions made during internships at Google. the treatment of monkeypox, both of which are in-stances of zoonotic diseases, or the risks and benefits of surgical procedures such as PRK and angioplasty."
"Other users may be more interested in African coun-tries such as Uganda and Angola, or active volca-noes like Etna and Kilauea."
"Note that zoonotic dis-eases, surgical procedures, African countries and active volcanoes serve as useful class labels that cap-ture the semantics of the associated sets of class in-stances."
Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes.
"Comprehensive and accurate class-instance in-formation is useful not only in search but also in a variety of other text processing tasks includ-ing co-reference resoluti[REF_CITE], named entity recogniti[REF_CITE]and seed-based information ex-tracti[REF_CITE]."
"We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combin-ing and ranking individual extractions in a princi-pled way with the Adsorption label-propagation al-gorithm[REF_CITE], reviewed in Section 3 below."
"A collection of labeled classes acquired from text[REF_CITE]is extended in two ways: 1. Class label coverage is increased by identify-ing additional class labels (such as public agen-cies and governmental agencies) for existing instances such as Office of War Information), 2."
The overall instance coverage is increased by extracting additional instances (such as Addi-son Wesley and Zebra Books) for existing class labels (book publishers).
The WebTables database constructed[REF_CITE]is used as the source of additional instances.
"Evaluations on gold-standard labeled classes and instances from existing linguistic re-sources[REF_CITE]indicate coverage im-provements relative to that[REF_CITE], while retaining similar precision levels."
"To show Adsorption’s ability to uniformly combine extractions from multiple sources and methods, we apply it to: 1) high-precision open-domain extrac-tions from free Web text[REF_CITE], and 2) high-recall extractions from WebTa-bles, a large database of HTML tables mined from the Web[REF_CITE]."
These two meth-ods were chosen to be representative of two broad classes of extraction sources: free text and structured Web documents.
"By filtering the class labels using distributional similarity, a large number of high-precision labeled clusters are extracted."
"The algorithm proceeds iteratively: at each step, all clus-ters are tested for label coherence and all coherent labels are tested for high cluster specificity."
"Label L is coherent if it is shared by at least J% of the instances in cluster C, and it is specific if the total number of other clusters C 0 ∈ C, C 0 6= C containing instances with label L is less than K. When a cluster is found to match these criteria, it is removed from C and added to an output set."
The procedure termi-nates when no new clusters can be removed from C. Table 1 shows a few randomly chosen classes and representative instances obtained by this procedure.
"To expand the instance sets extracted from free text, we use a table-based extraction method that mines structured Web data in the form of HTML tables."
A significant fraction of the HTML ta-bles in Web pages is assumed to contain coherent lists of instances suitable for extraction.
"Identifying such tables from scratch is hard, but seed instance lists can be used to identify potentially coherent ta-ble columns."
In this paper we use the WebTables database of around 154 million tables as our struc-tured data source[REF_CITE].
We employ a simple ranking scheme for candi-date instances in the WebTables corpus T .
Each ta-ble T ∈ T consists of one or more columns.
Each column g ∈ T consists of a set of candidate in-stances i ∈ g corresponding to row elements.
"We define the set of unique seed matches in g relative to semantic class C ∈ C as def M C (g) = {i ∈ I(C) : i ∈ g} where I(C) denotes the set of instances in seed class C. For each column g, we define its α-unique class coverage, that is, the set of classes that have at least α unique seeds in g,"
Q(g; α) def = {C ∈ C : |M C (g)| ≥ α}.
Using M and Q we define a method for scoring columns relative to each class.
"Intuitively, such a score should take into account not only the number of matches from class C, but also the total num-ber of classes that contribute to Q and their relative overlap."
"Towards this end, we introduce the scoring function class coherence |M} C |(g)| z { score(C, g; α) def = |M C (g)| · | {z } | S C 0 ∈Q(g;α) I(C 0 ) | seed matches which is the simplest scoring function combining the number of seed matches with the coherence of the table column."
"Coherence is a critical notion in WebTables extraction, as some tables contain in-stances across many diverse seed classes, contribut-ing to extraction noise."
"The class coherence intro-duced here also takes into account class overlap; that is, a column containing many semantically similar classes is penalized less than one containing diverse classes. 1 Finally, an extracted instance i is assigned a score relative to class C equal to the sum of all its column scores, score(i, C; α) def = 1 X score(C, g; α) Z C g∈T,T∈T where Z C is a normalizing constant set to the max-imum score of any instance in class C."
This scor-ing function assigns high rank to instances that oc-cur frequently in columns with many seed matches and high class specificity.
The ranked list of extracted instances is post-filtered by removing all instances that occur in less than d unique Internet domains.
"To combine the extractions from both free and struc-tured text, we need a representation capable of en-coding efficiently all the available information."
We chose a graph representation for the following rea-sons: • Graphs can represent complicated relationships between classes and instances.
"For example, an ambiguous instance such as Michael Jor-dan could belong to the class of both Profes-sors and NBA players."
"Similarly, an instance may belong to multiple nodes in the hierarchy of classes."
"For example, Blue Whales could be-long to both classes Vertebrates and Mammals, because Mammals are a subset of Vertebrates. • Extractions from multiple sources, such as Web queries, Web tables, and text patterns can be represented in a single graph. • Graphs make explicit the potential paths of in-formation propagation that are implicit in the more common local heuristics used for weakly-supervised information extraction."
"For exam-ple, if we know that the instance Bill Clinton belongs to both classes President and Politician then this should be treated as evidence that the class of President and Politician are related."
"Each instance-class pair (i,C) extracted in the first phase (Section 2) is represented as a weighted edge in a graph G = (V, E, W ), where V is the set of nodes, E is the set of edges and W : E → R + is the weight function which assigns positive weight to each edge."
"In particular, for each (i, C, w) triple from the set of base extractions, i and C are added to V and (i, C) is added to E, [Footnote_2] with W (i, C) = w. The weight w represents the total score of all extrac-tions with that instance and class."
"2 In practice, we use two directed edges, from i to C and from C to i, both with weight w."
Figure [Footnote_1] illustrates a portion of a sample graph.
"1 Note that this scoring function does not take into account class containment: if all seeds are both wind Instruments and instruments, then the column should assign higher score to the more specific class."
"This simple graph rep-resentation could be refined with additional types of nodes and edges, as we discuss in Section 7."
"In what follows, all nodes are treated in the same way, regardless of whether they represent instances or classes."
"In particular, all nodes can be assigned class labels."
"For an instance node, that means that the instance is hypothesized to belong to the class; for a class node, that means that the node’s class is hypothesized to be semantically similar to the label’s class (Section 5)."
We now formulate the task of assigning labels to nodes as graph label propagation.
"We are given a set of instances I and a set of classes C represented as nodes in the graph, with connecting edges as de-scribed above."
"We annotate a few instance nodes with labels drawn from C. That is, classes are used both as nodes in the graph and as labels for nodes."
"There is no necessary alignment between a class node and any of the (class) labels, as the final labels will be assigned by the Adsorption algorithm."
The Adsorption label propagation algo-rithm[REF_CITE]is now applied to the given graph.
"Adsorption is a general framework for label propagation, consisting of a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes."
"Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node."
"More specifically, Adsorption works on a graph G = (V, E, W ) and computes for each node v a la-bel distribution L v that represents which labels are more or less appropriate for that node."
Several in-terpretations of Adsorption-type algorithms have ap-peared in various fields ([REF_CITE];
"For details, the reader is referred[REF_CITE]."
We use two interpretations here:
"Adsorption through Random Walks: Let G r = (V, E r , W r ) be the edge-reversed version of the original graph G = (V, E, W ) where (a, b) ∈"
"E r iff (b,a) ∈ E; and W r (a,b) ="
"Now, choose a node of interest q ∈ V ."
"To es-timate L q for q, we perform a random walk on G r starting from q to generate values for a ran-dom label variable L. After reaching a node v during the walk, we have three choices: 1."
"With probability p contv , continue the ran-dom walk to a neighbor of v. 2."
"With probability p abndv , abandon the ran-dom walk."
This abandonment proba-bility makes the random walk stay rela-tively close to its source when the graph has high-degree nodes.
"When the ran-dom walk passes through such a node, it is likely that further transitions will be into regions of the graph unrelated to the source."
The abandonment probability mit-igates that effect. 3.
"With probability p injv , stop the random walk and emit a label L from I v ."
L q is set to the expectation of all labels L emit-ted from random walks initiated from node q.
Adsorption through Averaging:
For this interpre-tation we make some changes to the original graph structure and label set.
"We extend the la-bel distributions L v to assign a probability not only to each label in C but also to the dummy label ⊥, which represents lack of information about the actual label(s)."
"We represent the ini-tial knowledge we have about some node labels in an augmented graph G 0 = (V 0 , E 0 , W 0 ) as follows."
"For each v ∈ V , we define an ini-tial distribution I v = L ⊥ , where L ⊥ is the dummy distribution with L ⊥ (⊥) = 1, repre-senting lack of label information for v. In addi-tion, let V s ⊆ V be the set of nodes for which we have some actual label knowledge, and let V 0 = V ∪ {v̄ : v ∈ V s },E 0 = E ∪ {(v̄,v) : v ∈ V s }, and W 0 (v̄,v) = 1 for v ∈ V s , W 0 (u, v) ="
"W(u, v) for u, v ∈ V ."
"Finally, let I v̄ (seed labels) specify the knowledge about possible labels for v ∈ V s ."
"Less formally, the v̄ nodes in G 0 serve to inject into the graph the prior label distributions for each v ∈ V s ."
The algorithm proceeds as follows: For each node use a fixed-point computation to find label distributions that are weighted averages of the label distributions for all their neighbors.
This causes the non-dummy initial distribution of V s nodes to be propagated across the graph.
"Algorithm 1 combines the two views: instead of a random walk, for each node v, it itera-tively computes the weighted average of label distri-butions from neighboring nodes, and then uses the random walk probabilities to estimate a new label distribution for v."
"For the experiments reported in Section 4, we used the following heuristics[REF_CITE]to set the random walk probabilities: • Let c v = log(β +logexpβ H(v)) where H(v) = − P u p uv × log(p uv ) with p uv = P W(u,v) 0 . u0 W(u ,v)"
H(v) can be interpreted as the entropy of v’s neighborhood.
"Thus, c v is lower if v has many neighbors."
"We set β = 2. • j v = (1 − c v ) × pH(v) if I v 6= L &gt; and 0 otherwise. • Then let z v = max(c v + j v , 1) p contv = c v /z v p injv = j v /z v p abndv = 1 − p contv − p vabnd"
"Thus, abandonment occurs only when the con-tinuation and injection probabilities are low enough."
The algorithm is run until convergence which is achieved when the label distribution on each node ceases to change within some tolerance value.
"Alter-natively, the algorithm can be run for a fixed number of iterations which is what we used in practice [Footnote_3] ."
3 The number of iterations was set to 10 in the experiments reported in this paper.
"Finally, since Adsorption is memoryless, it eas-ily scales to tens of millions of nodes with dense edges and can be easily parallelized, as described[REF_CITE]."
"As mentioned in Section 3, one of the benefits of using Adsorption is that we can combine extrac-tions by different methods from diverse sources into a single framework."
"To demonstrate this capabil-ity, we combine extractions from free-text patterns and from Web tables."
"To the best of our knowl-edge, this is one of the first attempts in the area of minimally-supervised extraction algorithms where unstructured and structured text are used in a prin-cipled way within a single system."
"Open-domain (instance, class) pairs were ex-tracted by applying the method described[REF_CITE]on a corpus of over 100M English web documents."
"A total of 924K (instance, class) pairs were extracted, containing 263K unique instances in 9081 classes."
We refer to this dataset as A8.
"Using A8, an additional 74M unique (in-stance,class) pairs are extracted from a random 10% of the WebTables data, using the method outlined in Section 2.2."
"For maximum coverage we set α = 2 and d = 2, resulting in a large, but somewhat noisy collection."
We refer to this data set as WT.
"We applied the graph construction scheme described in Section 3 on the A8 and WT data combined, re-sulting in a graph with 1.4M nodes and 75M edges."
"Since extractions in A8 are not scored, weight of all edges originating from A8 were set at 1 4 ."
This graph is used in all subsequent experiments.
We evaluated the Adsorption algorithm under two experimental settings.
"First, we evaluate Adsorp-tion’s extraction precision on (instance, class) pairs obtained by Adsorption but not present in A8 (Sec-tion 5.1)."
This measures whether Adsorption can add to the A8 extractions at fairly high precision.
"Second, we measured Adsorption’s ability to assign labels to a fixed set of gold instances drawn from various classes (Section 5.2)."
"First we manually evaluated precision across five randomly selected classes from A8: Book Publish-ers, Federal Agencies, NFL Players, Scientific Jour-nals and Mammals."
"For each class, 5 seed in-stances were chosen manually to initialize Adsorp-tion."
These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the resulting ranked extractions were manually evalu-ated.
"Since the A8 system does not produce ranked lists of instances, we chose 100 random instances from the A8 results to compare to the top 100 instances produced by Adsorption."
"Each of the resulting 500 instance-class pairs (i, C) was presented to two hu-man evaluators, who were asked to evaluate whether the relation “i is a C” was correct or incorrect."
The user was also presented with Web search link to ver-ify the results against actual documents.
Results from these experiments are presented in Figure 2 and Table 4.
The results in Figure 2 show that the A8 system has higher precision than the Adsorption system.
This is not surprising since the A8 system is tuned for high precision.
"When considering individ-ual evaluation classes, changes in precision scores between the A8 system and the Adsorption system vary from a small increase from 87% to 89% for the class Book Publishers, to a significant decrease from 52% to 34% for the class Federal Agencies, with a decrease of 10% as an average over the 5 evaluation classes."
Table [Footnote_4] shows the precision of the Adsorption sys-tem for instances not extracted by the A8 system.
4 A8 extractions are assumed to be high-precision and hence we assign them the highest possible weight.
Such an evaluation is important as one of the main motivations of the current work is to increase cov-erage (recall) of existing high-precision extractors without significantly affecting precision.
Results in Table 4 show that Adsorption is indeed able to ex-traction with high precision (in 4 out of 5 cases) new instance-class pairs which were not extracted by the original high-precision extraction set (in this case A8).
Examples of a few such pairs are shown in Table 5.
This is promising as almost all state-of-the-art extraction methods are high-precision and low-recall.
The proposed method shows a way to overcome that limitation.
"As noted in Section 3, Adsorption ignores node type and hence the final ranked extraction may also contain classes along with instances."
"Thus, in ad-dition to finding new instances for classes, it also finds additional class labels similar to the seed class labels with which Adsorption was run, at no extra cost."
Some of the top ranked class labels extracted by Adsorption for the corresponding seed class la-bels are shown in Table 3.
"To the best of our knowl-edge, there are no other systems which perform both tasks simultaneously."
Next we evaluated each extraction method on its rel-ative ability to assign labels to class instances.
"For each test instance, the five most probably class la-bels are collected using each method and the Mean Reciprocal Rank (MRR) is computed relative to a gold standard target set."
"This target set, WN-gold, consists of the 38 classes[REF_CITE]or more instances."
"In order to extract meaningful output from Ad-sorption, it is provided with a number of labeled seed instances (1, 5, 10 or 25) from each of the 38 test classes."
"Regardless of the actual number of seeds used as input, all 25 seed instances from each class are removed from the output set from all methods, in order to ensure fair comparison."
The results from this evaluation are summarized in Table 6; AD x refers to the adsorption run with x seed instances.
"Overall, Adsorption exhibits higher MRR than either of the baseline methods, with MRR increasing as the amount of supervision is increased."
"Due to its high coverage, WT assigns labels to a larger number of the instance in WN-gold than any other method."
"However, the average rank of the correct class assignment is lower, resulting is lower MRR scores compared to Adsorption."
"This result highlights Adsorption’s ability to effectively combine high-precision, low-recall (A8) extractions with low-precision, high-recall extractions (WT) in a manner that improves both precision and coverage."
Graph based algorithms for minimally supervised information extraction methods have recently been proposed.
"For example,[REF_CITE]use a random walk on a graph built from entities and relations extracted from semi-structured text."
"Our work differs both conceptually, in terms of its focus on open-domain extraction, as well as methodologi-cally, as we incorporate both unstructured and struc-tured text."
"The re-ranking algorithm[REF_CITE]also constructs a graph whose nodes are in-stances and attributes, as opposed to instances and classes here."
Adsorption can be seen as a general-ization of the method proposed in that paper.
The field of open-domain information extraction has been driven by the growth of Web-accessible data.
"We have staggering amounts of data from various structured and unstructured sources such as general Web text, online encyclopedias, query logs, web ta-bles, or link anchor texts."
Any proposed algorithm to extract information needs to harness several data sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards that goal.
"In doing so, we achieved the following: 1."
Improved coverage relative to a high accuracy instance-class extraction system while main-taining adequate precision. 2.
Combined information from two different sources: free text and web tables. 3. Demonstrated a graph-based label propagation algorithm that given as little as five seeds per class achieved good results on a graph with more than a million nodes and 70 million edges.
"In this paper, we started off with a simple graph."
"For future work, we plan to proceed along the fol-lowing lines: 1."
"Encode richer relationships between nodes, for example instance-instance associations and other types of nodes. 2. Combine information from more data sources to answer the question of whether more data or diverse sources are more effective in increasing precision and coverage. 3. Apply similar ideas to other information extrac-tion tasks such as relation extraction."
Relationship discovery is the task of iden-tifying salient relationships between named entities in text.
"We propose novel approaches for two sub-tasks of the problem: identifying the entities of interest, and partitioning and describing the relations based on their semantics."
"In particular, we show that term frequency patterns can be used effectively instead of supervised NER, and that the p-median clustering objective function naturally uncovers relation exemplars appropriate for describing the partitioning."
"Furthermore, we introduce a novel application of relationship discovery: the unsupervised identification of protein-protein interaction phrases."
Relationship extraction (RE) is the task of extracting named relationships between entities in text given some information about the relationships of interest.
"Relationship discovery (RD), on the other hand, is the task of finding which relations exist in a corpus without any prior knowledge."
"The discovered rela-tionships can then be used to bootstrap RE, which is why RD has also been called unsupervised relation extracti[REF_CITE]."
"RD gen-erally involves three sub-tasks: entities of interest are either supplied or recognized in the corpus; sec-ond, of all phrases in which entities co-occur, those which express a relation are picked out; finally, these relationship phrases are partitioned based on their semantics and described."
This work considers only binary relations (those between exactly two entities).
"Finding entities of interest has involved either named entity recognition (NER) or general noun phrase (NP) chunking, to create the initial pool of candidate entities."
"In Section 2, we describe a corpus statistics approach, previously applied for web mining[REF_CITE], which we extend for relation discovery."
"Unlike supervised machine learning methods, this algorithm does not need training, is computationally efficient, and requires as input only the raw corpus and a small set of seed entities (as few as two)."
The result is a set of entities likely to be related to the seeds.
An assumption commonly held in RD work is that frequently co-occurring entity tuples are likely to stand in some fixed relati[REF_CITE].
Tuples which share similar contexts (the exact definition of context varies) are then grouped together in clusters of relations using variants of hi-erarchical agglomerate clustering (HAC).
"However, to our knowledge, no prior work has satisfactorily addressed the problem of describing the resulting clusters."
"In Section 3, we propose an approach which incorporates this requirement directly into the clustering objective: to find relation clusters which are well-described by a single exemplar."
"In Section 4, we apply RD to recognize protein-protein interaction (PPI) sentences, using proteins as seeds for the entity discovery phase."
We compare our results against special-purpose methods in terms of precision and recall on standard data sets.
The remainder of this paper is outlined below: Section 2 describes how a small number of input words (the entities of interest) are used as seeds for unsupervised entity discovery.
Section 3 de-scribes how discovered entities are used to discover relationships.
Section 4 describes evaluation methodology and results.
Section 5 describes related work.
Section 6 concludes and discusses directions for future work.
"For a corpus C, each sentence s ∈ C with words s = (w 1 ,w 2 ,...,w n ), is mapped to the sequence s 0 = f(s)."
The function f maps each word w ∈ s to a symbol based on its frequency in C as follows:  S if w is a seed word f(w) =  H otherwise if w is a frequent word  X otherwise
"For example, the sentence:"
"A and B are usually mediated by an overproduced C. might be mapped to the sequence (S, H, X, H, H, X, H, H, X, X), which we will write as SHXHHXHHXX for brevity."
"In this case, A is a seed term, while B and C are not."
The underlying assumption is that content words can be distinguished from other words based on their frequency in the corpus.
"In the example sentence, ‘A and B are usually mediated by an overproduced C’, ‘and’ is a good indicator that A,B share some aspect of their semantics; in this case, that they are both me-diated by an overproduced C, and are therefore also likely to belong to same family or type of entities."
The indicators ‘and’ and ‘or’ have together been used to discover word categories in lexical acquisiti[REF_CITE].
"However, there can be many other such indicators, many discourse or corpus specific."
"To discover them, we use a slightly modified version of the method presented[REF_CITE]."
"In particular, in this work we consider named entities of arbitrary length (i.e., longer than a single token)."
"The corpus is searched for all instances of the frequency pattern H 1 S 1 H 2 S 2 H 3 , for seed words S 1 , S 2 , and pattern (H 1 , H 2 , H 3 )."
"Of all these pat-tern instances, we keep those which also appear as H 1 S 2 H 2 S 1 H 3 ."
"If seed words appear on either side of the pattern, it is a good indication that the sym-metric pattern expresses some sort of a conjunction, often domain specific."
"This procedure is repeated for variations of HSHSH with the goal of capturing different forms of speech; for example, HSHSH will capture ‘; A , B and’, while HSHHSH will capture ‘; A but not B ,’ and so on."
"We enforce that frequent words appear before and after (i.e., sur-round) the two seed words to ensure they are stand-alone entities, and not part of a longer noun phrase."
"For example, the phrase ‘IFN-gamma mRNA and IL-6 are’ maps to XXHSH, and therefore ‘mRNA’ would (correctly) not be added to the entity pool."
New entities are added to the initial set of seed by matching symmetric patterns.
"If a seed word S is found to occur with an infrequent word X in any discovered symmetric pattern (as HSHXH or HXHSH), then we add X to the pool of entities."
This process can be bootstrapped as needed.
"In Section 3.1, sentences in which entities co-occur are clustered based on a measure of pairwise simi-larity."
The features used in this similarity calculation are based on the surrounding or connecting words in the sentence in which entities co-occur.
"To ensure the context is not polluted with words which actually belong the entity NP (such as ‘IFN-gamma mRNA’) rather than the context, we use frequency patterns to search the corpus for common NP chunks."
"In each sentence in which entities occur, we form a candidate chunk by matching the regular expres-sion HX ∗ SX ∗ H, which returns all content-words X bracketing the entity S. Of all candidate chunks, we keep those which occur frequently enough to significantly affect the similarity calculations."
"The remaining chunks are pruned based on the entropy of the words appearing immediately before and after the chunk in the corpus; if a given chunk appears in a variety of contexts, it is more likely to express a meaningful collocati[REF_CITE]."
"Therefore, as an efficient filter on the candidate chunks, we discard those which tend to occur in the same contexts (where the context is H...H)."
"Once the pool of entities has been recognized in the corpus, those which frequently co-occur are taken as likely to stand in a relation."
Order matters in that S 1 ..S 2 is considered a different entity co-occurrence (and therefore potential relation) than S 2 ..S 1 .
The effect of the co-occurrence threshold on the resulting relations is investigated in Section 4.
Partitioning the candidate relationships serves to identify groups of differently expressed relation-ships of similar semantics.
The resulting clusters should cover the most important relations in a cor-pus between the entities of interest.
The phrases in each cluster are expected to capture most syntactic variation in the expression of a given relationship.
"Therefore, the largest clusters are well suited as positive examples for training a relationship extractor[REF_CITE]."
"We take the context of a co-occurring tuple to be the terms connecting the two entities within the sentence in which they appear, and call the connecting terms a relation phrase (RP)."
Each RP is treated separately in the similarity calculations and the clustering.
Relations are modeled using a vector space model.
Each relation is treated as a vector of term frequencies (tf) weighted by tf × idf.
RPs are preprocessed by filtering stopwords [Footnote_1] .
"1 We use the English stopword list from the Snowball project, available[URL_CITE]"
"However, we do not stem the remaining words, as suffixes can be highly discriminative in determining the semantics of a relation (e.g., ‘production’ vs ‘produced’)."
"Af-ter normalizing vectors to unit length , we compute a similarity matrix by computing the dot product be-tween the vectors for each distinct RP pair."
The sim-ilarity matrix is then used as input for the clustering.
Prior approaches to relationship discovery have used HAC to identify relation clusters.
"HAC is attractive in unsupervised applications since the number of clusters is not required a priori, but can be determined from the resulting dendogram."
"On the other hand, a typical HAC implementation runs in Θ(N [Footnote_2] log(N)), which can be prohibitive on larger data sets 2 ."
2 An optimization to Θ(N 2 ) is possible for single-linkage HAC.
"A further feature of HAC, and many other par-titional clustering algorithms such as k-means and spectral cuts, is that the resulting clusters are not necessarily well-described by single instance."
"Re-lations, however, typically have a base or root form which would be desirable to uncover to describe the relation clusters."
"For example, in the following RPs: induced transient increases in induced biphasic increases in induced an increase in induced an increase in both induced a further increase in the phrase ‘induced an increase in’ is well suited as a base form of the relation and a descriptor for the cluster."
The p-median clustering objective is to find p clusters which are well-described by a single exemplar.
"Formally, given an N × N similarity matrix, the goal is to select p columns such that the sum of the maximum values within each row of the selected columns are maximized."
"Note that an exemplar can also be chosen a posteriori using some heuristic; for example, the most frequently occurring instance in a cluster can be taken as the exemplar."
"However, the p-median clustering objective is robust, and ensures that only those clusters which are well described by a single exemplar appear in the resulting partition of the relations."
"This means that the optimal number of clusters for the p-median clustering objective in a given data set will usually be quite different (usually higher) than the optimal number of groups according to the HAC, k-means, or normalized cut objectives."
"Affinity propagation (AP) is the most efficient approximation for the p-median problem that we are aware of, which also has the property of not requir-ing the number of clusters as an explicit input[REF_CITE]."
"Runtime is linear in the number of similarities, which in the worst case is N 2 (for N relations), but in practice many relations share no words in common, and therefore do not need to have their similarity considered in the clustering."
AP is an iterative message-passing procedure in which the objects being clustered compete to serve as cluster exemplars by exchanging two types of messages.
"The responsibility r(x,m), sent from object x ∈ X (for set X of objects to be clustered) to candidate exemplar m ∈ X, denotes how well-suited m is of being the exemplar for x by considering all other potential exemplars m 0 of x: s(x, m) − a(x, m 0 ) + s(x, m 0 ) max m 0 ∈X,m 0 6=m where s(x, m) is the similarity between x, m. The availability a(x, m) of each object x ∈ X is initially set to zero."
"Availabilities, sent from candidate exemplar m to object x, increase as evidence for m to serve as the exemplar for x increases:    max{0, r(x 0 , m)}  X min 0, r(m, m) +  x 0 ∈X,x 0 6∈{x,m} "
Each object to be clustered is assigned an initial preference of becoming a cluster exemplar.
"If there are no a priori preferences for cluster exemplars, the preferences are set to the median similarity (which can be thought of as the ‘knee’ of the objective function graph vs. number of clusters), and exem-plars emerge from the message passing procedure."
"However, shorter RP are more likely to contain base forms of relations (because longer phrases likely contain additional words specific to the sentence)."
"Therefore, we include a slight scaling factor in the preferences, which assigns shorter RP higher initial values (up to 1.5× the median similarity)."
"After clustering relation phrases with AP, we prune the resulting partition by evaluating the number of different relation instances appearing in each cluster, as well as the entities involved."
"In our experiments, we discard all clusters smaller than a certain threshold, since we ultimately wish to use the clustering to train RE, and small clusters do not provide enough positive examples for training (we investigate the effect of this threshold in Sec-tion 4.2)."
"We further assume that for a relationship to be useful, a number of different entities should stand in this relation."
"In particular, we inspect the set of left and right arguments in the cluster, which (in English) usually correspond to the subject and object of the sentence."
"If a single entity constitutes more than two thirds ( 23 ) of the left or right argu-ments of a cluster, then this cluster is discarded from the results."
Our assumption is that these clusters describe relations too specific to be useful.
"RD systems are usually evaluated based on their re-sults for a particular task such as RE[REF_CITE], or by a manual inspection of their results[REF_CITE], but we are not aware of any which examines the effects of parame-ters on performance exhaustively."
In this section we test several hypotheses of RD using data sets which are already labeled for sentences which contain entities of a particular type and in a fixed relation of some kind.
"In particular, we adapt the output of the discovery phase to identify phrases which express PPIs."
"While this task is traditionally performed using supervised algorithms such as support vector machines[REF_CITE], we show that RD is capable of achieving similar levels of precision without any manually annotated training data."
We construct a corpus of 87300 abstracts by query-ing the PubMed database with the proteins shown in Table 1.
"This corpus serves as input for the 



"
A PPI denotes a broad class of bio-medical relationships between two proteins.
One example of an interaction is where the two proteins bind together to form a structural complex of cellular machinery such as signal transduction machinery.
A second example is when one protein binds upstream of the DNA sequence encoding a gene which en-codes the second protein.
A final example is when proteins serve as enzymes catalyzing successive steps of a biochemical reaction.
More categories of interactions are continually being catalogued and hence unsupervised identification of PPIs is important in biomedical text mining.
"Method: To evaluate the performance of our sys-tem, we measure how well the relationships discov-ered compare with manually selected PPI sentences."
"To do so, we follow the same procedure and data sets used to evaluate semi-supervised classification of PPI sentences[REF_CITE]."
"The two data sets are AIMED and CB, which have been marked for protein entities and interaction phrases [Footnote_3] ."
3 Available in preprocessed form[URL_CITE]
"For each sentence in which n proteins appear, we build n phrases."
"Each phrase consists of 2 the words between each entity combination, and is labeled as positive if it describes a PPI, or negative otherwise."
This results in 4026 phrases for the
"AIMED data set (951 positive, 3075 negative), and 4056 phrases for the CB data set (2202 positive, 1854 negative)."
The output of the discovery phase is a clustering of RPs.
"For purpose of this experiment, we ignore the partition and treat the phrases in aggregate."
A phrase in the evaluation data set is classified as positive (describing a PPI) if any substring of the phrase matches an RP in our output.
"For example, if the phrase is:"
"A significantly inhibited B and the string ‘inhibited’ appears as a relation in our output, then this phrase is marked positive."
"Otherwise, the phrase is marked negative."
"Performance is evaluated using standard metrics of precision (P), recall (R), and F-measure (F 1 ), defined as:"
"P = TPTP+ FP ; R = TPTP+ FN where TP is the number of phrases correctly identified as describing a PPI, FP is the number of phrases incorrectly classified as describing a rela-tion, and FN is the number of interaction phrases (positives) marked negative."
F 1 is defined as:
F 1 = P2P+RR
"We calculate P, R, and F 1 for three parameters affecting which phrases are identified as expressing a relation: • the minimum co-occurrence threshold that con-trols which entity tuples are kept as likely to stand in some fixed relation • the minimum cluster size that controls which groups of relations are discarded • the minimum RP length that controls the smallest number of words appearing in relations"
The threshold on the length of the relations can be thought of as controlling the amount of contextual information expressed.
"A single term relation will be very general, while longer RPs express a relation very specific to the context in which they are written."
The results are reported in Figures 1 through 6.
Odd numbered figures use the AIMED corpus; even numbered figures the CB corpus.
Results: Discarding clusters below a certain size had no significant effect on precision.
"However, this step is still necessary for bootstrapping RE, since machine learning approaches require a sufficient number of positive examples to train the extractor."
"On the other hand, our results confirm the observation that frequently co-occurring pairs of entities are likely to stand in a fixed relation."
"On the CB corpus, precision ranges from 0.63 to 0.86 for phrases between entities co-occurring at least 50 times."
"On the AIMED corpus, precision ranges from 0.29 to 0.55 in the same threshold range."
"The minimum phrase length had the most impact on performance, which was particularly evident in the CB corpus: this corpus reached perfect precision discarding all RPs of fewer than 3 words."
"Lower thresholds result in significantly more relations, at the cost of precision."
The generally lower performance on the AIMED corpus suggests that our training data (retrieved from the seed proteins) provided less coverage for those interactions than for the those in the CB corpus.
Table 2 and Table 3 compare our results at fixed parameter settings with supervised approaches.
RD-F 1 reports parameters which give highest recall and RD-P highest precision.
"Specifically, both RD-F 1 and RD-P use a minimum RP length of 1, RD-F 1 uses a co-occurrence threshold of 10, and RD-P uses a co-occurrence threshold of 50."
"As expected, RD alone does not match combined precision and recall of state-of-the-art supervised systems."
"However, we show better performance than expected."
RD-F 1 outperforms the best results[REF_CITE].
RD-P settings out-perform or match the precision of top-performing systems on both datasets.
Method: We evaluate the appropriateness of the p-median clustering as follows.
"For each cluster, we take the cluster exemplar as defining the base relation."
"If the base relation does not express something meaningful, then we mark each mem-ber of the cluster incorrect."
"Otherwise, we label each member of the cluster either as semantically similar to the exemplar (correct) or different than the exemplar (incorrect)."
"Thus, clusters with inappropriate exemplars are heavily penalized."
These results are reported in Table 4.
"For purpose of this experiment, we use the same parameters as for RD-P, and evaluate the 20 largest clusters."
"Results:[REF_CITE]largest clusters, each cluster ex-emplar expressed something meaningful. 3 of the cluster exemplars were not representative of their other members."
We found that most error was due to stopwords not being considered in our similarity cal-culations.
"For example, ‘detected by’ and ‘detected in’ express the same relationship in our similarity calculations; however, they are clearly quite differ-ent."
Another source of error evident in Table 4 are mistakes in the pattern and entropy based chunking.
"The exemplar ‘mrna expression in’ includes the to-ken ‘mrna’, which belongs with the left protein NP in the relation chosen as an exemplar."
RD is a relatively new area of research.
Existing methods differ primarily in the amount of super-vision required and in how contextual features are defined and used.[REF_CITE]use NER to identify frequently co-occurring entities as likely relation phrases.
"As in this work, they use the vector model and cosine similarity to define a measure of simi-larity between relations, but build relation vectors out of all instances of each frequently co-occurring entity pair."
"Therefore, each mention of the same co-occurring pair is assumed to express the same relationship."
"These aggregate feature vectors are clustered using complete-linkage HAC, and cluster exemplars are determined by manual inspection for evaluation purposes.[REF_CITE]rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations.[REF_CITE]consider the use of RD for unsupervised relation extraction, and use a more complex pattern-learning approach to define feature vectors to cluster candidate relations, report-ing gains in accuracy compared with the tf × idf weighed features used[REF_CITE]and in this work."
"They also use HAC, and do not address the description of the relations."
"Arbitrary noun phrases obtained through shallow parsing are used as entities.[REF_CITE]use a feature ranking scheme using separability-based scores, and compare the performance of different variants of HAC (finding single-linkage to perform best)."
"The complexity of the feature ranking-scheme described can be greater than the clustering itself; in contrast, while we use simple features, our approach is much more efficient.[REF_CITE]introduce the use of term frequency patterns for relationship discovery."
"However, they search for a specific type of relation-ship; namely, attributes common to all entities of a particular type (for example, all countries have the attribute capital), and use a special purpose set of filters rather than entity co-occurrence and clustering."
"Our work can be seen as a generalization of theirs to relationships of any kind, and we extend the use of frequency patterns to finding general n-gram entities rather than single word entities.[REF_CITE]give an excellent overview of biomedical NER and RE."
"They propose a statis-tical system for RE, but rely on NER, POS tagging, and the creation of a dictionary for each domain of application."
"Also, they do not cluster relationships into semantically related groups."
Our work makes a series of important improvements to the state-of-the-art in relationship discovery.
"First, by incorporating entity discovery into the rela-tionship discovery pipeline, our method does not re-quire distinct training phases to accommodate differ-ent entity types, relations, or discourse types."
"Sec-ond, p-median clustering effectively uncovers the base form of relations present in the corpus, address-ing an important limitation in usability."
"In terms of specific hypotheses, we have tested and confirmed that co-occurrence can be a good indicator of the presence of a relationship but the size of a cluster is not necessarily a good indicator of the importance or strength of the discovered relationship."
"Further-more, we have shown that longer RPs with more context give higher precision (at the cost of reduced coverage)."
"Finally, the integration of ideas in our approach—unsupervisedness, efficiency, flexibility (in application), and specificity—is novel in itself."
"In future work, we seek to expand upon our RD methods in three directions."
"First, we would like to generalize the scope of our discovery pipeline beyond binary relations and with richer considera-tions of context, even across sentences."
"Second, we hope to achieve greater tunability of performance, to account for additional discovery metrics besides precision."
"Finally, we intend to induce entire con-cept maps from text using the discovered relations to bootstrap an RE phase, where the underlying problem is not just of inferring multiple types of relations, but to have sufficient co-ordination among the discovered relations to ensure connectedness among the resulting concepts."
"While our method requires no supervision in the form of manually annotated entities or relations, the effectiveness of the system relies on the careful tuning of a number of parameters."
"Nevertheless, the results reported in Section 4.2 suggest that the two parameters that most significantly affect perfor-mance exhibit predictable precision/recall behavior."
"Of the parameters not considered in Section 4.2, we would like to further investigate the benefits of chunking entities on the resulting base relations, ex-perimenting with different measures of collocation."
"While significant effort has been put into an-notating linguistic resources for several lan-guages, there are still many left that have only small amounts of such resources."
This paper investigates a method of propagat-ing information (specifically mention detec-tion information) into such low resource languages from richer ones.
"Experiments run on three language pairs (Arabic-English, Chinese-English, and Spanish-English) show that one can achieve relatively decent perfor-mance by propagating information from a lan-guage with richer resources such as English into a foreign language alone (no resources or models in the foreign language)."
"Fur-thermore, while examining the performance using various degrees of linguistic informa-tion in a statistical framework, results show that propagated features from English help improve the source-language system perfor-mance even when used in conjunction with all feature types built from the source language."
The experiments also show that using propa-gated features in conjunction with lexically-derived features only (as can be obtained di-rectly from a mention annotated corpus) yields similar performance to using feature types de-rived from many linguistic resources.
"Information extraction is a crucial step toward un-derstanding a text, as it identifies the important con-ceptual objects and relations between them in a dis-course."
"It includes classification, filtering, and se-lection based on the language content of the source data, i.e., based on the meaning conveyed by the data."
"It is a crucial step for several applications, such as summarization, information retrieval, data mining, question answering, language understand-ing, etc."
"This paper addresses an important and basic task of information extraction: mention detection [Footnote_1] : the identification and classification of textual refer-ences to objects/abstractions mentions, which can be either named (e.g. John Smith), nominal (the presi-dent) or pronominal (e.g. he, she)."
1 We adopt here the ACE[REF_CITE]nomenclature
"For instance, in the sentence"
"President John Smith said he has no comments. there are three mentions: President, John Smith and he."
This is similar to the named entity recognition (NER) task with the additional twist of also identi-fying nominal and pronominal mentions.
"A few languages have received a lot of attention in terms of natural language resources that were cre-ated – for instance, in English one has access to la-beled part-of-speech data, word sense information, parse tree structure, discourse, semantic role labeles, named entity data, to name just a few (our apologies if we missed your favorite resource)."
"There are a few other languages that also have annotated resources (such as Arabic, Chinese, German, French, Spanish, etc), but also a very large number of languages with few resources."
It would be very useful if one could make use of the resources in the former languages to help bootstrapping (or just the projection) of re-source in any resource-challenged language.
Information transfer from a language to another can be very useful when the “donor” language has more resources than the receiving one.
"As resources grow in quantity and quality in the receiving lan-guage, it becomes less and less likely that there will be a gain in performance by transfering information, as there are several sources of noise involved in the process - such as the translation (machine generated or not) and the inherent imperfection of the mention detection in the donor language."
"To test this hypoth-esis, we conducted experiments on systems build with a varied amount of resources in the receiv-ing language, starting with the case where there are none [Footnote_2] (all information is transferred through transla-tion alignment), and ending with the case where we used all the resources we could gather for that lan-guage."
"2 While applying this method in the case where the source language has absolutely no resources might be an interesting test case, we don’t see it as being realistic. Resources are build nowadays in a large variety of languages, and not making use of them is rather foolish (a certain big bird and sand comes to mind)."
"The experiments will show that the gain in performance decreases with the amount of resources used in the source language, but, still, even when all resources were used, a statistically significant gain was still observed."
"Similarly to classical NLP tasks such as text chunking[REF_CITE]and named entity recognition (Tjong[REF_CITE]), we for-mulate mention detection as a sequence classifica-tion problem, by assigning a label to each token in the text, indicating whether it starts a specific men-tion, is inside a specific mention, or is outside any mentions."
"The classification is performed with a sta-tistical approach, built around the maximum entropy (MaxEnt) principle[REF_CITE], that has the advantage of combining arbitrary types of informa-tion in making a classification decision."
There are several investigations in literature that explore using parallel corpora to transfer informa-tion content from one language (most of the time English) to another.
"The earliest investigations of the subject have been performed, on word sense disambiguation ([REF_CITE]; P.F.[REF_CITE]) (perhaps unsurpris-ingly given its close connection to machine trans-lation) – all propose and (lightly) evaluate methods to use word sense information extracted from the target language to help the sense resolution in the source language and machine translation.[REF_CITE]explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew), and show moderate improvement on a small data set [Footnote_3] ."
"3 Very small by “modern” standards - 137 examples. Prob-ably because at the time the article was written, there were no large publicly annotated databases, such as Semcor."
"More recently,[REF_CITE]presents a method for per-forming word sense tagging in both the source and target texts of parallel bilingual corpora with the En-glish WordNet sense inventory, by using translation correspondences."
"On more general cross-language information transfer,[REF_CITE]proposed and eval-uated a method of propagating POS tagging, named mention, base noun phrase, and morphological in-formation from English into a foreign language, which is very similar to the one presented in this article (experiments were run on French, Chinese, Czech, and Spanish – on human-generated transla-tions)."
"Their results show a significant improvement in performance while building an automatic classi-fier on the projected annotations over the same au-tomatic classifier trained on a small amount of an-notated data in the source language.[REF_CITE]extends the ideas[REF_CITE], by showing how it can be used, in conjunction with an automatically trained information extraction sys-tem on the source language, to bootstrap the annota-tion of resources in the target language."
"They show that they can obtain 48 F-measure on a information extraction task identifying locations, vehicles and victims in plane crashes.[REF_CITE]proposes a framework that enables the acquisition of syntactic dependency trees for low-resource languages by im-porting linguistic annotation from rich-resource lan-guages (English)."
"The authors run a large-scale ex-periment in which Chinese dependency parses were induced from English, and show that a parser trained on the resulting trees outperformed simple baselines.[REF_CITE]investigates a similar method of propagating syntactic treebank-like annotations from English to Spanish."
"Finally, a large body of research has been done on cross-language information retrieval, where the goal is to find information in one language (e.g. Chi-nese newswire) corresponding to a query in a differ-ent language (e.g. English) – although the list of rel-evant papers is too long to be mentioned here (see, for instance,[REF_CITE])."
"The work presented here differs from the infor-mation extraction investigations presented above in two aspects: • it handles unrestricted text and a full set of mention types (the ACE entity types) during the information transfer • it investigates whether using a resource-rich language (English) can improve on the perfor-mance obtained by using various degrees of ex-istent resources in the source language (Arabic, Chinese, Spanish) • the information transfer is performed over ma-chine generated translations and alignments."
"As mentioned in the introduction, the mention detec-tion problem is formulated as a classification prob-lem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is in-side a specific mention, or is outside any mentions."
"Good performance in many natural language pro-cessing tasks has been shown to depend heavily on integrating many sources of informati[REF_CITE]. [Footnote_4] Given this observation, we are interested in algorithms that can easily integrate and make ef-fective use of diverse input types."
"4 In fact, the feature set used for classification has a much larger impact on the performance of the resulting system than the classifier method itself."
"We select a ex-ponential classifier, the Maximum Entropy (MaxEnt henceforth) classifier that integrates arbitrary types of information and makes a classification decision by aggregating all information available for a given classification."
But the reader can replace it with her favorite feature-based classifier throughout the pa-per.
"To help with the presentation, we introduce some notations: let Y = {y 1 , . . . , y n } be the set of pre-dicted classes, X be the example space and F = {0,1} m be a feature space."
"Each example x ∈ X has associated a vector of m binary features f (x) = (f 1 (x) , . .. , f m (x))."
"The goal of the training pro-cess is to associate examples x ∈ X with either a probability distribution over the labels from Y, P (·|x)(if we are interested in soft classification) or associate one label y ∈ Y (if we are interested in hard classification)."
"The MaxEnt algorithm associates a set of weights {α ij } ji==11......nm with the features (f j ) i , and computes the probability distribution as m 1 f j (x,y i ) , P (y i |x) = Z(x) Y α ij (1) j=1 j (x,y i )"
Z(x) = X Y α ijf i j where Z(x) is a normalization factor.
The {α ij } j=1...m weights are estimated during the train-ing phase to maximize the likelihood of the data[REF_CITE].
"In this paper, the Max-Ent model is trained using the sequential condi-tional generalized iterative scaling (SCGIS) tech-nique[REF_CITE], and it uses a Gaussian prior for regularizati[REF_CITE]."
"Now take x N1 = (x 1 , x 2 , . . . x N ), a sequence of contiguous tokens (i.e., a sentence or a document) in the source language."
"The goal of mention detection system is to find the most likely sequence of labels y N1 = (y 1 , y 2 . . . y N ) that best matches the input x N1 ."
"In the mention detection case, each token x i in x N1 is tagged with a label y i as follows: [Footnote_5] • if it’s not part of any entity, y i ="
5 The mention encoding is the IOB2 encoding presented in (Tjong[REF_CITE]) and introduced[REF_CITE]for base noun phrase chunking.
"O (O for “out-side any mentions”) • if it is part of an entity, it is composed of a sub-tag specifying whether it starts a mention (B-) or is inside a mention (I-), and a sub-type cor-responding to mention type (e.g. B-PERSON)."
"In ACE, there are seven possible types: person, organization, location, facility, geopolitical en-tity (GPE), weapon, and vehicle."
"To compute the best sequence y N1 , we use y N1 = arg max P ŷ N1 |x N1 ŷ N1 = arg max Y P ŷ j |x N1 , ŷ 1j−1ŷ = arg max Y P ŷ j |x 1N , y j−kj−1ŷ j where P ŷ j |x N1 , y j−1j−k has an exponential form of the type (2)."
"We also used the standard Markov as-sumption that the probability P ŷ j |x N1 , ŷ 1j−1 only depends on the previous k classifications."
"This model is similar to the MEMM model[REF_CITE], but it does not separate the probability into generation probabilities and transition probabil-ities, and, crucially, has access to “future” observed features (i.e. it can examine the entire x N1 sequence, though in practice it will only examine some small part of it) – which is one way of eliminating label"
"The approach proposed in this article requires a mention detection system build in a resource-rich language, and a translation from the source lan-guage to the resource-rich language, together with word alignment."
"This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statis-tical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be[REF_CITE]."
We also find that there is a large number of parallel corpora available these days which cover many lan-guage pairs.
"For example, for the European Union’s 23 official languages we find 253 language pairs; each document in one language might have to be translated in all other 22 languages."
"This is in ad-dition to parallel corpora one could get from books, including religious texts such as the Bible, that are translated to a large number of languages."
"On the other hand, even though mention detection system is important for many natural language processing applications, we still find lack of mention-annotated corpora in many languages."
"In the approach we pro-pose below, the annotated corpus used to train the mention detection classifier does not have to be part of a parallel corpus."
"To start the process, we first use a SMT system to translate the source unit (document or sentence) x N1 into the resource-rich language, yielding the se-quence ξ 1M = (ξ 1 , ξ 2 , . . . ξ M )."
"Taking the sequence of tokens ξ 1M as input, the MaxEnt classifier assigns a mention label to each token, building the label se-quence ψ 1M = (ψ 1 , ψ 2 . . . ψ M )."
"Using the SMT-produced word alignment between source text x N1 and translated text ξ M1[REF_CITE],we propagate the target labels ψ M1 to the source language build-ing the label sequence ỹ 1N = (ỹ 1 , ỹ 2 . . . ỹ N ). [Footnote_8]"
8 Or by using Giza++ if your favorite engine does not give you word alignment.
"As an example, if a sequence of tokens in the resource-rich language ξ i ξ i+1 ξ i+2 is aligned to x j x j+1 in the source language and if ξ i ξ i+1 ξ i+2 is tagged as a lo-cation mention, then the sequence x j x j+1 can be la-beled as a location mention: B-LOC, I-LOC."
"Hence, each token x i in x N1 is tagged with a corresponding propagated label ỹ i in ỹ 1N , ỹ i = φ i, A, ψ 1M , where A is the alignment between the source and resource-rich languages."
"In cases when the alignment is 1-to-1 the function becomes the identity, but one can imagine different scenarios which can be used in many-to-many alignment cases."
"The alignement we use in this paper is 1-to-many ({1...n}) from the source language (eg., Arabic) to the resource-rich language (e.g., English)."
"Once we use SMT word alignment to propagate label sequence ψ 1M of ξ 1M to the corresponding text x N1 in the target language, we end up with a sequence of labels ỹ 1N where for each token x i in x 1N we attach its label ỹ i in ỹ 1N ."
"Hence, we label te entire span and if the strategy results in two mentions where one contains the other, we elim-inate the inner one."
Figure 1 displays the alignment between a Span-ish sentence and its English automatic translation.
It also shows a good match between the gold-standard tags in Spanish and the automatically extracted tags in English.
"There are three ways in which we propose using these propagated labels: 1. Consider ỹ N1 as the result of propagating the detected mentions in the original text x N1 , basi-cally selecting y N1 = ỹ N1 ."
"This situation corre-sponds to a case where no resources (annotated data) are available/needed on the source side, where the propagated labels are the output of the system. 2. Use the label sequence ỹ 1N as an additional fea-ture in the MaxEnt framework when predicting P y j |x N1 ,y j−kj−1 , together with other features built from resources available on the source language."
"We will call this model CDP (Con-text Dependent Propagation). 3. Starting with a large corpus (possibly including the training data), translate it into the resource-rich language and run mention detection."
Then select the word sequences in the source lan-guage associated with the found mentions in the translation and add them to a machine- generated gazetteer G [Footnote_9] .
9 This is in fact a way to automatically construct a source-side mention dictionary.
This gazetteer G is then used to construct features for classification.
We will call this model CIP (Context Independent Propagation).
"From a runtime point of view, the CIP method has the advantage that there is no need to perform ma-chine translation, and it can incorporate data from a very large amount of text."
"The CDP method, on the other hand, has the advantage that features are com-puted in context, and will not fire unless the corre-sponding mentions were found in the translated ver-sion (hence the name)."
"Of course, the CDP method can incorporate features generated in the dictionary G."
The experimental section analyzes the impact of each of these techniques on mention detection task performance.
"Experiments are conducted on the[REF_CITE]data sets [Footnote_10] , in four languages: Arabic, Chinese, English, and Spanish."
10 Same data as[REF_CITE].
"This data is selected from a variety of sources (broadcast news, broadcast conversations, newswire, web log, newswire, conversational tele-phony) and is labeled with 7 types: person, organi-zation, location, facility, GPE (geo-political entity), vehicle and weapon."
"Besides mention level informa-tion, also labeled are coreference between the men-tions, relations, events, and time resolution."
"Since the evaluation tests set are not publicly available, we have split the publicly available train-ing corpus into an 85%/15% data split."
"To facilitate future comparisons with work presented here, and to simulate a realistic scenario, the splits are created based on article dates: the test data is selected as the latest 15% of the data in chronological order, in each of the covered genres."
"This way, the documents in the training and test data sets do not overlap in time, and the content of the test data is more recent than the training data."
Table 1 presents the number of documents in the training/test datasets for each of the four languages.
"While performance on the ACE data is usually evaluated using a special-purpose measure - the ACE value metric[REF_CITE], given that we are interested in the mention detection task only, we decided to use the more intuitive and popular (un-weighted) F-measure, the harmonic mean of preci-sion and recall."
From the set of four languages[REF_CITE]we will unsurprisingly select English as the resource-rich language.
"Table 2 shows the performance of mention detection systems in all 4 languages one can obtain by using all available resources in that language, including lexical (words and morphs in a 3-word window, prefixes and suffixes of length up to 4, WordNet[REF_CITE]for English), syntac-tic (POS tags, text chunks), and the output of other information extraction models."
"Results show that the English mention detection system has a better performance when compared to systems dealing with other languages such as Ara-bic, Chinese and Spanish."
These results are not un-expected since the English model has access to a larger training data and uses richer set of informa-tion such as WordNet[REF_CITE]and the output of a larger set of information extraction models.
"To show the effectiveness of cross-language mention propagation information in improving mention de-tection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very com-petitive performance in terms[REF_CITE]."
"To give an idea of the SMT performance, Table 3 shows the performance of the translation systems on the three language pairs, computed on standard test sets."
The Arabic to English SMT system is similar to the one described[REF_CITE]; it has 0.55 BLEU score[REF_CITE]Arabic-English machine translation evaluation test set.
The Chi-nese to English SMT system has similar architecture to the one described[REF_CITE].
This system obtains a score of 0.32 cased BLUE[REF_CITE]Arabic-English machine trans-lation evaluation test set.
The Spanish to English SMT system is similar to the one described[REF_CITE]; it has a 0.55 BLEU score on the final text edition of the European Parliament Plenary Speech corpus[REF_CITE]evaluation.
"As mentioned earlier, these three SMT systems have very compet-itive performance and are ranked among top 2 sys-tems participating to NIST or TC-STAR evaluations."
"Also, the English mention detection system used for experiments has an F-measure of 82.7 and that has very competitive results among systems participat-ing in the[REF_CITE]evaluation."
"Experiments are conducted under several con-ditions in order to investigate the effectiveness of our approach in improving mention detection sys-tem performance on languages with different levels of resource availability (from simple to more com-plex): 1. the system does not have access to any train-ing data in the source language (no resources needed besides the MT system); 2. the system has access to only lexical informa-tion (information that can be directly derived exclusively from mention-labeled text); 3. the system has access to lexical and syntactic (e.g., POS tags, text chunks) information (re-quires mention-labeled text, and models to pre-dict POS tags, etc); 4. the system that has access to lexical, syntactic, and semantic information (requires even more models and labeled data)."
The rest of this section examines in detail these four cases.
"To measure whether the improvement in per-formance of a particular system over another one is statistically significant or not, we use the stratified bootstrap re-sampling significance test[REF_CITE]."
This approach was used in the named entity recognition shared task[REF_CITE][URL_CITE]2002).
"In the following tables, we add a dagger sign † to results that are not statistically significant when compared to the baseline results."
"In this first case, as described in Section 4, the men-tion labels in the source language are obtained di-rectly through the alignment from the mentions in the translated text."
"This is a very simple scenario, which can be implemented with ease, and, as we will see, yields reasonable performance out-of-the-box."
Experimental results presented in Table 4 show the performance of applying this information trans-fer approach.
"For each source language (Arabic, Chinese, or Arabic), we show the performance of propagating mentions from the English text."
"Even though no training data to build a source language mention classifier is available, we still can detect mentions with reasonably high accuracy."
"We con-sider the obtained accuracy as reasonably good be-cause, as an example, the performance of a sys-tem that attaches to every word its most frequent label (unigram) is around 25% F-measure on Ara-bic."
"Results in Table 4 also show that even though the Chinese-to-English SMT system is lower in term of BLEU than the Arbic-to-English SMT system (0.32 vs. 0.55), performance of the cross-language propagation from English mention detection system onto Chinese is better than the performance of the propagation from English mention detection system onto Arabic."
One reason for this is that we notice that Chinese-to-English SMT system translates and aligns ACE categories better than Arabic-to-English SMT system.
"In this section, we consider the case when we have available training data in the source language to be able to train a statistical classifier."
We also consider that the classifier has access to lexical information only.
Our goal here is to study the effectiveness of adding cross-language mention propagation infor-mation to improve mention detection performance on languages with limited resources.
"Table 5 shows the performance of the 3 languages with and without cross-language mention propaga-tion information from English, with the 3 propa-gation methods described in Section 4."
One can see that propagating mention propagation informa-tion results in system performance increase [Footnote_12] .
12 Only systems’ performance marked with † is not statisti-cally significantly better.
"When systems use the CIP method, no improvement can be observed on Arabic and Chinese, while a small improvement of 0.5F point is obtained on Spanish (74.5 vs. 75.0)."
"In contrast, when systems use the CDP method an improvement is obtained in recall – which is to be expected, given the method – lead-ing to systems with better performance in terms of F-measure: 1.6F points improvement for Arabic, 1.5F points improvement for Chinese and almost 3F points improvement for Spanish."
The results for all the CDP transfers and the CIP for Spanish are statis-tically significant.
"We represent in Table 6 mention detection system performance when syntactic resources are available in the source language, in addition to lexical re- sources available in the previous Subsection."
This experiment is important because it tests the effec-tiveness of the propagation approach in improving performance on languages with a typical level of re-sources.
"Results show that even in this situation, the use of cross language mention propagation informa-tion still lead to considerable improvement: using the CDP transfer method yields improvements from 1.1F in Chinese to 2.6F in Spanish."
"Similar to the previous section, the use of CIP information did not improve performance significantly on Arabic (77.5 vs. 77.1) and Chinese (75.5 vs. 75.5) systems, but we notice an improvement[REF_CITE]."
This final section investigates whether the access to cross-language mention propagation information can still improve the performance of existing com-petitive mention detection systems trained on lan-guages with large resources.
"In this case, systems have access to a full array of lexical, syntax, seman-tic information, including the output from other in-formation extraction models."
"Table 7 presents the performance of mention detection systems on the three languages, in the familiar 3 propagation meth-ods: again, results show that better performance is obtained when cross language mention informa-tion is used."
"Under CIP, almost no change in terms of performance is obtained for Arabic and Span- ish, though a slight improvement can be observed for Chinese (76.9F vs. 75.8F)."
"When CDP is used the performance of mention detection systems is im-proved by 0.9F for Arabic (80.9 vs. 80.0), 2.3F for Chinese (78.1F vs. 75.8F) and 1.9F for Span-ish (78.1 vs. 76.2F)."
"Once again, the results prove that the use of cross language mention propagation information, especially through CDP, is effective in improving the performance even in this case."
"By comparing results across tables, one can note that systems having access to only lexical and cross language mention propagation information are as ef-fective as systems having access to large set of in-formation."
"For Chinese, we obtain a performance of 75.8F when the system has access to lexical, syntac-tic and output of other information extraction mod-els."
"On the other hand, the same system has a slightly better performance of 76.0 when it has ac-cess to lexical and cross language mention propa-gation information."
"The same behavior is observed for Spanish, we obtain a performance of 76.2F when the system has access to lexical, syntactic and output of other information extraction models; compared to 77.4F when lexical and cross language mention in-formation are used."
This is not true for Arabic where having access to larger set of information led to bet-ter performance when compared to systems having access to lexical information and CDP information (80.0F vs. 78.0).
"We attribute this difference to the fact that in Arabic we use the output of larger number of information extraction models, and con-sequently a richer set of information."
The other observation that is worth making is that the improvement in performance has a decreasing tendency as more resources are available.
"The per-formance gain for CDP in Arabic goes from 1.6 to 1.5 to 0.9, and the one on Spanish goes from 2.9 to 2.6 to 1.9."
"The one on Chinese follows part of this trend, as it goes from 1.4 to 1.1 to 2.3."
"While the evidence here is not definitive, one can indeed note the reduced effectiveness of the method as more re-sources are available, which was indeed what we ex-pected."
"Results obtained by all these experiments help answer an important question: when trying to im-prove mention detection systems in a resource-poor language, should we invest in building resources or should we use propagation from a resource-rich lan-guage to (at least) bootstrap the process?"
The answer seems to be the latter.
"This paper presents a new approach to mention de-tection in low, medium or high-resource languages, which benefits from projecting the output from a resource-rich language such as English."
"We show that even when no training data is available in one source language, we can still build a decently per-forming baseline mention detection system by only using resources from English."
"This approach re-quires a mention detection system on a resource-rich language and an SMT system that translate text from the source to the resource-rich language, both of which can be attained."
"In cases when large resources are available in the source language, our cross language mention propa-gation technique is still able to further improve men-tion detection system performance."
"Experiments performed on the four languages[REF_CITE]with English chosen as the resource-rich language, show consistent and significant improvements across con-ditions and levels of linguistic sophistication."
"The experiments are conducted on clearly specified par-titions of the[REF_CITE]data set, so future compar-isons against the presented work can be correctly and accurately made."
"We also note that systems that have access to lexical and cross language men-tion propagation information are as accurate as those that have access to lexical, syntactic and output of other information extraction models in the source language (but no cross-language resources)."
"As fu-ture work, we plan to extend this work to use semi-supervised and unsupervised approaches that can make use of cross-language information propaga-tion."
"We believe that it is important for the research community to continue to invest in building better resources in “source” languages, as it looks the most promising approach."
"However, using a propagation approach can definitely help bootstrap the process."
B is the de facto standard for evaluation and development of statistical machine trans-lation systems.
We describe three real-world situations involving comparisons between dif-ferent versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd.
"These situ-ations arise because B lacks the property of decomposability, a property which is also computationally convenient for various appli-cations."
We propose a very conservative modi-fication to B and a cross between B and word error rate that address these issues while improving correlation with human judgments.
"B[REF_CITE]was one of the first au-tomatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics[REF_CITE], it remains the standard in the statistical MT literature."
"Both cases involve comparisons between statistical MT systems and other translation meth-ods (human post-editing and a rule-based MT sys-tem), and they recommend that the use of B be restricted to comparisons between related systems or different versions of the same systems."
"In B’s de-fense, comparisons between different versions of the same system were exactly what B was designed for."
"However, we show that even in such situations, difficulties with B can arise."
We illustrate three ways that properties of B can be exploited to yield improvements that are questionable or even absurd.
All of these scenarios arose in actual prac-tice and involve comparisons between different ver-sions of the same statistical MT systems.
"They can be traced to the fact that B is not decomposable at the sentence level: that is, it lacks the property that improving a sentence in a test set leads to an increase in overall score, and degrading a sentence leads to a decrease in the overall score."
"This prop-erty is not only intuitive, but also computationally convenient for various applications such as transla-tion reranking and discriminative training."
"We pro-pose a minimal modification to B that reduces its nondecomposability, as well as a cross between B and word error rate (WER) that is decompos-able down to the subsentential level (in a sense to be made more precise below)."
Both metrics correct the observed problems and correlate with human judg-ments better than B.
"Let g k (w) be the multiset of all k-grams of a sentence w. We are given a sequence of candidate translations c to be scored against a set of sequences of reference translations, {r j } = r 1 , . . . , r R : c = c 1 , c 2 , c 3 , . .. , c N r 1 = r 11 , r 12 , r 31 , . . . , r 1N . . . r R = r 1R , r 2R , r R3 , . .. , r RN"
"Then the B score of c is defined to be 4 Y 1 B(c, {r j }) = pr k (c, {r j }) 4 × bp(c, {r j }) (1) k=[Footnote_1] where 1"
"1 We use the following definitions about multisets: if X is a multiset, let # X (a) be the number of times a occurs in X. Then: X"
"P g (c ) ∩ S j g k (r ij ) i k i pr k (c, {r j }) = (2) P |g (c )| i k i is the k-gram precision of c with respect to {r j }, and bp(c, r), known as the brevity penalty, is defined as follows."
Let φ(x) = exp(1 − 1/x).
"In the case of a single reference r, ( P |c i |)! bp(c, r) = φ min 1, P i (3) i |r i |"
"In the multiple-reference case, the length |r i | is re-placed with an effective reference length, which can be calculated in several ways. • In the original definiti[REF_CITE], it is the length of the reference sentence whose length is closest to the test sentence. • In the NIST definition, it is the length of the shortest reference sentence. • A third possibility would be to take the average length of the reference sentences."
"The purpose of the brevity penalty is to prevent a system from generating very short but precise translations, and the definition of effective reference length impacts how strong the penalty is."
The NIST definition is the most tolerant of short translations and becomes more tolerant with more reference sen-tences.
The original definition is less tolerant but has the counterintuitive property that decreasing the length of a test sentence can eliminate the brevity penalty.
"Using the average reference length seems attractive but has the counterintuitive property that |X| ≡ # X (a) a # X∩Y (a) ≡ min{# X (a), # Y (a)} # X∪Y (a) ≡ max{# X (a), # Y (a)} an exact match with one of the references may not get a 100% score."
"Throughout this paper we use the NIST definition, as it is currently the definition most used in the literature and in evaluations."
The brevity penalty can also be seen as a stand-in for recall.
The fraction P i P |c i | in the definition of i |r i | the brevity penalty (3) indeed resembles a weak re-call score in which every guessed item counts as a match.
"However, with recall, the per-sentence score |c i | would never exceed unity, but with the brevity |r i | penalty, it can."
"This means that if a system generates a long translation for one sentence, it can generate a short translation for another sentence without fac-ing a penalty."
"This is a serious weakness in the B metric, as we demonstrate below using three scenar-ios, encountered in actual practice."
We are aware of two methods that have been pro-posed for significance testing with B: bootstrap resampling[REF_CITE]and the sign test[REF_CITE].
"In bootstrap re-sampling, we sample with replacement from the test set to synthesize a large number of test sets, and then we compare the performance of two systems on those synthetic test sets to see whether one is better 95% (or 99%) of the time."
Suppose we want to determine whether a set of outputs c from a test system is better or worse than a set of baseline outputs b.
"The sign test requires a function f (b i , c i ) that indicates whether c i is a better, worse, or same-quality translation relative to b i ."
"However, because B is not defined on single sentences, Collins et al. use an approximation: for each i, form a compos-ite set of outputs b 0 = {b 1 , . . . , b i−1 , c i , b i+1 , . . . , b N }, and compare the B scores of b and b 0 ."
"The goodness of this approximation depends on to what extent the comparison between b and b 0 is dependent only on b i and c i , and independent of the other sentences."
"However, B scores are highly context-dependent: for example, if the sentences in b are on average words longer than the reference sentences, then c i can be as short as (N − 1) words shorter than r i without incurring the brevity penalty."
"Moreover, since the c i are substituted in one at a time, we can do this for all of the c i ."
"Hence, c could have a disastrously low B score (because of the brevity penalty) yet be found by the sign test to be significantly better than the baseline."
"We have encountered this situation in practice: two versions of the same system with B scores of 29.6 (length ratio 1.02) and 29.3 (length ratio 0.97), where the sign test finds the second system to be sig-nificantly better than the first (and the first system significantly better than the second)."
"Clearly, in or-der for a significance test to be sensible, it should not contradict the observed scores, and should certainly not contradict itself."
"In the rest of this paper, except where indicated, all significance tests are performed using bootstrap resampling."
"For several years, much statistical MT research has focused on translating newswire documents."
One likely reason is that the DARPA TIDES program used newswire documents for evaluation for several years.
But more recent evaluations have included other genres such as weblogs and conversation.
"The conventional wisdom has been that if one uses a single statistical translation system to translate text from several different genres, it may perform poorly, and it is better to use several systems optimized sep-arately for each genre."
"However, if our task is to translate documents from multiple known genres, but they are evaluated together, the B metric allows us to use that fact to our advantage."
"To understand how, notice that our system has an optimal number of words that it should generate for the entire corpus: too few and it will be penalized by B’s brevity penalty, and too many increases the risk of additional non-matching k-grams."
But these words can be distributed among the sentences (and genres) in any way we like.
"In-stead of translating sentences from each genre with the best genre-specific systems possible, we can generate longer outputs for the genre we have more confidence in, while generating shorter outputs for the harder genre."
"This strategy will have mediocre performance on each individual genre (according to both intuition and B), yet will receive a higher B score on the combined test set than the com- bined systems optimized for each genre."
"In fact, knowing which sentence is in which genre is not even always necessary."
"In one recent task, we translated documents from two different genres, without knowing the genre of any given sentence."
"The easier genre, newswire, also tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs."
"For ex-ample, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Ara-bic word, but the weblog reference set had 1.52 En-glish words per Arabic word."
"Thus, a system that is uniformly verbose across both genres will appor-tion more of its output to newswire than to weblogs, serendipitously leading to a higher score."
This phe-nomenon has subsequently been observed[REF_CITE]as well.
"We trained three Arabic-English syntax-based statistical MT systems[REF_CITE]using max-B training[REF_CITE]: one on a newswire development set, one on a we-blog development set, and one on a combined devel-opment set containing documents from both genres."
"We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genre-specific system, and (2) all documents with the sys-tem trained on the combined (mixed-genre) devel-opment set."
"In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings."
"The genre-specific systems each outperform the mixed system on their own genre as expected, but when the same results are combined, the mixed sys-tem’s output is a full B point higher than the com-bination of the genre-specific systems."
"This is be-cause the mixed system produces outputs that have about 1.35 English words per Arabic word on av-erage: longer than the shortest newswire references, but shorter than the weblog references."
"The mixed system does worse on each genre but better on the combined test set, whereas, according to intuition, a system that does worse on the two subsets should also do worse on the combined test set."
A third way to take advantage of the B metric is to permit an MT system to delete arbitrary words in the input sentence.
We can do this by introduc-ing new phrases or rules into the system that match words in the input sentence but generate no output; to these rules we attach a feature whose weight is tuned during max-B training.
Such rules have been in use for some time but were only recently discussed[REF_CITE].
"When we add word-deletion rules to our MT sys-tem, we find that the B increases significantly (Table 6, line 2)."
Figure 1 shows some examples of deletion in Chinese-English translation.
"The first sentence has a proper name, ¦&lt;[[/maigesaisai ‘Magsaysay’, which has been mistokenized into four tokens."
"The baseline system attempts to translate the first two phonetic characters as “wheat Georgia,” whereas the other system simply deletes them."
"On the other hand, the second sentence shows how word deletion can sacrifice adequacy for the sake of flu-ency, and the third sentence shows that sometimes word deletion removes words that could have been translated well (as seen in the baseline translation)."
Does B reward word deletion fairly?
We note two reasons why word deletion might be desirable.
"First, some function words should truly be deleted: for example, the Chinese particle „/de and Chinese measure words often have no counterpart in English[REF_CITE]."
"Second, even content word deletion might be helpful if it allows a more fluent translation to be assembled from the remnants."
"We observe that in the above experiment, word deletion caused the absolute number of k-gram matches, and not just k-gram precision, to increase for all 1 ≤ k ≤ 4."
Human evaluation is needed to conclusively de-termine whether B rewards deletion fairly.
"But to control for these potentially positive effects of dele-tion, we tested a sentence-deletion system, which is the same as the word-deletion system but con-strained to delete all of the words in a sentence or none of them."
"This system (Table 6, line 3) deleted 8–10% of its input and yielded a B score with no significant decrease (p ≥ 0.05) from the base-line system’s."
"Given that our model treats sentences independently, so that it cannot move information from one sentence to another, we claim that dele-tion of nearly 10% of the input is a grave translation deficiency, yet B is insensitive to it."
What does this tell us about word deletion?
"While acknowledging that some word deletions can im- prove translation quality, we suggest in addition that because word deletion provides a way for the system to translate the test set selectively, a behavior which we have shown that B is insensitive to, part of the score increase due to word deletion is likely an artifact of B."
Are other metrics susceptible to the same problems as the B metric?
"In this section we examine sev-eral other popular metrics for these problems, pro-pose two of our own, and discuss some desirable characteristics for any new MT evaluation metric."
We ran a suite of other metrics on the above problem cases to see whether they were affected.
"In none of these cases did we repeat minimum-error-rate train-ing; all these systems were trained using max-B. The metrics we tested were: • METEOR[REF_CITE], version 0.6, using the exact, Porter-stemmer, and Word-Net synonmy stages, and the optimized param-eters α = 0.81, β = 0.83, γ = 0.28 as reported[REF_CITE]. • GTM[REF_CITE], version 1.4, with default settings, except e = 1.2, following the[REF_CITE]shared task[REF_CITE]. • MS[REF_CITE], more specifi-cally MS n , which skips the dependency re-lations."
"On the sign test (Table 2), all metrics found sig-nificant differences consistent with the difference in score between the two systems."
"The problem related to genre-specific training does not seem to affect the other metrics (see Table 4), but they still manifest the unintuitive result that genre-specific training is sometimes worse than mixed-genre training."
"Finally, all metrics but GTM disfavored both word deletion and sentence deletion (Table 7)."
A very conservative way of modifying the B met-ric to combat the effects described above is to im- pose a stricter brevity penalty.
"In Section 2, we pre-sented the brevity penalty as a stand-in for recall, but noted that unlike recall, the per-sentence score |c i | can exceed unity."
This suggests the simple fix of |r i | clipping the per-sentence recall scores in a similar fashion to the clipping of precision scores:
"P min {|c i |, |r i |}! bp(c, r) = φ i (4) P |r | i i"
"Then if a translation system produces overlong translations for some sentences, it cannot use those translations to license short translations for other sentences."
Call this revised metric B- (for B with strict brevity penalty).
We can test this revised definition on the prob-lem cases described above.
"Table 2 shows that B-  resolves the inconsistency observed between B and the sign test, using the example test sets from Section 3.1 (no max-B- training was per-formed)."
Table 5 shows the new scores of the mixed-genre example from Section 3.2 after max-B- training.
These results fall in line with intuition— tuning separately for each genre leads to slightly better scores in all cases.
"Finally, Table 8 shows the B- scores for the word-deletion example from Section 3.3, using both max-B training and max- B- training."
"We see that B- reduces the benefit of word deletion to an insignificant level on the test set, and severely punishes sentence deletion."
"When we retrain using max-B-, the rate of word deletion is reduced and sentence deletion is all but eliminated, and there are no significant differ-ences on the test set."
All of the problems we have examined—except for word deletion—are traceable to the fact that B is not a sentence-level metric.
"Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems."
Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property.
"Of the metrics surveyed in the[REF_CITE]evaluation-evaluati[REF_CITE], at least the following metrics have this property: WER[REF_CITE], TER[REF_CITE], and ParaEval-Recall[REF_CITE]."
"Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk de-coding[REF_CITE], selecting ora-cle translations for discriminative reranking[REF_CITE], and sentence-by-sentence comparisons of outputs during error analysis."
"A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and de-nominator[REF_CITE]; this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems il-lustrated above."
"The remaining issue, word deletion, is more dif-ficult to assess."
It could be argued that part of the gain due to word deletion is caused by B allow-ing a system to selectively translate those parts of a sentence on which higher precision can be ob-tained.
"It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be de-composable into subsentential scores, and we make no such claim."
"However, there is again a dovetail-ing engineering concern which is quite legitimate."
"If one wants to select the minimum-Bayes-risk trans-lation from a lattice (or shared forest) instead of an n-best list[REF_CITE], or to select an or-acle translation from a lattice[REF_CITE], or to perform discriminative training on all the examples contained in a lattice[REF_CITE], one would need a metric that can be calculated on the edges of the lattice."
"Of the metrics surveyed in the[REF_CITE]evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate[REF_CITE]."
"Here, we deal with the related word recognition rate[REF_CITE],"
WRR = 1 − WER = 1 − min I +
"D + S |r| = max M − I (5) |r| where I is the number of insertions, D of deletions, S of substitutions, and M = |r| − D − S the number of matches."
"The dynamic program for WRR can be formulated as a Viterbi search through a finite-state automaton: given a candidate sentence c and a refer-ence sentence r, find the highest-scoring path match-ing c through the automaton with states 0, . . . , |r|, initial state 0, final state |r|, and the following transi- tions (a ? matches any symbol):"
For 0 ≤ i &lt; |r|: r i+1 :1 i −−−−→ i + 1 match :0 i −−→ i + 1 deletion ? :0 i −−→ i + 1 substitution
For 0 ≤ i ≤ |r|: ?:−1 i −−−−→ i insertion
"This automaton can be intersected with a typical stack-based phrase-based decoder lattice[REF_CITE]or CKY-style shared forest[REF_CITE]in much the same way that a language model can, yielding a polynomial-time algorithm for extracting the best-scoring translation from a lattice or forest[REF_CITE]."
"Intuitively, the reason for this is that WRR, like most metrics, implicitly constructs a word alignment between c and r and only counts matches between aligned words; but unlike other metrics, this alignment is constrained to be mono-tone."
"We can combine WRR with the idea of k-gram matching in B to yield a new metric, the 4-gram recognition rate:"
"P 4 M k − αI − βD 4-GRR = max k=1 (6) P 4 |g (r)| k=1 k where M k is the number of k-gram matches, α and β control the penalty for insertions and deletions, and g k is as defined in Section 2."
"We presently set α = 1,β = 0 by analogy with WRR, but explore other settings below."
"To calculate 4-GRR on a whole test set, we sum the numerators and denominators as in micro-averaged recall."
"The 4-GRR can also be formulated as a finite-state automaton, with states {(i, m) | 0 ≤ i ≤ |r|, 0 ≤ m ≤ 3}, initial state (0, 0), final states (|r|, m), and the following transitions:"
"For 0 ≤ i &lt; |r|, 0 ≤ m ≤ 3: (i, m) −−−−−−→ r i+1 : m+1 (i + 1, min{m + 1, 3}) match (i, m) −−−→ :−β (i + 1, 0) deletion (i, m) −−→ ?:0 (i + 1, 0) substitution"
"For 0 ≤ i ≤ |r|, 0 ≤ m ≤ 3: (i, m) −−−−→ ?: −α (i, 0) insertion"
Therefore 4-GRR can also be calculated efficiently on lattices or shared forests.
"We did not attempt max-4-GRR training, but we evaluated the word-deletion test sets obtained by max-B and max-B- training using 4-GRR."
The results are shown in Table 7.
"In general, the re-sults are very similar to B- except that 4-GRR sometimes scores word deletion slightly lower than baseline."
"The shared task of the 2007 Workshop on Statistical Machine Translati[REF_CITE]was conducted with several aims, one of which was to measure the correlation of several automatic MT evaluation metrics (including B) against hu-man judgments."
"The task included two datasets (one drawn from the Europarl corpus and the other from the News Commentary corpus) and across three lan-guage pairs (from German, Spanish, and French to English, and back)."
"In our experiments, we focus on the tasks where the target language is English."
"For human evaluations of the MT submissions, four different criteria were used: • Adequacy: how much of the meaning ex-pressed in the reference translation is also ex-pressed in the hypothesis translation. • Fluency: how well the translation reads in the target language. • Rank: each translation is ranked from best to worst, relative to the other translations of the same sentence. • Constituent: constituents are selected from source-side parse-trees, and human judges are asked to rank their translations."
"We scored the workshop shared task submissions with B- and 4-GRR, then converted the raw scores to rankings and calculated the Spearman cor-relations with the human judgments."
"Table 1 shows the results along with B and the three metrics that achieved higher correlations than B: semantic role overlap[REF_CITE], ParaE-val recall[REF_CITE], and METEOR[REF_CITE]."
We find that both our proposed metrics correlate with human judgments better than B does.
"However, recall the parameters α and β in the def-inition of 4-GRR that control the penalty for inserted and deleted words."
"Experimenting with this param-eter reveals that α = −0.9,β = 1 yields a corre-lation of 78.9%."
"In other words, a metric that un-boundedly rewards spuriously inserted words corre-lates better with human judgments than a metric that punishes them."
We assume this is because there are not enough data points (systems) in the sample and ask that all these figures be taken with a grain of salt.
"As a general remark, it may be beneficial for human-correlation datasets to include a few straw-man sys-tems that have very short or very long translations."
We have described three real-world scenarios in-volving comparisons between different versions of the same statistical MT systems where B gives counterintuitive results.
All these issues center around the issue of decomposability: the sign test fails because substituting translations one sentence at a time can improve the overall score yet substitut-ing them all at once can decrease it; genre-specific training fails because improving the score of two halves of a test set can decrease the overall score; and sentence deletion is not harmful because gener-ating empty translations for selected sentences does not necessarily decrease the overall score.
"We proposed a minimal modification to B, called B-, and showed that it ameliorates these problems."
"We also proposed a metric, 4-GRR, that is decomposable at the sentence level and is therefore guaranteed to solve the sign test, genre-specific tun-ing, and sentence deletion problems; moreoever, it is decomposable at the subsentential level, which has potential implications for evaluating word deletion and promising applications to translation reranking and discriminative training."
We present Minimum Bayes-Risk (MBR) de-coding over translation lattices that compactly encode a huge number of translation hypothe-ses.
We describe conditions on the loss func-tion that will enable efficient implementation of MBR decoders on lattices.
We introduce an approximation to the BLEU score[REF_CITE]that satisfies these condi-tions.
The MBR decoding under this approx-imate BLEU is realized using Weighted Fi-nite State Automata.
"Our experiments show that the Lattice MBR decoder yields mod-erate, consistent gains in translation perfor-mance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks."
We conduct a range of experiments to understand why Lat-tice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.
"Statistical language processing systems for speech recognition, machine translation or parsing typically employ the Maximum A Posteriori (MAP) deci-sion rule which optimizes the 0-1 loss function."
"In contrast, these systems are evaluated using metrics based on string-edit distance (Word Error Rate), n-gram overlap (BLEU score[REF_CITE]), or precision/recall relative to human annotations."
Minimum Bayes-Risk (MBR) decoding[REF_CITE]aims to address this mismatch by se-lecting the hypothesis that minimizes the expected error in classification.
Thus it directly incorporates the loss function into the decision criterion.
"The ap-proach has been shown to give improvements over the MAP classifier in many areas of natural lan-guage processing including automatic speech recog-niti[REF_CITE], machine transla-ti[REF_CITE], bilingual word alignment[REF_CITE], and parsing[REF_CITE]."
"In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses."
This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU.
A different MBR inspired decoding approach is pursued[REF_CITE]for machine translation using Synchronous Context Free Gram-mars.
A forest generated by an initial decoding pass is rescored using dynamic programming to maxi-mize the expected count of synchronous constituents in the tree that corresponds to the translation.
"Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expected BLEU."
In this paper we explore a different strategy to perform MBR decoding over Translation Lat-tices[REF_CITE]that compactly encode a huge number of translation alternatives relative to an N-best list.
This is a model-independent approach in that the lattices could be produced by any statis-tical MT system — both phrase-based and syntax-based systems would work in this framework.
We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding.
We describe an approximation to the BLEU score[REF_CITE]that will satisfy these condi-tions.
Our Lattice MBR decoding is realized using Weighted Finite State Automata.
We expect Lattice MBR decoding to improve upon N-best MBR primarily because lattices con-tain many more candidate translations than the N-best list.
This has been demonstrated in speech recogniti[REF_CITE].
We conduct a range of translation experiments to analyze lattice MBR and compare it with N-best MBR.
An impor-tant aspect of our lattice MBR is the linear approxi-mation to the BLEU score.
We will show that MBR decoding under this score achieves a performance that is at least as good as the performance obtained under sentence-level BLEU score.
The rest of the paper is organized as follows.
We review MBR decoding in Section 2 and give the for-mulation in terms of a gain function.
"In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice."
The implementa-tion of lattice MBR with Weighted Finite State Au-tomata is presented in Section 4.
"In Section 5, we in-troduce the corpus BLEU approximation that makes it possible to perform efficient lattice"
An example of lattice MBR with a toy lattice is presented in Section 6.
We present lattice MBR experiments in Section 7.
A final discussion is pre-sented in Section 8.
Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model[REF_CITE].
We begin with a review of MBR decod-ing for Statistical Machine Translation (SMT).
Statistical MT[REF_CITE]can be described as a mapping of a word se-quence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder δ(F).
"If the reference transla-tion E is known, the decoder performance can be measured by the loss function L(E,δ(F))."
"Given such a loss function L(E, E 0 ) between an automatic translation E 0 and the reference E, and an under-lying probability model P (E|F ), the MBR decoder has the following form[REF_CITE]:"
Ê = argmin R(E 0 )
"E 0 ∈E = argmin X L(E, E 0 )P (E|F ), E 0 ∈E E∈E where R(E 0 ) denotes the Bayes risk of candidate translation E 0 under the loss function L."
"If the loss function between any two hypotheses can be bounded: L(E,E 0 ) ≤ L max , the MBR de-coder can be rewritten in terms of a gain function G(E, E 0 ) ="
"L max − L(E, E 0 ):"
"Ê = argmax X G(E, E 0 )P (E|F ). (1) E 0 ∈E E∈E"
"We are interested in performing MBR decoding under a sentence-level BLEU score[REF_CITE]which behaves like a gain function: it varies between 0 and 1, and a larger value reflects a higher similarity."
We will therefore use Equation 1 as the MBR decoder.
We note that E represents the space of transla-tions.
"For N-best MBR, this space E is the N-best list produced by a baseline decoder."
"We will investi-gate the use of a translation lattice for MBR decod-ing; in this case, E will represent the set of candi-dates encoded in the lattice."
"In general, MBR decoding can use different spaces for hypothesis selection and risk computa-tion: argmax and the sum in Equation 1[REF_CITE]."
"As an example, the hypothesis could be se-lected from the N-best list while the risk is com-puted based on the entire lattice."
"Therefore, the MBR decoder can be more generally written as fol-lows:"
"Ê = argmax X G(E, E 0 )P (E|F ), (2) E 0 ∈E h E∈E e where E h refers to the Hypothesis space from where the translations are chosen, and E e refers to the Evi-dence space that is used for computing the Bayes-risk."
We will present experiments (Section 7) to show the relative importance of these two spaces.
We now present MBR decoding on translation lat-tices.
A translation word lattice is a compact rep-resentation for very large N-best lists of transla-tion hypotheses and their likelihoods.
"Formally, it is an acyclic Weighted Finite State Acceptor (WFSA)[REF_CITE]consisting of states and arcs representing transitions between states."
Each arc is labeled with a word and a weight.
"Each path in the lattice, consisting of consecutive transitions begin-ning at the distinguished initial state and ending at a final state, expresses a candidate translation."
"Aggre-gation of the weights along the path 1 produces the weight of the path’s candidate H(E, F) according to the model."
"In our setting, this weight will imply the posterior probability of the translation E given the source sentence F : exp (αH(E, F ))"
P (E|F ) =
P (3) .
"E 0 ∈E exp (αH(E 0 , F ))"
"The scaling factor α ∈ [0, ∞) flattens the distribu-tion when α &lt; [Footnote_1], and sharpens it when α &gt; 1."
1 using the log semiring’s extend operator
"Because a lattice may represent a number of can-didates exponential in the size of its state set, it is of-ten impractical to compute the MBR decoder (Equa-tion 1) directly."
"However, if we can express the gain function G as a sum of local gain functions g i , then we now show that Equation 1 can be refactored and the MBR decoder can be computed efficiently."
We loosely call a gain function local if it can be ap-plied to all paths in the lattice via WFSA intersec-ti[REF_CITE]without significantly multiplying the number of states.
"In this paper, we are primarily concerned with lo-cal gain functions that weight n-grams."
"Let N = {w 1 , . . . , w |N| } be the set of n-grams and let a local gain function g w : E × E → R, for w ∈ N, be as follows: g w (E, E 0 ) = θ w # w (E 0 ) δ w (E), (4) where θ w is a constant, # w (E 0 ) is the number of times that w occurs in E 0 , and δ w (E) is 1 if w ∈ E and 0 otherwise."
"That is, g w is θ w times the number of occurrences of w in E 0 , or zero if w does not oc-cur in E. We first assume that the overall gain func-tion G(E, E 0 ) can then be written as a sum of local gain functions and a constant θ 0 times the length of the hypothesis E 0 ."
"G(E, E 0 ) = θ 0 |E 0 | + X g w (E, E 0 ) (5) w∈N = θ 0 |E 0 | + X θ w # w (E 0 ) δ w (E) w∈N"
"Given a gain function of this form, we can rewrite the risk (sum in Equation 1) as follows"
"X G(E, E 0 )P (E|F ) E∈E = X θ 0 |E 0 | + X θ w # w (E 0 ) δ w (E) P(E|F) E∈E w∈N = θ 0 |E 0 | + X θ w # w (E 0 ) X P(E|F), w∈N E∈E w where E w = {E ∈ E|δ w (E) &gt; 0} represents the paths of the lattice containing the n-gram w at least once."
The MBR decoder on lattices (Equation 1) can therefore be written as
Ê = argmax θ 0 |E 0 | +
X θ w # w (E 0 )p(w|E)o. (6) n E 0 ∈E w∈N
Here p(w|E) =
P E∈E w P(E|F) is the posterior probability of the n-gram w in the lattice.
"We have thus replaced a summation over a possibly exponen-tial number of items (E ∈ E) with a summation over the number of n-grams that occur in E, which is at worst polynomial in the number of edges in the lat-tice that defines E. We compute the posterior proba-bility of each n-gram w as: p(w|E) = X P(E|F) = Z(E w ), (7) Z(E) E∈E w where Z(E) = P E 0 ∈E exp(αH(E 0 , F)) (denomi-nator in Equation 3) and Z(E w ) ="
"P E 0 ∈E w exp(αH(E 0 , F ))."
Z(E) and Z(E w ) represent the sums [Footnote_2] of weights of all paths in the lattices E w and E respectively.
"2 in the log semiring, where log +(x, y) = log(e x + e y ) is the collect operator[REF_CITE]"
We now show how the Lattice MBR Decision Rule (Equation 6) can be implemented using Weighted Finite State Automata[REF_CITE].
There are four steps involved in decoding starting from weighted finite-state automata representing the candidate out-puts of a translation system.
We will describe these steps in the setting where the evidence lattice E e may be different from the hypothesis lattice E h (Equa-tion 2). 1. Extract the set of n-grams that occur in the ev-idence lattice E e .
"For the usual BLEU score, n ranges from one to four. 2. Compute the posterior probability p(w|E) of each of these n-grams. 3. Intersect each n-gram w, with an appropriate weight (from Equation 6), to an initially un-weighted copy of the hypothesis lattice E h . [Footnote_4]. Find the best path in the resulting automaton."
"4 in the (max, +) semiring[REF_CITE]"
"Computing the set of n-grams N that occur in a finite automaton requires a traversal, in topological order, of all the arcs in the automaton."
"Because the lattice is acyclic, this is possible."
Each state q in the automaton has a corresponding set of n-grams N q ending there. 1.
"For each state q, N q is initialized to { }, the set containing the empty n-gram. 2."
"Each arc in the automaton extends each of its source state’s n-grams by its word label, and adds the resulting n-grams to the set of its tar-get state. ( arcs do not extend n-grams, but transfer them unchanged.) n-grams longer than the desired order are discarded. 3. N is the union over all states q of N q ."
"Given an n-gram, w, we construct an automaton matching any path containing the n-gram, and in-tersect that automaton with the lattice to find the set of paths containing the n-gram (E w in Equation 7)."
"Suppose E represent the weighted lattice, we com-pute [Footnote_3] : E w = E ∩ (w w Σ ∗ ), where w = (Σ ∗ w Σ ∗ ) is the language that contains all strings that do not contain the n-gram w."
3 in the log semiring[REF_CITE]
The posterior probability p(w|E) of n-gram w can be computed as a ratio of the total weights of paths in E w to the total weights of paths in the original lattice (Equation 7).
"For each n-gram w ∈ N, we then construct an automaton that accepts an input E with weight equal to the product of the number of times the n-gram occurs in the input (# w (E)), the n-gram fac-tor θ w from Equation 6, and the posterior proba-bility p(w|E)."
The automaton corresponds to the weighted regular expressi[REF_CITE]: w̄(w/(θ w p(w|E)) w̄) ∗ .
We successively intersect each of these automata with an automaton that begins as an unweighted copy of the lattice E h .
This automaton must also incorporate the factor θ 0 of each word.
This can be accomplished by intersecting the unweighted lat-tice with the automaton accepting (Σ/θ 0 ) ∗ .
The resulting MBR automaton computes the total ex-pected gain of each path.
A path in this automa-ton that corresponds to the word sequence E 0 has cost: θ 0 |E 0 | + P w∈N θ w # w (E)p(w|E) (expression within the curly brackets in Equation 6).
"Finally, we extract the best path from the resulting automaton 4 , giving the lattice MBR candidate trans-lation according to the gain function (Equation 6)."
Our Lattice MBR formulation relies on the decom-position of the overall gain function as a sum of lo-cal gain functions (Equation 5).
We here describe a linear approximation to the log(BLEU score)[REF_CITE]which allows such a decomposi-tion.
This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hy-pothesis length.
Our strategy will be to use a first order Taylor-series approximation to what we call the corpus log(BLEU) gain: the change in corpus log(BLEU) contributed by the sentence relative to not including that sentence in the corpus.
"Let r be the reference length of the corpus, c 0 the candidate length, and {c n |1 ≤ n ≤ 4} the number of n-gram matches."
"Then, the corpus BLEU score B(r, c 0 , c n ) can be defined as follows[REF_CITE]: 4 r 1 X log c n log B = min 0, 1 − + , c 0 c 0 − ∆ n 4 n=1 4 r ≈ min 0, 1 − + 1 X log c n , c 0 4 c 0 n=1 where we have ignored ∆ n , the difference between the number of words in the candidate and the num- ber of n-grams."
"If L is the average sentence length in the corpus, ∆ n ≈ (n − 1) cL 0 ."
The corpus log(BLEU) gain is defined as the change in log(BLEU) when a new sentence’s (E 0 ) statistics are added to the corpus statistics:
"G = log B 0 − log B, where the counts in B 0 are those of B plus those for the current sentence."
We will assume that the brevity penalty (first term in the above approximation) does not change when adding the new sentence.
"In exper-iments not reported here, we found that taking into account the brevity penalty at the sentence level can cause large fluctuations in lattice MBR performance on different test sets."
We therefore treat only c n s as variables.
The corpus log BLEU gain is approximated by a first-order vector Taylor series expansion about the initial values of c n .
"N 0 G ≈ X(c n0 − c n ) ∂ log B , (8) n=0 ∂c 0n c 0n =c n where the partial derivatives are given by ∂ log B −1 = (9), ∂c 0 c 0 ∂ log B 1 = . ∂c n 4c n"
Substituting the derivatives in Equation 8 gives 4
"G = ∆ log B ≈ −∆c 0 + 1 X ∆c n , (10) c 0 4 c n n=1 where each ∆c n = c 0n − c n counts the statistic in the sentence of interest, rather than the corpus as a whole."
This score is therefore a linear function in counts of words ∆c 0 and n-gram matches ∆c n .
Our approach ignores the count clipping present in the exact BLEU score where a correct n-gram present once in the reference but several times in the hypoth-esis will be counted only once as correct.
Such an approach is also followed[REF_CITE].
"Using the above first-order approximation to gain in log corpus BLEU, Equation 9 implies that θ 0 , θ w from Section 3 would have the following values: −1 θ 0 = c 0 (11) 1 θ w = 4c |w| ."
We now describe how the n-gram factors[REF_CITE]are computed.
"The factors depend on a set of n-gram matches and counts (c n ; n ∈ {0, 1, 2, 3, 4})."
These factors could be obtained from a decoding run on a development set.
"However, do-ing so could make the performance of lattice MBR very sensitive to the actual BLEU scores on a partic-ular run."
"We would like to avoid such a dependence and instead, obtain a set of parameters which can be estimated from multiple decoding runs without MBR."
"To achieve this, we make use of the properties of n-gram matches."
It is known that the average n-gram precisions decay approximately exponentially with n[REF_CITE].
We now assume that the number of matches of each n-gram is a constant ratio r times the matches of the corresponding n − 1 gram.
"If the unigram precision is p, we can obtain the n-gram factors (n ∈ {1, 2, 3, 4})[REF_CITE]as a function of the parameters p and r, and the number of unigram tokens T : −1 (12) θ 0 ="
T 1 θ n = 4T p × r n−1
We set p and r to the average values of unigram pre-cision and precision ratio across multiple develop-ment sets.
"Substituting the above factors in Equa-tion 6, we find that the MBR decision does not de-pend on T ; therefore any value of T can be used."
Figure 1 shows a toy lattice and the final MBR au-tomaton (Section 4) for BLEU with a maximum n-gram order of 2.
We note that the MBR hypothesis (bcde) has a higher decoder cost relative to the MAP hypothesis (abde).
"However, bcde gets a higher ex-pected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda)."
This illustrates how a lattice can help select MBR translations that can differ from the MAP transla-tion.
We now present experiments to evaluate MBR de-coding on lattices under the linear corpus BLEU gain.
We start with a description of the data sets and the SMT system.
"We present our experiments on the constrained data track of the[REF_CITE]Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh) machine translation tasks. 5 In all language pairs, the parallel and monolingual data consists of all the allowed training sets in the constrained track."
"For each language pair, we use two development sets: one for Minimum Error Rate Training[REF_CITE], and the other for tun-ing the scale factor for MBR decoding."
"Our devel-opment sets consists of the[REF_CITE]/2003 evalu-ation sets for both aren and zhen, and[REF_CITE](NIST portion)/2003 evaluation sets for enzh."
We report results[REF_CITE]which is our blind test set.
Statistics computed over these data sets are re-ported in Table 1.
Our phrase-based statistical MT system is similar to the alignment template system described[REF_CITE].
The system is trained on parallel cor-pora allowed in the constrained track.
We first per-form sentence and sub-sentence chunk alignment on the parallel documents.
We then train word align-ment models[REF_CITE]using 6 Model-1 iterations and 6 HMM iterations.
An additional 2 it-erations of Model-4 are performed for zhen and enzh pairs.
Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework[REF_CITE].
An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments.
Several feature functions are then computed over the phrase-pairs. [URL_CITE]-gram word language models are trained on the allowed monolingual corpora.
Minimum Error Rate Training under BLEU is used for estimating approximately 20 feature function weights over the dev1 development set.
Translation is performed using a standard dy-namic programming beam-search decoder[REF_CITE]using two decoding passes.
The first de-coder pass generates either a lattice or an N-best list.
MBR decoding is performed in the second pass.
The MBR scaling parameter (α in Equation 3) is tuned on the dev2 development set.
We next report translation results from lattice MBR decoding.
All results will be presented on the[REF_CITE]evaluation sets.
We report results using the NIST implementation of the BLEU score which computes the brevity penalty using the shortest ref-erence translation for each segment ([REF_CITE]2008).
The BLEU scores are reported at the word-level for aren and zhen but at the character level for enzh.
We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling[REF_CITE].
"In all tables, systems in a column show statistically significant differences un-less marked with an asterisk."
We first compare lattice MBR to N-best MBR de-coding and MAP decoding (Table 2).
"In these ex-periments, we hold the likelihood scaling factor α a constant; it is set to 0.2 for aren and enzh, and 0.1 for zhen."
The translation lattices are pruned using Forward-Backward pruning[REF_CITE]so that the average numbers of arcs per word (lattice density) is 30.
"For N-best MBR, we use N-best lists of size 1000."
"To match the loss func-tion, Lattice MBR is performed at the word level for aren/zhen and at the character level for enzh."
"Our lattice MBR is implemented using the Google Open-Fst library. [URL_CITE] In our experiments, p, r[REF_CITE]have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48 for aren, zhen, and enzh respectively."
"We note that Lattice MBR provides gains of 0.2- 1.0 BLEU points over N-best MBR, which in turn gives 0.2-0.6 BLEU points over MAP."
These gains are obtained on top of a baseline system that has competitive performance relative to the results re-ported in the[REF_CITE]Evaluation. [Footnote_7]
7[URL_CITE]official results v0.html
This demon-strates the effectiveness of lattice MBR decoding as a realization of MBR decoding which yields sub-stantial gains over the N-best implementation.
The gains from lattice MBR over N-best MBR could be due to a combination of factors.
"These in-clude: 1) better approximation of the corpus BLEU score, 2) larger hypothesis space, and 3) larger evi-dence space."
We now present experiments to tease apart these factors.
Our first experiment restricts both the hypothesis and evidence spaces in lattice MBR to the 1000-best list (Table 3).
"We compare this to N-best MBR with: a) sentence-level BLEU, and b) sentence-level log BLEU."
"The results show that when restricted to the 1000-best list, Lattice MBR performs slightly better than N-best MBR (with sentence BLEU) on aren/enzh while N-best MBR is better on zhen."
"We hypothe- size that on aren/enzh, the linear corpus BLEU ga[REF_CITE]is better correlated to the actual cor-pus BLEU than sentence-level BLEU while the op-posite is true on zhen."
N-best MBR gives similar results with either sentence BLEU or sentence log BLEU.
This confirms that using a log BLEU score does not change the outcome of MBR decoding and further justifies our Taylor-series approximation of the log BLEU score.
We next attempt to understand factors 2 and 3.
"To do that, we carry out lattice MBR when either the hypothesis or the evidence space in Equation 2 is re-stricted to 1000-best hypotheses (Table 4)."
"For com-parison, we also include results from lattice MBR when both hypothesis and evidence spaces are iden-tical: either the full lattice or the 1000-best list (from Tables 2 and 3)."
These results show that lattice MBR results are almost unchanged when the hypothesis space is re-stricted to a 1000-best list.
"However, when the ev-idence space is shrunk to a 1000-best list, there is a significant degradation in performance; these lat-ter results are almost identical to the scenario when both evidence and hypothesis spaces are restricted to the 1000-best list."
This experiment throws light on what makes lattice MBR effective over N-best MBR.
"Relative to the N-best list, the translation lat-tice provides a better estimate of the expected BLEU score."
"On the other hand, there are few hypotheses outside the 1000-best list which are selected by lat-tice MBR."
"Finally, we show how the performance of lattice MBR changes as a function of the lattice density."
The lattice density is the average number of arcs per word and can be varied using Forward-Backward pruning[REF_CITE].
Figure 2 re-ports the average number of lattice paths and BLEU scores as a function of lattice density.
The results show that Lattice MBR performance generally im-proves when the size of the lattice is increased.
"However, on zhen, there is a small drop beyond a density of 10."
This could be due to low quality (low posterior probability) hypotheses that get included at the larger densities and result in a poorer estimate of the expected BLEU score.
"On aren and enzh, there are some gains beyond a lattice density of 30."
These gains are relatively small and come at the expense of higher memory usage; we therefore work with a lattice density of 30 in all our experiments.
We note that Lattice MBR is operating over lattices which are gigantic in comparison to the number of paths in an N-best list.
"At a lattice density of 30, the lattices in aren contain on an average about 10 81 hypotheses!"
We next examine the role of the scale factor α in lattice MBR decoding.
The MBR scale factor de-termines the flatness of the posterior distribution (Equation 3).
It is chosen using a grid search on the dev2 set (Table 1).
Figure 3 shows the variation in BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-tor.
The optimal scale factor is identical for all three language pairs.
"In experiments not reported in this paper, we have found that the optimal scaling factor on a moderately sized development set carries over to unseen test sets."
Lattice MBR Decoding (Equation 6) involves com-puting a posterior probability for each n-gram in the lattice.
We would like to speed up the Lattice MBR computation (Section 4) by restricting the maximum order of the n-grams in the procedure.
"The results (Table 5) show that on aren, there is no degradation if we limit the maximum order of the n-grams to 3."
"However, on zhen/enzh, there is improvement by considering 4-grams."
We can therefore reduce Lat-tice MBR computations in aren.
We have presented a procedure for performing Min-imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR decoder operates over a very large number of trans-lations.
"In contrast, the current N-best implementa-tion of MBR can be scaled to, at most, a few thou-sands of hypotheses."
"If the number of hypotheses is greater than, say 20,000, the N-best MBR be-comes computationally expensive."
The lattice MBR technique is efficient when performed over enor-mous number of hypotheses (up to 10 80 ) since it takes advantage of the compact structure of the lat-tice.
"Lattice MBR gives consistent improvements in translation performance over N-best MBR decod-ing, which is used in many state-of-the-art research translation systems."
"Moreover, we see gains on three different language pairs."
There are two potential reasons why Lattice MBR decoding could outperform N-best MBR: a larger hypothesis space from which translations could be selected or a larger evidence space for computing the expected loss.
Our experiments show that the main improvement comes from the larger evidence space: a larger set of translations in the lattice provides a better estimate of the expected BLEU score.
"In other words, the lattice provides a better posterior distri-bution over translation hypotheses relative to an N-best list."
This is a novel insight into the workings of MBR decoding.
We believe this could be possi-bly employed when designing discriminative train-ing approaches for machine translation.
"More gener-ally, we have found a component in machine transla-tion where the posterior distribution over hypotheses plays a crucial role."
We have shown the effect of the MBR scaling fac- tor on the performance of lattice MBR.
The scale factor determines the flatness of the posterior distri-bution over translation hypotheses.
A scale of 0.0 means a uniform distribution while 1.0 implies that there is no scaling.
This is an important parameter that needs to be tuned on a development set.
There has been prior work in MBR speech recognition and machine translati[REF_CITE]which has shown the need for tuning this factor.
Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP transla-tion.
"As a result, it is necessary to flatten the prob-ability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis."
Our Lattice MBR implementation is made pos-sible due to the linear approximation of the BLEU score.
This linearization technique has been applied elsewhere when working with BLEU:
"In both cases, a linear metric makes it easier to compute the expectation."
"While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions."
"In par-ticular, our framework might be useful with transla- tion metrics such as TER[REF_CITE]or METEOR[REF_CITE]."
"In contrast to a phrase-based SMT system, a syn-tax based SMT system (e.g.[REF_CITE]) can generate a hypergraph that rep-resents a generalized translation lattice with words and hidden tree structures."
We believe that our lat-tice MBR framework can be extended to such hy-pergraphs with loss functions that take into account both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-cedures are not yet common in statistical machine translation.
"However, they are promising because the search space of translations is much larger than the typical N-best list[REF_CITE]."
"We hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear, global loss functions such as BLEU."
The conditional phrase translation probabil-ities constitute the principal components of phrase-based machine translation systems.
"These probabilities are estimated using a heuristic method that does not seem to opti-mize any reasonable objective function of the word-aligned, parallel training corpus."
"Ear-lier efforts on devising a better understood estimator either do not scale to reasonably sized training data, or lead to deteriorating performance."
"In this paper we explore a new approach based on three ingredients (1) A generative model with a prior over latent segmentations derived from Inversion Trans-duction Grammar (ITG), (2) A phrase ta-ble containing all phrase pairs without length limit, and (3) Smoothing as learning ob-jective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization."
"Where others conclude that latent segmentations lead to overfitting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data."
A major component in phrase-based statistical Ma-chine translation (PBSMT)[REF_CITE]is the table of conditional prob-abilities of phrase translation pairs.
The pervading method for estimating these probabilities is a sim-ple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs ex-tracted from the word-aligned corpus[REF_CITE].
"While this heuristic estimator gives good em-pirical results, it does not seem to optimize any intu-itively reasonable objective function of the (word-aligned) parallel corpus (see e.g.,[REF_CITE])"
The mounting number of efforts attacking this problem over the last few years[REF_CITE]exhibits its difficulty.
"So far, none has lead to an alternative method that performs as well as the heuristic on rea-sonably sized data (approx. 1000k sentence pair)."
"Given a parallel corpus, an estimator for phrase-tables in PBSMT involves two interacting decisions (1) which phrase pairs to extract, and (2) how to as-sign probabilities to the extracted pairs."
"The heuris-tic estimator employs word-alignment (Giza++)[REF_CITE]and a few thumb rules for defining phrase pairs, and then extracts a multi-set of phrase pairs and estimates their conditional prob-abilities based on the counts in the multi-set."
"Us-ing this method for extracting a set of phrase pairs,[REF_CITE]aim at defining a better estimator for the probabilities."
"Generally speaking, both efforts report deteriorating translation performance relative to the heuristic."
"Instead of employing word-alignment to guide phrase pair extraction, it is theoretically more ap-pealing to aim at phrase alignment as part of the esti-mation process[REF_CITE]."
"This way, phrase pair extraction goes hand-in-hand with estimating the probabilities."
"How-ever, in practice, due to the huge number of possi-ble phrase pairs, this task is rather challenging, both computationally and statistically."
"It is hard to define both a manageable phrase pair translation model and a well-founded training regime that would scale up to reasonably sized parallel corpora (see e.g.,[REF_CITE])."
It remains to be seen whether this the-oretically interesting approach will lead to improved phrase probability estimates.
In this paper we also start out from a stan-dard phrase extraction procedure based on word-alignment and aim solely at estimating the condi-tional probabilities for the phrase pairs and their reverse translation probabilities.
"Unlike preceding work, we extract all phrase pairs from the training corpus and estimate their probabilities, i.e., without limit on length."
We present a novel formulation of a conditional translation model that works with a prior over segmentations and a bag of conditional phrase pairs.
"We use binary Synchronous Context-Free Grammar (bSCFG), based on Inversion Trans-duction Grammar (ITG)[REF_CITE], to define the set of eligible segmentations for an aligned sentence pair."
"We also show how the num-ber of spurious derivations per segmentation in this bSCFG can be used for devising a prior probabil-ity over the space of segmentations, capturing the bias in the data towards monotone translation."
"The heart of the estimation process is a new smoothing estimator, a penalized version of Deleted Estima-tion, which averages the temporary probability es-timates of multiple parallel EM processes at each joint iteration."
For evaluation we use a state-of-the-art baseline system (Moses)[REF_CITE]which works with a log-linear interpolation of feature func-tions optimized by MERT[REF_CITE].
We sim-ply substitute our own estimates for the heuristic phrase translation estimates (both directions and the phrase penalty score) and compare the two within the Moses decoder.
"While our estimates differ sub-stantially from the heuristic, their performance is on par with the heuristic estimates."
This is remark-able given the fact that comparable previous work[REF_CITE]did not match the performance of the heuristic estima-tor using large training sets.
We find that smooth-ing is crucial for achieving good estimates.
"This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima’an, 2006), and agrees with the most up-to-date work that em- ploys Bayesian priors over the estimates[REF_CITE]."
Marcu and Wong[REF_CITE]realize that the problem of extracting phrase pairs should be intertwined with the method of probability esti-mation.
They formulate a joint phrase-based model in which a source-target sentence pair is generated jointly.
"However, the huge number of possible phrase-alignments prohibits scaling up the estima-tion by Expectation-Maximization (EM)[REF_CITE]to large corpora."
Birch et al[REF_CITE]provide soft measures for including word-alignments in the estimation process and obtain im-proved results only on small data sets.
"Coming up-to-date,[REF_CITE]at-tempt a related estimation problem[REF_CITE], using the expanded phrase pair set[REF_CITE], working with an exponential model and concentrating on marginalizing out the latent segmentation variable."
"Also most up-to-date,[REF_CITE]report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrat-ing the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses."
"The latter two efforts report improved performance, albeit again on a limited training set (approx. 140k sentences up to a certain length)."
This model involves a hidden segmentation variable that is set uniformly (or to prefer shorter phrases over longer ones).
"Fur-thermore, the model involves a reordering compo-nent akin to the one used in IBM model 3."
"De-spite this, the heuristic estimator remains superior because ”EM learns overly determinized segmen-tations and translation parameters, overfitting the training data and failing to generalize”."
"More re-cently,[REF_CITE]devise a estimator working with a model that does not include a hid-den segmentation variable but works with a heuris-tic iterative procedure (rather than MLE or EM)."
The translation results remain inferior to the heuristic but the authors note an interesting trade-off between de-coding speed and the various settings of this estima-tor.
"Our work expands on the general approach taken[REF_CITE]but arrives at insights similar to those of the most recent work[REF_CITE], albeit in a com-pletely different manner."
The present work differs from all preceding work in that it employs the set of all phrase pairs during training.
It differs[REF_CITE]in that it does postulate a la-tent segmentation variable and puts the prior di-rectly over that variable rather than over the ITG synchronous rule estimates.
Our method neither excludes phrase pairs before estimation nor does it prune the space of possible segmentations/analyses during training/estimation.
"As well as smoothing , we find (in the same vein[REF_CITE]) that setting effective priors/smoothing is crucial for EM to arrive at better estimates."
"Given a word-aligned parallel corpus of source-target sentences, it is common practice to extract a set of phrase pairs using extraction heuristics (cf.[REF_CITE])."
"These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram, the other end is in the other ngram (and there is at least one such alignment)[REF_CITE]."
"For ef-ficiency and sparseness, the practitioners of PBSMT constrain the length of the source phrase to a certain maximum number of words."
An All Phrase Pairs Model: In this work we train a phrase-translation table that consists of all phrase-pairs that can be extracted from the word-aligned training data according to the standard phrase ex-traction heuristic.
"After training, we can still limit the set of phrase pairs to those selected by a cut-off on phrase length."
"The reason for using all phrase pairs during training is that it gives a clear point of reference for an estimator, without implicit, acciden- tal biases that might emerge due to length cut-off 1 ."
"The Generative Model: Given a word-aligned source-target sentence pair hf, e, ai, the generative story underlying our model goes as follows: 1."
"Abiding by the word-alignments in a, segment the source-target sentence pair hf, ei into a se-quence of I containers σ I1 , and a bag of I phrase pairs σ 1I (f,e) = {hf j ,e j i} Ij=1 ."
"Each container σ j = hl f ,r f ,l e ,r e i consists of the start l f and end r f positions [Footnote_2] for a phrase in f and the start l e and end r e positions for an aligned phrase in e. 2."
2 The NULL alignments (word-to-NULL) in the training data can also be marked with actual positions on both sides in order to allow for this definition of containers.
"For a given segmentation σ 1I , for every con-tainer σ j (1 ≤ j ≤ I) generate the phrase-pair hf j ,e j i, independently from all other phrase-pairs."
This leads to the following probabilistic model:
P(f | e; a) = X P(σ 1I )
"Y P(f j | e j ) (1) σ 1I ∈Σ(a) hf j ,e j i∈σ 1I (f,e)"
"Where Σ(a) is the set of binarizable segmenta-tions (defined next) that are eligible according to the word-alignments a between f and e. These segmen-tations into bilingual containers (where segmenta-tions are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g.,[REF_CITE]) which must generate the alignment on top of the segmentations."
"Note how the different phrase pairs hf j , e j i are generated from their bilingual con-tainers in the given segmentation σ I1 ."
We will dis-cuss our choice of prior probability over segmenta-tions P (σ I1 ) after we discuss the definition of the bi-narizable segmentations Σ(a).
"Following[REF_CITE], every sequence of phrase alignments can be viewed as a sequence of integers [Footnote_1],... I together with a permuted version of this sequence π(1), . . . , π(I), where the two copies of an integer in the two se-quences are assumed aligned/paired together."
"1 For example, if the cut-off on phrase pairs is ten words, all sentence pairs smaller than ten words in the training data will be included as phrase pairs as well. These sentences are treated differently from longer sentences, which are not allowed to be phrase pairs."
"For example, possible permutations of {1,2,3,4} are {2, 1, 3, 4} and {2, 4, 1, 3}."
"Because a segmenta-tion σ I1 of a sentence pair is also a sequence of aligned phrases, it also constitutes a permuted se-quence."
"A binarizable permutation x is either of length one, or can be properly split into two binariz-able sub-sequences y and z such that either [Footnote_3] z &lt; y or y &lt; z."
"3 For two sequences of numbers, the notation y &lt; z stands for ∀y ∈ y, ∀z ∈ z : y &lt; z."
"For example, one way to binarize the permutation {2, 1, 3, 4} is to introduce a proper split into {2, 1; 3, 4}, then recursively another proper split of {2, 1} into {2; 1} and {3, 4} into {3; 4}."
"In con-trast, the permutation {2, 4, 1, [Footnote_3]} is non-binarizable."
"3 For two sequences of numbers, the notation y &lt; z stands for ∀y ∈ y, ∀z ∈ z : y &lt; z."
"Graphically speaking, the recursive definition of binarizable permutations can be depicted as a bi-nary tree structure where the nodes correspond to recursive proper splits of the permutation, and the leaves are decorated with the naturals."
Figure 1 ex-hibits two possible binarizations of the same permu-tation where &lt;&gt; and [] denote inverted and mono-tone proper splits respectively.
"Note that the number of possible binarizations of a binarizable permuta-tion is a recursive function of the number of possi-ble proper splits and reaches its maximum for fully monotone permutations (all binary trees, which is a factorial function of the length of the permutation)."
"By definition (cf.[REF_CITE]), a binarizable segmentation/permutation can be recognized by a binarized Synchronous Context-Free Grammar (SCFG), i.e., an SCFG in which the right hand sides of all non-lexical rules constitute binarizable permutations."
"In particular, this holds for the SCFG implementing Inversion"
This SCFG[REF_CITE]has two binary synchronous rules that correspond resp. to the contiguous monotone and inverted alignments:
"XP → XP 1 XP 2 , XP 1 XP 2 (2) XP → XP 1 XP 2 , XP 2 XP 1"
The boxed integers in the superscripts on the non-terminal XP denote synchronized rewritings.
"In this work, we employ a binary SCFG (bSCFG) working with these two synchronous rules to-gether with a set of lexical rules {XP → f, e | hf, ei is a phrase pair}."
"In this bSCFG, every derivation corresponds to a binarization of a segmentation of the input."
Note that the bSCFG defined in equation 2 generates all possible binarizations for every segmentation of the input.
"It is possible to constrain this bSCFG such that it generates a single, canonical derivation per segmentation."
"However, in section 3.2 we show that the number of such derivations is a good measure of phrase pair productivity."
It is well known that there are alignments and segmentations that this bSCFG does not cover (see[REF_CITE]).
"Recently, strong evidence emerged (e.g.,[REF_CITE]) showing that most word-alignments of actual parallel corpora can be covered by a binarized SCFG of the ITG type."
"Furthermore, because our model employs the set of all phrase-pairs that can be extracted from a given training set, it will always find segmentations that cover every sentence pair in the training data [Footnote_4] ."
4 In the worst case the whole sentence pair is a phrase pair with a trivial segmentation.
"This implies that while our model might discard non-binarizable segmentations for certain complex word alignments, we do manage to train the model on the binarizable segementations of all sentence pairs."
"Up to the prior over segmentations (see next), we implement the above model using a weighted ver-sion of the binary SCFG as follows: • The weight for lexical rules is given by P(XP → f,e) :="
"P(f | e), where hf,ei is a phrase-pair."
These are the trainable parame-ters of our model. • The weights for the two non-lexical rules in equation 2 are fixed at 1.0.
These weights are not trained at all.
Where we use the notation P(.) for the weight of a synchronous rule.
"As it has been found out[REF_CITE], it is not easy to come up with a simple, effec-tive prior distribution over segmentations that al-lows for improved phrase pair estimates."
"Within a Maximum-Likelihood estimator, preference for seg-mentations σ 1I consisting of longer containers could lead to overfitting as we will explain in section 4."
"Alternatively, it is tempting to have preference for segmentations σ 1I that consist of shorter contain-ers, because (generally speaking) shorter contain-ers have higher expected coverage of new sentence pairs."
"However, mere bias for shorter containers will not give better estimates as observed[REF_CITE]."
One case where this bias clearly fails is the case of a contiguous sequence of con-tainers with a complex alignment structure (cross-ing alignments).
"For example (see figure 2), for the alignment {1, 3, 4, 2, 5} there is a segmentation into five containers {1; 3; 4; 2; 5}, and another into three {1; 3, 4, 2; 5}."
"The first segmentation involves shorter containers that have crossing brackets among them, while the second one consists of three con-tainers including a longer container {3,4,2}."
"In the first segmentation, due to their crossing align-ments, each of the containers {3}, {4} and {2} will not combine with the surrounding context ({1} and {5}) on its own, i.e., without the other two contain-ers."
"Furthermore, there is only a single binariza- tion of {3,4,2}."
"Hence, while the first segmen-tation involves shorter containers than the second one, these shorter containers are as productive as the large container {3, 4, 2}, i.e., they combine with surrounding containers in the same number of ways as the large container."
"In such and similar cases, there are no grounds for the bias towards shorter phrases/containers."
"The notion of container productivity (the num-ber of ways in which it combines with surrounding containers during training) seems to correlate with the expected number of ways a container can be used during decoding, which should be correlated with expected coverage."
"During training, contain-ers that are often surrounded by other, monotoni-cally aligned containers are expected to be more pro-ductive than alternative containers that are often sur-rounded by crossing alignments."
"Hence, the num-ber of binarizations that a segmentation has under the bSCFG is a direct function of the ways in which the containers combine among themselves (mono-tone vs. inverted/crossing) within segmentations, and provides a more accurate measure of container productivity than container length."
"Hence, the final model we employ is the following:"
P (f | e; a) = N(σ 1I )
X Y P (f j | e j ) (3) Z(Σ(a)) σ
"I1 ∈Σ(a) hf j ,e j i∈σ 1I (f,e)"
"Where N(σ 1I ) is the number of binary deriva-tions/trees that σ 1I has in the binary SCFG (bSCFG), and Z(Σ(a)) ="
"P σ J ∈Σ(a) N(σ 1J ), i.e., this prior is 1 the ratio of number of derivations of σ I1 to the to-tal number of derivations that hf, e, ai has under the bSCFG."
"In contrast with the model[REF_CITE], who define the segmentations over the source sen-tence f alone, our model employs bilingual con-tainers thereby segmenting both source and target sides simultaneously."
"Therefore, un[REF_CITE], our model does not need to gener-ate the word-alignments explicitly, as these are em-bedded in the segmentations."
"Similarly, our model does not include explicit penalty terms for reorder- ing/inversion but includes a related bias in the prior probabilities over segmentations P (σ 1I )."
"In a way, the segmentations and bilingual contain-ers we use can be viewed as similar to the concepts used in the Joint Model of Marcu and Wong[REF_CITE]."
"Un[REF_CITE], however, our model works with conditional proba-bilities and starts out from the word-alignments."
"The novel aspects of our model are three (1) It de-fines the set of segmentations using a bSCFG, (2) It includes a novel, refined prior probability over seg-mentations, and (3) It employs all phrase pairs that can be extracted from a word-aligned training par-allel corpus."
"For these novel elements to produce reasonable estimates, we devise our own estimator."
"In principle, we are dealing here with a translation model that employs all phrase pairs (of unbounded size), extracted from a word-aligned parallel cor-pus."
"Under this model, where a phrase pair and its sub-phrase pairs are included in the model, the MLE can be expected to overfit the data [Footnote_5] unless a suitable prior probability over segmentations is em-ployed."
"5 One trivial MLE solution would give the longest container, consisting of the longest phrase pairs, a probability of one, at the cost of all shorter alternatives. A similar problem arises in Data-Oriented Parsing, see (Sima’an and[REF_CITE]; Zoll-mann and Sima’an, 2006). Note that models that employ an upperbound on phrase pair length will still risk overfitting train-ing sentences of lengths that fall within this upperbound."
"Indeed, the prior over segmentations defined in the preceding section prevents the MLE from completely overfitting the training data."
"However, we find empirical evidence that this prior is insuffi-cient for avoiding overfitting."
Our model behaves like a memory-based model because it memorizes all extractable phrase pairs found in the training data including the training sen-tence pairs themselves.
Such memory-based mod-els are related to nonparametric models such as K-NN and kernel methods[REF_CITE].
"For memory-based models, consistent estimation for novel instances proceeds by local density estimation from the surroundings of the instance, which is akin to smoothing for parametric models."
"Hence, next we describe our own version of a smoothed Maximum-Likelihood estimator for phrase translation probabil- ities."
"For a latent variable model, it is usually common to employ Expectation-Maximization (EM)[REF_CITE]as a search method for a (local) maximum-likelihood estimate (MLE) of the train-ing data."
"Instead of mere EM we opt for a smoothed version: we present a new method, that combines Deleted Estimati[REF_CITE]with the Jackknife[REF_CITE]as the core estima-tor."
Figure 3 shows the pseudo-code for our estima-tor.
"Like in Deleted Estimation, we split the training data into ten equal portions."
This way we create ten different splits of extraction/heldout sets of respec-tively 90%/10% of the training set.
"For every split 1 ≤ i ≤ 10, we extract a set of phrase pairs π i from the extraction set E i and train it (under our model) on the heldout set H i ."
"Naturally, the phrase pair sets π i (1 ≤ i ≤ 10) are subsets of (or equal to) the set of phrase pairs π = ∪ i π i extracted from the total training data (i.e., π is the set of model parameters)."
"The training of the different π i ’s, each on its corre-sponding heldout set H i , is done by ten separate EM processes, which are synchronized in their initializa- tion, their iterations as well as stop condition."
The EM processes start out from uniform conditional es-timates of the phrase pairs in all π i .
"After every EM iteration j, when the M-step has finished, the esti-mates in all π ji (1 ≤ i ≤ 10) are set to the average (over 1 ≤ i ≤ 10) of the estimates in π ij leading to j (following the Jackknife method)."
"The resulting π b i j are then used as the cur-averaged probabilities in b π i rent phrase pair estimates, which feed into the next iteration j + 1 of the different EM processes (each working on a different heldout set H i with a differ-ent set of phrase pairs π i )."
There are two special boundary cases which de-mand special attention during estimation:
"Sparse distributions: For a phrase e that does oc-cur both in H i and E i , there could be a phrase pair hf,ei that does occur in H i but does not occur in π i ."
"To prevent EM from giving the extra probability mass to all other pairs hf, e ′ i unjustifiably, we apply smoothing."
"We add the missing pair hf, ei to π i and set its probability to a fixed number 10 −5∗len , where len is the length of the phrase pair."
"In effect, we backoff our model (equation 1) to a word-level model with fixed word translation probability (10 −5 )."
"Zero distributions: When a phrase e does not oc-cur in H i , all its pairs hf,ei in π i will have zero counts."
"During each EM iteration, when the M-step is applied, the distribution P(· | e) is undefined by MLE, since it is irrelevant for the likelihood of H i ."
In this case any choice of proper distribution P (· | e) will constitute an MLE solution.
We choose to set this case to a uniform distribution every time again.
"Since our model and estimator are implemented within the bSCFG framework, we use a bilingual CYK parser[REF_CITE]under the grammar in equation 2."
"This parser builds for every input hf, a, ei all binarizations/derivations for every seg-mentation in Σ(a)."
"For implementing EM, we em-ploy the Inside-Outside algorithm[REF_CITE]."
"During estimation, because the input, output and word-alignment are known in advance, the time and space requirements re-main manageable despite the worst-case complexity O(n 6 ) in target sentence length n."
"Penalized Deleted Estimation: In contrast with our method, Deleted Estimation sums the expected counts (rather than probabilities) obtained from the different splits before applying the M-step (normalization)."
"While the rationale behind Deleted Estimation comes from MLE over the original training data, our method has a smoothing objective (inspired by the Jackknife ): generally speaking, the averages over different heldout sets (under different subsets of the model) give less sharp estimates than MLE."
"By averaging the different heldout estimates, this estimator employs a penalty term that depends on the marginal count of e in the heldout set 6 ."
"Interestingly, when the phrase e is very frequent [Footnote_7] , it will approximately occur almost as often in the different heldout sets."
"7 Theoretically speaking, when the training data is unbound-edly large, our estimator will converge to the same estimates as the Deleted Estimation. When the data is still sparse, our estimator is biased, unlike the MLE which will overfit."
"In this case, our method reduces to Deleted Estimation, where it effectively sums the counts [Footnote_8] ."
"8 When calculating the conditional probabilities, the denom-inators used are approximately equal to one another."
"Yet, when the target phrase e does occur only very few times, it is likely that its count in some splits will be zero."
"In our method, at every EM iteration, during the Maximization step, we set such cases back to uniform."
"By averaging the probabilities from the different splits over many EM iterations, setting these cases to uniform constitutes a kind of prior that prevents the final estimates from falling too far from uniform."
"In contrast, in Deleted Interpolation the zero counts are simply summed with the other corresponding counts of the same phrase pair, which leads to sharper probability distributions."
"In all experiments that we conducted, our method (which we call Penalized Deleted Estimation) gave more successful estimates than mere Deleted Estimation."
"On the theoretical side, the choice for a fixed 6 Define count y (x) to be the count of event x in data y."
"The Deleted Estimation (DE) estimate is P count (f,e)/count (e), which can be written as prior over segmentations (ITG prior) implies that our model cannot be estimated to converge (in proba-bility) to the relative frequency estimates (RFE) of source-target sentence pairs in the limit of the train-ing data (a sufficiently large parallel corpus)."
A prior probability over segmentations that would allow our estimator to converge in the limit to the RFE must gradually prefer segmentations consisting of larger containers as the data grows large.
We set the de-sign and estimation of such a prior aside for future work.
"Decoding and Baseline Model: In this work we employ an existing decoder, Moses[REF_CITE], which defines a log-linear model interpolating feature functions, with interpo-lation scores λ f e ∗ = arg max e P f∈Φ λ f H f (f, e)."
The λ f are optimized by Minimum-Error Training (MERT)[REF_CITE].
"The set Φ consists of the following feature functions (see[REF_CITE]): a 5-gram target language model, the stan-dard reordering scores, the word and phrase penalty scores, the conditional lexical estimates obtained from the word-alignment in both directions, and the conditional phrase translation estimates in both di-rections P(f | e) and"
P(e | f).
"Keeping the other five feature functions fixed, we compare our esti-mates of P(f | e) and P(e | f) (and the phrase penalty) to the commonly used heuristic estimates."
"Because our model employs a latent segmenta-tion variable, this variable should be marginalized out during decoding to allow selecting the highest probability translation given the input."
This turns out crucial for improved results (cf.[REF_CITE]).
"However, such a marginalization can be NP-Complete, in analogy to a similar problem in Data-Oriented Parsing (Sima’an, 2002) [Footnote_9] ."
9 A reduction of simple instances of the first problem to in-stances of the latter problem should be possible.
We do not have a decoder yet that can approximate this marginaliza-tion efficiently and we employ the standard Moses decoder for this work.
"Experimental Setup: The training, development and test data all come from the French-English translation shared task of the[REF_CITE]Second"
Workshop on Statistical[REF_CITE].
"Af-ter pruning sentence pairs with word length more than 40 on either side, we are left with 949K sen-tence pairs for training."
The development and test data are composed of 2K sentence pairs each.
All data sets are lower-cased.
"For both the baseline system and our method, we produce word-level alignments for the parallel training corpus using GIZA++."
"We use 5 iterations of each IBM Model 1 and HMM alignment mod-els, followed by 3 iterations of each Model 3 and Model 4."
"From this aligned training corpus, we ex-tract the phrase pairs according to the heuristics[REF_CITE]."
The baseline system extracts all phrase-pairs upto a certain maximum length on both sides and employs the heuristic estimator.
The language model used in all systems is a 5-gram lan-guage model trained on the English side of the paral-lel corpus.
Minimum-Error Rate Training (MERT) is applied on the development set to obtain opti-mal log-linear interpolation weights for all systems.
"Performance is measured by computing the BLEU scores[REF_CITE]of the system’s trans-lations, when compared against a single reference translation per sentence."
Results: We compare different versions of our system against the baseline system using the heuris-tic estimator.
We observe the effects of the ITG prior in the translation model as well as the method of es-timation (Deleted Estimation vs. Penalized Deleted Estimation).
Table 1 exhibits the BLEU scores for the sys- tems.
"Our own system (with ITG prior and Pe-nalized Deleted Estimation and maximum phrase-length ten words) scores (33.14), slightly outper-forming the best baseline system (33.03)."
"When us-ing straight Deleted Estimation over EM, this leads to deterioration (32.73)."
When also the ITG prior is excluded (by having a single derivation per segmen-tation) this leads to further deterioration (32.67).
"By using mere EM with an ITG prior, performance goes down to 32.50, exhibiting the crucial role of the es-timation by smoothing."
"Clearly, Penalized Deleted Estimation and the ITG prior are important for the improved phrase translation estimates."
"As table 1 shows we also varied the phrase length cutoff (seven, ten or none=all phrase pairs)."
The length cutoff pertains to both sides of a phrase-pair.
"For our estimator, we always train all phrase pairs, applying the length cutoff only after training (no re-normalization is applied at that point)."
"Interestingly, we find out that the heuristic estima-tor cannot benefit performance by including longer phrase pairs."
"Our estimator does benefit perfor-mance by including phrase pairs of length upto ten words, but then it degrades again when including all phrase pairs."
We take the latter finding to sig-nal remaining overfitting that proved resistant to the smoothing applied by our estimator.
The heuristic estimator exhibits a similar degradation.
"We also tried to vary the treatment of Sparse Dis-tributions (section 4, page 7) during heldout estima-tion from fixed word-translation probabilities to the lexical model probabilities."
This lead to slight dete-rioration of results (32.94).
It is unclear whether this deterioration is meaningful or not.
"We did not ex-plore mere EM without any smoothing or ITG prior, as we expect it will directly overfit the training data as reported[REF_CITE]."
"We note that for French-English translation it is hard to outperform the heuristic within the PBSMT framework, since it already performs very well."
"Preliminary, most recent experiments on German-English (also[REF_CITE]data) exhibit that our estima-tor outperforms the heuristic."
"The most similar efforts to ours, mainly[REF_CITE], conclude that segmentation variables in the generative translation model lead to overfit-ting while attaining higher likelihood of the train-ing data than the heuristic estimator."
Based on this advise[REF_CITE]exclude the latent segmentation variables and opt for a heuristic train-ing procedure.
In this work we also start out from a generative model with latent segmentation variables.
"However, we find out that concentrating the learning effort on smoothing is crucial for good performance."
"For this, we devise ITG-based priors over segmenta-tions and employ a penalized version of Deleted Es-timation working with EM at its core."
The fact that our results (at least) match the heuristic estimates on a reasonably sized data set (947k parallel sentence pairs) is rather encouraging.
"The work[REF_CITE]has a simi-lar flavor to our work, yet the two differ substan-tially."
Both depart from Maximum-Likelihood to-wards non-overfitting estimators.
"Where Zhang et al choose for sparse priors (leading to sharp phrase dis-tributions) and put the smoothing burden on the ITG rule parameters and a pruning strategy, we choose for a prior over segmentations determined by the ITG derivation space and smooth the MLE directly with a penalized version of Deleted Estimation."
It remains to be seen how the two biases compare to one another on the same task.
There are various strands of future research.
"Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior."
"Secondly,[REF_CITE]show, marginalizing out the different segmentations during decoding leads to improved performance."
We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.
"Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent es-timation."
"Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper."
Acknowledgments: Both authors are supported by a VIDI grant (nr. 639.022.604) from The Nether-lands Organization for Scientific Research (NWO).
David Chiang and Andy Way are acknowledged for stimulating discussions on machine translation and parsing.
We present a generative model for unsuper-vised coreference resolution that views coref-erence as an EM clustering process.
"For comparison purposes, we revisit Haghighi and Klein’s (2007) fully-generative Bayesian model for unsupervised coreference resolu-tion, discuss its potential weaknesses and con-sequently propose three modifications to their model."
Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and com-pares favorably to the modified model.
"Coreference resolution is the problem of identifying which mentions (i.e., noun phrases) refer to which real-world entities."
The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in re-cent years.
"The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two men-tions are co-referring (e.g.,[REF_CITE],[REF_CITE],[REF_CITE]) to the de-velopment of rich linguistic features (e.g.,[REF_CITE],[REF_CITE]) and the ex-ploitation of advanced techniques that involve joint learning (e.g.,[REF_CITE]) and joint inference (e.g.,[REF_CITE]) for coreference resolution and a related extraction task."
"The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which lit-tle or no annotated data exists."
"Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsuper-vised and weakly supervised approaches for the au-tomatic processing of resource-scarce languages has become more important than ever."
"In fact, several popular weakly supervised learn-ing algorithms such as self-training, co-training[REF_CITE], and EM[REF_CITE]have been applied to coreference resolu-ti[REF_CITE]and the related task of pronoun resoluti[REF_CITE]."
"Given a small number of coreference-annotated documents and a large number of unlabeled documents, these weakly supervised learners aim to incrementally augment the labeled data by iteratively training a classifier [Footnote_1] on the labeled data and using it to label mention pairs randomly drawn from the unlabeled documents as COREFERENT or NOT COREFERENT ."
"1 For co-training, a pair of view classifiers are trained; and for EM, a generative model is trained instead."
"However, classifying mention pairs using such iterative ap-proaches is undesirable for coreference resolution: since the non-coreferent mention pairs significantly outnumber their coreferent counterparts, the result-ing classifiers generally have an increasing tendency to (mis)label a pair as non-coreferent as bootstrap-ping progresses (see[REF_CITE])."
"Motivated in part by these results, we present a generative, unsupervised model for probabilistically inducing coreference partitions on unlabeled doc-uments, rather than classifying mention pairs, via EM clustering (Section 2)."
"In fact, our model com-bines the best of two worlds: it operates at the document level, while exploiting essential linguistic constraints on coreferent mentions (e.g., gender and number agreement) provided by traditional pairwise classification models."
"For comparison purposes, we revisit a fully-generative Bayesian model for unsupervised coref-erence resolution recently introduced[REF_CITE], discuss its potential weaknesses and consequently propose three modifications to their model (Section 3)."
Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and compares fa-vorably to the modified model (Section 4).
"In this section, we will explain how we recast un-supervised coreference resolution as EM clustering."
We begin by introducing some of the definitions and notations that we will use in this paper.
"A mention can be a pronoun, a name (i.e., a proper noun), or a nominal (i.e., a common noun)."
An en-tity is a set of coreferent mentions.
"Given a docu-ment D consisting of n mentions, m 1 , . .. , m n , we use P airs(D) to denote the set of n[Footnote_2] mention pairs, {m ij | 1 ≤ i &lt; j ≤ n}, where m ij is formed from mentions m i and m j ."
2[REF_CITE]for details on feature value compu-tations. Note that all feature values are computed automatically.
The pairwise probabil-ity formed from m i and m j refers to the probabil-ity that the pair m ij is coreferent and is denoted as P coref (m ij ).
"A clustering of n mentions is an n x n Boolean matrix C, where C ij (the (i,j)-th entry of C) is 1 if and only if mentions m i and m j are coref-erent."
An entry in C is relevant if it corresponds to a mention pair in Pairs(D).
A valid clustering is a clustering in which the relevant entries satisfy the transitivity constraint.
"In other words, C is valid if and only if (C ij = 1 ∧ C jk = 1) =⇒ C ik = 1 ∀ 1 ≤ i &lt; j &lt; k ≤ n."
"Hence, a valid clustering corresponds to a partition of a given set of mentions, and the goal of coreference resolution is to produce a valid clustering in which each cluster corresponds to a distinct entity."
"As mentioned previously, our generative model op-erates at the document level, inducing a valid clus-tering on a given document D. More specifically, our model consists of two steps."
"It first chooses a clustering C based on some clustering distribution P (C), and then generates D given C:"
"P (D, C) = P (C)P (D | C)."
"To facilitate the incorporation of linguistic con-straints defined on a pair of mentions, we represent D by its mention pairs, Pairs(D)."
"Now, assuming that these mention pairs are generated conditionally independently of each other given C ij ,"
Y P (D | C) =
P (m ij | C ij ). m ij ∈Pairs(D)
"Next, we represent m ij as a set of seven features that is potentially useful for determining whether m i and m j are coreferent (see Table 1). 2"
"Hence, we can rewrite P (D | C) as"
"P (m 1ij , . . . , m 7ij | C ij ), Y m ij ∈Pairs(D) where m kij is the value of the kth feature of m ij ."
"To reduce data sparseness and improve the es-timation of the above probabilities, we make con-ditional independence assumptions about the gen-eration of these feature values."
"Specifically, as shown in the first column of Table 1, we di-vide the seven features into three groups (namely, strong coreference indicators, linguistic constraints, and mention types), assuming that two feature values are conditionally independent if and only if the corresponding features belong to differ-ent groups."
"With this assumption, we can de-compose"
"P(m 1ij ,...m 7ij | C ij ) into a product of three probabilities:"
"P(m 1ij , m ij2 , m ij3 | C ij ), P (m ij4 , m 5ij , m 6ij | C ij ), and P (m 7ij | C ij )."
"Each of these distributions represents a pair of multinomial distributions, one for the coreferent mention pairs (C ij = 1) and the other for the non-coreferent men-tion pairs (C ij = 0)."
"Hence, the set of parameters of our model, Θ, consists of P(m 1 ,m 2 ,m 3 | c), P (m 4 , m 5 , m 6 | c), and P (m 7 | c)."
"To induce a clustering C on a document D, we run EM on our model, treating D as observed data and C as hidden data."
"Specifically, we use EM to itera-tively estimate the model parameters, Θ, from doc-uments that are probabilistically labeled (with clus-terings) and apply the resulting model to probabilis-tically re-label a document (with clusterings)."
"More formally, we employ the following EM algorithm: E-step: Compute the posterior probabilities of the clusterings, P (C|D, Θ), based on the current Θ."
"M-step: Using P (C|D, Θ) computed in the E-step, find the Θ ′ that maximizes the expected complete"
"C P (C|D, Θ) log P (D, C|Θ ′ ). log likelihood, P We begin the induction process at the M-step. [Footnote_3] To find the Θ that maximizes the expected complete log likelihood, we use maximum likelihood estimation with add-one smoothing."
"3 Another possibility, of course, is to begin at the E-step by making an initial guess at Θ."
"Since P(C|D, Θ) is not available in the first EM iteration, we instead use an initial distribution over clusterings, P(C)."
"The question, then, is: which P (C) should we use?"
One possibility is the uniform distribution over all (possi-bly invalid) clusterings.
"Another, presumably better, choice is a distribution that assigns non-zero prob-ability mass to only the valid clusterings."
Yet an-other possibility is to set P(C) based on a docu-ment labeled with coreference information.
"In our experiments, we employ this last method, assigning a probability of one to the correct clustering of the labeled document (see Section 4.1 for details)."
"After (re-)estimating Θ in the M-step, we proceed to the E-step, where the goal is to find the condi-tional clustering probabilities."
"Given a document D, the number of coreference clusterings is expo-nential in the number of mentions in D, even if we limit our attention to those that are valid."
"To cope with this computational complexity, we ap-proximate the E-step by computing only the condi-tional probabilities that correspond to the N most probable coreference clusterings given the current Θ. We identify the N most probable clusterings and compute their probabilities as follows."
"First, using the current Θ, we reverse the generative model and compute P coref (m ij ) for each mention pair m ij in P airs(D)."
"Next, using these pairwise probabilities, we apply Luo et al.’s (2004) Bell tree approach to coreference resolution to compute the N-best clus-terings and their probabilities (see Section 2.4 for details)."
"Finally, to obtain the required conditional clustering probabilities for the E-step, we normalize the probabilities assigned to the N-best clusterings so that they sum to one."
"As described above, given the pairwise probabilities, we use Luo et al.’s (2004) algorithm to heuristically compute the N-best clusterings (or, more precisely, N-best partitions [Footnote_4] ) and their probabilities based on the Bell tree."
"4 Note that Luo et al.’s search algorithm only produces valid clusterings, implying that the resulting N-best clusterings are"
"Informally, each node in a Bell tree corresponds to an ith-order partial partition (i.e., a partition of the first i mentions of the given docu-ment), and the ith level of the tree contains all possi-ble ith-order partial partitions."
"Hence, the set of leaf nodes constitutes all possible partitions of all of the mentions."
"The search for the N most probable parti-tions starts at the root, and a partitioning of the men-tions is incrementally constructed as we move down the tree."
"Since an exhaustive search is computation-ally infeasible, Luo et al. employ a beam search pro-cedure to explore only the most probable paths at each step of the search process."
Figure 1 shows our implementation of this heuristic search algorithm.
"The algorithm takes as input a set of n mentions (and their pairwise probabilities), and returns the N most probable partitionings of the mentions."
It uses data structures S and the H i ’s to store intermediate results.
"Specifically, S(PP) stores the score of the partial partition PP."
"H i is associated with the ith level of the Bell tree, and is used to store the most probable ith-order partial partitions."
"Each H i has a maximum size of 2N: if more than 2N partitions are inserted into a given H i , then only the 2N most probable ones will be stored."
"This amounts to prun-ing the search space by employing a beam size of 2N (i.e., expanding only the 2N most probable par-tial partitions) at each step of the search."
"The algorithm begins by initializing H 1 with the only partial partition of order one, {[m 1 ]}, which has a score of one (line 2)."
"Then it processes the mentions sequentially, starting with m 2 (line 4)."
"When processing m i , it takes each partial partition PP in H i−1 and creates a set of ith-order parti-tions by extending P P with m i in all possible ways."
"Specifically, for each cluster C (formed by a subset of the first i–1 mentions) in P P , the algorithm gen-erates a new ith-order partition, P P ′ , by linking m i to C (line 9), and stores PP ′ in H i (line 11)."
"The score of PP ′ , S(PP ′ ), is computed by using the pairwise coreference probabilities as follows:"
S(P P ′ ) = S(P P ) · max P coref (m ki ). m k ∈C
"Of course, PP can also be extended by putting m i into a new cluster (line 12)."
"This yields PP δ , an-other partition to be inserted into H i (line 14), and"
"S(P P δ ) = δ·S(P P )·(1− max P coref (m ki )), k∈{1,...,i−1} where δ (the start penalty) is a positive constant (&lt; 1) used to penalize partitions that start a new clus-ter."
"After processing each of the n mentions using the above steps, the algorithm returns the N most probable partitions in H n (line 15)."
Our implementation of Luo et al.’s search algo-rithm differs from their original algorithm only in terms of the number of pruning strategies adopted.
"Specifically, Luo et al. introduce a number of heuris-tics to prune the search space in order to speed up the search."
"We employ only the beam search heuristic, with a beam size that is five times larger than theirs."
"Our larger beam size, together with the fact that we do not use other pruning strategies, implies that we are searching through a larger part of the space than them, thus potentially yielding better partitions."
"To gauge the performance of our model, we com-pare it with a Bayesian model for unsupervised coreference resolution that was recently proposed[REF_CITE]."
"In this section, we will give an overview of their model, discuss its weak-nesses and propose three modifications to the model."
"For consistency, we follow Haghighi and Klein’s (H&amp;K) notations."
"Z is the set of random variables that refer to (indices of) entities. φ z is the set of parameters associated with entity z. φ is the entire set of model parameters, which includes all the φ z ’s. Finally, X is the set of observed variables (e.g., the head of a mention)."
"Given a document, the goal is to find the most probable assignment of entity in-dices to its mentions given the observed values."
"In other words, we want to maximize P(Z|X)."
"In a Bayesian approach, we compute this probability by integrating out all the parameters."
P (Z|X) =
"Z P (Z|X, φ)P (φ|X)dφ."
"The original H&amp;K model is composed of a set of models: the basic model and two other models (namely, the pronoun head model and the salience model) that aim to improve the basic model. [Footnote_5]"
"5 H&amp;K also present a cross-document coreference model, but since it focuses primarily on cross-document coreference and improves within-document coreference performance by only 1.5% in F-score, we will not consider this model here."
The basic model generates a mention in a two-step process.
"First, an entity index is chosen according to an entity distribution, and then the head of the men-tion is generated given the entity index based on an entity-specific head distribution."
"Here, we assume that (1) all heads H are observed and (2) a mention is represented solely by its head noun, so nothing other than the head is generated."
"Furthermore, we assume that the head distribution is drawn from a symmetric Dirichlet with concentration λ H ."
"P (H i,j = h|Z, H −i,j ) ∝ n h,z + λ H where H i,j is the head of mention j in document i, and n h,z is the number of times head h is emit-ted by entity index z in (Z,H −i,j ). [Footnote_6] On the other hand, since the number of entities in a document is not known a priori, we draw the entity distribution from a Dirichlet process with concentration α, ef-fectively yielding a model with an infinite number of mixture components."
"6 H −i,j is used as a shorthand for H – {H i,j }."
"Using the Chinese restau-rant process representation (see[REF_CITE]),"
"P(Z ij = z|Z −i,j ) ∝ ( nα , if z = z new z , otherwise where n z is the number of mentions in Z −i,j labeled with entity index z, and z new is a new entity index not already in Z −i,j ."
"To perform inference, we use Gibbs sampling[REF_CITE]to gen-erate samples from this conditional distribution:"
"P (Z i,j |Z −i,j , H) ∝ P (Z i,j |Z −i,j )P (H i,j |Z, H −i,j ) where the two distributions on the right are defined as above."
"Starting with a random assignment of en-tity indices to mentions, the Gibbs sampler itera-tively re-samples an entity index according to this posterior distribution given the current assignment."
Head generation in the basic model is too simplis-tic: it has a strong tendency to assign the same en-tity index to mentions having the same head.
This is particularly inappropriate for pronouns.
"Hence, we need a different model for generating pronouns."
"Before introducing this pronoun head model, we need to augment the set of entity-specific param-eters, which currently contains only a distribution over heads (φ hZ )."
"Specifically, we add distributions φ tZ , φ gZ , and φ nZ over entity properties: φ tZ is a distribution over semantic types ( PER , ORG , LOC , MISC ), φ gZ over gender ( MALE , FEMALE , EITHER , NEUTER ), and φ nZ over number ( SG , PL )."
We assume that each of these distributions is drawn from a sym-metric Dirichlet.
"A small concentration parameter is used, since each entity should have a dominating value for each of these properties."
"Now, to estimate φ tZ , φ gZ , and φ nZ , we need to know the gender, number, and semantic type of each mention."
"For some mentions (e.g., “he”), these properties are easy to compute; for others (e.g., “it”), they are not."
"Whenever a mention has unobserved properties, we need to fill in the missing values."
"We could resort to sampling, but sampling these prop-erties is fairly inefficient."
"So, following H&amp;K, we keep soft counts for each of these properties and use them rather than perform hard sampling."
"When an entity z generates a pronoun h using the pronoun head model, [Footnote_7] it first generates a gender g, a number n, and a semantic type t independently from the distributions φ gz , φ nz , and φ tz ; and then generates h using the distribution P(H = h|G = g,N = n,T = t,θ)."
"7 While pronouns are generated by this pronoun head model, names and nominals continue to be handled by the basic model."
Note that this last distribution is a global distribution that is independent of the chosen entity index. θ is a parameter drawn from a symmet-ric Dirichlet (with concentration λ P ) that encodes our prior knowledge of the relationship between a semantic type and a pronoun.
"For instance, given the type PERSON , there is a higher probability of gener-ating “he” than “it”."
"As a result, we maintain a list of compatible semantic types for each pronoun, and give a pronoun a count of (1 + λ P ) if it is compatible with the drawn semantic type; otherwise, we give it a count of λ P ."
"In essence, we use this prior to prefer the generation of pronouns that are compatible with the chosen semantic type."
"Pronouns typically refer to salient entities, so the basic model could be improved by incorporating salience."
We start by assuming that each entity has an activity score that is initially set to zero.
"Given a set of mentions and an assignment of entity in-dices to mentions, Z, we process the mentions in a left-to-right manner."
"When a mention, m, is encoun-tered, we multiply the activity score of each entity by 0.5 and add one to the activity score of the entity to which m belongs."
This captures the intuitive notion that frequency and recency both play a role in deter-mining salience.
"Next, we rank the entities based on their activity scores and discretize the ranks into five “salience” buckets S: TOP (1), HIGH (2–3), MID (4– 6), LOW (7+), and NONE ."
"Finally, this salience in-formation is used to modify the entity distribution: [Footnote_8]"
"8 Rather than having just one probability term on the right hand side of the sampling equation, H&amp;K actually have a prod-uct of probability terms, one for each mention that appears later than mention j in the given document. However, they acknowl-edge that having the product makes sampling inefficient, and decided to simplify the equation to this form in their evaluation."
"P (Z ij = z|Z −i,j ) ∝ n z · P (M i, j |S i,j , Z) where S i,j is the salience value of the jth mention in document i, and M i,j is its mention type, which can take on one of three values: pronoun, name, and nominal."
"P(M i,j |S i,j ,Z), the distribution of men-tion type given salience, was computed from H&amp;K’s development corpus (see Table 2)."
"According to the table, pronouns are preferred for salient entities, whereas names and nominals are preferred for enti-ties that are less active."
"Next, we discuss the potential weaknesses of H&amp;K’s model and propose three modifications to it."
Relaxed head generation.
"The basic model fo-cuses on head matching, and is therefore likely to (incorrectly) posit the large airport and the small airport as coreferent, for instance."
"In fact, head matching is a relatively inaccurate indicator of coref-erence, in comparison to the “strong coreference in-dicators” shown in the first three rows of Table 1."
"To improve H&amp;K’s model, we replace head matching with these three strong indicators as follows."
"Given a document, we assign each of its mentions a head index, such that two mentions have the same head index if and only if at least one of the three strong indicators returns a value of True."
"Now, instead of generating a head, the head model generates a head index, thus increasing the likelihood that aliases are assigned the same entity index, for instance."
Note that this modification is applied only to the basic model.
"In particular, pronoun generation continues to be handled by the pronoun head model and will not be affected."
"We hypothesize that this modifica-tion would improve precision, as the strong indica-tors are presumably more precise than head match."
"While the pronoun head model naturally prefers that a pronoun be generated by an entity whose gender and number are compati-ble with those of the pronoun, the entity (index) that is re-sampled for a pronoun according to the sam-pling equation for P (Z i,j |Z −i,j , H) may still not be compatible with the pronoun with respect to gen-der and number."
The reason is that an entity in-dex is assigned based not only on the head distri-bution but also on the entity distribution.
"Since enti-ties with many mentions are preferable to those with few mentions, it is possible for the model to favor the assignment of a grammatically incompatible en-tity (index) to a pronoun if the entity is sufficiently large."
"To eliminate this possibility, we enforce the agreement constraints at the global level."
"Specifi-cally, we sample an entity index for a given mention with a non-zero probability if and only if the corre-sponding entity and the head of the mention agree in gender and number."
We hypothesize that this modi-fication would improve precision.
"In Section 3.2.3, we mo-tivate the need for salience using pronouns only, since proper names can to a large extent be resolved using string-matching facilities and are not particu-larly sensitive to salience."
"Nominals (especially def-inite descriptions), though more sensitive to salience than names, can also be resolved by simple string-matching heuristics in many cases[REF_CITE]."
"Hence, we hypothe-size that the use of salience for names and nominals would adversely affect their resolution performance, as incorporating salience could diminish the role of string match in the resolution process, according to the sampling equations."
"Consequently, we modify H&amp;K’s model by limiting the application of salience to the resolution of pronouns only."
We hypothesize that this change would improve precision.
"To evaluate our EM-based model and H&amp;K’s model, we use the[REF_CITE]coreference corpus, which is composed of three sections: Broadcast News (BNEWS), Newswire (NWIRE), and Newspaper (NPAPER)."
Each section is in turn composed of a training set and a test set.
"Due to space limitations, we will present evaluation results only for the test sets of BNEWS and NWIRE, but verified that the same performance trends can be observed on NPA-PER as well."
"Unlike H&amp;K, who report results us-ing only true mentions (extracted from the answer keys), we show results for true mentions as well as system mentions that were extracted by an in-house noun phrase chunker."
The relevant statistics of the BNEWS and NWIRE test sets are shown in Table 3.
"To score the output of the coreference models, we employ the commonly-used MUC scoring program[REF_CITE]and the recently-developed CEAF scoring program[REF_CITE]."
"In the MUC scorer, recall is computed as the percentage of coreference links in the reference partition that appear in the system partition; preci-sion is computed in the same fashion as recall, ex-cept that the roles of the reference partition and the system partition are reversed."
"As a link-based scor-ing program, the MUC scorer (1) does not reward successful identification of singleton entities and (2) tends to under-penalize partitions that have too few entities."
The entity-based CEAF scorer was pro-posed in response to these two weaknesses.
"Specif-ically, it operates by computing the optimal align-ment between the set of reference entities and the set of system entities."
"CEAF precision and recall are both positively correlated with the score of this optimal alignment, which is computed by summing over each aligned entity pair the number of mentions that appear in both entities of that pair."
"As a conse-quence, a system that proposes too many entities or too few entities will have low precision and recall."
We use a small amount of labeled data for parameter initialization for the two models.
"Specifically, for evaluations on the BNEWS test data, we use as labeled data one randomly-chosen document from the BNEWS train-ing set, which has 58 true mentions and 102 system mentions."
"Similarly for NWIRE, where the chosen document has 42 true mentions and 72 system men-tions."
"For our model, we use the labeled document to initialize the parameters."
"Also, we set N (the number of most probable partitions) to 50 and δ (the start penalty used in the Bell tree) to 0.8, the latter being recommended[REF_CITE]."
"For H&amp;K’s model, we use the labeled data to tune the concentration parameter α."
"While H&amp;K set α to 0.4 without much explanation, a moment’s thought reveals that the choice of α should reflect the frac-tion of mentions that appear in a singleton cluster."
"We therefore estimate this value from the labeled document, yielding 0.4 for true mentions (which is consistent with H&amp;K’s choice) and 0.7 for system mentions."
"The remaining parameters, the λ’s, are all set to e −4 , following H&amp;K."
"In addition, as is com-monly done in Bayesian approaches, we do not sam-ple entities directly from the conditional distribution P(Z|X); rather, we sample from this distribution raised to the power exp k−1ci , where c=1.5, i is the current iteration number that starts at 0, and k (the number of sampling iterations) is set to 20."
"Finally, due to sampling and the fact that the initial assign-ment of entity indices to mentions is random, all the reported results for H&amp;K’s model are averaged over five runs."
The Heuristic baseline.
"As our first baseline, we employ a simple rule-based system that posits two mentions as coreferent if and only if at least one of the three strong coreference indicators listed in Ta-ble 1 returns True."
"Results of this baseline, reported in terms of recall (R), precision (P), and F-score (F) using the MUC scorer and the CEAF scorer, are shown in row 1 of Tables 4 and 5, respectively."
Each row in these tables shows performance using true mentions and system mentions for the BNEWS and NWIRE data sets.
"As we can see, (1) recall is gen-erally low, since this simple heuristic can only iden-tify a small fraction of the coreference relations; (2) CEAF recall is consistently higher than MUC recall, since CEAF also rewards successful identification of non-coreference relations; and (3) precision for true mentions is higher than that for system mentions, since the number of non-coreferent pairs that satisfy the heuristic is larger for system mentions."
The Degenerate EM baseline.
Our second base-line is obtained by running only one iteration of our EM-based coreference model.
"Specifically, it starts with the M-step by initializing the model parame-ters using the labeled document, and ends with the E-step by applying the resulting model (in combi-nation with the Bell tree search algorithm) to ob-tain the most probable coreference partition for each test document."
"Since there is no parameter re-estimation, this baseline is effectively a purely su-pervised system trained on one (labeled) document."
Results are shown in row 2 of Tables 4 and 5.
"As we can see, recall is consistently much higher than precision, suggesting that the model has pro-duced fewer entities than it should."
"Perhaps more interestingly, in comparison to the Heuristic base- line, Degenerate EM performs consistently worse according to CEAF but generally better according to MUC."
"This discrepancy stems from the aforemen-tioned properties that MUC under-penalizes parti-tions with too few entities, whereas CEAF lowers both recall and precision when given such partitions."
Our EM-based coreference model.
"Our model operates in the same way as the Degenerate EM baseline, except that EM is run until convergence, with the test set being used as unlabeled data for pa-rameter re-estimation."
Any performance difference between our model and Degenerate EM can thus be attributed to EM’s exploitation of the unlabeled data.
Results of our model are shown in row 3 of Tables 4 and 5.
"In comparison to Degenerate EM, MUC F-score increases by 4-5% for BNEWS and 4-21% for NWIRE; CEAF F-score increases even more dra-matically, by 10-17%[REF_CITE]-27% for NWIRE."
Improvements stem primarily from large gains in precision and comparatively smaller loss in recall.
Such improvements suggest that our model has effectively exploited the unlabeled data.
"In comparison to the Heuristic baseline, we gener-ally see increases in both recall and precision when system mentions are used, and as a result, F-score improves substantially by 7-15%."
"When true men-tions are used, we still see gains in recall, but these gains are accompanied by loss in precision."
"F-score generally increases (by 2-22%), except for the case with NWIRE where we see a 0.5% drop in CEAF F-score as a result of a larger decrease in precision."
The Original H&amp;K model.
We use as our third baseline the Original H&amp;K model (see Section 3.2).
Results of this model are shown in row 4 of Tables 4 and 5. [Footnote_9]
"9 The H&amp;K results shown here are not directly comparable with those reported[REF_CITE], since H&amp;K evaluated their system on the[REF_CITE]coreference corpus."
"Overall, it underperforms our model by 6- 16% in MUC F-score and 6-14% in CEAF F-score, due primarily to considerable drop in both recall and precision in almost all cases."
The Modified H&amp;K model.
"Next, we incorporate our three modifications into the Original H&amp;K base-line one after the other."
Results are shown in rows 5-7 of Tables 4 and 5.
Several points deserve men-tioning.
"First, the addition of each modification im-proves the F-score for both true and system mentions in both data sets using both scorers."
These results provide suggestive evidence that our modifications are highly beneficial.
"The three modifications, when applied in combination, improve Original H&amp;K sub-stantially by 5-8% in MUC F-score and 7-12% in CEAF F-score, yielding results that compare favor-ably to those of our model in almost all cases."
"Second, the use of agreement constraints yields larger improvements with CEAF than with MUC."
"This discrepancy can be attributed to the fact that CEAF rewards the correct identification of non-coreference relations, whereas MUC does not."
"Since agreement constraints are intended primarily for dis-allowing coreference, they contribute to the success-ful identification of non-coreference relations and as a result yield gains in CEAF recall and precision."
"Third, the results are largely consistent with our hypothesis that these modifications enhance preci-sion."
"Together, they improve the precision of the Original H&amp;K baseline by 8-16% (MUC) and 11- 16% (CEAF), yielding a coreference model that compares favorably with our EM-based approach."
Comparison with a supervised model.
"Finally, we compare our EM-based model with a fully super-vised coreference resolver."
"Inspired by state-of-the-art resolvers, we create our supervised classification model by training a discriminative learner (the C4.5 decision tree induction system[REF_CITE]) with a diverse set of features (the 34 features described[REF_CITE]) on a large training set (the entire[REF_CITE]coreference training corpus), and cluster using the Bell tree search algorithm."
"The fully supervised results shown in row 8 of Tables 4 and 5 suggest that our EM-based model has room for improvements, especially when system mentions are used."
We have presented a generative model for unsuper-vised coreference resolution that views coreference as an EM clustering process.
Experimental results indicate that our model outperforms Haghighi and Klein’s (2007) coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model.
"Despite these im-provements, its performance is still not comparable to that of a fully supervised coreference resolver."
"A natural way to extend these unsupervised coref-erence models is to incorporate additional linguis-tic knowledge sources, such as those employed by our fully supervised resolver."
"However, feature en-gineering is in general more difficult for generative models than for discriminative models, as the former typically require non-overlapping features."
We plan to explore this possibility in future work.
"Machine learning approaches to coreference resolution are typically supervised, and re-quire expensive labeled data."
"Some unsuper-vised approaches have been proposed (e.g.,[REF_CITE]), but they are less accurate."
"In this paper, we present the first un-supervised approach that is competitive with supervised ones."
"This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typ-ically used in supervised methods, and by us-ing Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals."
"On MUC and ACE datasets, our model outper-forms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art su-pervised models."
The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities.
"This is a key subtask in many NLP applications, including information extraction, ques-tion answering, machine translation, and others."
"Su-pervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCal-lum &amp;[REF_CITE])."
"While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them."
This limits their applicability.
Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.
"However, unsupervised coreference resolution is much more difficult."
"Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags su-pervised ones by a substantial margin."
"Extending it appears difficult, due to the limitations of its Dirich-let process-based representation."
"The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones."
Relations that have been exploited in supervised coreference resolution include transitivity (McCal-lum &amp;[REF_CITE]) and anaphoricity (Denis &amp;[REF_CITE]).
"However, there is little work to date on joint inference for unsupervised resolution."
"We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson &amp;[REF_CITE])."
"Markov logic allows us to easily build models involving rela-tions among mentions, like apposition and predi-cate nominals."
"By extending the state-of-the-art al-gorithms for inference and learning, we developed the first general-purpose unsupervised learning al-gorithm for Markov logic, and applied it to unsuper-vised coreference resolution."
We test our approach on standard MUC and ACE datasets.
"Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one."
"Our full model, using apposition and other relations for joint inference, is often as accu-rate as the best supervised models, or more."
We begin by reviewing the necessary background on Markov logic.
"We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used."
"Finally, we present our experiments and re-sults."
"Most existing supervised learning approaches for coreference resolution are suboptimal since they re-solve each mention pair independently, only impos-ing transitivity in postprocessing[REF_CITE]."
"More-over, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be help-ful (Poon &amp;[REF_CITE])."
"Using graph parti-tioning, McCallum &amp;[REF_CITE]incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions."
"Denis &amp;[REF_CITE]determined anaphoricity and pairwise classification jointly using integer pro-gramming, but they did not incorporate transitivity or other relations."
"While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date."
"Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp;[REF_CITE], using a nonparamet-ric Bayesian model based on hierarchical Dirichlet processes."
"At the heart of their system is a mixture model with a few linguistically motivated features such as head words, entity properties and salience."
"Their approach is a major step forward in unsuper-vised coreference resolution, but extending it is chal-lenging."
"The main advantage of Dirichlet processes is that they are exchangeable, allowing parameters to be integrated out, but Haghighi and Klein forgo this when they introduce salience."
"Their model thus requires Gibbs sampling over both assignments and parameters, which can be very expensive."
Haghighi and Klein circumvent this by making approxima-tions that potentially hurt accuracy.
"At the same time, the Dirichlet process prior favors skewed clus-ter sizes and a number of clusters that grows loga-rithmically with the number of data points, neither of which seems generally appropriate for coreference resolution."
"Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon &amp;[REF_CITE]), making it difficult to leverage many linguistic regularities."
"For exam-ple, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster."
"However, Gibbs sampling can only move one mention at a time from one cluster to another, and this is unlikely to happen, because it would require breaking the apposition rule."
"Blocked sampling can alleviate this problem by sampling multiple mentions together, but it requires that the block size be predetermined to a small fixed number."
"When we incorporate apposition and other regular-ities the blocks can become arbitrarily large, mak-ing this infeasible."
"For example, suppose we also want to leverage predicate nominals (i.e., the sub-ject and the predicating noun of a copular verb are likely coreferent)."
"Then a sentence like “He is Bill Gates, the chairman of Microsoft” requires a block of four mentions: “He”, “Bill Gates”, “the chair-man of Microsoft”, and “Bill Gates, the chairman of Microsoft”."
Similar difficulties occur with other inference methods.
"Thus, extending Haghighi and Klein’s model to include richer linguistic features is a challenging problem."
"Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson &amp;[REF_CITE])."
"Like Haghighi and Klein’s, our model is cluster-based rather than pairwise, and implicitly imposes tran-sitivity."
"We do not predetermine anaphoricity of a mention, but rather fuse it into the integrated reso-lution process."
"As a result, our model is inherently joint among mentions and subtasks."
"It shares sev-eral features with Haghighi &amp; Klein’s model, but re-moves or refines features where we believe it is ap-propriate to."
"Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use."
"We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains."
"It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum &amp;[REF_CITE]nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp;[REF_CITE]))."
"In many NLP applications, there exist rich relations among objects, and recent work in statistical rela-tional learning (Getoor &amp;[REF_CITE]) and struc-tured predicti[REF_CITE]has shown that leveraging these can greatly improve accuracy."
"One of the most powerful representations for joint infer-ence is Markov logic, a probabilistic extension of first-order logic (Richardson &amp;[REF_CITE])."
A Markov logic network (MLN) is a set of weighted first-order clauses.
"Together with a set of con-stants, it defines a Markov network with one node per ground atom and one feature per ground clause."
The weight of a feature is the weight of the first-order clause that originated it.
"The probability of a state x in such a network is given by P(x) = (1/Z)exp( P i w i f i (x)), where Z is a normaliza-tion constant, w i is the weight of the ith clause, f i = 1 if the ith clause is true, and f i = 0 other-wise."
Markov logic makes it possible to compactly specify probability distributions over complex re-lational domains.
Efficient inference can be per-formed using MC-SAT (Poon &amp;[REF_CITE]).
MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm.
"Slice sampling introduces auxil-iary variables u that decouple the original ones x, and alternately samples u conditioned on x and vice-versa."
"To sample from the slice (the set of states x consistent with the current u), MC-SAT calls Sam-pleSAT[REF_CITE], which uses a combina-tion of satisfiability testing and simulated annealing."
"The advantage of using a satisfiability solver (Walk-SAT) is that it efficiently finds isolated modes in the distribution, and as a result the Markov chain mixes very rapidly."
The slice sampling scheme ensures that detailed balance is (approximately) preserved.
"MC-SAT is orders of magnitude faster than previous MCMC algorithms like Gibbs sampling, making ef-ficient sampling possible on a scale that was previ-"
"Algorithm weights,1 MC-SAT(clauses, num samples) x (0) ← Satisfy(hard clauses) for i ← 1 to num samples do M ←∅ for all c k ∈ clauses satisfied by x (i−1) do With probability 1 − e −w k add c k to M end for"
Sample x (i) ∼ U SAT(M) end for
Algorithm 1 gives pseudo-code for MC-SAT.
"At iteration i − 1, the factor φ k for clause c k is ei-ther e w k if c k is satisfied in x (i−1) , or 1 otherwise."
"MC-SAT first samples the auxiliary variable u k uni-formly from (0, φ k ), then samples a new state uni-formly from the set of states that satisfy φ 0k ≥ u k for all k (the slice)."
"Equivalently, for each k, with probability 1 − e −w k the next state must satisfy c k ."
"In general, we can factorize the probability distribu-tion in any way that facilitates inference, sample the u k ’s, and make sure that the next state is drawn uni-formly from solutions that satisfy φ 0k ≥ u k for all factors."
"MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the pred-icate and clause arities."
"We developed a general method for producing a “lazy” version of relational inference algorithms (Poon &amp;[REF_CITE]), which carries exactly the same inference steps as the original algorithm, but only maintains a small sub-set of “active” predicates/clauses, grounding more as needed."
"We showed that Lazy-MC-SAT, the lazy version of MC-SAT, reduced memory and time by orders of magnitude in several domains."
We use Lazy-MC-SAT in this paper.
"Supervised learning for Markov logic maximizes the conditional log-likelihood L(x, y) = log P (Y = y|X = x), where Y represents the non-evidence predicates, X the evidence predicates, and x, y their values in the training data."
"For simplicity, from now on we omit X, whose values are fixed and always conditioned on."
"The optimization problem is convex and a global optimum can be found using gradient descent, with the gradient being ∂w i ∂ L(y) = n i (y) − P y 0"
P(Y = y 0 ) n i (y 0 ) = n i (y) − E Y [n i ]. where n i is the number of true groundings of clause i.
The expected count can be approximated as 1 X N E Y [n i ] ≈ n i (y k ) N k=[Footnote_1] where y k are samples generated by MC-SAT.
"1 Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hes-sian. Readers are referred to their paper for details."
"To combat overfitting, a Gaussian prior is imposed on all weights."
"In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses."
Lowd &amp;[REF_CITE]used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.
"This estimates the optimal step size in each step as −d T g α = d T Hd + λd T d. where g is the gradient, d the conjugate update direc-tion, and λ a parameter that is automatically tuned to trade off second-order information with gradient descent."
"H is the Hessian matrix, with the (i,j)th entry being ∂w i ∂w j ∂ 2 L(y) ="
"E Y [n i ] · E Y [n j ] − E Y [n i · n j ] = −Cov Y [n i , n j ]."
The Hessian can be approximated with the same samples used for the gradient.
Its negative inverse diagonal is used as the preconditioner. 1
The open-source Alchemy package[REF_CITE]provides implementations of existing algo-rithms for Markov logic.
"In Section 5, we develop the first general-purpose unsupervised learning al-gorithm for Markov logic by extending the existing algorithms to handle hidden predicates. [Footnote_2]"
"2 Alchemy includes a discriminative EM algorithm, but it as-sumes that only a few values are missing, and cannot handle completely hidden predicates. Kok &amp;[REF_CITE]applied Markov logic to relational clustering, but they used hard EM."
"In this section, we present our MLN for joint unsu-pervised coreference resolution."
Our model deviates from Haghighi &amp; Klein’s (2007) in several impor-tant ways.
"First, our MLN does not model saliences for proper nouns or nominals, as their influence is marginal compared to other features; for pronoun salience, it uses a more intuitive and simpler def-inition based on distance, and incorporated it as a prior."
Another difference is in identifying heads.
"For the ACE datasets, Haghighi and Klein used the gold heads; for the MUC-6 dataset, where labels are not available, they crudely picked the rightmost token in a mention."
We show that a better way is to determine the heads using head rules in a parser.
This improves resolution accuracy and is always applicable.
"Cru-cially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein."
"In our approach, what it takes is just adding two formulas to the MLN."
"As common in previous work, we assume that true mention boundaries are given."
We do not as-sume any other labeled information.
"In particu-lar, we do not assume gold name entity recogni-tion (NER) labels, and unlike Haghighi &amp;[REF_CITE], we do not assume gold mention types (for ACE datasets, they also used gold head words)."
"We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser."
We detected pronouns using a list.
"The main query predicate is InClust(m, c!), which is true iff mention m is in cluster c."
"The “t!” notation signifies that for each m, this predicate is true for a unique value of c."
"The main evidence predicate is Head(m, t!), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head mixture model, where the mixture component priors are represented by the unit clause"
"InClust(+m, +c) and the head distribution is represented by the head prediction rule"
"InClust(m, +c) ∧ Head(m, +t)."
All free variables are implicitly universally quanti-fied.
"The “+” notation signifies that the MLN con-tains an instance of the rule, with a separate weight, for each value combination of the variables with a plus sign."
"By convention, at each inference step we name each non-empty cluster after the earliest mention it contains."
"This helps break the symmetry among mentions, which otherwise produces multiple op-tima and makes learning unnecessarily harder."
"To encourage clustering, we impose an exponential prior on the number of non-empty clusters with weight −1."
"The above model only clusters mentions with the same head, and does not work well for pronouns."
"To address this, we introduce the predicate IsPrn(m), which is true iff the mention m is a pronoun, and adapt the head prediction rule as follows: ¬IsPrn(m) ∧ InClust(m, +c) ∧ Head(m, +t)"
"This is always false when m is a pronoun, and thus applies only to non-pronouns."
Pronouns tend to resolve with men-tions that are semantically compatible with them.
"Thus we introduce predicates that represent entity type, number, and gender: Type(x, e!), Number(x, n!), Gender(x, g!), where x can be either a cluster or mention, e ∈ {Person, Organization, Location, Other}, n ∈ {Singular, Plural} and g ∈ {Male, Female, Neuter}."
"Many of these are known for pronouns, and some can be inferred from simple linguistic cues (e.g., “Ms. Galen” is a singular female person, while “XYZ Corp.” is an organization). [Footnote_3] Entity type assignment is represented by the unit clause"
"3 We used the following cues: Mr., Ms., Jr., Inc., Corp., cor-poration, company. The proportions of known properties range from 14% to 26%."
"Type(+x, +e) and similarly for number and gender."
A mention should agree with its cluster in entity type.
This is ensured by the hard rule (which has infinite weight and must be satisfied)
"InClust(m, c) ⇒ (Type(m, e) ⇔ Type(c, e))"
There are similar hard rules for number and gender.
"Different pronouns prefer different entity types, as represented by"
"IsPrn(m) ∧ InClust(m, c) ∧Head(m, +t) ∧ Type(c, +e) which only applies to pronouns, and whose weight is positive if pronoun t is likely to assume entity type e and negative otherwise."
There are similar rules for number and gender.
"Aside from semantic compatibility, pronouns tend to resolve with nearby mentions."
"To model this, we impose an exponential prior on the distance (number of mentions) between a pronoun and its antecedent, with weight −1. [Footnote_4]"
"4 For simplicity, if a pronoun has no antecedent, we define the distance to be ∞. So a pronoun must have an antecedent in our model, unless it is the first mention in the document or it can not resolve with previous mentions without violating hard con-straints. It is straightforward to soften this with a finite penalty."
"This is similar to Haghighi and Klein’s treatment of salience, but simpler."
Syntactic relations among mentions often suggest coreference.
Incorporating such relations into our MLN is straightforward.
We illustrate this with two examples: apposition and predicate nominals.
"We introduce a predicate for apposition, Appo(x, y), where x, y are mentions, and which is true iff y is an appositive of x. We then add the rule"
"Appo(x, y) ⇒ (InClust(x, c) ⇔ InClust(y, c)) which ensures that x, y are in the same cluster if y is an appositive of x."
"Similarly, we introduce a predi-cate for predicate nominals, PredNom(x, y), and the corresponding rule. [Footnote_5] The weights of both rules can be learned from data with a positive prior mean."
"5 We detected apposition and predicate nominatives using simple heuristics based on parses, e.g., if (NP, comma, NP) are the first three children of an NP, then any two of the three noun phrases are apposition."
"For simplicity, in this paper we treat them as hard con-straints."
"We also consider a rule-based system that clusters non-pronouns by their heads, and attaches a pro-noun to the cluster which has no known conflicting type, number, or gender, and contains the closest an-tecedent for the pronoun."
This system can be en-coded in an MLN with just four rules.
"Three of them are the ones for enforcing agreement in type, num-ber, and gender between a cluster and its members, as defined in the base MLN."
"The fourth rule is ¬IsPrn(m1) ∧ ¬IsPrn(m2) ∧Head(m1, h1) ∧ Head(m2, h2) ∧InClust(m1, c1) ∧ InClust(m2, c2) ⇒ (c1 = c2 ⇔ h1 = h2)."
"With a large but not infinite weight (e.g., 100), this rule has the effect of clustering non-pronouns by their heads, except when it violates the hard rules."
The MLN can also include the apposition and predicate-nominal rules.
"As in the base MLN, we impose the same exponential prior on the number of non-empty clusters and that on the distance between a pronoun and its antecedent."
"This simple MLN is remarkably competitive, as we will see in the exper-iment section."
Unsupervised learning in Markov logic maximizes the conditional log-likelihood
"L(x, y) = log P (Y = y|X = x) = log P z P (Y = y, Z = z|X = x) where Z are unknown predicates."
"In our coref-erence resolution MLN, Y includes Head and known groundings of Type,Number and Gender, Z includes InClust and unknown groundings of Type,Number,Gender, and X includes IsPrn, Appo and PredNom. (For simplicity, from now on we drop X from the formula.)"
"With Z, the opti-mization problem is no longer convex."
"However, we can still find a local optimum using gradient descent, with the gradient being ∂ L(y) ="
"E Z|y [n i ] − E Y,Z [n i ] ∂w i where n i is the number of true groundings of the ith clause."
We extended PSCG for unsupervised learn-ing.
"The gradient is the difference of two expec-tations, each of which can be approximated using samples generated by MC-SAT."
"The (i, j)th entry of the Hessian is now ∂ 2 L(y) ="
"Cov Z|y [n i , n j ] − Cov Y,Z [n i , n j ] ∂w i ∂w j and the step size can be computed accordingly."
"Since our problem is no longer convex, the nega-tive diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner."
We also adjusted λ more conserva-tively than Lowd &amp;[REF_CITE].
"Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather suf-ficient statistics for learning."
We developed an ef-ficient parallelized implementation of our unsuper-vised learning algorithm using the message-passing interface (MPI).
"Learning in MUC-6 took only one hour, and[REF_CITE]two and a half."
"To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT[REF_CITE], rather than a random solution to the hard clauses."
"In the existing implementation in Alchemy[REF_CITE], SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!))."
"Such predicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications."
We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied.
"By default, MC-SAT treats each ground clause as a separate factor while de-termining the slice."
This can be very inefficient for highly correlated clauses.
"For example, given a non-pronoun mention m currently in cluster c and with head t, among the mixture prior rules involv-ing m InClust(m, c) is the only one that is satisfied, and among those head-prediction rules involving m, ¬IsPrn(m)∧InClust(m, c)∧Head(m, t) is the only one that is satisfied; the factors for these rules mul-tiply to φ = exp(w m,c + w m,c,t ), where w m,c is the weight for InClust(m, c), and w m,c,t is the weight for ¬IsPrn(m) ∧ InClust(m, c) ∧ Head(m, t), since an unsatisfied rule contributes a factor of e 0 = 1."
We extended MC-SAT to treat each set of mutually ex-clusive and exhaustive rules as a single factor.
"E.g., for the above m, MC-SAT now samples u uniformly from (0, φ), and requires that in the next state φ 0 be no less than u. Equivalently, the new cluster and head for m should satisfy w m,c 0 + w m,c 0 ,t 0 ≥ log(u)."
"We extended SampleSAT so that when it consid-ers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints."
"The final clustering is found using the MaxWalk-SAT weighted satisfiability solver[REF_CITE], with the appropriate extensions."
"We first ran a MaxWalkSAT pass with only finite-weight formu-las, then ran another pass with all formulas."
We found that this significantly improved the quality of the results that MaxWalkSAT returned.
We implemented our method as an extension to the Alchemy system[REF_CITE].
"Since our learn-ing uses sampling, all results are the average of five runs using different random seeds."
"Our optimiza-tion problem is not convex, so initialization is im-portant."
The core of our model (head mixture) tends to cluster non-pronouns with the same head.
"There-fore, we initialized by setting all weights to zero, and running the same learning algorithm on the base MLN, while assuming that in the ground truth, non-pronouns are clustered by their heads. (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.)"
"6 Each sample actually contains a large number of ground-ings, so 100 samples yield sufficiently accurate statistics for learning."
"We conducted experiments on MUC-6,[REF_CITE]and ACE Phrase-2 (ACE-2)."
We evaluated our sys-tems using two commonly-used scoring programs: MUC[REF_CITE]and B 3 (Amit &amp;[REF_CITE]).
"To gain more insight, we also report pairwise resolution scores and mean absolute error in the number of clusters."
The MUC-6 dataset consists of 30 documents for testing and 221 for training.
"To evaluate the contri-bution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect."
"We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries."
"In this experiment, the base MLN was used, and the head was chosen crudely as the rightmost token in a mention."
"Our system was run on each test document separately, using a minimum of training data (the document itself)."
MLN-30 Our system was trained on all 30 test doc-uments together.
This tests how much can be gained by pooling information.
"MLN-H The heads were determined using the head rules in the Stanford parser (Klein &amp;[REF_CITE]), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.”"
MLN-HA The apposition rule was added.
MLN-HAN The predicate-nominal rule was added.
This is our full model.
We also compared with two rule-based MLNs:
"RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule."
"Past results on ACE were obtained on different releases of the datasets, e.g.,[REF_CITE]used the[REF_CITE]training corpus,[REF_CITE]and[REF_CITE]used ACE Phrase-2, and[REF_CITE]used the[REF_CITE]formal test set."
"In this paper, we used the[REF_CITE]training corpus and ACE Phrase-2 (ACE-2) to enable direct comparisons with Haghighi &amp;[REF_CITE],[REF_CITE], and[REF_CITE]."
"Due to license restrictions, we were not able to obtain the[REF_CITE]formal test set and so cannot compare directly[REF_CITE]."
"The En-glish version of the[REF_CITE]training corpus con-tains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively."
ACE-2 contains a training set and a test set.
"In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively."
"Table 1 compares our system with previous ap-proaches on the MUC-6 dataset, in MUC scores."
"Our approach greatly outperformed Haghighi &amp;[REF_CITE], the state-of-the-art unsupervised sys-tem."
"Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outper-formed it trained on 381 documents."
"Training on the 30 test documents together resulted in a signif-icant gain. (We also ran experiments using more documents, and the results were similar.)"
"Better head identification (MLN-H) led to a large improve-ment in accuracy, which is expected since for men-tions with a right modifier, the rightmost tokens con-fuse rather than help coreference (e.g., “the chair-man of Microsoft”)."
Notice that with this improve-ment our system already outperforms a state-of-the- art supervised system (McCallum &amp;[REF_CITE]).
"Leveraging apposition resulted in another large im-provement, and predicate nominals also helped."
"Our full model scores about 9% higher than Haghighi &amp;[REF_CITE], and about 6% higher than McCallum &amp;[REF_CITE]."
"To our knowledge, this is the best coreference accuracy reported on MUC-6 to date. [Footnote_7] The B 3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B 3 .)"
"7 As pointed out by Haghighi &amp;[REF_CITE],[REF_CITE]obtained a very high accuracy on MUC-6, but their sys-tem used gold NER features and is not directly comparable."
"Interest-ingly, the rule-based MLN (RULE) sufficed to out-perform Haghighi &amp;[REF_CITE], and by using bet-ter heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp;[REF_CITE], the supervised system."
"The MLNs with learning (MLN-30 and MLN-HAN), on the other hand, substantially outperformed the corre-sponding rule-based ones."
Table 2 compares our system to Haghighi &amp;[REF_CITE]on the[REF_CITE]training set in MUC scores.
"Again, our system outperformed theirs by a large margin."
"The B 3 scores of MLN-HAN on the[REF_CITE]dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp;[REF_CITE]did not report B 3 .)"
"Due to license re-strictions, we could not compare directly[REF_CITE], who reported overall B 3 -[REF_CITE].3 on the formal test set."
"Tables 3 and 4 compare our system to two re-cent supervised systems,[REF_CITE]and Denis &amp;[REF_CITE]."
Our approach significantly outperformed[REF_CITE].
"It tied with Denis &amp;[REF_CITE]on NWIRE, and was somewhat less accurate on BNEWS and NPAPER."
B 3 suffers less from this prob-lem but is not perfect.
"Thus we also report pairwise resolution scores (Table 5), the gold number of clus-ters, and our mean absolute error in the number of clusters (Table 6)."
"Systems that simply merge all mentions will have exceedingly low pairwise preci- sion (far below 50%), and very large errors in the number of clusters."
"Our system has fairly good pair-wise precisions and small mean error in the number of clusters, which verifies that our results are sound."
Many of our system’s remaining errors involve nom-inals.
Additional features should be considered to distinguish mentions that have the same head but are different entities.
"For pronouns, many remaining er-rors can be corrected using linguistic knowledge like binding theory and salience hierarchy."
"Our heuris-tics for identifying appositives and predicate nomi-nals also make many errors, which often can be fixed with additional name entity recognition capabilities (e.g., given “Mike Sullivan, VOA News”, it helps to know that the former is a person and the latter an organization)."
"The most challenging case involves phrases with different heads that are both proper nouns (e.g., “Mr. Bush” and “the White House”)."
Handling these cases requires domain knowledge and/or more powerful joint inference.
This paper introduces the first unsupervised coref-erence resolution system that is as accurate as su-pervised systems.
"It performs joint inference among mentions, using relations like apposition and predi-cate nominals."
"It uses Markov logic as a representa-tion language, which allows it to be easily extended to incorporate additional linguistic and world knowl-edge."
"Future directions include incorporating addi-tional knowledge, conducting joint entity detection and coreference resolution, and combining corefer-ence resolution with other NLP tasks."
"This paper investigates two strategies for im-proving coreference resolution: (1) training separate models that specialize in particu-lar types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classification function."
"In addition to being conceptually simple, these modifications of the standard single-model, classification-based approach also deliver sig-nificant performance improvements."
"Specifi-cally, we show that on the ACE corpus both strategies produce f-score gains of more than 3% across the three coreference evaluation metrics ( MUC , B 3 , and CEAF )."
"Coreference resolution is the task of partitioning a set of entity mentions in a text, where each par-tition corresponds to some entity in an underlying discourse model."
"While early machine learning ap-proaches for the task relied on local, discriminative classifiers[REF_CITE], more recent ap-proaches use joint and/or global models[REF_CITE]."
"This shift im-proves performance, but the systems are consider-ably more complex and often less efficient."
"Here, we explore two simple modifications of the first type of approach that yield performance gains which are comparable, and sometimes better, to those obtained with these more complex systems."
"These modifica-tions involve: (i) the use of rankers instead of clas- sifiers, and (ii) the use of linguistically motivated, specialized models for different types of mentions."
Ranking models provide a theoretically more ad-equate and empirically better alternative approach to pronoun resolution than standard classification-based approaches[REF_CITE].
"In essence, ranking models directly capture during training the competition among potential antecedent candidates, instead of considering them indepen-dently."
This gives the ranker additional discrimina-tive power and in turn better antecedent selection ac-curacy.
"Here, we show that ranking is also effective for the wider task of coreference resolution."
"Coreference resolution involves several different types of anaphoric expressions: third-person pro-nouns, speech pronouns (i.e., first and second person pronouns), proper names, definite descriptions and other types of nominals (e.g., anaphoric uses of in-definite, quantified, and bare noun phrases)."
"Differ-ent anaphoric expressions exhibit different patterns of resolution and are sensitive to different factors ([REF_CITE]; van der[REF_CITE]), yet most machine learning approaches have ignored these differences and handle these different phenomena with a single, monolithic model."
A few exceptions are worth noting.
Other partially capture the differential preferences between different anaphors via different sample selection strategies during train-ing[REF_CITE].
"More recently,[REF_CITE]use the distinc-tion between pronouns, nominals and proper nouns in their unsupervised, generative model for corefer-ence resolution; for their model, this is absolutely critical for achieving better accuracy."
"Here, we show that using specialized models for different types of referential expressions improves performance for supervised models (both classifiers and rankers)."
"Both these strategies lead to improvements for all three standard coreference metrics: MUC[REF_CITE], B 3[REF_CITE], and CEAF[REF_CITE]."
"In particular, our specialized ranker system provides absolute f-score improve-ments against an otherwise identical standard clas-sifier system by 3.2%, 3.1%, and 3.6% for MUC , B 3 , and CEAF , respectively."
"Numerous approaches to anaphora and coreference resolution reduce these tasks to a binary classifica-tion task, whereby pairs of mentions are classified as coreferential or not[REF_CITE]."
"Usually used in combination with a greedy right-to-left clus-tering, these approaches make very strong indepen-dence assumptions."
"Not only do they model each coreference decision separately, they actually model each pair of mentions as a separate event."
Recast-ing these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion.
"Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse se-lecti[REF_CITE], parse reranking[REF_CITE], question-answering[REF_CITE]."
The twin-candidate classification approach pro-posed[REF_CITE]shares some similarities with the ranker in making the comparison between candidate antecedents part of training.
"An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fash-ion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once."
"Another advantage of the ranking approach is that its com- plexity is only square in the number of mentions, while that of the twin-candidate model is cubic (see[REF_CITE]for a more detailed comparison in the context of pronoun resolution)."
"Our ranking models for coreference take the fol-lowing log-linear form: m exp P w j f j (π, α i ) j=1 P rk (α i |π) = (1) m P exp P w j f j (π, α k ) k j=1 where π stands for the anaphoric expression, α i for an antecedent candidate, f j the weighted features of the model."
The denominator consists of a normal-ization factor over the k candidate mentions.
"Model parameters were estimated with the limited memory variable metric algorithm and Gaussian smoothing (σ 2 =1000), using TADM[REF_CITE]."
"For the training of the different ranking models, we use the following procedure."
"For each model, in-stances are created by pairing each anaphor of the proper type (e.g., definite description) with a set of candidates which contains: (i) a true antecedent, and (ii) a set of non-antecedents."
"The selection of the true antecedent varies depending on the model we are training: for pronominal forms, the antecedent is selected as the closest preceding mention in the chain; for non-pronominal forms, we used the clos-est preceding non-pronominal mention in the chain as the antecedent."
"For the creation of the non-antecedent set, we collect all the non-antecedents that appear in a window of two sentences around the antecedent. [Footnote_1]"
"1 We suspect that different varying windows might be more appropriate for different types of expressions, but leaves this for further investigations."
"At test time, we consider all preceding mentions as potential antecedents."
"Not all referential expressions in a given docu-ment are anaphors: some expressions introduce a discourse entity, rather than accessing an existing one."
"Thus, coreference resolvers must have a way of identifying such “discourse-new” expressions."
"This is easily handled in the standard classification ap-proach: a mention will not be resolved if none of its candidates is classified positively (i.e., as coreferen-tial)."
"The problem is more troublesome for rankers, which always pick an antecedent from the candidate set."
A natural solution is to use a model that specifi-cally predicts the discourse status (discourse-new vs. discourse-old) of each expression: only expressions that are classified as “discourse-old” by this model are considered by rankers.
Ng and Cardie[REF_CITE]introduced the use of an “anaphoricity” classifier to act as a fil-ter for coreference resolution in order to correct er-rors where antecedents are mistakenly identified for non-anaphoric mentions or antecedents are not de-termined for mentions which are indeed anaphoric.
"Their approach produced significant improvements in precision, but with consequent larger losses in re-call."
"By using joint inference for anaphoricity and coreference,[REF_CITE]avoid cascade-induced errors without the need to separately optimize the threshold."
We use a similar discourse status classifier to Ng and Cardie’s as a filter on mentions for our rankers.
"We rely on three main types of information sources: (i) the form of mention (e.g., type of linguistic ex-pression, number of tokens), (ii) positional features in the text, (iii) comparisons of the given mention to the mentions that precede it in the text."
"Evaluated on the ACE datasets, training the model on the train texts, and applying the classifier to the devtest texts, the model achieves an overall accuracy score of 80.8%, compared to a baseline of 59.7% when predicting the majority class (“discourse-old”)."
"Our second strategy is to use different, specialized models for different referential expressions, simi-larly to Elwell and Baldridge’s (2008) use of connec-tive specific models for identifying the arguments of discourse connectives."
"For this, one must determine along which dimension to split such expressions."
"For example,[REF_CITE]learns models for each set of anaphors that are lexically identical (e.g., I, he, they, etc.)."
"This option is possible for closed sets like pronouns, but not for other types of anaphors like proper names and definite descriptions."
"Another option is to rely on the particular linguistic form of the different expressions, as signaled by the head word category and the determiner (if any)."
"More concretely, we use separate models for the follow- ing types: (i) third person pronouns, (ii) speech pro-nouns, (iii) proper names, (iv) definite descriptions, and (v) others (i.e., all expressions that don’t fall into the previous categories)."
The correlation between the form of a referen-tial expression and its anaphoric behavior is actually central to various linguistic accounts[REF_CITE].
"Basically, the idea is that linguistic form is an indicator of the status of the corresponding referent in the discourse model."
"That is, the use by the speaker of a particular lin-guistic form corresponds to a particular level of acti-vation (or familiarity or salience or accessibility) in (what she thinks is) the addressee’s discourse model."
"For many authors, the relation takes the form of a continuum and is often represented in the form of a referential hierarchy, such as:"
Zero pronouns &gt;&gt; Pronouns &gt;&gt; Demonstra-tive pronouns &gt;&gt; Demonstrative NPs &gt;&gt; Short PNs &gt;&gt; Definite descriptions &gt;&gt; Full PNs &gt;&gt; Full PNs + appositive
"The higher up, the more accessible (or salient) the entity is."
At the extremes are pronouns (these forms typically require a previous mention in the local con-text) and proper names (these forms are often used without previous mentions of the entity).
This type of hierarchy is validated by corpus studies of the distribution of different types of expressions.
"For instance, pronouns find their antecedents very lo-cally (in a window of 1-2 sentences), while proper names predominantly find theirs at longer distances[REF_CITE]. [Footnote_2]"
2 Haghighi and Klein’s (2007) generative coreference model mirrors this in the posterior distribution which it assigns to men-tion types given their salience (see their Table 1).
"Using discourse structure,[REF_CITE]show that while anaphoric pronouns sys-tematically obey the right-frontier constraint (i.e., their antecedents have to appear on the right edge of the discourse graph), this is less so for definites, and even less so for proper names."
"From a machine learning perspective, these find-ings suggest that features encoding some aspect of salience (e.g., distance, syntactic context) are likely to receive different sets of parameters depending on the form of the anaphor."
"This therefore suggests that better parameters are likely to be learned in the context of different models. 3 While the above stud-ies focus primarily on salience, there are of course other dimensions according to which anaphors differ in their resolution preferences."
"Thus, the resolution of lexical expressions like definite descriptions and proper names is likely to benefit from the inclusion of features that compare the strings of the anaphor and the candidate antecedent (e.g., string matching) and features that identify particular syntactic config-urations like appositive structures."
This type of in-formation is however much less likely to help in the resolution of pronominal forms.
"The problem is that, within a single model, such features are likely to re-ceive strong parameters (due to the fact that they are good predictors for lexical anaphors) in a way that might eventually hurt pronominal resolutions."
Note that our split of referential types only partially cover the referential hierarchies[REF_CITE]or[REF_CITE].
"Thus, there is no sep-arate model for demonstrative noun phrases and pro-nouns: these are very rare in the corpus we used (i.e., the ACE corpus). [Footnote_4] These expressions were therefore handled through the “others” model."
4 There are only 114 demonstrative NP s and 12 demonstra-tive pronouns in the entire ACE training.
"There is how-ever a model for first and second person pronouns (i.e., speech pronouns): this is justified by the fact that these pronouns behave differently from their third person counterparts."
"These forms indeed of-ten behave like deictics (i.e., they refer to discourse participants) or they appear within a quote."
"The total number of anaphors (i.e., of mentions that are not chain heads) in the data is 19, 322 and 4, 599 for training and testing, respectively."
The dis-tribution of each anaphoric type is presented in Ta-ble 1.
"Roughly, third person pronouns account for 22-24% of all anaphors in the entire corpus, speech pronouns for 11-13%, proper names for 33-40%, and definite descriptions for 16-17%."
"The distribu-tion is slightly different from one dataset to another, probably reflecting genre differences."
"For instance, BNEWS shows a larger proportion of pronouns in general (pronominal forms account for 40-44% of all the anaphoric forms)."
"We use five broad types of features for all mention types, plus three others used by specific types, sum-marized in Table 3."
"Our feature extraction relies on limited linguistic processing: we only made use of a sentence detector, a tokenizer, a POS tagger (as pro-vided by the OpenNLP Toolkit [URL_CITE] ) and the WordNet [URL_CITE] database."
"Since we did not use parser, lexical heads for the NP mentions were computed using simple heuristics relying solely on POS sequences."
"Table 2 describes in detail the entire feature set, and Table [Footnote_3] shows which features were used for which models."
3 Another possible approach would consist in introducing different salience-based features encoding the form of the anaphor.
"Linguistic form: the referential form of the an-tecedent candidate: a proper name, a definite de-"
"Context: the context of the antecedent candidate: these features can be seen as approximations of the grammatical roles, as indicators of the salience of the potential candidate[REF_CITE]."
"For instance, this includes the part of speech tags sur-rounding the candidate, as well as a feature that indicates whether the potential antecedent is the first mention in a sentence (approximating subject-hood), and a feature indicating whether the candi-date is embedded inside another mention."
"Distance: the distance between the anaphor and the candidate, measured by the number of sentences and mentions between them."
"Morphosyntactic agreement: indicators of the gender, number, and person of the two mentions."
"These are determined for non-pronominal NP s with heuristics based on POS tags (e.g., NN vs. NNS for number) and actual mention strings (e.g., whether the mention contains a male/female first name or honorific for gender)."
"These features consist of pairs of attributes, ensuring that not only strict agreement (e.g., singular-singular) but also mere compatibility (e.g., masculine-unknown) is captured."
Semantic compatibility: features designed to as-sess whether the two mentions are semantically compatible.
"For these features, we use the Word-Net database: in particular, we collected the syn-onym set (or synset) as well as the synset of their direct hypernyms associated with each mention."
"In the case of common nouns, we used the synset asso-ciated with the first sense associated with the men-tion’s head word."
"In the case of proper names, we used the synset associated with the name if avail-able, and the string itself otherwise."
"For pronouns (which are not part of Wordnet), we simply used the pronominal form."
All these features were used in all five models.
"While one may question the use of distance for non-pronominal anaphors, [Footnote_7] their inclusion can be justi-fied in that they might predict some “obviation” ef-fects."
"7 In fact,[REF_CITE]does not use distance in this case."
"Definite descriptions and proper names are sensitive to distance too, although not in the same way as pronouns are: they show a preference for an-tecedents that appear outside a window of one or two sentences[REF_CITE]."
Several features are used only for particular men-tion types:
String similarity: similarity of the anaphor and the candidate strings.
"Examples are perfect string match, substring matches, and head match (i.e., the two mentions share the same head word)."
Appositive: whether the anaphor is an appositive of the antecedent candidate.
"Since we do not have access to syntactic structure, we use heuristics (e.g., the presence of a comma between the two mentions) to extract this feature."
"Acronym: whether the anaphor string is an acronym of the candidate string (or vice versa): e.g., NSF and National Science Foundation."
We evaluate several systems to explore the effect of ranking versus classification and specialized versus monolithic models.
The different systems follow a generic architecture.
Let M be the set of mentions present in a document.
"For all models, each mention m ∈ M is associated at test time with a set of an-tecedent candidates C m , which includes all the men-tions that linearly precede m. The best candidate is determined by the model in use."
"The final output of each system consists in a list of mention pairs (i.e., the coreference links) which in turn defines (through reflexive, transitive closure) a partition over the set M. Our models are summarized in Table 4."
The use of the discourse status filter is straightfor-ward.
"For each mention m∈M, the discourse status model is first applied to determine whether m intro-duces a new discourse entity (i.e., it is classified as “new”) or refers back to an existing entity (i.e., it is classified as “old”)."
"If m is classified as “new”, the process terminates and goes to the next mention."
"If m is classified as “old”, m along with its set of antecedent candidates C m is sent to the model."
"For classifiers, we replicate the procedures[REF_CITE]."
"During training, instances are formed by pairing each anaphor with each of its pre-ceding candidates, until the antecedent is reached: the closest preceding antecedent in the case of a pronominal anaphor, or the closest non-pronominal antecedent for other anaphor types."
"For classifiers, the use of a discourse status filter at test time is op-tional."
"When a filter is not used, then a mention is left unresolved if none of the pairs created for a given mention is classified positively."
"If several pairs for a given mention are classified positively, then the pair with the highest score is selected (i.e., “Best-First” link selection)."
"If a filter is used, then the can-didate with the highest score is selected, even if the probability of coreference is less than one-half. [Footnote_8]"
"8 This is very similar to the approach[REF_CITE]. An important difference is that their system does not necessarily yield an antecedent for each of the anaphors pro-posed by the discourse status model. In their system, if the coreference classifier finds that none of the candidates for a “new” mention are coreferential, it leaves it unresolved. In this case, the coreference model acts as an additional filter. Not sur-prisingly, these authors report gains in precision but compar-atively larger losses in recall. Our development experiments revealed that forcing a decision on items identified as new pro-vided performed better across all metrics."
"The use of specialized models is simple, for both classifiers and rankers."
"Specialized models are cre-ated for: (i) third person pronouns, (ii) speech pro-nouns, (iii) proper names, (iv) definite descriptions, (v) other types of phrases."
"The mention type is de- termined and the best candidate is chosen by the appropriate model[REF_CITE], these models could be interpolated with a monolithic model, or even word specific models, but we have not explored that option here."
The feature sets for the classifiers in the base-line systems includes all the features that were used for the described in Section 3.
"For the classi-fiers that do not use specialized models ( CLASS and CLASS + DS ), we have also added extra features de-scribing the linguistic form of the potential anaphor (whether it is a pronoun, a proper name, and so on)."
This is in accordance with standard feature sets in the pairwise approach.
"It gives these models a chance to learn weights more appropriately for the different types within a single, monolithic model."
We use the ACE corpus (Phase 2).
"The corpus has three parts, each corresponding to a different genre: newspaper texts ( NPAPER ), newswire texts ( NWIRE ), and broadcast news ( BNEWS )."
Each set is split into a train part and a devtest part.
"In our experi-ments, we consider only true ACE mentions."
We first evaluate the specialized ranker models individually on the task of anaphora resolution: their ability to select a correct antecedent for each anaphor.
"Following common practice in this task, we report results in terms of accuracy, which is sim-ply the ratio of correctly resolved anaphors."
The candidate set during testing was formed by taking all the mentions that appear before the anaphor.
"Also, we assume that correctly resolving an anaphor amounts to selecting any of the previous mentions in the entity as the antecedent."
The accuracy scores for the different models are presented in Table 5.
"The best accuracy results on the entire ACE cor-pus are found first for the proper name resolver with a score of 83.5%, then for the third person pronoun resolver with 82.2%, then for the definite descrip-tion and speech pronoun resolvers with 66.9% and 66.5% respectively."
The worst scores are obtained for the “others” category.
"The high scores for the third person pronoun and the proper name rankers most likely follow from the fact that the resolution of these expressions relies on simple, reliable pre-dictors, such as distance and morphosyntactic agree-ment for pronouns, and string similarity features for proper names."
"The resolution of definite descrip-tions and other types of lexical NP s (which are han-dled through the “others” model) are much more challenging: they rely on lexical semantic and world knowledge, which is only partially encoded via our WordNet-based features."
"Finally, note that the reso-lution of speech pronouns is also much harder than that of the other pronominal forms: these expres-sions are much less (if at all) constrained by re-cency and agreement."
"Furthermore, these expres-sions show a lot of cataphoric uses, which are not considered by our models."
The low scores for the “others” category is likely due to the fact that it en-compasses very different referential expressions.
"For evaluating the coreference performance, we rely on three primary metrics: (i) the link based MUC metric[REF_CITE], the mention based B 3 metric[REF_CITE], and the entity based CEAF metric[REF_CITE]."
"Common to these metrics is: (i) they operate by comparing the set of chains S produced by the system against the true chains T , and (ii) they report performance in terms of recall and precision."
"There are however impor-tant differences in how each metric computes these scores, each producing a different bias."
MUC scores are based on the number of links (pairs of mentions) common to S and T .
Recall is the number of common links divided by the to-tal number of links in T ; precision is the number of common links divided by the total number of links in S.
This focus gives MUC two main biases.
"First, it favors systems that create large chains (and thus fewer entities)."
"For instance, a system that produces a single chain achieves 100% recall without severe degradation in precision."
"Second, it ignores single mention entities, which are involved in no links. [Footnote_9]"
9 It is worth noting that the MUC corpus does not annotate single mention entities.
The B 3 metric was designed to address the MUC metric’s shortcomings.
It is mention-based: it com-putes both recall and precision scores for each men-tion i.
"Let S be the system chain containing m, T be the true chain containing m."
The set of correct elements in S is thus |S ∩ T|.
"The recall score for a mention i is |S∩T|T| | , while the precision score for i is |S∩T|S| | ."
Overall recall/precision is obtained by av-eraging over the individual mention scores.
The fact that this metric is mention-based by definition solves the problem of single mention entities.
"Also solved is the bias favoring larger chains, since this will be penalized in the precision score of each mention."
The Constrained Entity Aligned F-Measure ( CEAF )[REF_CITE]. aligns each system chain S with at most one true chain T .
"It finds the best one-to-one mapping between the set of chains S and T , which is equivalent to finding the optimal alignment in a bipartite graph."
"The best mapping maximizes the similarity over pairs of chains (S i ,T i ), where the similarity between two chains is the number of common mentions to the two chains."
"With CEAF , recall is computed as the total similarity divided by the number of mentions in all the T (i.e., the self-similarity), while precision is the total similarity di-vided by the number of mentions in S."
Table 6 gives scores for all three metrics for the different models on the entire ACE corpus.
Two main patterns emerge: sig-nificant improvements are obtained by using specialized models ( CLASS vs CLASS + SP and CLASS + DS vs CLASS + DS + SP ) and by using a ranker ( CLASS + DS + SP vs RANK +
DS + SP ).
"Overall, the RANK + DS + SP system significantly outperforms the other systems on the three different metrics. [Footnote_10]"
"10 Statistical significance was determined with t-tests for both recall and precision scores, with p &lt; 0.05."
The f-scores for RANK +
"DS +[REF_CITE].6% with the[REF_CITE].7% with the B 3 , and 67.0% with the CEAF metric."
"These scores place the RANK + DS + SP among the best coreference resolu-tion systems, since most existing systems are typi-cally under the bar of the 70% in f-score with the"
MUC and B 3 metrics[REF_CITE].
"An interesting point of comparison is provided[REF_CITE], who also relies on true mentions and reports MUC f-scores only slightly superior to ours (73.8%) while relying on perfect semantic class information."
His best results otherwise are 64.6%.
"The fact that our improvements are consistent across the different evaluation metrics is remarkable, especially given that these three metrics are quite different in the way they compute their scores."
"The gains in f-score range from 1.2 to 5.4% on the MUC metric (i.e., error reductions of 4 to 15.9%), from 1.4 to 3.5% on the B 3 metric (i.e., error reductions of 4.8 to 11.4%), and from 1.7 to 4.7% on the CEAF met-ric (i.e., error reductions of 6.9 to 17%)."
"The larger improvements come from recall, with improvements ranging from 1.9 to 7.1% with MUC , from 2.4 to 5.6% with B 3 . [Footnote_11]"
"11 Recall that recall and precision scores are identical with CEAF , due to the fact that we are using true mention boundaries."
This suggests that RANK + DS + SP predicts many more valid coreference links than the other systems.
Smaller but still significant gains are made in precision: RANK +
DS + SP is also able to re-duce the proportion of invalid links.
The overall improvements found with RANK + DS + SP suggest that it is able to capi-talize on the better antecedent selection capabilities offered by the ranking approach.
This is supported by the error analysis on the development data.
"Errors made by a coreference system can be con-ceptualized as falling into three main classes: (i) “missed anaphors” (i.e., an anaphoric mention that fails to be linked to a previous mention), (ii) “spuri-ous anaphors” (i.e., an non-anaphoric mention that is linked to a previous mention), and (iii) “invalid resolutions” (i.e., a true anaphor that is linked to a incorrect antecedent)."
"The two first types of error pertain to the determination of the discourse status of the mention, while the third regards the selection of an antecedent (i.e., anaphora resolution)."
"Con-sidering the systems’ invalid resolutions, we found that the RANK + DS + SP had a much lower error rate: only 17.9% of all true anaphors were incorrectly resolved by this system, against 23.1% for CLASS , 24.9% for CLASS + DS , 20.4% for CLASS + SP , and 22.1% for CLASS + DS + SP ."
"Despite the fact that this MUC score beats RANK + DS + SP ’s, it is ac-tually worse than even the basic model CLASS for B 3 and CEAF ."
"This difference fact that MUC gives more recall credit for large chains without a conse-quent precision reduction, and shows the importance of using B 3 and CEAF scores in addition to MUC ."
"The best model reported there ( JOINT - DS - NE - AE - ILP ) obtains f-scores of 70.1%, 72.7%, and 66.2% for MUC , B 3 , and CEAF , respectively."
"Interestingly, RANK + DS + SP actually performs better across all metrics despite being a simpler model with fewer sources of information."
Using specialized rankers with a discourse status classifier yields coreference performance superior to that given by various classification-based baseline systems.
"Crucially, these improvements have been possible using a discourse status model that has an accuracy of just 80.8%."
"Clearly, the performance of the discourse status module has a direct impact on the performance of the entire coreference sys-tem."
"On the one hand, misclassified anaphors are simply not resolved by the rankers: this limits the recall of the coreference system."
"On the other hand, misclassified non-anaphors are linked to a previous mention: this limits precision."
"In order to further assess the impact of the er-rors made by the discourse status classifier, we build two different oracle systems."
"The first oracle sys-tem, RANK + DS - ORACLE + SP , uses the specialized rankers in combination with a perfect discourse sta-tus classifier."
"That is, this system knows for each mention whether it is anaphoric or not: the only er-rors made by such a system are invalid resolutions."
RANK + DS - ORACLE + SP thus provides an upper-bound for the RANK +
DS + SP model.
The results for this oracle are given in Table 7: they show substan-tial improvements over RANK +
"DS + SP , which sug-gests that the RANK + DS + SP has also the potential to be further improved if used in combination with a more accurate discourse status classifier."
"The second oracle system, LINK - ORACLE , uses the discourse status classifier with a perfect corefer-ence resolver."
"That is, this system has perfect knowl-edge regarding the antecedents of anaphors: the er-rors made by such a system are only errors in the discourse status of mentions."
The results for LINK - ORACLE are also reported in Table 7.
"These figures show that however accurate our models are at pick-ing a correct antecedent for a true anaphor, the best they can achieve in terms of f-scores is 88.1% with MUC , 85.2% with B 3 , and 79.7% with CEAF ."
"We present and evaluate two straight-forward tac-tics for improving coreference resolution: (i) rank- ing models, and (ii) separate, specialized models for different types of referring expressions."
The specialized rankers are used in combination with a discourse status classifier which determines the mentions that are sent to the rankers.
"This simple pipeline architecture produces significant improve-ments over various implementations of the standard, classifier-based coreference system."
"In turn, these strategies could be integrated with the joint infer-ence models we have explored elsewhere[REF_CITE]and which have ob-tained performance improvements that are orthogo-nal to those obtained here."
"This paper’s improvements are consistent across the three main coreference evaluation metrics: MUC , B 3 , and CEAF . [Footnote_12]"
12 We strongly advocate that coreference results should never be presented in terms of MUC scores alone.
"We attribute improvements to: (i) the better antecedent selection capabilities offered by the ranking approach, and (ii) the division of la-bor between specialized models, allowing each one to better model the corresponding distribution."
We present a novel learning framework for pipeline models aimed at improving the com-munication between consecutive stages in a pipeline.
Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute prob-abilistic features used at other stages down-stream.
We describe a simple method of in-tegrating probabilistic features into the linear scoring functions used by state of the art ma-chine learning algorithms.
"Experimental eval-uation on dependency parsing and named en-tity recognition demonstrate the superiority of our approach over the baseline pipeline mod-els, especially when upstream stages in the pipeline exhibit low accuracy."
Machine learning algorithms are used extensively in natural language processing.
"Applications range from fundamental language tasks such as part of speech (POS) tagging or syntactic parsing, to higher level applications such as information extraction (IE), semantic role labeling (SRL), or question an-swering (QA)."
Learning a model for a particular lan-guage processing problem often requires the output from other natural language tasks.
"Syntactic pars-ing and dependency parsing usually start with a tex-tual input that is tokenized, split in sentences and POS tagged."
"In information extraction, named en-tity recognition (NER), coreference resolution, and relation extraction (RE) have been shown to benefit from features that use POS tags and syntactic depen-dencies."
"Similarly, most SRL approaches assume a parse tree representation of the input sentences."
"The common practice in modeling such dependen-cies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence."
One advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language process-ing.
"The key disadvantage is that errors propagate between stages in the pipeline, significantly affect-ing the quality of the final results."
"One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models."
Their empirical results show the superiority of the integrated model over the pipeline approach.
"While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algo-rithms and their computational complexity."
Recent negative results on the integration of syntactic pars-ing with SRL[REF_CITE]provide additional evidence for the difficulty of this general approach.
"When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by Roth and"
"More recently,[REF_CITE]model the linguistic pipelines as Bayesian networks on which they perform Monte Carlo inference in order to find the most likely out-put for the final stage in the pipeline."
"In this paper, we present a new learning method for pipeline models that mitigates the problem of er-ror propagation between the tasks."
Our method ex-ploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream.
"We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art machine learning algorithms such as CRFs and Support Vector Machines[REF_CITE]."
"Experimental results on dependency parsing and named entity recogni-tion show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy."
We consider that the task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y(x).
Each input x is also associated with a different set of outputs z ∈ Z(x) for which we are given a probabilistic confidence measure p(z|x).
"In a pipeline model, z would correspond to the annotations performed on the input x by all stages in the pipeline other than the stage that produces y. For example, in the case of dependency parsing, x is a sequence of words, y is a set of word-word dependencies, z is a sequence of POS tags, and p(z|x) is a measure of the confi-dence that the POS tagger has in the output z. Let φ be a representation function that maps an exam-ple (x, y, z) to a feature vector φ(x, y, z) ∈ R d , and w ∈ R d a parameter vector."
"Equations (1) and (2) below show the traditional method for computing the optimal output ŷ in a pipeline model, assuming a linear scoring function defined by w and φ. ŷ(x) = argmax w · φ(x, y, ẑ(x)) (1) y∈Y(x) ẑ(x) = argmax p(z|x) (2) z∈Z(x)"
The weight vector w is learned by optimizing a pre-defined objective function on a training dataset.
"In the model above, only the best annotation ẑ produced by upstream stages is used for determining the optimal output ŷ."
"However, ẑ may be an incor-rect annotation, while the correct annotation may be ignored because it was assigned a lower confidence value."
"We propose exploiting all possible annota-tions and their probabilities as illustrated in the new model below: ŷ(x) = argmax w · ψ(x, y) (3) y∈Y(x) X p(z|x) · φ(x, y, z) ψ(x, y) = (4) z∈Z(x)"
"In most cases, directly computing ψ(x, y) is unfeasi-ble, due to a large number of annotations in Z(x)."
"In our dependency parsing example, Z(x) contains all possible POS taggings of sentence x; consequently its cardinality is exponential in the length of the sen-tence."
"A more efficient way of computing ψ(x,y) can be designed based on the observation that most components φ i of the original feature vector φ utilize only a limited amount of evidence from the example (x, y, z)."
"We define (x̃, ỹ, z̃) ∈ F i (x, y, z) to cap-ture the actual evidence from (x, y, z) that is used by one instance of feature function φ i ."
"We call (x̃, ỹ, z̃) a feature instance of φ i in the example (x,y,z)."
"Correspondingly, F i (x,y,z) is the set of all fea-ture instances of φ i in example (x, y, z)."
"Usually, φ i (x, y, z) is set to be equal with the number of in-stances of φ i in example (x, y, z), i.e. φ i (x, y, z) = |F i (x,y,z)|."
"Table 1 illustrates three feature in-stances (x̃, ỹ, z̃) generated by three typical depen-dency parsing features in the example from Figure 1."
"Because the same feature may be instantiated multi- ple times in the same example, the components of each feature instance are annotated with their po-sitions relative to the example."
"Given these defi-nitions, the feature vector ψ(x,y) from (4) can be rewritten in a component-wise manner as follows: ψ(x, y) = [ψ 1 (x, y) . . . ψ d (x, y)] (5) X p(z|x) · φ ψ i (x, y) = i (x, y, z) z∈Z(x)"
"X p(z|x) · |F = i (x, y, z)| z∈Z(x)"
"X p(z|x) X 1 = z∈Z(x) (x̃,ỹ,z̃)∈F i (x,y,z) X X = p(z|x) z∈Z(x) (x̃,ỹ,z̃)∈F i (x,y,z)"
"X X p(z|x) = (x̃,ỹ,z̃)∈F i (x,y,Z(x)) z∈Z(x),z⊇z̃ where F i (x, y, Z(x)) stands for:"
"F i (x, y, Z(x)) = [ F i (x, y, z) z∈Z(x)"
We introduce p(z̃|x) to denote the expectation:
"X p(z̃|x) = p(z|x) z∈Z(x),z⊇z̃"
"Then ψ i (x, y) can be written compactly as:"
"X ψ i (x, y) = (6)p(z̃|x) (x̃,ỹ,z̃)∈F i (x,y,Z(x))"
"The total number of terms in (6) is equal with the number of instantiations of feature φ i in the exam-ple (x, y) across all possible annotations z ∈ Z(x), i.e. |F i (x, y, Z(x))|."
Usually this is significantly smaller than the exponential number of terms in (4).
"The actual number of terms depends on the particu-lar feature used to generate them, as illustrated in the last row of Table 1 for the three features used in de-pendency parsing."
"The overall time complexity for calculating ψ(x, y) also depends on the time com-plexity needed to compute the expectations p(z̃|x)."
"When z is a sequence, p(z̃|x) can be computed ef-ficiently using a constrained version of the forward-backward algorithm (to be described in Section 3)."
"When z is a tree, p(z̃|x) will be computed using a constrained version of the CYK algorithm (to be de-scribed in Section 4)."
"The time complexity can be further reduced if in-stead of ψ(x, y) we use its subcomponent ψ̂(x, y) that is calculated based only on instances that appear in the optimal annotation ẑ: ψ̂(x, y) = [ψ̂ 1 (x, y) . . . ψ̂ d (x, y)] (7) X ψ̂ i (x, y) = p(z̃|x) (8) (x̃,ỹ,z̃)∈F i (x,y,ẑ)"
The three models are summarized in Table 2 below.
In the next two sections we illustrate their applica- tion to two common tasks in language processing: dependency parsing and named entity recognition.
"In a traditional dependency parsing pipeline (model M 1 in Table 2), an input sentence x is first aug- mented with a POS tagging ẑ(x), and then pro-cessed by a dependency parser in order to obtain a dependency structure ŷ(x)."
"To evaluate the new pipeline models we use MSTP ARSER 1 , a linearly scored dependency parser developed[REF_CITE]."
"Following the edge based factorization method[REF_CITE], the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree."
"Equivalently, the feature vector of a dependency tree is defined as the sum of the feature vectors of all edges in the tree: M 1 : φ(x, y) = X φ(x, u→v, ẑ(x)) u→v∈y M 2 : ψ(x, y) = X ψ(x, u→v) u→v∈y M 3 : ψ̂(x, y) = X ψ̂(x, u→v) u→v∈y For each edge u → v ∈ y, MSTP ARSER generates features based on a set of feature templates that take into account the words and POS tags at positions u, v, and their left and right neighbors u±1, v ±[Footnote_1]."
"For example, a particular feature template T used inside MSTP ARSER generates the following POS bigram features: 1, if hz u , z v i = ht 1 , t 2 i φ i (x, u→v, z) = 0, otherwise where t 1 ,t 2 ∈ P are the two POS tags associated with feature index i. By replacing y with u → v in the feature expressions from Table 2, we obtain the following formulations: 1, if hẑ u , ẑ v i=ht 1 , t 2 i M 1 :φ i (x, u→v) = 0, otherwise"
"M 2 :ψ i (x, u→v) = p(z̃ =ht 1 , t 2 i|x)"
"M 3 :ψ̂ i (x, u→v) = p(z̃ =ht 1 , t 2 i|x), if hẑ u , ẑ v i=ht 1 , t 2 i 0, otherwise where, following the notation from Section 2, z̃ = hz u ,z v i is the actual evidence from z that is used by feature i, and ẑ is the top scoring annotation produced by the POS tagger."
The implementation in MSTP ARSER corresponds to the traditional pipeline model M 1 .
"Given a method for computing feature probabilities p(z̃ = ht 1 , t 2 i|x), it is straightforward to modify MSTP ARSER to implement models M 2 and M 3 – we simply replace the feature vectors φ with ψ and ψ̂ respectively."
"As mentioned in Sec-tion 2, the time complexity of computing the fea-ture vectors ψ in model M 2 depends on the com-plexity of the actual evidence z̃ used by the fea-tures."
"For example, the feature template T used above is based on the POS tags at both ends of a de-pendency edge, consequently it would generate |P| 2 features in model M 2 for any given edge u → v."
"There are however feature templates used in MST-P ARSER that are based on the POS tags of up to 4 tokens in the input sentence, which means that for each edge they would generate |P| 4 ≈ 4.5M fea-tures."
Whether using all these probabilistic features is computationally feasible or not also depends on the time complexity of computing the confidence measure p(z̃|x) associated with each feature.
"The new pipeline models M 2 and M 3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output."
"We chose to use linear chain CRFs[REF_CITE]since CRFs can be eas-ily modified to compute expectations of the type p(z̃|x), as needed by M 2 and M 3 ."
The CRF tagger was implemented in M AL - LET[REF_CITE]using the original feature templates[REF_CITE].
The model was trained on sections 2–21 from the English Penn Treebank[REF_CITE].
"When tested on sec-tion 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger."
We have also implemented in M ALLET a con-strained version of the forward-backward procedure that allows computing feature probabilities p(z̃|x).
"If z̃ = ht i 1 t i 2 ...t i k i specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions between i 1 and i k by constraining the state transitions to pass through the specified tags at the k positions."
A similar approach was used by Culotta et al. in (2004) in order to asso-ciate confidence values with sequences of contigu-ous tokens identified by a CRF model as fields in an information extraction task.
"The constrained proce- dure requires (i k − i 1 )|P| 2 = O(N|P| 2 ) multipli-cations in an order 1 Markov model, where N is the length of the sentence."
"Because MSTP ARSER uses an edge based factorization of the scoring function, the constrained forward procedure will need to be run for each feature template, for each pair of tokens in the input sentence x."
"If the evidence z̃ required by the feature template T constrains the tags at k posi-tions, then the total time complexity for computing the probabilistic features p(z̃|x) generated by T is:"
O(N 3 |P| k+2 )=O(N|P| 2 ) · O(N 2 ) ·
O(|P| k ) (9)
"As mentioned earlier, some feature templates used in the dependency parser constrain the POS tags at 4 positions, leading to a O(N 3 |P| 6 ) time complexity for a length N sentence."
"Experimental runs on the same machine that was used for CRF training show that such a time complexity is not yet feasible, espe-cially because of the large size of P (46 POS tags)."
"In order to speed up the computation of probabilis-tic features, we made the following two approxima-tions: 1. Instead of using the constrained forward-backward procedure, we enforce an indepen-dence assumption between tags at different po-sitions and rewrite p(z̃ = ht i 1 t i 2 ...t i k i|x) as: k p(t i 1 t i 2 ...t i k |x) ≈ Y p(t i j |x) j=1"
The marginal probabilities p(t i j |x) are easily computed using the original forward and back-ward parameters as: α i j (t i j |x)β i j (t i j |x) p(t i j |x) = Z(x)
This approximation eliminates the factor
O(N|P| 2 ) from the time complexity in (9). 2.
"If any of the marginal probabilities p(t i j |x) is less than a predefined threshold (τ|P|) −1 , we set p(z̃|x) to 0."
"When τ ≥ 1, the method is guaranteed to consider at least the most proba-ble state when computing the probabilistic fea-tures."
"Looking back at Equation (4), this is equivalent with summing feature vectors only over the most probable annotations z ∈ Z(x)."
The approximation effectively replaces the fac-tor O(|P| k ) in (9) with a quasi-constant factor.
"The two approximations lead to an overall time com-plexity of O(N 2 ) for computing the probabilistic features associated with any feature template T , plus O(N|P| 2 ) for the unconstrained forward-backward procedure."
We will use M 2′ to refer to the model M 2 that incorporates the two approximations.
The independence assumption from the first approxima-tion can be relaxed without increasing the asymp-totic time complexity by considering as independent only chunks of contiguous POS tags that are at least a certain number of tokens apart.
"Consequently, the probability of the tag sequence will be approxi-mated with the product of the probabilities of the tag chunks, where the exact probability of each chunk is computed in constant time with the constrained forward-backward procedure."
We will use M 2′′ to refer to the resulting model.
"MSTP ARSER was trained on sections 2–21 from the WSJ Penn Treebank, using the gold standard POS tagging."
"The parser was then evaluated on section 23, using the POS tagging output by the CRF tagger."
For model M 1 we need only the best output from the POS tagger.
"For models M 2′ and M 2′′ we com-pute the probability associated with each feature us-ing the corresponding approximations, as described in the previous section."
In model M 2′′ we consider as independent only chunks of POS tags that are 4 tokens or more apart.
"If the distance between the chunks is less than 4 tokens, the probability for the entire tag sequence in the feature is computed ex-actly using the constrained forward-backward pro-cedure."
"Table 3 shows the accuracy obtained by models M 1 , M 2′ (τ) and M 2′′ (τ) for various values of the threshold parameter τ."
The accuracy is com- puted over unlabeled dependencies i.e. the percent-age of words for which the parser has correctly iden-tified the parent in the dependency tree.
The pipeline model M 2′ that uses probabilistic features outper-forms the traditional pipeline model M 1 .
"As ex-pected, M 2′′ performs slightly better than M 2′ , due to a more exact computation of feature probabilities."
"Overall, only by using the probabilities associated with the POS features, we achieve an absolute er-ror reduction of 0.19%, in a context where the POS stage in the pipeline already has a very high accu-racy of 96.25%."
We expect probabilistic features to yield a more substantial improvement in cases where the pipeline model contains less accurate upstream stages.
Such a case is that of NER based on a com-bination of POS and dependency parsing features.
"In Named Entity Recognition (NER), the task is to identify textual mentions of predefined types of en-tities."
"Traditionally, NER is modeled as a sequence classification problem: each token in the input sen-tence is tagged as being either inside (I) or outside (O) of an entity mention."
Most sequence tagging approaches use the words and the POS tags in a limited neighborhood of the current sentence posi-tion in order to compute the corresponding features.
We augment these flat features with a set of tree features that are computed based on the words and POS tags found in the proximity of the current to-ken in the dependency tree of the sentence.
"We argue that such dependency tree features are better at capturing predicate-argument relationships, espe-cially when they span long stretches of text."
"Figure 2 shows a sentence x together with its POS tagging z 1 , dependency links z 2 , and an output tagging y. As-suming the task is to recognize mentions of people, the word sailors needs to be tagged as inside."
"If we extracted only flat features using a symmetric win-dow of size 3, the relationship between sailors and thought would be missed."
"This relationship is use- ful, since an agent of the predicate thought is likely to be a person entity."
"On the other hand, the nodes sailors and thought are adjacent in the dependency tree of the sentence."
"Therefore, their relationship can be easily captured as a dependency tree feature using the same window size."
"For every token position, we generate flat features by considering all unigrams, bigrams and trigrams that start with the current token and extend either to the left or to the right."
"Similarly, we generate tree features by considering all unigrams, bigrams and trigrams that start with the current token and extend in any direction in the undirected version of the de-pendency tree."
The tree features are also augmented with the actual direction of the dependency arcs be-tween the tokens.
"If we use only words to create n-gram features, the token sailors will be associated with the following features: • Flat: sailors, the sailors, hSi the sailors, sailors mistakenly, sailors mistakenly thought. • Tree: sailors, sailors ← the, sailors → thought, sailors → thought ← must, sailors → thought ← mistakenly."
"We also allow n-grams to use word classes such as POS tags and any of the following five categories: h1Ci for tokens consisting of one capital letter, hACi for tokens containing only capital letters, hFCi for tokens that start with a capital letter, followed by small letters, hCDi for tokens containing at least one digit, and hCRTi for the current token."
"The set of features can then be defined as a Carte-sian product over word classes, as illustrated in Fig-ure 3 for the original tree feature sailors → thought ← mistakenly."
"In this case, instead of one com-pletely lexicalized feature, the model will consider 12 different features such as sailors → VBD ← RB, NNS → thought ← RB, or NNS → VBD ← RB."
"The pipeline model M 2 uses features that appear in all possible annotations z = hz 1 ,z 2 i, where z 1 and z 2 are the POS tagging and the dependency parse respectively."
"If the corresponding evidence is z̃ = hz̃ 1 , z̃ 2 i, then: p(z̃|x) = p(z̃ 2 |z̃ 1 , x)p(z̃ 1 |x)"
"For example, NNS 2 → thought 4 ← RB 3 is a feature instance for the token sailors in the annotations from Figure 2."
"This can be construed as having been gen-erated by a feature template T that outputs the POS tag t i at the current position, the word x j that is the parent of x i in the dependency tree, and the POS tag t k of another dependent of x j (i.e. t i → x j ← t k )."
"The probability p(z̃|x) for this type of features can then be written as: p(z̃|x) = p(i→j ←k|t i , t k , x) · p(t i , t k |x)"
The two probability factors can be computed exactly as follows: 1.
"The M 2 model for dependency parsing from Section 3 is used to compute the probabilistic features ψ(x, u → v|t i , t k ) by constraining the POS annotations to pass through tags t i and t k at positions i and k."
"The total time complexity for this step is O(N 3 |P| k+2 ). 2. Having access to ψ(x, u → v|t i , t k ), the factor p(i→j←k|t i , t k , x) can be computed in O(N 3 ) time using a constrained version of Eisner’s al-gorithm, as will be explained in Section 4.1. 3."
"As described in Section 3.1, computing the expectation p(t i ,t k |x) takes O(N|P 2 |) time using the constrained forward-backward algo-rithm."
"The current token position i can have a total of N values, while j and k can be any positions other than i. Also, t i and t k can be any POS tag from"
"P. Consequently, the feature template T induces O(N 3 |P| 2 ) feature instances."
"Overall, the time complexity for computing the feature instances gen-erated by T is O(N 6 |P| k+4 ), as results from:"
O(N 3 |P| 2 ) · (O(N 3 |P| k+2 ) + O(N 3 ) +
O(N|P| 2 ))
"While still polynomial, this time complexity is fea-sible only for small values of N. In general, the time complexity for computing probabilistic features in the full model M 2 increases with both the number of stages in the pipeline and the complexity of the features."
"Motivated by efficiency, we decided to use the pipeline model M 3 in which probabilities are com-puted only over features that appear in the top scor-ing annotation ẑ = hẑ 1 , ẑ 2 i, where ẑ 1 and ẑ 2 repre-sent the best POS tagging, and the best dependency parse respectively."
"In order to further speed up the computation of probabilistic features, we made the following approximations: 1."
"We consider the POS tagging and the depen-dency parse independent and rewrite p(z̃|x) as: p(z̃|x) = p(z̃ 1 , z̃ 2 |x) ≈ p(z̃ 1 |x)p(z̃ 2 |x) 2."
We enforce an independence assumption be-tween POS tags.
"Thus, if z̃ 1 = ht i 1 t i 2 ...t i k i specifies the tags at k positions in the sentence, then p(z̃ 1 |x) is rewritten as: k p(t i 1 t i 2 ...t i k |x) ≈ Y p(t i j |x) j=1 3."
We also enforce a similar independence as-sumption between dependency links.
"Thus, if z̃ 2 = hu 1 → v 1 ...u k → v k i specifies k depen-dency links, then p(z̃ 2 |x) is rewritten as: k p(u 1 →v 1 ...u k →v k |x) ≈ Y p(u l →v l |x) l=1"
"For example, the probability p(z̃|x) of the feature instance NNS 2 → thought 4 ← RB 3 is approximated as: p(z̃|x) ≈ p(z̃ 1 |x) · p(z̃ 2 |x) p(z̃ 1 |x) ≈ p(t 2 =NNS|x) · p(t 3 =RB|x) p(z̃ 2 |x) ≈ p(2→4|x) · p(3→4|x)"
We will use M 3′ to refer to the resulting model.
"The probabilistic POS features p(t i |x) are computed using the forward-backward procedure in CRFs, as described in Section 3.1."
"To completely specify the pipeline model for NER, we also need an efficient method for computing the probabilistic dependency features p(u → v|x), where u → v is a dependency edge between positions u and v in the sentence x. MSTP ARSER is a large-margin method that com-putes an unbounded score s(x, y) for any given sen-tence x and dependency structure y ∈ Y(x) using the following edge-based factorization: s(x, y) = X s(x, u→v) = w X φ(x, u→v) u→v∈y u→v∈y"
The following three steps describe a general method for associating probabilities with output substruc-tures.
"The method can be applied whenever a struc-tured output is associated a score value that is un-bounded in R, assuming that the score of the entire output structure can be computed efficiently based on a factorization into smaller substructures."
"Map the unbounded score s(x,y) from R into [0, 1] using the softmax functi[REF_CITE]: e s(x,y) n(x, y) = y∈Y(x) e s(x,y) P"
"The normalized score n(x, y) preserves the ranking given by the original score s(x, y)."
The normaliza-tion constant at the denominator can be computed in O(N 3 ) time by replacing the max operator with the sum operator inside Eisner’s chart parsing algorithm.
Compute a normalized score for the sub-structure by summing up the normalized scores of all the complete structures that contain it.
"In our model, dependency edges are substructures, while dependency trees are complete structures."
The nor-malized score will then be computed as:
"X n(x, u→v) = n(x, y) y∈Y(x),u→v∈y"
The sum can be computed in O(N 3 ) time using a constrained version of the algorithm that computes the normalization constant in step S1.
This con-strained version of Eisner’s algorithm works in a similar manner with the constrained forward back-ward algorithm by restricting the dependency struc-tures to contain a predefined edge or set of edges.
"Use the isotonic regression method[REF_CITE]to map the normalized scores n(x, u → v) into probabilities p(u → v|x)."
"A potential problem with the softmax function is that, depending on the distribution of scores, the expo-nential transform could dramatically overinflate the higher scores."
"Isotonic regression, by redistributing the normalized scores inside [0, 1], can alleviate this problem."
We test the pipeline model M 3′ versus the traditional model M 1 on the task of detecting mentions of per-son entities in the ACE dataset [Footnote_2] .
We use the standard training – testing split of the[REF_CITE]dataset in which the training dataset is also augmented with the documents from the[REF_CITE]dataset.
The com-bined dataset contains 674 documents for training and 97 for testing.
"We implemented the CRF model in M ALLET using three different sets of features: Tree, Flat, and Full corresponding to the union of all flat and tree features."
"The POS tagger and the de-pendency parser were trained on sections 2-21 of the Penn Treebank, followed by an isotonic regression step on section 23 for the dependency parser."
"We compute precision recall (PR) graphs by varying a threshold on the token level confidence output by the CRF tagger, and summarize the tagger performance using the area under the curve."
Table 4 shows the re-sults obtained by the two models under the three fea-ture settings.
"The model based on probabilistic fea- tures consistently outperforms the traditional model, especially when only tree features are used."
Depen-dency parsing is significantly less accurate than POS tagging.
"Consequently, the improvement for the tree based model is more substantial than for the flat model, confirming our expectation that probabilis-tic features are more useful when upstream stages in the pipeline are less accurate."
"Figure 4 shows the PR curves obtained for the tree-based models, on which we see a significant 5% improvement in precision over a wide range of recall values."
In terms of the target task – improving the perfor-mance of linguistic pipelines – our research is most related to the work[REF_CITE].
"In their approach, output samples are drawn at each stage in the pipeline conditioned on the samples drawn at previous stages, and the final output is deter-mined by a majority vote over the samples from the final stage."
"The method needs very few sam-ples for tasks such as textual entailment, where the final outcome is binary, in agreement with a theo-retical result on the rate of convergence of the vot-ing Gibbs classifier due[REF_CITE]."
"While their sampling method is inherently approx-imate, our full pipeline model M 2 is exact in the sense that feature expectations are computed exactly in polynomial time whenever the inference step at each stage can be done in polynomial time, irrespec-tive of the cardinality of the final output space."
"Also, the pipeline models M 2 and M [Footnote_3] and their more effi-cient alternatives propagate uncertainty during both training and testing through the vector of probabilis-tic features, whereas the sampling method takes ad-vantage of the probabilistic nature of the outputs only during testing."
3 The Java source code will be released on my web page.
"Overall, the two approaches can be seen as complementary."
"In order to be ap-plicable with minimal engineering effort, the sam-pling method needs NLP researchers to write pack-ages that can generate samples from the posterior."
"Similarly, the new pipeline models could be easily applied in a diverse range of applications, assum-ing researchers develop packages that can efficiently compute marginals over output substructures."
"We have presented a new, general method for im-proving the communication between consecutive stages in pipeline models."
"The method relies on the computation of probabilities for count features, which translates in adding a polynomial factor to the overall time complexity of the pipeline whenever the inference step at each stage is done in polynomial time, which is the case for the vast majority of infer-ence algorithms used in practical NLP applications."
We have also shown that additional independence assumptions can make the approach more practical by significantly reducing the time complexity.
Ex-isting learning based models can implement the new method by replacing the original feature vector with a more dense vector of probabilistic features 3 .
"It is essential that every stage in the pipeline produces probabilistic features, and to this end we have de-scribed an effective method for associating proba-bilities with output substructures."
"We have shown for NER that simply using the probabilities associated with features that appear only in the top annotation can lead to useful im-provements in performance, with minimal engineer-ing effort."
"In future work we plan to empirically evaluate NER with an approximate version of the full model M 2 which, while more demanding in terms of time complexity, could lead to even more significant gains in accuracy."
We also intend to com-prehensively evaluate the proposed scheme for com-puting probabilities by experimenting with alterna-tive normalization functions.
We present an algorithmic framework for learning multiple related tasks.
Our frame-work exploits a form of prior knowledge that relates the output spaces of these tasks.
We present PAC learning results that analyze the conditions under which such learning is pos-sible.
"We present results on learning a shal-low parser and named-entity recognition sys-tem that exploits our framework, showing con-sistent improvements over baseline methods."
"When two NLP systems are run on the same data, we expect certain constraints to hold between their out-puts."
This is a form of prior knowledge.
We propose a self-training framework that uses such information to significantly boost the performance of one of the systems.
The key idea is to perform self-training only on outputs that obey the constraints.
Our motivating example in this paper is the task pair: named entity recognition (NER) and shallow parsing (aka syntactic chunking).
Consider a hid-den sentence with known POS and syntactic struc-ture below.
Further consider four potential NER se-quences for this sentence.
POS: NNP NNP VBD TO NNP NN
"Without ever seeing the actual sentence, can we guess which NER sequence is correct?"
NER1 seems wrong because we feel like named entities should not be part of verb phrases.
NER2 seems wrong be-cause there is an NNP [Footnote_1] (proper noun) that is not part of a named entity (word 5).
"1 When we refer to NNP , we also include NNPS ."
NER3 is amiss because we feel it is unlikely that a single name should span more than one NP (last two words).
NER4 has none of these problems and seems quite reasonable.
"In fact, for the hidden sentence, NER4 is correct [Footnote_2] ."
2 The sentence is: “George Bush spoke to Congress today”
The remainder of this paper deals with the prob-lem of formulating such prior knowledge into a workable system.
There are similarities between our proposed model and both self-training and co-training; background is given in Section 2.
"We present a formal model for our approach and per-form a simple, yet informative, analysis (Section 3)."
This analysis allows us to define what good and bad constraints are.
"Throughout, we use a running example of NER using hidden Markov models to show the efficacy of the method and the relation-ship between the theory and the implementation."
"Fi-nally, we present full-blown results on seven dif-ferent NER data sets (one from CoNLL, six from ACE), comparing our method to several competi-tive baselines (Section 4)."
"We see that for many of these data sets, less than one hundred labeled NER sentences are required to get state-of-the-art perfor-mance, using a discriminative sequence labeling al-gorithm[REF_CITE]."
Self-training works by learning a model on a small amount of labeled data.
This model is then evalu- ated on a large amount of unlabeled data.
"Its predic-tions are assumed to be correct, and it is retrained on the unlabeled data according to its own predic-tions."
"Although there is little theoretical support for self-training, it is relatively popular in the natu-ral language processing community."
Its success sto-ries range from parsing[REF_CITE]to machine translati[REF_CITE].
"In some cases, self-training takes into account model confidence."
"Co-training[REF_CITE]is related to self-training, in that an algorithm is trained on its own predictions."
Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for in-stance by training with disjoint feature sets).
These models are both applied to a large repository of un-labeled data.
Examples on which these two mod-els agree are extracted and treated as labeled for a new round of training.
"In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident."
"The original, and simplest analysis of co-training is due[REF_CITE]."
"It does not take into account confidence (to do so requires a significantly more detailed analysis[REF_CITE]), but is useful for understanding the process."
We define a formal PAC-style[REF_CITE]model that we call the “hints model” [Footnote_3] .
3 The name comes from thinking of our knowledge-based constraints as “hints” to a learner as to what it should do.
We have an instance space X and two output spaces Y 1 and Y 2 .
We as-sume two concept classes C 1 and C 2 for each output space respectively.
"Let D be a distribution over X, and f 1 ∈ C 1 (resp., f 2 ∈ C 2 ) be target functions."
"The goal, of course, is to use a finite sample of examples drawn from D (and labeled—perhaps with noise— by f 1 and f 2 ) to “learn” h 1 ∈ C 1 and h 2 ∈ C 2 , which are good approximations to f 1 and f 2 ."
So far we have not made use of any notion of con-straints.
"Our expectation is that if we constrain h 1 and h 2 to agree (vis-a-vis the example in the Intro-duction), then we should need fewer labeled exam-ples to learn either. (The agreement should “shrink” the size of the corresponding hypothesis spaces.)"
"To formalize this, let χ : Y 1 × Y 2 → {0, 1} be a con- straint function."
"We say that two outputs y 1 ∈ Y 1 and y 2 ∈ Y 2 are compatible if χ(y 1 ,y 2 ) = 1."
We need to assume that χ is correct:
"We say that χ is correct with respect to D, f 1 , f 2 if whenever x has non-zero probability under D, then χ(f 1 (x), f 2 (x)) = 1."
R UNNING E XAMPLE
"In our example, Y 1 is the space of all POS/chunk sequences and Y 2 is the space of all NER se-quences."
We assume that C 1 and C 2 are both represented by HMMs over the appropriate state spaces.
"The functions we are trying to learn are f 1 , the “true” POS/chunk labeler and f 2 , the “true”"
"NER labeler. (Note that we assume f 1 ∈ C 1 , which is obviously not true for language.)"
Our constraint function χ will require the follow-ing for agreement: (1) any NNP must be part of a named entity; (2) any named entity must be a sub-sequence of a noun phrase.
This is precisely the set of constraints discussed in the introduction.
"The question is: given this additional source of knowledge (i.e., χ), has the learning problem be-come easier?"
"That is, can we learn f 2 (and/or f 1 ) us-ing significantly fewer labeled examples than if we did not have χ?"
"Moreover, we have assumed that χ is correct, but is this enough?"
"Intuitively, no: a func-tion χ that returns 1 regardless of its inputs is clearly not useful."
"Given this, what other constraints must be placed on χ."
We address these questions in Sec-tions 3.3.
"However, first we define our algorithm."
We begin by considering a simplified version of the “learning with hints” problem.
Suppose that all we care about is learning f 2 .
We have a small amount of data labeled by f 2 (call this D) and a large amount of data labeled by f 1 (call this
"D unlab –”unlab” because as far as f 2 is concerned, it is unlabeled)."
"We call the following algorithm “One-Sided Learning with Hints,” since it aims only to learn f 2 :"
"In the two-sided version, we assume that we have a small amount of data labeled by f 1 (call this D 1 ), a small amount of data labeled by f 2 (call this D 2 ) and a large amount of unlabeled data (call this D unlab )."
"The algorithm we propose for learning hypotheses for both tasks is below: 1: Learn h 1 on D 1 and h 2 on D 2 . 2: For each example x ∈ D unlab : 3: Compute y 1 = h 1 (x) and y 2 = h 2 (x) 4: If χ(y 1 , y 2 ) add (x, y 1 ) to D 1 , (x, y 2 ) to D 2 5: Relearn h 1 on D 1 and h 2 on D 2 . 6: Go to (2) if desired"
R UNNING E XAMPLE
We use the remaining 18447 examples as unlabeled data.
"The baseline HMMs achieve F-scores of 50.8 and 76.3, respectively."
"In step 2, we add 7512 examples to each data set."
"After step 3, the new models achieve F-scores of 54.6 and 79.2, respectively."
The gain for NER is lower than be-fore as it is trained against “noisy” syntactic labels.
Our goal is to prove that one-sided learning with hints “works.”
"That is, if C 2 is learnable from large amounts of labeled data, then it is also learn-able from small amounts of labeled data and large amounts of f 1 -labeled data."
This is formalized in Theorem 1 (all proofs are in Appendix A).
"How-ever, before stating the theorem, we must define an “initial weakly-useful predictor” (terminology[REF_CITE]), and the notion of noisy PAC-learning in the structured domain."
We say that h is a weakly-useful pre-dictor of f if for all y:
Pr D [h(x) = y] ≥ and Pr D [f(x) = y | h(x) = y 0 =6 y] ≥
Pr D [f(x) = y] + .
This definition simply ensures that (1) h is non-trivial: it assigns some non-zero probability to every possible output; and (2) h is somewhat indicative of f.
"In practice, we use the hypothesis learned on the small amount of training data during step (1) of the algorithm as the weakly useful predictor."
We say that C is PAC-learnable with noise (in the structured setting) if there exists an algorithm with the following properties.
"For any c ∈ C, any distribution D over X, any 0 ≤ η ≤ 1/|Y|, any 0 &lt; &lt; 1, any 0 &lt; δ &lt; 1 and any η ≤ η 0 &lt; 1/|Y|, if the algorithm is given access to examples drawn EX ηSN (c, D) and inputs , δ and η 0 , then with probability at least 1 − δ, the algo-rithm returns a hypothesis h ∈ C with error at most ."
"Here, EX ηSN (c,D) is a structured noise oracle, which draws examples from D, labels them by c and randomly replaces with another label with prob. η."
"Note here the rather weak notion of noise: en-tire structures are randomly changed, rather than in-dividual labels."
"Furthermore, the error is 0/1 loss over the entire structure."
"While not stated directly in terms of PAC learnability, it is clear that his results apply."
This suggests that the re-quirement of 0/1 loss is weaker.
"As suggested before, it is not sufficient for χ to simply be correct (the constant 1 function is cor-rect, but not useful)."
"We need it to be discriminating, made precise in the following definition."
"We say the discrimination of χ for h 0 is Pr D [χ(f 1 (x), h 0 (x))] −1 ."
"In other words, a constraint function is discrim-inating when it is unlikely that our weakly-useful predictor h 0 chooses an output that satisfies the con-straint."
"This means that if we do find examples (in our unlabeled corpus) that satisfy the constraints, they are likely to be “useful” to learning. we see the appropriate trend."
The value for the con-straint based only on POS tags is 39.1 (worse) and for the NP constraint alone is 27.0 (much worse).
"Suppose C 2 is PAC-learnable with noise in the structured setting, h 02 is a weakly use-ful predictor of f 2 , and χ is correct with respect to D, f 1 , f 2 , h 02 , and has discrimination ≥ 2(|Y| − 1)."
Then C 2 is also PAC-learnable with one-sided hints.
"The way to interpret this theorem is that it tells us that if the initial h 2 we learn in step 1 of the one-sided algorithm is “good enough” (in the sense that it is weakly-useful), then we can use it as specified by the remainder of the one-sided algorithm to obtain an arbitrarily good h 2 (via iterating)."
The dependence on |Y| is the discrimination bound for χ is unpleasant for structured problems.
"If we wish to find M unlabeled examples that satisfy the hints, we’ll need a total of at least 2M(|Y| − 1) total."
This dependence can be improved as follows.
"Suppose that our structure is represented by a graph over vertices V , each of which can take a label from a set Y ."
"Then, |Y| = Y V , and our result requires that χ be discriminating on an order exponential in V ."
"Under the assumption that χ decomposes over the graph structure (true for our example) and that C 2 is PAC-learnable with per-vertex noise, then the discrimination requirement drops to 2 |V | (|Y | − 1)."
"In NER, |Y| = 9 and |V| ≈ 26."
This means that the values from the previous example look not quite so bad.
"In the 0/1 loss case, they are com-pared to 10 25 ; in the Hamming case, they are com-pared to only 416."
The ability of the one-sided al-gorithm follows the same trends as the discrimi-nation values.
Recall the baseline performance is 50.8.
"With both constraints (and a discrimination value of 41.6), we obtain a score of 58.9."
"With just the POS constraint (discrimination of 39.1), we ob-tain a score of 58.1."
"With just the NP constraint (discrimination of 27.0, we obtain a score of 54.5."
The final question is how one-sided learning re-lates to two-sided learning.
"The following definition and easy corollary shows that they are related in the obvious manner, but depends on a notion of uncor-relation between h 01 and h 02 ."
"We say that h 1 and h 2 are un-correlated if Pr D [h 1 (x) = y 1 | h 2 (x) = y 2 , x] ="
Pr D [h 1 (x) = y 1 | x].
"Suppose C 1 and C 2 are both PAC-learnable in the structured setting, h 01 and h 02 are weakly useful predictors of f 1 and f 2 , and χ is correct with respect to D,f 1 ,f 2 ,h 01 and h 02 , and has discrimination ≥ 4(|Y| − 1) 2 (for 0/1 loss) or ≥ 4 |V | 2 (|Y |−1) 2 (for Hamming loss), and that h 01 and h 02 are uncorrelated."
Then C 1 and C 2 are also PAC-learnable with two-sided hints.
"Unfortunately, Corollary 1 depends quadratically on the discrimination term, unlike Theorem 1."
"In this section, we describe our experimental results."
We have already discussed some of them in the con-text of the running example.
"In Section 4.1, we briefly describe the data sets we use."
A full descrip-tion of the HMM implementation and its results are in Section 4.2.
"Finally, in Section 4.3, we present results based on a competitive, discriminatively-learned sequence labeling algorithm."
All results for NER and chunking are in terms of F-score; all re-sults for POS tagging are accuracy.
"Our results are based on syntactic data drawn from the Penn Treebank[REF_CITE], specifi-cally the portion used[REF_CITE]shared task (Tjong[REF_CITE])."
Our NER data is from two sources.
The first source is the[REF_CITE]shared task date (Tjong[REF_CITE]) and the second source is the 2004 NIST Automatic Content Extracti[REF_CITE].
"The ACE data constitute six separate data sets from six domains: weblogs (wl), newswire (nw), broadcast conversations (bc), United Nations (un), direct telephone speech (dts) and broadcast news (bn)."
"Of these, bc, dts and bn are all speech data sets."
All the examples from the previous sec-tions have been limited to the CoNLL data.
The experiments discussed in the preceding sections are based on a generative hidden Markov model for both the NER and syntactic chunking/POS tagging tasks.
The HMMs constructed use first-order tran-sitions and emissions.
The emission vocabulary is pruned so that any word that appears ≤ 1 time in the training data is replaced by a unique *unknown* token.
"The transition and emission probabilities are smoothed with Dirichlet smoothing, α = 0.001 (this was not-aggressively tuned by hand on one setting)."
The HMMs are implemented as finite state models in the Carmel toolkit[REF_CITE].
The various compatibility functions are also im-plemented as finite state models.
"We implement them as a transducer from POS/chunk labels to NER labels (though through the reverse operation, they can obviously be run in the opposite direction)."
"The construction is with a single state with transitions: • (NNP,?) maps to B-* and I-* • (?,B-NP) maps to B-* and O • (?,I-NP) maps to B-* , I-* and O • Single exception: (NNP,x) , where x is not an NP tag maps to anything (this is simply to avoid empty composition problems)."
This occurs in 100 of the 212k words in the Treebank data and more rarely in the automatically tagged data.
"In this section, we describe the results of one-sided discriminative labeling with hints."
We use the true syntactic labels from the Penn Treebank to derive the constraints (this is roughly 9000 sentences).
"We use the LaSO sequence labeling software[REF_CITE], with its built-in feature set."
Our goal is to analyze two things: (1) what is the effect of the amount of labeled NER data? (2) what is the effect of the amount of labeled syntactic data from which the hints are constructed?
"To answer the first question, we keep the amount of syntactic data fixed (at 8936 sentences) and vary the amount of NER data in N ∈ {100, 200, 400, 800, 1600}."
We compare models with and without the default gazetteer information from the LaSO software.
"We have the following models for comparison: • A default “Baseline” in which we simply train the NER model without using syntax. • In “POS-feature”, we do the same thing, but we first label the NER data using a tagger/chunker trained on the 8936 syntactic sentences."
"These labels are used as features for the baseline. • A “Self-training” setting where we use the 8936 syntactic sentences as “unlabeled,” label them with our model, and then train on the results. (This is equivalent to a hints model where χ(·,·) = 1 is the constant 1 func-tion.)"
We use model confidence as[REF_CITE]. [Footnote_4]
4 Results without confidence were significantly worse.
The results are shown in Figure 1.
"The trends we see are the following: • More data always helps. • Self-training usually helps over the baseline (though not always: for instance in wl and parts of cts and bn). • Adding the gazetteers help. • Adding the syntactic features helps. • Learning with hints, especially for ≤ 1000 training data points, helps significantly, even over self-training."
We further compare the algorithms by looking at how many training setting has each as the winner.
"In particular, we compare both hints and self-training to the two baselines, and then compare hints to self-training."
"If results are not significant at the 95% level (according to McNemar’s test), we call it a tie."
The results are in Table 1.
"In our second set of experiments, we consider the role of the syntactic data."
"For this experiment, we hold the number of NER labeled sentences constant (at N = 200) and vary the amount of syntactic data in M ∈ {500, 1000, 2000, 4000, 8936}."
The results of these experiments are in Figure 2.
"The trends are: • The POS feature is relatively insensitive to the amount of syntactic data—this is most likely because it’s weight is discriminatively adjusted by LaSO so that if the syntactic information is bad, it is relatively ignored. • Self-training performance often degrades as the amount of syntactic data increases. • The performance of learning with hints in-creases steadily with more syntactic data."
"As before, we compare performance between the different models, declaring a “tie” if the difference is not statistically significant at the 95% level."
The results are in Table 2.
"In experiments not reported here to save space, we experimented with several additional settings."
"In one, we weight the unlabeled data in various ways: (1) to make it equal-weight to the labeled data; (2) at 10% weight; (3) according to the score produced by the first round of labeling."
"None of these had a significant impact on scores; in a few cases perfor-mance went up by 1, in a few cases, performance went down about the same amount."
"In this section, we explore the use of two-sided discriminative learning to boost the performance of our syntactic chunking, part of speech tagging, and named-entity recognition software."
We continue to use LaSO[REF_CITE]as the se-quence labeling technique.
"The results we present are based on attempting to improve the performance of a state-of-the-art system train on all of the training data. (This is in contrast to the results in Section 4.3, in which the effect of us-ing limited amounts of data was explored.)"
"For the POS tagging and syntactic chunking, we being with all 8936 sentences of training data from CoNLL."
"For the named entity recognition, we limit our presenta-tion to results from the[REF_CITE]NER shared task."
"For this data, we have roughly 14k sentences of training data, all of which are used."
"In both cases, we reserve 10% as development data."
The develop-ment data is use to do early stopping in LaSO.
"As unlabeled data, we use 1m sentences extracted from the North American National Corpus of En- glish (previously used for self-training of parsers[REF_CITE])."
These 1m sentences were selected by dev-set relativization against the union of the two development data sets.
"Following similar ideas to those presented[REF_CITE], we employ two slight modifications to the algorithm presented in Sec-tion 3.2."
"First, in step (2b) instead of adding all allowable instances to the labeled data set, we only add the top R (for some hyper-parameter R), where “top” is determined by average model confidence for the two tasks."
"Second, Instead of using the full un-labeled set to label at each iteration, we begin with a random subset of 10R unlabeled examples and an-other add random 10R every iteration."
We use the same baseline systems as in one-sided learning: a Baseline that learns the two tasks inde-pendently; a variant of the Baseline on which the output of the POS/chunker is used as a feature for the NER; a variant based on self-training; the hints-based method.
"In all cases, we do use gazetteers."
We run the hints-based model for 10 iterations.
"For self-training, we use 10R unlabeled examples (so that it had access to the same amount of unlabeled data as the hints-based learning after all 10 iterations)."
"We used three values of R: 50, 100, 500."
We select the best-performing model (by the dev data) over these ten iterations.
The results are in Table 3.
"As we can see, performance for syntactic chunk-ing is relatively stagnant: there are no significant improvements for any of the methods over the base-line."
"This is not surprising: the form of the con-straint function we use tells us a lot about the NER task, but relatively little about the syntactic chunking task."
"In particular, it tells us nothing about phrases other than NPs."
"On the other hand, for NER, we see that both self-training and learning with hints im-prove over the baseline."
"The improvements are not enormous, but are significant (at the 95% level, as measured by McNemar’s test)."
"Unfortunately, the improvements for learning with hints over the self-training model are only significant at the 90% level."
We have presented a method for simultaneously learning two tasks using prior knowledge about the relationship between their outputs.
This is related to joint inference[REF_CITE].
"How-ever, we do not require that that a single data set be labeled for multiple tasks."
"In all our examples, we use separate data sets for shallow parsing as for named-entity recognition."
"Although all our exper-iments used the LaSO framework for sequence la-beling, there is noting in our method that assumes any particular learner; alternatives include: condi-tional random fields[REF_CITE], indepen-dent predictors[REF_CITE], max-margin Markov networks[REF_CITE], etc."
"Our approach, both algorithmically and theoreti-cally, is most related to ideas in co-training[REF_CITE]."
"The key difference is that in co-training, one assumes that the two “views” are on the inputs; here, we can think of the two out-put spaces as being the difference “views” and the compatibility function χ being a method for recon-ciling these two views."
"Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence."
"Also like that work, we do not currently have a theoret-ical framework for this (more complex) model. [Footnote_5] It would also be interesting to explore soft hints, where the range of χ is [0, 1] rather than {0, 1}."
"5[REF_CITE]proved, three years later, that a for-mal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations."
"Recently,[REF_CITE]proposed a co-regularization framework for learning across multi-ple related tasks with different output spaces."
Their approach hinges on a constrained EM framework and addresses a quite similar problem to that ad-dressed by this paper.
The show very promis-ing results in the context of semantic role labeling.
"Given the apparent (very!) recent interest in this problem, it would be ideal to directly compare the different approaches."
"In addition to an analysis of the theoretical prop-erties of the algorithm presented, the most com-pelling avenue for future work is to apply this frame-work to other task pairs."
"With a little thought, one can imagine formulating compatibility functions be-tween tasks like discourse parsing and summariza-ti[REF_CITE], parsing and word alignment, or summarization and information extraction."
The proof of Theorem 1 closes follows that[REF_CITE].
"Proof (Theorem 1, sketch)."
Use the following nota-tion: c k =
"Pr D [h(x) = k], p l ="
"Pr D [f(x) = l], q l|k ="
Pr D [f(x) = l | h(x) = k].
Denote by A the set of outputs that satisfy the constraints.
"We are interested in the probability that h(x) is erroneous, given that h(x) satisfies the constraints: p (h(x) ∈ A\{l} | f(x) = l) = X p (h(x) = k | f(x) = l) = X c k q l|k /p l k∈A\{l} k∈A\{l} ≤ X c k (|Y| − 1 + X 1/p l ) ≤ 2 X c k (|Y| − 1) k∈A l6=k k∈A"
"Here, the second step is Bayes’ rule plus definitions, the third step is by the weak initial hypothesis as-sumption, and the last step is by algebra."
"Thus, in order to get a probability of error at most η, we need k∈A c k = Pr[h(x) ∈"
A] ≤ η/(2(|Y| − 1)).
The proof of Corollary 1 is straightforward.
"Proof (Corollary 1, sketch)."
"Write out the probabil-ity of error as a double sum over true labels y 1 , y 2 and predicted labels ŷ 1 , ŷ 2 subject to χ(ŷ 1 , ŷ 2 )."
Use the uncorrelation assumption and Bayes’ to split this into the product two terms as in the proof of Theo-rem 1.
Bound as before.
"NLP tasks are often domain specific, yet sys-tems can learn behaviors across multiple do-mains."
We develop a new multi-domain online learning framework based on parameter com-bination from multiple classifiers.
"Our algo-rithms draw from multi-task learning and do-main adaptation to adapt multiple source do-main classifiers to a new target domain, learn across multiple similar domains, and learn across a large number of disparate domains."
We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment clas-sification and spam filtering.
"Statistical classifiers routinely process millions of websites, emails, blogs and other text every day."
Variability across different data sources means that training a single classifier obscures differences and separate classifiers ignore similarities.
"Similarly, adding new domains to existing systems requires adapting existing classifiers."
"We present new online algorithms for three multi-domain learning scenarios: adapting existing classi-fiers to new domains, learning across multiple simi-lar domains and scaling systems to many disparate domains."
"Multi-domain learning combines char-acteristics of both multi-task learning and domain adaptation and drawing from both areas, we de-velop a multi-classifier parameter combination tech-nique for confidence-weighted (CW) linear classi-fiers[REF_CITE]."
We focus on online algo-rithms that scale to large amounts of data.
"Next, we describe multi-domain learning and re-view the CW algorithm."
We then consider our three settings using multi-classifier parameter combina-tion.
We conclude with related work.
"In online multi-domain learning, each instance x is drawn from a domain d specific distribution x ∼ D d over a vectors space R N and labeled with a domain specific function f d with label y ∈ {−1,+1} (for binary classification.)"
"On round i the classifier re-ceives instance x i and domain identifier d i and pre-dicts label ŷ i ∈ {−1, +1}."
"It then receives the true label y i ∈ {−1, +1} and updates its prediction rule."
"As an example, consider a multi-user spam fil-ter, which must give high quality predictions for new users (without new user data), learn on multi-ple users simultaneously and scale to thousands of accounts."
"While a single classifier trained on all users would generalize across users and extend to new users, it would fail to learn user-specific prefer-ences."
"Alternatively, separate classifiers would cap-ture user-specific behaviors but would not general-ize across users."
The approach we take to solv-ing multi-domain problems is to combine domain-specific classifiers.
"In the adaptation setting, we combine source domain classifiers for a new tar-get domain."
"For learning across domains, we com-bine domain-specific classifiers and a shared classi-fier learned across all domains."
For learning across disparate domains we learn which domain-specific and shared classifiers to combine.
Multi-domain learning combines properties of both multi-task learning and domain adaptation.
"As in multi-task learning, we consider domains that are labeled with different classification functions."
"For example, one user may enjoy some emails that an-other user considers spam: differing in their classifi-cation function."
The goal of multi-task learning is to generalize across tasks/domains[REF_CITE].
"Furthermore, as in do-main adaptation, some examples are draw from dif-ferent distributions."
"For example, one user may re-ceive emails about engineering while another about art, differing in their distribution over features."
Do-main adaptation deals with these feature distribution changes[REF_CITE].
Our work combines these two areas by learning both across distributions and behaviors or functions.
"Confidence-weighted (CW) linear classificati[REF_CITE], a new online algorithm, main-tains a probabilistic measure of parameter confi-dence, which may be useful in combining parame-ters from different domain distributions."
We sum-marize CW learning to familiarize the reader.
Parameter confidence is formalized by a Gaussian distribution over weight vectors with mean µ ∈ R N and diagonal covariance Σ ∈ R
"The values µ j and Σ j,j represent knowledge of and confidence in the parameter for feature j."
"The smaller Σ j,j , the more confidence we have in the mean parameter value µ j ."
In this work we consider diagonal covari-ance matrices to scale to NLP data.
"A model predicts the highest probability label, arg max Pr w∼N(µ,Σ) [y i (w · x i ) ≥ 0] . y∈{±1}"
"The Gaussian distribution over parameter vectors w induces a univariate Gaussian distribution over the score S i = w · x i parameterized by µ, Σ and the instance x i :"
"S i ∼ N µ i , σ 2i , with mean µ i = µ·x i and variance σ i2 = x &gt;i"
Σx i .
The CW algorithm is inspired by the Passive Ag-gressive (PA) update[REF_CITE]— which ensures a positive margin while minimizing parameter change.
CW replaces the Euclidean dis-tance used in the PA update with the Kullback-Leibler (KL) divergence over Gaussian distribu-tions.
"It also replaces the minimal margin constraint with a minimal probability constraint: with some given probability η ∈ (0.5, 1] a drawn classifier will assign the correct label."
"This strategy yields the fol-lowing objective solved on each round of learning: min D KL (N (µ, Σ) k N (µ i , Σ i )) s.t."
"Pr [y i (w · x i ) ≥ 0] ≥ η , where (µ i ,Σ i ) are the parameters on round i and w ∼ N (µ, Σ)."
"The constraint ensures that the re-sulting parameters µ i+1 , Σ i+1 will correctly clas-sify x i with probability at least η."
"For convenience we write φ = Φ −1 (η), where Φ is the cumula-tive function of the normal distribution."
"The opti-mization problem above is not convex, but a closed form approximation of its solution has the follow-ing additive form: µ i+1 = µ i + α i y i Σ i x i and Σ −1i+1 = Σ −1i + 2α i φx i x &gt;i for, q −(1+2φµ i )+ (1+2φµ i ) 2 −8φ µ i −φσ i2 α i = . 4φσ i2"
"Each update changes the feature weights µ, and in-creases confidence (variance Σ always decreases)."
"We employ CW classifiers since they provide con-fidence estimates, which are useful for classifier combination."
"Additionally, since we require per-parameter confidence estimates, other confidence based classifiers are not suitable for this setting."
The basis of our approach to multi-domain learning is to combine the parameters of CW classifiers from separate domains while respecting parameter confi-dence.
"A combination method takes M CW classi-fiers each parameterized by its own mean and vari-ance parameters {(µ m ,Σ m )} Mm=1 and produces a single combined classifier (µ c , Σ c )."
A simple tech-nique would be to average the parameters of classi-fiers into a new classifier.
"However, this ignores the difference in feature distributions."
Consider for ex-ample that the weight associated with some word in a source classifier has a value of 0.
This could either mean that the word is very rare or that it is neutral for prediction (like the work “the”).
"The informa-tion captured by the variance parameter allow us to distinguish between the two cases: an high-variance indicates a lack of confidence in the value of the weight vectors because of small number of exam-ples (first case), and vise-versa, small-variance indi-cates that the value of the weight is based on plenty of evidence."
We favor combinations sensitive to this distinction.
"Since CW classifiers are Gaussian distributions, we formalize classifier parameter combination as finding a new distribution that minimizes the weighted-divergence to a set of given distributions:"
"M (µ c , Σ c ) = arg min X D((µ c , Σ c )||(µ m , Σ m ) ; b m ) , m where (since Σ is diagonal),"
"D((µ c , Σ c )||(µ, Σ) ; b) ="
"P N b D((µ cf , Σ cf,f )||(µ f , Σ f,f )) . f f"
The (classifier specific) importance-weights b m ∈ N
R + are used to weigh certain parameters of some domains differently in the combination.
"When D is the Euclidean distance (L2), we have,"
"D((µ cf , Σ cf,f )||(µ f , Σ f,f )) = (µ cf − µ f ) 2 + (Σ cf,f − Σ f,f ) 2 . and we obtain:"
"M 1 µ cf = X b mf µ mf , P M b mm f m M Σ cf,f = P 1 Xb mf Σ mf,f . (1) m∈M b mf m"
Note that this is a (weighted) average of parameters.
The other case we consider is when D is a weighted KL divergence we obtain a weighting of µ by Σ −1 :
"M ! −1 M µ cf = X(Σ mf,f ) −1 b fm X(Σ mf,f ) −1 µ mf b mf m m M ! −1 M (Σ c ) −1 = M X b mf X(Σ mf ) −1 b fm . (2) m m"
"While each parameter is weighed by its variance in the KL, we can also explicitly encode this behavior as b mf = a − Σ mf,f ≥ 0, where a is the initializa-tion value for Σ mf,f ."
We call this weighting “vari-ance” as opposed to a uniform weighting of param-eters (b mf = 1).
We therefore have two combination methods (L2 and KL) and two weighting methods (uniform and variance).
For evaluation we selected two domain adaptation datasets: spam[REF_CITE]and sentiment[REF_CITE].
"The spam data contains two tasks, one with three users (task A) and one with 15 (task B)."
The goal is to classify an email (bag-of-words) as either spam or ham (not-spam) and each user may have slightly different preferences and fea-tures.
"The sentiment data contains product reviews from Amazon for four product types: books, dvds, elec-tronics and kitchen appliances and we extended this with three additional domains: apparel, music and videos."
We follow Blitzer et. al. for feature ex-traction.
We created different datasets by modify-ing the decision boundary using the ordinal rating of each instance (1-5 stars) and excluding boundary instances.
"We use four versions of this data: • All - 7 domains, one per product type • Books - 3 domains of books with the binary decision boundary set to 2, 3 and 4 stars • DVDs - Same as Books but with DVD reviews • Books+DVDs - Combined Books and DVDs"
"The All dataset captures the typical domain adap-tation scenario, where each domain has the same decision function but different features."
Books and DVDs have the opposite problem: the same features but different classification boundaries.
Books+DVDs combines both issues.
"We begin by examining the typical domain adapta-tion scenario, but from an online perspective since learning systems often must adapt to new users or domains quickly and with no training data."
"For ex-ample, a spam filter with separate classifiers trained on each user must also classify mail for a new user."
"Since other user’s training data may have been deleted or be private, the existing classifiers must be combined for the new user."
We combine the existing user-specific classifiers into a single new classifier for a new user.
"Since nothing is known about the new user (their deci-sion function), each source classifier may be useful."
"However, feature similarity – possibly measured us-ing unlabeled data – could be used to weigh source domains."
"Specifically, we combine the parameters of each classifier according to their confidence us-ing the combination methods described above."
"We evaluated the four combination strategies – L2 vs. KL, uniform vs. variance – on spam and sen-timent data."
"For each evaluation, a single domain was held out for testing while separate classifiers were trained on each source domain, i.e. no target training."
Source classifiers are then combined and the combined classifier is evaluated on the test data (400 instances) of the target domain.
Each classi-fier was trained for 5 iterations over the training data (to ensure convergence) and each experiment was repeated using 10-fold cross validation.
The CW parameter φ was tuned on a single randomized run for each experiment.
"We include several baselines: training on target data to obtain an upper bound on performance (Target), training on all source do-mains together, a useful strategy if all source data is maintained (All Src), selecting (with omniscience) the best performing source classifier on target data (Best Src), and the expected real world performance of randomly selecting a source classifier (Avg Src)."
"While at least one source classifier achieved high performance on the target domain (Best Src), the correct source classifier cannot be selected without target data and selecting a random source classifier yields high error."
"In contrast, a combined classifier almost always improved over the best source domain classifier (table 1)."
That some of our results improve over the best training scenario is likely caused by in-creased training data from using multiple domains.
Increases over all available training data are very in-teresting and may be due to a regularization effect of training separate models.
The L2 methods performed best and KL improved 7 out of 10 combinations.
Classifier parameter com-bination can clearly yield good classifiers without prior knowledge of the target domain.
"In addition to adapting to new domains, multi-domain systems should learn common behaviors across domains."
"Naively, we can assume that the domains are either sufficiently similar to warrant one classifier or different enough for separate clas-sifiers."
The reality is often more complex.
"Instead, we maintain shared and domain-specific parameters and combine them for learning and prediction."
"Multi-task learning aims to learn common behav-iors across related problems, a similar goal to multi-domain learning."
The primary difference is the na-ture of the domains/tasks: in our setting each domain is the same task but differs in the types of features in addition to the decision function.
A multi-task ap-proach can be adapted to our setting by using our classifier combination techniques.
We seek to learn domain specific parameters guided by shared parameters.
"Specifically, they as-sumed that the weight vector for problem d could be represented as w c = w d +w s , where w d are task spe-cific parameters and w s are shared across all tasks."
"In this framework, all tasks are close to some under-lying mean w s and each one deviates from this mean by w d ."
"Their SVM style multi-task objective mini-mizes the loss of w c and the norm of w d and w s , with a tradeoff parameter allowing for domain deviance from the mean."
The simple domain adaptation al-gorithm of feature splitting used[REF_CITE]is a special case of this model where the norms are equally weighted.
"An analogous CW objective is: 1 min d , Σ d k N µ di , Σ di  λ 1 D KL N µ 1 s , Σ s ) k N (µ si , Σ si )) λ 2 D KL (N (µ + s.t."
"Pr w∼N(µ c ,Σ c ) [y i (w · x i ) ≥ 0] ≥ η . (3) µ d , Σ d are the parameters for domain d, (µ s , Σ s ) for the shared classifier and (µ c ,Σ c ) for the com-bination of the domain and shared classifiers."
The parameters are combined via (2) with only two ele-ments summed - one for the shared parameters s and the other for the domain parameters d .
This captures the intuition of Evgeniou and Pontil: updates en-force the learning condition on the combined param-eters and minimize parameter change.
"For conve-nience, we rewrite λ 2 = 2 − 2λ 1 , where λ 1 ∈ [0, 1]."
"If classifiers are combined using the sum of the indi-vidual weight vectors and λ 1 = 0.5, this is identical to feature splitting (Daumé) for CW classifiers."
The domain specific and shared classifiers can be updated using the closed form solution to (3) as: µ s = µ si + λ 2 αy i Σ c x i (Σ s ) −1 = (Σ si ) −1 + 2λ 2 αφx i x Ti µ d = µ di + λ 1 αy i Σ ci x i (Σ d ) −1 = (Σ di ) −1 + 2λ 1 αφx i x Ti (4)
We call this objective Multi-Domain Regulariza-tion (MDR).
"As before, the combined parameters are produced by one of the combination methods."
"On each round, the algorithm receives instance x i and domain d i for which it creates a combined clas-sifier (µ c , Σ c ) using the shared (µ s , Σ s ) and domain specific parameters µ d ,Σ d ."
A prediction is is-sued using the standard linear classifier prediction rule sign(µ c · x) and updates follow (4).
"The ef-fect is that features similar across domains quickly converge in the shared classifier, sharing informa-tion across domains."
The combined classifier re-flects shared and domain specific parameter confi-dences: weights with low variance (i.e. greater con-fidence) will contribute more.
"We evaluate MDR on a single pass over a stream of instances from multiple domains, simulating a real world setting."
Parameters λ 1 and φ are iter-atively optimized on a single randomized run for each dataset.
All experiments use 10-fold CV.
"In ad-dition to evaluating the four combination methods with MDR, we evaluate the performance of a sin-gle classifier trained on all domains (Single), a sep-arate classifier trained on each domain (Separate), Feature Splitting (Daumé) and feature splitting with optimized λ 1 (MDR)."
Table 3 shows results on test data and table 2 shows online training error.
"In this setting, L2 combinations prove best on 5 of 6 datasets, with the variance weighted combina-tion doing the best."
"MDR (optimizing λ 1 ) slightly improves over feature splitting, and the combination methods improve in every case."
"Our best result is statistically significant compared to Feature Split-ting using McNemar’s test (p = .001) for Task B, Books, DVD, Books+DVD."
"While a single or sepa-rate classifiers have a different effect on each dataset, MDR gives the best performance overall."
So far we have considered settings with a small number of similar domains.
"While this is typical of multi-task problems, real world settings present many domains which do not all share the same be-haviors."
Online algorithms scale to numerous ex-amples and we desire the same behavior for numer-ous domains.
"Consider a spam filter used by a large email provider, which filters billions of emails for millions of users."
Suppose that spammers control many accounts and maliciously label spam as legiti-mate.
"Alternatively, subsets of users may share pref-erences."
"Since behaviors are not consistent across domains, shared parameters cannot be learned."
We seek algorithms robust to this behavior.
"Since subsets of users share behaviors, these can be learned using our MDR framework."
"For example, discovering spammer and legitimate mail accounts would enable intra-group learning."
The challenge is the online discovery of these subsets while learning model parameters.
We augment the MDR frame-work to additionally learn this mapping.
We begin by generalizing MDR to include k shared classifiers instead of a single set of shared pa-rameters.
Each set of shared parameters represents a different subset of domains.
"If the corresponding shared parameters are known for a domain, we could use the same objective (3) and update (4) as before."
"If there are many fewer shared parameters than do-mains (k D), we can benefit from multi-domain learning."
"Next, we augment the learning algorithm to learn a mapping between the domains and shared classifiers."
"Intuitively, a domain should be mapped to shared parameters that correctly classify that do-main."
"A common technique for learning such ex-perts in the Weighted Majority algorithm[REF_CITE], which weighs a mixture of experts (classifiers)."
"However, since we require a hard assignment — pick a single shared parameter set s — rather than a mixture, the algorithm reduces to picking the classifier s with the fewest mistakes in predicting domain d."
This requires tracking the number of mistakes made by each shared classifier on each domain once a label is revealed.
"For learn-ing, the shared classifier with the fewest mistakes for a domain is selected for an MDR update."
Clas-sifier ties are broken randomly.
"While we experi- mented with more complex techniques, this simple method worked well in practice."
"When a new do-main is added to the system, it takes fewer exam-ples to learn which shared classifier to use instead of learning a new model from scratch."
"While this approach adds another free parameter (k) that can be set using development data, we ob-serve that k can instead be fixed to a large constant."
"Since only a single shared classifier is updated each round, the algorithm will favor selecting a previ-ously used classifier as opposed to a new one, using as many classifiers as needed but not scaling up to k."
"This may not be optimal, but it is a simple."
"To evaluate a larger number of domains, we cre-ated many varying domains using spam and senti-ment data."
"For spam, 6 email users were created by splitting the 3 task A users into 2 users, and flipping the label of one of these users (a malicious user), yielding 400 train and 100 test emails per user."
"For sentiment, the book domain was split into 3 groups with binary boundaries at a rating of 2, 3 or 4."
"Each of these groups was split into 8 groups of which half had their labels flipped, creating 24 domains."
"The same procedure was repeated for DVD reviews but for a decision boundary of 3, 6 groups were created, and for a boundary of 2 and 4, 3 groups were created with 1 and 2 domains flipped respectively, resulting in 12 DVD domains and 36 total domains with var-ious decision boundaries, features, and inverted de-cision functions."
Each domain used 300 train and 100 test instances. 10-fold cross validation with one training iteration was used to train models on these two datasets.
Parameters were optimized as before.
"Experiments were repeated for various settings of k. Since L2 performed well before, we evaluated MDR+L2 and MDR+L2-Var."
The results are shown in figure 1.
"For both spam and sentiment adding additional shared parameters beyond the single shared classifier significantly re-duces error, with further reductions as k increases."
This yields a 45% error reduction for spam and a 38% reduction for sentiment over the best baseline.
"While each task has an optimal k (about 5 for spam, 2 for sentiment), larger values still achieve low error, indicating the flexibility of using large k values."
"While adding parameters clearly helps for many domains, it may be impractical to keep domain-specific classifiers for thousands or millions of do-mains."
"In this case, we could eliminate the domain-specific classifiers and rely on the k shared clas-sifiers only, learning the domain to classifier map-ping."
"We compare this approach using the best result from MDR above, again varying k. Figure 2 shows that losing domain-specific parameters hurts perfor-mance, but is still an improvement over baseline methods."
"Additionally, we can expect better perfor-mance as the number of similar domains increases."
This may be an attractive alternative to keeping a very large number of parameters.
Multi-domain learning intersects two areas of re-search: domain adaptation and multi-task learning.
"In domain adaptation, a classifier trained for a source domain is transfered to a target domain using either unlabeled or a small amount of labeled target data."
"In a complimentary approach,[REF_CITE]weighed training instances based on their similarity to unlabeled target domain data."
"Several approaches utilize source data for training on a limited number of target labels, including feature splitting[REF_CITE]and adding the source classifier’s prediction as a feature[REF_CITE]."
"Others have considered transfer learning, in which an existing domain is used to improve learning in a new do-main, such as constructing priors ([REF_CITE];"
"These methods largely require batch learn-ing, unlabeled target data, or available source data at adaptation."
"In contrast, our algorithms operate purely online and can be applied when no target data is available."
"Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultane-ously[REF_CITE]."
"The most relevant approach is that of Regularized Multi-Task Learning[REF_CITE], which we use to motivate our online algorithm."
We generalize this work to both include an arbi-trary classifier combination and many shared classi-fiers.
Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’[REF_CITE]).
There are many techniques for combining the out-put of multiple classifiers for ensemble learning or mixture of experts.
Kittler et al.[REF_CITE]provide a theoretical framework for combining classifiers.
"Some empirical work has considered adding versus multiplying classifier output[REF_CITE], using local accuracy estimates for combinati[REF_CITE], and applications to NLP tasks[REF_CITE]."
"However, these papers consider combin-ing classifier output for prediction."
"In contrast, we consider parameter combination for both prediction and learning."
We have explored several multi-domain learning settings using CW classifiers and a combination method.
"Our approach creates a better classifier for a new target domain than selecting a random source classifier a prior, reduces learning error on multiple domains compared to baseline approaches, can han-dle many disparate domains by using many shared classifiers, and scales to a very large number of do-mains with a small performance reduction."
These scenarios are realistic for NLP systems in the wild.
This work also raises some questions about learning on large numbers of disparate domains: can a hi- erarchical online clustering yield a better represen-tation than just selecting between k shared parame-ters?
"Additionally, how can prior knowledge about domain similarity be included into the combination methods?"
We plan to explore these questions in fu-ture work.
Acknowledgements This material is based upon work supported by the Defense Advanced Re-search Projects Agency (DARPA) under Contract No.[REF_CITE]-07-D-0185.
"Previous work on ordering events in text has typically focused on local pairwise decisions, ignoring globally inconsistent labels."
"How-ever, temporal ordering is the type of domain in which global constraints should be rela-tively easy to represent and reason over."
This paper presents a framework that informs lo-cal decisions with two types of implicit global constraints: transitivity (A before B and B be-fore C implies A before C) and time expression normalization (e.g. last month is before yes-terday).
"We show how these constraints can be used to create a more densely-connected network of events, and how global consis-tency can be enforced by incorporating these constraints into an integer linear programming framework."
"We present results on two event ordering tasks, showing a 3.6% absolute in-crease in the accuracy of before/after classifi-cation over a pairwise model."
Being able to temporally order events is a neces-sary component for complete document understand-ing.
Interest in machine learning approaches for this task has recently been encouraged through the cre-ation of the Timebank Corpus[REF_CITE].
"However, most work on event-event order-ing has focused on improving classifiers for pair-wise decisions, ignoring obvious contradictions in the global space of events when misclassifications occur."
A global framework to repair these event or-dering mistakes has not yet been explored.
"This paper addresses three main factors involved in a global framework: the global optimization al-gorithm, the constraints that are relevant to the task, and the level of connectedness across pairwise de-cisions."
"We employ Integer Linear Programming to address the first factor, drawing from related work in paragraph ordering[REF_CITE]."
"After finding minimal gain with the initial model, we ex-plore reasons for and solutions to the remaining two factors through temporal reasoning and transitivity rule expansion."
We analyze the connectivity of the Timebank Cor-pus and show how textual events can be indirectly connected through a time normalization algorithm that automatically creates new relations between time expressions.
We show how this increased con-nectivity is essential for a global model to improve performance.
"We present three progressive evaluations of our global model on the Timebank Corpus, showing a 3.6% gain in accuracy over its original set of re-lations, and an 81% increase in training data size from previous work."
"In addition, we present the first results on Timebank that include an unknown rela-tion, establishing a benchmark for performance on the full task of document ordering."
Recent work on classifying temporal relations within the Timebank Corpus built 6-way relation classifiers over 6 of the corpus’ 13 relations[REF_CITE].
"A wide range of features are used, ranging from sur-face indicators to semantic classes."
Classifiers make local pairwise decisions and do not consider global implications between the relations.
"The TempEval-07[REF_CITE]contest recently used two relations, before and after, in a semi-complete textual classification task with a new third relation to distinguish relations that can be la-beled with high confidence from those that are un-certain, called vague."
"The task was a simplified clas-sification task from Timebank in that only one verb, the main verb, of each sentence was used."
"Thus, the task can be viewed as ordering the main events in pairwise sentences rather than the entire document."
"This paper uses the core relations of TempEval (before,after,vague) and applies them to a full docu-ment ordering task that includes every labeled event in Timebank."
"In addition, we extend the previous work by including a temporal reasoning component and embedding it within a global constraint model."
"The Timebank Corpus[REF_CITE]is a corpus of 186 newswire articles that are tagged for events, time expressions, and relations between the events and times."
"The individual events are fur-ther tagged for temporal information such as tense, modality and grammatical aspect."
Time expressions use the TimeML[REF_CITE]markup language.
"There are 6 main relations and their inverses in Timebank: before, ibefore, includes, begins, ends and simultaneous."
"This paper describes work that classifies the re-lations between events, making use of relations be-tween events and times, and between the times themselves to help inform the decisions."
"Our initial model has two components: (1) a pair-wise classifier between events, and (2) a global con-straint satisfaction layer that maximizes the confi-dence scores from the classifier."
The first is based on previous work[REF_CITE]and the second is a novel contribution to event-event classification.
Classifying the relation between two events is the basis of our model.
A soft classification with confi- dence scores is important for the global maximiza-tion step that is described in the next section.
"As[REF_CITE], we build support vec-tor machine (SVM) classifiers and use the probabili-ties from pairwise SVM decisions as our confidence scores."
These scores are then used to choose an op-timal global ordering.
"Following our previous work, we use the set of features summarized in figure 1."
"They vary from POS tags and lexical features surrounding the event, to syntactic dominance, to whether or not the events share the same tense, grammatical aspect, or aspec-tual class."
These features are the highest performing set on the basic 6-way classification of Timebank.
"We use Timebank’s hand tagged attributes in the feature values for the purposes of this comparative study of global constraints, described next."
Pairwise classifiers can make contradictory classifi-cations due to their inability to consider other deci-sions.
"For instance, the following three decisions are in conflict:"
A before B B before C A after C
Transitivity is not taken into account.
"In fact, there are several ways to resolve the conflict in this exam-ple."
"Given confidence scores (or probabilities) for each possible relation between the three pairs, we can compute an optimal label assignment."
Differ-ent scores can lead to different conflict resolutions.
Figure 2 shows two resolutions given different sets of scores.
"The first chooses before for all three rela-tions, while the second chooses after."
"They found that Integer Linear Programming (ILP) performed the best on a paragraph ordering task, consistent with its property of being able to find the optimal solution for a set of constraints."
"Other approaches are variations on a greedy strategy of adding pairs of events one at a time, ordered by their confidence."
"These can lead to suboptimal configurations, although they are guar-anteed to find a solution."
"We also implemented a greedy best-first strategy, but found ILP outperformed it."
"Our Integer Linear Programming framework uses the following objective function: max X X p ij x ij (1) i j with added constraints: ∀i∀j x ij ∈ {0,1} (2) ∀i x i1 + x i2 + ... + x im = 1 (3) where x ij represents the ith pair of events classified as the jth relation of m relations."
"Thus, each pair of events generates m variables."
"Given n pairs of events, there are n ∗ m variables. p ij is the proba-bility of classifying pair i with relation j. Equation 2 (the first constraint) simply says that each variable must be 0 or [URL_CITE]."
Equation 3 contains m variables for a single pair of events i representing its m possible relations.
It states that one relation must be set to 1 and the rest to 0.
"In other words, a pair of events cannot have two relations at the same time."
"Finally, a transitivity constraint is added for all connected pairs i, j, k, for each transitivity condition that infers relation c given a and b: x ia + x jb − x kc &lt;= 1 (4)"
We generated the set of constraints for each doc-ument and used lpsolve 1 to solve the ILP constraint problem.
The transitivity constraints are only effective if the available pairwise decisions constitute a con-nected graph.
"If pairs of events are disconnected, then transitivity makes little to no contribution be-cause these constraints are only applicable to con-nected chains of events."
"In order to connect the event graph, we draw on work[REF_CITE]and apply transitive closure to our documents."
"Transitive closure was first proposed not to address the problem of con-nected event graphs, but rather to expand the size of training data for relations such as before."
Time-bank is a relatively small corpus with few examples of each relation.
One way of expand the training set is through transitive rules.
A few rules are given here:
A simultaneous B ∧ A before C → B before C A includes B ∧ A ibefore C → B before C A before B ∧ A ends C → B after C
"While the original motivation was to expand the training size of tagged relations, this approach also creates new connections in the graph, replacing pre-viously unlabeled event pairs with their true rela-tions."
We adopted this approach and closed the orig-inal set of 12 relations to help connect the global constraint model.
The first evaluation of our global temporal model is on the Timebank Corpus over the labeled rela-tions before and after.
"We merged ibefore and iafter into these two relations as well, ignoring all oth-ers."
We use this task as a reduced evaluation to study the specific contribution of global constraints.
We also chose this strict ordering task because it is well defined from a human understanding perspec-tive.
An evaluation includ-ing unknown (or vague as in TempEval) is presented later.
We expanded the corpus (prior to selecting the be-fore/after relations) using transitive closure over all 12 relations as described above.
Figure 3 shows the increase in data size.
The number of before and after relations increase by a factor of six.
We trained and tested the system with 10-fold cross validation and micro-averaged accuracies.
The folds were randomly generated to separate the 186 files into 10 folds (18 or 19 files per fold).
We used libsvm [URL_CITE] to implement our SVM classifiers.
Figure 4 shows the results from our ILP model with transitivity constraints.
The first row is the baseline pairwise classification trained and tested on the original Timebank relations.
The second row gives performance with ILP.
The model shows no improvement.
"The global ILP constraints did affect local decisions, changing 175 of them (out of 7324), but the changes cancelled out and had no affect on overall accuracy."
Why didn’t a global model help?
The problem lies in the graph structure of Timebank’s annotated rela-tions.
The Timebank annotators were not required to annotate relations between any particular pair of events.
"Instead, they were instructed to annotate what seemed appropriate due to the almost insur-mountable task of annotating all pairs of events."
"A modest-sized document of 30 events, for example, would contain 302 = 435 possible pairs."
"Anno-tators thus marked relations where they deemed fit, most likely between obvious and critical relations to the understanding of the article."
"The vast majority of possible relations are untagged, thus leaving a large set of unlabeled (and disconnected) unknown rela-tions."
Figure 5 graphically shows all relations that are annotated between events and time expressions in one of the shorter Timebank documents.
"Nodes rep-resent events and times (event nodes start with the letter ’e’, times with ’t’), and edges represent tempo-ral relations."
"Solid lines indicate hand annotations, and dotted lines indicate new rules from transitive closure (only one, from event e4 to time t14)."
"As can be seen, the graph is largely disconnected and a global model contributes little information since transitivity constraints cannot apply."
The large amount of unlabeled relations in the corpus presents several problems.
"First, building a classifier for these unknown relations is easily over-whelmed by the huge training set."
"Second, many of the untagged pairs have non-unknown ordering rela-tions between them, but were missed by the annota-tors."
This point is critical because one cannot filter this noise when training an unknown classifier.
The noise problem will appear later and will be discussed in our final experiment.
"Finally, the space of an-notated events is very loosely connected and global constraints cannot assist local decisions if the graph is not connected."
The results of this first experiment illustrate this latter problem.
"They performed an independent annotation of 129 of Timebank’s 186 documents, tagging all events in verb-clause relationships."
"They found over 600 valid before/after relations that are untagged in Timebank, on average three per docu-ment."
"One must assume that if these nearby verb-clause event pairs were missed by the annotators, the much larger number of pairs that cross sentence boundaries were also missed."
The next model thus attempts to fill in some of the gaps and further connect the event graph by using two types of knowledge.
"The first is by integrating Bethard’s data, and the second is to perform tempo-ral reasoning over the document’s time expressions (e.g. yesterday or january 1999)."
"Our initial model contained two components: (1) a pairwise classifier between events, and (2) a global constraint satisfaction layer."
"However, due to the sparseness in the event graph, we now introduce a third component addressing connectivity: (3) a temporal reasoning component to inter-connect the global graph and assist in training data expansion."
"One important aspect of transitive closure in-cludes the event-time and time-time relations during closure, not just the event-event links."
"Starting with 5,947 different types of relations, transitive rules in-crease the dataset to approximately 12,000."
"How-ever, this increase wasn’t enough to be effective in global reasoning."
"To illustrate the sparsity that still remains, if each document was a fully connected graph of events, Timebank would contain close to 160,000 relations [Footnote_3] , more than a 13-fold increase."
"3 Sum over the # of events n d in each document d, n2 d"
More data is needed to enrich the Timebank event graph.
"Two types of information can help: (1) more event-event relations, and (2) a separate type of in-formation to indirectly connect the events: event- X-event."
We incorporate the new annotations[REF_CITE]to address (1) and introduce a new temporal reasoning procedure to address (2).
The following section describes this novel approach to adding time expression information to further connect the graph.
"As described above, we use event-time relations to produce the transitive closure, as well as annotated time-time relations."
It is unclear if[REF_CITE]used these latter relations in their work.
"However, we also add new time-time links that are deduced from the logical time intervals that they describe."
Time expressions can be resolved to time intervals with some accuracy through simple rules.
New time-time relations can then be added to our space of events through time stamp comparisons.
Take this newswire example:
The first two expressions (‘previous Friday’ and ‘Oct. 13’) are in a clear before relation-ship that Timebank annotators captured.
"The ‘current’ expression, is correctly tagged with the PRESENT REF attribute to refer to the document’s timestamp."
Both ‘previous Friday’ and ‘Oct. 13’ should thus be tagged as being before this expres-sion.
"However, the annotators did not tag either of these two before relations, and so our timestamp resolution procedure fills in these gaps."
"This is a common example of two expressions that were not tagged by the annotators, yet are in a clear temporal relationship."
We use Timebank’s gold standard TimeML an-notations to extract the dates and times from the time expressions.
"In addition, those marked as PRESENT REF are resolved to the document times-tamp."
Time intervals that are strictly before or after each other are thus labeled and added to our space of events.
We create new before relations based on the following procedure: if event1.year &lt; event2.year return true if event1.year == event2.year if event1.month &lt; event2.month return true if event1.month == event2.month if event1.day &lt; event2.day return true end end return false
"All other time-time orderings not including the before relation are ignored (i.e. includes is not cre-ated, although could be with minor changes)."
This new time-time knowledge is used in two sep-arate stages of our model.
"The first is just prior to transitive closure, enabling a larger expansion of our tagged relations set and reduce the noise in the un-known set."
The second is in the constraint satisfac-tion stage where we add our automatically computed time-time relations (with the gold event-time rela-tions) to the global graph to help correct local event-event mistakes.
"Our second evaluation continues the use of the two-way classification task with before and after to ex-plore the contribution of closure, time normaliza-tion, and global constraints."
We augmented the corpus with the labeled rela-tions[REF_CITE]and added the au-tomatically created time-time relations as described in section 5.1.
We then expanded the corpus using transitive closure.
Figure 6 shows the progressive data size increase as we incrementally add each to the closure algorithm.
The time-time generation component automati-cally added 2459 new before and after time-time re-lations into the 186 Timebank documents.
"This is in comparison to only 157 relations that the human annotators tagged, less than 1 per document on av-erage."
The second row of figure 6 shows the dras-tic effect that these time-time relations have on the number of available event-event relations for train-ing and testing.
Adding both Bethard’s data and the time-time data increases our training set by 81% over closure without it.
"We again performed 10-fold cross validation with micro-averaged accuracies, but each fold tested only on the transitively closed Timebank data (the first row of figure 6)."
The training set used all available data (the third row of figure 6) including the Bethard data as well as our new time-time links.
Figure 7 shows the results from the new model.
The first row is the baseline pairwise classification trained and tested on the original relations only.
Our model improves by 3.6% absolute.
"This improve-ment is statistically significant (p &lt; 0.000001, Mc-Nemar’s test, 2-tailed)."
"To further illustrate why our model now improves local decisions, we continue our previous graph ex-ample."
The actual text for the graph in figure 5 is shown here: docstamp: 10/30/89 (t14) Trustcorp Inc. will become(e1) Society Bank &amp; Trust when its merger(e3) is completed(e4) with Society Corp.
"The automatic time normalizer computes and adds three new time-time relations, two connecting t15 and t17 with the document timestamp, and one con-necting t15 and t17 together."
These are not other-wise tagged in the corpus.
Figure 8 shows the augmented document.
The double-line arrows indicate the three new time-time relations and the dotted edges are the new relations added by our transitive closure procedure.
"Most crit-ical to this paper, three of the new edges are event-event relations that help to expand our training data."
"If this document was used in testing (rather than training), these new edges would help inform our transitive rules during classification."
"Even with this added information, disconnected segments of the graph are still apparent."
"However, the 3.6% performance gain encourages us to move to the final full task."
Our final evaluation expands the set of relations to include unlabeled relations and tests on the entire dataset available to us.
"The following is now a clas-sification task between the three relations: before, after, and unknown."
We duplicated the previous evaluation by adding the labeled relations[REF_CITE]and our automatically created time-time relations.
We then expanded this dataset using transitive closure.
"Unlike the previous evaluation, we also use this en-tire dataset for testing, not just for training."
"Thus, all event-event relations in Bethard as well as Timebank are used to expand the dataset with transitive closure and are used in training and testing."
We wanted to fully evaluate document performance on every pos-sible event-event relation that logically follows from the data.
"As before, we converted IBefore and IAfter into before and after respectively, while all other rela-tions are reduced to unknown."
This relation set co-incides with TempEval-07’s core three relations (al-though they use vague instead of unknown).
"Rather than include all unlabeled pairs in our un-known set, we only include the unlabeled pairs that span at most one sentence boundary."
"In other words, events in adjacent sentences are included in the un-known set if they were not tagged by the Timebank annotators."
"The intuition is that annotators are more likely to label nearby events, and so events in adja-cent sentences are more likely to be actual unknown relations if they are unlabeled."
"It is more likely that distant events in the text were overlooked by con-venience, not because they truly constituted an un-known relationship."
"The set of possible sentence-adjacent unknown re-lations is very large (approximately 50000 unknown compared to 7000 before), and so we randomly se-lect a percentage of these relations for each evalu- ation."
We used the same SVM approach with the features described in section 4.1.
Results are presented in figure 9.
The rows in the table are different training/testing runs on varying sizes of unknown training data.
There are three columns with accuracy results of increasing com-plexity.
"The first, base, are results from pairwise classification decisions over Timebank and Bethard with no global model."
"The second, global, are re-sults from the Integer Linear Programming global constraints, using the pairwise confidence scores from the base evaluation."
"Finally, the global+time column shows the ILP results when all event-time, time-time, and automatically induced time-time re-lations are included in the global graph."
"The ILP approach does not alone improve perfor-mance on the event-event tagging task, but adding the time expression relations greatly increases the global constraint results."
This is consistent with the results from out first two experiments.
The evalua-tion with 1% of the unknown tags shows an almost 2% improvement in accuracy.
The gain becomes smaller as the unknown set increases in size (1.0% gain with 13% unknown).
Unknown relations will tend to be chosen as more weight is given to un-knowns.
"When there is a constraint conflict in the global model, unknown tends to be chosen because it has no transitive implications."
"All improvements from base to global+time are statistically significant (p &lt; 0.000001, McNemar’s test, 2-tailed)."
"The first row of figure 9 corresponds to the re-sults in our second experiment in figure 7, but shows higher accuracy."
The reason is due to our different test sets.
This final experiment includes Bethard’s event-event relations in testing.
"The improved per-formance suggests that the clausal event-event rela-tions are easier to classify, agreeing with the higher accuracies originally found[REF_CITE]."
This set was chosen for comparison because it has a similar num-ber of unknown labels as before labels.
"We see an increase in precision in both the before and after de-cisions by up to 2.7%, an increase in recall up to 2.9%, and an fscore by as much as 2.0%."
"The un-known relation shows mixed results, possibly due to its noisy behavior as discussed throughout this pa-per."
Our results on the two-way (before/after) task show that adding additional implicit temporal constraints and then performing global reasoning results in significant improvements in temporal ordering of events (3.6% absolute over simple pairwise deci-sions).
Both before and after also showed increases in precision and recall in the three-way evaluation.
"However, unknown did not parallel this improve-ment, nor are the increases as dramatic as in the two-way evaluation."
We believe this is consistent with the noise that exists in the Timebank corpus for un-labeled relations.
"Evidence from Bethard’s indepen- dent annotations directly point to missing relations, but the dramatic increase in the size of our closure data (81%) from adding a small amount of time-time relations suggests that the problem is widespread."
This noise in the unknown relation may be damp-ening the gains that the two way task illustrates.
This work is also related to the task of event-time classification.
"While not directly addressed in this paper, the global methods described within clearly apply to pairwise models of event-time ordering as well."
"Further progress in improving global constraints will require new methods to more accurately iden-tify unknown events, as well as new approaches to create implicit constraints over the ordering."
We ex-pect such an improved ordering classifier to be used to improve the performance of tasks such as summa-rization and question answering about the temporal nature of events.
Chinese is a language that does not have mor-phological tense markers that provide explicit grammaticalization of the temporal location of situations (events or states).
"However, in many NLP applications such as Machine Transla-tion, Information Extraction and Question An-swering, it is desirable to make the temporal location of the situations explicit."
We describe a machine learning framework where differ-ent sources of information can be combined to predict the temporal location of situations in Chinese text.
Our experiments show that this approach significantly outperforms the most frequent tense baseline.
"More importantly, the high training accuracy shows promise that this challenging problem is solvable to a level where it can be used in practical NLP applica-tions with more training data, better modeling techniques and more informative and general-izable features."
"In a language like English, tense is an explicit (and maybe imperfect) grammaticalization of the tempo-ral location of situations, and such temporal location is either directly or indirectly defined in relation to the moment of speech."
Chinese does not have gram-maticalized tense in the sense that Chinese verbs are not morphologically marked for tense.
"This is not to say, however, that Chinese speakers do not at-tempt to convey the temporal location of situations when they speak or write, or that they cannot inter-pret the temporal location when they read Chinese text, or even that they have a different way of repre-senting the temporal location of situations."
"In fact, there is evidence that the temporal location is rep-resented in Chinese in exactly the same way as it is represented in English and most world languages: in relation to the moment of speech."
"One piece of evi-dence to support this claim is that Chinese temporal expressions like 8U (“today”), ²U (“tomorrow”) and U (“yesterday”) all assume a temporal deixis that is the moment of speech in relation to which all temporal locations are defined."
"Such temporal expressions, where they are present, give us a clear indication of the temporal location of the situations they are associated with."
"However, not all Chinese sentences have such temporal expressions associated with them."
"In fact, they occur only infrequently in Chinese text."
"It is thus theoretically interesting to ask, in the absence of grammatical tense and explicit temporal expressions, how do readers of a particular piece of text interpret the temporal location of situa-tions?"
"There are a few linguistic devices in Chinese that provide obvious clues to the temporal location of situations, and one such linguistic device is aspect markers."
"Although Chinese does not have grammat-ical tense, it does have grammaticalized aspect in the form of aspect markers."
These aspect markers often give some indication of the temporal location of an event.
"For example, Chinese has the perfective as-pect marker and L, and they are often associated with the past."
"Progressive aspect marker X, on the other hand, is often associated with the present."
"In addition to aspect, certain adverbs also provide clues to the temporal location of the situations they are as- sociated with."
"For example, ® or ®² (”already”), often indicates that the situation they are associated with has already occurred and is thus in the past. 3, another adverbial modifier, often indicates that the situation it modifies is in the present."
"However, such linguistic associations are imperfect, and they can only be viewed as tendencies rather than rules that one can use to deterministically infer the temporal location of a situation."
"For example, while ® in-deed indicates that the situation described in (1) is in the past, when it modifies a stative verb as it does in (1b), the situation is still in the present. (1) a. ¦ [®]  T 8 &quot; he already finish this project . ”He already finished the project.” b. ¥I [®] Pk ) ­.?"
China already has produce world-class ^ Ä: &quot; software DE foundation . ”China already has the foundation to pro-duce world-class software.”
"More importantly, only a small proportion of verb instances in any given text have such explicit tempo-ral indicators and therefore they cannot be the whole story in the temporal interpretation of Chinese text."
It is thus theoretically interesting to go beyond the obvious and investigate what additional information is relevant in determining the temporal location of a situation in Chinese.
Being able to infer the temporal location of a situ-ation has many practical applications as well.
"For example, this information would be highly valu-able to Machine Translation."
"To translate a lan-guage like Chinese into a language like English in which tense is grammatically marked with inflec-tional morphemes, an MT system will have to in-fer the necessary temporal information to determine the correct tense for verbs."
"Statistical MT systems, the currently dominant research paradigm, typically do not address this issue directly."
"As a result, when evaluated for tense, current MT systems often per-form miserably."
"For example, when a simple sen-tence like “¦/he ²U/tomorrow £/return þ °/Shanghai” is given to Google’s state-of-the-art"
"Machine Translation system [Footnote_1] , it produces the out-put “He returned to Shanghai tomorrow”, instead of the correct “he will return to Shanghai tomorrow”."
The past tense on the verb “returned” contradicts the temporal expression “tomorrow”.
Determining the temporal location is also important for an Infor-mation Extraction task that extracts events so that the extracted events are put in a temporal context.
"Similarly, for Question Answering tasks, it is also important to know whether a situation has already happened or it is going to happen, for example."
"In this paper, we are interested in investigating the kind of information that is relevant in inferring the temporal location of situations in Chinese text."
We approach this problem by manually annotating each verb in a Chinese document with a “tense” tag that indicates the temporal location of the verb [Footnote_2] .
"2 For simplicity, we use the term “tense” exchangeably with the temporal location of an event or situation, even though tense usually means grammatical tense while temporal location is a more abstract semantic notion."
We then formulate the tense determination problem as a classification task where standard machine learn-ing techniques can be applied.
Figuring out what linguistic information contributes to the determina-tion of the temporal location of a situation becomes a feature engineering problem of selecting features that help with the automatic classification.
"In Sec-tion 2, we present a linguistic annotation framework that annotates the temporal location of situations in Chinese text."
In Section 3 we describe our setup for an automatic tense classification experiment and present our experimental results.
In Section 4 we focus in on the features we have used in our exper-iment and attempt to provide a quantitative as well as intuitive explanation of the contribution of the in-dividual features and speculate on what additional features could be useful.
In Section 5 we discuss related work and Section 6 concludes the paper and discusses future work.
"It is impossible to define the temporal loca-tion without a reference point, a temporal deixis."
"As we have shown in Section 1, there is con-vincing evidence from the temporal adverbials like U(“yesterday”), 8U(“today”) and ²U £“tomorrow”) that Chinese, like most if not all lan-guages of the world, use the moment of speech as this reference point."
"In written text, which is the pri-mary source of data that we are dealing with, the temporal deixis is the document creation time."
"All situations are temporally related to this document creation time except in direct quotations, where the temporal location is relative to the moment of speech of the speaker who is quoted."
"In addition to the moment of speech or document creation time in the case of written text, Reference Time and Situation Time are generally accepted as important to determining the temporal location since[REF_CITE]first proposed them."
Situation Time is the time that a situation actually occurs while Reference time is the temporal perspective from which the speaker invites his audience to con-sider the situation.
"Reference Time does not nec-essarily overlap with Situation Time, as in the case of present perfective tense, where the situation hap-pened in the past but the reader is invited to look at it from the present moment and focus on the state of completion of the situation."
Reference Time is in our judgment too subtle to be annotated consistently and thus in our annotation scheme we only consider the relation between Situation Time and the document creation time when defining the temporal location of situations.
Another key decision we made when formulating our annotation scheme is to define an abstract “tense” that do not necessarily model the ac-tual tense system in any particular language that has grammatical tense.
"In a given language, the gram-matical tense reflected in the morphological system may not have a one-to-one mapping between the grammatical tense and the temporal location of a sit-uation."
"For example, in an English sentence like “He will call me after he gets here”, while his “getting here” happens at a time in the future, it is assigned the present tense because it is in a clause introduced by “after”."
"It makes more sense to ask the annota-tor, who is necessarily a native speaker of Chinese, to make a judgment of the temporal location of the situation defined in terms of the relation between the Situation Time and the moment of speech rather than by such language-specific idiosyncracies of another language."
Temporal locations that can be defined in terms of the relation between Situation Time and the moment of speech are considered to be absolute tense.
"In some cases, the temporal location of a situation can-not be directly defined in relation to the moment of speech."
"For example in (2), the temporal location of k¿ (“intend”) cannot be determined independently of that of ß³(“reveal”)."
"The temporal location of k¿ is simultaneous with ß³. If the temporal location of ß³ is in the past, then the temporal location of k¿ is also in the past."
"If the temporal location of ß³ is in the future, then the temporal location of k¿ is also in the future."
"In this spe-cific case, the situation denoted by the matrix verb ß³ is in the past."
"Therefore the situation denoted by k¿ is also located in the past. (2) ¦ ß³ k¿ 38 c he also reveal Russia intend in next ten years S , KJø Éì . within , to Iran provide weapons . “He also revealed that Russia intended to pro-vide weapons to Iran within the next ten years.”"
"Therefore in our Chinese “tense” annotation task, we annotate both absolute and relative tenses."
"We define three absolute tenses based on whether the sit-uation time is anterior to (in the past), simultaneous with (in the present), or posterior to (in the future) document creation time."
"In addition to the absolute tenses, we also define one relative tense, future-in-past, which happens when a future situation is em-bedded in a past context."
We do not assign a tense tag to modal verbs or verb particles.
The set of tense tags are described in more detail below:
A situation is assigned the present tense if it is true at an interval of time that includes the present moment.
The present tense is compatible with states and ac-tivities.
"When non-stative situations are temporally located in the present, they either have an imperfec-tive aspect or have a habitual or frequentive reading which makes them look like states, e.g., (3) ¦ ~~ ë\ r ¹Ä &quot; he often attend outdoors activities . “He often attends outdoors activities.”"
Situations that happen before the moment of speech (or the document creation time) are temporally lo-cated in the past as in (4): (4) ¥ &lt; 9 { Chinese personnel and Chinese nationals  l  &quot; safely withdraw from Chad . “Chinese personnel and Chinese nationals safely withdrew from Chad.”
Situations that happen posterior to the moment of speech are temporally located in the future.
Future situations are not simply the opposite of past situa-tions.
"While past situations have already happened by definition, future situations by nature are charac-terized by uncertainty."
"That is, future situations may or may not happen."
"Therefore, future situations are often linked to possibilities, not just to situations that will definitely happen."
A example of future tense is given in (5): (5) ¬ 3#\· Þ1 &quot;²c conference next year in Singapore hold . “The conference will be held in Singapore next year.”
The temporal interpretation of one situation is often bound by the temporal location of another situation.
"One common scenario in which this kind of depen-dency occurs is when the target situation, the situa-tion we are interested in at the moment, is embedded in a reference situation as its complement."
"Just as the absolute “tense” represents a temporal relation be-tween the situation time and the moment of speech or document creation time, the relative “tense” rep-resents a relation between the temporal location of a situation and its reference situation."
"Although theo-retically the target situation can be anterior to, simul-taneous with, or posterior to the reference situation, we only have a special tense label when the target situation is posterior to the reference situation and the reference situation is located in the past."
In this case the label for the target situation is future-in-past as illusrated in (6): (6) úi ß³ 5(S2 6ÿÁó company personnel reveal “ Star 2 ” trial  =ò ¡­ &quot; version soon face the world . “The company personnel revealed that ‘Star 2’ trial version would soon face the world.”
Modals and verb particles do not receive a tense la-bel: (7) ¢»
Úu ÖÏ§ Kosovo independence may cause riot . éÜI&lt;  l® &quot; UN personnel already prepare withdraw . “Kosovo independence may cause riot.
UN personnel have already prepared to leave.”
"The “situations” that we are interested in are ex-pressed as clauses centered around a verb, and for the sake of convenience we mark the “tense” on the verb itself instead of the entire clause."
"How-ever, when inferring the temporal location of a sit-uation, we have to take into consideration the en-tire clause, because the arguments and modifiers of a verb are just as important as the verb itself when determining the temporal location of the situation."
"The annotation is performed on data selected from the Chinese Treebank[REF_CITE], and more detailed descriptions and justifications for the anno-tation scheme is described[REF_CITE]."
"Data selection is important for tense annotation because, unlike POS-tagging and syntactic annotation, which applies equally well to different genres of text, tem-poral annotation in more relevant in some genres than others."
The data selection task is made eas-ier by the fact that the Chinese Treebank is already annotated with POS tags and Penn Treebank-style syntactic structures.
Therefore we were able to just select articles based on how many constituents in the article are annotated with the temporal function tag -TMP.
"We have annotated 42 articles in total, and all verbs in an article are assigned one of the five tags described above: present, past, future, future-in-past, and none."
The tense determination task is then a simple five-way classification task.
Theoretically any standard machine learning algorithm can be applied to the task.
For our purposes we used the Maximum En-tropy algorithm implemented as part of the Mallet machine learning package[REF_CITE]for its competitive training time and performance tradeoff.
"There might be algorithms that could achieve higher classification accuracy, but our goal in this paper is not to pursue the absolute high performance."
"Rather, our purpose is to investigate what information when used as features is relevant to determining the tem-poral location of a situation in Chinese, so that these features can be used to design high performance practical systems in the future."
"The annotation of 42 articles yielded 5709 verb instances, each of which is annotated with one of the five tense tags."
"For our automatic classification experiments, we randomly divided the data into a training set and a test set based on a 3-to-1 ratio, so that the training data has 4,250 instances while the test set has 1459 instances."
"As expected, the past tense is the most frequent tense in both the training and test data, although they vary quite a bit in the proportions of verbs that are labeled with the past tense."
"In the training data, 2145, or 50.5% of the verb instances are labeled with the past tense while in the test data, 911 or 62.4% of the verb instances are labeled with the past tense."
This is a very high baseline given that the much smaller proportion of verbs that are assigned the past tense in the training data.
"Instead of raw text, the input to the classifica-tion algorithm is parsed sentences from the Chinese Treebank that has the syntactic structure information as well as the part-of-speech tags."
"As we will show in the next section, information extracted from the parse tree as well as the part-of-speech tags prove to be very important in determining the temporal loca-tion of a situation."
The reason for using “correct” parse trees in the Chinese Treebank is to factor out noises that are inevitable in the output of an auto-matic parser and evaluate the contribution of syntac-tic information in the “ideal” scenario.
"In a realistic setting, one of course has to use an automatic parser."
The results are presented in Table 1.
"The overall accuracy is 67.1%, exceeding the baseline of choos-ing the most frequent tense in the test, which is 62.4%."
"It is worth noting that the training accu-racy is fairly high, 93%, and there is a steep drop-off from the training accuracy to the test accuracy al-though this is hardly unexpected given the relatively small training set."
"The high training accuracy never-theless attests the relevance of the features we have chosen for the classification, which we will look at in greater detail in the next section."
"Our classification algorithm scans the verbs in a sen-tence one at a time, from left to right."
Features are extracted from the context of the verb in the parse tree as well as from previous verbs the tense of which have already been examined.
We view fea-tures for the classification algorithm as information that contributes to the determination of the temporal location of situations in the absence of morpholog-ical markers of tense.
The features we used for the classification task can all be extracted from a parse tree and the POS information of a word.
They are described below: • Verb Itself:
"The character string of the verbs, e.g., Pk(“own”), ´(“be”), etc. • Verb POS:"
"The part-of-speech tag of the verb, as defined in the Chinese Treebank."
"There are three POS tags for verbs, VE for existential verbs such as k(“have, exist”), VC for cop-ula verbs like ´(“be”), VA for stative verbs like p(“tall”), and VV for all other verbs. • Position of verb in compound: If the target verb is part of a verb compound, the position of the compound is used as a feature in com-bination with the compound type."
"The possi-ble values for the position are first and last, and the compound type is one of the six defined in the Chinese Treebank:"
"VSB, VCD, VRD, VCP, VNV, and VPT."
"An example feature might be “last+VRD”. • Governing verb and its tense: Chinese is an SVO language, and the governing verb, if there is one, is on the left and is higher up in the tree."
"Since we are scanning verbs in a sentence from left to right, the tense for the governing verb is available at the time we look at the target verb."
So we are using the character string of the gov-erning verb as well as its tense as features.
"In cases where there are multiple levels of embed-ding and multiple governing verbs, we select the closest governing verb. • Left ADV: Adverbial modifiers of the target verb are generally on the left side of the verb, therefore we are only extracting adverbs on the left."
We first locate the adverbial phrases and then find the head of the adverbial phrase and use character string of the head as feature. • Left NT: NT is a POS in the Chinese Treebank for nominal expressions that are used as tem-poral modifiers of a verb.
"The procedure for extracting the NT modifers is similar to the pro-cedure for finding adverbial modifiers, the only difference being that we are looking for NPs headed by nouns POS-tagged NT. • Left PP: Like adverbial modifiers, PP modifiers are also generally left modifiers of a verb."
"If there is a PP modifier, the character string of the head preposition combined with the char-acter string of the head noun of its NP com-plement is used as a feature, e.g., “3+Ïm” (“at+period”). • Left LC: Left localizer phrases."
Localizers phrases are also called post-positions by some and they function similarly as left PP modifiers.
"If the target verb has a left localizer phrase modifier and the character string of its head is used as a feature, e.g., ±5(“since”). • Left NN: This feature is intended to capture the head of the subject NP."
The character string of the head of the NP is used as a feature. • Aspect marker.
Aspect markers are grammati-calizations of aspect and they immediately fol-low the verb.
"If the target verb is associated with an aspect marker, the character string of that aspect marker is used as a feature, e.g., “ ”. • DER:"
"DER is the POS tag for , a charac-ter which introduces a resultative construction when following a verb."
"When it occurs together with the target verb, it is used as a feature. • Quantifier in object: When there is a quantifier in the NP object for the target verb, its character string is used as a feature. • Quotation marks: Finally the quotation marks are used as a feature when they are used to quote the clause that contains the target verb."
We performed an ablation evaluation of the fea-tures to see how effective each feature type is.
"Ba-sically, we took out each feature type, retrained the classifier and reran the classifier on the test data."
The accuracy without each of the feature types are pre-sented in Table 2.
The features are ranked from the most effective to the least effective.
Features that lead to the most drop-off when they are taken out of the classification algorithm are considered to be the most effective.
"As shown in Table 2, the most ef-fective features are the governing verb and its tense, while the least effective features are the quantifiers in the object."
Most of the features are lexicalized in that the character strings of words are used as features.
"When lexicalized features are used, fea-tures that appear in the training data do not neces-sarily appear in the test data and vice versa."
This provides a partial explanation of the large discrep-ancy between the training and test accuracy.
"In or-der to reduce this discrepancy, one would have to use a larger training set, or make the features more generalized."
Some of these features can in fact be generalized or normalized.
"For example, a temporal modifier such as the date “1987” can be reduced to something like “before the document creation time”, and this is something that we will experiment with in our future work."
"The training set used here is suffi-cient to show the efficacy of the features, but to im-prove the tense classification to a satisfactory level of accuracy, more training data need to be annotated."
"Features like adverbial, prepositional, localizer phrase modifiers and temporal noun modifiers pro-vide explicit temporal information that is relevant in determining the temporal location."
The role of the governing verb in determining the temporal location of a situation is also easy to understand.
"As we have shown in Section 2, when the target verb occurs in an embedded clause, its temporal location is necessar-ily affected by the temporal location of the govern-ing verb of this embedded clause because the tempo-ral location of the former is often defined in relation to that of the latter."
"Not surprisingly, the governing verb proves to be the most effective feature."
"Quota-tion marks in written text change the temporal deixis from the document creation time to the moment of speech of the quoted speaker, and the temporal lo-cation in quoted speech does not follow the same patterns as target verbs in embedded clauses."
"As-pect markers are tied closely to tense, even though the contributions they made are small due to their rare occurrences in text."
The relevance of other features are less obvious.
The target verb itself and its POS made the most contribution other than the governing verb.
It is im-portant to understand why they are effective or use- ful at all.
"In a theoretic work on the temporal inter-pretation of verbs in languages like Chinese which lacks tense morphology,[REF_CITE]pointed out that there is a default interpretation for bounded and unbounded situations."
"Specifically, bounded situations are temporally located in the past by default while unbounded situations are located in the future."
"The default interpretation, by defini-tion, can be overwritten when there is explicit evi-dence to the contrary."
"Recast in statistical terms, this means that bounded events have a tendency to be lo-cated in the past while unbounded events have a ten-dency to be located in the present, and this tendency can be quantified in a machine-learning framework."
"Boundedness has many surface manifestations that can be directly observed, and one of them is whether the verb is stative or dynamic."
The target verb it-self and its POS tag represents this information.
"Re-sultatives in the form of resultative verb compound and the DER construction, quantifiers in the object are other surface reflections of the abstract notion of boundedness."
The fact that these features have contributed to the determination of the temporal lo-cation of situations to certain extent lends support to Smith’s theoretical claim.
Inferring the temporal location is a difficult problem that is not yet very well understood.
It has not been studied extensively in the context of Natural Lan-guage Processing.
Olson et al (2000; 2001) realized the importance of using the aspectual information (both grammatical and lexical aspect) to infer tense in the context of a Chinese-English Machine Trans-lation system.
They encoded the aspectual informa-tion such as telicity as part of the Lexical Conceptual Structure and use it to heuristically infer tense when generating the English output.
This rule-based ap-proach is not very suited for modeling the tempo-ral location information in Chinese.
"As they them-selves noted, aspectual information can only be used as a tendency rather than a deterministic rule."
"We believe this problem can be better modeled in a ma-chine learning framework where different sources of information, each one being imperfect, can be com-bined based on their effectiveness to provide a more reasonable overall prediction."
She used Chinese-English parallel data to manually map the tense in-formation from English to Chinese and trained a Conditional Random Field classifier to make predic-tions about tense.
"She used only a limited number of surface cues such as temporal adverbials and aspect markers as features and did not attempt to model the lexical aspect information such as boundedness, which we believe would have helped her system per-formance."
Her data appeared to have a much larger percentage of verb instances that have the past tense and thus her results are mostly incomparable with that of ours.
We have defined the automatic inference of the tem-poral location of situations in Chinese text as a ma-chine learning problem and demonstrated that a lot more information in the form of features contributes to the solution of this challenging problem than pre-viously realized.
"The accuracy on the held-out test is a significant improvement over the baseline, the proportion of verbs assigned the most frequent tense (the past tense)."
"Although there is a large drop-off from the training accuracy to the test accuracy due to the lexical nature of the features, the high training accuracy does show promise that this challenging problem is solvable with a larger training set, bet-ter modeling techniques and more refined features."
In the future we will attempt to solve this problem along these lines and work toward a system that can be used in practical applications.
In this paper we present a machine learning system that finds the scope of negation in biomedical texts.
"The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of these nega-tion signals."
Our approach to negation detec-tion differs in two main aspects from existing research on negation.
"First, we focus on find-ing the scope of negation signals, instead of determining whether a term is negated or not."
"Second, we apply supervised machine learn-ing techniques, whereas most existing systems apply rule-based algorithms."
"As far as we know, this way of approaching the negation scope finding task is novel."
In this paper we present a machine learning system that finds the scope of negation in biomedical texts.
"The system consists of two classifiers, one that de-cides if the tokens in a sentence are negation sig-nals (i.e., words indicating negation), and another that finds the full scope of these negation signals."
Finding the scope of a negation signal means deter-mining at sentence level which words in the sentence are affected by the negation.
Our approach differs in two main aspects from existing research.
"First, we focus on finding the scope of negation signals, in-stead of determining whether a term is negated or not."
"Second, we apply supervised machine learn-ing techniques, whereas most existing systems apply rule-based algorithms."
"Predicting the scope of negation is important in information extraction from text for obvious rea-sons; instead of simply flagging the sentences con-taining negation as not suited for extraction (which is currently the best that can be done), correct se-mantic relations can be extracted when the scope of negation is known, providing a better recall."
Not being able to recognize negation can also hinder automated indexing systems[REF_CITE].
"They highlight the need to detect negations in examples like “no ev-idence of fracture”, so that an information retrieval system does not return irrelevant reports."
A system that does not deal with negation would treat these cases as false positives.
"The goals of this research are to model the scope finding task as a classification task similar to the se-mantic role labeling task, and to test the performance of a memory–based system that finds the scope of negation signals."
"Memory-based language process-ing (Daelemans and van den[REF_CITE]) is based on the idea that NLP problems can be solved by reuse of solved examples of the problem in mem-ory, applying similarity-based reasoning on these examples in order to solve new problems."
"As lan-guage processing tasks typically involve many sub-regularities and (pockets of) exceptions, it has been argued that lazy learning is at an advantage in solv-ing these highly disjunctive learning problems com-pared to eager learning, as the latter eliminates not only noise but also potentially useful exceptions[REF_CITE]."
"Memory-based algorithms have been successfully applied in language process-ing to a wide range of linguistic tasks, from phonol-ogy to semantic analysis, such as semantic role la-beling[REF_CITE]."
The paper is organised as follows.
"In Section 2, we summarise related work."
"In Section 3, we de-scribe the corpus with which the system has been trained."
"In Section 4, we introduce the task to be performed by the system, which is described in Sec-tion 5."
The results are presented and discussed in Section 6.
"Finally, Section 7 puts forward some con-clusions."
Negation has been a neglected area in open-domain natural language processing.
"Most research has been performed in the biomedical domain and has fo-cused on detecting if a medical term is negated or not, whereas in this paper we focus on detecting the full scope of negation signals."
The re-ported results are 94.51 precision and 77.84 recall.
"It consists of two tools: a lexi-cal scanner called lexer that uses regular expressions to generate a finite state machine, and a parser."
The reported results are 95.70 recall and 91.80 precision.
"They treat all types of negation: (i) Af-fixal negation, which is expressed by an affix. (ii) Noun phrase or emphatic negation, expressed syn-tactically by using a negative determiner (e.g. no, nothing). (iii) Inherent negation, expressed by words with an inherently negative meaning (e.g. absent). (iv) Negation with explicit negative particles (e.g. no, not)."
The texts are 50 journal articles.
"The pre- liminary results reported range from 54.32 F-score to 76.68, depending on the method applied."
"Then, a rule based system is used to decide if a concept has been positively, negatively, or uncertainly asserted."
The system achieves 97.20 recall and 98.80 precision.
The systems mentioned above are essentially based on lexical information.
Their hybrid system that combines regular expression matching with grammatical parsing achieves 92.60 recall and 99.80 precision.
"Additionally,[REF_CITE]incorporate the treatment of negation in a system, MEHR, that extracts from electronic health records all the in-formation required to generate automatically patient chronicles."
According to the authors “the nega-tion treatment module inserts markers in the text for negated phrases and determines scope of negation by using negation rules”.
"However, in the paper there is no description of the rules that are used and it is not explained how the results presented for negation recognition (57% of negations correctly recognised) are evaluated."
The above-mentioned research applies rule-based algorithms to negation finding.
Machine learning techniques have been used in some cases.
"Their corpus contains 207 selected sentences from hospital reports, in which a negation appears."
They use Naive Bayes and Decision Trees and achieve a maximum of 90 F-score.
"According to the authors, their main finding is that “when negation of a UMLS term is triggered with the negation phrase not, if the term is preceded by the then do not negate”."
They show that the regular expression-based methods have better agreement with humans and better accuracy than the classifica-tion methods.
"Like in most of the mentioned work, the task consists in determining if a medical term is negated."
The originality of the algorithm lies in that it automatically learns patterns similar to the manually written patterns for negation detection.
"Apart from work on determining whether a term is negated or not, we are not aware of research that has focused on learning the full scope of negation sig-nals inside or outside biomedical natural language processing."
The research presented in this paper pro-vides a new approach to the treatment of negation scope in natural language processing.
"The corpus used is a part of the BioScope cor-pus[REF_CITE]1 , a freely available re-source that consists of medical and biological texts."
"Every sentence is annotated with information about negation and speculation that indicates the bound-aries of the scope and the keywords, as shown in (1). (1) PMA treatment, and &lt;xcope id=“X1.4.1”&gt;&lt;cue type=“negation” ref=”X1.4.1”&gt;not&lt;cue&gt; retinoic acid treatment of the[REF_CITE]cells&lt;/xcope&gt; acts in inducing NF-KB expression in the nuclei."
"A first characteristic of the annotation of scope in the BioScope corpus is that all sentences that assert the non-existence or uncertainty of something are annotated, in contrast to other corpora where only sentences of interest in the domain are annotated."
A second characteristic is that the annotation is ex-tended to the biggest syntactic unit possible so that scopes have the maximal length.
"In (2) below, nega-tion signal no scopes over primary impairment of glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics."
The part used in our experiments are the biologi-cal paper abstracts from the GENIA corpus[REF_CITE].
"This part consists of 11,872 sentences in 1,273 abstracts."
We automatically discarded five sentences due to annotation errors.
"The total num-ber of words used is 313,222, [Footnote_1],739 of which are negation signals that belong to the different types described[REF_CITE]."
1 Web page[URL_CITE]
"We processed the texts with the GENIA tag-ger[REF_CITE], a bidirectional inference based tagger that an-alyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format 2 ."
"Additionally, we converted the annotation about scope of negation into a token-per-token representation."
Table 1 shows an example sentence of the corpus that results from converting and processing the Bio-Scope representation.
"Following the standard for-mat of the CoNLL[REF_CITE], sentences are separated by a blank line and fields are separated by a single tab character."
"A sentence consists of tokens, each one starting on a new line."
A token consists of the following 10 fields: 1.
ABSTRACT ID: number of the GENIA ab-stract. [Footnote_2].
2 The accuracy of the tagger might be inflated due to the fact that it was trained on the GENIA corpus.
"SENTENCE ID: sentence counter starting at 1 for each new abstract. 3. TOKEN ID: token counter, starting at 1 for each new sentence. 4."
FORM: word form or punctuation symbol. 5.
LEMMA: lemma of word form. 6.
Penn Treebank part-of-speech tags described[REF_CITE]. 7.
"CHUNK TAG: IOB (Inside, Outside, Begin) tags produced by the GENIA tagger that indi-cate if a token is inside a certain chunk, outside, or at the beginning. 8."
"NE TAG: IOB named entity tags produced by the GENIA tagger that indicate if a token is in- side a certain named entity, outside, or at the beginning. 9. NEG SIGNAL: tokens that are negation signals are marked as NEG."
"Negation signals in the BioScope corpus are not always single words, like the signal could not."
After the tagging pro-cess the signal cannot becomes also multiword because the tagger splits it in two words.
In these cases we assign the NEG mark to not. 10. NEG SCOPE:
"IO tags that indicate if a token is inside the negation scope (I-NEG), or out-side (O-NEG)."
These tags have been obtained by converting the xml files of BioScope.
"Each token can have one or more NEG SCOPE tags, depending on the number of negation signals in the sentence."
"We approach the scope finding task as a classifica-tion task that consists of classifying the tokens of a sentence as being a negation signal or not, and as being inside or outside the scope of the negation signal(s)."
This happens as many times as there are negation signals in the sentence.
Our conception of the task is inspired by Ramshaw and Marcus’ rep-resentation of text chunking as a tagging problem[REF_CITE].
The information that can be used to train the sys-tem appears in columns 1 to 8 of Table 1.
The infor-mation to be predicted by the system is contained in columns 9 and 10.
"As far as we know, approaching the negation scope finding task as a token per token classifica-tion task is novel, whereas at the same time it con-forms to the well established standards of the re-cent CoNLL Shared Tasks [Footnote_3] on dependency parsing[REF_CITE]and semantic role labeling[REF_CITE]."
3 Web page of CoNLL[URL_CITE]
"By setting up the task in this way we show that the nega-tion scope finding task can be modelled in a way similar to semantic role labeling, and by conform-ing to existing standards we show that learning the scope of negation can be integrated in a joint learn-ing task with dependency parsing and semantic role labeling."
"In order to solve the task, we apply supervised ma-chine learning techniques."
"We build a memory-based scope finder, that tackles the task in two phases."
"In the first phase a classifier predicts if a to-ken is a negation signal, and in the second phase an-other classifier predicts if a token is inside the scope of each of the negation signals."
"Additionally, the output of the second classifier is postprocessed with an algorithm that converts non-consecutive blocks of scope into consecutive, as explained in Section 5.3."
"As for the first and second phases, we use a memory–based classifier as implemented in TiMBL (version 6.1.2)[REF_CITE], a super-vised inductive algorithm for learning classification tasks based on the k-nearest neighbor classification rule[REF_CITE]."
Similarity is defined by computing (weighted) overlap of the feature values of a test instance and training instances.
The metric combines a per-feature value distance metric[REF_CITE]with gain ratio[REF_CITE]based global feature weights that account for relative differences in discriminative power of the features.
"In this phase, a classifier predicts whether a token is a negation signal or not."
"The memory-based classi-fier was parameterised by using overlap as the sim-ilarity metric, gain ratio for feature weighting, and using 7 k-nearest neighbors."
All neighbors have equal weight when voting for a class.
"The instances represent all tokens in the corpus and they have the following features: • Of the token: Form, lemma, part of speech, and chunk IOB tag. • Of the token context: Form, POS, and IOB tag of the three previous and three next tokens."
"In the first step of this phase, a classifier predicts whether a token is in the scope of each of the nega-tion signals of a sentence."
A pair of a negation signal and a token from the sentence represents an instance.
This means that all tokens in a sentence are paired with all negation signals that occur in the sentence.
"For example, token NF-kappa in Table 1 will be rep-resented in two instances as shown in (3)."
"An in-stance represents the pair [NF–KAPPA, absent] and another one represents the pair [NF–KAPPA, not]. (3) NF-kappa absent [features] I-NEG NF-kappa not [features] O-NEG"
Negation signals are those that have been classi-fied as such in the previous phase.
Only sentences that have negation signals are selected for this phase.
"The memory–based algorithm was parameterised in this case by using overlap as the similarity metric, gain ratio for feature weighting, using 7 k-nearest neighbors, and weighting the class vote of neighbors as a function of their inverse linear distance."
"The features of the scope finding classifier are: • Of the negation signal: Form, POS, chunk IOB tag, type of chunk (NP, VP, ...), and form, POS, chunk IOB tag, type of chunk, and named en-tity of the 3 previous and 3 next tokens. • Of the paired token: form, POS, chunk IOB tag, type of chunk, named entity, and form, POS, chunk IOB tag, type of chunk, and named entity type of the 3 previous and 3 next tokens. • Of the tokens between the negation signal and the token in focus: Chain of POS types, dis-tance in number of tokens, and chain of chunk IOB tags. • Others: A binary feature indicating whether the token and the negation signal are in the same chunk, and location of the token relative to the negation signal (pre, post, same)."
"Negation signals in the BioScope corpus always have one consecutive block of scope tokens, includ-ing the signal token itself."
"However, the scope find-ing classifier can make predictions that result in non-consecutive blocks of scope tokens: we observed that 54% of scope blocks predicted by the sys-tem given gold standard negation signals are non– consecutive."
"This is why in the second step of the scope finding phase, we apply a post-processing al-gorithm in order to increase the number of fully cor-rect scopes."
A scope is fully correct if all tokens in a sentence have been assigned their correct class label for a given negation signal.
Post-processing ensures that the resulting scope is one consecutive block of tokens.
In the BioScope corpus negation signals are inside of their scope.
The post-processing algorithm that we apply first checks if the negation signal is in its scope.
"If the signal is out, the algorithm overwrites the predicted scope in order to include the signal in its scope."
"Given the position of the signal in the sentence, the algorithm locates the starting and ending tokens of the consecutive block of predicted scope tokens that surrounds the signal."
"Other blocks of predicted scope tokens may have been predicted outside of this block, but they are separated from the current block, which contains the signal, by tokens that have been predicted not to be in the scope of the negation, as in Figure 1."
"The post-processing algorithm decides whether the detached blocks should be connected as one con-secutive block of scope tokens, or whether the de-tached block of scope tokens should be discarded from the scope."
"Dependent on this decision, ei-ther the classification of the separated blocks, or the separating non-scope tokens are considered noisy, and their classification is updated to produce one consecutive block of scope tokens for each signal."
This check is performed iteratively for all detached blocks of scope tokens.
"As in Figure 1, consider a sentence where the negation signal is in one block K of predicted scope of length k tokens and another block M of m con-secutive tokens that is predicted as scope but is sep-arated from the latter scope block by l out-of-scope tokens."
"If non-consecutive blocks are near each other, i.e., if l is sufficiently small in comparison with k and m, then the intermediate tokens that have been pre-dicted out of scope could be considered as noise and converted into scope tokens."
"In contrast, if there are too many intermediate tokens that separate the two blocks of scope tokens, then the additional block of scope is probably wrongly classified."
"Following this logic, if l &lt; α(k + m), with a specifically chosen α, the intermediate out-of-scope tokens are re-classified as scope tokens, and the separated blocks are connected to form one bigger block containing the negation signal."
"Otherwise, the loose block of scope is re-classified to be out of scope."
"When the main scope is extended, and more blocks are found that are separated from the main scope block, the algorithm reiterates this procedure until one consecutive block of scope tokens has been found."
"Our implementation first looks for separated blocks from right to left, and then from left to right."
"Dependent on whether blocks need to be added be-fore or after the main scope block, we have observed in preliminary tests that α = 0.2 for extending the main scope block from right to left, and α = 0.3 for extending the block from left to right into the sen-tence provide the best results."
Algorithm 1 details the above procedure in pseudo code.
Algorithm 1 Post-processing K ← scope block that contains signal while M ← nearest separated scope block do L ← non-scope block between K and M if |L| &lt; α(|K| + |M|) then include L in scope else exclude M from scope end if K ← scope block that contains signal end while
The results have been obtained by performing 10-fold cross validation experiments.
"The evaluation is made using the precision and recall measures[REF_CITE], and their harmonic mean, F-Measure."
We calculate micro F1.
"In the negation finding task, a negation token is correctly classified if it has been assigned a NEG class."
"In the scope finding task, a token is correctly classified if all the IO tag(s) that it has been assigned are correct."
"This means that when there is more than one negation signal in the sentence, the token has to be correctly assigned an IO tag for as many negation signals as there are."
"For example, token NF-kappa from Table 1 reproduced in (4) will not be correct if it is assigned classes I-NEG I-NEG or O-NEG I-NEG. (4) 10415075 07 1 NF-kappa NF-kappa NN B-NP B-protein I-NEG O-NEG"
"Additionally, we evaluated the percentage of fully correct scopes (PCS)."
We calculate two baselines for negation signal find-ing.
"Baseline 1 (B1) is calculated by assigning the NEG class to all the tokens that had no or not as lemma, which account for 72.80% of the negation signals."
The F1 of the baseline is 80.66.
"Baseline 2 (B2) is calculated by assigning the NEG class to all the tokens that had no, not, lack, neither, unable, without, fail, absence, or nor as lemma."
These lem-mas account for 85.85 % of the negation signals.
Table 3 shows the overall results of the negation signal finding system and the results per negation signal.
Precision and recall are very simi-lar.
Scores show a clear unbalance between different negation signals.
Those with the lowest frequencies get lower scores than those with the highest frequen-cies.
"Probably, this could be avoided by training the system with a bigger corpus."
"However, a bigger corpus would not help solve all the errors because some of them are caused by in-consistency in the annotation."
"For example, absence is annotated as a negation signal in 57 cases, whereas in 22 cases it is not annotated as such, although in all cases it is used as a negation signal."
"Example 5 (a) shows one of the 22 cases of absence that has not been annotated, and Example 5 (b) shows one of the 57 cases of absence annotated as a negation signal."
"Also fail is not annotated as a negation signal in 13 cases where it should. (5) (a) Retroviral induction of TIMP-1 not only resulted in cell survival but also in continued DNA synthesis for up to 5 d in the absence of serum, while controls underwent apoptosis. (b) A significant proportion of transcripts appear to terminate prematurely in the &lt;xcope id=[REF_CITE].8.1 &gt;&lt;cue type= negation ref=[REF_CITE].8.1 &gt; absence &lt;/cue&gt; of transactivators &lt;/xcope&gt;."
Other negation signals are arbitrarily annotated.
"Failure is annotated as a negation signal in 8 cases where it is followed by a preposition, like in Exam-ple 6 (a), and it is not annotated as such in 26 cases, like Example 6 (b), where it is modified by an adjec-tive."
"The errors in detecting with as a negation signal are caused by the fact that it is embedded in the ex-pression with the exception of, which occurs 6 times in contrast with the 5265 occurrences of with."
"Could appears as a negation signal because the tagger does not assign to it the lemma can, but could, causing the wrong assignment of the tag NEG to not, instead of could when the negation cue in BioScope is could not."
We provide the results of the classifier and the re-sults of applying the postprocessing algorithm to the output of the classifier.
"Table 4 shows results for two versions of the scope finding classifier, one based on gold standard negation signals (GS NEG), and another (PR NEG) based on negation signals predicted by the classifier described in the previous section."
"The F1 of PR NEG is 7.18 points lower than the F1 of GS NEG, which is an expected effect due to the performance of classifier that finds negation sig-nals."
"Precision and recall of GS NEG are very bal-anced, whereas PR NEG has a lower recall than pre-cision."
"These measures are the result of a token per token evaluation, which does not guarantee that the complete sequence of scope is correct."
This is re-flected in the low percentage of fully correct scopes of both versions of the classifier.
"In Table 5, we present the results of the system af-ter applying the postprocessing algorithm."
"The most remarkable result is the 29.60 and 21.58 error reduc-tion in the percentage of fully correct scopes of GS NEG and PR NEG respectively, which shows that the algorithm is efficient."
Also interesting is the in-crease in F1 of GS NEG and PR NEG.
Table 6 shows detailed results of the system based on predicted negation signals after applying the postprocessing algorithm.
Classes O-NEG and I-NEG are among the most frequent and get high scores.
Classes composed only of O-NEG tags are easier to predict.
Table 7 shows information about the percentage of correct scopes per negation signal after applying the algorithm to PR-NEG.
A clear example of an incorrect prediction is the occurrence of box in the list.
"The signal with the highest percentage of PCS is without, followed by no (determiner), rather and not, which are above 50%."
"It would be interesting to investigate how the syntactic properties of the nega-tion signals are related to the percentage of correct scopes, and how does the algorithm perform depend-ing on the type of signal."
"Given the fact that a significant portion of biomed-ical text is negated, recognising negated instances is important in NLP applications."
In this paper we have presented a machine learning system that finds the scope of negation in biomedical texts.
"The sys-tem consists of two memory-based classifiers, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of the negation signals."
"The first classifier achieves 94.40 F1, and the sec-ond 80.99."
"However, the evaluation in terms of cor-rect scopes shows the weakness of the system."
This is why a postprocessing algorithm is applied.
"The algorithm achieves an error reduction of 21.58, with 50.05 % of fully correct scopes in the system based on predicted negation signals."
"These results suggest that unsupervised machine learning algorithms are suited for tackling the task, as it was expected from results obtained in other natural language processing tasks."
"However, results also suggest that there is room for improvement."
"A first improvement would consist in predicting the scope chunk per chunk instead of token per token, because most negation scope boundaries coincide with boundaries of chunks."
"We have highlighted the fact that our approach to negation detection focuses on finding the scope of negation signals, instead of determining whether a term is negated or not, and on applying super-vised machine learning techniques."
"As far as we know, this approach is novel."
"Unfortunately, there are no previous comparable approaches to measure the quality of our results."
"Additionally, we have shown that negation find-ing can be modelled as a classification task in a way similar to other linguistic tasks like semantic role la-beling."
"In our model, tokens of a sentence are clas-sified as being a negation signal or not, and as being inside or outside the scope of the negation signal(s)."
This representation would allow to integrate the task with other semantic tasks and exploring the interac-tion between different types of knowledge in a joint learning setting.
Further research is possible in several directions.
"In the first place, other machine learning algorithms could be integrated in the system in order to opti-mise performance."
"Secondly, the system should be tested in different types of biomedical texts, like full papers or medical reports to check its robustness."
"Finally, the postprocessing algorithm could be im-proved by using more sophisticated sequence classi-fication techniques[REF_CITE]."
Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
"To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations."
The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.
"Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder."
"In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice."
"Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes."
The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.
Experi-ments conducted on the[REF_CITE]translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.
Many statistical methods in natural language pro-cessing aim at minimizing the probability of sen-tence errors.
"In practice, however, system quality is often measured based on error metrics that assign non-uniform costs to classification errors and thus go far beyond counting the number of wrong de-cisions."
"Examples are the mean average precision for ranked retrieval, the F-measure for parsing, and the BLEU score for statistical machine transla-tion (SMT)."
A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training (MERT) and has been suggested for SMT[REF_CITE].
MERT aims at estimating the model parameters such that the decision under the zero-one loss func-tion maximizes some end-to-end performance mea-sure on a development corpus.
"In combination with log-linear models, the training procedure allows for a direct optimization of the unsmoothed error count."
"The criterion can be derived from Bayes’ decision rule as follows: Let f f 1 , ..., f J denote a source sentence (’French’) which is to be translated into a target sentence (’English’) e e 1 ,..., e I ."
"Under the zero-one loss function, the translation which maximizes the a posteriori probability is chosen: arg max Pr p e | f q ( (1) ê e"
"Since the true posterior distribution is unknown, Pr p e | f q is modeled via a log-linear translation model which combines some feature functions h m p e,f q with feature function weights λ m , m 1, ..., M:"
"Pr p e | f q p λ M p e | f q 1 ° M λ h p e, f q exp ° mM1 m m m 1 λ m h m p e 1 ,f q ° (2) e 1 exp"
"The feature function weights are the parameters of the model, and the objective of the MERT criterion is to find a parameter set λ M1 that minimizes the error count on a representative set of training sentences."
"More precisely, let f 1S denote the source sentences of a training corpus with given reference translations r S , and let C s t e s,1 , ..., e s,K u denote a set of K 1 candidate translations."
"Assuming that the corpus-based error count for some translations e S1 is addi-vidual sentences, i.e., E p r S , e S q tively decomposable into the error counts °"
"Ss of the indi- 1 E p r s , e s q , 1 1 the MERT criterion is given as: # +"
"Ş λ̂ M1 E r s , ê p f s ; λ M q arg min (3) 1 λ M1 s 1 # + Ş Ķ E p r s , e s,k q δ ê p f s ; λ M q , e s, k arg min 1 λ M1 s 1k 1 with # + M̧ ê p f s ; λ M q λ m h m p e, f s q 1 arg max (4) e m 1"
"In[REF_CITE], it was shown that linear models can effectively be trained under the MERT criterion us-ing a special line optimization algorithm."
This line optimization determines for each feature function h m and sentence f s the exact error surface on a set of candidate translations C s .
The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.
Candidate translations in MERT are typically rep-resented as N-best lists which contain the N most probable translation hypotheses.
"A downside of this approach is, however, that N-best lists can only capture a very small fraction of the search space."
"As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on C s and preventing the optimization procedure from stopping in a poor local optimum."
"In this paper, we present a novel algorithm that allows for efficiently constructing and representing the unsmoothed error surface for all translations that are encoded in a phrase lattice."
The number of candidate translations thus taken into account increases by several orders of magnitudes compared to N-best MERT.
Lattice MERT is shown to yield significantly faster convergence rates while it ex-plores a much larger space of candidate translations which is exponential in the lattice size.
"Despite this vast search space, we show that the suggested algorithm is always efficient in both running time and memory."
The remainder of this paper is organized as fol-lows.
Section 2 briefly reviews N-best MERT and introduces some basic concepts that are used in order to develop the line optimization algorithm for phrase lattices in Section 3.
Section 4 presents an upper bound on the complexity of the unsmoothed error surface for the translation hypotheses repre-sented in a phrase lattice.
This upper bound is used to prove the space and runtime efficiency of the suggested algorithm.
Section 5 lists some best practices for MERT.
Section 6 discusses related work.
Section 7 reports on experiments conducted on the[REF_CITE]translation tasks.
The paper concludes with a summary in Section 8.
The goal of MERT is to find a weights set that minimizes the unsmoothed error count on a rep-resentative training corpus (cf. Eq. (3)).
This can be accomplished through a sequence of line minimizations along some vector directions t d M u . 1
"Starting from an initial point λ 1M , computing the most probable sentence hypothesis out of a set of K candidate translations C s t e 1 , ..., e K u along the line λ M1 γ d M1 results in the following optimiza-tion problem[REF_CITE]: ! ê p f s ; γ q arg max p λ M γ d M1 q J h M p e, f s q ) 1 1 e P C s &quot;¸ * λ m h m p e, f s q γ ¸ d m h m p e, f s q arg max e P C s loooooooomoooooooon m loooooooomoooooooon m a p e,f s q b p e,f s q arg max a looooooooooomooooooooooon p e, f s q γ b p e, f s q ( (5) e P C s"
"Hence, the total score  for any candidate trans-lation corresponds to a line in the plane with γ as the independent variable."
"For any particular choice of γ, the decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment."
"Overall, the candidate repository C s defines K lines where each line may be divided into at most K line segments due to possible intersections with the other K 1 lines."
The sequence of the topmost line segments constitute the upper envelope which is the pointwise maximum over all lines induced by C s .
The upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in γ[REF_CITE]:
"Env p f q max a p e, f q γ b p e, f q : γ P R ( (6) e P C"
The importance of the upper envelope is that it pro-vides a compact encoding of all possible outcomes that a rescoring of C s may yield if the parameter set λ M1 is moved along the chosen direction.
"Once the upper envelope has been determined, we can project its constituent line segments onto the error counts of the corresponding candidate translations (cf. Figure 1)."
This projection is independent of how the envelope is generated and can therefore be applied to any set of line segments 1 .
An effective means to compute the upper enve-lope is a sweep line algorithm which is often used in computational geometry to determine the intersec-tion points of a sequence of lines or line segments[REF_CITE].
The idea is to shift (“sweep”) a vertical ray from  to 8 over the plane while keeping track of those points where two or more lines intersect.
"Since the upper envelope is fully specified by the topmost line segments, it suffices to store the following components for each line object ℓ: the x-intercept ℓ.x with the left-adjacent line, the slope ℓ.m, and the y-intercept ℓ.y; a fourth component, ℓ.t, is used to store the candi-date translation."
Algorithm 1 shows the pseudo code for a sweep line algorithm which reduces an input array a[0..K-1] consisting of the K line objects of the candidate repository C s to its upper envelope.
"By construction, the upper envelope consists of at most K line segments."
The endpoints of each line } a.resize(j) ; return a; segment define the interval boundaries at which the decision made by the decoder will change.
"Hence, as γ increases from  to 8 , we will see that the most probable translation hypothesis will change whenever γ passes an intersection point."
"Let γ f1 s γ f2 s f s ... γ N s denote the sequence of interval boundaries and let ∆E f1 s , ∆E f2 s , ..., ∆E fN ss denote the corresponding sequence of changes in the error count where ∆E nf s is the amount by which the error count will change if γ is moved from a point in r γ n , γ q to a point in r γ n , γ n q ."
Both sequences f s f s f s f s 1 n 1 together provide an exhaustive representation of the unsmoothed error surface for the sentence f s along the line λ M1 γ d M1 .
The error surface for the whole training corpus is obtained by merging the interval boundaries (and their corresponding error counts) over all sentences in the training corpus.
The optimal γ can then be found by traversing the merged error surface and choosing a point from the interval where the total error reaches its minimum.
"After the parameter update, λ̂ M1 λ M1 γ opt d M1 , the decoder may find new translation hypotheses which are merged into the candidate repositories if they are ranked among the top N candidates."
The relation K N holds therefore only in the first iteration.
"From the second iteration on, K is usually larger than N. The sequence of line optimizations and decodings is repeated until ([Footnote_1]) the candidate repositories remain unchanged and (2) γ opt 0."
"1 For lattice MERT, it will therefore suffice to find an efficient way to compute the upper envelope over all translations that are encoded in a phrase graph."
"In this section, the algorithm for computing the upper envelope on N-best lists is extended to phrase lattices."
"For a description on how to generate lattices, see[REF_CITE]."
"Formally, a phrase lattice for a source sentence f is defined as a connected, directed acyclic graph G f p V f , E f q with vertice set V f , unique source and sink nodes s, t P V f , and a set of arcs E f  V V f . f"
"Each arc is labeled with a phrase ϕ ij e i 1 , ..., e i j and the (local) feature function values h M1 p ϕ ij ,f q ."
"A path π p v 0 , ε 0 , v 1 , ε 1 , ..., ε n 1 , v n q in G f (with ε i P E f and v i ,v i 1"
"P V f as the tail and head of ε i , 0 ¤ i n) defines a partial translation e π of f which is the concatenation of all phrases along this path."
"The corresponding feature function values are obtained by summing over the arc-specific feature function values: π : ÝÝÝÝÝÝÑ ÝÝÝÝÝÝÑ ÝÝÝÝÝÝÝÝÑ ϕ ϕϕ n 1,n0,1 1,2 v 0 h M1 p ϕ ,f q v 1 h M1 p ϕ ,f q h M1 p ϕ ,f q v n 0,1 1,2 n 1,n e π ϕ ϕ ... ϕ 0,1 n 1,n v i Ñ v j P π ij ¸ i,j : h 1M p e π ,f q h M1 p ϕ ,f q ij v i Ñ v j P π i,j :"
"In the following, we use the notation in p v q and out p v q to refer to the set of incoming and outgoing arcs for a node v P V f ."
"Similarly, head p ε q and tail p ε q denote the head and tail of ε P E f ."
"To develop the algorithm for computing the up-per envelope of all translation hypotheses that are encoded in a phrase lattice, we first consider a node v P V f with some incoming and outgoing arcs: v 1 ε v"
Each path that starts at the source node s and ends in v defines a partial translation hypothesis which can be represented as a line (cf. Eq. (5)).
We now assume that the upper envelope for these partial translation hypotheses is known.
"The lines that constitute this envelope shall be denoted by f 1 , ..., f N ."
Next we consider continuations of these partial translation candidates by following one of the outgoing arcs ε P out p v q .
Each such arc defines another line denoted by g p ε q .
"If we add the slope and y-intercept of g p ε q to each line in the set t f 1 , ..., f N u , then the upper envelope will be constituted by segments of f 1 g p ε q , ..., f N g p ε q ."
"This operation neither changes the number of line segments nor their rela-tive order in the envelope, and therefore it preserves the structure of the convex hull."
"As a consequence, we can propagate the resulting envelope over an Other incoming arcs for v 1 may be associated head with p ε q . outgoing arc ε to a successor node v 1 different upper envelopes, and all that remains is to merge these envelopes into a single combined envelope."
"This is, however, easy to accomplish since the combined envelope is simply the convex hull of the union over the line sets which constitute the individual envelopes."
"Thus, by merging the arrays that store the line segments for the incoming arcs and applying Algorithm 1 to the resulting array we obtain the combined upper envelope for all partial translation candidates that are associated with paths starting at the source node s and ending in v 1 ."
The correctness of this procedure is based on the following two observations: (1) A single translation hypothesis cannot consti-tute multiple line segments of the same envelope.
"This is because translations associated with different line segments are path-disjoint. (2) Once a partial translation has been discarded from an envelope because its associated line f˜ is completely covered by the topmost line segments of the convex hull, there is no path continuation that could bring back f˜ into the upper envelope again."
"Proof: Suppose that such a continuation exists, then this continuation can be represented as a line g, and since f˜ has been discarded from the envelope, the path associated with g must also be a valid continuation for the line segments f 1 ,...,f N that constitute the envelope."
"Thus it follows that max p f g q max p f 1 , ..., f N q g 1 g, ..., f N f˜ g for some γ P R. This, however, is in contra-diction with the premise that f˜ max p f 1 , ..., f N q for all γ P R."
"To keep track of the phrase expansions when propagating an envelope over an outgoing arc ε P tail p v q , the phrase label ϕ v, head p ε q has to be appended from the right to all partial translation hypotheses in the envelope."
The complete algorithm then works as follows:
"First, all nodes in the phrase lattice are sorted in topological order."
"Starting with the source node, we combine for each node v the upper envelopes that are associated with v’s incoming arcs by merging their respective line arrays and reducing the merged array into a combined upper envelope using Algorithm 1."
The combined envelope is then propagated over the outgoing arcs by associating each ε P out p v q with a copy of the combined envelope.
This copy is modified by adding the parameters (slope and y-intercept) of the line g p ε q to the envelope’s constituent line segments.
The envelopes of the incoming arcs are no longer needed and can be deleted in order to release memory.
"The envelope computed at the sink node is by construc-tion the convex hull over all translation hypotheses represented in the lattice, and it compactly encodes those candidates which maximize the decision rule Eq. (1) for any point along the line λ 1M γ d M1 ."
Algorithm 2 shows the pseudo code.
Note that the component ℓ.x does not change and therefore requires no update.
It remains to verify that the suggested algorithm is efficient in both running time and memory.
"For this purpose, we first analyze the complexity of Algorithm 1 and derive from it the running time of Algorithm 2."
"After sorting, each line object in Algorithm 1 is visited at most three times."
The first time is when it is picked by the outer loop.
The second time is when it either gets discarded or when it terminates the inner loop.
"Whenever a line object is visited for the third time, it is irrevocably removed from the envelope."
The runtime complexity is therefore dominated by the initial sorting and amounts to O p K log K q can be performed in time Θ p| V | | E |q .
As will p V
"Topological sort on a phrase lattice G ,E q be shown in Section 4, the size of the upper envelope for G can never exceed the size of the arc set E."
"The same holds for any subgraph G r s,v s of G which is induced by the paths that connect the source node s with v P V. Since the envelopes propagated from the source to the sink node can only increase linearly in the number of previously processed arcs, the total running time amounts to a worst case complexity of O p| V | | E | log | E |q ."
The memory efficiency of the suggested algorithm results from the following theorem which provides a novel upper bound for the number of cost mini-mizing paths in a directed acyclic graph with arc-specific affine cost functions.
"The bound is not only meaningful for proving the space efficiency of lattice MERT, but it also provides deeper insight into the structure and complexity of the unsmoothed error surface induced by log-linear models."
"Since we are examining a special class of shortest paths problems, we will invert the sign of each local feature function value in order to turn the feature scores into cor-responding costs."
"Hence, the objective of finding the best translation hypotheses in a phrase lattice becomes the problem of finding all cost-minimizing paths in a graph with affine cost functions."
"Theorem: Let G p V, E q be a connected directed acyclic graph with vertex set V, unique source and sink nodes s,t P V, and an arc set E  V which each arc ε P E is associated with an affineV in cost function c ε p γ q b ε , a ε ,b ε P R.a ε γ Counting ties only once, the cardinality of the union over the sets of all cost-minimizing paths for all γ P R is then upper-bounded by | E | : π p G; γ q is a cost-minimizing ¤ π : π γ P R path in G given γ ( ¤ | E | (7)"
"Proof: The proposition holds for the empty graph arcs ε P E joining the source and sink t as well as for the case that V s,t u with all node."
Let G therefore be a larger graph.
Then we perform an s-t cut and split G into two subgraphs G 1 (left subgraph) and G 2 (right subgraph).
Arcs spanning the section boundary are duplicated (with the costs of the copied arcs in G 2 being set to zero) and connected with a newly added head or tail node:
G: c 1 G 1 : c 1 G 2 : 0 c 3 c 3 c 2 c 2 c 4 c 4
"The zero-cost arcs in G 2 that emerged from the duplication process are contracted, which can be done without loss of generality because zero-cost arcs do not affect the total costs of paths in the lattice."
The contraction essentially amounts to a removal of arcs and is required in order to ensure that the sum of edges in both subgraphs does not exceed the number of edges in G. All nodes in G 1 with out-degree zero are then combined into a single sink node t 1 .
"Similarly, nodes in G 2 whose in-degree is zero are combined into a single source node s 2 ."
Let N 1 and N 2 denote the number of arcs in G 1 and | GE | .
"Both subgraphs are smaller 2 , respectively."
"By construction, N 1 N 2 than G and thus, due to the induction hypothesis, their lower envelopes consist of at most N 1 and N 2 line segments, respectively."
"We further notice that either envelope is a convex hull whose constituent line segments inscribe a convex polygon, in the following denoted by P 1 and P 2 ."
"Now, we combine both subgraphs into a single graph G 1 by merging the sink node t 1 in G 1 with the source node s 2 in G 2 ."
"The merged node is an articulation point whose removal would disconnect both subgraphs, and hence, all paths in G 1 that start at the source node s and stop in the sink node t lead through this articulation point."
"The graph G 1 has at least as many cost minimizing paths as G, although these paths as well as their associated costs might be different from those in G."
The additivity of the cost function and the articulation point allow us to split the costs for any path from s to t into two portions: the first portion can be attributed to G 1 and must be a line inside P 1 ; the remainder can be attributed to G 2 and must therefore be a line inside P 2 .
"Hence, the total costs for any path in G 1 can be bounded by the convex hull of the superposition of P 1 and P 2 ."
"This convex hull is again a convex polygon which consists of at most N 1 N 2 edges, and therefore, also in G) is upper bounded by N 1 N 2 .the number of cost minimizing paths in G 1 (and thus l Corollary: The upper envelope for a phrase lattice G f p V f , E f q consists of at most | E | f line segments."
This bound can even be refined and one obtains (proof omitted) | E | | V | 2.
Both bounds are tight.
"This result may seem somewhat surprising as it states that, independent of the choice of the direction along which the line optimization is performed, the structure of the error surface is far less complex than one might expect based on the huge number of alternative translation candidates that are rep-resented in the lattice and thus contribute to the error surface."
"In fact, this result is a consequence of using a log-linear model which constrains how costs (or scores, respectively) can evolve due to hypothesis expansion."
"If instead quadratic cost functions were used, the size of the envelopes could not be limited in the same way."
"The above theorem does not, however, provide any additional guidance that would help to choose more promising directions in the line optimization algorithm to find better local optima."
"To alleviate this problem, the following section lists some best practices that we found to be useful in the context of MERT."
"This section addresses some techniques that we found to be beneficial in order to improve the performance of MERT. (1) Random Starting Points: To prevent the line optimization algorithm from stopping in a poor local optimum, MERT explores additional starting points that are randomly chosen by sampling the parameter space. (2) Constrained Optimization: This technique allows for limiting the range of some or all feature function weights by defining weights restrictions."
"The weight restriction for a feature function h m is R  , 8u which defines the r specified as an interval R m l m ,r m s , l m ,r m P admissible region from which the feature function weight λ m can be chosen."
"If the line optimization is performed under the presence of weights restrictions, γ needs to be chosen such that the following constraint holds: l M1 ¤ λ M γ d M1 ¤ r M (8) 1 1 (3) Weight Priors: Weight priors give a small (pos-itive or negative) boost ω on the objective function if the new weight is chosen such that it matches a certain target value λ m : ! ¸"
"E r s , ê p f s ; γ q γ opt arg min γ ¸ s δ p λ m γ d m , λ m  ω ) (9) m"
A zero-weights prior (λ m 0) provides a means of doing feature selection since the weight of a feature function which is not discriminative will be set to zero.
An initial-weights prior (λ m λ m ) can be used to confine changes in the parameter update with the consequence that the new parameter may be closer to the initial weights set.
"Initial weights priors are useful in cases where the starting weights already yield a decent baseline. count ∆E i 1 i 1 has a larger range, and the choice of γ opt may be more reliable. (5) Random Directions: If the directions chosen in the line optimization algorithm are the coordinate axes of the M-dimensional parameter space, each iteration will result in the update of a single feature function only."
"While this update scheme provides a ranking of the feature functions according to their discriminative power (each iteration picks the fea-ture function for which changing the corresponding weight yields the highest gain), it does not take possible correlations between the feature functions into account."
"As a consequence, the optimization procedure may stop in a poor local optimum."
"On the other hand, it is difficult to compute a direction that decorrelates two or more correlated feature functions."
This problem can be alleviated by ex-ploring a large number of random directions which update many feature weights simultaneously.
The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M-dimensional hypersphere with the hypersphere’s center.
The center of the hypersphere is defined as the initial parameter set.
"As suggested[REF_CITE], an alternative method for the optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization ([REF_CITE]p. 509)."
"In[REF_CITE], the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm ([REF_CITE]p. 503)."
"The optimization proce-dure allows for optimizing other objective function as, e.g., the expected BLEU score."
"A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions."
A different approach to minimize the expected BLEU score is suggested[REF_CITE]who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more com-plex risk surface.
"A large variety of different search strategies for MERT are investigated[REF_CITE], which provides many fruitful insights into the optimization process."
"In[REF_CITE], MERT is used to boost the BLEU score on"
N-best re-ranking tasks.
The incorporation of a large number of sparse feature functions is described[REF_CITE].
The paper investigates a perceptron-like online large-margin training for sta-tistical machine translation.
The described approach is reported to yield significant improvements on top of a baseline system which employs a small number of feature functions whose weights are optimized under the MERT criterion.
"A study which is comple-mentary to the upper bound on the size of envelopes derived in Section 4 is provided[REF_CITE]which shows that the number of inference functions of any graphical model as, for instance, Bayesian networks and Markov random fields is polynomial in the size of the model if the number of parameters is fixed."
"Experiments were conducted on the[REF_CITE]translation tasks under the conditions of the con-strained data track for the language pairs Arabic-to-English (aren), English-to-Chinese (enzh), and Chinese-to-English (zhen)."
The development cor-pora were compiled from test data used in the 2002 and 2004 NIST evaluations.
Each corpus set provides 4 reference translations per source sen-tence.
Table 1 summarizes some corpus statistics.
Translation results were evaluated using the mixed-case BLEU score metric in the implementation as suggested[REF_CITE].
"Translation results were produced with a state-of-the-art phrase-based SMT system which uses EM-trained word alignment models (IBM1, HMM) and a 5-gram language model built from the Web-1T collection 2 ."
Translation hypotheses produced on the blind test data were reranked using the Minimum-Bayes Risk (MBR) decision rule[REF_CITE].
Each system uses a log-linear combination of 20 to 30 feature functions.
"In a first experiment, we investigated the conver-gence speed of lattice MERT and N-best MERT."
Figure 2 shows the evolution of the BLEU score in the course of the iteration index on the zhen-dev1 corpus for either method.
"In each iteration, the training procedure translates the development corpus using the most recent weights set and merges the top ranked candidate translations (either repre-sented as phrase lattices or N-best lists) into the candidate repositories before the line optimization is performed."
"For N-best MERT, we used N 50 which yielded the best results."
"In contrast to lattice MERT, N-best MERT optimizes all dimensions in each iteration and, in addition, it also explores a large number of random starting points before it re-decodes and expands the hypothesis set."
"As is typical for N-best MERT, the first iteration causes a dramatic performance loss caused by overadapting the candidate repositories, which amounts to more than 27.3 BLEU points."
"Although this performance loss is recouped after the 5th iteration, the initial decline makes the line optimization under N-best MERT more fragile since the optimum found at the end of the training procedure is affected by the initial performance drop rather than by the choice of the initial start weights."
Lattice MERT on the other hand results in a significantly faster convergence speed and reaches its optimum already in the 5th iteration.
"For lattice MERT, we used a graph density of 40 arcs per phrase which corresponds to an N-best size of more than two octillion p 2 10 27 q entries."
"This huge number of alternative candidate translations makes updating the weights under lattice MERT more reliable and robust and, compared to N-best MERT, it becomes less likely that the same feature weight needs to be picked again and adjusted in subsequent iterations."
Figure 4 shows the evolution of the BLEU score on the zhen-dev1 corpus using # random dev1+dev[Footnote_2] blind task directions 0-1 MBR 0-1 MBR aren – 57.4 57.4 43.7 43.9 1000 57.6 57.7 43.9 44.5 zhen – 39.6 39.7 27.6 28.2 500 39.5 39.9 27.9 28.3 lattice MERT with 5 weights updates per iteration.
The performance drop in iteration 1 is also attributed to overfitting the candidate repository.
"The decline of less than 0.5% in terms of BLEU is, however, almost negligible compared to the performance drop of more than 27% in case of N-best MERT."
"The vast number of alternative translation hypotheses represented in a lattice also increases the number of phase transitions in the error surface, and thus prevents MERT from selecting a low performing feature weights set at early stages in the optimization procedure."
"This is illustrated in Figure 3, where lattice MERT and N-best MERT find different op-tima for the weight of the phrase penalty feature function after the first iteration."
Table 2 shows the BLEU score results on the[REF_CITE]blind test using the combined dev1+dev2 corpus as training data.
"While only the aren task shows improvements on the development data, lattice MERT provides consistent gains over N-best MERT on all three blind test sets."
The reduced performance for N-best MERT is a consequence of the performance drop in the first iteration which causes the final weights to be far off from the initial parameter set.
This can impair the ability of N-best MERT to generalize to unseen data if the initial weights are already capable of producing a decent baseline.
Lattice MERT on the other hand can produce weights sets which are closer to the initial weights and thus more likely to retain the ability to generalize to unseen data.
It could therefore be worthwhile to investigate whether a more elaborated version of an initial-weights prior allows for alleviating this effect in case of N-best MERT.
Table 3 shows the effect of optimizing the feature function weights along some randomly chosen directions in addition to the coordinate axes.
The different local optima found on the development set by using random directions result in additional gains on the blind test sets and range from 0.1% to 0.6% absolute in terms of BLEU.
We presented a novel algorithm that allows for efficiently constructing and representing the un-smoothed error surface over all sentence hypotheses that are represented in a phrase lattice.
The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical ma-chine translation system under the Minimum Error Rate Training (MERT) criterion.
"Lattice MERT was shown analytically and experimentally to be supe-rior over N-best MERT, resulting in significantly faster convergence speed and a reduced number of decoding steps."
"While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too."
One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on con-fusion networks[REF_CITE].
"It is also straightforward to extend the suggested method to hypergraphs and forests as they are used, e.g., in hierarchical and syntax-augmented systems[REF_CITE]."
Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.
"An important problem in translation neglected by most recent statistical machine translation systems is insertion and deletion of words, such as function words, motivated by linguistic structure rather than adjacent lexical context."
Phrasal and hierarchical systems can only insert or delete words in the context of a larger phrase or rule.
"While this may suffice when translating in-domain, it performs poorly when trying to translate broad domains such as web text."
Various syntactic approaches have been proposed that begin to address this problem by learning lexicalized and unlexicalized rules.
"Among these, the treelet approach uses unlexicalized order templates to model ordering separately from lexical choice."
"We introduce an extension to the latter that allows for structural word insertion and deletion, without requiring a lexical anchor, and show that it produces gains of more than 1.0% BLEU over both phrasal and baseline treelet systems on broad domain text."
"Among the phenomena that are modeled poorly by modern SMT systems is the insertion and deletion of words, such as function words, that are motivated by the divergent linguistic structure between source and target language."
"To take the simplest of examples, the English noun compound “file name” would typically be translated into Spanish as “nombre de archivo”, which requires the insertion of the preposition “de”."
"Conversely, when translating from Spanish to English, the “de” must be deleted."
"At first glance, the problem may seem trivial, yet the presence and position of these function words can have crucial impact on the adequacy and fluency of translation."
"In particular, function words are often used to denote key semantic information."
"They may be used to denote case information, in languages such as Japanese."
Failing to insert the proper case marker may render a sentence unreadable or significantly change its meaning.
Learning these operations can be tricky for MT models best suited to contiguous word sequences.
"From a fluency standpoint, proper insertion of determiners and prepositions can often make the difference between laughably awkward output and natural sounding translations; consider the output “it’s a cake piece” as opposed to “it’s a piece of cake”."
"Furthermore, since missing or spurious function words can confuse the target language model, handling these words properly can have an impact beyond the words themselves."
"This paper focuses on methods of inserting and deleting words based on syntactic cues, to be used in the context of a syntax-informed translation system."
"While the models we build are relatively simple and the underlying templates are easy to extract, they add significant generalization ability to the base translation system, and result in significant gains."
"As a motivating example, let us return to the English/Spanish pair “file name” and “nombre de archivo”."
"In principle, we would want a machine translation system to be capable of learning the following general transformation: “ NOUN 1 NOUN 2 ” ՜ “ NOUN 2 de NOUN 1 ” (1)"
Yet even this simple example is beyond the capabilities of many common approaches.
"The heavily lexicalized approaches of phrasal systems[REF_CITE], are inherently incapable of this generalization."
"As a proxy, they acquire phrase pairs such as “nombre de archivo” ՜ “file name”, “nombre de” ՜ “name” and “de archivo” ՜ “file”."
Note that the inserted word is attached to adjacent context word(s).
"When the test set vocabulary has significant overlap with the training vocabulary, the correct translation can often be assembled based on the head or the modifying noun."
"However, as we show in this paper, this is woefully inadequate when translating truly out-of-domain input."
"In principle, phrase-based translation systems may employ insertion phrase pairs such as “[ NULL ]” ՜ “de” (2) but the ungrounded nature of this transformation makes its use during decoding difficult."
"Since there are no constraints on where such a rule may apply and the rule does not consume any input words, the decoder must attempt these rules at every point in the search."
"The reverse operation “de” ՜ “[ NULL ]” (3) is more feasible to implement, though again, there is great ambiguity – a source word may be deleted at any point during the search, with identical target results."
Few systems allow this operation in practice.
Estimating the likelihood of this operation and correctly identifying the contexts in which it should occur remain challenging problems.
"Hierarchical systems, such[REF_CITE]in principle have the capacity to learn insertions and deletions grounded by minimal lexical cues."
"However, the extracted rules use a single non-terminal."
"Hence, to avoid explosive ambiguity, they are constrained to contain at least one aligned pair of words."
This restriction successfully limits computational complexity at a cost of generalization power.
Syntax-based approaches provide fertile context for grounding insertions and deletions.
Often we may draw a strong correspondence between function words in one language and syntactic constructions in another.
"For instance, the syntactic approach[REF_CITE]can learn unlexicalized rules that insert function words in isolation, such as:"
NP(NN:x0 NN:x1) ՜ x1 de x0 (4)
"However, as discussed in (Wang, Knight &amp;[REF_CITE]), joint modeling of structure and lexical choice can exacerbate data sparsity, a problem that they attempt to address by tree binarization."
"Nevertheless, as we show below, unlexicalized structural transformation rules such as (1) and (4) that allow for insertion of isolated function words, are essential for good quality translation of truly out-of-domain test data."
"In the treelet translation approach (Menezes &amp;[REF_CITE]), lexical choice and syntactic re-ordering are modeled separately using lexicalized treelets and unlexicalized order templates."
We discuss this approach in more detail in Section 4.
"In Section 5, we describe how we extend this approach to allow for structural insertion and deletion, without the need for content word anchors."
There is surprisingly little prior work in this area.
"We previously (Menezes &amp;[REF_CITE]) explored the use of deletion operations such as (3) above, but these were not grounded in any syntactic context, and the estimation was somewhat heuristic [Footnote_1] ."
1 We assigned channel probabilities based on the sum of the Model1 probability of the source word being aligned to NULL or one of a list of &quot;garbage collector&quot; words. This exploits the property of Model1 that certain high-frequency words tend to act as &quot;garbage collectors&quot; for words that should remain unaligned.
"The tuple translation model[REF_CITE], a joint model over source and target translations, also provides a means of deleting words."
"In training, sentence pairs such as “nombre de archivo” / “file name” are first word aligned, then minimal bilingual tuples are identified, such as “nombre / name”, “de / NULL ” and “archivo / file”."
"The tuples may involve deletion of words by allowing an empty target side, but do not allow insertion tuples with an empty source side."
These inserted words are bound to an adjacent neighbor.
An n-gram model is trained over the tuple sequences.
"As a result, deletion probabilities have the desirable property of being conditioned on adjacent context, yet this context is heavily lexicalized, therefore unlikely to generalize well."
"Li et. al. (2008) describe three models for handling “single word deletion” (they discuss, but do not address, word insertion)."
The first model uses a fixed probability of deletion
"P( NULL ), independent of the source word, estimated by counting null alignments in the training corpus."
"The second model estimates a deletion probability per-word, P( NULL |w), also directly from the aligned corpus, and the third model trains an SVM to predict the probability of deletion given source language context (neighboring and dependency tree-adjacent words and parts-of-speech)."
All three models give large gains of 1.5% BLEU or more on Chinese-English translation.
"It is interesting to note that the more sophisticated models provide a relatively small improvement over the simplest model in-domain, and no benefit out-of-domain."
"As a baseline, we use the treelet translation approach (which we previously described in Menezes &amp;[REF_CITE]), a linguistically syntax-based system leveraging a source parser."
It first unifies lexicalized treelets and unlexicalized templates to construct a sentence-specific set of synchronous rewrite rules.
It then finds the highest scoring derivation according to a linear combination of models.
We briefly review this system before describing our current extension.
"Sentence-specific rewrite rules are constructed by unifying information from three sources: a dependency parse of the input sentence, a set of treelet translation pairs, and a set of unlexicalized order templates."
"Dependency parses are represented as trees: each node has a lexical label and a part of speech, as well as ordered lists of pre-and post-modifiers."
A treelet represents a connected subgraph of a dependency tree; treelet translation pairs consist of source and target treelets and a node alignment.
This alignment is represented by indices: each node is annotated with an integer alignment index.
A source node and a target node are aligned iff they have the same alignment index.
For instance: ((old 1 /JJ) man 2 /NN) ՜ (hombre 2 (viejo 1 )) (5) (man 1 /NN) ՜ (hombre 1 ) (6)
"Order templates are unlexicalized transduction rules that describe the reorderings, insertions and deletions associated with a single group of nodes that are aligned together."
For instance: ((x0: څ /DT) (x1: څ /JJ) څ 1 /NN) ՜ ((x0) څ 1 (x1)) (7) ((x0: څ /DT) (x1: څ /JJ) څ 1 /NN) ՜ ((x0) (x1) څ 1 ) (8) ((x0: څ /DT) څ 1 /NN) ՜ ((x0) څ 1 ) (9) ((x0: څ /RB) څ 1 /JJ) ՜ ((x0) څ 1 ) (10)
Each node is either a placeholder or a variable.
"Placeholders, such as څ 1 /NN on the source side or څ 1 on the target side, have alignment indices and constraints on their parts-of-speech on the source side, but are unconstrained lexically (represented by the څ )."
These unify at translation time with lexicalized treelet nodes with matching parts-of-speech and alignment.
"Variables, such as x0: څ /DT on the source side and x0: څ on the target side, also have parts-of-speech constraints on the source side."
Variables are used to indicate where rewrite rules are recursively applied to translate subtrees.
"Thus each variable label such as x0, must occur exactly once on each side."
"In effect, a template specifies how all the children of a given source node are reordered during translation."
"If translation were a word-replacement task, then templates would be just simple, single-level tree transducers."
"However, in the presence of one-to-many and many-to-one translations and unaligned words, templates may span multiple levels in the tree."
"As an example, order template (7) indicates that an NN with two pre-modifying subtrees headed by DT and JJ may be translated by using a single word translation of the NN, placing the translation of the DT subtree as a pre-modifier, and placing the translation of the JJ subtree as a post-modifier."
"As discussed below, this template can unify with the treelet (6) to produce the following rewrite rule: ((x0:DT) (x1:JJ) man/NN) ՜ ((x0) hombre (x1)) (11)"
Matching: A treelet translation pair matches an input parse iff there is a unique correspondence between the source side of the treelet pair and a connected subgraph of the input parse.
"An order template matches an input parse iff there is a unique correspondence between the source side of the template and the input parse, with the additional restriction that all children of input nodes that correspond to placeholder template nodes must be included in the correspondence."
"For instance, order template (7) matches the parse ((the/DT) (young/JJ) colt/NN) (12) but not the parse ((the/DT) (old/JJ) (grey/JJ) mare/NN) (13)"
"Finally, an order template matches a treelet translation pair at a given node iff, on both source and target sides, there is a correspondence between the treelet translation nodes and template nodes that is consistent with their tree structure and alignments."
"Furthermore, all placeholder nodes in the template must correspond to some treelet node."
Constructing a sentence-specific rewrite rule is then a process of unifying each treelet with a matching combination of order templates with respect to an input parse.
Each treelet node must be unified with one and only one order template placeholder node.
Unifying under these constraints produces a rewrite rule that has a one-to-one correspondence between variables in source and target.
"For instance, given the input parse: ((the/DT) ((very/RB) old/JJ) man/NN) (14) we can create a rewrite rule from the treelet translation pair (5) by unifying it with the order template (7), which matches at the node man and its descendents, and template (10), which matches at the node old, to produce the following sentence-specific rewrite rule: ((the/DT) ((x1: څ /RB) old/JJ) man/NN) ՜ ((el) hombre ((x1) viejo)) (15)"
"Note that by using different combinations of order templates, a single treelet can produce multiple rewrite rules."
"Also, note how treelet translation pairs capture contextual lexical translations but are underspecified with respect to ordering, while order templates separately capture arbitrary reordering phenomena yet are underspecified lexically."
"Keeping lexical and ordering information orthogonal until runtime allows for the production of novel transduction rules never actually seen in the training corpus, leading to improved generalization power."
"Decoding: Given a set of sentence-specific rewrite rules, a standard beam search algorithm is used to find the highest scoring derivation."
Derivations are scored according to a linear combination of models.
The process of extracting treelet translation pairs and order templates begins with parallel sentences.
"First, the sentence pairs are word segmented on both sides, and the source language sentences are parsed."
"Next, the sentence pairs are word aligned and the alignments are used to project a target language dependency tree."
"Treelet extraction: From each sentence pair ܵ , with the alignment relation ~ , a treelet translation pair consisting of the source treelet ܵ and the target treelet is extracted iff: (1) There exist such that ~ .and (2)"
"For all ܵ , and such that ~ , iff ."
Order template extraction is attempted starting from each node S root in the source whose parent is not also aligned to the same target word(s).
"We identify T root , the highest target node aligned to S root ."
We initialize the sets S 0 as {S root } and
T 0 as {T root }.
"We expand S 0 to include all nodes adjacent to some element of S 0 that are (a) unaligned, or (b) aligned to some node in T 0 ."
The converse is applied to T 0 .
This expansion is repeated until we reach a fixed point.
"Together, S 0 and T 0 make up the placeholder nodes in the extracted order template."
We then create one variable in the order template for each direct child of nodes in S 0 and T 0 that is not already included in the order template.
"Iff there is a one-to-one word alignment correspondence between source and target variables, then a template is extracted."
"This restriction leads to clean templates, at the cost of excluding all templates involving extraposition."
"In this paper, we extend our previous work to allow for insertion and deletion of words, by allowing unaligned lexical items as part of the otherwise unlexicalized order templates."
Grounding insertions and deletions in templates rather than treelets has two major benefits.
"First, insertion and deletion can be performed even in the absence of specific lexical context, leading to greater generalization power."
"Secondly, this increased power is tempered by linguistically informative unlexicalized context."
"Rather than proposing insertions and deletions in any arbitrary setting, we are guided by specific syntactic phenomena."
"For instance, when translating English noun compounds into Spanish, we often must include a preposition; this generalization is naturally captured using just parts-of-speech."
"The inclusion of lexical items in order templates affects the translation system in only a few places: dependency tree projection, order template extraction, and rewrite rule construction at runtime."
"Dependency tree projection: During this step of the baseline treelet system, unaligned words are by default attached low, to the lowest aligned neighbor."
"Although this worked well in conjunction with the discriminative order model, it prevents unaligned nodes from conditioning on relevant context in order templates."
"Therefore, we change the default attachment of unaligned nodes to be to the highest aligned neighbor; informal experiments showed that this did not noticeably impact translation quality in the baseline system."
"For example, consider the source parse and aligned target sentence: ((calibrated 1 /JJ) (camera 2 /NN) file 3 /NN) archivo 3 de 4 cámara 2 calibrado 1 (16)"
Using the baseline projection algorithm would produce this target dependency tree: (archivo 3 ((de 4 ) cámara 2 ) (calibrado 1 )) (17)
"Instead, we attach unaligned words high: (archivo 3 (de 4 ) (cámara 2 ) (calibrado 1 )) (18)"
"Order template extraction: In addition to the purely unlexicalized templates extracted from each training sentence, we also allow templates that include lexical items for each unaligned token."
"For each point in the original extraction procedure, where S 0 or T 0 contain unaligned nodes, we now extract two templates: The original unlexicalized template, and a new template in which only the unaligned node(s) contain the specific lexical item(s)."
"From the example sentence pair (16), using the projected parse (18) we would extract the following two templates: ((x0: څ /JJ) (x1: څ /NN) څ 1 /NN) ՜ ( څ 1 ( څ 2 ) (x1) (x0)) (19) ((x0: څ /JJ) (x1: څ /NN) څ 1 /NN) ՜ ( څ 1 (de 2 ) (x1) (x0)) (20)"
Template matching and unification: We extend the template matching against the input parse to require that any lexicalized source template nodes match the input exactly.
"When matching templates to treelet translation pairs, any unaligned treelet nodes must be consistent with the corresponding template node (i.e. the template node must be unlexicalized, or the lexical items must match)."
"On the other hand, lexicalized template nodes do not need to match any treelet nodes -- insertions or deletions may now come from the template alone."
Consider the following example input parse: ((digital/JJ) (camera/NN) (file/NN) extension/NN) (21)
"The following treelet translation pair provides a contextual translation for some of the children, including the insertion of one necessary preposition: ((file 1 /NN) extension 2 /NN) ՜ (extension 2 (de 3 ) (archivo 1 )) (22)"
The following order template can provide relative ordering information between nodes as well as insert the remaining prepositions: ((x0: څ /JJ) (x1: څ /NN) (x2: څ /NN) څ 1 /NN) ՜ ( څ 1 (de 2 ) (x2) (de 3 ) (x0) (x1)) (23)
"The unification of this template and treelet is somewhat complex: the first inserted de is agreed upon by both template and treelet, whereas the second is inserted by the template alone."
This results in the following novel rewrite rule: ((x0: څ /JJ) (x1: څ /NN) (file) extension) ՜ (extension (de) (archivo) (de) (x0) (x1)) (24)
These relatively minimal changes produce a powerful contextualized model of insertion and deletion.
Parameter estimation: The underlying treelet system includes a template probability estimated by relative frequency.
We estimate our lexicalized templates in the same way.
"However early experiments showed that this feature alone was not enough to allow even common insertions, since the probability of even the most common insertion templates is much lower than that of unlexicalized templates."
"To improve the modeling capability, we included two additional feature functions: a count of structurally inserted words, and a count of structurally deleted words."
Consider the following English test sentence and corresponding Spanish human translation:
September is National Cholesterol Education Month
Septiembre es el Mes Nacional para la Educación sobre el Colesterol
The baseline treelet system without structural insertions translates this sentence as:
Septiembre es Nacional Colesterol Educación Mes
"Not only is the translation missing the appropriate articles and prepositions, but also in their absence, it fails to reorder the content words correctly."
"Without the missing prepositions, the language model does not show a strong preference among various orderings of &quot;nacional&quot; &quot;colesterol&quot; &quot;educación&quot; and &quot;mes&quot;."
"Using structural insertion templates, the highest scoring translation of the sentence is now:"
Septiembre es el Mes Nacional de Educación de colesterol
"Although the choice of prepositions is not the same as the reference, the fluency is much improved and the translation is quite understandable."
"Figure 6.1, lists the structural insertion templates that are used to produce this translation, and shows how they are unified with treelet translation pairs to produce sentence-specific rewrite rules, which are in turn composed during decoding to produce this translation."
We evaluated the translation quality of the system using the BLEU metric[REF_CITE].
"We compared three systems: (a) a standard phrasal system using a decoder based on Pharaoh,[REF_CITE], (b) A baseline treelet system using unlexicalized order templates and (c)"
"The present work, which adds structural insertion and deletion templates."
"We report results for two language pairs, English-Spanish and English- Japanese."
"For English-Spanish we use two training sets: (a) the Europarl corpus provided by the[REF_CITE]Statistical Machine Translation workshop (b) a “general-domain” data set that includes a broad spectrum of data such as governmental data, general web data and technical corpora."
For English-Japanese we use only the “general-domain” data set.
For English-Spanish we report results using the four test sets listed in Table 7.2.
For English-Japanese we use only the web test set.
The first two tests are from the 2006 SMT workshop and the newswire test is from the 2008 workshop.
"The web test sets were selected from a random sampling of English web sites, with target language translations provided by professional translation vendors."
All test sets have one reference translation.
"The baseline treelet translation system uses all the models described in Menezes &amp;[REF_CITE], namely: • Treelet log probabilities, maximum likelihood estimates with absolute discounting. • Forward and backward lexical weighting, using Model-1 translation log probabilities. • Trigram language model using modified Kneser-Ney smoothing. • Word and phrase count feature functions. • Order template log probabilities, maximum likelihood estimates, absolute discounting. • Count of artificial source order templates. [Footnote_2] • Discriminative tree-based order model."
"2 When no template is compatible with a treelet, the decoder creates an artificial template that preserves source order. This count feature allows MERT to deprecate the use of such templates. This is analogous to the glue rules[REF_CITE]."
"The present work does not use the discriminative tree-based order model [Footnote_3] but adds: • Count of structural insertions: This counts only words inserted via templates, not lexical insertions via treelets. • Count of structural deletions: This counts only words deleted via templates, not lexical deletions via treelets."
"3 In our experiments, we find that the impact of this model is small in the presence of order templates; also, it degrades the overall speed of the decoder."
The comparison phrasal system was constructed using the same alignments and the heuristic combination described[REF_CITE].
"This system used a standard set of models: • Direct and inverse log probabilities, both relative frequency and lexical weighting. • Word count, phrase count. • Trigram language model log probability. • Length based distortion model. • Lexicalized reordering model."
"We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser able to produce syntactic analyses at varying levels of depth[REF_CITE]."
"For the purposes of these experiments, we used a dependency tree output with part-of-speech tags and unstemmed, case-normalized surface words."
"For word alignment we used a training regimen of five iterations of Model 1, followed by five iterations of a word-dependent HMM model[REF_CITE]in both directions."
The forward and backward alignments were combined using a dependency tree-based heuristic combination.
The word alignments and English dependency tree were used to project a target tree.
From the aligned tree pairs we extracted treelet and order template tables.
"For the Europarl systems, we use a phrase/treelet size of 7 and train model weights using 2000 sentences of Europarl data."
"For the “general-domain” systems, we use a phrase/treelet size of 4, and train model weights using 2000 sentences of web data."
"For any given corpus, all systems used the same treelet or phrase size (see Table 7.1) and the same trigram language model."
"Model weights were trained separately for each system, data set and experimental condition, using minimum error rate training to maximize BLEU[REF_CITE]."
Tables 8.1 and 8.4 compare baseline phrasal and treelet systems with systems that use various types of insertion and deletion templates.
"As one might expect, the use of structural insertion and deletion has the greatest impact when translating between languages such as English and Japanese that show significant structural divergence."
"In this language pair, both insertions and deletions have an impact, for a total gain of 1.1% BLEU over the baseline treelet system, and 3.6% over the phrasal system."
"To aid our understanding of the system, we tabulated the most commonly inserted and deleted words when translating from English into Japanese in Tables 8.2 and 8.3 respectively."
"Satisfyingly, most of the insertions and deletions correspond to well-known structural differences between the languages."
"For instance, in English the thematic role of a noun phrase, such as subject or object, is typically indicated by word order, whereas Japanese uses case markers to express this information."
"Hence, case markers such as “ を ” and “ は ” need to be translated, an intervening postposition such as “ の ” inserted."
"Also, when noun compounds are is usually needed."
Among the most common deletions are “the” and “a”.
This is because Japanese does not have a notion of definiteness.
"Similarly, pronouns are often dropped in Japanese."
"English-Spanish: We note, in Table 8.4 that even between such closely related languages, structural insertions give us noticeable improvements over the baseline treelet system."
On the smaller Europarl training corpus the improvements range from 0.5% to 1.1% BLEU.
"On the larger training corpus we find that for the more in-domain governmental [Footnote_4] and news test sets, the effect is smaller or even slightly negative, but on the very broad web test set we still see an improvement of about 0.7% BLEU."
"4 The &quot;general domain&quot; training corpus is a superset of the Europarl training set, therefore, the Europarl tests sets are &quot;in-domain&quot; in both cases."
"As one might expect, as the training data size increases, the generalization power of structural insertion and deletions becomes less important when translating in-domain text, as more insertions and deletions can be handled lexically."
"Nevertheless, the web test results indicate that if one hopes to handle truly general input the need for structural generalizations remains."
"Unlike in English-Japanese, when translating from English to Spanish, structural deletions are less helpful."
Used in isolation or in combination with insertion templates they have a slightly negative and/or insignificant impact in all cases.
"We hypothesize that when translating from English into Spanish, more words need to be inserted than deleted."
"Conversely, when translating in the reverse direction, deletion templates may play a bigger role."
We were unable to test the reverse direction because our syntax-based systems depend on a source language parser.
In future work we hope to address this.
"In table 8.5 and 8.6, we list the words most commonly inserted and deleted when translating the web test using the general English-Spanish system."
"As in English-Japanese, we find that the insertions are what one would expect on linguistic grounds."
"However, deletions are used much less frequently than insertions and also much less frequently than they are in English-Japanese."
"Furthermore, the most common deletion is of quotation marks, which is incorrect in most cases, even though such deletion is evidenced in the training corpus [Footnote_5] ."
"5 In many parallel corpora, quotes are not consistently preserved between source and target languages."
"On the other hand, the next most common deletions “I” and “it” are linguistically well grounded, since Spanish often drops pronouns."
"We have presented an extension of the treelet translation method to include order templates with structural insertion and deletion, which improves translation quality under a variety of scenarios, particularly between structurally divergent languages."
"Even between closely related languages, these operations significantly improve the generalizability of the system, providing benefit when handling out-of-domain test data."
"Our experiments shed light on a little-studied area of MT, but one that is nonetheless crucial for high quality broad domain translation."
"Our results affirm the importance of structural insertions, in particular, when translating from English into other de 3509 74%[REF_CITE]12%[REF_CITE]5.3%[REF_CITE]1.6% Reflexive pronoun que 63 1.3% Relative pronoun los 63 1.3%[REF_CITE]1.2% Preposition+Determiner , 42 0.89%[REF_CITE]0.63%[REF_CITE]0.44% Preposition lo 9 0.19% Pronoun las 6 0.13% Determiner languages, and the importance of both insertions and deletions when translating between divergent languages."
"In future, we hope to study translations from other languages into English to study the role of deletions in such cases."
The performance of machine translation sys-tems varies greatly depending on the source and target languages involved.
Determining the contribution of different characteristics of language pairs on system performance is key to knowing what aspects of machine transla-tion to improve and which are irrelevant.
This paper investigates the effect of different ex-planatory variables on the performance of a phrase-based system for 110 European lan-guage pairs.
"We show that three factors are strong predictors of performance in isolation: the amount of reordering, the morphological complexity of the target language and the his-torical relatedness of the two languages."
"To-gether, these factors contribute 75% to the variability of the performance of the system."
"Statistical machine translation (SMT) has improved over the last decade of intensive research, but for some language pairs, translation quality is still low."
Certain systematic differences between languages can be used to predict this.
Many researchers have speculated on the reasons why machine translation is hard.
"However, there has never been, to our knowl-edge, an analysis of what the actual contribution of different aspects of language pairs is to translation performance."
This understanding of where the diffi-culties lie will allow researchers to know where to most gainfully direct their efforts to improving the current models of machine translation.
Many of the challenges of SMT were first out-lined[REF_CITE].
"The original IBM Models were broken down into separate translation and distortion models, recognizing the importance of word order differences in modeling translation."
"Brown et al. also highlighted the importance of mod-eling morphology, both for reducing sparse counts and improving parameter estimation and for the cor-rect production of translated forms."
"We see these two factors, reordering and morphology, as fundamental to the quality of machine translation output, and we would like to quantify their impact on system per-formance."
"It is not sufficient, however, to analyze the mor-phological complexity of the source and target lan-guages."
"It is also very important to know how sim-ilar the morphology is between the two languages, as two languages which are morphologically com-plex in very similar ways, could be relatively easy to translate."
"Therefore, we also include a measure of the family relatedness of languages in our analysis."
The impact of these factors on translation is mea-sured by using linear regression models.
We perform the analysis with data from 110 different language pairs drawn from the Europarl project[REF_CITE].
"This contains parallel data for the 11 official language pairs of the European Union, providing a rich variety of different language characteristics for our experiments."
Many research papers report re-sults on only one or two languages pairs.
"By analyz-ing so many language pairs, we are able to provide a much wider perspective on the challenges facing machine translation."
This analysis is important as it provides very strong motivation for further research.
"The findings of this paper are as follows: (1) each of the main effects, reordering, target language com-plexity and language relatedness, is a highly signif-icant predictor of translation performance, (2) indi-vidually these effects account for just over a third of the variation of the B LEU score, (3) taken together, they account for 75% of the variation of the B LEU score, (4) when removing Finnish results as out-liers, reordering explains the most variation, and fi-nally (4) the morphological complexity of the source language is uncorrelated with performance, which suggests that any difficulties that arise with sparse counts are insignificant under the experimental con-ditions outlined in this paper."
"In order to analyze the influence of different lan-guage pair characteristics on translation perfor-mance, we need access to a large variety of compa-rable parallel corpora."
A good data source for this is the Europarl Corpus[REF_CITE].
"It is a collection of the proceedings of the European Parliament, dat-ing back to 1996."
Version 3 of the corpus consists of up to 44 million words for each of the 11 official lan-guages of the European Union:
"Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv)."
"In trying to determine the effect of properties of the languages involved in translation performance, it is very important that other variables be kept con-stant."
"Using Europarl, the size of the training data for the different language pairs is very similar, and there are no domain differences as all sentences are roughly trained on translations of the same data."
The morphological complexity of the language pairs involved in translation is widely recognized as one of the factors influencing translation performance.
"However, most statistical translation systems treat different inflected forms of the same lemma as com-pletely independent of one another."
This can result in sparse statistics and poorly estimated models.
"Fur-thermore, different variations of the lemma may re-sult in crucial differences in meaning that affect the quality of the translation."
Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity[REF_CITE]or including morphological information in decod- ing[REF_CITE].
"Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different lev-els of morphology have on translation."
We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance.
"Some languages seem to be intuitively more com-plex than others, for instance Finnish appears more complex than English."
"There is, however, no obvi-ous way of measuring this complexity."
"One method of measuring complexity is by choosing a number of hand-picked, intuitive properties called complex-ity indicators[REF_CITE]and then to count their occurrences."
Examples of morpholog-ical complexity indicators could be the number of in-flectional categories or morpheme types in a typical sentence.
This method suffers from the major draw-back of finding a principled way of choosing which of the many possible linguistic properties should be included in the list of indicators.
A simple alternative employed[REF_CITE]is to use vocabulary size as a measure of morpho-logical complexity.
"Vocabulary size is strongly in-fluenced by the number of word forms affected by number, case, tense etc. and it is also affected by the number of agglutinations in the language."
The com-plexity of the morphology of languages can there-fore be approached by looking at vocabulary size.
Figure 1 shows the vocabulary size for all rele-vant languages.
"Each language pair has a slightly different parallel corpus, and so the size of the vo-cabularies for each language needs to be averaged."
"You can see that the size of the Finnish vocabulary is about six times larger (510,632 words) than the En-glish vocabulary size (88,880 words)."
"The reason for the large vocabulary size is that Finnish is character-ized by a rich inflectional morphology, and it is typo-logically classified as an agglutinative-fusional lan-guage."
"As a result, words are often polymorphemic, and become remarkably long."
The morphological complexity of each language in isolation could be misleading.
Large differences in morphology between two languages could be more relevant to translation performance than a complex morphology that is very similar in both languages.
Languages which are closely related could share morphological forms which might be captured rea-sonably well in translation models.
We include a measure of language relatedness in our analyses to take this into account.
Comparative linguistics is a field of linguistics which aims to determine the historical relatedness of languages.
"Lexicostatistics, developed by Morris Swadesh in the 1950s[REF_CITE], is an ap-proach to comparative linguistics that is appropriate for our purposes because it results in a quantitative measure of relatedness by comparing lists of lexical cognates."
The lexicostatistic percentages are extracted as follows.
"First, a list of universal culture-free mean-ings are generated."
Words are then collected for these meanings for each language under consider-ation.
Lists for particular purposes have been gen-erated.
"For example, we use the data[REF_CITE]who developed a list of 200 meanings for 84 Indo-European languages."
Cognacy decisions are then made by a trained linguist.
"For each pair of lists the cognacy of a form can be positive, negative or in-determinate."
"Finally, the lexicostatistic percentage is calculated."
"This percentage is related to the propor-tion of meanings for a particular language pair that are cognates, i.e. relative to the total without inde-terminacy."
"Factors such as borrowing, tradition and"
Reordering refers to differences in word order that occur in a parallel corpus and the amount of reorder-ing affects the performance of a machine translation system.
"In order to determine how much it affects performance, we first need to measure it."
Reordering is largely driven by syntactic differences between languages and can involve complex rear-rangements between nodes in synchronous trees.
Modeling reordering exactly would require a syn-chronous tree-substitution grammar.
"This represen-tation would be sparse and heterogeneous, limiting its usefulness as a basis for analysis."
We make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful.
We assume that reordering is a binary process occurring between two blocks that are adjacent in the source.
"This is similar to the ITG constraint[REF_CITE], however our reorder-ings are not dependent on a synchronous grammar or a derivation which covers the sentences."
"There are also similarities with the Human-Targeted Transla- tion Edit Rate metric (HTER)[REF_CITE]which attempts to find the minimum number of hu-man edits to correct a hypothesis, and admits mov-ing blocks of words, however our algorithm is auto-matic and does not consider inserts or deletes."
Before describing the extraction of reorderings we need to define some concepts.
"We define a block A as consisting of a source span, A s , which contains the positions from A smin to A smax and is aligned to a set of target words."
"The minimum and maximum positions (A tmin and A tmax ) of the aligned target words mark the block’s target span, A t ."
"A reordering r consists of the two blocks r A and r B , which are adjacent in the source and where the relative order of the blocks in the source is reversed in the target."
"More formally: r A s &lt; r B s , r A t &gt; r B t , r A smax = r B smin − 1"
A consistent block means that between A tmin and A tmax there are no target word positions aligned to source words outside of the block’s source span
A s .
A reordering is consistent if the block projected from r A smin to r B smax is consistent.
The following algorithm detects reorderings and determines the dimensions of the blocks involved.
"We step through all the source words, and if a word is reordered in the target with respect to the previ-ous source word, then a reordering is said to have occurred."
These two words are initially defined as the blocks A and B.
"Then the algorithm attempts to grow block A from this point towards the source starting position, while the target span of A is greater than that of block B, and the new block A is consis-tent."
"Finally it attempts to grow block B towards the source end position, while the target span of B is less than that of A and the new reordering is incon-sistent."
See Figure 3 for an example of a sentence pair with two reorderings.
Initially a reordering is de-tected between the Chinese words aligned to “from” and “late”.
The block A is grown from “late” to in-clude the whole phrase pair “late last night”.
Then the block B is grown from “from” to include “Bei-jing” and stops because the reordering is then con-sistent.
The next reordering is detected between “ar-rived in” and “Beijing”.
We can see that block A at-tempts to grow as large a block as possible and block
B attempts to grow the smallest block possible.
The reorderings thus extracted would be comparable to those of a right-branching ITG with inversions.
This allows for syntactically plausible embedded reorder-ings.
This algorithm has the worst case complexity 2 of O( n2 ) when the words in the target occur in the reverse order to the words in the source.
Our reordering extraction technique allows us to an-alyze reorderings in corpora according to the dis-tribution of reordering widths.
"In order to facilitate the comparison of different corpora, we combine statistics about individual reorderings into a sen-tence level metric which is then averaged over a cor-pus."
P r∈R |r
A s | + |r B s | RQuantity =
"I where R is the set of reorderings for a sentence, I is the source sentence length, A and B are the two blocks involved in the reordering, and |r A s | is the size or span of block A on the source side."
"RQuan-tity is thus the sum of the spans of all the reordering blocks on the source side, normalized by the length of the source sentence."
Reorderings extracted from manually aligned data can be reliably assumed to be correct.
The only exception to this is that embedded reorderings are always right branching and these might contradict syntactic structure.
"In this paper, however, we use alignments that are automatically extracted from the training corpus using GIZA++."
Automatic align-ments could give very different reordering results.
"In order to justify using reordering data extracted from automatic alignments, we must show that they are similar enough to gold standard alignments to be useful as a measure of reordering."
We select the German-English language pair be-cause it has a reasonably high level of reordering.
A manually aligned German-English corpus was pro-vided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translati[REF_CITE]test set.
This test set is from a held out portion of the Europarl corpus.
The automatic alignments were extracted by ap-pending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++[REF_CITE]and the grow-final-diag algorithm[REF_CITE].
"In order to use automatic alignments to extract re-ordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments."
We first look at global reordering statistics and then we look in more detail at the reordering dis-tribution of the corpora.
"Table 2 shows the amount of reordering in the[REF_CITE]test corpora, with both manual and automatic alignments, and in the auto-matically aligned Europarl DE-EN parallel corpus."
We can see that all three corpora show a similar amount of reordering.
Figure 4 shows that the distribution of reorder-ings between the three corpora is also very similar.
These results provide evidence to support our use of automatic reorderings in lieu of manually annotated alignments.
"Firstly, they show that our[REF_CITE]test corpus is very similar to the Europarl data, which means that any conclusions that we reach using the[REF_CITE]test corpus will be valid for the Europarl data."
"Secondly, they show that the reordering behav-ior of this corpus is very similar when looking at automatic vs. manual alignments."
"Although differences between the reorderings de-tected in the manually and automatically aligned German-English corpora are minor, there we accept that there could be a language pair whose real re-ordering amount is very different to the expected amount given by the automatic alignments."
A par-ticular language pair could have alignments that are very unsuited to the stochastic assumptions of the IBM or HMM alignment models.
"However, manu-ally aligning 110 language pairs is impractical."
Extracting the amount of reordering for each of the 110 language pairs in the matrix required a sam-pling approach.
We randomly extracted a subset of 2000 sentences from each of the parallel training corpora.
From this subset we then extracted the av- erage RQuantity.
In Figure 5 the amount of reordering for each of the language pairs is proportional to the width of the relevant square.
Note that the matrix is not quite symmetrical - reordering results differ de-pending on which language is chosen to measure the reordering span.
"The lowest reordering scores are generally for languages in the same language group (like Portuguese-Spanish, 0.20, and Danish-Swedish, 0.24) and the highest for languages from different groups (like German-French, 0.64, and Finnish-Spanish, 0.61)."
"In this paper we use linear regression models to de-termine the correlation and significance of various explanatory variables with the dependent variable, the BLEU score."
"Ideally the explanatory variables involved should be independent of each other, how-ever the amount of reordering in a parallel corpus could easily be influenced by family relatedness."
We investigate the correlation between these variables.
Figure 6 shows the plot of the reordering amount against language similarity.
The regression is highly significant and has an R 2 of 0.2347.
This means that reordering is correlated with language similarity and that 23% of reordering can be explained by language similarity.
"We used the phrase-based model Moses[REF_CITE]for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model."
Tests were run on the[REF_CITE]test set[REF_CITE].
We use the B LEU score[REF_CITE]to evaluate our systems.
"While the role of B LEU in machine translation evaluation is a much discussed topic, it is generally assumed to be a adequate metric for comparing systems of the same type."
Figure 7 shows the B LEU score results for the ma-trix.
Comparing this figure to Figure 5 there seems to be a clear negative correlation between reordering amount and translation performance.
"We perform multiple linear regression analyses us-ing measures of morphological complexity, lan-guage relatedness and reordering amount as our in-dependent variables."
"The dependent variable is the translation performance metric, the B LEU score."
We then use a t-test to determine whether the co-efficients for the independent variables are reliably different from zero.
We also test how well the model explains the data using an R 2 test.
"The two-tailed significance levels of coefficients and R 2 are also given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001."
The first question we are interested in answering is which factors contribute most and how they interact.
We fit a multiple regression model to the data.
The source vocabulary size has no significant effect on the outcome.
All explanatory variable vectors were normalized to be more comparable.
In Table 3 we can see the relative contribution of the different features to the model.
Source vocabu-lary size did not contribute significantly to the ex-planatory power of this multiple regression model and was therefore not included.
"The fraction of the variance explained by the model, or its goodness of fit, the R 2 , is 0.750 which means that 75% of the variation in B LEU can be explained by these three factors."
"The interaction of reordering amount and language relatedness is the product of the values of these two features, and in itself it is an important ex-planatory feature."
"To make sure that our regression is valid, we need to consider the special case of Finnish."
Data points where Finnish is the target language are outliers.
"Finnish has the lowest language similarity with all other languages, and the largest vocabulary size."
"It also has very high amounts of reordering, and the lowest B LEU scores when it is the target language."
"The multiple regression of Table 3 where Finnish as the source and target language is excluded, shows that all the effects are still very significant, with the model’s R 2 dropping only slightly to 0.68."
The coefficients of the variables in the multiple regression model have only limited usefulness as a measure of the impact of the explanatory variables in the model.
"One important factor to consider is that if the explanatory variables are highly correlated, then the values of the coefficients are unstable."
The model could attribute more importance to one or the other variable without changing the overall fit of the model.
This is the problem of multicollinearity.
"Our explanatory variables are all correlated, but a large amount of this correlation can be explained by look-ing at language pairs with Finnish as the target lan-guage."
"Excluding these data points, only language relatedness and reordering amount are still corre-lated, see Section 5.5 for more details."
"In order to establish the relative contribution of vari-ables, we isolate their impact on the B LEU score by modeling them in separate linear regression models."
Figure 8 shows a simple regression model over the plot of B LEU scores against target vocabulary size.
This figure shows groups of data points with the same target language in almost vertical lines.
"Each language pair has a separate parallel training corpus, but the target vocabulary size for one language will be very similar in all of them."
"The variance in B LEU amongst the group with the same target language is then largely explained by the other factors, similarity and reordering."
Figure 9 shows a simple regression model over the plot of B LEU scores against source vocabulary size.
"This regression model shows that in isolation source vocabulary size is significant (p &lt; 0.05), but that this is due to the distorting effect of Finnish."
"Excluding results that include Finnish, there is no longer any significant correlation with B LEU ."
"The source mor-phology might be significant for models trained on smaller data sets, where model parameters are more sensitive to sparse counts."
"This graph shows that with more reorder-ing, the performance of the translation model re-duces."
Data points with low levels of reordering and high B LEU scores tend to be language pairs where both languages are Romance languages.
High B LEU scores with high levels of reordering tend to have German as the source language and a Romance lan-guage as the target.
The left hand line of points are the results involving Finnish.
"The vertical group of points just to the right, are results where Greek is involved."
The next set of points are the results where the translation is between Germanic and Ro-mance languages.
"The final cloud to the right are re-sults where languages are in the same family, either within the Romance or the Germanic languages."
Table 4 shows the amount of the variance of B LEU explained by the different models.
"As these are simple regression models, with just one explana-tory variable, multicolinearity is avoided."
"This table shows that each of the main effects explains about a third of the variance of B LEU , which means that they can be considered to be of equal importance."
"When Finnish examples are removed, only reordering re-tains its power, and target vocabulary and language similarity reduce in importance and source vocabu-lary size no longer correlates with performance."
We have broken down the relative impact of the characteristics of different language pairs on trans- lation performance.
"The analysis done is able to ac-count for a large percentage (75%) of the variabil-ity of the performance of the system, which shows that we have captured the core challenges for the phrase-based model."
"We have shown that their im-pact is about the same, with reordering and target vocabulary size each contributing about 0.38%."
"These conclusions are only strictly relevant to the model for which this analysis has been performed, the phrase-based model."
"However, we suspect that the conclusions would be similar for most statisti-cal machine translation models because of their de-pendence on automatic alignments."
This will be the topic of future work.
"The graph-based ranking algorithm has been recently exploited for multi-document sum-marization by making only use of the sen-tence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable."
"However, given a document set to be summarized, dif-ferent documents are usually not equally im-portant, and moreover, different sentences in a specific document are usually differently im-portant."
This paper aims to explore document impact on summarization performance.
We propose a document-based graph model to in-corporate the document-level information and the sentence-to-document relationship into the graph-based ranking process.
Various meth-ods are employed to evaluate the two factors.
Experimental results on the[REF_CITE]datasets demonstrate that the good effectiveness of the proposed model.
"More-over, the results show the robustness of the proposed model."
"Multi-document summarization aims to produce a summary describing the main topic in a document set, without any prior knowledge."
Multi-document summary can be used to facilitate users to quickly understand a document cluster.
"For example, a number of news services (e.g. NewsInEssence [URL_CITE] ) have been developed to group news articles into news topics, and then produce a short summary for each news topic."
"Users can easily understand the topic they have interest in by taking a look at the short summary, without looking into each individ-ual article within the topic cluster."
Automated multi-document summarization has drawn much attention in recent years.
"In the com-munities of natural language processing and infor-mation retrieval, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summari-zation techniques and produced a couple of ex-perimental online systems."
"A particular challenge for multi-document sum-marization is that a document set might contain diverse information, which is either related or un-related to the main topic, and hence we need effec-tive summarization methods to analyze the information stored in different documents and ex-tract the globally important information to reflect the main topic."
"In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features."
"Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents[REF_CITE]."
The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences.
The sentences with large rank scores are chosen into the summary.
"However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-to-document relationship."
"Actually, given a document set, different documents are not equally important."
"For example, the documents close to the main top-ics of the document set are usually more important than the documents far away from the main topics of the document set."
This document-level informa-tion is deemed to have great impact on the sen-tence ranking process.
"Moreover, the sentences in the same document cannot be treated uniformly, because some sentences in the document are more important than other sentences because of their different positions in the document or different distances to the document’s centroid."
"In brief, nei-ther the document-level information nor the sen-tence-to-document relationship has been taken into account in the previous graph-based model."
"In order to overcome the limitations of the pre-vious graph-based model, this study proposes the document-based graph model to explore document impact on the graph-based summarization, by in-corporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."
We develop various methods to evaluate the document-level informa-tion and the sentence-to-document relationship.
"Experiments on the[REF_CITE]data-sets have been performed and the results demon-strate the good effectiveness of the proposed model, i.e., the incorporation of document impact can much improve the performance of the graph-based summarization."
"Moreover, the proposed model is robust with respect to most incorporation schemes."
The rest of this paper is organized as follows.
We first introduce the related work in Section 2.
"The basic graph-based summarization model and the proposed document-based graph model are de-scribed in detail in Sections 3 and 4, respectively."
We show the experiments and results in Section 5 and finally we conclude this paper in Section 6.
"Generally speaking, summarization methods can be abstractive summarization or extractive summa-rization."
"Extractive summarization is a simple but robust method for text summarization and it in-volves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and ex-tracting those with highest scores, while abstrac-tion summarization usually needs information fusi[REF_CITE], sentence compres-si[REF_CITE]and reformulati[REF_CITE]."
"In this study, we focus on extractive summarization."
The centroid-based method[REF_CITE]is one of the most popular extractive summariza- tion methods.
"MEAD [URL_CITE] is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, in-cluding cluster centroids, position, TFIDF, etc. NeATS[REF_CITE]is a project on multi-document summarization at ISI based on the sin-gle-document summarizer-SUMMARIST."
"Sen-tence position, term frequency, topic signature and term clustering are used to select important content."
MMR[REF_CITE]is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coher-ence.
"To further explore user interface issues, iNeATS[REF_CITE]is developed based on NeATS."
XDoX[REF_CITE]is a cross document summarizer designed specifically to summarize large document sets.
"It identifies the most salient themes within the set by passage clus-tering and then composes an extraction summary, which reflects these main themes."
"Much other work also explores to find topic themes in the documents for summarization, e.g.[REF_CITE]investigate five different topic representations and introduce a novel representa-tion of topics based on topic themes."
"In addition,[REF_CITE]selects important sentences based on the discourse structure of the text."
TNO’s system[REF_CITE]scores sentences by combining a unigram language model approach with a Bayes-ian classifier based on surface features.
Graph-based models have been proposed to rank sentences or passages based on the PageRank algo-rithm[REF_CITE]or its variants.
Websumm[REF_CITE]uses a graph-connectivity model and operates under the assump-tion that nodes which are connected to many other nodes are likely to carry salient information.
Lex-PageRank[REF_CITE]is an approach for computing sentence importance based on the concept of eigenvector centrality.
It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to Pag-eRank.
"All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageR-ank."
"Other related work includes topic-focused multi-document summarization (Daumé. and[REF_CITE]), which aims to produce summary biased to a given topic or query."
"It is noteworthy that our proposed ap-proach is inspired[REF_CITE], which proposes the Conditional Markov Random Walk Model based on two-layer web graph in the tasks of web page retrieval."
The basic graph-based model is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph.
The basic idea is that of “voting” or “recommendation” between the ver-tices.
A link between two vertices is considered as a vote cast from one vertex to the other vertex.
"The score associated with a vertex is determined by the votes that are cast for it, and the score of the verti-ces casting these votes."
"Formally, given a document set D, let G=(V, E) be an undirected graph to reflect the relationships be-tween sentences in the document set, as shown in Figure 1."
V is the set of vertices and each vertex v i in V is a sentence in the document set.
E is the set of edges.
"Each edge e ij in E is associated with an affinity weight f(v i , v j ) between sentences v i and v j (i≠j)."
"The weight is computed using the standard cosine measure between the two sentences. vr ⋅vr f (v i , v j ) = sim cosine (v i , v j ) = r i r j (1) v i × v j where vr and vr are the corresponding term vec- j i tors of v i and v j ."
"Here, we have f(v i , v j )=f(v j , v i )."
"Two vertices are connected if their affinity weight is larger than 0 and we let f(v i , v i )=0 to avoid self transition."
We use an affinity matrix M to describe G with each entry corresponding to the weight of an edge in the graph.
"M = (M i,j ) |V|×|V| is defined as follows: ⎧ f (v i , v j ), if v i and v j is connected M i,j = ⎪⎨ (2) and i ≠ j; ⎪ ⎩0, otherwise"
Then M is normalized to M~ as follows to make the sum of each row equal to 1: ⎧ |V| |V|
"M~ = ⎪M i,j ∑ M , if ∑ M ≠ 0 i,j (3) i,j i,j ⎨ j=1 j=1 ⎪ ⎩0 , otherwise"
"Based on matrix M~ , the saliency score Sen-Score(v i ) for sentence v i can be deduced from those of all other sentences linked with it and it can be formulated in a recursive form as in the PageRank algorithm:"
"SenScore(v i ) = µ ⋅ ∑ SenScore(v j ) ⋅M~ j,i + (1|−V µ | ) (4) all j≠i"
"And the matrix form is: λr = µ M~ T λr + (1 − µ ) er (5) |V | where λr =[SenScore(v i )] |V|×1 is the vector of sen-tence saliency scores. re is a vector with all ele-ments equaling to 1. µ is the damping factor usually set to 0.85, as in the PageRank algorithm."
The above process can be considered as a Markov chain by taking the sentences as the states and the corresponding transition matrix is given by A = µ M~ T + (1 − µ ) erer T .
The stationary prob- |V| ability distribution of each state is obtained by the principal eigenvector of the transition matrix.
"For implementation, the initial scores of all sen-tences are set to 1 and the iteration algorithm in Equation (4) is adopted to compute the new scores of the sentences."
Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences falls below a given threshold (0.0001 in this study).
"We can see that the basic graph-based model is built on the single-layer sentence graph and the transition probability between two sentences in the Markov chain depends only on the sentences them-selves, not taking into account the document-level information and the sentence-to-document rela-tionship."
"As we mentioned in previous section, there may be many factors that can have impact on the impor-tance analysis of the sentences."
This study aims to examine the document impact by incorporating the document importance and the sentence-to-document correlation into the sentence ranking process.
"Our assumption is that the sentences, whi-ch belong to an important document and are highly correlated with the document, will be more likely to be chosen into the summary."
"In order to incorporate the document-level in-formation and the sentence-to-document relation-ship, the document-based graph model is proposed based on the two-layer link graph including both sentences and documents."
The novel representation is shown in Figure 2.
"As can be seen, the lower layer is just the traditional link graph between sen-tences that has been well studied in previous work."
And the upper layer represents the documents.
The dashed lines between these two layers indicate the conditional influence between the sentences and the documents.
"Formally, the new representation for the two-layer graph is denoted as G * =&lt;V s , V d , E ss , E sd &gt;, where V s =V={v i } is the set of sentences and V d =D={d j } is the set of documents; E ss =E={e ij |v i , v j ∈V s } includes all possible links between sen-tences and E sd ={e ij |v i ∈V s , d j ∈V d and d j =doc(v i )} includes the correlation link between any sentence and its belonging document."
"Here, we use doc(v i ) to denote the document containing sentence v i ."
"For further discussions, we let π(doc(v i )) ∈[0,1] de-note the importance of document doc(v i ) in the document set, and let ω(v i , doc(v i )) ∈[0,1] denote the strength of the correlation between sentence v i and its document doc(v i )."
"The two factors are incorporated into the affinity weight between sentences and the new sentence-to-sentence affinity weight is denoted as f(v i , v j |doc(v i ), doc(v j )), which is conditioned on the two docu-ments containing the two sentences."
"The new con-ditional affinity weight is computed by linearly combining the affinity weight conditioned on the first document (i.e. f(v i ,v j |doc(v i ))) and the affinity weight conditioned on the second document (i.e. f(v i ,v j |doc(v j )))."
"Formally, the conditional affinity weight is computed as follows to incorporate the two factors: where λ∈[0,1] is the combination weight control-ling the relative contributions from the first docu-ment and the second document."
"Note that usually f(v i , v j |doc(v i ), doc(v j )) is not equal to f(v j , v i |doc(v j ), doc(v i )), but the two scores are equal when λ is set to 0.5."
"Various methods can be used to evaluate the document importance and the sentence-document correlation, which will be described in next sec-tions."
"The new affinity matrix M * is then constructed based on the above conditional sentence-to-sentence affinity weight. ⎧f (v i ,v j |doc(v i ),doc(v j )), if if v i and v j isconnected M *i,j =⎪⎨ (7) and i ≠ j ⎪ ⎩0, otherwise"
"Likewise, M * is normalized to M~ * and the itera-tive computation as in Equation (4) is then based on M~ * ."
The transition matrix in the Markov chain is then denoted by A * = µ M~ *T + (1 − µ ) reer T and |V| the sentence scores is obtained by the principle eigenvector of the new transition matrix A * .
The function π(doc(v i )) aims to evaluate the impor-tance of document doc(v i ) in the document set D.
"The following three methods are developed to evaluate the document importance. π 1 : It uses the cosine similarity value between the document and the whole document set as the importance score of the document 3 : π 1 (doc(v i )) = sim cosine (doc(v i ), D) (8) π 2 : It uses the average similarity value between the document and any other document in the document set as the importance score of the docu-ment: ∑ sim cosine (doc(v i ),d&apos;) (9) π 2 (doc(v i )) = d ∈&apos; D and d&apos;≠doc | (v D | −1 i ) π 3 : It constructs a weighted graph between docu-ments and uses the PageRank algorithm to com-pute the rank scores of the documents as the importance scores of the documents."
The link weight between two documents is computed using the cosine measure.
The equation for iterative computation is the same with Equation ([Footnote_4]).
"The function ω(v i , doc(v i )) aims to evaluate the correlation between sentence v i and its document doc(v i )."
The following four methods are developed to compute the strength of the correlation.
"The first three methods are based on sentence position in the document, under the assumption that the first sen-tences in a document are usually more important than other sentences."
"The last method is based on the content similarity between the sentence and the document. ω 1 : The correlation strength between sentence v i and its document doc(v i ) is based on the position of the sentence as follows: ω 1 (v i ,doc(v i )) = ⎧⎨1 if pos(v i ) ≤ [Footnote_3] (10) ⎩0.5 Otherwise where pos(v i ) returns the position number of sen-tence v i in its document."
3 A document set is treated as a single text by concatenating all the document texts in the set.
"For example, if v i is the first sentence in its document, pos(v i ) is 1. ω 2 : The correlation strength between sentence v i and its document doc(v i ) is based on the position of the sentence as follows: pos(v i ) −1 ω 2 (v i , doc(v i )) =1− sen_ count(doc(v (11) i )) where sen_count(doc(v i )) returns the total number of sentences in document doc(v i ). ω 3 : The correlation strength between sentence v i and its document doc(v i ) is based on the position of the sentence as follows: 1 ω 3 (v i ,doc(v i )) = 0.5 + pos(v (12) i ) +1 ω 4 : The correlation strength between sentence v i and its document doc(v i ) is based on the cosine similarity between the sentence and the document: ω 4 (v i ,doc(v i )) = sim cosine (v i ,doc(v i )) (13)"
"Generic multi-document summarization has been one of the fundamental tasks[REF_CITE]4 and[REF_CITE]5 (i.e. task 2[REF_CITE]and task 2[REF_CITE]), and we used the two tasks for evalua-tion."
The documents were news articles collected from TREC-9.
The sen-tences in each article have been separated and the sentence information has been stored into files.
The summary of the two datasets are shown in Ta-ble 1.
"We used the ROUGE[REF_CITE]toolkit (i.e. ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation."
"It measured summary quality by counting overlap-ping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary."
ROUGE-N was an n-gram recall measure computed as follows: ∑ ∑ Count match (n − gram) (14)
"ROUGE − N = S∈{Ref Sum ∑ }n-gram∈S ∑ Count(n − gram) S∈{Ref Sum}n-gram∈S where n stood for the length of the n-gram, and Count match (n-gram) was the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries."
Count(n-gram) was the number of n-grams in the reference summaries.
"ROUGE toolkit reported separate scores for 1, 2, 3 and 4-gram, and also for longest common subse-quence co-occurrences."
"Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy. 2003)."
"We showed three of the ROUGE metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on weighted long-est common subsequence, weight=1.2)."
"In order to truncate summaries longer than length limit, we used the “-l” option in ROUGE toolkit."
We also used the “-m” option for word stemming.
"In the experiments, the combination weight λ for the proposed summarization model is typically set to 0.5 without tuning, i.e. the two documents for two sentences have equal influence on the summa-rization process."
"Note that after the saliency scores of sentences have been obtained, a greedy algo-rithm[REF_CITE]is applied to remove redundancy and finally choose both informative and novel sentences into the summary."
The algo-rithm is actually a variant version of the MMR al-gorithm[REF_CITE].
"The proposed document-based graph model (de-noted as DGM) with different settings is compared with the basic graph-based Model (denoted as GM), the top three performing systems and two baseline systems[REF_CITE]respectively."
"The top three systems are the systems with highest ROUGE scores, chosen from the performing sys-tems on each task respectively."
The lead baseline and coverage baseline are two baselines employed in the generic multi-document summarization tasks[REF_CITE].
"The lead baseline takes the first sentences one by one in the last document in the collection, where documents are assumed to be ordered chronologically."
And the coverage baseline takes the first sentence one by one from the first document to the last document.
Tables 2 and 3 show the comparison results[REF_CITE]respectively.
"In Table 1, SystemN, SystemP and System T are the top three performing systems[REF_CITE]."
"In Table 2,[REF_CITE]are the top three per-forming systems[REF_CITE]."
"The document-based graph model is configured with different settings (i.e. π 1 -π 3 , ω 1 -ω 4 )."
"For example, DGM(π 1 +ω 1 ) refers to the DGM model with π 1 to evaluate the document importance and ω 1 to evalu-ate the correlation between a sentence and its docu-ment."
"Seen from the tables, the proposed document-based graph model with different settings can out-perform the basic graph-based model and other baselines over almost all three metrics on both"
"The results demonstrate the good effectiveness of the proposed model, i.e. the incorporation of document impact does benefit the graph-based summarization model."
It is interesting that the three methods for comput-ing document importance and the four methods for computing the sentence-document correlation are almost as effective as each other on the[REF_CITE]dataset.
"However, π 1 does not perform as well as π 2 and π 3 , and ω 1 and ω 4 does not perform as well as ω 2 and ω 3 on the[REF_CITE]dataset."
"In order to investigate the relative contributions from the two documents for two sentences to the summarization performance, we varies the combi-nation weight λ from 0 to 1 and Figures 3-6 show the ROUGE-1 and ROUGE-W curves[REF_CITE]respectively."
The similar ROUGE-2 curves are omitted here.
"We can see from the figures that the proposed document-based graph model with different set-tings can almost always outperform the basic graph-based model, with respect to different values of λ."
The results show the robustness of the pro-posed model.
"We can also see that for most set-tings of the propose model, very large values or very small values of λ can deteriorate the summari-zation performance, i.e. both the first document and the second document in the computation of the conditional affinity weight between sentences have great impact on the summarization performance."
This paper examines the document impact on the graph-based model for multi-document summari-zation.
The document-level information and the sentence-to-document relationship are incorporated into the graph-based ranking algorithm.
The ex-perimental results[REF_CITE]demonstrate the good effectiveness of the proposed model.
"In this study, we directly make use of the coarse-grained document-level information."
"Actually, a document can be segmented into a few subtopic passages by using the TextTiling algorithm[REF_CITE], and we believe the subtopic passage is more fine-grained than the original document."
"In future work, we will exploit this kind of subtopic-level information to further improve the summarization performance."
Information of interest to users is often dis-tributed over a set of documents.
Users can specify their request for information as a query/topic – a set of one or more sentences or questions.
Producing a good summary of the relevant information relies on understand-ing the query and linking it with the associ-ated set of documents.
To “understand” the query we expand it using encyclopedic knowl-edge in Wikipedia.
The expanded query is linked with its associated documents through spreading activation in a graph that represents words and their grammatical connections in these documents.
The topic expanded words and activated nodes in the graph are used to produce an extractive summary.
The method proposed is tested on the DUC summariza-tion data.
"The system implemented ranks high compared to the participating systems in the DUC competitions, confirming our hypothesis that encyclopedic knowledge is a useful addi-tion to a summarization system."
Topic-driven summarization reflects a user-based summarization task: from a set of documents de-rive a summary that contains information on a spe-cific topic of interest to a user.
"Producing a good summary relies on “understanding” the user’s infor-mation request, and the documents to be summa-rized."
It is commonly agreed that the verbal part of a text provides pointers to a much larger body of knowledge we assume the listener has.
"An Amer-ican citizen, for example, when told There will be fireworks on July 4 th , understands that there will be a celebration involving fireworks on the occasion of the U.S. Independence Day."
"Understanding an utterance implies lexical, common-sense and ency-clopedic knowledge."
"Lexical knowledge is usually incorporated in systems through machine readable dictionaries, wordnets or thesauri."
"Common-sense and encyclopedic knowledge were harder to capture, but recently Wikipedia has opened the possibility of accessing such knowledge on a large scale, and in numerous languages."
To “understand” a user’s information request – one or more sentences or questions (the topic of the summary) – summarization systems try to ex-pand it.
This will provide later stages of process-ing with more keywords/keyphrases for retrieving from the documents relevant fragments.
In this pa-per we experiment with Wikipedia for topic expan-sion.
"The body of research involving Wikipedia as a source of knowledge is growing fast, as the NLP community finds more and more applications of this useful resource: it is used to acquire knowl-edge[REF_CITE]; to induce taxonomies and compute semantic relat-edness (Ponzetto &amp;[REF_CITE]; 2007a); as a source of features for text classification (Gabrilovich &amp;[REF_CITE]) and for answering questions[REF_CITE]."
The work pre-sented here uses hyperlinks in Wikipedia articles to expand keywords and keyphrases extracted from the query.
"Ambiguous words are disambiguated using the context provided by the query. “Understanding” the documents to be summa-rized implies identifying the entities mentioned, how they are connected, and how they are related to the entities in the topic."
"For this, we start again from the topic, and spread an activation signal in a large graph that covers all documents for this topic – nodes are words/named entities in the texts, links are gram-matical relations."
"This way we cross from the topic to the documents, and combine information which is important in the topic with information which is important and relevant in the documents."
"We take the most highly activated nodes as additional topic expansions, and produce an extractive summary by choosing from the sentences that connect the topic expansion words in the large document graph."
"The experiments confirm that Wikipedia is a source of useful knowledge for summarization, and that further expanding the topic within the associ-ated set of documents improves the summarization results even more."
We compare the performance of the summarization system to that of participating systems in the DUC competitions.
"The system we describe ranks 2 nd , 9 th and 5 th in terms of ROUGE-SU4 on the[REF_CITE]data respectively."
"While the recent exponential increase in the amount of information with which we must cope makes summarization a very desirable tool in the present, summarization is not a novel task."
"Nowadays, summarization methods try to incorpo-rate tools, methodologies and resources developed over the past decades."
"The NIST organized com-petitions under the Document Understanding Con-ferences – DUC (since 2008, Text Analysis Confer-ence (TAC)) [URL_CITE] events provide a forum for the compar-ison of a variety of approaches, ranging from knowl-edge poor –[REF_CITE]rely exclusively on a parser, without any additional sources of informa-tion – to knowledge rich and complex –"
"GISTexter[REF_CITE]combines question answering, textual entailment, topic signature modules and a va- riety of knowledge sources for summarization."
"The most frequently used knowledge source in NLP in general, and also for summarization, is WordNet[REF_CITE]."
Barzilay &amp;[REF_CITE]use WordNet to model a text’s content rel-ative to a topic based on lexical chains.
The sen-tences intersected by the most and strongest chains are chosen for the extractive summary.
Alterna-tive sources for query expansion and document pro-cessing have also been explored.
"Amini &amp;[REF_CITE]use the documents to be summarized them-selves to cluster terms, and thus expanding the query “internally”."
More advanced methods for query ex-pansion use “topic signatures” – words and gram-matically related pairs of words that model the query and even the expected answer from sets of docu-ments marked as relevant or not (Lin &amp;[REF_CITE]).
Graph-based methods for text summarization work usually at the level of sentences (Erkan &amp;[REF_CITE]; Mihalcea &amp;[REF_CITE]).
"Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to deter-mine the sentences that are the most salient from a collection of documents and closest to a given topic."
"At the word level,[REF_CITE]build a document graph using subject-verb-object triples, semantic normalization and coreference resolution."
"They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the network, and use these as attribute values in a machine learning algorithm, where the attribute that is learned is whether the node should appear in the final summary or not."
Annotations for train-ing come from human produced summaries.
Mo-hamed &amp;[REF_CITE]incrementally build a graph for a document collection by combining graph-representations of sentences.
Links between entities in a sentence can be isa (within an NP) or related to (between different phrases in a sen-tence).
"Nodes and relations are weighted according to their connectivity, and sentence selection for the final summary is based on the most highly connected nodes."
"Ye &amp;[REF_CITE]build an extractive sum-mary based on a concept lattice, which captures in a hierarchical structure co-occurrences of concepts among sentences."
"Nodes higher in this structure cor-respond to frequently co-occurring terms, and are assumed to be more representative with respect to the document topic."
"Mani &amp;[REF_CITE]build a “chronologi-cal” graph, in which sentence order is respected and each occurrence of a concept is a separate node."
"Edges between nodes cover several types of rela-tions: adjacency (ADJ); identity – instance of the same word (SAME); other semantic links, in par-ticular synonymy and hypernymy; PHRASE links connect components of a phrase; NAME indicate named entities; COREF link coreferential name in-stances."
"Among other things, they identify regions of the text salient to a user’s query, based on spread-ing activation starting from query words in this doc-ument graph."
Spreading activation was introduced in the 60s and 70s to model psychological processes of memory activation in humans ([REF_CITE]; Collins &amp;[REF_CITE]).
In this approach we use Wikipedia as a source of knowledge for related concepts – the texts of hyper-links in an article describing a concept are taken as its related concepts.
The query is further expanded by using spreading activation to move away from the topic in a large graph that covers all documents for a given topic.
From the nodes thus reached we se-lect using a PageRank algorithm the ones that are most important in the documents.
"We study the im-pact of a decay parameter which controls how far to move from the topic, and the number of highest ranked nodes to be added to the expanded topic."
The summary is built based on word associations in the documents’ graph.
"In DUC topic-driven multi-document summariza-tion, the topic has a title, an ID that links it to a set of documents, and one or more sentences and/or ques-tions, as illustrated in Figure 1."
Topic processing is done in several steps: 1.
Preprocessing: Produce the dependency pair representation of the topics using the Stanford Parser 2 .
"Pairs that have closed-class words are fil-tered out, and the remaining words are lemmatized [Footnote_3] ."
3 Using XTAG[URL_CITE]database //[URL_CITE]morph-1.5.tar.gz.
"We extract named entities (NEs), as the parser works at the word level."
In the dependency pairs we replace an NE’s fragments with the complete NE. [Footnote_2]a.
"Query expansion with Wikipedia: Extract all open-class words and NEs from the topic, and expand them using Wikipedia articles whose titles are these words or phrases."
For each Wikipedia article we extract as related concepts the texts of the hyperlinks in the first para-graph (see Figure 2 [Footnote_4] ).
"4 The left side shows the first paragraph as it appears on the page, the right side shows the corresponding fragment from the source file, with the annotations specific to Wikipedia."
"The reason for not including links from the entire article body is that apart from the first paragraph, which is more focused, often times hyperlinks are included whenever the under-lying concept appears in Wikipedia, without it being particularly relevant to the current article."
"To expand a word (or NE) W from the query, we search for an article having W as the title, or part of the title. 1."
"If one exact match is found (e.g. Southern Poverty Law Center), extract the related con-cepts for this article. 2."
"If several exact or partial matches are found, use the larger context of the query to narrow down to the intended meaning."
"For example, Turkey – referring to the country – appears in several topics in the[REF_CITE]data."
"There are multiple entries for “Turkey” in Wikipedia – for the country, the bird, cities with this name in the U.S. among others."
"We use a Lesk-like measure, and compute the overlap between the topic query and the set of hyperlinks in the first paragraph[REF_CITE]."
We choose the ex-pansion for the entry with the highest overlap.
"If the query context does not help in disam-biguation, we use the expansions for all partial matches that tie for the highest overlap. 3."
"If an article with the required name does not exist, the word will not be expanded. 2b."
Query expansion with WordNet:
"Extract all nouns and NEs from the topic, and expand them with hypernyms, hyponyms and antonyms in Word-Net 2.0: 1."
"If an word (or NE) W from the query corre-sponds to an unambiguous entry in WordNet, expand that entry. 2."
"If W has multiple senses, choose the sense(s) which have the highest overlap with the query."
"To compute overlap, for a sense we take its ex-pansions (one step hypernyms, hyponyms and antonyms) and the words from the definition. 3."
"If W has no senses in WordNet, the word will not be expanded. 3."
Filter the list of related concepts: keep only terms that appear in the docu-ment collection for the current topic.
"Table 1 includes the expansions obtained from Wikipedia and from WordNet respectively for a number of words in topics from the[REF_CITE]col-lection. mining is a specific activity, involving a lim-ited set of materials."
"While such connections cannot be retrieved through hypernym, meronym or other semantic relations in WordNet, they are part of ency-clopedic knowledge, and can be found in Wikipedia. flight is a more general concept – there are spe-cific types of flight, which appear as hyponyms in WordNet, while in Wikipedia it is more gener-ally described as the motion of an object through air, which does not provide us with interesting re-lated concepts. status is a very general concept, and rather vague, for which neither WordNet nor Wikipedia can provide very useful information."
"Fi-nally, Wikipedia is rich in named entities, which are not in the scope of a semantic lexicon."
"WordNet does contain named entities, but not on the scale on which Wikipedia does."
"This difference comes from the fact that with Wikipedia it is mostly the NEs that are expanded, whereas with WordNet the common nouns, which are more numerous in the topics."
"The overlap between the two sets of expan-sions is 48 words (0.046 relative to Wikipedia ex-pansions, 0.019 relative to WordNet)."
Concepts related to the ones in the topic provide a good handle on the documents to summarize – they indicate parts of the document that should be in-cluded in the summary.
"It is however obvious that the summary should contain more than that, and this information comes from the documents to be summarized."
Amini &amp;[REF_CITE]have shown that expanding the query within the set of docu-ments leads to good results.
"Following this idea, to find more relevant concepts we look for words/NEs which are related to the topic, and at the same time important in the collection of documents for the given topic."
The methods described in this section are applied on a large graph that covers the entire document collection for one topic.
"The documents are processed in a similar way to the query – parsed with the Stanford Parser, output in dependency rela-tion format, lemmatized using XTag’s morpholog-ical data file."
"The graph consists of nodes corre-sponding to lemmatized words and NEs in the doc-uments, and edges correspoding to grammatical de-pendency relations."
"To find words/NEs related to the topic we spread an activation signal starting from the topic words and their expansions (in a manner similar to (Mani &amp;[REF_CITE]), and using an algorithm inspired[REF_CITE]), which are given a node weight of 1."
"As we traverse the graph starting from these nodes, the signal is propagated by assigning a weight to each edge and each node traversed based on the signal strength."
"The signal strength diminishes with the distance from the node of origin depending on a signal decay parameter, according to the formula: w n (N 0 ) = 1; w n (N t ) s t = (1 − decay) ∗ Out(N t ); w n (N t+1 ) = s t ; w e (N t , N t+1 ) t+1 = w e (N t , N t+1 ) t + s t ; where N t is the current node; N t+1 is the node we are moving towards; w n (N t ) is the weight of node N t ; s t is the signal strength at step t; Out(N t ) is the number of outgoing edges from node N t ; w e (N t ,N t+1 ) t is the weight of the edge between N t and N t+1 at time t (i.e., before actually travers-ing the edge and spreading the activation from N t ); w e (N t ,N t+1 ) t+1 is the weight of the edge after spreading activation."
"The weight of the edges is cu-mulative, to gather strength from all signals that pass through the edge."
Activation is spread sequentially from each node in the (expanded) topic.
"The decay parameter is used to control how far the influence of the starting nodes should reach – the lower the decay, the farther the signal can reach."
"The previous step has assigned weights to edges in the graph, such that higher weights are closer to topic and/or topic expanded words."
"After this ini-tialization of the graph, we run a PageRank algo-rithm (Brin &amp;[REF_CITE]) to determine more impor-tant nodes."
"By running this algorithm after initializ-ing the graph edge weights, from the nodes that are closer to topic and topic expanded words we boost those that are more important in the documents."
The starting point of the PageRank algorithm is the graph with weighted edges obtained in the pre-vious step.
The node weights are initialized with 1 (the starting value does not matter).
"Analysis of the documents graph for several topics has revealed that there is a large highly interconnected structure, and many disconnected small (2-3 nodes) fragments."
Page Rank will run on this dense core structure.
The PageRank algorithm is guaranteed to converge if the graph is aperiodic and irreducible (Grimmett &amp;[REF_CITE]).
Aperiodicity implies that the greatest common divisor of the graph’s cycles is 1 – this condition is met.
"Irreducibility of the graph means that it has no leaves, and there are no two nodes with the same set of neighbours."
"The rem-edy in such cases is to connect each leaf to all other nodes in the graph, and conflate nodes with the same set of neighbours."
"Once the graph topology meets the PageRank convergence conditions, we run the algorithm."
The original formula for computing the rank of a node at each iteration step is:
"P R(n i ) = 1 − d + d X P R(n j ) N n j ∈Adj ni Out(n j ) where n i is a node, d is the damping factor (usually d = 0.85 and this is the value we use as well), N is the number of nodes in the graph, PR(n i ) is the rank of node n i , Adj n i is the set of nodes adjacent to n i , and Out(n j ) is the number of outgoing edges from n j (our graph is non-directed, so this number is the total number of edges with one end in n j )."
"We adjust this formula to reflect the weights of the edges, and the version used is the following:"
"P R(n i ) = 1 − d + d X P R(n j )w out (n j ); N n j ∈Adj ni w out (n j ) = X w e (n k , n j ) n k ∈Adj nj"
"In Table 2 we show examples of top ranked nodes for several topics, extracted with this algorithm."
"The words in italics are keywords/phrases from the topic query, and the top ranked nodes are listed in decreas-ing order of their rank."
"The summarization method implemented is based on the idea that the entities or events mentioned in the query are somehow connected to each other, and the documents to be summarized contain informa-tion that allows us to make these connections."
"We use again the graph for all the documents in the col-lection related to one topic, built using the depen-dency relation representation of the texts."
"The nodes in this graph are words/NEs, and the links are gram-matical relations."
We extract from this graph the subgraph that cov-ers connections between all open class words/NEs in the topic or expanded topic query.
Each edge in the extracted subgraph corresponds to a grammati-cal relation in a sentence of a document.
"We col-lect all sentences thus represented in the subgraph, and rerank them based on the number of edges they cover, and the occurrence of topic or expanded topic terms."
We use the following formula to compute a sentence score:
"Score(S) = topicWords ∗ w word + expandedWords ∗ w expandedWord + topRankedWords ∗ w topRankedWord + edgesCovered ∗ w subgraphEdge + depRelation ∗ w depRelation w word , w expandedWord , w topRankedWord , w subgraphEdge and w depRelation are weight pa-rameters that give different importance to exact words from the topic, expanded words, top ranked words and edges covered in the extracted subgraph."
During all experiments these parameters are fixed. [Footnote_5]
"5 The values used were set following a small number of ex-periments[REF_CITE]data, as the purpose was not to tune the system for best performance, but rather to study the impact of more interesting parameters, in particular expansion type, decay and node ranking. The values used are the following: w word = 5, w expandedWord = 2.5, w topRankedWord = 0.5, w subgraphEdge = 2, w depRelation = 0."
"To form the summary we traverse the ranked list of sentences starting with the highest ranked one, and add sentences to a summary, or delete from the existing summary, based on a simple lexical overlap measure."
"We stop when the desired summary length is reached –[REF_CITE]–2007, 250 words (last sentence may be truncated to fill the summary up to the allowed word limit)."
"Experiments are run[REF_CITE]main summa-rization task data, for the last experiment we used the[REF_CITE]data as well."
"Perfor-mance is evaluated in terms of ROUGE-2, ROUGE-SU4 and BE recall, following the methodology and using the same parameters as in the DUC summa-rization events."
"We analyze several types of topic expansion: no expansion, WordNet, Wikipedia, and within doc-ument collection expansion using spreading acti-vation and Page Rank."
The spreading activation method has several parameters whose values must be determined.
"We first compare the summaries produced with no topic expansion, WordNet (WN) and Wikipedia (Wiki) respectively."
Table 3 shows the results in terms of ROUGE and BE recall on the[REF_CITE](main) data.
"Word sense disambiguation (WSD) for expansion with WordNet did not work very well, as evidenced by the lower results for disambiguated expansion (WN with WSD) compared to the non- disambiguated one."
A better disambiguation algo-rithm may reverse the situation.
Expanding a topic only with Wikipedia hyperlinks gives the best re-sults.
"At the document level, the results are not as clear cut."
"Figure 3 shows a comparison in terms of ROUGE-SU4 recall scores at the document level of the Wikipedia and WN (no WSD) expansion meth-ods, sorted in increasing order of the Wikipedia-based expansion scores."
The points are connected to allow the reader to follow the results for each method.
"Because the overlap between Wikipedia and WordNet expanded queries was very low, we ex-pected the two types of expansion to be complemen-tary, and the combination to give better results than either expansion by itself."
"An analysis of results for each document with the three expansion meth-ods – Wikipedia, WordNet, and their combination – showed that the simple combination of the expanded words cannot take advantage of the situations when one of the two methods performs better."
"In future work we will explore how to detect, based on the words in the query, which type of expansion is best, and how to combine them using a weighting scheme."
"We choose the best configuration from above (Wikipedia expansion), and further expand the query through spreading activation and PageRank."
"This new type of expansion has two main parameters which influence the summarization outcome: num-ber of top ranked nodes to add to the topic expan-sion, and the decay of the spreading activation algo-rithm."
The decay parameter determines how far the in-fluence of the starting nodes (words from query or Wikipedia-expanded query) should be felt.
"The re-sults in Figure 4 – for decay values 0.1, 0.5, 0.95, 0.99, 0.999, 0.9999, 1 – indicate that faster decay (reflected through a higher decay value) keeps the summary more focused around the given topic, and leads to better results. [Footnote_6] For a high enough decay – and eventually a decay of 1 – the weights of the edges become extremely small, and due to real num-ber representation in memory, practically 0."
"6 During this set of experiments all other parameters are fixed, the number of top ranked nodes added to the topic ex-pansion is 30."
"In this situation PageRank has no effect, and all nodes have the same rank."
"We fix the decay parameter to 0.9999, and we study the impact of the number of top nodes chosen after ranking with PageRank."
Figure 5 shows the re-sults when the number of top ranked nodes chosen varies.
Adding highly ranked nodes benefits the per-formance of the system only up to a certain limit.
"From the values we tested, the best results were ob-tained when adding 40 nodes to the expanded topic."
The best system configuration from the ones ex-plored [Footnote_7] is run on the[REF_CITE]and 2007 (main) data.
"7 Wikipedia expansion + 40 top nodes after spreading acti-vation and PageRank, decay = 0.9999, w expandedWord = 3.5, w depRelation = 1, the other parameters have the same values as before."
The performance and rank (in parenthe-ses) compared to participating systems is presented in Table 4.
The experiments conducted within the summa-rization framework of the Document Understand-ing Conference have confirmed that encyclopedic knowledge extracted from Wikipedia can benefit the summarization task.
"Wikipedia articles are a source of relevant related concepts, that are useful for ex-panding a summarization query."
"Furthermore, in-cluding information from the documents to be sum-marized by choosing relevant concepts – based on closeness to topic keywords and relative importance – improves even more the quality of the summaries, judged through ROUGE-2, ROUGE-SU4 and BE recall scores, as it is commonly done in the DUC competitions."
"The topic expansion methods ex-plored lead to high summarization performance – ranked 2 nd , 9 th and 5 th[REF_CITE]and 2007 respectively according to ROUGE-SU4 scores – compared to (more than 30) DUC participating systems."
The graph representation of the documents is cen-tral to the summarization method we described.
"Be-cause of this, we plan to improve this representation by collapsing together coreferential nodes and clus-tering together related concepts, and verify whether such changes impact the summarization results, as we expect they would."
"Being able to move away from the topic within the set of documents and discover new relevant nodes is an important issue, especially from the point of view of a new summarization style – updates."
"In update summaries the starting point is a topic, which a summarization system must track in consecutive sets of documents."
We can adjust the spreading activation parameters to how far a new set of documents is from the topic.
Future work includes testing the spreading activation and page ranking method in the context of the update summarization task and exploring methods of extracting related concepts from the full text of Wikipedia articles.
"Acknowledgments This work was funded by the Klaus Tschira Foundation, Heidelberg, Germany."
We thank the anonymous reviewers for insightful comments and suggestions.
In this paper we describe research on sum-marizing conversations in the meetings and emails domains.
"We introduce a conver-sation summarization system that works in multiple domains utilizing general conversa-tional features, and compare our results with domain-dependent systems for meeting and email data."
"We find that by treating meet-ings and emails as conversations with general conversational features in common, we can achieve competitive results with state-of-the-art systems that rely on more domain-specific features."
Our lives are increasingly comprised of multimodal conversations with others.
"We email for business and personal purposes, attend meetings in person and remotely, chat online, and participate in blog or forum discussions."
It is clear that automatic summa-rization can be of benefit in dealing with this over-whelming amount of interactional information.
Au-tomatic meeting abstracts would allow us to prepare for an upcoming meeting or review the decisions of a previous group.
Email summaries would aid corpo-rate memory and provide efficient indices into large mail folders.
"When summarizing in each of these domains, there will be potentially useful domain-specific fea-tures – e.g. prosodic features for meeting speech, subject headers for emails – but there are also un-derlying similarites between these domains."
"They are all multiparty conversations, and we hypothe-size that effective summarization techniques can be designed that would lead to robust summarization performance on a wide array of such conversation types."
Such a general conversation summarization system would make it possible to summarize a wide variety of conversational data without needing to develop unique summarizers in each domain and across modalities.
"While progress has been made in summarizing conversations in individual domains, as described below, little or no work has been done on summarizing unrestricted, multimodal conversa-tions."
"In this research we take an extractive approach to summarization, presenting a novel set of conver-sational features for locating the most salient sen-tences in meeting speech and emails."
"We demon-strate that using these conversational features in a machine-learning sentence classification framework yields performance that is competitive or superior to more restricted domain-specific systems, while having the advantage of being portable across con-versational modalities."
"The robust performance of the conversation-based system is attested via several summarization evaluation techniques, and we give an in-depth analysis of the effectiveness of the indi-vidual features and feature subclasses used."
"In this section we give a brief overview of previous research on meeting summarization and email sum-marization, respectively."
"Among early work on meeting summarization,[REF_CITE]implemented a modified version of the Maximal Marginal Relevance algorithm[REF_CITE]applied to speech tran-scripts, presenting the user with the n best sentences in a meeting browser interface."
"Though rele-vance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker infor-mation linking and question/answer detection."
"More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information."
"Murray et al. (2005a; 2005b) compared purely textual summarization approaches with feature-based approaches incorporating prosodic features, with human judges favoring the feature-based ap-proaches."
"In subsequent work (2006; 2007), they began to look at additional speech-specific char-acteristics such as speaker status, discourse mark-ers and high-level meta comments in meetings, i.e. comments that refer to the meeting itself."
"Galley found that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance accord-ing to Pyramid evaluation."
Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads.
"They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, which scores words highly if they are frequent in the document but rare across all documents), features that consider the thread to be a sequence of turns (e.g. the position of the turn in the thread), and email-specific features such as number of recipients and subject line similarity."
"A single node in the graph represents an email fragment, a portion of the email that behaves as a unit in a fine-grain representation of the conversation structure."
A fragment sometimes consists of an entire email and sometimes a portion of an email.
"For example, if a given email has the structure"
"A &gt; B C where B is a quoted section in the middle of the email, then there are three email fragments in total: two new fragments A and C separated by one quoted fragment B. Sentences in a fragment are weighted according to the Clue Word Score (CWS) measure, a lexical cohesion metric based on the recurrence of words in parent and child nodes."
"In subsequent work,[REF_CITE]determined that subjectivity detection (i.e., whether the sentence contains sentiments or opinions from the author) gave additional improvement for email thread summaries."
"Also on the Enron corpus,[REF_CITE]com-pared Collective Message Summarization (CMS) to Individual Message Summarization (IMS) and found the former to be a more effective technique for summarizing email data."
"CMS essentially treats thread summarization as a multi-document summa-rization problem, while IMS summarizes individual emails in the thread and then concatenates them to form a thread summary."
"In our work described below we also address the task of thread summarization as opposed to sum- marization of individual email messages, following Carenini et al. and the CMS approach of Zajic et al."
"In this section we describe the classifier employed for our machine learning experiments, the corpora used, the relevant summarization annotations for each corpus, and the evaluation methods employed."
Our approach to extractive summarization views sentence extraction as a classification problem.
"For all machine learning experiments, we utilize logistic regression classifiers."
"This choice was partly moti-vated by our earlier summarization research, where logistic regression classifiers were compared along-side support vector machines (SVMs)[REF_CITE]."
"The two classifier types yielded very similar results, with logistic regression classifiers being much faster to train and thus expediting fur-ther development."
The liblinear toolkit [URL_CITE] implements simple feature subset selection based on the F statistic[REF_CITE].
"For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization."
The Enron email corpus [URL_CITE] is a collection of emails released as part of the investigation into the Enron corporati[REF_CITE].
"It has become a popular corpus for NLP research (e.g.[REF_CITE]) due to being realistic, naturally-occurring data from a corporate environ-ment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread."
"The annotators were asked to select 30% of the sentences in a thread, subsequently la-beling each selected sentence as either ’essential’ or ’optional.’"
Essential sentences are weighted three times as highly as optional sentences.
"A sentence score, or GSValue, can therefore range between 0 and 15, with the maximum GSValue achieved when all five annotators consider the sentence essential, and a score of 0 achieved when no annotator selects the given sentence."
"For the purpose of training a bi-nary classifier, we rank the sentences in each email thread according to their GSValues, then extract sen-tences until our summary reaches 30% of the to-tal thread word count."
We label these sentences as positive instances and the remainder as the negative class.
"Because the amount of labeled data available for the Enron email corpus is fairly small, for our classi-fication experiments we employ a leave-one-out pro-ceedure for the 39 email threads."
The labeled data as a whole total just under 1400 sentences.
"For our meeting summarization experiments, we use the scenario portion of the AMI corpus[REF_CITE]."
The corpus consists of about 100 hours of recorded and annotated meetings.
"In the scenario meetings, groups of four participants take part in a series of four meetings and play roles within a ficti-tious company."
"While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural."
"For this corpus, annotators wrote abstract sum-maries of each meeting and extracted transcript dia-logue act segments (DAs) that best conveyed or sup-ported the information in the abstracts."
"A many-to-many mapping between transcript DAs and sen-tences from the human abstract was obtained for each annotator, with three annotators assigned to each meeting."
"It is possible for a DA to be extracted by an annotator but not linked to the abstract, but for training our binary classifiers, we simply consider a dialogue act to be a positive example if it is linked to a given human summary, and a negative example otherwise."
This is done to maximize the likelihood that a data point labeled as “extractive” is truly an informative example for training purposes.
"The AMI corpus contains automatic speech recognition (ASR) output in addition to manual meeting transcripts, and we report results on both transcript types."
"The ASR output was provided by the AMI-ASR team[REF_CITE], and the word error rate for the AMI corpus is 38.9%."
"For evaluating our extractive summaries, we imple-ment existing evaluation schemes from previous re-search, with somewhat similar methods for meet-ings versus emails."
These are described and com-pared below.
We also evaluate our extractive classi-fiers more generally by plotting the receiver operator characteristic (ROC) curve and calculating the area under the curve (AUROC).
This allows us to gauge the true-positive/false-positive ratio as the posterior threshold is varied.
We use the differing evaluation metrics for emails versus meetings for two primary reasons.
"First, the differing summarization annotations in the AMI and Enron corpora naturally lend themselves to slightly divergent metrics, one based on extract-abstract links and the other based on the essen-tial/option/uninformative distinction."
"Second, and more importantly, using these two metrics allow us to compare our results with state-of-the-art results in the two fields of speech summarization and email summarization."
In future work we plan to use a sin-gle evaluation metric.
To evaluate meeting summaries we use the weighted f-measure metric[REF_CITE].
This evaluation scheme relies on the multiple human annotated summary links described in Section 3.2.2.
"Both weighted precision and recall share the same numerator num = X X L(s M N i , a j ) (1) i=1 j=1 where L(s i ,a j ) is the number of links for a DA s i in the machine extractive summary according to annotator a i , M is the number of DAs in the ma-chine summary, and N is the number of annotators."
Weighted precision is defined as: num precision = (2) N · M and weighted recall is given by num recall = P i=1
"P (3) N L(s i , a j ) O j=1 where O is the total number of DAs in the meeting, N is the number of annotators, and the denominator represents the total number of links made between DAs and abstract sentences by all annotators."
The weighted f-measure is calculated as the harmonic mean of weighted precision and recall.
The intuition behind weighted f-score is that DAs that are linked multiple times by multiple annotators are the most informative.
"For evaluating email thread summaries, we follow[REF_CITE]by implementing their pyra-mid precision scheme, inspired by Nenkova’s pyra-mid scheme (2004)."
"In Section 3.2.1 we introduced the idea of a GSValue for each sentence in an email thread, based on multiple human annotations."
We can evaluate a summary of a given length by com-paring its total GSValues to the maximum possible total for that summary length.
"For instance, if in a thread the three top scoring sentences had[REF_CITE]12 and 12, and the sentences selected by a given automatic summarization method had GSVal-ues of 15, 10 and 8, the pyramid precision would be 0.85."
"Pyramid precision and weighted f-score are simi-lar evaluation schemes in that they are both sentence based (as opposed to, for example, n-gram based) and that they score sentences based on multiple hu-man annotations."
Pyramid precision is very simi-lar to equation 3 normalized by the maximum score for the summary length.
For now we use these two slightly different schemes in order to maintain con-sistency with prior art in each domain.
"In our conversation summarization approach, we treat emails and meetings as conversations com-prised of turns between multiple participants."
"We follow[REF_CITE]in working at the finer granularity of email fragments, so that for an email thread, a turn consists of a single email fragment in the exchange."
"For meetings, a turn is a sequence of dialogue acts by one speaker, with the turn bound-aries delimited by dialogue acts from other meet-ing participants."
The features we derive for summa-rization are based on this view of the conversational structure.
We calculate two length features.
"For each sen-tence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word-count feature normalized by the longest sentence in the turn (SLEN2)."
Sentence length has previously been found to be an effective feature in speech and text summarization (e.g.[REF_CITE]).
"There are several structural features used, in-cluding position of the sentence in the turn (TLOC) and position of the sentence in the conversation (CLOC)."
We also include the time from the begin-ning of the conversation to the current turn (TPOS1) and from the current turn to the end of the conversa-tion (TPOS2).
"Conversations in both modalities can be well-structured, with introductory turns, general discussion, and ultimate resolution or closure, and sentence informativeness might significantly corre-late with this structure."
"We calculate two pause-style features: the time between the following turn and the current turn (SPAU), and the time between the cur-rent turn and previous turn (PPAU), both normalized by the overall length of the conversation."
These fea-tures are based on the email and meeting transcript timestamps.
"We hypothesize that pause features may be useful if informative turns tend to elicit a large number of responses in a short period of time, or if they tend to quickly follow a preceding turn, to give two examples."
There are two features related to the conversation participants directly.
"One measures how dominant the current participant is in terms of words in the conversation (DOM), and the other is a binary fea-ture indicating whether the current participant ini-tiated the conversation (BEGAUTH), based simply on whether they were the first contributor."
It is hy-pothesized that informative sentences may more of-ten belong to participants who lead the conversation or have a good deal of dominance in the discussion.
There are several lexical features used in these experiments.
"For each unique word, we calculate two conditional probabilities."
"For each conversation participant, we calculate the probability of the par-ticipant given the word, estimating the probability from the actual term counts, and take the maximum of these conditional probabilities as our first term score, which we will call Sprob."
Sprob(t) = max p(S|t) S where t is the word and S is a participant.
"For ex-ample, if the word budget is used ten times in total, with seven uses by participant A, three uses by par-ticipant B and no uses by the other participants, then the Sprob score for this term is 0.70."
"The intuition is that certain words will tend to be associated with one conversation participant more than the others, owing to varying interests and expertise between the people involved."
"Using the same procedure, we calculate a score called Tprob based on the probability of each turn given the word."
T prob(t) = max p(T |t)
"The motivating factor for this metric is that certain words will tend to cluster into a small number of turns, owing to shifting topics within a conversation."
"Having derived Sprob and Tprob, we then calcu-late several sentence-level features based on these term scores."
"Each sentence has features related to max, mean and sum of the term scores for the words in that sentence (MXS, MNS and SMS for Sprob, and MXT, MNT and SMT for Tprob)."
"Us-ing a vector representation, we calculate the cosine between the conversation preceding the given sen-tence and the conversation subsequent to the sen-tence, first using Sprob as the vector weights (COS1) and then using Tprob as the vector weights (COS2)."
"This is motivated by the hypothesis that informative sentences might change the conversation in some fashion, leading to a low cosine between the preced-ing and subsequent portions."
"We similarly calculate two scores measuring the cosine between the cur-rent sentence and the rest of the converation, using each term-weight metric as vector weights (CENT1 for Sprob and CENT2 for Tprob)."
This measures whether the candidate sentence is generally similar to the conversation overall.
"There are three word entropy features, calculated using the formula went(s) ="
P i=1N p(x i ) · − log(p(x i )) ( N1 · − log( N1 )) ·
"M where s is a string of words, x i is a word type in that string, p(x i ) is the probability of the word based on its normalized frequency in the string, N is the number of word types in the string, and M is the number of word tokens in the string."
Note that word entropy essentially captures infor-mation about type-token ratios.
"For example, if each word token in the string was a unique type then the word entropy score would be 1."
"We calculate the word entropy of the current sentence (THISENT), as well as the word entropy for the conversation up until the current sentence (PENT) and the word en-tropy for the conversation subsequent to the current sentence (SENT)."
"We hypothesize that informative sentences themselves may have a diversity of word types, and that if they represent turning points in the conversation they may affect the entropy of the sub-sequent conversation."
"Finally, we include a feature that is a rough ap-proximation of the ClueWordScore (CWS) used[REF_CITE]."
For each sentence we remove stopwords and count the number of words that occur in other turns besides the current turn.
The CWS is therefore a measure of conversation cohesion.
"For ease of reference, we hereafter refer to this conversation features system as ConverSumm."
"In order to compare the ConverSumm system with state-of-the-art systems for meeting and email sum-marization, respectively, we also present results us-ing the features described[REF_CITE]for meetings and the features described[REF_CITE]for email."
"Because the work by Murray and Renals used the same dataset, we can compare our scores directly."
"However, Rambow car-ried out summarization work on a different, unavail-able email corpus, and so we re-implemented their summarization system for our current email data."
"In their work on meeting summarization,[REF_CITE]-word summaries of each meeting using several classes of features: prosodic, lexical, structural and speaker-related."
"While there are two features overlapping between our systems (word-count and speaker/participant dominance), their system is primarily domain-dependent in its use of prosodic features while our features represent a more general conversational view."
"There is again a slight overlap in features between our two systems, as we both include length and position of the sen-tence in the thread/conversation."
"Here we present, in turn, the summarization results for meeting and email data."
"Figure 1 shows the F statistics for each Conver-summ feature in the meeting data, providing a mea-sure of the usefulness of each feature in discriminat-ing between the positive and negative classes."
"Some features such as participant dominance have very low F statistics because each sentence by a given participant will receive the same score; so while the feature itself may have a low score because it does not discriminate informative versus non-informative sentences on its own, it may well be useful in con-junction with the other features."
"The best individual ConverSumm features for meeting summarization are sentence length (SLEN), sum of Sprob scores, sum of Tprob scores, the simplified CWS score (CWS), and the two centroid measures (CENT1 and CENT2)."
The word entropy of the candidate sen-tence is very effective for manual transcripts but much less effective on ASR output.
This is due to the fact that ASR errors can incorrectly lead to high entropy scores.
"Table 2 provides the weighted f-scores for all summaries of the meeting data, as well as AUROC scores for the classifiers themselves."
There are no significant differences accord-ing to paired t-test.
"For the AUROC measures, there are again no significant differences between the con- versation summarizers and speech-specific summa-rizers."
The AUROC for the conversation system is slightly lower on manual transcripts and slightly higher when applied to ASR output.
For all systems the weighted f-scores are some-what low.
"This is partly owing to the fact that out-put summaries are very short, leading to high pre-cision and low recall."
The low f-scores are also in-dicative of the difficulty of the task.
"Human perfor-mance, gauged by comparing each annotator’s sum-maries to the remaining annotators’ summaries, ex-hibits an average weighted f-score of 0.47 on the same test set."
"The average kappa value on the test set is 0.48, showing the relatively low inter-annotator agreement that is typical of summarization annota-tion."
There is no additional benefit to combining the conversational and speech-specific features.
"In that case, the weighted f-scores are 0.23 for both manual and ASR transcripts."
The overall AUROC is 0.85 for manual transcripts and 0.86 for ASR.
We can expand the features analysis by consid-ering the effectiveness of certain subclasses of fea-tures.
"Specifically, we group the summarization fea-tures into lexical, structural, participant and length features."
"Figure 2 shows the AUROCs for the fea-ture subset classifiers, illustrating that the lexical subclass is very effective while the length features also constitute a challenging baseline."
"A weakness of systems that depend heavily on length features, however, is that recall scores tend to decrease be-cause the extracted units are much longer - weighted recall scores for the 700 word summaries are sig-nificantly worse according to paired t-test (p&lt;0.05) when using just length features compared to the full feature set."
Figure 3 shows the F statistic for each ConverSumm feature in the email data.
The two most useful fea-tures are sentence length and CWS.
The Sprob and Tprob features rate very well according to the F statistic.
The two centroid features incorporating Sprob and Tprob are comparable to one another and are very effective features as well.
The results are given in Table 3.
"On average, the Rambow system is slightly higher with a score of 0.50 compared with 0.46 for the con-versational system, but there is no statistical differ-ence according to paired t-test."
"The average AUROC for the Rambow system is 0.64 compared with 0.75 for the ConverSumm sys- tem, with ConverSumm system significantly better according to paired t-test (p&lt;0.05)."
Random classi-fication performance would yield an AUROC of 0.5.
Combining the Rambow and ConverSumm fea-tures does not yield any overall improvement.
The Pyramid Precision score in that case is 0.47 while the AUROC is 0.74.
"Figure 4 illustrates that the lexical and length features are the most effective feature subclasses, though the best results overall are derived from a combination of all feature classes."
"According to multiple evaluations, the ConverSumm features yield competitive summarization perfor-mance with the comparison systems."
"There is a clear set of features that are similarly effective in both do-mains, especially CWS, the centroid features, the Sprob features, the Tprob features, and sentence length."
There are other features that are more ef-fective in one domain than the other.
"For exam-ple, the BEGAUTH feature, indicating whether the current participant began the conversation, is more useful for emails."
It seems that being the first per-son to speak in a meeting is not as significant as being the first person to email in a given thread.
"SLEN2, which normalizes sentence length by the longest sentence in the turn, also is much more ef- fective for emails."
"The reason is that many meet-ing turns consist of a single, brief utterance such as “Okay, yeah.”"
"The finding that the summary evaluations are not significantly worse on noisy ASR compared with manual transcripts has been previously attested[REF_CITE], and it is encouraging that our ConverSumm features are sim-ilarly robust to this noisy data."
We have shown that a general conversation summa-rization approach can achieve results on par with state-of-the-art systems that rely on features specific to more focused domains.
We have introduced a conversation feature set that is similarly effective in both the meetings and emails domains.
"The use of multiple summarization evaluation techniques con-firms that the system is robust, even when applied to the noisy ASR output in the meetings domain."
Such a general conversation summarization system is valuable in that it may save time and effort re-quired to implement unique systems in a variety of conversational domains.
"We are currently working on extending our sys-tem to other conversation domains such as chats, blogs and telephone speech."
"We are also investigat-ing domain adaptation techniques; for example, we hypothesize that the relatively well-resourced do-main of meetings can be leveraged to improve email results, and preliminary findings are encouraging."
"In this paper, we present an algorithm for learning a generative model of natural lan-guage sentences together with their for-mal meaning representations with hierarchi-cal structures."
The model is applied to the task of mapping sentences to hierarchical rep-resentations of their underlying meaning.
We introduce dynamic programming techniques for efficient training and decoding.
"In exper-iments, we demonstrate that the model, when coupled with a discriminative reranking tech-nique, achieves state-of-the-art performance when tested on two publicly available cor-pora."
The generative model degrades robustly when presented with instances that are differ-ent from those seen in training.
This allows a notable improvement in recall compared to previous models.
To enable computers to understand natural human language is one of the classic goals of research in natural language processing.
"Recently, researchers have developed techniques for learning to map sen-tences to hierarchical representations of their under-lying meaning[REF_CITE]."
One common approach is to learn some form of probabilistic grammar which includes a list of lexi-cal items that models the meanings of input words and also includes rules for combining lexical mean-ings to analyze complete sentences.
"This approach performs well but is constrained by the use of a sin-gle, learned grammar that contains a fixed set of lexical entries and productions."
"In practice, such a grammar may lack the rules required to correctly parse some of the new test examples."
"In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the cor-respondence between sentences and their meanings with a generative process."
This model is defined over hybrid trees whose nodes include both natu-ral language words and meaning representation to-kens.
"Inspired by the work[REF_CITE], the generative model builds trees by recursively creating nodes at each level according to a Markov process."
This implicit grammar representation leads to flexi-ble learned models that generalize well.
"In practice, we observe that it can correctly parse a wider range of test examples than previous approaches."
The generative model is learned from data that consists of sentences paired with their meaning rep-resentations.
"However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees."
"This creates a challenging, hidden-variable learning problem that we address with the use of an inside-outside algorithm."
"Specifically, we develop a dy-namic programming parsing algorithm that leads to O(n 3 m) time complexity for inference, where n is the sentence length and m is the size of meaning structure."
This approach allows for efficient train-ing and decoding.
"In practice, we observe that the learned generative models are able to assign a high score to the correct meaning for input sentences, but that this correct meaning is not always the highest scoring option."
"To address this problem, we use a simple rerank-ing approach to select a parse from a k-best list of parses."
This pipelined approach achieves state-of-the-art performance on two publicly available cor-pora.
"In particular, the flexible generative model leads to notable improvements in recall, the total percentage of sentences that are correctly parsed."
"In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider."
S ILT[REF_CITE]learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures.
W ASP[REF_CITE]is a system motivated by statistical machine translation techniques.
It acquires a set of synchronous lexical entries by running the IBM alignment model[REF_CITE]and learns a log-linear model to weight parses.
K RISP[REF_CITE]is a dis-criminative approach where meaning representation structures are constructed from the natural language strings hierarchically.
It is built on top of SVM struct with string kernels.
"Additionally, there is substantial related research that is not directly comparable to our approach."
"Some of this work requires different levels of super-vision, including labeled syntactic parse trees[REF_CITE]."
Others do not perform lexical learning[REF_CITE].
"Finally, recent work has explored learning to map sentences to lambda-calculus meaning rep-resentations[REF_CITE]."
We restrict our meaning representation (MR) for-malism to a variable free version as presented[REF_CITE].
A training instance consists of a natural language sentence (NL sentence) and its corresponding mean-ing representation structure (MR structure).
Con-sider the following instance taken from the G EO - QUERY corpus[REF_CITE]:
"The NL sentence “How many states do not have rivers ?” consists of 8 words, in-cluding punctuation."
"The MR is a hierarchical tree structure, as shown in Figure 1."
"Following an inorder traversal of this MR tree, we can equivalently represent it with the following list of meaning representation productions (MR produc-tions): (0) Q UERY : answer (N UM ) (1) N UM : count (S TATE ) (2) S TATE : exclude (S TATE 1 S TATE 2 ) (3) S TATE : state (all) (4) S TATE : loc 1 (R IVER ) (5) R IVER : river (all)"
"Each such MR production consists of three com-ponents: a semantic category, a function symbol which can be omitted (considered empty), and a list of arguments."
An argument can be either a child se-mantic category or a constant.
"Take production (1) for example: it has a semantic category “N UM ”, a function symbol “count”, and a child semantic cate-gory “S TATE ” as its only argument."
"Production (5) has “R IVER ” as its semantic category, “river” as the function symbol, and “all” is a constant."
"We describe in this section our proposed generative model, which simultaneously generates a NL sen-tence and an MR structure."
"We denote a single NL word as w, a contiguous sequence of NL words as w, and a complete NL b sentence as w. In the MR structure, we denote a semantic category as M. We denote a single MR production as m a , or M a : p α (M b , M c ), where M a is the semantic category for this production, p α is the function symbol, and M b , M c are the child semantic categories."
"We denote m a as an MR structure rooted by an MR production m a , and c m a an MR structure for a complete sentence rooted by an MR production m a ."
The model generates a hybrid tree that represents structure m a b a sentence w = w 1 ... w 2 ... paired with an MR c rooted by m a .
Figure 2 shows part of a hybrid tree that is gen-erated as follows.
"Given a semantic category M a , we first pick an MR production m a that has the form M a : p α (M b , M c ), which gives us the function sym-bol p α as well as the child semantic categories M b and M c ."
"Next, we generate the hybrid sequence of child nodes w 1 M b w 2 M c , which consists of NL words and semantic categories."
"After that, two child MR productions m b and m c are generated."
"These two productions will in turn generate other hybrid sequences and productions, re-cursively."
"This process produces a hybrid tree T, whose nodes are either NL words or MR produc-tions."
"Given this tree, we can recover a NL sentence w by recording the NL words visited in depth-first traversal order and can recover an MR structure m by following a tree-specific traversal order, defined by the hybrid-patterns we introduce below."
Figure 3 gives a partial hybrid tree for the training example from Section 3.
Note that the leaves of a hybrid tree are always NL tokens.
"With several independence assumptions, the b b probability of generating w, m, T is defined as:"
"P(wb, b m, T ) ="
P(M a ) ×
P(m a |M a ) ×
"P(w 1 M b w 2 M c |m a ) ×P(m b |m a , arg = 1) × P(. . . |m b ) ×P(m c |m a , arg = 2) × P(. . . |m c ) (1) where “arg” refers to the position of the child se-mantic category in the argument list."
"Motivated by Collins’ syntactic parsing models[REF_CITE], we consider the generation process for a hybrid sequence from an MR production as a Markov process."
"Given the assumption that each MR production has at most two semantic categories in its arguments (any production can be transformed into a sequence of productions of this form), Table 1 includes the list of all possible hybrid patterns."
"In this table, m is an MR production, Y and Z are respectively the first and second child seman-tic category in m’s argument list."
"The symbol w refers to a contiguous sequence of NL words, and anything inside [] can be optionally omitted."
The last row contains hybrid patterns that reflect reorder-ing of one production’s child semantic categories during the generation process.
"For example, con-sider the case that the MR production S TATE : exclude (S TATE 1 S TATE 2 ) generates a hybrid se-quence S TATE 1 do not S TATE 2 , the hybrid pattern m → YwZ is associated with this generation step."
"For the example hybrid tree in Figure 2, we can decompose the probability for generating the hybrid sequence as follows:"
P(w 1 M b w 2 M c |m a ) =
P(m → wYwZ|m a ) ×
"P(w 1 |m a ) ×P(M b |m a , w 1 ) ×"
"P(w 2 |m a , w 1 , M b ) ×P(M c |m a , w 1 , M b , w 2 ) ×"
"P(END|m a , w 1 , M b , w 2 , M c ) (2)"
"Note that unigram, bigram, or trigram assump-tions can be made here for generating NL words and semantic categories."
"For example, under a bigram assumption, the second to last term can be written as P(M c |m a , w 1 , M b , w 2 ) ≡"
"P(M c |m a , w k2 ), where w k2 is the last word in w 2 ."
"We call such additional information that we condition on, the context."
Note that our generative model is different from the synchronous context free grammars (SCFG) in a number of ways.
A standard SCFG produces a correspondence between a pair of trees while our model produces a single hybrid tree that represents the correspondence between a sentence and a tree.
"Also, SCFGs use a finite set of context-free rewrite rules to define the model, where the rules are possi-bly weighted."
"In contrast, we make use of the more flexible Markov models at each level of the genera-tive process, which allows us to potentially produce a far wider range of possible trees."
There are three categories of parameters used in the model.
"The first category of parameters models the generation of new MR productions from their parent MR productions: e.g., P(m b |m a ,arg = 1); the second models the generation of a hybrid se-quence from an MR production: e.g., P(w 1 |m a ), P(M b |m a , w 1 ); the last models the selection of a hy-brid pattern given an MR production, e.g.,"
P(m → wY|m a ).
"We will estimate parameters from all cate-gories, with the following constraints:"
"P 1. m ′ ρ(m ′ |m j , arg=k)=1 for all j and k = 1, 2."
"These parameters model the MR structures, and can be referred to as MR model parameters."
"P 2. t θ(t|m j , Λ)=1 for all j, where t is a NL word, the “END” symbol, or a semantic category."
Λ is the context associated with m j and t.
"These parameters model the emission of NL words, the “END” symbol, and child semantic categories from an MR production."
We call them emission parameters.
"P 3. r φ(r|m j ) = 1 for all j, where r is a hybrid pattern listed in Table 1."
These parameters model the selection of hybrid patterns.
We name them pattern parameters.
"With different context assumptions, we reach dif-ferent variations of the model."
"In particular, we con-sider three assumptions, as follows:"
"We make the following assumption: θ(t k |m j , Λ) = P(t k |m j ) (3) where t k is a semantic category or a NL word, and m j is an MR production."
"In other words, generation of the next NL word depends on its direct parent MR production only."
"Such a Unigram Model may help in recall (the num-ber of correct outputs over the total number of in-puts), because it requires the least data to estimate."
"Model II We make the following assumption: θ(t k |m j , Λ) = P(t k |m j , t k−1 ) (4) where t k−1 is the semantic category or NL word to the left of t k , i.e., the previous semantic category or NL word."
"In other words, generation of the next NL word depends on its direct parent MR production as well as the previously generated NL word or semantic category only."
This model is also referred to as Bi-gram Model.
"This model may help in precision (the number of correct outputs over the total number of outputs), because it conditions on a larger context."
"Model III We make the following assumption: 1 θ(t k |m j , Λ) = 2 × P(t k |m j ) + P(t k |m j , t k−1 ) (5)"
"We can view this model, called the Mixgram Model, as an interpolation between Model I and II."
This model gives us a balanced score for both preci-sion and recall.
The MR model parameters can be estimated inde-pendently from the other two.
"These parameters can be viewed as the “language model” parameters for the MR structure, and can be estimated directly from the corpus by simply reading off the counts of occur-rences of MR productions in MR structures over the training corpus."
"To resolve data sparseness problem, a variant of the bigram Katz Back-Off Model[REF_CITE]is employed here for smoothing."
Learning the remaining two categories of parameters is more challenging.
"In a conventional PCFG pars-ing task, during the training phase, the correct cor-respondence between NL words and syntactic struc-tures is fully accessible."
"In other words, there is a single deterministic derivation associated with each training instance."
Therefore model parameters can be directly estimated from the training corpus by counting.
"However, in our task, the correct corre-spondence between NL words and MR structures is unknown."
"Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree."
The hybrid tree is constructed using hidden vari-ables and estimated from the training set.
"An effi-cient inside-outside style algorithm can be used for model estimation, similar to that used[REF_CITE], as discussed next."
"In this section, we discuss how to estimate the emission and pattern parameters with the Expecta-tion Maximization (EM) algorithm[REF_CITE], by using an inside-[REF_CITE]dy-namic programming approach."
"Denote n i ≡ hm i , w i i as the i-th training instance, where m i and w i are the MR structure and the NL sentence of the i-th instance respectively."
"We also denote n v ≡ hm v ,w v i as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production m v will correspond to (i.e., hierarchically generate) the NL substring w v ."
"The symbol h is used to de-note a hybrid sequence, and the function Parent(h) gives the unique MR substructure-NL subsequence pair which can be decomposed as h. Parent(n v ) re-turns the set of all possible hybrid sequences un-der which the pair n v can be generated."
"Similarly, Children(h) gives the NL-MR pairs that appear di-rectly below the hybrid sequence h in a hybrid tree, and Children(n) returns the set of all possible hybrid sequences that n can be decomposed as."
Figure 4 gives a packed tree structure representing the rela-tions between the entities.
The formulas for computing inside and outside probabilities as well as the equations for updating parameters are given in Figure 5.
We use a CKY-style parse chart for tracking the probabilities.
"It is reasonable to believe that different MR pro-ductions that share identical function symbols are likely to generate NL words with similar distribu-tion, regardless of semantic categories."
"The inside (β) probabilities are defined as • If n v ≡ hm v , w v i is leaf β(n v ) ="
"P(w v |m v ) (6) • If n v ≡ hm v , w v i is not leaf X Y β(n v ) = (7)P(h|m v ) × β(n v ′ ) h∈Children( n v ) n v′ ∈Children(h)"
"The outside (α) probabilities are defined as • If n v ≡ hm v , w v i is root α(n v ) = 1 ( 8) • If n v ≡ hm v , w v i is not root α(n v ) = X α Parent(h) h∈Parent( n v ) Y ×P h|Parent(h) × β(n v ′ ) (9) n v′ ∈Children(h),v ′ ,v"
Parameter Update • Update the emission parameter
"The count c i (t,m v ,Λ k ), where t is a NL word or a semantic category, for an instance pair n i ≡ hm i , w i i:"
R IVER : largest (R IVER ) and C ITY : largest (C ITY ) are both likely to generate the word “biggest”.
"In view of this, a smoothing technique is de-ployed."
We assume half of the time words can be generated from the production’s function symbol alone if it is not empty.
"Mathematically, assuming m a with function symbol p a , for a NL word or se-mantic category t, we have: θ(t|m a , Λ) = ( θ θ e (t|m a , Λ)"
"If p a is empty e (t|m a , Λ) + θ e (t|p a , Λ) /2 otherwise where θ e models the generation of t from an MR production or its function symbol, together with the context Λ."
"Though the inside-outside approach already em-ploys packed representations for dynamic program-ming, a naive implementation of the inference algo-rithm will still require O(n 6 m) time for 1 EM iter-ation, where n and m are the length of the NL sen-tence and the size of the MR structure respectively."
"This is not very practical as in one of the corpora we look at, n and m can be up to 45 and 20 respectively."
"In this section, we develop an efficient dynamic programming algorithm that enables the inference to run in O(n 3 m) time."
The idea is as follows.
"In-stead of treating each possible hybrid sequence as a separate rule, we efficiently aggregate the already computed probability scores for hybrid sequences that share identical hybrid patterns."
Such aggregated scores can then be used for subsequent computa-tions.
"By doing this, we can effectively avoid a large amount of redundant computations."
The algorithm supports both unigram and bigram context assump-tions.
"For clarity and ease of presentation, we pri-marily make the unigram assumption throughout our discussion."
"We use β (m v , w v ) to denote the inside probabil-ity for m v -w v pair, b r [m v , w v , c] to denote the aggre-gated probabilities for the MR sub-structure m v to generate all possible hybrid sequences based on w v with pattern r that covers its c-th child only."
"In addi-tion, we use w (i,j) to denote a subsequence of w with start index i (inclusive) and end index j (exclusive)."
"We also use β r ~m v , w v  to denote the aggregated in-side probability for the pair hm v , w v i, if the hybrid pattern is restricted to r only."
"By definition we have: β (m v , w v ) =Xφ(r m ) | v ×β r ~m v ,w v ×θ(END|m v ) (12) r"
Relations between β r and b r can also be estab-lished.
"For example, if m v has one child semantic category, we have: β m→ w Y ~m v , w v  = b m→ w Y [m v , w v , 1] (13)"
"For the case when m v has two child semantic cat-egories as arguments, we have, for example: β m→ w YZ w ~m v , w (i,j)  = X b m→ w Y [m v , w (i,k) , 1] i+2≤k≤j−2 ×b m→Y w [m v , w (k,j) , 2] (14)"
"Note that there also exist relations amongst b terms for more efficient computation, for example: b m→ w Y [m v , w (i,j) , c] = θ(w i |m v ) × b m→ w Y [m v , w (i+1,j) , c] + b m→Y [m v , w (i+1,j) , c] (15)"
Analogous but more complex formulas are used for computing the outside probabilities.
Updating of parameters can be incorporated into the computation of outside probabilities efficiently.
"In the decoding phase, we want to find the optimal MR structure m ∗ given a new NL sentence w: m|w) = arg max X P( , b b ∗ where T is a possible hybrid c m tree associated b b with m = argmaxP(b bb m T |w) (16) m c T b the m-bw pair."
"However, it is expensive to compute the summation over all possible hybrid trees."
"We therefore find the most likely hybrid tree instead: ∗ m =arg max maxP(b bb m, T |w)=argmax max"
"P(b b w, m, T ) (17) m"
T T c m c
We have implemented an exact top-k decoding al-gorithm for this task.
Dynamic programming tech-niques similar to those discussed in Section 6 can also be applied when retrieving the top candidates.
"We also find the Viterbi hybrid tree given a NL-MR pair, which can be done in an analogous way."
This tree will be useful for reranking.
"Due to the various independence assumptions we have made, the model lacks the ability to express some long range dependencies."
We therefore post-process the best candidate predictions with a dis-criminative reranking algorithm.
The averaged perceptron algorithm[REF_CITE]has previously been applied to various NLP tasks[REF_CITE]for discriminative reranking.
The detailed algorithm can be found[REF_CITE].
"In this section, we extend the con-ventional averaged perceptron by introducing an ex-plicit separating plane on the feature space."
Our reranking approach requires three compo-nents during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each train-ing instance; and a feature function Φ that defines a mapping from a hybrid tree to a feature vector.
"The algorithm learns a weight vector w that associates a weight to each feature, such that a score w·Φ(T ) can be assigned to each candidate hybrid tree T ."
"Given a new instance, the hybrid tree with the highest score is then picked by the algorithm as the output."
"In this task, the GEN function is defined as the output hybrid trees of the top-k (k is set to 50 in our experiments) decoding algorithm, given the learned model parameters."
The correct reference hybrid tree is determined by running the Viterbi algorithm on each training NL-MR pair.
The feature function is discussed in section 8.2.
"While conventional perceptron algorithms usually optimize the accuracy measure, we extend it to allow optimization of the F-measure by introducing an ex-plicit separating plane on the feature space that re-jects certain predictions even when they score high-est."
"The idea is to find a threshold b after w is learned, such that a prediction with score below b gets rejected."
We pick the threshold that leads to the optimal F-measure when applied to the training set.
We list in Table 2 the set of features we used.
Ex-amples are given based on the hybrid tree in Figure 3.
Some of the them are adapted[REF_CITE]for a natural language parsing task.
"Fea-tures 1-5 are indicator functions (i.e., it takes value 1 if a certain combination as the ones listed in Table 2 is present, 0 otherwise), while feature 6 is real val-ued."
Features that do not appear more than once in the training set are discarded.
"Our evaluations were performed on two corpora, G EOQUERY and R OBOCUP ."
The G EOQUERY cor-pus contains MR defined by a Prolog-based lan-guage used in querying a database on U.S. geogra-phy.
The R OBOCUP corpus contains MR defined by a coaching language used in a robot coaching com-petition.
There are in total 880 and 300 instances for the two corpora respectively.
"To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the ex-periments[REF_CITE]."
"Given a training set, we first run a variant of IBM alignment model 1[REF_CITE]for 100 iter-ations, and then initialize Model I with the learned parameter values."
"This IBM model is a word-to-word alignment model that does not model word order, so we do not have to linearize the hierarchi-cal MR structure."
"Given this initialization, we train Model I for 100 EM iterations and use the learned parameters to initialize Model II which is trained for another 100 EM iterations."
Model III is simply an interpolation of the above two models.
"As for the reranking phase, we initialize the weight vector with the zero vector 0, and run the averaged perceptron algorithm for 10 iterations."
"Again following[REF_CITE], we define the cor-rect output MR structure as follows."
"For the G EO - QUERY corpus, an MR structure is considered cor-rect if and only if it retrieves identical results as the reference MR structure when both are issued as queries to the underlying Prolog database."
"For the R OBOCUP corpus, an MR structure is considered correct if and only if it has the same string represen-tation as the reference MR structure, up to reorder-ing of children of MR productions whose function symbols are commutative, such as and, or, etc."
"We evaluated the three models, with and with-out reranking."
The results are presented in Table 3.
"Comparing Model I and Model II, we noticed that for both corpora, Model I in general achieves bet-ter recall while Model II achieves better precision."
This observation conforms to our earlier expecta-tions.
"Model III, as an interpolation of the above two models, achieves a much better F-measure on G EO - QUERY corpus."
"However, it is shown to be less ef-fective on R OBOCUP corpus."
"We noticed that com-pared to the G EOQUERY corpus, R OBOCUP corpus contains longer sentences, larger MR structures, and a significant amount of non-compositionality."
These factors combine to present a challenging problem for parsing with the generative model.
"Interestingly, al-though Model III fails to produce better best pre-dictions for this corpus, we found that its top-k list contains a relatively larger number of correct pre- dictions than Model I or Model II."
This indicates the possibility of enhancing the performance with reranking.
The reranking approach is shown to be quite ef-fective.
We observe a consistent improvement in both precision and F-measure after employing the reranking phase for each model.
"Among all the previous models, S ILT , W ASP , and K RISP are directly comparable to our model."
They required the same amount of supervision as our sys-tem and were evaluated on the same corpora.
"We compare our model with these models in Ta-ble 4, where the performance scores for the previous systems are taken[REF_CITE]."
"For G EO - QUERY corpus, our model performs substantially better than all the three previous models, with a no-table improvement in the recall score."
"In fact, if we look at the recall scores alone, our best-performing model achieves a 6.7% and 9.8% absolute improve-ment over two other state-of-the-art models W ASP and K RISP respectively."
"This indicates that over-all, our model is able to handle over 25% of the inputs that could not be handled by previous sys-tems."
"On the other hand, in terms of F-measure, we gain a 4.1% absolute improvement over K RISP , which leads to an error reduction rate of 22%."
"On the R OBOCUP corpus, our model’s performance is also ranked the highest [Footnote_1] ."
1 We are unable to perform statistical significance tests be-cause the detailed performance for each fold of previously pub-lished research work is not available.
"As a generic model that requires minimal assump-tions on the natural language, our model is natural language independent and is able to handle various other natural languages than English."
"To validate this point, we evaluated our system on a subset of the G EOQUERY corpus consisting of 250 instances, with four different NL annotations."
"As we can see from Table 5, our model is able to achieve performance comparable to W ASP as re-ported[REF_CITE]."
"Our model is generic, which requires no domain-dependent knowledge and should be applicable to a wide range of different domains."
"Like all re-search in this area, the ultimate goal is to scale to more complex, open-domain language understand-ing problems."
"In future, we would like to create a larger corpus in another domain with multiple natu-ral language annotations to further evaluate the scal-ability and portability of our approach."
We presented a new generative model that simulta-neously produces both NL sentences and their cor-responding MR structures.
The model can be effec-tively applied to the task of transforming NL sen-tences to their MR structures.
We also developed a new dynamic programming algorithm for efficient training and decoding.
"We demonstrated that this approach, augmented with a discriminative rerank-ing technique, achieves state-of-the-art performance when tested on standard benchmark corpora."
"In future, we would like to extend the current model to have a wider range of support of MR for-malisms, such as the one with lambda-calculus sup-port."
We are also interested in investigating ways to apply the generative model to the inverse task: gen-eration of a NL sentence that explains a given MR structure.
Determining the polarity of a sentiment-bearing expression requires more than a sim-ple bag-of-words approach.
"In particular, words or constituents within the expression can interact with each other to yield a particu-lar overall polarity."
"In this paper, we view such subsentential interactions in light of composi-tional semantics, and present a novel learning-based approach that incorporates structural in-ference motivated by compositional seman-tics into the learning procedure."
"Our exper-iments show that (1) simple heuristics based on compositional semantics can perform bet-ter than learning-based methods that do not in-corporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learn-ing performs better than all other alterna-tives (90.7%)."
"We also find that “content-word negators”, not widely employed in pre-vious work, play an important role in de-termining expression-level polarity."
"Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered."
Determining the polarity of sentiment-bearing ex-pressions at or below the sentence level requires more than a simple bag-of-words approach.
One of the difficulties is that words or constituents within the expression can interact with each other to yield a particular overall polarity.
"To facilitate our discus-sion, consider the following examples: 1: [I did [not] ¬ have any [doubt] − about it.] + 2: [The report [eliminated] ¬ my [doubt] − .] + 3: [They could [not] ¬ [eliminate] ¬ my [doubt] − .] −"
"In the first example, “doubt” in isolation carries a negative sentiment, but the overall polarity of the sentence is positive because there is a negator “not”, which flips the polarity."
"In the second example, both “eliminated” and “doubt” carry negative sentiment in isolation, but the overall polarity of the sentence is positive because “eliminated” acts as a negator for its argument “doubt”."
"In the last example, there are effectively two negators – “not” and “eliminated” – which reverse the polarity of “doubt” twice, result-ing in the negative polarity for the overall sentence."
These examples demonstrate that words or con-stituents interact with each other to yield the expression-level polarity.
And a system that sim-ply takes the majority vote of the polarity of indi-vidual words will not work well on the above exam-ples.
"Indeed, much of the previous learning-based research on this topic tries to incorporate salient in-teractions by encoding them as features."
"One ap-proach includes features based on contextual va-lence shifters [Footnote_1][REF_CITE], which are words that affect the polarity or intensity of sen-timent over neighboring text spans (e.g.,[REF_CITE],[REF_CITE],"
"1 For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”."
"Another approach encodes frequent sub-sentential patterns (e.g.,[REF_CITE]) as features; these might indirectly capture some of the subsentential interactions that affect polarity."
"How- ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to ade-quately reflect the complex structural nature of the underlying subsentential interactions.[REF_CITE]"
"In short, the Principle of Compositionality states that the meaning of a compound expression is a func-tion of the meaning of its parts and of the syntac-tic rules by which they are combined (e.g.,[REF_CITE],[REF_CITE])."
"Their approach can be viewed as a type of structural inference, but their hand-written rules have not been empirically compared to learning-based alternatives, which one might expect to be more effective in handling some aspects of the polarity classification task."
"In this paper, we begin to close the gap between learning-based approaches to expression-level po-larity classification and those founded on composi-tional semantics: we present a novel learning-based approach that incorporates structural inference mo-tivated by compositional semantics into the learning procedure."
"Adopting the view point of compositional seman-tics, our working assumption is that the polarity of a sentiment-bearing expression can be determined in a two-step process: (1) assess the polarities of the con-stituents of the expression, and then (2) apply a rela-tively simple set of inference rules to combine them recursively."
"Rather than a rigid application of hand-written compositional inference rules, however, we hypothesize that an ideal solution to the expression-level polarity classification task will be a method that can exploit ideas from compositional seman-tics while providing the flexibility needed to handle the complexities of real-world natural language — exceptions, unknown words, missing semantic fea-tures, and inaccurate or missing rules."
The learning-based approach proposed in this paper takes a first step in this direction.
"In addition to the novel learning approach, this paper presents new insights for content-word nega-tors, which we define as content words that can negate the polarity of neighboring words or con-stituents. (e.g., words such as “eliminated” in the example sentences)."
"Unlike function-word nega-tors, such as “not” or “never”, content-word nega-tors have been recognized and utilized less actively in previous work. (Notable exceptions include e.g.,[REF_CITE],[REF_CITE], and"
2 See §5. Related Work for detailed discussion.
"In our experiments, we compare learning- and non-learning-based approaches to expression-level polarity classification — with and without com-positional semantics — and find that (1) simple heuristics based on compositional semantics outper-form (89.7% in accuracy) other reasonable heuris-tics that do not incorporate compositional seman-tics (87.7%); they can also perform better than sim-ple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compo-sitional semantics further improves the performance (90.7%), (3) content-word negators play an impor-tant role in determining the expression-level polar-ity, and, somewhat surprisingly, we find that (4) expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered."
"In what follows, we first explore heuristic-based approaches in §2, then we present learning-based ap-proaches in §3."
"Next we present experimental results in §4, followed by related work in §5."
This section describes a set of heuristic-based meth-ods for determining the polarity of a sentiment-bearing expression.
"Each assesses the polarity of the words or constituents using a polarity lexicon that indicates whether a word has positive or negative polarity, and finds negators in the given expression using a negator lexicon."
The methods then infer the expression-level polarity using voting-based heuris-tics (§ 2.1) or heuristics that incorporate composi-tional semantics (§2.2).
The lexicons are described in §2.3.
We first explore five simple heuristics based on vot-ing.
V OTE is defined as the majority polarity vote by words in a given expression.
"That is, we count the number of positive polarity words and negative polarity words in a given expression, and assign the majority polarity to the expression."
"In the case of a tie, we default to the prevailing polarity of the data."
"For N EG (1) , we first determine the majority polar-ity vote as above, and then if the expression contains any function-word negator, flip the polarity of the majority vote once."
"N EG ( N ) is similar to N EG (1) , ex-cept we flip the polarity of the majority vote n times after the majority vote, where n is the number of function-word negators in a given expression."
"N EG E X (1) and N EG E X ( N ) are defined similarly as N EG (1) and N EG ( N ) above, except both function-word negators and content-word negators are con-sidered as negators when flipping the polarity of the majority vote."
See Table 1 for summary.
Note that a word can be both a negator and have a negative prior polarity.
"For the purpose of voting, if a word is de-fined as a negator per the voting scheme, then that word does not participate in the majority vote."
"For brevity, we refer to N EG (1) and N EG ( N ) col-lectively as N EG , and N EG E X (1) and N EG E X ( N ) col-lectively as N EG E X ."
"Whereas the heuristics above use voting-based in-ference, those below employ a set of hand-written rules motivated by compositional semantics."
Table 2 shows the definition of the rules along with moti-vating examples.
"In order to apply a rule, we first detect a syntactic pattern (e.g., [destroyed]"
"VP [the terrorism] NP ), then apply the Compose function as defined in Table 2 (e.g., Compose([destroyed], [the terrorism]) by rule #2). [Footnote_3]"
3 Our implementation uses part-of-speech tags and function-words to coarsely determine the patterns. An implementation
"Compose first checks whether the first argument is a negator, and if so, flips the polarity of the second argument."
"Otherwise, Compose resolves the polar-ities of its two arguments."
"Note that if the second argument is a negator, we do not flip the polarity of the first argument, because the first argument in gen-eral is not in the semantic scope of the negation. [Footnote_4] In-stead, we treat the second argument as a constituent with negative polarity."
4[REF_CITE]provide more detailed dis-cussion on the semantic scope of negations and the semantic priorities in resolving polarities.
"We experiment with two variations of the Com-pose function depending on how conflicting polari-ties are resolved: C OMPO MC uses a Compose func-tion that defaults to the Majority Class of the po-larity of the data, [Footnote_5] while C OMPO PR uses a Compose function that selects the polarity of the argument that has higher semantic PRiority."
5 The majority polarity of the data we use for our experi-ments is negative.
"For brevity, we refer to C OMPO PR and C OMPO MC collectively as C OMPO ."
"The polarity lexicon is initialized with the lexicon[REF_CITE]and then expanded using the General Inquirer dictionary. [Footnote_6] In particular, a word contained in at least two of the following categories is considered as positive: P OSITIV , P STV , P OS A FF , P LEASUR , V IRTUE , I NCREAS , and a word contained in at least one of the following categories is consid-ered as negative: N EGATIV , N GTV , N EG A FF , P AIN , V ICE , H OSTILE , F AIL , E NL L OSS , W LB L OSS , T RAN - L OSS ."
"6 Available[URL_CITE]When consulting the General Inquirer dictionary, senses with less than 5% frequency and senses specific to an idiom are dropped."
"For the (function- and content-word) negator lex-icon, we collect a handful of seed words as well as General Inquirer words that appear in either N OT L W or D ECREAS category."
Then we expand the list of content-negators using the synonym information of WordNet[REF_CITE]to take a simple vote among senses.
"While we expect that a set of hand-written heuristic rules motivated by compositional semantics can be effective for determining the polarity of a sentiment-bearing expression, we do not expect them to be per-fect."
Interpreting natural language is such a com-plex task that writing a perfect set of rules would be extremely challenging.
"Therefore, a more ideal solution would be a learning-based method that can exploit ideas from compositional semantics while providing the flexibility to the rigid application of the heuristic rules."
"To this end, we present a novel learning-based approach that incorporates inference rules inspired by compositional semantics into the learning procedure (§3.2)."
"To assess the effect of compositional semantics in the learning-based meth-ods, we also experiment with a simple classifica-tion approach that does not incorporate composi-tional semantics (§3.1)."
The details of these two approaches are elaborated in the following subsec-tions.
"Given an expression x consisting of n words x 1 , ..., x n , the task is to determine the polarity y ∈ {positive, negative} of x. In our simple binary classification approach, x is represented as a vec-tor of features f(x), and the prediction y is given by argmax y w·f(x, y), where w is a vector of parameters learned from training data."
"In our experiment, we use an online SVM algorithm called MIRA (Margin Infused Relaxed Algorithm)[REF_CITE][Footnote_7] for training."
7 We use the Java implementation of this algorithm available[URL_CITE]
"For each x, we encode the following features: • Lexical: We add every word x i in x, and also add the lemma of x i produced by the CASS partial parser toolkit[REF_CITE]. • Dictionary: In order to mitigate the problem of unseen words in the test data, we add features that describe word categories based on the Gen-eral Inquirer dictionary."
"We add this feature for each x i that is not a stop word. • Vote: We experiment with two variations of voting-related features: for SC-V OTE , we add a feature that indicates the dominant polarity of words in the given expression, without consid-ering the effect of negators."
"For SC-N EG E X , we count the number of content-word nega-tors as well as function-word negators to de-termine whether the final polarity should be flipped."
Then we add a conjunctive feature that indicates the dominant polarity together with whether the final polarity should be flipped.
"For brevity, we refer to SC-V OTE and SC-N EG E X collectively as SC ."
"Notice that in this simple binary classification set-ting, it is inherently difficult to capture the compo-sitional structure among words in x, because f(x, y) is merely a flat bag of features, and the prediction is governed simply by the dot product of f(x, y) and the parameter vector w."
"Next, instead of determining y directly from x, we introduce hidden variables z = (z 1 ,...,z n ) as intermediate decision variables, where z i ∈ {positive,negative, negator,none}, so that z i represents whether x i is a word with posi-tive/negative polarity, or a negator, or none of the above."
"For simplicity, we let each intermediate de-cision variable z i (a) be determined independently from other intermediate decision variables, and (b) depend only on the input x, so that z i = argmax z i w· f(x, z i , i), where f(x, z i , i) is the feature vector en-coding around the ith word (described on the next page)."
"Once we determine the intermediate decision variables, we apply the heuristic rules motivated by compositional semantics (from Table 2) in order to obtain the final polarity y of x. That is, y = C(x, z), where C is the function that applies the composi-tional inference, either C OMPO PR or C OMPO MC ."
"For training, there are two issues we need to handle: the first issue is dealing with the hidden variables z."
"Because the structure of composi-tional inference C does not allow dynamic program-ming, it is intractable to perform exact expectation-maximization style training that requires enumerat-ing all possible values of the hidden variables z."
"In-stead, we propose a simple and tractable training rule based on the creation of a soft gold standard for z. In particular, we exploit the fact that in our task, we can automatically construct a reasonably accu-rate gold standard for z, denoted as z ∗ : as shown in Figure 2, we simply rely on the negator and polar-ity lexicons."
"Because z ∗ is not always correct, we allow the training procedure to replace z ∗ with po-tentially better assignments as learning proceeds: in the event that the soft gold standard z ∗ leads to an in-correct prediction, we search for an assignment that leads to a correct prediction to replace z ∗ ."
"The exact procedure is given in Figure 1, and will be discussed again shortly."
Figure 1 shows how we modify the parameter up-date rule of MIRA[REF_CITE]to reflect the aspect of compositional inference.
"In the event that the soft gold standard z ∗ leads to an incor-rect prediction, we search for z good , the assignment with highest score that leads to a correct prediction, and replace z ∗ with z good ."
"In the event of no such z good being found among the K-best assignments of z, we stick with z ∗ ."
The second issue is finding the assignment of z with the highest score(z) =
"P i w · f(x, z i , i) that leads to an incorrect prediction y = C(x,z)."
"Be-cause the structure of compositional inference C does not allow dynamic programming, finding such an assignment is again intractable."
We resort to enu-merating only over K-best assignments instead.
"If none of the K-best assignments of z leads to an in-correct prediction y, then we skip the training in-stance for parameter update."
"For each x i in x, we encode the follow-ing features: • Lexical: We include the current word x i as well as the lemma of x i produced by CASS partial parser toolkit[REF_CITE]."
"We also add a boolean feature to indicate whether the current word is a stop word. • Dictionary: In order to mitigate the problem with unseen words in the test data, we add fea-tures that describe word categories based on the General Inquirer dictionary."
We add this fea-ture for each x i that is not a stop word.
We also add a number of boolean features that pro-vide following properties of x i using the polar-ity lexicon and the negator lexicon: – whether x i is a function-word negator – whether x i is a content-word negator – whether x i is a negator of any kind – the polarity of x i according[REF_CITE]’s polarity lexicon – the polarity of x i according to the lexicon derived from the General Inquirer dictio-nary – conjunction of the above two features • Vote: We encode the same vote feature that we use for SC-N EG E X described in § 3.1.
"As in the heuristic-based compositional semantics approach (§ 2.2), we experiment with two variations of this learning-based approach: CCI-C OMPO PR and CCI-C OMPO MC , whose compositional infer-ence rules are C OMPO PR and C OMPO MC respec-tively."
"For brevity, we refer to both variations col-lectively as CCI-C OMPO ."
The experiments below evaluate our heuristic- and learning-based methods for subsentential sentiment analysis (§ 4.1).
"In addition, we explore the role of context by expanding the boundaries of the sentiment-bearing expressions (§ 4.2)."
"For evaluation, we use the Multi-Perspective Ques-tion Answering (MPQA) corpus[REF_CITE], which consists of 535 newswire documents manually annotated with phrase-level subjectivity information."
"We evaluate on all strong (i.e., inten-sity of expression is ‘medium’ or higher), sentiment-bearing (i.e., polarity is ‘positive’ or ‘negative’) ex-pressions. [Footnote_8] As a result, we can assume the bound-aries of the expressions are given."
8 We discard expressions with confidence marked as ‘uncer-tain’.
Performance is reported using 10-fold cross-validation on 400 doc-uments; a separate 135 documents were used as a development set.
"Based on pilot experiments on the development data, we set parameters for MIRA as follows: slack variable to 0.5, and the number of incorrect labels (constraints) for each parameter up-date to 1."
"The number of iterations (epochs) for training is set to 1 for simple classification, and to 4 for classification with compositional inference."
We use K = 20 for classification with compositional inference.
Performance is reported in Table 3.
"In-terestingly, the heuristic-based methods N EG (∼ 82.2%) that only consider function-word negators perform even worse than V OTE (86.5%), which does not consider negators."
"On the other hand, the N EG E X methods (87.7%) that do consider content-word negators as well as function-word negators perform better than V OTE ."
This confirms the importance of content-word negators for determining the polari-ties of expressions.
"The heuristic-based methods motivated by compositional semantics C OMPO fur-ther improve the performance over N EG E X , achiev-ing up to 89.7% accuracy."
"In fact, these heuris-tics perform even better than the SC learning-based methods (∼ 89.1%)."
This shows that heuristics that take into account the compositional structure of the expression can perform better than learning-based methods that do not exploit such structure.
"Finally, the learning-based methods that in-corporate compositional inference CCI-C OMPO (∼ 90.7%) perform better than all of the previous methods."
The difference between CCI-C OMPO PR (90.7%) and SC-N EG E X (89.1%) is statistically sig-nificant at the .05 level by paired t-test.
The dif-ference between C OMPO and any other heuristic that is not based on computational semantics is also statistically significant.
"In addition, the difference between CCIC OMPO PR (learning-based) and C OM - PO MC (non-learning-based) is statistically signifi-cant, as is the difference between N EG E X and V OTE ."
One might wonder whether employing additional context outside the annotated expression boundaries could further improve the performance.
"Indeed, con-ventional wisdom would say that it is necessary to employ such contextual information (e.g.,[REF_CITE])."
"In any case, it is important to determine whether our results will apply to more real-world settings where human-annotated expression bound-aries are not available."
"To address these questions, we gradually relax our previous assumption that the exact boundaries of expressions are given: for each annotation bound-ary, we expand the boundary by x words for each direction, up to sentence boundaries, where x ∈ {1, 5, ∞}."
"We stop expanding the boundary if it will collide with the boundary of an expression with a different polarity, so that we can consistently re-cover the expression-level gold standard for evalua-tion."
"This expansion is applied to both the training and test data, and the performance is reported in Ta-ble 4."
"From this experiment, we make the following observations: • Expanding the boundaries hurts the perfor- mance for any method."
"This shows that most of relevant context for judging the polarity is con-tained within the expression boundaries, and motivates the task of finding the boundaries of opinion expressions. • The N EG E X methods perform better than V OTE only when the expression boundaries are rea-sonably accurate."
"When the expression bound-aries are expanded up to sentence boundaries, they perform worse than V OTE ."
We conjecture this is because the scope of negators tends to be limited to inside of expression boundaries. • The C OMPO methods always perform better than any other heuristic-based methods.
And their performance does not decrease as steeply as the N EG E X methods as the expression boundaries expand.
"We conjecture this is be-cause methods based on compositional seman-tics can handle the scope of negators more ade-quately. • Among the learning-based methods, those that involve compositional inference ( CCI-C OMPO ) always perform better than those that do not ( SC ) for any boundaries."
"And learning with compositional inference tend to perform bet-ter than the rigid application of heuristic rules ( C OMPO ), although the relative performance gain decreases once the boundaries are relaxed."
The task focused on in this paper is similar to that[REF_CITE]in that the general goal of the task is to determine the polarity in context at a sub-sentence level.
"However,[REF_CITE]for-mulated the task differently by limiting their evalua-tion to individual words that appear in their polarity lexicon."
"Also, their approach was based on a flat bag of features, and only a few examples of what we call content-word negators were employed."
"Our use of compositional semantics for the task of polarity classification is preceded[REF_CITE], but our work differs in that we integrate the key idea of compositional seman-tics into learning-based methods, and that we per-form empirical comparisons among reasonable al-ternative approaches."
"For comparison, we evalu- ated our approaches on the polarity classification task from SemEval-07[REF_CITE]."
"9 For lack of space, we only report our performance on in-stances with strong intensities as defined[REF_CITE], which amounts to only 208 test instances. The cross-validation set of MPQA contains 4.9k instances."
There are a number of possible reasons for our lower performance vs.[REF_CITE]on this data set.
"First, SemEval-07 does not include a training data set for this task, so we use 400 documents from the MPQA corpus instead."
"In addition, the SemEval-07 data is very different from the MPQA data in that (1) the polarity annotation is given only at the sentence level, (2) the sentences are shorter, with simpler structure, and not as many negators as the MPQA sentences, and (3) there are many more instances with positive polarity than in the MPQA corpus."
"However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis."
"In partic-ular, it refers to the degree of “commitment” of the author to the truth or falsity of a complement clause for a textual entailment task."
But deci-sions at each sentence level does not consider struc-tural inference within the sentence.
"Among the studies that examined content-word negators,[REF_CITE]manually collected a small set of such words (referred as “words that change phases”), but their lexicon was designed mainly for the medical domain and the type of nega-tors was rather limited."
"In this paper, we consider the task of determining the polarity of a sentiment-bearing expression, con-sidering the effect of interactions among words or constituents in light of compositional semantics."
We presented a novel learning-based approach that in-corporates structural inference motivated by compo-sitional semantics into the learning procedure.
Our approach can be considered as a small step toward bridging the gap between computational semantics and machine learning methods.
Our experimen-tal results suggest that this direction of research is promising.
Future research includes an approach that learns the compositional inference rules from data.
The alignment problem—establishing links between corresponding phrases in two related sentences—is as important in natural language inference (NLI) as it is in machine transla-tion (MT).
"But the tools and techniques of MT alignment do not readily transfer to NLI, where one cannot assume semantic equiva-lence, and for which large volumes of bitext are lacking."
"We present a new NLI aligner, the MANLI system, designed to address these challenges."
"It uses a phrase-based alignment representation, exploits external lexical re-sources, and capitalizes on a new set of su-pervised training data."
We compare the per-formance of MANLI to existing NLI and MT aligners on an NLI alignment task over the well-known Recognizing Textual Entailment data.
"We show that MANLI significantly out-performs existing aligners, achieving gains of 6.2% in F 1 over a representative NLI aligner and 10.5% over GIZA++."
The problem of natural language inference (NLI) is to determine whether a natural-language hypothesis H can reasonably be inferred from a given premise text P .
"In order to recognize that Kennedy was killed can be inferred from JFK was assassinated, one must first recognize the correspondence between Kennedy and JFK, and between killed and assas-sinated."
"Consequently, most current approaches to NLI rely, implicitly or explicitly, on a facility for alignment—that is, establishing links between cor-responding entities and predicates in P and H. Re-cent entries in the annual Recognizing Textual En-tailment (RTE) competiti[REF_CITE]have addressed the alignment problem in a variety of ways, though often without distinguishing it as a separate subproblem."
"Jijkoun and de[REF_CITE], among others, have ex-plored approaches based on measuring the degree of lexical overlap between bags of words."
"While ig-noring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment."
"At the other extreme,[REF_CITE]and[REF_CITE]have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness."
"In such approaches, the corre-spondence between the words of P and H is implicit in the steps of the proof."
"Increasingly, however, the most successful RTE systems have made the alignment problem explicit."
"However, each of these systems has pursued align-ment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult."
In this paper we undertake the first systematic study of alignment for NLI.
"We propose a new NLI alignment system which uses a phrase-based repre-sentation of alignment, exploits external resources for knowledge of semantic relatedness, and capi-talizes on the recent appearance of new supervised training data for NLI alignment."
"In addition, we examine the relation between NLI alignment and MT alignment, and investigate whether existing MT aligners can usefully be applied in the NLI setting."
"The alignment problem is familiar in machine trans-lation (MT), where recognizing that she came is a good translation for elle est venue requires establish- ing a correspondence between she and elle, and be-tween came and est venue."
"The MT community has developed not only an extensive literature on align-ment[REF_CITE], but also standard, proven alignment tools such as GIZA++[REF_CITE]."
Can off-the-shelf MT aligners be applied to NLI?
There is reason to be doubtful.
"Alignment for NLI differs from alignment for MT in several important respects, including: 1."
"Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2."
"It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3."
"Indeed, one cannot assume even approximate semantic equivalence—usually a given in MT."
"Because NLI problems include both valid and invalid inferences, the semantic content of H may diverge substantially from P. An NLI aligner must be designed to accommodate fre-quent unaligned words and phrases. 4."
Little training data is available.
"MT align-ment models are typically trained in unsu-pervised fashion, inducing lexical correspon-dences from massive quantities of sentence-aligned bitexts."
"While NLI aligners could in principle do the same, large volumes of suit-able data are lacking."
"NLI aligners must there-fore depend on smaller quantities of supervised training data, supplemented by external lexi-cal resources."
"Conversely, while existing MT aligners can make use of dictionaries, they are not designed to harness other sources of infor-mation on degrees of semantic relatedness."
"Consequently, the tools and techniques of MT align-ment may not transfer readily to NLI alignment."
We investigate the matter empirically in section 5.2.
"Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn."
"Happily, that has begun to change, with the release by Microsoft Re-search (MSR) of human-generated alignment anno- tations[REF_CITE]for inference problems from the second Recognizing Textual Entailment (RTE2) challenge[REF_CITE]."
"To our knowl-edge, this work is the first to exploit this data for training and evaluation of NLI alignment models."
"The RTE2 data consists of a development set and a test set, each containing 800 inference problems."
Each problem consists of a premise and a hypoth-esis.
"The premises contain 29 words on average; the hypotheses, 11 words."
"Each problem is marked as a valid or invalid inference (50% each); how-ever, these annotations are ignored during align-ment, since they would not be available during test-ing of a complete NLI system."
"The MSR annotations use an alignment repre-sentation which is token-based, but many-to-many, and thus allows implicit alignment of multi-word phrases."
Figure 1 shows an example in which very few has been aligned with poorly represented.
"In the MSR data, every alignment link is marked as SURE or POSSIBLE ."
"In making this distinction, the annotators have followed a convention common in MT, which permits alignment precision to be measured against both SURE and POSSIBLE links, while recall is measured against only SURE links."
"In this work, however, we have chosen to ignore POSSIBLE links, embracing the argument made[REF_CITE]that their use has impeded progress in MT alignment models, and that SURE - only annotation is to be preferred."
"Each RTE[Footnote_2] problem was independently annotated by three people, following carefully designed an-notation guidelines."
2 The SURE / POSSIBLE distinction is taken as significant in computing all these figures.
"Inter-annotator agreement was high:[REF_CITE]reports Fleiss’ kappa 1 scores of about 0.73 (“substantial agreement”) for map-pings from H tokens to P tokens; and all three an-notators agreed on ∼70% of proposed links, while at least two of three agreed on more than 99.7% of proposed links, 2 attesting to the high quality of the annotation data."
"For this work, we merged the three independent annotations, using majority rule, 3 to obtain a gold-standard annotation containing an average of 7.[Footnote_3] links per RTE problem."
"3 The handful of three-way disagreements were treated as POSSIBLE links, and thus were not used here."
"In this section, we describe the MANLI aligner, a new alignment system designed expressly for NLI alignment."
"The MANLI system consists of four el-ements: (1) a phrase-based representation of align-ment, (2) a feature-based linear scoring function for alignments, (3) a decoder which uses simulated an-nealing to find high-scoring alignments, and ([Footnote_4]) per-ceptron learning to optimize feature weights."
"4 DEL and INS edits of size &gt; 1 are possible in principle, but are not used in our training data."
"MANLI uses an alignment representation which is intrinsically phrase-based. (Following the usage common in MT, we use “phrase” to mean any con-tiguous span of tokens, not necessarily correspond-ing to a syntactic phrase.)"
"We represent an alignment E between a premise P and a hypothesis H as a set of phrase edits {e 1 , e 2 , . . .}, each belonging to one of four types: • an EQ edit connects a phrase in P with an equal (by word lemmas) phrase in H • a SUB edit connects a phrase in P with an un-equal phrase in H • a DEL edit covers an unaligned phrase in P • an INS edit covers an unaligned phrase in H"
"For example, the alignment shown in fig-ure 1 can be represented by the set { DEL (In 1 ),"
"DEL (most 2 ), DEL (Pacific 3 ), DEL (countries 4 ), DEL (there 5 ), EQ (are 6 , are 2 ), SUB (very 7 few 8 , poorly 3 represented 4 ), EQ (women 9 , Women 1 ), EQ (in 10 , in 5 ), EQ (parliament 11 , parliament 6 ), EQ (. 12 , . 7 )}. 4"
"Alignments are constrained to be one-to-one at the phrase level: every token in P and H belongs to exactly one phrase, which participates in exactly one edit (possibly DEL or INS )."
"However, the phrase representation permits alignments which are many-to-many at the token level."
"In fact, this is the chief motivation for the phrase-based representation: we can align very few and poorly represented as units, without being forced to make an arbitrary choice as to which word goes with which word."
"Moreover, our scoring function can make use of lexical resources which have information about semantic relatedness of multi-word phrases, not merely individual words."
"However, by merging contiguous token links into phrase edits of size &gt; [Footnote_1], most MSR align-ments (about 92%) can be straightforwardly con-verted into MANLI-reachable alignments."
1 Fleiss’ kappa generalizes Cohen’s kappa to the case where there are more than two annotators.
"For the purpose of model training (but not for the evalua-tion described in section 5.4), we generated a ver-sion of the MSR data in which all alignments were converted to MANLI-reachable form. [Footnote_5]"
"5 About 8% of the MSR alignments contain non-contiguous links, most commonly because P contains two references to an entity (e.g., Christian Democrats and CDU) which are both linked to a reference to the same entity in H (e.g., Christian Democratic Union). In such cases, one or more links must be eliminated to achieve a MANLI-reachable alignment. We used a string-similarity heuristic to break such conflicts, but were obliged to make an arbitrary choice in about 2% of cases."
"To score alignments, we use a simple feature-based linear scoring function, in which the score of an alignment is the sum of the scores of the edits it con-tains (including not only SUB and EQ edits, but also DEL and INS edits), and the score of an edit is the dot product of a vector encoding its features and a vector of weights."
"If E is a set of edits constituting an alignment, and Φ is a vector of feature functions, the score s is given by: s(E) = X s(e) = X w · Φ(e) e∈E e∈E"
We’ll explain how the feature weights w are set in section 4.4.
The features used to characterize each edit are as follows: Edit type features.
We begin with boolean fea-tures encoding the type of each edit.
"We expect EQ s to score higher than SUB s, and (since P is commonly longer than H) DEL s to score higher than INS s."
"Next, we have features which encode the sizes of the phrases involved in the edit, and whether these phrases are non-constituents (in syntactic parses of the sentences involved)."
Lexical similarity feature.
"For SUB edits, a very important feature represents the lexical similarity of the substituends, as a real value in [0, 1]."
"This simi-larity score is computed as a max over a number of component scoring functions, some based on exter-nal lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widely-used measure due[REF_CITE], based on manually constructed lexical re-sources such as WordNet and NomBank • a function based on the well-known distribu-tional similarity metric[REF_CITE], which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text"
The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI.
"Even when the lexical sim-ilarity for a SUB edit is high, it may not be a good match."
"If P or H contains multiple occur-rences of the same word—which happens frequently with function words, and occasionally with content words—lexical similarity may not suffice to deter-mine the right match."
"To remedy this, we introduce contextual features for SUB and EQ edits."
"A real-valued distortion feature measures the difference • an alignment problem hP, Hi • a number of iterations N (e.g. 100) • initial temperature T 0 (e.g. 40) and multiplier r (e.g. 0.9) • a bound on edit size max (e.g. 6) • an alignment scoring function, SCORE (E) Initialize • Let E be an “empty” alignment for hP,Hi (containing only DEL and INS edits, no EQ or SUB edits) • Set Ê = E"
"Repeat for i = 1 to N • Let {F 1 , F 2 , ...} be the set of possible successors of E."
"To generate this set: – Consider every possible edit f up to size max – Let C(E,f) be the set of edits in E which “con-flict” with f (i.e., involve at least some of the same tokens as f) – Let F = E ∪ {f} \ C(E, f) • Let s(F) be a map from successors of E to scores gener-ated by SCORE • Set p(F) = exps(F), and then normalize p(F), trans-forming the score map to a probability distribution • Set T i = r · T i−1 • Set p(F) = p(F) 1/T i , smoothing or sharpening p(F) • Renormalize p(F) • Choose a new value for E by sampling from p(F) • If SCORE (E) &gt; SCORE (Ê), set Ê = E between the relative positions of the substituends within their respective sentences, while boolean matching neighbors features indicate whether the to-kens before and after the substituends are equal or similar."
"The problem of decoding—that is, finding a high-scoring alignment for a particular inference problem—is made more complex by our choice of a phrase-based alignment representation."
"For a model which uses a token-based representation (say, one which simply maps H tokens to P tokens), decod-ing is trivial, since each token can be aligned inde-pendently of its neighbors. (This is the case for the bag-of-words aligner described in section 5.1.)"
"But with a phrase-based representation, things are more complicated."
"The segmentation into phrases is not given in advance, and every phrase pair considered for alignment must be consistent with its neighbors with respect to segmentation."
"Consequently, the de-coding problem cannot be factored into a number of independent decisions."
"To address this difficulty, we have devised a stochastic alignment algorithm, MANLI - ALIGN (fig-ure 2), which uses a simulated annealing strategy."
"Beginning from an arbitrary alignment, we make a series of local steps, at each iteration sampling from a set of possible successors according to scores as-signed by our scoring function."
The sampling is con-trolled by a “temperature” which falls over time.
"At the beginning of the process, successors are sampled with nearly uniform probability, which helps to en-sure that the space of possibilities is explored and local maxima are avoided."
"As the temperature falls, there is a ever-stronger bias toward high-scoring suc-cessors, so that the algorithm converges on a near-optimal alignment."
Clever use of memoization helps to ensure that computational costs remain manage-able.
"Using the parameter values suggested in fig-ure 2, aligning an average RTE problem takes about two seconds."
"While MANLI - ALIGN is not guaranteed to pro-duce optimal alignments, there is reason to believe that it usually comes very close."
"After training, the alignment found by MANLI scored at least as high as the gold alignment for 99.6% of RTE problems. [Footnote_6]"
"6 This figure is based on the MANLI-reachable version of the gold-standard data described in section 4.1. For the raw gold-standard data, the figure is 88.1%. The difference is almost entirely attributable to unreachable gold alignments, which tend to score higher simply because they contain more edits (and because the learned weights are mostly positive)."
"To tune the parameters w of the model, we use an adaptation of the averaged perceptron algorithm[REF_CITE], which has proven successful on a range of NLP tasks."
The algorithm is shown in fig-ure 3.
"After initializing w to 0, we perform N train-ing epochs. (Our experiments used N = 50.)"
"In each epoch, we iterate through the training data, up-dating the weight vector at each training example ac-cording to the difference between the features of the target alignment and the features of the alignment produced by the decoder using the current weight vector."
The size of the update is controlled by a learning rate which decreases over time.
"At the end of each epoch, the weight vector is normalized and stored."
The final result is the average of the stored
"Inputs • training problems hP j , H j i, j = 1..n • corresponding gold-standard alignments E j • a number of learning epochs N (e.g. 50) • a “burn-in” period N 0 &lt; N (e.g. 10) • initial learning rate R 0 (e.g. 1) and multiplier r (e.g. 0.8) • a vector of feature functions Φ(E) • an alignment algorithm ALIGN (P,H;w) which finds a good alignment for hP, Hi using weight vector w"
Initialize • Set w = 0
"Repeat for i = 1 to N • Set R i = r · R i−1 , reducing the learning rate • Randomly shuffle the training problems • For j = 1 to n: – Set Ê j = ALIGN (P j , H j ; w) – Set w = w + R i · (Φ(E j ) − Φ(Ê j )) • Set w = w/kwk 2 (L2 normalization) • Set w[i] = w, storing the weight vector for this epoch"
Return an averaged weight vector: • w avg = 1/(N − N 0 )
"P Ni=N 0 +1 w[i] weight vectors, omitting vectors from a fixed num-ber of epochs at the beginning of the run (which tend to be of poor quality)."
"Using the parameter values suggested in figure 3, training runs on the RTE2 de-velopment set required about 20 hours."
"In this section, we describe experiments designed to evaluate the performance of various alignment sys-tems on the MSR gold-standard data described in section 3."
"For each system, we report precision, recall, and F-measure (F 1 ). [Footnote_7] Note that these are macro-averaged statistics, computed per problem by counting aligned token pairs, [Footnote_8] and then averaged over all problems in a problem set. [Footnote_9] We also re- port the exact match rate, that is, the proportion of problems in which the guessed alignment exactly matches the gold alignment."
"7 MT researchers conventionally report results in terms of alignment error rate (AER). Since we use only SURE links in the gold-standard data (see section 3), AER is equivalent to 1−F 1 ."
"8 For phrase-based alignments like those generated by MANLI, two tokens are considered to be aligned iff they are contained within phrases which are aligned."
"9 MT evaluations conventionally use micro-averaging, which gives greater weight to problems containing more aligned pairs. This makes sense in MT, where the purpose of alignment is to induce phrase tables. But in NLI, where the ultimate goal is to maximize the number of inference problems answered cor-rectly, it is more fitting to give all problems equal weight, and so we macro-average. We have also generated all results using micro-averaging, and found that the relative comparisons are"
The results are sum-marized in table 1.
"As a baseline, we use a simple alignment algorithm inspired by the lexical entailment model[REF_CITE], and similar to the simple heuristic model described[REF_CITE]."
"Each hy-pothesis word h is aligned to the premise word p to which it is most similar, according to a lexical sim-ilarity function sim(p,h) which returns scores in [0, 1]."
"While Glickman et al. used a function based on web co-occurrence statistics, we use a much sim-pler function based on string edit distance: dist(lem(w 1 ), lem(w 2 )) sim(w 1 , w 2 ) = 1 − max(|lem(w 1 )|, |lem(w 2 )|) (Here lem(w) denotes the lemma of word w; dist() denotes Levenshtein string edit distance; and | · | de-notes string length.)"
"This model can be easily extended to generate an alignment score, which will be of interest in sec-tion 6."
"We define the score for a specific hypoth-esis token h to be the log of its similarity with the premise token p to which it is aligned, and the score for the complete alignment of hypothesis H to premise P to be the sum of the scores of the to-kens in H, weighted by inverse document frequency in a large corpus [Footnote_10] (so that common words get less weight), and normalized by the length of H: score(h|P ) = log max sim(p, h) p∈P score(H|P ) = 1 X idf(h) · score(h|P ) |H| h∈H"
"10 We use idf(w) = log(N/N w ), where N is the number of documents in the corpus, and N w is the number of documents containing word w."
"Despite the simplicity of this alignment model, its performance is fairly robust, with good recall."
"Its precision, however, its mediocre—chiefly because, by design, it aligns every h with some p."
"The model could surely be improved by allowing it to leave some H tokens unaligned, but this was not pursued. not greatly affected."
"Given the importance of alignment for NLI, and the availability of standard, proven tools for MT align-ment, an obvious question presents itself: why not use an off-the-shelf MT aligner for NLI?"
"Although we have argued (section 2) that this is unlikely to succeed, to our knowledge, we are the first to inves-tigate the matter empirically. [Footnote_11]"
"11 However,[REF_CITE]explore a closely-related topic: using an MT aligner to identify paraphrases."
"The best-known MT aligner is undoubtedly GIZA++[REF_CITE], which contains im-plementations of various IBM models[REF_CITE], as well as the HMM model[REF_CITE]."
"Most practitioners use GIZA++ as a black box, via the Moses MT toolkit[REF_CITE]."
"We followed this practice, running with Moses’ de-fault parameters on the RTE2 data to obtain asym-metric word alignments in both directions (P-to-H and H-to-P)."
We then performed symmetrization using the well-known INTERSECTION heuristic.
"Unsurprisingly, the out-of-the-box performance was quite poor, with most words aligned apparently at random."
Precision was fair (72%) but recall was very poor (46%).
"Even equal words were usually not aligned—because GIZA++ is designed for cross-linguistic use, it does not consider word equality be-tween source and target sentences."
"To remedy this, we supplied GIZA++ with a lexicon, using a trick common in MT: we supplemented the training data with synthetic data consisting of matched pairs of equal words."
"This gives GIZA++ a better chance of learning that, e.g., man should align with man."
"The result was a big boost in recall (+23%), and a smaller gain in precision."
The results for GIZA++ shown in table 1 are based on using the lexicon and INTERSECTION .
"With these settings, GIZA++ prop-erly aligned most pairs of equal words, but contin-ued to align other words apparently at random."
"Next, we compared the performance of INTER - SECTION with other symmetrization heuristics de-fined in Moses—including UNION , GROW , GROW - DIAG , GROW - DIAG - FINAL (the default), and GROW - DIAG - FINAL - AND —and with asymmetric align-ments in both directions."
"While all these alterna-tives achieved better recall than INTERSECTION , all showed substantially worse precision and F 1 ."
"On the RTE2 test set, the asymmetric alignment from H to P scored 68% in F 1 ;[REF_CITE]%; and all other alternatives scored below 52%."
"As an additional experiment, we tested the Cross-EM aligner[REF_CITE]from the Berke-leyAligner package on the MSR data."
"While this aligner is in many ways simpler than GIZA++ (it lacks any model of fertility, for example), its method of jointly training two simple asymmetric HMM models has outperformed GIZA++ on standard eval-uations of MT alignment."
"As with GIZA++, we ex-perimented with a variety of symmetrization heuris-tics, and ran trials with and without a supplemental lexicon."
"The results were broadly similar: INTER - SECTION greatly outperformed alternative heuris-tics, and using a lexicon provided a big boost (up to 12% in F 1 )."
"Under optimal settings, the Cross-EM aligner showed better recall and worse preci-sion than GIZA++, with F 1 just slightly lower."
"Like GIZA++, it did well at aligning equal words, but aligned most other words at random."
"The mediocre performance of MT aligners on NLI alignment comes as no surprise, for reasons dis-cussed in section 2."
"Above all, the quantity of train-ing data is simply too small for unsupervised learn-ing to succeed."
"A successful NLI aligner will need to exploit supervised training data, and will need ac-cess to additional sources of knowledge about lexi-cal relatedness."
A better comparison is thus to an alignment sys-tem expressly designed for NLI.
"For this purpose, we used the alignment component of the Stanford RTE system[REF_CITE]."
"The Stanford aligner performs decoding and learning in a simi-lar fashion to MANLI, but uses a simpler, token-based alignment representation, along with a richer set of features for alignment scoring."
It represents alignments as an injective map from H tokens to P tokens.
"Phrase alignments are not directly repre-sentable, although the effect can be approximated by a pre-processing step which collapses multi-token named entities and certain collocations into single tokens."
"The features used for alignment scoring in-clude not only measures of lexical similarity, but also syntactic features intended to promote the align-ment of similar predicate-argument structures."
"Despite this sophistication, the out-of-the-box performance of the Stanford aligner is mediocre, as shown in table 1."
The low recall figures are partic-ularly noteworthy.
"However, a partial explanation is readily available: by design, the Stanford system ignores punctuation. [Footnote_12] Because punctuation tokens constitute about 15% of the aligned pairs in the MSR data, this sharply reduces measured recall."
"12 In fact, it operates on a dependency-graph representation from which punctuation is omitted."
"However, since punctuation matters little in inference, such re-call errors probably should be forgiven."
"Thus, ta-ble 1 also shows adjusted statistics for the Stanford system in which all recall errors involving punctua-tion are (generously) ignored."
"Even after this adjustment, the recall figures are unimpressive."
Error analysis reveals that the Stan-ford aligner does a poor job of aligning function words.
"While function words matter less in inference than nouns and verbs, they are not irrelevant, and because sentences often contain multiple instances of a par-ticular function word, matching them properly is by no means trivial."
"If matching prepositions and ar-ticles were ignored (in addition to punctuation), the gap in F 1 between the MANLI and Stanford systems would narrow to about 2.8%."
"Finally, the Stanford aligner is handicapped by its token-based alignment representation, often failing (partly or completely) to align multi-word phrases such as peace activists with protesters, or hackers with non-authorized personnel."
"As table 1 indicates, the MANLI aligner was found to outperform all other aligners evaluated on ev-ery measure of performance, achieving an F 1 score 10.5% higher than GIZA++ and 6.2% higher than the Stanford aligner (even with the punctuation cor-rection). [Footnote_13] MANLI achieved a good balance be-tween precision and recall, and matched more than 20% of the gold-standard alignments exactly."
13 Reported results for MANLI are averages over 10 runs.
Three factors seem to have contributed most to MANLI’s success.
"First, MANLI is able to outper-form the MT aligners principally because it is able to leverage lexical resources to identify the similar-ity between pairs of words such as jail and prison, prevent and stop, or injured and wounded."
"Second, MANLI’s contextual features enable it to do bet-ter than the Stanford aligner at matching function words, a weakness of the Stanford aligner discussed in section 5.3."
"Third, MANLI gains a marginal ad-vantage because its phrase-based representation of alignment permits it to properly align phrase pairs such as death penalty and capital punishment, or ab-dicate and give up."
"However, the phrase-based representation con-tributed far less than we had hoped."
"Setting MANLI’s maximum phrase size to 1 (effectively, restricting it to token-based alignments) caused F 1 to fall by just 0.2%."
"We do not interpret this to mean that phrase alignments are not useful—indeed, about 2.6% of the links in the gold-standard data in-volve phrases of size &gt; 1."
"Rather, we think it shows that we have failed to fully exploit the advantages of the phrase-based representation, chiefly because we lack lexical resources providing good informa-tion on similarity of multi-word phrases."
Error analysis suggests that there is ample room for improvement.
"A large proportion of recall errors (perhaps 40%) occur because the lexical similarity function assigns too low a value to pairs of words or phrases which are clearly similar, such as con-servation and protecting, server and computer net-works, organization and agencies, or bone fragility and osteoporosis."
Better exploitation of lexical re-sources could help to reduce such errors.
"Another important category of recall errors (about 12%) re-sult from the failure to identify one- and multi-word versions of the name of some entity, such as Lennon and John Lennon, or Nike Inc. and Nike."
A special-purpose similarity function could help here.
"Note, however, that about 10% of recall errors are un-avoidable, given our choice of alignment represen-tation, since they involve cases where the gold stan-dard aligns one or more tokens on one side to a non-contiguous set of tokens on the other side."
Precision errors may be harder to reduce.
"These errors are dominated by cases where we mistakenly align two equal function words (49% of precision er-rors), two forms of the verb to be (21%), two equal punctuation marks (7%), or two words or phrases of other types having equal lemmas (18%)."
"Be-cause such errors often occur because the aligner is forced to choose between nearly equivalent alter-natives, they may be difficult to eliminate."
"The re-maining 5% of precision errors result mostly from aligning words or phrases rightly judged to be highly similar, such as expanding and increasing, labor and birth, figures and number, or 223,000 and 220,000."
"In section 5, we evaluated the ability of aligners to recover gold-standard alignments."
"But since align-ment is just one component of the NLI problem, we might also examine the impact of different align-ers on the ability to recognize valid inferences."
"If a high-scoring alignment indicates a close correspon-dence between H and P, does this also indicate a valid inference?"
"We have previously emphasized[REF_CITE]that there is more to infer-ential validity than close lexical or structural corre-spondence: negations, modals, non-factive and im-plicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment."
"Nevertheless, alignment score can be a strong pre-dictor of inferential validity, and some NLI systems (e.g.,[REF_CITE]) rely entirely on some measure of alignment quality to predict validity."
"If an aligner generates real-valued alignment scores, we can use the RTE data to test its ability to predict inferential validity with the following simple method."
"For a given RTE problem, we predict YES (valid) if its alignment score [Footnote_14] exceeds a threshold τ, and NO otherwise."
"14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally."
"We tune τ to maximize accu-racy on the RTE2 development set, and then measure performance on the RTE2 test set using the same τ."
"Table 2 shows results for several NLI aligners, along with some results for complete RTE systems, including the LCC system (the top performer at RTE2) and an average of all systems participating in RTE2."
"While none of the aligners rivals the perfor-mance of the LCC system, all achieve respectable results, and the Stanford and MANLI aligners out-perform the average RTE2 entry."
"Thus, even if align-ment quality does not determine inferential validity, many NLI systems could be improved by harnessing a well-designed NLI aligner."
"Given the extensive literature on phrase-based MT, it may be helpful further to situate our phrase-based alignment model in relation to past work."
The stan-dard approach to training a phrase-based MT system is to apply phrase extraction heuristics using word-aligned training sets[REF_CITE].
"Unfortunately, word alignment mod-els assume that source words are individually trans- lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional."
"More re-cently, several works[REF_CITE]have presented more unified phrase-based systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT per-formance."
We would argue that previous work in MT phrase alignment is orthogonal to our work.
"In MANLI, the need for phrases arises when word-based rep-resentations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality."
"In MT phrase alignment, it is beneficial to account for arbitrarily large phrases, since the larger contexts offered by these phrases can help realize more dependencies among translated words (e.g., word order, agreement, subcategorization)."
"Per-haps because MT phrase alignment is dealing with much larger contexts, no existing work in MT phrase alignment (to our knowledge) directly models word insertions and deletions, as in MANLI."
"For exam-ple, in figure 1, MANLI can just skip In most Pacific countries there, while an MT phrase-based model would presumably align In most Pacific countries there are to Women are."
"Hence, previous work is of limited applicability to our problem."
"While MT aligners succeed by unsupervised learn-ing of word correspondences from massive amounts of bitext, NLI aligners are forced to rely on smaller quantities of supervised training data."
"With the MANLI system, we have demonstrated how to over-come this lack of data by utilizing external lexical resources, and how to gain additional power from a phrase-based representation of alignment."
The authors wish to thank the anonymous reviewers for their helpful comments on an earlier draft of this paper.
This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM and in part by the CIA ATP as part of the OCCAM project.
We introduce a method for solving substi-tution ciphers using low-order letter n-gram models.
"This method enforces global con-straints using integer programming, and it guarantees that no decipherment key is over-looked."
We carry out extensive empirical ex-periments showing how decipherment accu-racy varies as a function of cipher length and n-gram order.
We also make an empirical in-vestigation of Shannon’s (1949) theory of un-certainty in decipherment.
A number of papers have explored algorithms for automatically solving letter-substitution ciphers.
"Some use heuristic methods to search for the best de-terministic key[REF_CITE], often using word dictionaries to guide that search."
Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models[REF_CITE].
"In this paper, we introduce an exact decipherment method based on integer programming."
"We carry out extensive de-cipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods."
We also empirically explore the concepts in Shan-non’s (1949) paper on information theory as applied to cipher systems.
"We provide quantitative plots for uncertainty in decipherment, including the famous unicity distance, which estimates how long a cipher must be to virtually eliminate such uncertainty."
We find the ideas in Shannon’s (1949) paper rel-evant to problems of statistical machine translation and transliteration.
"When first exposed to the idea of statistical machine translation, many people natu-rally ask: (1) how much data is needed to get a good result, and (2) can translation systems be trained without parallel data?"
"These are tough questions by any stretch, and it is remarkable that Shannon was already in the 1940s tackling such questions in the realm of code-breaking, creating analytic formulas to estimate answers."
"Our novel contributions are as follows: • We outline an exact letter-substitution deci-pherment method which: - guarantees that no key is overlooked, and - can be executed with standard integer pro-gramming solvers • We present empirical results for decipherment which: - plot search-error-free decipherment results at various cipher lengths, and - demonstrate accuracy rates superior to EM-based methods • We carry out empirical testing of Shannon’s formulas for decipherment uncertainty"
We work on letter substitution ciphers with spaces.
"We look for the key (among 26! possible ones) that, when applied to the ciphertext, yields the most English-like result."
"We take “English-like” to mean most probable according to some statistical lan-guage model, whose job is to assign some proba-bility to any sequence of letters."
"According to a 1-gram model of English, the probability of a plaintext p 1 ...p n is given by:"
P(p 1 ...p n ) = P(p 1 ) · P(p 2 ) · ... · P(p n )
"That is, we obtain the probability of a sequence by multiplying together the probabilities of the in-dividual letters that make it up."
"This model assigns a probability to any letter sequence, and the proba-bilities of all letter sequences sum to one."
We col-lect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium[REF_CITE].
We also estimate 2- and 3-gram models using the same re-sources:
P(p 1 ...p n ) = P(p 1 | START) ·
P(p 2 | p 1 ) · P(p 3 | p 2 ) · ... · P(p n | p n−1 ) · P(END | p n )
P(p 1 ...p n ) = P(p 1 | START) ·
P(p 2 | START p 1 ) · P(p 3 | p 1 p 2 ) · ... · P(p n | p n−2 p n−1 ) ·
P(END | p n−1 p n )
"Unlike the 1-gram model, the 2-gram model will assign a low probability to the sequence “ae” be-cause the probability P (e | a) is low."
"Of course, all these models are fairly weak, as already known[REF_CITE]."
"When we stochastically generate text according to these models, we get, for example:"
We can further estimate the probability of a whole English sentence or phrase.
"For example, the prob-abilities of two plaintext phrases “het oxf” and “the fox” (which have the same letter frequency distribution) is shown below."
"The 1-gram model which counts only the frequency of occurrence of each letter in the phrase, estimates the same proba-bility for both the phrases “het oxf” and “the fox”, since the same letters occur in both phrases."
"On the other hand, the 2-gram and 3-gram models, which take context into account, are able to distinguish be-tween the English and non-English phrases better, and hence assign a higher probability to the English phrase “the fox”."
Model P(het oxf)
P(the fox) 1-gram 1.83 × 10 −9 1.83 × 10 −9 2-gram 3.26 × 10 −11 1.18 × 10 −7 3-gram 1.89 × 10 −13 1.04 × 10 −6
"Over a longer sequence X of length N, we can also calculate −log 2 (P(X))/N, which (per Shan-non) gives the compression rate permitted by the model, in bits per character."
"In our case, we get: 1 [Footnote_1]-gram: 4.19 [Footnote_2]-gram: 3.51 3-gram: 2.93"
1-gram: ... thdo detusar ii c ibt deg irn toihytrsen ... 2-gram: ... itariaris s oriorcupunond rke uth ... 3-gram: ... ind thnowelf jusision thad inat of ... 4-gram: ... rece bence on but ther servier ... 5-gram: ... mrs earned age im on d the perious ... 6-gram: ... a party to possible upon rest of ... 7-gram: ... t our general through approve the ...
"2 For an overview of integer and linear programming, see for example[REF_CITE]."
"Given a ciphertext c 1 ...c n , we search for the key that yields the most probable plaintext p 1 ...p n ."
"How-ever, we can still find the best one in a guaranteed fashion."
We do this by taking our most-probable-plaintext problem and casting it as an integer pro-gramming problem. 2
"Here is a sample integer programming problem: variables: x, y minimize: 2x + y subject to: x + y &lt; 6.9 y − x &lt; 2.5 y &gt; 1.1"
We require that x and y take on integer values.
"A solution can be obtained by typing this integer pro-gram into the publicly available lp solve program, or the commercially available CPLEX program, which yields the result: x = 4, y = 2."
"Suppose we want to decipher with a 2-gram lan-guage model, i.e., we want to find the key that yields the plaintext of highest 2-gram probability."
"Given the ciphertext c 1 ...c n , we create an integer program-ming problem as follows."
"First, we set up a net-work of possible decipherments (Figure 1)."
Each of the 27 · 27 · (n − 1) links in the network is a binary variable in the integer program—it must be assigned a value of either 0 or 1.
"We name these variables link XY Z , where X indicates the column of the link’s source, and Y and Z represent the rows of the link’s source and destination (e.g. variables link 1aa , link 1ab , link 5qu , ...)."
Each distinct left-to-right path through the net-work corresponds to a different decipherment.
"For example, the bold path in Figure 1 corresponds to the decipherment “decade”."
Decipherment amounts to turning some links “on” (assigning value 1 to the link variable) and others “off” (assigning value 0).
"Not all assignments of 0’s and 1’s to link variables result in a coherent left-to-right path, so we must place some “subject to” constraints in our integer program."
"We observe that a set of variables forms a path if, for every node in columns 2 through n−1 of the net-work, the following property holds: the sum of the values of the link variables entering the node equals the sum of the link variables leaving the node."
"For nodes along a chosen decipherment path, this sum will be 1, and for others, it will be 0. [Footnote_3]"
"3 Strictly speaking, this constraint over nodes still allows multiple decipherment paths to be active, but we can rely on the rest of our integer program to select only one."
"Therefore, we create one “subject to” constraint for each node (“ ” stands for space)."
"For example, for the node in column 2, row e we have: subject to: link 1ae + link 1be + link 1ce + ... + link 1 e = link 2ea + link 2eb + link 2ec + ... + link 2e"
Now we set up an expression for the “minimize” part of the integer program.
Recall that we want to select the plaintext p 1 ...p n of highest probability.
"For the 2-gram language model, the following are equivalent: (a) Maximize P(p 1 ...p n ) (b) Maximize log 2 P (p 1 ...p n ) (c) Minimize −log 2 P (p 1 ...p n ) (d) Minimize −log 2 [ P (p 1 | ST ART ) −log 2 P (p n | p n−1 ) −log 2 P (END | p n )"
"We can guarantee this last outcome if we con-struct our minimization function as a sum of 27 · 27 · (n − 1) terms, each of which is a link XY Z variable multiplied by −log 2 P (Z|Y ):"
Minimize link 1aa · −log 2 P (a | a) + link 1ab · −log 2 P (b | a) + link 1ac · −log 2 P (c | a) + ... + link 5qu · −log 2 P (u | q) + ...
"When we assign value 1 to link variables along some decipherment path, and 0 to all others, this function computes the negative log probability of that path."
We must still add a few more “subject to” con-straints.
We need to ensure that the chosen path im-itates the repetition pattern of the ciphertext.
"While the bold path in Figure 1 represents the fine plain-text choice “decade”, the dotted path represents the choice “ababab”, which is not consistent with the repetition pattern of the cipher “QWBSQW”."
"To make sure our substitutions obey a consistent key, we set up 27 · 27 = 729 new key xy variables to represent the choice of key."
"These new variables are also binary, taking on values 0 or 1."
"If variable key aQ = 1, that means the key maps plaintext a to ciphertext Q."
"Clearly, not all assignments to these 729 variables represent valid keys, so we augment the “subject to” part of our integer program by re-quiring that for any letter x, subject to: key xA + key xB + ... + key xZ + key x = 1 key Ax + key Bx + ... + key Zx + key x = 1"
"That is, every plaintext letter must map to exactly one ciphertext letter, and every ciphertext letter must map to exactly one plaintext letter."
We also add a constraint to ensure that the ciphertext space charac-ter maps to the plaintext space character: subject to: key = 1
"Finally, we ensure that any chosen decipherment path of link XYZ variables is consistent with the chosen key."
"We know that for every node A along the decipherment path, exactly one active link has A as its destination."
"For all other nodes, zero active links lead in."
"Suppose node A represents the de-cipherment of ciphertext letter c i as plaintext letter p j —for all such nodes, we stipulate that the sum of values for link (i−1)xp j (for all x) equals the value of key p j c i ."
"In other words, whether a node lies along the chosen decipherment path or not, the chosen key must support that decision."
Figure 2 summarizes the integer program that we construct from a given ciphertext c 1 ...c n .
The com-puter code that transforms any given cipher into a corresponding integer program runs to about one page.
Variations on the decipherment network yield 1-gram and 3-gram decipherment capabilities.
"Once an integer program is generated by machine, we ask the commercially-available CPLEX software to solve it, and then we note which key XY variables are assigned value 1."
"Because CPLEX computes the optimal key, the method is not fast—for ciphers of length 32, the number of variables and constraints encoded in the integer program (IP) along with aver-age running times are shown below."
It is possible to obtain less-than-optimal keys faster by interrupting the solver.
"We solve these with 1-gram, 2-gram, and 3-gram language models."
"We record the average percentage of ciphertext tokens decoded incorrectly. 50% error means half of the ciphertext tokens are deciphered wrong, while 0% means perfect decipherment."
Here we illustrate some automatic decipherments with er-ror rates: 42% error: the avelage ongrichman hal cy wiof a sevesonme qus antizexty that he buprk lathes we blung than soment - fotes mmasthes 11% error: the average englishman has so week a reference for antiality that he would rather be prong than recent - deter quarteur 2% error: the average englishman has so keep a reference for antiquity that he would rather be wrong than recent - peter mcarthur 0% error: the average englishman has so deep a reverence for antiquity that he would rather be wrong than recent - peter mcarthur
Figure 3 shows our automatic decipherment re-sults.
"We note that the solution method is exact, not heuristic, so that decipherment error is not due to search error."
Our use of global key constraints also leads to accuracy that is superior[REF_CITE].
"With a 2-gram model, their EM algorithm gives 10% error for a 414-letter cipher, while our method provides a solution with only 0.5% error."
"At shorter cipher lengths, we observe much higher improvements when using our method."
"For exam- ple, on a 52-letter textbook cipher, using a 2-gram model, the solution from our method resulted in 21% error as compared to 85% error given by the EM so-lution."
We see that deciphering with 3-grams works well on ciphers of length 64 or more.
This confirms that such ciphers can be attacked with very limited knowledge of English (no words or grammar) and little custom programming.
"The 1-gram model works badly in this scenario, which is consistent with Bauer’s (2006) observation that for short texts, mechanical decryption on the ba-sis of individual letter frequencies does not work."
"If we had infinite amounts of ciphertext and plaintext drawn from the same stochastic source, we would expect the plain and cipher frequencies to eventually line up, allowing us to read off a correct key from the frequency tables."
The upper curve in Figure 3 shows that convergence to this end is slow.
Very short ciphers are hard to solve accurately.
"For example, given a short cipher like XY Y X, we can never be sure if the answer is peep, noon, anna, etc."
"Shannon defined a mathemat-ical measure of our decipherment uncertainty, which he called equivocation (now called entropy)."
"Let C be a cipher, M be the plaintext message it encodes, and K be the key by which the encoding takes place."
"Before even seeing C, we can compute our uncertainty about the key K by noting that there are 26! equiprobable keys: [Footnote_4]"
"4[REF_CITE]The entropy associated with a set of pos-sible events whose probabilities of occurrence are p 1 , p 2 , ..., p n is given by H = − P n p · log 2 (p i ). i=1 i"
H(K) = −(26!) · (1/26!) · log 2 (1/26!) = 88.4 bits
"That is, any secret key can be revealed in 89 bits."
"When we actually receive a cipher C, our uncer-tainty about the key and the plaintext message is re-duced."
"Shannon described our uncertainty about the plaintext message, letting m range over all decipher-ments:"
H(M|C) = equivocation of plaintext message = − X P(m|C) · log 2 P(m|C) m
"P (m|C) is probability of plaintext m (according to the language model) divided by the sum of proba-bilities of all plaintext messages that obey the repeti-tion pattern of C. While integer programming gives us a method to find the most probable decipherment without enumerating all keys, we do not know of a similar method to compute a full equivocation with-out enumerating all keys."
"Therefore, we sample up to 100,000 plaintext messages in the neighborhood of the most probably decipherment [Footnote_5] and compute H(M|C) over that subset. [Footnote_6]"
"5 The sampling used to compute H(M|C) starts with the optimal key and expands out a frontier, by swapping letters in the key, and recursing to generate new keys (and corresponding plaintext message decipherments). The plaintext messages are remembered so that the frontier expands efficiently. The sam-pling stops if 100,000 different messages are found."
"6 Interestingly, as we grow our sample out from the most probable plaintext, we do not guarantee that any intermediate result is a lower bound on the equivocation. An example is pro-vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadily increases. However, if we add a 14th item whose P(m) is 0.12, the entropy suddenly decreases from 2.79 to 2.78."
"Shannon also described H(K|C), the equivoca-tion of key."
"This uncertainty is typically larger than H(M|C), because a given message M may be de-rived from C via more than one key, in case C does not contain all 26 letters of the alphabet."
"We compute H(K|C) by letting r(C) be the number of distinct letters in C, and letting q(C) be (26 − r(C))!."
"Letting i range over our sample of plaintext messages, we get:"
H(K|C) = equivocation of key = − X q(C) · (P(i)/q(C)) · log 2 (P(i)/q(C)) i = − X P(i) · log 2 (P(i)/q(C)) i = − X P(i) · (log 2 P(i) − log 2 q(C)) i = − X P(i) · log 2 P(i) + X P(i) · log 2 q(C) i i = H(M|C) + log 2 q(C)
"Shannon’s curve is drawn for a human-level language model, and the y-axis is given in “decimal digits” instead of bits."
"For comparison, we plot in Figures 5 and 6 the av-erage equivocations as we empirically observe them using our 1-, 2-, and 3-gram language models."
"The shape of the key equivocation curve follows Shannon, except that it is curved from the start, rather than straight."
"The message equivocation curve follows Shan-non’s prediction, rising then falling."
"Because very short ciphers have relatively few solutions (for ex- ample, a one-letter cipher has only 26), the overall uncertainty is not that great. [Footnote_7] As the cipher gets longer, message equivocation rises."
"7 Uncertainty is only loosely related to accuracy—even if we are quite certain about a solution, it may still be wrong."
"At some point, it then decreases, as the cipher begins to reveal its secret through patterns of repetition."
Shannon’s analytic model also predicts a sharp decline of message equivocation towards zero.
He defines the unicity distance (U) as the cipher length at which we have virtually no more uncertainty about the plaintext.
"Using analytic means (and vari-ous approximations), he gives:"
U = H(K)/(A − B) where: A = bits per character of a 0-gram model (4.7) B = bits per character of the model used to decipher
"For a human-level language model (B ∼ 1.2), he concludes U ∼ 25, which is confirmed by practice."
"For our language models, the formula gives:"
U = 173 (1-gram) U = 74 (2-gram) U = 50 (3-gram)
"These numbers are in the same ballpark[REF_CITE], who gives 167, 74, and 59."
"We note that these predicted unicity distances are a bit too rosy, according to our empirical message equivoca-tion curves."
"Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution."
We provide a method for deciphering letter substi-tution ciphers with low-order models of English.
"This method, based on integer programming, re-quires very little coding and can perform an opti-mal search over the key space."
"We conclude by not-ing that English language models currently used in speech recogniti[REF_CITE]and automated language translati[REF_CITE]are much more powerful, employing, for example, 7-gram word models (not letter models) trained on trillions of words."
"Obtaining optimal keys accord-ing to such models will permit the automatic deci-pherment of shorter ciphers, but this requires more specialized search than what is provided by gen-eral integer programming solvers."
"Methods such as these should also be useful for natural language decipherment problems such as character code con-version, phonetic decipherment, and word substitu-tion ciphers with applications in machine translati[REF_CITE]."
"The authors wish to gratefully acknowledge Jonathan Graehl, for providing a proof to support the argument that taking a larger number of samples does not necessarily increase the equivocation."
This research was supported by the Defense Advanced Research Projects Agency under SRI International’s prime Contract[REF_CITE].
"To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is intro-duced to exploit multi-level linguistic knowl-edge."
"In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for in-tegrating multi-level knowledge sources."
"Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs."
"Moreover, as the knowledge in each level is modeled independently and the combination is processed in the model level, the information inherently in each knowledge source has a chance to be thoroughly ex-ploited."
"By simulations, the effectiveness of the analyzer is investigated, and then a LVCSR system embedding the presented ana-lyzer is evaluated."
"Experimental results reveal that this unified framework is an effective ap-proach which significantly improves the per-formance of speech recognition with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used Mandarin speech recognition task."
"Language modeling is essential for large vocabu-lary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W , p(W )."
"Although the word-based n-gram language model remains the mainstream for most speech recognition systems, the utilization of linguistic knowledge is too limited in this model."
"Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language[REF_CITE]."
"Recently, structured language models have been introduced to make use of syntactic hierarchi-cal characteristics[REF_CITE]."
"Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models."
"In contrast, the class-based language model groups the words that have similar functions of syntax or semantics into mean-ingful classes."
"As a result, it handles the questions of data sparsity and generalization of unseen event."
"In practice, the part-of-speech (POS) information, cap-turing the syntactic role of words, has been widely used in clustering words[REF_CITE]."
"In Heeman’s POS language model[REF_CITE], the joint probability of word sequence and associ-ated POS sequence was estimated directly, which has been demonstrated to be superior to the condi-tional probability previously used in the class-based models[REF_CITE]."
"Moreover, a SuperARV language model was presented[REF_CITE], in which lexical features and syntactic con-straints were tightly integrated into a linguistic struc-ture of SuperARV serving as a class in the model."
"Thus, these knowledge was integrated in the rep-resentation level, and then the joint probabilities of words and corresponding SuperARVs were esti-mated."
"However, in the class-based language mod-els, words are taken as the model units, while other units smaller or larger than words are unfeasible for modeling simultaneously, such as the Chinese char-acters for Chinese names."
"Usually, speech recognition systems can only rec-ognize the words within a predefined dictionary."
"With the increase of unknown words, i.e., out-of-vocabulary (OOV) words, the performance will de-grade dramatically."
"This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected."
"Thus, many efforts have been made to deal with the is-sue of OOV words[REF_CITE], and various model units smaller than words have been examined to rec-ognize OOVs from speech, such as phonemes[REF_CITE], variable-length phoneme se-quence[REF_CITE], syllable[REF_CITE]and sub-word[REF_CITE]."
"Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed[REF_CITE]."
All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively.
"In this study, a uni-fied framework based approach, which aims to ex-ploit information from multi-level linguistic knowl-edge, is presented."
"Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose."
"WFSTs were formerly introduced to simplify the integration of models in speech recog-nition, including acoustic models, phonetic mod-els and word n-gram[REF_CITE]."
"In recent years, the WFST has been suc-cessfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project[REF_CITE], IBM[REF_CITE]and AT&amp;T[REF_CITE], and in various fields of natural language processing, such as smoothed n-gram model, partial parsing[REF_CITE], named entities recogniti[REF_CITE], semantic interpretati[REF_CITE]and machine translati[REF_CITE]."
"In[REF_CITE], the WFST has been further used for language model adaptation, where language models of different vo-cabularies that represented different styles were in-tegrated through the framework of speech transla-tion."
"In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm[REF_CITE]com-bines these representations flexibly and efficiently."
"Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance."
"The combined WFST will be more efficient by optimizing with determiniza-tion, minimization and pushing algorithms of WF-STs[REF_CITE]."
"Besides, the researches on opti-mizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently ([REF_CITE]; Diamantino[REF_CITE])."
"In this study, we extend the linguistic knowledge used in speech recognition."
"As WFSTs provide a common and natural representation for lexical con-straints, n-gram language model, Hidden Markov Model models and context-dependency, multi-level knowledge sources can be encoded into WFSTs un-der the uniform transducer representation."
Then this group of WFSTs is flexibly combined together to obtain an analyzer representing knowledge of per-son and location names as well as POS information.
"Afterwards, the presented analyzer is incorporated into LVCSR to evaluate the linguistic correctness of recognition candidates by an n-best rescoring."
"Unlike other methods, this approach holds two distinct features."
"Firstly, as all multi-level knowl-edge sources are modeled independently, the model units such as character, words, phrase, etc., can be chosen freely."
"Meanwhile, the integration of these information sources is conducted in the model level rather than the representation level."
This setup will help to model each knowledge source sufficiently and may promote the accuracy of speech recogni-tion.
"Secondly, under this unified framework, it is easy to combine additional knowledge source into the framework with the only requirement that the new knowledge source can be represented by WF-STs."
"Moreover, since all knowledge sources are fi-nally represented by a single WFST, additional ef-forts are not required for decoding the new knowl-edge source."
The remainder of this paper is structured as fol-lows.
"In section 2, we introduce our analyzer in de-tail, and incorporate it into a Mandarin speech recog-nition system."
"In section 3, the simulations are per-formed to evaluate the analyzer and test its effective-ness when being applied to LVCSR."
The conclusion appears in section 4.
"In this section, we start by giving a brief descrip-tion on WFSTs."
"Then some special characteristics of Chinese are investigated, and the model units are fixed."
"Afterwards, each knowledge source is rep-resented with WFSTs, and then they are combined into a final WFST, so-called analyzer."
"At last, this analyzer is incorporated into Mandarin LVCSR."
"The Weighted Finite State Transducer (WFST) is the generalization of the finite state automata, in which, besides of an input label, an output label and a weight are also placed on each transition."
"With these labels, a WFST is capable of realizing a weighted re-lation between strings."
"In our system, log probabili-ties are adopted as transition weights and the relation between two strings is associated with a weight indi-cating the probability of the mapping between them."
"Given a group of WFSTs, each of which models a stage of a mapping cascade, the composition opera-tion provides an efficient approach to combine them into a single one[REF_CITE]."
"In particular, for two WFSTs R and S, the composition T = R o S represents the composition of relations realized by R and S. The combination is performed strictly on R’s output and S’s input."
"It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint opti-mal results for multi-level weighted relations."
This study primarily takes the person and location names as well as the POS information into account.
"To deal with Chinese OOV words, different from the western language in which the phoneme, sylla-ble or sub-word are used as the model units[REF_CITE], Chinese characters are taken as the basic units."
"In general, a person name of Han nation-ality consists of a surname and a given name usu-ally with one or two characters."
Surnames com-monly come from a fixed set that has been histori-cally used.
"According to a recent investigation on surnames involving 296 million people, 4100 sur-names are found, and 129 most used surnames ac-count for 87% (conducted by the Institute of Genet-ics and Developmental Biology, Chinese Academy of Sciences)."
"In contrast, the characters used in given names can be selected freely, and in many situ-ations, some commonly used words may also appear in names, such as ”胜利” (victory) and ”长江” (the Changjiang River)."
"Therefore, both Chinese charac-ters and words are considered as model units in this study, and a word re-segmentation process on recog-nition hypotheses is necessary, where an n-gram lan-guage model based on word classes is adopted."
"In this work, we ignore the word boundaries of n-best hypotheses and perform a word re-segmentation for names recognition."
"Given an input Chinese character, it is encoded by a finite state acceptor FSA input ."
"For example, the input ”合成分子时” (while synthesizing molecule) is represented as in Figure 1(a)."
"Then a dictionary is represented by a transducer with empty weights, denoted as F ST dict ."
"Figure 1(b) illustrates a toy dictionary listed in Ta-ble 1, in which a successful path encodes a mapping from a Chinese character sequence to some word in the dictionary."
"In practice, all Chinese charac- ters should appear in the dictionary for further in-corporating models of names."
"Then the combination of F SA input and F ST dict , F ST seg = F SA input ◦ FST dict , will result in a WFST embracing all the possible candidate segmentations."
Afterwards an n-gram language model based on word classes is used to weight the candidate segmentations.
"As in Fig-ure 2, a toy bigram with three words is depicted by W F ST n−gram , and the word classes are defined in Table 2."
"Here, both in the training and test stages, the strings of numbers or letters in sentences are ex- tracted according to the rules, and then substituted with the class tags, ”NUM” and ”LETTER” respec-tively."
"At the same time, the words, such as ”三月” and ”A型”, are replaced with ”NUM月” and ”LET-TER型” in the dictionary."
"In addition, name classes, including ”CNAME”, ”TNAME” and ”LOC”, will be set according to names recognition."
Hidden Markov Models (HMMs) are adopted both for names recognition and POS tagging.
"Here, each HMM is represented with two WFSTs."
"Tak-ing the POS tagging as an example, the toy POS WFSTs with 3 different tags are illustrated in Fig-ure 3."
"The emission probability of a word by a POS, (P(word/pos)), is represented as in Figure 3(a), and the bigram transition probabilities between POS tags are represented as in Figure 3(b), similar to the word n-gram."
"In terms of names recognition, the HMM states correspond to 30 role tags of names, some for model units of Chinese characters, such as surname, the first or second character of a given per-son name with two characters, the first or last charac-ter of a location name and so on, but others for model units of words, such as the word before or after a name, the words in a name and so on."
"When rec-ognizing the person names, since there is a big dif-ference between the translated names and the names of Han nationality, two types of person names are modeled separately, and substituted with two differ-ent class tags in the segmentation language model, as ”TNAME” and ”CNAME”."
"Some rules, which can be encoded into WFSTs, are responsible for the transformation from a role sequence to correspond-ing name class (for example, a role sequence might consist of the surname, the first character of the given name, and the second character of the given name, which will be transformed to ”CNAME” in FST seg )."
"Hence, taking names recognition into ac-count, a WFST, including all possible segmentations as well as recognized candidates of names, can be obtained as below, denoted as W F ST words :"
F SA input ◦
F ST dict ◦
W F ST ne ◦
W F SA n−gram (1) POS information is integrated as follows. (α ∗ W F ST words ) ◦
W F ST POS (2)
"Consequently, the desired analyzer, a combined WFST that represents multi-level linguistic knowl-edge sources, has been obtained."
"The presented analyzer models linguistic knowledge at different levels, which will be useful to find an optimal words sequence among a large number of speech recognition hypotheses."
"Thus in this re-search, the analyzer is incorporated after the first pass recognition, and the n-best hypotheses are reranked according to the total path scores adjusted with the analyzer scores as follows. +γ ∗ log (P Analyzer (W )) (3) where P AM (O|W ) and P LM (W ) are the acoustic and language scores produced in first pass decoding, and P Analyzer (W ) reflects the linguistic correctness of one hypothesis scored by the analyzer."
"Through the reranking paradigm, a new best sentence hypoth-esis is obtained."
"Under the unified framework, multi-level linguistic knowledge is represented by the analyzer as men-tioned above."
"To guarantee the effectiveness of the introduced framework in integrating knowledge sources, the analyzer is evaluated in this section."
Then the experiments using an LVCSR system in which the analyzer is embedded are performed.
"Considering the function of the analyzer, cascaded subtasks of word segmentation, names recognition and POS tagging can be processed jointly, while they are traditionally handled in a pipeline manner."
"Hence, a comparison between the analyzer and the pipeline system can be used to evaluate the effec-tiveness of the introduced framework for knowledge integration."
"As illustrated in Figure 4, two systems based on the presented analyzer and the pipeline manner are constructed respectively."
"The evaluation data came from the People’s Daily[REF_CITE]from January to June (annotated by the Institute of Computational Linguistics of Peking University [URL_CITE] ), among which the January to May data was taken as the training set, and the June data was taken as the test set (consisted of 21,143 sentences and about 1.2 million words)."
"The first two thou-sand sentences from the June data were extracted as the development set, used to fix the composition weight α in equation 2."
"A dictionary including about 113,000 words was extracted from the training data, in which a person or location name was accounted as a word in vocabulary, only when the number of its appearances was no less than three."
"In Figure 5, the analyzer is compared with the pipeline system, where the analyzer outperforms the pipeline manner on all the subtasks in terms of F 1-score metric."
"Furthermore to detect the differences, the statistical significance test using approximate randomization approach[REF_CITE]is done on the word segmentation results."
"Since there are more than 21,000 sentences in the test set, which is not appropriate for approximate randomization test, ten sets (500 sentences for each) are randomly selected from the test corpus."
"For each set, we run 1048576 shuffles twice and calculate the significance level p-value according to the shuffled results."
It has been shown that all p-value are less than 0.001 on the ten sets.
Accordingly the improvement is statistically significant.
"Actually, this significant improvement is reasonable, since the joint processing avoids error propagation and provides the opportunity of shar-ing information between different level knowledge sources."
"The superiority of this analyzer also shows that the integration of multi-level linguistic knowl-edge under the unified framework is effective, which may lead to improved LVCSR."
"In the baseline speech recognition system, the acoustic models consisted of context-dependent Initial-Final models, in which the left-to-right model topology was used to represent each unit."
"Accord-ing to the phonetic structures, the number of states in each model was set to 2 or 3 for initials, and 4 or 5 for tonal finals."
Each state was trained to have 32 Gaussian mixtures.
"Since in this work we focused on modeling knowl-edge of language in Mandarin LVCSR, only clean male acoustic models were trained with a speech database that contained about 360 hours speech of over 750 male speakers."
"This training data was picked up from three continuous Mandarin speech corpora: the 863-I, 863-II and Intel corpora."
The brief information about these three speech corpora was listed in Table 3.
"As in this work, the eval-uation data was the 1997 HUB-4 Mandarin broad-cast news evaluation data (HUB-4 test set), to bet-ter fit this task, the acoustic models were adapted by the approach of maximum a posterior (MAP) adaptation."
"The adaption data was drawn from the HUB4 training set, excluding the HUB-4 develop- ing set, where only the cleaned male speech data (data under condition f0 defined[REF_CITE]) was used."
"The partition for the clean data was done with the acoustic segmentation software CMUseg 0.5 [Footnote_2][REF_CITE], and finally 8.6 hours adaptation data was obtained."
2 Acoustic segmentation software downloaded[URL_CITE]
"The language model was a word-based trigram built on 60,000 words entries and trained with a cor-pus about 1.5 billion characters."
"The training set consisted of broadcast news data from the Xinhua News Agency released by LDC (Xinhua part of Chi-nese Gigaword), seven years data of People’s Daily[REF_CITE]to 2002 released by People’s Daily Online [URL_CITE] , and some other data from news web-sites, such as yahoo, sina and so on."
"In addition, the analyzer incorporated in speech recognition was trained with a larger corpus from People’s Daily of China, including the data in 1998 from January to June and the data in 2000 from January to November (annotated by the Institute of Computational Linguistics of Peking University)."
The December data in 2000 was taken as the devel-opment set used to fix the composition weight α in equation 2.
"In our experiments, the clean male speech data from the Hub-4 test set was used, and 238 sentences were finally extracted for testing."
"The weight of the ana-lyzer was empirically derived from the development set, including 649 clean male sentences from the de-vSet of HUB-4 Evaluation."
The recognition results are shown in Table 4.
The baseline system has a character error rate (CER) of 14.85%.
"When the an-alyzer is incorporated, a 9.9% relative reduction is achieved."
"Furthermore, we ran the statistical signif-icance test to detect the performance improvement, in which the approximate randomization approach[REF_CITE]was modified to output the significance level, p-value, for the CER metric."
"The p-levels pro-duced through two rounds of 1048576 shuffles are 0.0058 and 0.0057 respectively, both less than 0.01."
Thus the performance improvement imposed by the utilization of the analyzer is statistically significant.
"Addressing the challenges of Mandarin large vocab-ulary continuous speech recognition task, within the unified framework of WFSTs, this study presents an analyzer integrating multi-level linguistic knowl-edge."
"Unlike other methods, model units, such as characters and words, can be chosen freely in this approach since multi-level knowledge sources are modeled independently."
"As a consequence, the fi-nal analyzer can be derived from the combination of better optimized models based on proper model units."
"Along with two level knowledge sources, i.e., the person and location names as well as the part-of-speech information, the analyzer is built and evalu-ated by a comparative simulation."
Further evaluation is also conducted on an LVCSR system in which the analyzer is embedded.
"Experimental results consis-tently reveal that the approach is effective, and suc-cessfully improves the performance of speech recog-nition by a 9.9% relative reduction of character error rate on the HUB-4 test set."
"Also, the unified frame-work based approach provides a property of integrat-ing additional linguistic knowledge flexibly, such as organization name and syntactic structure."
"Further-more, the presented approach has a benefit of ef-ficiency that additional efforts are not required for decoding as new knowledge comes, since all knowl-edge sources are finally encoded into a single WFST."
"In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora."
"Since the n-grams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on fea-tures derived from readily available segmen-tation and metadata information for each cor-pus."
"Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task."
Many application domains in machine learning suf-fer from a dearth of matched training data.
"However, partially matched data sets are often available in abundance."
Past attempts to utilize the mismatched data for training often result in models that exhibit biases not observed in the target domain.
"In this work, we will investigate the use of the often readily available data segmentation and metadata attributes associated with each corpus to reduce the effect of such bias."
We will examine this approach in the con-text of language modeling for lecture transcription.
"Compared with other types of audio data, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with special termi-nologies[REF_CITE]."
"While we may have existing transcripts from general lectures or written text on the precise topic, training data that matches both the style and topic of the target lecture is often scarce."
"Thus, past research has investigated various adaptation and interpolation techniques to make use of partially matched corpora[REF_CITE]."
"Training corpora are often segmented into docu-ments with associated metadata, such as title, date, and speaker."
"For lectures, if the data contains even a few lectures on linear algebra, conventional lan-guage modeling methods that lump the documents together will tend to assign disproportionately high probability to frequent terms like vector and matrix."
Can we utilize the segmentation and metadata infor-mation to reduce the biases resulting from training data mismatch?
"In this work, we present such a technique where we weight each n-gram count in a standard n-gram language model (LM) estimation procedure by a rel-evance factor computed via a log-linear combina-tion of n-gram features."
"Utilizing features that cor-relate with the specificity of n-grams to subsets of the training documents, we effectively de-emphasize out-of-domain n-grams."
"By interpolating models, such as general lectures and course textbook, that match the target domain in complementary ways, and optimizing the weighting and interpolation pa-rameters jointly, we allow each n-gram probabil-ity to be modeled by the most relevant interpolation component."
"Using a combination of features derived from multiple partitions of the training documents, the resulting weighted n-gram model achieves up to a 1.2% absolute word error rate (WER) reduc-tion over a linearly interpolated baseline on a lecture transcription task."
"To reduce topic mismatch in LM estimation, we (2006) have previously assigned topic labels to each word by applying HMM-LDA[REF_CITE]to the training documents."
"Using an ad hoc method to reduce the effective counts of n-grams ending on topic words, we achieved better perplexity and WER than standard trigram LMs."
"Intuitively, de-emphasizing such n-grams will lower the transition probability to out-of-domain topic words from the training data."
"In this work, we further explore this intuition with a principled feature-based model, in-tegrated with LM smoothing and estimation to allow simultaneous optimization of all model parameters."
"To ad-dress the mismatch, they partition the training doc-uments by their metadata attributes and compute a measure of the likelihood that an n-gram will appear in a new partitioned segment."
"By pruning n-grams with generality probability below a given threshold, the resulting model achieves lower perplexity than a count-cutoff model of equal size."
"Complementary to our work, this technique also utilizes segmentation and metadata information."
"However, our model en-ables the simultaneous use of all metadata attributes by combining features derived from different parti-tions of the training documents."
"Given a limited amount of training data, an n-gram appearing frequently in a single document may be assigned a disproportionately high probability."
"For example, an LM trained from lecture transcripts tends to assign excessive probability to words from observed lecture topics due to insufficient coverage of the underlying document topics."
"On the other hand, excessive probabilities may also be assigned to n-grams appearing consistently across documents with mismatched style, such as the course textbook in the written style."
Traditional n-gram smoothing techniques do not address such issues of insufficient topic coverage and style mismatch.
One approach to addressing the above issues is to weight the counts of the n-grams according to the concentration of their document distributions.
"Assigning higher weights to n-grams with evenly spread distributions captures the style of a data set, as reflected across all documents."
"On the other hand, emphasizing the n-grams concentrated within a few documents focuses the model on the topics of the individual documents."
"In theory, n-gram weighting can be applied to any smoothing algorithm based on counts."
"However, because many of these algorithms assume integer counts, we will apply the weighting factors to the smoothed counts, instead."
"For modified Kneser-Ney smoothing[REF_CITE], applying n-gram weighting yields: p(w|h) = Pβ(hw)c̃ 0 (hw) + α(h)p(w|h 0 ) w β(hw)c̃(hw) where p(w|h) is the probability of word w given his-tory h, c̃ is the adjusted Kneser-Ney count, c̃ 0 is the discounted count, β is the n-gram weighting factor, α is the normalizing backoff weight, and h 0 is the backoff history."
"Although the weighting factor β can in general be any function of the n-gram, in this work, we will consider a log-linear combination of n-gram fea-tures, or β(hw) = exp(Φ(hw) · θ), where Φ(hw) is the feature vector for n-gram hw and θ specifies the parameter vector to be learned."
"To better fit the data, we allow independent parameter vectors θ o for each n-gram order o. Note that with β(hw) = 1, the model degenerates to the original modified Kneser-Ney formulation."
"Furthermore, β only specifies the relative weighting among n-grams with a common history h. Thus, scaling β(hw) by an arbitrary func-tion g(h) has no effect on the model."
"In isolation, n-gram weighting shifts probability mass from out-of-domain n-grams via backoff to the uniform distribution to improve the generality of the resulting model."
"However, in combination with LM interpolation, it can also distribute prob-abilities to LM components that better model spe-cific n-grams."
"For example, n-gram weighting can de-emphasize off-topic and off-style n-grams from general lectures and course textbook, respectively."
Tuning the weighting and interpolation parameters jointly further allows the estimation of the n-gram probabilities to utilize the best matching LM com-ponents.
"To address the issue of sparsity in the document topic distribution, we can apply n-gram weight-ing with features that measure the concentration of the n-gram distribution across documents."
"Simi-lar features can also be computed from documents partitioned by their categorical metadata attributes, such as course and speaker for lecture transcripts."
"Whereas the features derived from the corpus docu-ments should correlate with the topic specificity of the n-grams, the same features computed from the speaker partitions might correspond to the speaker specificity."
"By combining features from multiple partitions of the training data to compute the weight-ing factors, n-gram weighting allows the resulting model to better generalize across categories."
"To guide the presentation of the n-gram features below, we will consider the following example parti-tion of the training corpus."
Words tagged by HMM-LDA as topic words appear in bold.
ABA A CC A B AB B A A C CBA ABA ACB AA C A BB A
"One way to estimate the specificity of an n-gram across partitions is to measure the n-gram frequency f, or the fraction of partitions containing an n-gram."
"For instance, f(A) = 3/3, f(C) = 2/3."
"However, as the size of each partition increases, this ratio in-creases to 1, since most n-grams have a non-zero probability of appearing in each partition."
"Thus, an alternative is to compute the normalized entropy of the n-gramPdistribution S across the S partitions, or h = log−[Footnote_1]S s=1 p(s) log p(s), where p(s) is the fraction of an n-gram appearing in partition s. For example, the normalized entropy of the unigram C is h(C) = log−13 [ 26 log 26 + 64 log 46 + 0] = .58."
"1 HMM-LDA is performed using 20 states and 50 topics with a 3rd-order HMM. Hyperparameters are sampled with a log-normal Metropolis proposal. The model with the highest likeli-hood from among 10,000 iterations of Gibbs sampling is used."
N-grams clustered in fewer partitions have lower entropy than ones that are more evenly spread out.
"Following[REF_CITE], we also con-sider features derived from the HMM-LDA word topic labels. 1"
"Specifically, we compute the empir-ical probability t that the target word of the n-gram is labeled as a topic word."
"In the example corpus, t(C) = 3/6, t(A C) = 2/4."
All of the above features can be computed for any partitioning of the training data.
"To better illustrate the differences, we compute the features on a set of lecture transcripts (see Section 4.1) partitioned by lecture (doc), course, and speaker."
"Furthermore, we include the log of the n-gram counts c and random values between 0 and 1 as baseline features."
Table 1 lists all the features examined in this work and their values on a select subset of n-grams.
"To tune the n-gram weighting parameters θ, we ap-ply Powell’s method[REF_CITE]to numeri-cally minimize the development set perplexity[REF_CITE]."
"Although there is no guarantee against converging to a local minimum when jointly tuning both the n-gram weighting and interpolation parameters, we have found that initializing the pa-rameters to zero generally yields good performance."
"In this work, we evaluate the perplexity and WER of various trigram LMs trained with n-gram weighting on a lecture transcription task[REF_CITE]."
"The target data consists of 20 lectures from an in-troductory computer science course, from which we withhold the first 10 lectures for the development set ( CS Dev ) and use the last 10 for the test set ( CS Test )."
"For training, we will consider the course textbook with topic-specific vocabulary ( Textbook ), numerous high-fidelity transcripts from a variety of general seminars and lectures ( Lectures ), and the out-of-domain LDC Switchboard corpus of spon-taneous conversational speech ( Switchboard )[REF_CITE]."
Table 2 summarizes all the evaluation data.
"To compute the word error rate, we use a speaker-independent speech recognizer[REF_CITE]with a large-margin discriminative acoustic model[REF_CITE]."
The lectures are pre-segmented into utter-ances via forced alignment against the reference transcripts[REF_CITE].
"Since all the models con-sidered in this work can be encoded as n-gram back-off models, they are applied directly during the first recognition pass instead of through a subsequent n-best rescoring step."
"In Table 3, we compare the performance of n-gram weighting with the h doc document entropy feature for various modified Kneser-Ney smoothing config-urations[REF_CITE]on the Lec-tures dataset."
"Specifically, we considered varying the number of discount parameters per n-gram order from 1 to 10."
"The original and modified Kneser-Ney smoothing algorithms correspond to a setting of 1 and 3, respectively."
"Furthermore, we explored using both fixed parameter values estimated from n-gram count statistics and tuned values that minimize the development set perplexity."
"In this task, while the test set perplexity tracks the development set perplexity well, the WER corre-lates surprisingly poorly with the perplexity on both the development and test sets."
"Nevertheless, n-gram weighting consistently reduces the absolute test set WER by a statistically significant average of 0.3%, according to the Matched Pairs Sentence Segment Word Error test[REF_CITE]."
"Given that we obtained the lowest development set WER with the fixed 3-parameter modified Kneser-Ney smoothing, all subsequent experiments are conducted using this smoothing configuration."
"Applied to the Lectures dataset in isolation, n-gram weighting with the h doc feature reduces the test set WER by 0.3% by de-emphasizing the probability contributions from off-topic n-grams and shifting their weights to the backoff distributions."
"Ideally though, such weights should be distributed to on-topic n-grams, perhaps from other LM components."
"In Table 4, we present the performance of apply-ing n-gram weighting to the Lectures and Textbook models individually versus in combination via linear interpolation (LI), where we optimize the n-gram weighting and interpolation parameters jointly."
The interpolated model with n-gram weighting achieves perplexity improvements roughly additive of the re-ductions obtained with the individual models.
"How-ever, the 1.0% WER drop for the interpolated model significantly exceeds the sum of the individual re-ductions."
"Thus, as we will examine in more detail in Section 5.1, n-gram weighting allows probabili-ties to be shifted from less relevant n-grams in one component to more specific n-grams in another."
"With n-gram weighting, we can model the weight-ing function β(hw) as a log-linear combination of any n-gram features."
"In Table 5, we show the effect various features have on the performance of linearly interpolating Lectures and Textbook ."
"As the docu-ments from the Lectures dataset is annotated with course and speaker metadata attributes, we include the n-gram frequency f, entropy h, and topic proba-bility t features computed from the lectures grouped by the 16 unique courses and 299 unique speakers. [Footnote_2]"
"2 Features that are not applicable to a particular corpus (e.g. h course for Textbook ) are removed from the n-gram weighting computation for that component. Thus, models with course and speaker features have fewer tunable parameters than the others."
"In terms of perplexity, the use of the Random feature has negligible impact on the test set per-formance, as expected."
"On the other hand, the log(c) count feature reduces the perplexity by nearly 3%, as it correlates with the generality of the n-grams."
"By using features that leverage the infor-mation from document segmentation and associated metadata, we are generally able to achieve further perplexity reductions."
"Overall, the frequency and entropy features perform roughly equally."
"However, by considering information from the more sophisti-cated HMM-LDA topic model, the topic probability feature t doc achieves significantly lower perplexity than any other feature in isolation."
"In terms of WER, the Random feature again shows no effect on the baseline[REF_CITE].7%."
"However, to our surprise, the use of the simple log(c) feature achieves nearly the same WER im-provement as the best segmentation-based feature, whereas the more sophisticated features computed from HMM-LDA labels only obtain half of the re-duction even though they have the best perplexities."
"When comparing the performance of different n-gram weighting features on this data set, the per-plexity correlates poorly with the WER, on both the development and test sets."
"Fortunately, the features that yield the lowest perplexity and WER on the de-velopment set also yield one of the lowest perplex-ities and WERs, respectively, on the test set."
"Thus, during feature selection for speech recognition ap-plications, we should consider the development set WER."
"Specifically, since the differences in WER are often statistically insignificant, we will select the feature that minimizes the sum of the development set WER and log perplexity, or cross-entropy. [Footnote_3]"
3 The choice of cross-entropy instead of perplexity is par-tially motivated by the linear correlation reported[REF_CITE]between cross-entropy and WER.
"In Tables 5 and 6, we have underlined the per-plexities and WERs of the features with the lowest corresponding development set values (not shown) and bolded the lowest test set values."
The features that achieve the lowest combined cross-entropy and WER on the development set are starred.
"Unlike most previous work, n-gram weighting en-ables a systematic integration of features computed from multiple document partitions."
"In Table 6, we compare the performance of various feature combi-nations."
We experiment with incrementally adding features that yield the lowest combined development set cross-entropy and WER.
"Overall, this metric ap-pears to better predict the test set WER than either the development set perplexity or WER alone."
"Using the combined feature selection technique, we notice that the greedily selected features tend to differ in the choice of document segmentation and feature type, suggesting that n-gram weighting can effectively integrate the information provided by the document metadata."
"By combining features, we are able to further reduce the test set WER by a statis-tically significant (p &lt; 0.001) 0.2% over the best single feature model."
"While n-gram weighting with all three features is able to reduce the test set WER by 1.2% over the linear interpolation baseline, linear interpolation is not a particularly effective interpolation technique."
"In Table 7, we compare the effectiveness of n-gram weighting in combination with better interpolation techniques, such as count merging (CM)[REF_CITE]and generalized linear interpolation (GLI)[REF_CITE]."
"As expected, the use of more sophisticated interpolation techniques decreases the perplexity and WER reductions achieved by n-gram weighting by roughly half for a variety of feature combinations."
"However, all improvements remain statistically significant."
"Although the WER reductions from better inter-polation techniques are initially statistically signif-icant, as we add features to n-gram weighting, the differences among the interpolation methods shrink significantly."
"With all three features combined, the test set WER difference between linear interpolation and generalized linear interpolation loses its statisti-cal significance."
"In fact, we can obtain statistically the same[REF_CITE].4% using the simpler model of count merging and n-gram weighting with h course ."
"To obtain further insight into how n-gram weighting improves the resulting n-gram model, we present in Table 8 the optimized parameter values for the linear interpolation model between Lectures and Textbook using n-gram weighting with h doc and t doc features."
"Using β(hw) = exp(Φ(hw) · θ) to model the n-gram weights, a positive value of θ i corresponds to increasing the weights of the i th order n-grams with positive feature values."
"For the h doc normalized entropy feature, values close to 1 correspond to n-grams that are evenly dis-tributed across the documents."
"When interpolating Lectures and Textbook , we obtain consistently pos-itive values for the Lectures component, indicating a de-emphasis on document-specific terms that are unlikely to be found in the target computer science domain."
"On the other hand, the values correspond-ing to the Textbook component are consistently neg-ative, suggesting a reduced weight for mismatched style terms that appear uniformly across textbook sections."
"For t doc , values close to 1 correspond to n-grams ending frequently on topic words with uneven dis-tribution across documents."
"Thus, as expected, the signs of the optimized parameter values are flipped."
"By de-emphasizing topic n-grams from off-topic components and style n-grams from off-style com-ponents, n-gram weighting effectively improves the performance of the resulting language model."
"So far, we have assumed the availability of a large development set for parameter tuning."
"To obtain a sense of how n-gram weighting performs with smaller development sets, we randomly select utter-ances from the full development set and plot the test set perplexity in Figure 1 as a function of the devel-opment set size for various modeling techniques."
"As expected, GLI outperforms both LI and CM."
"However, whereas LI and CM essentially converge in test set perplexity with only 100 words of devel- opment data, it takes about 500 words before GLI converges due to the increased number of parame-ters."
"By adding n-gram weighting with the h course feature, we see a significant drop in perplexity for all models at all development set sizes."
"However, the performance does not fully converge until 3,000 words of development set data."
"As shown in Figure 2, the test set WER behaves more erratically, as the parameters are tuned to min-imize the development set perplexity."
"Overall, n-gram weighting decreases the WER significantly, except when applied to GLI with less than 1000 words of development data when the perplexity of GLI has not itself converged."
"In that range, CM with n-gram weighting performs the best."
"However, with more development data, GLI with n-gram weight-ing generally performs slightly better."
"From these results, we conclude that although n-gram weight-ing increases the number of tuning parameters, they are effective in improving the test set performance even with only 100 words of development set data."
"To characterize the effectiveness of n-gram weight-ing as a function of the training set size, we evalu-ate the performance of various interpolated models with increasing subsets of the Lectures corpus and the full Textbook corpus."
"Overall, every doubling of the number of training set documents decreases both the test set perplexity and WER by approximately 7 points and 0.8%, respectively."
"To better compare re-sults, we plot the performance difference between various models and linear interpolation in Figures 3 and 4."
"Interestingly, the peak gain obtained from n-gram weighting with the h doc feature appears at around 16 documents for all interpolation techniques."
"We suspect that as the number of documents initially increases, the estimation of the h doc features im-proves, resulting in larger perplexity reduction from n-gram weighting."
"However, as the diversity of the training set documents increases beyond a certain threshold, we experience less document-level spar-sity."
"Thus, we see decreasing gain from n-gram weighting beyond 16 documents."
"For all interpolation techniques, even though the perplexity improvements from n-gram weighting decrease with more documents, the WER reductions actually increase."
N-gram weighting showed sta-tistically significant reductions for all configurations except generalized linear interpolation with less than 8 documents.
"Although count merging with n-gram weighting has the lowest WER for most training set sizes, GLI ultimately achieves the best test set WER with the full training set."
"In Table 9, we compare the performance of n-gram weighting with different combination of training corpora and interpolation techniques to determine its effectiveness across different training conditions."
"With the exception of interpolating Lectures and Switchboard using count merging, all other model combinations yield statistically significant improve-ments with n-gram weighting using h course , t doc , and f doc features."
The results suggest that n-gram weighting with these features is most effective when interpolating corpora that differ in how they match the target do-main.
"Whereas the Textbook corpus is the only cor-pus with matching topic, both Lectures and Switch-board have a similar matching spoken conversa-tional style."
"Thus, we see the least benefit from n-gram weighting when interpolating Lectures and Switchboard ."
"By combining Lectures , Textbook , and Switchboard using generalized linear interpola-tion with n-gram weighting using h course , t doc , and f doc features, we achieve our best test set[REF_CITE].2% on the lecture transcription task, a full 1.5% over the initial linear interpolation baseline."
"In this work, we presented the n-gram weighting technique for adjusting the probabilities of n-grams according to a set of features."
"By utilizing features derived from the document segmentation and asso-ciated metadata inherent in many training corpora, we achieved up to a 1.2% and 0.6% WER reduc-tion over the linear interpolation and count merging baselines, respectively, using n-gram weighting on a lecture transcription task."
We examined the performance of various n-gram weighting features and generally found entropy-based features to offer the best predictive perfor-mance.
"Although the topic probability features derived from HMM-LDA labels yield additional improvements when applied in combination with the normalized entropy features, the computational cost of performing HMM-LDA may not justify the marginal benefit in all scenarios."
"In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied[REF_CITE]."
Synthetic metadata information may also be ob-tained via clustering techniques[REF_CITE].
"Although we have primarily focused on n-gram weighting features derived from segmentation information, it is also possible to consider other fea-tures that correlate with n-gram relevance."
N-gram weighting and other approaches to cross-domain language modeling require a matched devel-opment set for model parameter tuning.
"Thus, for future work, we plan to investigate the use of the ini-tial recognition hypotheses as the development set, as well as manually transcribing a subset of the test set utterances."
"As speech and natural language applications shift towards novel domains with limited matched train-ing data, better techniques are needed to maximally utilize the often abundant partially matched data."
"In this work, we examined the effectiveness of the n-gram weighting technique for estimating language models in these situations."
"With similar investments in acoustic modeling and other areas of natural lan-guage processing, we look forward to an ever in-creasing diversity of practical speech and natural language applications."
Availability An implementation of the n-gram weighting algorithm is available in the MIT Lan-guage Modeling (MITLM) toolkit[REF_CITE][URL_CITE].
Confusion networks are a simple representa-tion of multiple speech recognition or transla-tion hypotheses in a machine translation sys-tem.
A typical operation on a confusion net-work is to find the path which minimizes or maximizes a certain evaluation metric.
"In this article, we show that this problem is gener-ally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU."
This also holds for more complex representations like generic word graphs.
"In addition, we give an efficient polynomial-time algorithm to cal-culate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again."
"Since finding the optimal solution is thus not always feasible, we introduce an approximat-ing algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solu-tion for n-gram BLEU in polynomial time."
"In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences."
Typical applications include translation of different speech recognition hypothe-ses[REF_CITE]or system combinati[REF_CITE].
A typical operation on a given CN is to find the path which minimizes or maximizes a certain eval-uation metric.
"This operation can be used in ap-plications like Minimum Error Rate Training[REF_CITE], or optimizing system combination as de-scribed[REF_CITE]."
"Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described[REF_CITE], current research in MT uses more sophisti-cated measures, like the BLEU score[REF_CITE]."
"While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity."
The same can be expected for their BLEU algorithm.
"These word graphs have some properties which simplify the calculation; for exam-ple, no edge is labeled with the empty word, and all paths have the same length and end in the same node."
"Even then, their decoder does not optimize the true BLEU score, but an approximate version which uses a language-model-like unmodified pre-cision."
We give a very short introduction to CNs and the BLEU score in Section 2.
"In Section 3 we show that finding the best BLEU score is an NP-hard problem, even for a simplified variant of BLEU which only scores unigrams and bigrams."
"The main reason for this problem to be-come NP-hard is that by looking at bigrams, we al-low for one decision to also influence the following decision, which itself can influence the decisions af-ter that."
We also show that this also holds for uni-gram BLEU and the position independent error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols.
The con-catenation of symbols corresponds to the interde-pendency of decisions in the case of bigram matches above.
"NP-hard problems are quite common in machine translation; for example,[REF_CITE]has shown that even for a simple form of statistical MT mod-els, the decoding problem is NP-complete."
"More recently,[REF_CITE]have proven the NP-completeness of the phrase alignment problem."
"But even a simple, common procedure as BLEU scoring, which can be performed in linear time on single sentences, becomes a potentially intractable problem as soon as it has to be performed on a slightly more powerful representation, such as con-fusion networks."
This rather surprising result is the motivation of this paper.
"The problem of finding the best unigram BLEU score in an unaugmented variant of CNs is not NP-complete, as we show in Section 4."
We present an algorithm that finds such a unigram BLEU-best path in polynomial time.
"An important corollary of this work is that calcu-lating the BLEU-best path on general word graphs is also NP-complete, as CNs are a true subclass of word graphs."
"It is still desirable to calculate a “good” path in terms of the BLEU score in a CN, even if calculating the best path is infeasible."
"In Sec-tion 5, we present an algorithm which can calculate “good” solutions for CNs in polynomial time."
This algorithm can easily be extended to handle arbitrary word graphs.
"We assess the algorithm experimen-tally on real-world MT data in Section 6, and draw some conclusions from the results in this article in Section 7."
"A confusion network (CN) is a word graph where each edge is labeled with exactly zero or one sym-bol, and each path from the start node to the end node visits each node of the graph in canonical or-der."
"Usually, we represent unlabeled edges by label-ing them with the empty word ε."
"Within this paper, we represent a CN by a list of lists of words {w i,j }, where each w i,j corresponds to a symbol on an edge between nodes i and i + 1."
"A path in this CN can be written as a string of inte-gers, a n1 = a 1 , . . . , a n , such that the path is labeled w 1,a 1 w 2,a 2 . . . w n,a n ."
"Note that there can be a differ-ent number of possible words, j, for different posi-tions i."
"The BLEU score, as defined[REF_CITE], is the modified n-gram precision of a hy- pothesis, with 1 ≤ n ≤ N, given a set of reference translations R. “Modified precision” here means that for each n-gram, its maximum number of oc-currences within the reference sentences is counted, and only up to that many occurrences in the hypothe-sis are considered to be correct."
"The geometric mean over the precisions for all n is calculated, and mul-tiplied by a brevity penalty bp."
"This brevity penalty is 1.0 if the hypothesis sentence is at least as long as the reference sentence (special cases occur if multi-ple reference sentences with different length exists), and less than 1.0 otherwise."
"The exact formulation can be found in the cited paper; for the proofs in our paper it is enough to note that the BLEU score is 1.0 exactly if all n-grams in the hypothesis oc-cur at least that many times in a reference sentence, and if there is a reference sentence which is as long as or shorter than the hypothesis."
"Assuming that we can always provide a dummy reference sentence shorter than this length, we do not need to regard the brevity penalty in these proofs."
"Within the fol-lowing proofs of NP-hardness, we will only require confusion networks (and word graphs) which do not contain empty words, and where all paths from the start node to the end node have the same length."
"Usually, in the definition of the BLEU score, N is set to 4; within this article we denote this metric as 4BLEU."
"We can also restrict the calculations to un-igrams only, which would be 1BLEU, or to bigrams and unigrams, which we denote as 2BLEU."
"Similar to the 1BLEU metric is the Position in-dependent Error Rate PER[REF_CITE], which counts the number of substitutions, insertions, and deletions that have to be performed on the uni-gram counts to have the hypothesis counts match the reference counts."
"Unlike 1BLEU, for PER to be op-timal (here, 0.0), the reference counts must match the candidate counts exactly."
"Given a CN {w i,j } and a set of reference sen-tences R, we define the optimization problem"
Definition 1 (CN-2BLEU-OPTIMIZE)
"Among all paths a 1I through the CN, what is the path with the highest 2BLEU score?"
Related to this is the decision problem
Definition 2 (CN-2BLEU-DECIDE)
"Among all paths a I1 through the CN, is there a path with a 2BLEU score of 1.0?"
"Similarly we define CN-4BLEU-DECIDE, CN-PER-DECIDE, etc."
We now show that CN-2BLEU-DECIDE is NP-complete.
It is obvious that the problem is in NP:
"Given a path a 1I , which is polynomial in size to the problem, we can decide in polynomial time whether a I1 is a solution to the problem – namely by calcu-lating the BLEU score."
We now show that there is a problem known to be NP-complete which can be polynomially reduced to CN-2BLEU-DECIDE.
"For our proof, we choose 3SAT."
Consider the following problem:
"Definition 3 (3SAT) Let X {x 1 , . . . , x n }= be a set of Boolean variables, let F = V k (L ∨L i,2 ∨L i,3 ) be a Boolean formula, i=1 i,1 where each literal L i,j is either a variable x or its negate x. Is there a assignment β : X → {0,1} such that β |= F?"
"In other words, if we replace each x in F by β(x), and each x by 1 − β(x), does F become true?"
It has been shown[REF_CITE]that 3SAT is NP-complete.
"Consequently, if for another problem in NP there is polynomial-size and -time reduction of an arbitrary instance of 3SAT to an instance of this new problem, this new problem is also NP-complete."
"CN-2BLEU-DECIDE Let F be a Boolean formula in 3CNF, and let k be its size, as in Definition 3."
We will now reduce it to a corresponding CN-2BLEU-DECIDE problem.
"This means that we create an alphabet Σ, a confusion net-work C, and a set of reference sentences R, such that there is a path through C with a BLEU score of 1.0 exactly if F is solvable:"
"Create an alphabet Σ based on F as Σ := {x 1 , . . . , x n } ∪ {x 1 , . . . , x n } ∪ { }."
"Here, the x i and x i symbols will correspond to the variable with the same name or their negate, respectively, whereas will serve as an “isolator symbol”, to avoid un-wanted bigram matches or mismatches between sep-arate parts of the constructed CN or sentences."
"It is straightforward to modify the construction above to create an equivalent CN-4BLEU-DECIDE problem instead: Replace each occurrence of the isolating symbol in A, B, C, R by three consecu-tive isolating symbols ."
"Then, everything said about unigrams still holds, and bi-, tri- and four-grams are handled equivalently:"
"Previous unigram matches on correspond to uni-, bi-, and trigram matches on , , ."
"Bigram matches on  corre-spond to bi-, tri-, and fourgram matches on , , , and similar holds for bigram matches , , ."
"Unigram matches x, x, and bigram matches xx etc. stay the same."
"Consequently, CN-4BLEU-DECIDE is also an NP-complete problem."
Is it possible to get rid of the necessity for bi-gram counts in this proof?
"One possibility might be to look at slightly more powerful graph structures, CN*."
"In these graphs, each edge can be labeled by arbitrarily many symbols (instead of just zero or one)."
"Then, consider a CN* graph C 0 :="
"A 111000000111000111000111 B 0 , with B 0 as in Figure 2."
"R 0 := {(x 1 ) k (x 1 ) k . . . (x n ) k (x n ) k ( ) k } we can again assume that either x i or x i ap-pears k times in the B 0 -part of a path a K1 with 1BLEU(a K1 , R 0 ) = 1.0, and that for every solution β to F there is a corresponding path a K1 through C 0 and vice versa."
"In this construction, we also have exact matches of the counts, so we can also use PER in the decision problem."
"While CN* are generally not word graphs by themselves due to the multiple symbols on edges, it is straightforward to create an equivalent word graph from a given CN*, as demonstrated in Fig-ure 3."
"Consequently, deciding unigram BLEU and unigram PER are NP-complete problems for general word graphs as well."
"It is not a coincidence that we had to resort to bigrams or to edges with multiple symbols for NP-completeness: It turns out that CN-1BLEU-DECIDE, where the order of the words does not matter at all, can be decided in polynomial time using the following algorithm, which disregards a brevity penalty for the sake of simplicity:"
"Given a vocabulary X, a CN {w i,j }, and a set of reference sentences R together with their unigram BLEU counts c(x) : X → N and C := P x∈X c(x), 1. Remove all parts from w where there is an edge labeled with the empty word ε."
"This step will always increase unigram precision, and can not hurt any higher n-gram precision here, because n = 1."
"In the example in Figure 4, the edges labeled very and ε respectively are affected in this step. 2. Create nodes A 0 := {1, . . . , n}, one for each node with edges in the CN."
"In the example in Figure 5, the three leftmost column heads cor-respond to these nodes. 3."
"Create nodes B := {x.j | x ∈ X, 1≤j ≤c(x)}."
"In other words, create a unique node for each “running” word in R – e.g. if the first and second reference sentence contain x once each, and the third reference contains x twice, create exactly x.1 and x.2."
"In Figure 5, those are the row heads to the right. 4. Fill A with empty nodes to match the total length:"
A := A 0 ∪ {ε.j | 1 ≤ j ≤ C − n}.
"If n &gt; C, the BLEU precision can not be 1.0."
"The five rightmost columns in Figure 5 corre-spond to those. 5. Create edges E := {(i, w i,j .k) | 1≤i≤n, all j, 1≤c(w i,j )} ∪ {(i, ε.j) | 1 ≤ i ≤ n, all j}."
"These edges are denoted as ◦ or • in Figure 5. erences R, for which the best 1BLEU path can be constructed by the algorithm above."
"The bipartite graph constructed in Step 1 to Step 4 for this exam-ple, given in matrix form, can be found in Figure 5."
"Such a solution to Step 6, if found, corresponds exactly to a path through the confusion network with 1BLEU=1.0, and vice versa: for each position 1 ≤ i ≤ n, the matched word corresponds to the word that is selected for the position of the path; “surplus” counts are matched with εs."
Step 6 can be performed in polynomial time[REF_CITE]O((C + n) 5/2 ); all other steps in linear time O(C + n).
"Consequently, CN-1BLEU can be decided in polynomial time O((C + n) 5/2 )."
"Similarly, an actual optimum 1BLEU score can be calculated in O((C + n) 5/2 )."
"It should be noted that the only alterations in the hypothesis length, and as a result the only alterations in the brevity penalty, will come from Step 1."
"Con-sequently, the brevity penalty can be taken into ac-count as follows: Consider that there are M nodes with an empty edge in {w i,j }."
"Instead of remov-ing them in Step 1, keep them in, but for each 1 ≤ m ≤ M, run through steps 2 to 6, but add m nodes ε.1, . . . , ε.m to B in Step 3, and add corre-sponding edges to these nodes to E in Step 5."
"After each iteration (which leads to a constant hypothesis length), calculate precision and brevity penalty."
Se-lect the best product of precision and brevity penalty in the end.
The overall time complexity now is in M · O((C + n) 5/2 ).
A PER score can be calculated in a similar fash-ion.
"Knowing that the problem of finding the BLEU-best path is an NP-complete problem is an unsatisfactory answer in practice – in many cases, having a good, but not necessarily optimum path is preferable to having no good path at all."
"A simple approach would be to walk the CN from the start node to the end node, keeping track of n-grams visited so far, and choosing the word next which maximizes the n-gram precision up to this word."
"Track is kept by keeping n-gram count vec-tors for the hypothesis path and the reference sen-tences, and update those in each step."
"The main problem with this approach is that of-ten the local optimum is suboptimal on the global scale, for example if a word occurs on a later posi-tion again."
"As they suspect, the search space can become exponentially large."
"In this paper, we suggest a compromise between these two extremes, namely keeping active a suffi-ciently large number of “path hypotheses” in terms of n-gram precision, instead of only the first best, or of all."
"But even then, edges with empty words pose a problem, as stepping along an empty edge will never decrease the precision of the local path."
"In certain cases, steps along empty edges may affect the n-gram precision for higher n-grams."
"But this will only take effect after the next non-empty step, it does not influence the local decision in a node."
"Step-ping along a non-empty edge will often decrease the local precision, though."
"As a consequence, a simple algorithm will prefer paths with shorter hypotheses, which leads to a suboptimal total BLEU score, be-cause of the brevity penalty."
One can counter this problem for example by using a brevity penalty al-ready during the search.
"But this is problematic as well, because it is difficult to define a proper partial reference length in this case."
"The approach we propose is to compare only par-tial path hypotheses with the same number of empty edges, and ending in the same position in the confu-sion network."
This idea is illustrated in Figure 6: We compare only the partial precision of path hypothe-ses ending in the same node.
"Due to the simple na-ture of this search graph, it can easily be traversed in a left-to-right, top-to-bottom manner."
"With regard to a node currently being expanded, only the next node in the same row, and the corresponding columns in the next row need to be kept active."
"When imple-menting this algorithm, Hypotheses should be com-pared on the modified BLEUS precision[REF_CITE]because the original BLEU precision equals zero as long as there are no higher n-gram matches in the partial hypotheses, which renders meaningful comparison hard or impossible."
"In the rightmost column, all path hypotheses within a node have the same hypothesis length."
"Con-sequently, we can select the hypothesis with the best (brevity-penalized) BLEU score by multiplying the appropriate brevity penalty to the precision of the best path ending in each of these nodes."
"If we al-ways expand all possible path hypotheses within the nodes, and basically run a full search, we will al-ways find the BLEU-best path this way."
"From the proof above, it follows that the number of path hy-pothesis we would have to keep can become expo-nentially large."
"Fortunately, if a “good” solution is good enough, we do not have to keep all possible path hypotheses, but only the S best ones for a given constant S, or those with a precision not worse than c times the precision of the best hypothesis within the node."
"Assuming that adding and removing an element to/from a size-limited stack of size S takes time O(log S), that we allow at most E empty edges in a solution, and that there are j edges in each of the n positions, this algorithm has a time complexity of"
O(E · n · j · S log S).
"To reduce redundant duplicated path hypotheses, and by this to speed up the algorithm and reduce the risk that good path hypotheses are pruned, the confu-sion network should be simplified before the search, as shown in Figure 6: 1. Remove all words in the CN which do not ap-pear in any reference sentence, if there at least one “known” non-empty word at the same po-sition."
"If there is no such “known” word, re-place them all by a single token denoting the “unknown word”. 2. Remove all duplicate edges in a position, that is, if there are two or more edges carrying the same label in one position, remove all but one for them."
These two steps will keep at least one of the BLEU-best paths intact.
"But they can remove the average branching factor (j) of the CN significantly, which leads to a significantly lower number of duplicate path hypotheses during the search."
Our algorithm can easily be extended to handle ar-bitrary word graphs instead of confusion networks.
"In this case, each “row” in Figure 6 will reflect the structure of the word graph instead of the “linear” structure of the CN."
"While this algorithm searches for the best path for a single sentence only, a common task is to find the best BLEU score over a whole test set – which can mean suboptimal BLEU scores for in-dividual sentences."
This adds an additional com-binatorial problem over the sentences to the actual decoding process.
"In our exper-iments, we used the per-sentence BLEUS score as (greedy) decision criterion, as this is also the prun-ing criterion."
"One possibility to adapt this approach to Zens’s/Dreyer’s greedy approach for system-level BLEU scores might be to initialize n-gram counts and hypothesis length not to zero at the beginning of each sentence, but to those of the corpus so far."
"But as this diverts from our goal to optimize the sentence-level scores, we have not implemented it so far."
The question arises how many path hypotheses we need to retain in each step to obtain optimal paths.
"To examine this, we created confusion networks out of the translations of the four best MT systems of the[REF_CITE]and 2006 Chinese–English evalu-ation campaigns, as available from the Linguistic Data Consortium (LDC)."
"The hypotheses of the best single system served as skeleton, those of the three remaining systems were reordered and aligned to the skeleton hypothesis."
This corpus is described in Ta-ble 1.
"Figures 7 and 8 show the measured BLEU scores in three different definitions, versus the max-imum number of path hypotheses that are kept in each node of the search graph."
"Shown are the av-erage sentence-wise BLEUS score, which is what the algorithm actually optimizes, for comparison the average sentence-wise BLEU score, and the total document-wise BLEU score."
"All scores increase with increasing number of re-tained hypotheses, but stabilize around a total of 15 hypotheses per node."
"The difference over a greedy approach, which corresponds to a maximum of one hypothesis per node if we leave out the separation by path length, is quite significant."
"No further improve-ments can be expected for a higher number of hy-potheses, as experiments up to 100 hypotheses show."
"In this paper, we showed that deciding whether a given CN contains a path with a BLEU score of 1.0 is an NP-complete problem for n-gram lengths ≥ 2."
"The problem is also NP-complete if we only look at unigram BLEU, but allow for CNs where edges may contain multiple symbols, or for arbitrary word graphs."
"As a corollary, any proposed algorithm to find the path with an optimal BLEU score in a CN, even more in an arbitrary word graph, which runs in worst case polynomial time can only deliver an approximation [Footnote_1] ."
"1 provided that P 6= NP, of course."
"We gave an efficient polynomial time algorithm for the simplest variant, namely deciding on a uni-gram BLEU score for a CN."
"This algorithm can eas-ily be modified to decide on the PER score as well, or to calculate an actual unigram BLEU score for the hypothesis CN."
"Comparing these results, we conclude that the ability to take bi- or higher n-grams into account, be it in the scoring (as in 2BLEU), or in the graph structure (as in CN*), is the key to render the prob-lem NP-hard."
"Doing so creates long-range depen-dencies, which oppose local decisions."
We also gave an efficient approximating algo-rithm for higher-order BLEU scores.
"This algorithm is based on a multi-stack decoder, taking into ac-count the empty arcs within a path."
Experimental results on real-world data show that our method is indeed able to find paths with a significantly better
BLEU score than that of a greedy search.
"The re-sulting BLEUS score stabilizes already on a quite restricted search space, showing that despite the proven NP-hardness of the exact problem, our al-gorithm can give useful approximations in reason-able time."
"It is yet an open problem in how far the problems of finding the best paths regarding a sentence-level BLEU score, and regarding a system-level BLEU score correlate."
Our experiments here suggest a good correspondence.
This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.[REF_CITE]-06-C-0023.
"The proofs and algorithms in this paper emerged while the first author was visiting researcher at the Interactive Language Technologies Group of the National Research Council (NRC) of Canada, Gatineau."
The author wishes to thank NRC and Aachen University for the opportunity to jointly work on this project.
"While phrase-based statistical machine trans-lation systems currently deliver state-of-the-art performance, they remain weak on word order changes."
"Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance re-orderings possible with syntax-based systems."
"In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly in-tegrates with a standard phrase-based system with little loss of computational efficiency."
"We show that this model can successfully han-dle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase."
"We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53[REF_CITE]and +0.71[REF_CITE]) and Arabic-English (+0.55[REF_CITE])."
"Statistical phrase-based systems[REF_CITE]have consistently de-livered state-of-the-art performance in recent ma-chine translation evaluations, yet these systems re-main weak at handling word order changes."
"The re-ordering models used in the original phrase-based systems penalize phrase displacements proportion-ally to the amount of nonmonotonicity, with no con-sideration of the fact that some words are far more likely to be displaced than others (e.g., in English-to- Japanese translation, a verb should typically move to the end of the clause)."
"Recent efforts[REF_CITE]have directly addressed this issue by introducing lexicalized reordering models into phrase-based systems, which condition reordering probabilities on the words of each phrase pair."
"These models distinguish three orientations with respect to the previous phrase—monotone (M), swap (S), and discontinuous (D)—and as such are primarily de-signed to handle local re-orderings of neighboring phrases."
"Fig. 1(a) is an example where such a model effectively swaps the prepositional phrase in Luxem-bourg with a verb phrase, and where the noun min-isters remains in monotone order with respect to the previous phrase EU environment."
"While these lexicalized re-ordering models have shown substantial improvements over unlexicalized phrase-based systems, these models only have a limited ability to capture sensible long distance re-orderings, as can be seen in Fig. 1(b)."
"The phrase of the region should swap with the rest of the noun phrase, yet these previous approaches are unable to model this movement, and assume the orientation of this phrase is discontinuous (D)."
"Observe that, in a shortened version of the same sentence (without and progress), the phrase orientation would be dif-ferent (S), even though the shortened version has es-sentially the same sentence structure."
"Coming from the other direction, such observations about phrase reordering between different languages are precisely the kinds of facts that parsing approaches to machine translation are designed to handle and do success-fully handle[REF_CITE]."
"In this paper, we introduce a novel orientation model for phrase-based systems that aims to bet-ter capture long distance dependencies, and that presents a solution to the problem illustrated in Fig. 1(b)."
"In this example, our reordering model effectively treats the adjacent phrases the develop-ment and and progress as one single phrase, and the displacement of of the region with respect to this phrase can be treated as a swap."
"To be able iden-tify that adjacent blocks (e.g., the development and and progress) can be merged into larger blocks, our model infers binary (non-linguistic) trees reminis-cent[REF_CITE]."
"Crucially, our work distinguishes itself from previous hierarchical models in that it does not rely on any cubic-time parsing algorithms such as CKY (used in, e.g.,[REF_CITE]) or the Earley algorithm (used[REF_CITE])."
"Since our reordering model does not attempt to resolve natural language ambigui-ties, we can effectively rely on (linear-time) shift-reduce parsing, which is done jointly with left-to-right phrase-based beam decoding and thus intro-duces no asymptotic change in running time."
"As such, the hierarchical model presented in this pa-per maintains all the effectiveness and speed advan-tages of statistical phrase-based systems, while be-ing able to capture some key linguistic phenomena (presented later in this paper) which have motivated the development of parsing-based approaches."
"We also illustrate this with results that are significantly better than previous approaches, in particular the lexical reordering models of Moses, a widely used phrase-based SMT system[REF_CITE]."
This paper is organized as follows: the train-ing of lexicalized re-ordering models is described in Section 3.
"In Section 4, we describe how to combine shift-reduce parsing with left-to-right beam search phrase-based decoding with the same asymp-totic running time as the original phrase-based de-coder."
We finally show in Section 6 that our ap-proach yields results that are significantly better than previous approaches for two language pairs and dif-ferent test sets.
We compare our re-ordering model with related work[REF_CITE]using a log-linear approach common to many state-of-the-art statistical machine translation systems[REF_CITE].
"Given an input sentence f, which is to be translated into a target sentence e, the decoder searches for the most probable translation ê accord-ing to the following decision rule: ê = argmax p(e|f) (1) e J = argmax ∑ λ j h j (f,e) (2) e j=1 h j (f,e) are J arbitrary feature functions over sentence pairs."
"These features include lexicalized re-ordering models, which are parameterized as follows: given an input sentence f, a sequence of target-language phrases e = (e 1 ,...,e n ) currently hypothesized by the decoder, and a phrase alignment a = (a 1 ,...,a n ) that defines a source f a i for each translated phrase e i , these models estimate the prob-ability of a sequence of orientations o = (o 1 ,...,o n ) n p(o|e,f) = ∏ p(o i |e i , f a i ,a i−1 ,a i ), (3) i=[Footnote_1] where each o i takes values over the set of possi-ble orientations O = {M,S,D}. 1"
"1 We note here that the parameterization and terminology[REF_CITE]is slightly different. We purposely ignore these differences in order to enable a direct comparison between Till-man’s, Moses’, and our approach."
The probability is conditioned on both a i−1 and a i to make sure that the label o i is consistent with the phrase alignment.
"Specifically, probabilities in these models can be greater than zero only if one of the following con-ditions is true: • o i = M and a i − a i−1 = 1 • o i = S and a i − a i−1 = −1 • o i = D and |a i − a i−1 | 6= 1"
"At decoding time, rather than using the log-probability of Eq. 3 as single feature function, we follow the approach of Moses, which is to assign three distinct parameters (λ m ,λ s ,λ d ) for the three feature functions: • f m = ∑ ni=1 log p(o i = M|...) • f s = ∑ ni=1 log p(o i = S|...) • f d = ∑ ni=1 log p(o i = D|...)."
"There are two key differences between this work and previous orientation models[REF_CITE]: (1) the estimation of factors in Eq. 3 from data; (2) the segmentation of e and f into phrases, which is static in the case[REF_CITE], while it is dynamically updated with hierarchical phrases in our case."
These differ-ences are described in the two next sections.
"We present here three approaches for computing p(o i |e i , f a i ,a i−1 ,a i ) on word-aligned data using rel-ative frequency estimates."
"We assume here that phrase e i spans the word range s,...,t in the target sentence e and that the phrase f a i spans the range u,...,v in the source sentence f. All phrase pairs in this paper are extracted with the phrase-extract algo-rithm[REF_CITE], with maximum length set to 7."
"Word-based orientation model: This model an-alyzes word alignments at positions (s−1,u−1) and (s−1,v+1) in the alignment grid shown in Fig. 2(a)."
"Specifically, orientation is set to o i = M if (s − 1,u − 1) contains a word alignment and (s − 1,v + 1) contains no word alignment."
"It is set to o i = S if (s − 1,u − 1) contains no word alignment and (s − 1,v + 1) contains a word alignment."
"In all other cases, it is set to o i = D. This procedure is exactly the same as the one implemented in Moses. [URL_CITE] Phrase-based orientation model: The model presented[REF_CITE]is similar to the word-based orientation model presented above, except that it analyzes adjacent phrases rather than specific word alignments to determine orientations."
"Specif-ically, orientation is set to o i = M if an adjacent phrase pair lies at (s−1,u−1) in the alignment grid."
"It is set to S if an adjacent phrase pair cov-ers (s − 1,v + 1) (as shown in Fig. 2(b)), and is set to D otherwise."
Hierarchical orientation model: This model an-alyzes alignments beyond adjacent phrases.
"Specif-ically, orientation is set to o i =M if the phrase-extract algorithm is able to extract a phrase pair at (s−1,u−1) given no constraint on maximum phrase length."
"Orientation is S if the same is true at (s − 1,v + 1), and orientation is D otherwise."
Table 1 displays overall class distributions accord-ing to the three models.
"It appears clearly that occur-rences of M and S are too sparsely seen in the word-based model, which assigns more than 80% of its probability mass to D. Conversely, the hierarchical model counts considerably less discontinuous cases, and is the only model that accounts for the fact that real data is predominantly monotone."
"Since D is a rather uninformative default cat-egory that gives no clue how a particular phrase should be displaced, we will also provide MT evalu-ation scores (in Section 6) for a set of classes that distinguishes between left and right discontinuity {M,S,D l ,D r }, a choice that is admittedly more lin-guistically motivated."
Table 2 displays orientation probabilities for con-crete examples.
"Each example was put under one of the four categories that linguistically seems the best match, and we provide probabilities for that cat-egory according to each model."
"Note that, while we have so far only discussed left-to-right reorder-ing models, it is also possible to build right-to-left models by substituting a i−1 with a i+1 in Eq. 3."
Ex-amples for right-to-left models appear in the second half of the table.
The table strongly suggests that the hierarchical model more accurately determines the orientation of phrases with respect to large con-textual blocks.
"In Examples 1 and 2, the hierarchi-cal model captures the fact that coordinated clauses almost always remain in the same order, and that words should generally be forbidden to move from one side of “and” to the other side, a constraint that is difficult to enforce with the other two reorder-ing models."
"In Example 4, the first two models completely ignore that “he said” sometimes rotates around its neighbor clause."
"Computing reordering scores during decoding with word-based 3 and phrase-based models[REF_CITE]is trivial, since they only make use of local information to determine the orientation of a new in-coming block b i ."
"For a left-to-right ordering model, b i is scored based on its orientation with respect to b i−1 ."
"For instance, if b i has a swap orientation with respect to the previous phrase in the current trans-lation hypothesis, feature p(o i = S|...) becomes ac-tive."
"Computing lexicalized reordering scores with the hierarchical model is more complex, since the model must identify contiguous blocks—monotone or swapping—that can be merged into hierarchical blocks."
"The employed method is an instance of the well-known shift-reduce parsing algorithm, and re-lies on a stack (S) of foreign substrings that have already been translated."
"Each time the decoder adds a new block to the current translation hypothesis, it shifts the source-language indices of the block onto S, then repeatedly tries reducing the top two ele-ments of S if they are contiguous. [Footnote_4] This parsing algorithm was first applied in computational geome-try to identify convex hulls[REF_CITE], and its running time was shown to be linear in the length of the sequence (a proof is presented[REF_CITE], which applies the same algorithm to the binarization of SCFG rules)."
"4 It is not needed to store target-language indices onto the stack, since the decoder proceeds left to right, and thus suc-cessive blocks are always contiguous with respect to the target language."
"Figure 3 provides an example of the execution of this algorithm for the translation output shown in Figure 4, which was produced by a decoder in-corporating our hierarchical reordering model."
"The decoder successively pushes source-language spans [1], [2], [[Footnote_3]], which are successively merged into [1-3], and all correspond to monotone orientations."
"3 We would like to point out an inconsistency in Moses be-tween training and testing. Despite the fact that Moses estimates a word-based orientation model during training (i.e., it analyzes the orientation of a given phrase with respect to adjacent word alignments), this model is then treated as a phrase-based orien-tation model during testing (i.e., as a model that orients phrases with respect to other phrases)."
It then encounters a discontinuity that prevents the next block [11] from being merged with [1-3].
"As the decoder reaches the last words of the sentence (in the near future), [4-5] is successively merged with [6-12], then [1-3], yielding a stack that contains only [1-12]."
A nice property of this parsing algorithm is that it does not worsen the asymptotic running time of beam-search decoders such as Moses[REF_CITE].
"Such decoders run in time O(n 2 ), where n is the length of the input sentence."
"Indeed, each time a partial translation hypothesis is expanded into a longer one, the decoder must perform an O(n) op-eration in order to copy the coverage set (indicating which foreign words have already been translated) into the new hypothesis."
"Since this copy operation must be executed O(n) times, the overall time com-plexity is quadratic."
"The incorporation of the shift-reduce parser into such a decoder does not worsen overall time complexity: whenever the decoder ex-pands a given partial translation into a longer hy-pothesis, it simply copies its stack into the newly created hypothesis (similarly to copying the cover-age vector, this is an O(n) operation)."
"Hence, the incorporation of the hierarchical models described in the paper into a phrase-based decoder preserves the O(n 2 ) running time."
"In practice, we observe based on a set of experiments for Chinese-English and Arabic-English translation that our phrase-based decoder is on average only 1.35 times slower when it is running using hierarchical reordering features and the shift-reduce parser."
We finally note that the decoding algorithm pre-sented in this section can only be applied left-to-right if the decoder itself is operating left-to-right.
"In order to predict orientations relative to the right-to-left hierarchical reordering model, we must re-sort to approximations at decoding time."
"We experi-mented with different approximations, and the one that worked best (in the experiments discussed in Section 6) is described as follows."
"First, we note that an analysis of the alignment grid often reveals that certain orientations are impossible."
"For instance, the block issue in Figure 4 can only have discontinuous orientation with respect to what comes next in En-glish, since words surrounding the Chinese phrase have already been translated."
"When several hier-archical orientations are possible according to the alignment grid, we choose according to the follow-ing order of preference: (1) monotone, (2) swap, (3) discontinuous."
"For instance, in the case of with iran in Figure 4, only swap and discontinuous orienta-tions are possible (monotone orientation is impossi-ble because of the block hold consultations), hence we give preference to swap."
This prediction turns out to be the correct one according to the decoding steps that complete the alignment grid.
"We now analyze the system output of Figure 4 to fur-ther motivate the hierarchical model, this time from the perspective of the decoder."
We first observe that the prepositional phrase in the future should rotate around a relatively large noun phrase headed by con-sultations.
"Unfortunately, localized reordering mod-els such[REF_CITE]have no means of identi-fying that such a displacement is a swap (S)."
"Accord-ing to these models, the orientation of in the future with respect to what comes previously is discontin-uous (D), which is an uninformative fall-back cate-gory."
"By identifying h 2 (hold ... issue) as a hierarchi-cal block, the hierarchical model can properly deter-mine that the block in the near future should have a swap orientation. 5 Similar observations can be made regarding blocks h 1 and h 3 , which leads our model to predict either monotone orientation (between h 3 and “to” and between h 3 and “.”) or swap orienta-tion (between h 1 and with Iran) while local models would predict discontinuous in all cases."
"Another benefit of the hierarchical model is that its representation of phrases remains the same dur-ing both training and decoding, which is not the case for word-based and phrase-based reordering mod-els."
"The deficiency of these local models lies in the fact that blocks handled by phrase-based SMT sys-tems tend to be long at training time and short at test time, which has adverse consequences on non-hierarchical reordering models."
"For instance, in Fig-ure 4, the phrase-based reordering model categorizes the block in the near future as discontinuous, though if the sentence pair had been a training example, this block would count as a swap because of the ex-tracted phrase on this issue."
"In our experiments, we use a re-implementation of the Moses decoder[REF_CITE]."
"Ex-cept for lexical reordering models, all other fea-tures are standard features implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score."
We experiment with two language pairs: Chinese-to-English (C-E) and Arabic-to-English (A-E).
"For C-E, we trained translation models using a subset of the Chinese-English parallel data released by LDC (mostly news, in particular FBIS and Xinhua News)."
"This subset comprises 12.2M English words, and 11M Chinese words."
Chinese words are segmented with a conditional random field (CRF) classifier that conforms to the Chinese Treebank (CTB) standard.
"The training set for our A-E systems also includes mostly news parallel data released by LDC, and contains 19.5M English words, and 18.7M Arabic tokens that have been segmented using the Arabic Treebank (ATB)[REF_CITE]standard. [Footnote_6]"
6 Catalog numbers for C-E:[REF_CITE]
"For our language model, we trained a [Footnote_5]-gram model using the Xinhua and AFP sections of the Gigaword corpus[REF_CITE], in addition to the target side of the parallel data."
"5 Note that the hierarchical phrase hold ... issue is not a well-formed syntactic phrase – i.e., it neither matches the bracketing of the verb phrase hold ... future nor matches the noun phrase consultations ... issue – yet it enables sensible reordering."
"For both C-E and A-E, we manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets."
"The language model was smoothed with the modified Kneser-Ney algorithm, and we kept only trigrams, 4-grams, and 5-grams that respectively occurred two, three, and three times in the training data."
"Parameters were tuned with minimum error-rate training[REF_CITE]on the NIST evaluation set of 2006[REF_CITE]for both C-E and A-E. Since MERT is prone to search errors, especially with large num-bers of parameters, we ran each tuning experiment four times with different initial conditions."
"This pre-caution turned out to be particularly important in the case of the combined lexicalized reordering models (the combination of phrase-based and hierarchical discussed later), since MERT must optimize up to 26 parameters at once in these cases. [Footnote_7]"
"7 We combine lexicalized reordering models by simply treat-ing them as distinct features, which incidentally increases the number of model parameters that must be tuned with MERT."
"For testing, we used the NIST evaluation sets of 2005 and 2008[REF_CITE]for Chinese-English, and the test set of 2005[REF_CITE]for Arabic-English."
"Statistical significance is computed using the approximate randomization test[REF_CITE], whose application to MT evaluati[REF_CITE]was shown to be less sensitive to type-I errors (i.e., incorrectly concluding that im-provement is significant) than the perhaps more widely used bootstrap resampling method[REF_CITE]."
Tuning set performance is shown in Figure 5.
"Since this paper studies various ordering models, it is interesting to first investigate how the distor- tion limit affects performance. [Footnote_8] As has been shown in previous work in Chinese-English and Arabic-English translation, limiting phrase displacements to six source-language words is a reasonable choice."
8 Note that we ran MERT separately for each distinct distor-tion limit.
"For both C-E and A-E, the hierarchical model is sig-nificantly better (p ≤ .05) than either other models for distortion limits equal to or greater than 6 (ex-cept for distortion limit 12 in the case of C-E)."
"Since a distortion limit of 6 works reasonably well for both language pairs and is the default in Moses, we used this distortion limit value for all test-set experiments presented in this paper."
Our main results for Chinese-English are shown in Table 3.
It appears that hierarchical models pro-vide significant gains over all non-hierarchical mod-els.
Improvements[REF_CITE]are very sig-nificant (p ≤ .01).
In the case[REF_CITE]significant improvement is reached through the combination of both phrase-based and hierarchical models.
"We of-ten observe substantial gains when we combine such models, presumably because we get the benefit of identifying both local and long-distance swaps."
"Since most orientations in the phrase-based model are discontinuous, it is reasonable to ask whether the relatively poor performance of the phrase-based model is the consequence of an inadequate set of ori-entation labels."
"To try to answer this question, we use the set of orientation labels {M,S,D l ,D r } de-scribed in Section 3."
Results for this different set of orientations are shown in Table 4.
"While the phrase-based model appears to benefit more from the dis-tinction between left- and right-discontinuous, sys-tems that incorporate hierarchical models remain the most competitive overall: their best performance[REF_CITE]and[REF_CITE]are respectively 34.36, 32.85, and 27.03."
"The best non-hierarchical models achieve only 33.79, 32.32, and 26.32, respectively."
"All these differences (i.e., .57, .53, and .71) are sta-tistically significant at the .05 level."
Our results for Arabic-English are shown in Ta-bles 5 and 6.
"Similarly to C-E, we provide results for two orientation sets: {M,S,D} and {M,S,D l ,D r }."
"We note that the four-class orientation set is overall less effective for A-E than for C-E. This is probably due to the fact that there is less probability mass in A-E assigned to the D category, and thus it is less helpful to split the discontinuous category into two."
"For both orientation sets, we observe in A-E that the hierarchical model significantly outperforms the local ordering models."
Gains provided by the hierar-chical model are no less significant than for Chinese-to-English.
"This positive finding is perhaps a bit surprising, since Arabic-to-English translation gen-erally does not require many word order changes compared to Chinese-to-English translation, and this translation task so far has seldom benefited from hi- erarchical approaches to MT."
"In our case, one possi-ble explanation is that Arabic-English translation is benefiting from the fact that orientation predictions of the hierarchical model are consistent across train-ing and testing, which is not the case for the other ordering models discussed in this paper (see Sec-tion 4)."
"Overall, hierarchical models are the most effective on the two sets: their best performances[REF_CITE]are respectively 45.64 and 56.07."
The best non-hierarchical models obtain only 45.01 and 55.52 respectively for the same sets.
"All these differences (i.e., .63 and .55) are statistically signifi-cant at the .05 level."
"In this paper, we presented a lexicalized orientation model that enables phrase movements that are more complex than swaps between adjacent phrases."
This model relies on a hierarchical structure that is built as a by-product of left-to-right phrase-based decod-ing without increase of asymptotic running time.
We show that this model provides statistically signifi-cant improvements for five NIST evaluation sets and for two language pairs.
"In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as[REF_CITE]."
We believe such an extension would improve translation quality in the case of larger distortion limits.
"We also plan to experiment with discrimi-native approaches to estimating reordering probabil-ities[REF_CITE], which could also be applied to our work."
"We think the abil-ity to condition reorderings on any arbitrary feature functions is also very effective in the case of our hi-erarchical model, since information encoded in the trees would seem beneficial to the orientation pre-diction task."
The authors wish to thank the anonymous reviewers for their comments on an earlier draft of this paper.
This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM.
"The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred."
"Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model."
"While bi-lingual parallel data are expensive to generate, mono-lingual data are relatively common."
"Yet mono-lingual data have been under-utilized, having been used primarily for training a language model in the target language."
This paper de-scribes a novel method for utilizing monolin-gual target data to improve the performance of a statistical machine translation system on news stories.
The method exploits the exis-tence of comparable text—multiple texts in the target language that discuss the same or similar stories as found in the source language document.
"For every source document that is to be translated, a large monolingual data set in the target language is searched for docu-ments that might be comparable to the source documents."
These documents are then used to adapt the MT system to increase the prob-ability of generating texts that resemble the comparable document.
Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.
"While the amount of parallel data available to train a statistical machine translation system is sharply lim-ited, vast amounts of monolingual data are generally available, especially when translating to languages such as English."
Yet monolingual data are generally only used to train the language model of the trans-lation system.
Previous work ([REF_CITE];
"More recently,[REF_CITE]and[REF_CITE]have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel."
This newly discovered bilin-gual data can then be used as additional training data for the translation system.
Such methods generally have a very low yield leaving vast amounts of data that is only used for language modeling.
"These methods rely upon comparable corpora, that is, multiple corpora that are of the same gen-eral genre."
"In addition to this, documents can be comparable—two documents that are both on the same event or topic."
"Comparable documents occur because of the repetition of information across lan-guages, and in the case of news data, on the fact that stories reported in one language are often reported in another language."
"In cases where no direct trans-lation can be found for a source document, it is of-ten possible to find documents in the target language that are on the same story, or even on a related story, either in subject matter or historically."
Such docu-ments can be classified as comparable to the origi-nal source document.
"Phrases within this compara-ble document are likely to be translations of phrases in the source document, even if the documents them-selves are not parallel."
"Figure 1 shows an excerpt of the reference trans-lation of an Arabic document, and figure 2 shows a comparable passage. 1"
"In this case, the two new sto-ries are not translations of each other and were not reported at the same time—the comparable passage being an older news story—but both discuss actress Angelina Jolie’s visit to India."
"Many phrases and words are shared between the two, including: the name of the movie, the name and relationship of the actress’ character, the name and age of her son and many others."
"Such a pairing is extremely compara-ble, although even less related document pairs could easily be considered comparable."
We seek to take advantage of these comparable documents to inform the translation of the source document.
This can be done by augmenting the ma-jor components of the statistical translation system: the Language Model and the Translation Model.
"This work is in the same tradition[REF_CITE],[REF_CITE], and[REF_CITE]."
These adapted lan-guage models were shown to improve performance for both automatic speech recognition as well as ma-chine translation.
"In addition to language model adaptation we also modify the translation model, adding additional translation rules that enable the translation of new words and phrases in both the source and target lan-guages, as well as increasing the probability of ex-isting translation rules."
"Translation adaptation us-ing the translation system’s own output, known as Self-Training[REF_CITE]has previously shown gains by augmenting the translation model with ad-ditional translation rules."
"In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model."
"Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown[REF_CITE]to yield significant gains over a baseline system."
The trans-lation model was adapted by selecting comparable sentences from parallel corpora for each of the sen-tences to be translated.
"In addition to selecting out-of-domain data to adapt the translation model, com-parable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance[REF_CITE]."
"The research presented in this paper utilizes a dif-ferent approach to translation model adaptation us-ing comparable monolingual text rather than parallel text, exploiting data that would otherwise be unused for estimating the translation model."
"In addition, this data also informs the translation system by in-terpolating the original language model with a new language model trained from the same comparable documents."
We discuss the selection of comparable text for model adaptation in section 2.
"In sections 3.1 and 3.2, we describe the model adaptation for the language model and translation model, respectively."
Experimental results describing the application of model adaptation to a hierarchical Arabic-to-English MT system are presented in section 4.
Finally we draw conclusions in sections 5.
Comparable text is selected for every source doc-ument from a large monolingual corpus in the tar-get language.
"In practice, one could search the World Wide Web for documents that are compara-ble to a set of source documents, but this approach presents problems for ensuring the quality of the re-trieved documents."
The experiments in this paper use comparable text selected from a collection of English news texts.
"Because these texts are all flu-ent English, and of comparable genre to the test set, they are also used for training the standard language model training."
The problem of selecting comparable text has been widely studied in the information retrieval community and cross-lingual information retrieval (CLIR)[REF_CITE]has been largely successful at the task of selecting comparable or relevant documents in one language given a query in another language.
"We use CLIR to select a ranked list of documents in our target lan-guage, English in the experiments described in this paper, for each source document, designated as the query in the CLIR framework, that we wish to trans-late."
"The CLIR problem can be framed probabilisti-cally as: Given a query Q, find a document D that maximizes the equation Pr(D is rel|Q)."
This equa-tion can be expanded using Bayes’ Law as shown in equation 1.
"The prior probability of a document being relevant can be viewed as uniform, and thus in this work, we assume Pr(D is rel) is a constant. [Footnote_2]"
"2 In fact, it can be beneficial to use features of the document"
The Pr(Q) is constant across all documents.
There-fore finding a document to maximize Pr(D is rel|Q) is equivalent to finding a document that maximizes Pr(Q|D is rel).
Pr(D is rel) Pr(Q|D is rel)
Pr(D is rel|Q) = (1) Pr(Q)
A method of calculating the probability of a query given a document was proposed[REF_CITE][Footnote_3] and is shown in Equation 2.
3[REF_CITE]formulated this for the selection of foreign documents given an English query. We reverse this to select English documents given a foreign query.
"In this formulation, each foreign word, f, in the query is generated from the foreign vocabulary with probability α and from the English document with probability 1 − α, where α is a constant. [Footnote_4]"
"4 As[REF_CITE], a value of 0.3 was used for α."
"The probability of f being generated by the general foreign vocabulary, F , is Pr(f|F ) = freq(f, F)/|F|, the frequency of the word f in the vocabulary divided by the size of the vocabulary."
"The probability of the word being generated by the English document is the sum of the probabilities of it being generated by each English word, e, in the doc-ument which is the frequency of the English word in the document, (Pr(e|D) = freq(e,D)/|D|) multi-plied by the probability of the translation of the En-glish word to the foreign word, Pr(f|e)."
Y (α Pr(f|F )+ (2) f∈Q (1 − α) X Pr(e|D) Pr(f|e)) e
This formulation favors longer English docu-ments over shorter English documents.
"In addition, many documents cover multiple stories and topics."
"For the purposes of adaptation, shorter, fully com-parable documents are preferred to longer, only par-tially comparable documents."
"We modify the CLIR system by taking the 1000 highest ranked target lan-guage documents found by the CLIR system for each source document, and dividing them into over-lapping passages of approximately 300 words. [Footnote_5] Sen-to estimate Pr(D is rel)[REF_CITE]but we have not explored that here. tence boundaries are preserved when creating pas-sages, insuring that the text is fluent within each pas-sage."
5 The length of 300 was chosen as this was approximately the same length as the source documents.
"These passages are then scored again by the CLIR system, resulting in a list of passages of about 300 words each for each source document."
"Finally, we select the top N passages to be used for adapta-tion."
The N passages selected by this method are not guaranteed to be comparable and are often largely unrelated to the story or topic in the source docu-ment.
"We shall refer to the set of passages selected by the CLIR system as the bias text to differentiate it from comparable text, as the adaptation methods will use this text to bias the MT system so that its output will be more similar to the bias text."
"While we have not conducted experiments using other CLIR systems, the adaptation methods pre-sented in this paper could be applied without modifi-cation using another CLIR system, as the adaptation method treats the CLIR system as a black box."
"With the exception of running a second pass of CLIR, we use the algorithm[REF_CITE]without any significant modification, including the use of a stop word list for both the English and foreign texts."
The parameters for Pr(f|F ) and Pr(f|e) were estimated using the same parallel data that our translation sys-tem was trained on.
The bias text selected for a source document is used to adapt the language model (described in sec-tion 3.1) and the translation model (described in sec-tion 3.2) when translating that source document.
We use the same bias text to adapt both the lan-guage model and the translation model.
"For lan-guage model adaptation, we increase the probability of the word sequences in the bias text, and for trans-lation model adaptation we use additional phrasal translation rules."
"The adaptations can be done in-dependently and while they can augment each other when used together, this is not required."
"It is not necessary to use the same number of passages for both forms of adaptation, although doing so makes it more likely both that the English side of the new translation rule will be assigned a high probability by the adapted language model, and that the transla-tion model produces the English text to which the language model has been adapted."
Bias text that is used by one adaptation but not the other will re-ceive no special treatment by the other model.
"This could result in new translation rules that produce text to which the language assigns low probability, or it could result in the language model being able to as-sign a high probability to a good English translation that cannot be produced by the translation model due to a lack of necessary translation rules."
"While both adaptation methods are integrated into a hierarchical translation model[REF_CITE], they are largely implementation independent."
"Lan-guage model adaptation could be integrated into any statistical machine translation that uses a language model over words, while translation model adapta-tion could be added to any statistical machine trans-lation that can utilize phrasal translation rules."
"For every source document, we estimate a new lan-guage model, the bias language model, from the cor-responding bias text."
"Since this bias text is short, the corresponding bias language model is small and spe-cific, giving high probabilities to those phrases that occur in the bias text."
The bias language model is interpolated with the generic language model that would otherwise be used for translation if no LM adaptation was used.
"The new bias language model is of the same order as the generic language model, so that if a trigram language model is used for the MT decoding, then the biased language model will also be a trigram language model."
The bias lan-guage model is created using the same settings as the generic language model.
"In our particular im-plementation however, the generic language model uses Kneser-Ney smoothing, while the biased lan-guage model uses Witten-Bell smoothing due to im-plementation limitations."
In principle the biased lan-guage model can be smoothed in the same manner as the generic language model.
"We interpolate the bias language model and the generic language model as shown in equa-tion 3, where Pr g and Pr b are the probabilities from the generic language model and the bias lan-guage model, respectively."
"A constant interpolation weight, λ is used to weight the two probabilities for all documents."
"While a value for λ could be cho-sen that minimizes perplexity on a tuning set, in a similar fashion[REF_CITE], it is unclear that such a weight would be ideal when the interpolated lan-guage model is used as part of a statistical translation system."
"In practice we have observed that weights other than one that minimizes perplexity, typically a lower weight, can yield better translation results on the tuning set."
Pr(e) = (1 − λ) Pr(e) + λ Pr(e) (3) g b
"The resulting interpolated language model is then used in place of the generic language model in the translation process, increasing the probability that the translation output will resemble the bias text."
"It is important to note that, unlike the translation model adaptation described in section 3.2, no new infor-mation is added to the system with language model adaptation."
"Because the bias text is extracted from the same monolingual corpus that the generic lan-guage model was estimated from, all of the word se-quences used for training the bias language model were also used for training the generic language model."
Language model adaptation only increases the weight of the portion of the language model data that was selected as comparable.
"It is frequently the case in machine translation that unknown words or phrases are present in the source document, or that the known translations of source words are based on a very small number of oc-currences in the training data."
"In other cases, translations may be known for individual words in the source document, but not for longer phrases."
Translation model adaptation seeks to generate new phrasal translation rules for these source words and phrases.
"The bias text for a source document may, if comparable, contain a number of English words and phrases that are the English side of these desired rules."
"Because the source data and the bias text are not translations of each other and are not sen-tence aligned, conventional alignment tools, such as GIZA++[REF_CITE], cannot be used to align the source and bias text."
"Because the passages in the bias text are not translations of the source doc-ument, it will always be the case that portions of the source document have no translation in the bias text, and portions of the bias text have no translation in the source document."
"In addition a phrase in one of these texts might have multiple, differing transla-tions in the other text."
"Unlike language model adaptation, the entirety of the bias text is not used for translation adaptation."
We extract those phrases that occur in at least M of the passages in the bias texts.
"A phrase is only counted once for every passage in which it occurs, so that repeated use of a phrase within a passage does not affect whether it used to generate new rules."
"Typically, passages selected by the CLIR tend to be very similar to each other if they are comparable to the source document and are very different from each other if they are not comparable to the source document."
"Phrases that are identical across passages are the ones that are most likely to be comparable, whereas a phrase or word that occurs in only one passage is likely to be present only by chance or if the passage it is in is not comparable."
"Filtering the target phrases to those that occur in multiple pas-sages therefore serves not only to reduce the total number of rules, but also to filter out phrases from passages that are not comparable."
"For each phrase in the source document we gener-ate a new translation to each of the phrases selected from the bias text, and assign it a low uniform prob-ability. [Footnote_6] For each translation rule we also have a lexical translation probability that we estimate cor-rectly from the trained word model."
6 A probability of 1/700 is arbitrarily used for the bias rules although it is then weighted by the bias translation rule weight.
These new rules are then added to the phrase table of the existing translation model when translating the source doc-ument.
"Rather than adding probability to the ex-isting generic rules, the new rules are marked as bias rules by the system and given their own fea-ture weight."
"While the vast majority of these rules are incorrect translations, these incorrect rules will be naturally biased against by the translation sys-tem."
"If the source side of a translation already has a number of observed translations, then the low prob-ability of the new bias rule will cause it to not be selected by the translation system."
"If the new trans-lation rules would produce garbled English, then it will be biased against by the language model."
"When this is combined with the language model adapta- tion, a natural pressure is exerted to use the bias rules for source phrases primarily when it would cause the output to look more like the bias text."
"We evaluated the performance of language and translation model adaptation with our translation system on two conditions, the details of which are presented in section 4.1."
"One condition involved a small amount of parallel training, such as one might find when translating a less commonly taught lan-guage (LCTL)."
The other condition involved the full amount of training available for Arabic-to-English translation.
In the case of LCTLs we expect our translation model to have the most deficiencies and be most in need of additional translation rules.
"So, it is under such a condition we would expect the translation model adaptation to be the most bene-ficial."
We evaluate the system’s performance under this condition in section 4.2.
"The effectiveness of this technique on state-of-the-art systems, and its ef-ficiency when used with a well trained generic trans-lation model is presented in section 4.3."
Both language-model and translation-model adap-tation are implemented on top of a hierarchical Arabic-to-English translation system with string-to-dependency rules as described[REF_CITE].
"While generalized rules are generated from the par-allel data, rules generated by the translation model adaptation are not generalized and are used only as phrasal rules."
"A trigram language model was used during decoding, and a 5-gram language model was used to re-score the n-best list after decoding."
"In ad-dition to the features described[REF_CITE], a new feature is added to the model for the bias rule weight, allowing the translation system to ef-fectively tune the probability of the rules added by translation model adaptation in order to improve per-formance on the tuning set."
"Bias texts were selected from three mono-lingual corpora: the English Gigaword cor-pus (2,793,350,201 words), the FBIS corpus (28,465,936 words), and a collection of news archive data collected from the websites of various on-line, public news sites (828,435,409 words)."
All three corpora were also part of the generic language model training data.
Language model adaptation on both the trigram and 5-gram language models used 10 comparable passages with an interpolation weight of 0.1.
Translation model adaptation used 10 comparable passages for the bias text and a value of 2 for M.
"Each selected passage contains approximately 300 words, so in the case where 10 comparable pas-sages are used to create a bias text, the resulting text will be 3000 words long on average."
The language models created using these bias texts are very spe-cific giving large probability to n-gram sequences seen in those texts.
"The construction of the bias texts increases the overall run-time of the translation system, although in practice this is a small expenditure."
"The most in-tensive portion is the initial indexing of the monolin-gual corpus, but this is only required once and can be reused for any subsequent test set that is evaluated."
This index can then be quickly searched for com-parable passages.
"When considering research envi-ronments, test sets are used repeatedly and bias texts only need to be built once per set, making the build-ing cost negligible."
"Otherwise, the time required to build the bias text is still small compared to the ac-tual translation time."
All conditions were optimized using BLEU[REF_CITE]and evaluated using both BLEU and Translation Edit Rate (TER)[REF_CITE].
"BLEU is an accuracy measure, so higher values indicate better performance, while TER is an error metric, so lower values indicate better perfor-mance."
"Optimization was performed on a tuning set of newswire data, comprised of portions[REF_CITE]and[REF_CITE]newswire de-velopment data, a total of 48921 words[REF_CITE]segments and 173 documents."
Results were measured on the[REF_CITE]
"Arabic Evalu-ation set, which was 55578 words[REF_CITE]segments and 104 documents."
Four reference trans-lations were used for scoring each translation.
"Parameter optimization method was done using n-best optimization, although the adaptation process is not tied to this method."
"The MT decoder is run on the tuning set generating an n-best list (where n = 300), on which all of the translation features (including bias rule weights) are optimized using"
"These new weights are then used to decode again, repeating the whole process, using a cumulative n-best list."
This continues for several iterations until performance on the tuning set stabi-lizes.
The resulting feature weights are used when decoding the test set.
"A similar, but simpler, method is used to determine the feature weights after 5-gram rescoring."
This n-best optimization method has sub-tle implications for translation model adaptation.
"In the first iteration, few bias rules are used in decoding the 300-best, and those that are used frequently help, although the overall gain is small due to the small number of bias rules used."
"This causes the opti-mizer to greatly increase the weight of the bias rules, causing the decoder to overuse the bias rules in the next iteration causing a sharp decrease in translation quality."
Several iterations are needed for the cumu-lative n-best to achieve sufficient diversity and size to assign a weight for the bias translation rules that results in an increase in performance over the base-line.
Alternative optimization methods could likely circumvent this process.
Language model adapta-tion does not suffer from this phenomenon.
"In order to better examine the nature of translation model adaptation, we elected to work with a transla-tion model that was trained on only 5 million words of parallel Arabic-English text."
"Limiting the trans-lation model training in this way simulates the prob-lem of translating less commonly taught languages (LCTL) where less parallel text is available, a situa-tion that is not the case for Arabic."
"Since the model is trained on less parallel data, it is lacking a large number of translation rules, which is expected to be addressed by the translation model adaptation."
"By working in an environment with a more deprived baseline translation model, we are giving the trans-lation model adaptation more room to assist."
The experiments described below use a 5 million word Arabic parallel text corpus constructed from the[REF_CITE]corpora.
The full monolingual English data were used for the lan-guage model and for selection of comparable doc-uments.
Unless otherwise specified no language model adaptation was used.
"We first establish an upper limit on the gain us- ing translation model adaptation, using the reference data to adapt the translation system."
"These reference data can be considered to be extremely comparable, better than could ever be hoped to gain by compara-ble document selection."
"We first aligned this data using GIZA++ to the source data, simulating the ideal case where we can perfectly determine which source words translate to which comparable words."
"Because our translation model adaptation system as-signs uniform probability to all bias rules, we ignore the correct rule probabilities that we could extract from word alignment and assign uniform probabil-ity to all of the bias translation rules."
"As expected, this gives a large gain over the baseline."
"We also examine limiting these new translation rules to those rules whose target side occurs in the top 100 passages selected by CLIR, thus minimiz-ing the adaption to those rules that it theoretically could learn from the bias text."
The results of these experiments and an unadapted baseline are shown in table 1.
"The fair translation model adaptation system, however, does not align source phrases to the cor-rect bias text phrases in such a fashion, and instead aligns all source words to all target words."
"To in-vestigate the effect of this over production of rules, we again used the reference translations as if they were comparable data, but we ignored the align-ments learned by GIZA++, and instead allowed all source phrases to translate to all English phrases in the reference text, with uniform probability."
"This still shows large gains in translation quality over the baseline, as measured by TER and BLEU."
"Again, we also examined limiting the text used for transla-tion model adaptation to those phrases that occur in both the reference text and the top 100 comparable passages selected the CLIR system."
"While this de-creased performance, the system still performs sig-nificantly better than the baseline, as shown in the following table 2."
"Applying translation model and language model adaptation fairly, using only bias text from the com-parable data selection, yields smaller gains on both the tuning and[REF_CITE]sets, as shown in table 3."
The combination of language-model and translation-model adaptation exceeds the gains that would be achieved over the baseline by either method sepa-rately.
"While the simulation described in section 4.2 used only 5 million words of parallel training, 230 mil-lion words of parallel data from 18.5 million seg-ments were used for training the full Arabic-to- English translation system."
"This parallel data in-cludes the[REF_CITE]”ISI Arabic-English Auto-matically Extracted Parallel Text” corpus[REF_CITE], which was created from monolin-gual corpora in English and Arabic using the algo-rithm described[REF_CITE], as the techniques used in that work are separate and independent from the adaptation methods we de-scribe in this paper. [Footnote_7] Language model adaptation and translation model adaptation were applied both independently and jointly to the translation system, and the results were evaluated against an unadapted baseline, as shown in table 4."
"7 The two methods are not directly comparable, and so we do not make any attempt to do so.[REF_CITE]creates new parallel corpora from two monolingual corpora. This new parallel data is generally applicable for training a translation model but does not target any particular test set. Our adaptation method does not generate new parallel data, but cre-ates a new specific translation model for a test document that is being translated."
"While gains from language model adaptation were substantial on the tuning set, on the[REF_CITE]test set they are reduced to a 0.65% gain on BLEU and a negligible improvement in TER."
The translation model adaptation performs better with 1.37% im-provement in BLEU and a 0.26% improvement in TER.
"This gain increases to a 2.07% improvement in BLEU and a 0.64% improvement in TER when language adaptation is used in conjunction with the translation model adaptation, showing the impor-tance of using both adaptation methods."
"While it could be expected that a more heavily trained trans-lation model might not require the benefit of lan-guage and translation model adaptation, a more sub-stantial gain over the baseline can be seen when both forms of adaptation are used than in the case with less parallel training—a difference of 2.07% BLEU versus 0.68% BLEU."
"Of the comparable passages selected by the CLIR system for the[REF_CITE]test set in the full training experiment, 16.3% were selected from the News"
"A slightly different distribution was found for the Tuning set, where 17.8% of the passages were selected from the News Archive cor-pus, 77.1% were selected from the English Giga-Word corpus, and 5.1% were selected from the FBIS corpus."
The reuse of a monolingual corpus that was already used by a translation system for language model training to perform both language and translation model adaptation shows large gains over an un-adapted baseline.
"By leveraging off of a CLIR sys-tem, which itself contains no information not al-ready given to the translation system, [Footnote_8] potentially comparable passages can be found which allow im-proved translation."
8 The probabilistic parameters of the CLIR system are esti-mated from the same parallel corpora that is used to train the generic translation model.
"Surprisingly, these gains are largest when the baseline model is better trained, in-dicating that a strong reliance of the adaptation on the existing models."
One explantation for these counter-intuitive results–larger gains in the full training scenario ver-sus the LCTL scenario–is that the lexical probabili-ties are better estimated in the former case.
The bias rules all have equal translation probability and only vary in probability according to the lexical proba-bility of the rules.
Better estimates of these lexical probabilities may enable the translation system to more clearly distinguish between helpful and harm-ful bias rules.
There are many clear directions for the improve-ment of these methods.
The current adaptation method does not utilize the probabilities from the CLIR system and treats the top-ranked passages all as equally comparable regardless of the probabil-ity assigned.
"Variable weighting of passages could prove beneficial to both language model adaptation, where the passages could be weighted proportion-ally to the probability of the passage being relevant, and translation model adaptation, where the require-ment on repetition of phrases across passages could be weighted, as could the probability of the new rules produced by the translation system."
"In ad-dition, the CLIR score, among other possible fea-tures such as phrase overlap, could be used to de-termine those documents where no comparable pas-sage could be detected and where it would be bene-ficial to not adapt the models."
A clear limitation of using comparable documents to adapt the language and translation model is that comparable documents must be found.
"For many source documents, none of the top passages found by the CLIR system were comparable."
"We suspect that while this will always occur to some extent, this becomes more common as the monolingual data be-comes less like the source data, such as when there is a large time gap between the two."
The full extent of this and the effect of the level of document compa-rability on translation remains an open question.
"In addition, while newswire is an excellent source of comparable text, it is unclear how well this method can be used on newsgroups or spoken data, where the fluency of the source text is diminished."
"When translating news stories, this technique is not lim-ited to major news events."
"While many of the events discussed in the source data receive world-wide at-tention, many are local events that are unreported in the English comparable data used in our experi-ments."
"Events of a similar nature or events involving many of the same people often do occur in the En-glish comparable data, allowing improvement even when the stories are quite different."
"The adaptation methods described in this paper are not limited to a particular framework of statis-tical machine translation, but have applicability to any statistical machine translation system that uses a language model or translation rules."
"We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement."
"The model is formally a latent variable CRF gram-mar over trees, learned by iteratively splitting grammar productions (not categories)."
"Dif-ferent regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars[REF_CITE]."
"In addition, our discriminative approach integrally admits features beyond lo-cal tree configurations."
We present a multi-scale training method along with an efficient CKY-style dynamic program.
"On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars."
"In latent variable approaches to parsing[REF_CITE], one models an ob-served treebank of coarse parse trees using a gram-mar over more refined, but unobserved, derivation trees."
"The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntac-tic processes."
"In recent years, latent variable meth-ods have been shown to produce grammars which are as good as, or even better than, earlier parsing work[REF_CITE]."
"In particular,[REF_CITE]we exhibited a very accurate category-splitting approach, in which a coarse ini-tial grammar is refined by iteratively splitting each grammar category into two subcategories using the EM algorithm."
"Of course, each time the number of grammar categories is doubled, the number of bi-nary productions is increased by a factor of eight."
"As a result, while our final grammars used few cat-egories, the number of total active (non-zero) pro-ductions was still substantial (see Section 7)."
"In ad-dition, it is reasonable to assume that some genera-tively learned splits have little discriminative utility."
"In this paper, we present a discriminative approach which addresses both of these limitations."
"We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see Figure 2)."
"We use the general framework of hidden variable CRFs[REF_CITE], where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations."
"With multi-scale grammars, it is natural to refine productions rather than categories."
"As a result, a category such as NP can be complex in some re-gions of the grammar while remaining simpler in other regions."
"Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features[REF_CITE], giving the bene-fit of some input features integrally in our dynamic program."
Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars[REF_CITE].
"In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages."
"Discriminative parsing has been investigated be-fore, such as[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE], and, most similarly,[REF_CITE]."
"However, in all of these cases, the final parsing performance fell short of the best generative models by several per-centage points or only short sentences were used."
Only in combination with a generative model was a discriminative component able to produce high parsing accuracies[REF_CITE].
"Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy."
Treebanks are typically not annotated with fully de-tailed syntactic structure.
"Rather, they present only a coarse trace of the true underlying processes."
"As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks."
A manual approach might take the category NP and subdivide it into one subcategory NPˆS for subjects and another subcategory NPˆVP for objects[REF_CITE].
"However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully auto-mated approach, in which each symbol is split into unconstrained subcategories."
Latent variable grammars augment the treebank trees with latent variables at each node.
This cre-ates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories.
For each observed category A we now have a set of latent subcategories A x .
"For example, NP might be split into NP 1 through NP 8 ."
"The parameters of the refined productions A x → B y C z , where A x is a subcategory of A, B y of B, and C z of C, can then be estimated in var-ious ways; past work has included both generative[REF_CITE]and dis-criminative approaches[REF_CITE]."
We take the discriminative log-linear approach here.
"Note that the comparison is only between estimation methods,[REF_CITE]show that the model classes are the same."
"In a log-linear latent variable grammar, each pro-duction r ="
A x → B y C z is associated with a multiplicative weight φ r[REF_CITE](sometimes we will use the log-weight θ r when convenient).
The probability of a derivation t of a sentence w is proportional to the product of the weights of its productions r:
P (t|w) ∝ Y φ r r∈t
The score of a parse T is then the sum of the scores of its derivations:
P (T |w) = X P (t|w) t∈T
Grammar refinement becomes challenging when the number of subcategories is large.
"If each category is split into k subcategories, each (binary) produc-tion will be split into k 3 ."
The resulting memory lim-itations alone can prevent the practical learning of highly split grammars[REF_CITE].
"This issue was partially addressed[REF_CITE], where categories were repeatedly split and some splits were re-merged if the gains were too small."
"However, while the grammars are indeed compact at the (sub-)category level, they are still dense at the production level, which we address here."
"As[REF_CITE], we arrange our subcat-egories into a hierarchy, as shown in Figure 1."
"In practice, the construction of the hierarchy is tightly coupled to a split-based learning process (see Sec-tion 5)."
"We use the naming convention that an origi-nal category A becomes A 0 and A 1 in the first round; A 0 then becoming A 00 and A [Footnote_1] in the second round, and so on."
"1 Conversely, x̂ is a coarser version of x, or, in the language[REF_CITE], x̂ is a projection of x."
We will use x̂ ≻ x to indicate that the subscript or subcategory x is a refinement of x̂. 1
"We will also say that x̂ dominates x, and x will refer to fully refined subcategories."
"The same terminology can be applied to (binary) productions, which split into eight refinements each time the subcategories are split in two."
"The core observation leading to multi-scale gram-mars is that when we look at the refinements of a production, many are very similar in weight."
It is therefore advantageous to record productions only at the level where they are distinct from their children in the hierarchy.
"A multi-scale grammar is a grammar in which some productions reference fine categories, while others reference coarse categories."
"As an example, con-sider the multi-scale grammar in Figure 2, where the NP category has been split into two subcategories (NP 0 , NP 1 ) to capture subject and object distinc-tions."
"Since it can occur in subject and object po-sition, the production NP → it has remained unsplit."
"In contrast, in a single-scale grammar, two produc-tions NP 0 → it and NP 1 → it would have been nec-essary."
"We use * as a wildcard, indicating that NP ∗ can combine with any other NP, while NP 1 can only combine with other NP 1 ."
"Whenever subcategories of different granularity are combined, the resulting constituent takes the more specific label."
"In terms of its structure, a multi-scale grammar is a set of productions over varyingly refined symbols, where each production is associated with a weight."
Consider the refinement of the production shown in Figure 1.
The original unsplit production (at top) would naively be split into a tree of many subpro-ductions (downward in the diagram) as the grammar categories are incrementally split.
"However, it may be that many of the fully refined productions share the same weights."
"This will be especially common in the present work, where we go out of our way to achieve it (see Section 5)."
"For example, in Figure 1, the productions DT x → the have the same weight for all categories DT x which refine DT 1 . [Footnote_2]"
2 We define dominating productions and refining productions analogously as for subcategories.
"A multi-scale grammar can capture this behavior with just 4 productions, while the single-scale grammar has 8 productions."
For binary productions the savings will of course be much higher.
"In terms of its semantics, a multi-scale grammar is simply a compact encoding of a fully refined latent variable grammar, in which identically weighted re-finements of productions have been collapsed to the coarsest possible scale."
"Therefore, rather than at-tempting to control the degree to which categories are split, multi-scale grammars simply encode pro-ductions at varying scales."
"It is hence natural to speak of refining productions, while considering the categories to exist at all degrees of refinement."
"Multi-scale grammars enable the use of coarse (even unsplit) categories in some regions of the grammar, while requiring very specific subcategories in others, as needed."
"As we will see in the following, this flex-ibility results in a tremendous reduction of grammar parameters, as well as improved parsing time, be-cause the vast majority of productions end up only partially split."
"Since a multi-scale grammar has productions which can refer to different levels of the category hierarchy, there must be constraints on their coher-ence."
"Specifically, for each fully refined produc-tion, exactly one of its dominating coarse produc-tions must be in the grammar."
"More formally, the multi-scale grammar partitions the space of fully re-fined base rules such that each r maps to a unique dominating rule r̂, and for all base rules r ′ such that r̂ ≻ r ′ , r ′ maps to r̂ as well."
"This constraint is al-ways satisfied if the multi-scale grammar consists of fringes of the production refinement hierarchies, in-dicated by the shading in Figure 1."
A multi-scale grammar straightforwardly assigns scores to derivations in the corresponding fully re-fined single scale grammar: simply map each refined derivation rule to its dominating abstraction in the multi-scale grammar and give it the corresponding weight.
The fully refined grammar is therefore triv-ially (though not compactly) reconstructable from its multi-scale encoding.
It is possible to directly define a derivational se-mantics for multi-scale grammars which does not appeal to the underlying single scale grammar.
"However, in the present work, we use our multi-scale grammars only to compute expectations of the underlying grammars in an efficient, implicit way."
We now consider how to discriminatively learn multi-scale grammars by iterative splitting produc-tions.
There are two main concerns.
"First, be-cause multi-scale grammars are most effective when many productions share the same weight, sparsity is very desirable."
"In the present work, we exploit L 1 -regularization, though other techniques such as structural zeros[REF_CITE]could also potentially be used."
"Second, training requires repeated parsing, so we use coarse-to-fine chart caching to greatly accelerate each iteration."
We learn discriminative multi-scale grammars in an iterative fashion (see Figure 1).
"As[REF_CITE], we start with a simple X-bar grammar from an input treebank."
"The parameters θ of the grammar (production log-weights for now) are estimated in a log-linear framework by maximizing the penalized log conditional likelihood L cond − R(θ), where:"
L cond (θ) = log Y P(T i |w i ) i R(θ) = X |θ r | r
We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS[REF_CITE]in our imple-mentation).
To handle the non-diferentiability of the L 1 -regularization term R(θ) we use the orthant-wise method[REF_CITE].
Fitting the log-linear model involves the following derivatives: ∂L cond (θ) = X E θ [f r (t)|T i ] − E θ [f r (t)|w i ] ∂θ r i where the first term is the expected count f r of a pro-duction r in derivations corresponding to the correct parse tree T i and the second term is the expected count of the production in all derivations of the sen-tence w i .
Note that r may be of any scale.
"As we will show below, these expectations can be com-puted exactly using marginals from the chart of the inside/outside algorithm[REF_CITE]."
"Once the base grammar has been estimated, all categories are split in two, meaning that all binary productions are split in eight."
"When splitting an al-ready refined grammar, we only split productions whose log-weight in the previous grammar deviates from zero. [Footnote_3]"
3 L 1 -regularization drives more than 95% of the feature weights to zero in each round.
This creates a refinement hierarchy over productions.
"Each newly split production r is given a unique feature, as well as inheriting the features of its parent productions r̂ ≻ r: φ r = exp  θ r̂ r̂≻r"
The parent productions r̂ are then removed from the grammar and the new features are fit as described above.
We detect that we have split a production too far when all child production features are driven to zero under L 1 regularization.
"In such cases, the chil-dren are collapsed to their parent production, which forms an entry in the multi-scale grammar."
"In order to compute the expected counts needed for training, we need to parse the training set, score all derivations and compute posteriors for all sub-categories in the refinement hierarchy."
The in-side/outside algorithm[REF_CITE]is an efficient dynamic program for summing over deriva-tions under a context-free grammar.
"It is fairly straightforward to adapt this algorithm to multi-scale grammars, allowing us to sum over an expo-nential number of derivations without explicitly re-constructing the underlying fully split grammar."
"For single-scale latent variable grammars, the in-side score I(A x , i, j) of a fully refined category A x spanning hi,ji is computed by summing over all possible productions r ="
"A x → B y C z with weight φ r , spanning hi, ki and hk, ji respectively: [Footnote_4]"
"4 These scores lack any probabilistic interpretation, but can be normalized to compute the necessary expectations for train-ing[REF_CITE]."
"I(A x , i, j) = X φ X I(B r y , i, k)I(C z , k, j) r k"
Note that this involves summing over all relevant fully refined grammar productions.
"The key quantities we will need are marginals of the form I(A x , i, j), the sum of the scores of all fully refined derivations rooted at any A x dominated by A x and spanning hi,ji."
We define these marginals in terms of the standard inside scores of the most refined subcategories A x :
"I(A x , i, j) = X I(A x , i, j) x≺x"
"When working with multi-scale grammars, we expand the standard three-dimensional chart over spans and grammar categories to store the scores of all subcategories of the refinement hierarchy, as il-lustrated in Figure 3."
This allows us to compute the scores more efficiently by summing only over rules r̂ =
A x̂ → B ŷ C ẑ ≻ r:
"I(A x , i, j) = X X φ X I(B r y , i, k)I(C z , k, j) r̂ r≺r̂ k = X φ X X I(B r̂ y , i, k)I(C z , k, j) r̂ r≺r̂ k = X φ X X X I(B r̂ y , i, k)I(C z , k, j) r̂ y≺ŷ z≺ẑ k = X φ X X I(B y , i, k) X I(C r̂ z , k, j) r̂ k y≺ŷ z≺ẑ = X φ X I(B r̂ ŷ , i, k)I(C ẑ , k, j) r̂ k"
"Of course, some of the same quantities are computed repeatedly in the above equation and can be cached in order to obtain further efficiency gains."
"Due to space constraints we omit these details, and also the computation of the outside score, as well as the han-dling of unary productions."
"Estimating discriminative grammars is challenging, as it requires repeatedly taking expectations over all parses of all sentences in the training set."
"To make this computation practical on large data sets, we use the same approach[REF_CITE]."
"Therein, the idea of coarse-to-fine parsing[REF_CITE]is extended to handle the repeated pars-ing of the same sentences."
"Rather than computing the entire coarse-to-fine history in every round of training, the pruning history is cached between train-ing iterations, effectively avoiding the repeated cal-culation of similar quantities and allowing the effi-cient approximation of feature count expectations."
"The discriminative framework gives us a convenient way of incorporating additional, overlapping fea-tures."
We investigate two types of features: un-known word features (for predicting the part-of-speech tags of unknown or rare words) and span fea-tures (for determining constituent boundaries based on individual words and the overall sentence shape).
Building a parser that can process arbitrary sen-tences requires the handling of previously unseen words.
"Typically, a classification of rare words into word classes is used[REF_CITE]."
"In such an ap-proach, the word classes need to be manually de-fined a priori, for example based on discriminating word shape features (suffixes, prefixes, digits, etc.)."
"While this component of the parsing system is rarely talked about, its importance should not be un-derestimated: when using only one unknown word class, final parsing performance drops several per-centage points."
"Some unknown word features are universal (e.g. digits, dashes), but most of them will be highly language dependent (prefixes, suf-fixes), making additional human expertise necessary for training a parser on a new language."
It is there-fore beneficial to automatically learn what the dis-criminating word shape features for a language are.
The discriminative framework allows us to do that with ease.
In our experiments we extract prefixes and suffixes of length ≤ 3 and add those features to words that occur 25 times or less in the training set.
These unknown word features make the latent vari-able grammar learning process more language inde-pendent than in previous work.
There are many features beyond local tree config-urations which can enhance parsing discrimination;[REF_CITE]presents a varied list.
"In reranking, one can incorporate any such features, of course, but even in our dynamic programming ap-proach it is possible to include features that decom-pose along the dynamic program structure, as shown[REF_CITE]."
"We use non-local span fea-tures, which condition on properties of input spans[REF_CITE]."
"We illustrate our span features with the following example and the span h1, 4i: 0 “ 1 [ Yes 2 ” 3 , ] 4 he [Footnote_5] said 6 . 7"
5 Synthetic constituents are nodes that are introduced during binarization.
"We first added the following lexical features: • the first (Yes), last (comma), preceding (“) and following (he) words, • the word pairs at the left edge h“,Yesi, right edge hcomma,hei, inside border hYes,commai and outside border h“,hei."
Lexical features were added for each span of length three or more.
"We used two groups of span features, one for natural constituents and one for synthetic ones. 5 We found this approach to work slightly better than anchoring the span features to particular constituent labels or having only one group."
"We also added shape features, projecting the sentence to abstract shapes to capture global sen-tence structures."
Punctuation shape replaces ev-ery non-punctuation word with x and then further collapses strings of x to x+.
"Our example be-comes #‘‘x’’,x+.#, and the punctuation feature for our span is ‘‘[x’’,]x."
"Capitalization shape projects the example sentence to #.X..xx.#, and .[X..]x for our span."
Span features are a rich source of information and our experiments should be seen merely as an initial investigation of their ef-fect in our system.
"We ran experiments on a variety of languages and corpora using the standard training and test splits, as described in Table 1."
"In each case, we start with a completely unannotated X-bar grammar, ob-tained from the raw treebank by a simple right-branching binarization scheme."
"We then train multi-scale grammars of increasing latent complexity as described in Section 5, directly incorporating the additional features from Section 6 into the training procedure."
Hierarchical training starting from a raw treebank grammar and proceeding to our most re-fined grammars took three days in a parallel im-plementation using 8 CPUs.
"At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct pro-ductions, as[REF_CITE]."
We compare to a baseline of discriminatively trained latent variable grammars[REF_CITE].
"We also compare our discriminative multi-scale grammars to their generative split-and-merge cousins, which have been shown to produce the state-of-the-art figures in terms of accuracy and effi-ciency on many corpora."
For those comparisons we use the grammars[REF_CITE].
One of the main motivations behind multi-scale grammars was to create compact grammars.
Fig-ure 4 shows parsing accuracies vs. grammar sizes.
"Focusing on the grammar size for now, we see that multi-scale grammars are extremely compact - even our most refined grammars have less than 50,000 ac-tive productions."
"The graph also shows that this compactness is due to controlling production sparsity, as the single-scale discriminative grammars are two orders of magnitude larger."
Figure 4 shows development set results for En-glish.
"In terms of parsing accuracy, multi-scale grammars significantly outperform discriminatively trained single-scale latent variable grammars and perform on par with the generative split-and-merge grammars."
The graph also shows that the unknown word and span features each add about 0.5% in final parsing accuracy.
"Note that the span features im-prove the performance of the unsplit baseline gram-mar by 8%, but not surprisingly their contribution gets smaller when the grammars get more refined."
"Section 8 contains an analysis of some of the learned features, as well as a comparison between discrimi-natively and generatively trained grammars."
"In coarse-to-fine parsing the sentence is rapidly pre-parsed with increasingly re-fined grammars, pruning away unlikely chart items in each pass."
"In their work the grammar is pro-jected onto coarser versions, which are then used for pruning."
"Multi-scale grammars, in contrast, do not require projections."
The refinement hierarchy is built in and can be used directly for coarse-to-fine pruning.
Each production in the grammar is associ-ated with a set of hierarchical features.
"To obtain a coarser version of a multi-scale grammar, one there-fore simply limits which features in the refinement hierarchy can be accessed."
"In our experiments, we start by parsing with our coarsest grammar and al-low an additional level of refinement at each stage of the pre-parsing."
"Compared to the generative parser[REF_CITE], parsing with multi-scale grammars requires the evaluation of 29% fewer pro-ductions, decreasing the average parsing time per sentence by 36% to 0.36 sec/sentence."
For each corpus we selected the grammar that gave the best performance on the development set to parse the final test set.
"Table 2 summarizes our final test set performance, showing that multi-scale grammars achieve state-of-the-art performance on most tasks."
"On WSJ-English, the discriminative grammars per-form on par with the generative grammars[REF_CITE], falling slightly short in terms of F1, but having a higher exact match score."
"When trained on WSJ-English but tested on the Brown corpus, the discriminative grammars clearly outperform the generative grammars, suggesting that the highly reg-ularized and extremely compact multi-scale gram-mars are less prone to overfitting."
"All those meth-ods fall short of reranking parsers[REF_CITE]and[REF_CITE], which, however, have access to many additional features, that cannot be used in our dynamic program."
"When trained on the French and German tree-banks, our multi-scale grammars achieve the best figures we are aware of, without any language spe-cific modifications."
"This confirms that latent vari- able models are well suited for capturing the syn-tactic properties of a range of languages, and also shows that discriminative grammars are still effec-tive when trained on smaller corpora."
It can be illuminating to see the subcategories that are being learned by our discriminative multi-scale grammars and to compare them to generatively es-timated latent variable grammars.
"Compared to the generative case, the lexical categories in the discrim-inative grammars are substantially less refined."
"For example, in the generative case, the nominal cate-gories were fully refined, while in the discrimina-tive case, fewer nominal clusters were heavily used."
One reason for this can be seen by inspecting the first two-way split in the NNP tag.
"The genera-tive model split into initial NNPs (San, Wall) and final NNPs (Francisco, Street)."
"In contrast, the dis-criminative split was between organizational entities (Stock, Exchange) and other entity types (September, New, York)."
This constrast is unsurprising.
Genera-tive likelihood is advantaged by explaining lexical choice – New and York occur in very different slots.
"However, they convey the same information about the syntactic context above their base NP and are therefore treated the same, discriminatively, while the systematic attachment distinctions between tem-porals and named entities are more predictive."
Analyzing the syntactic and semantic patterns learned by the grammars shows similar trends.
In Table 3 we compare the number of subcategories in the generative split-and-merge grammars to the average number of features per unsplit production with that phrasal category as head in our multi-scale grammars after 5 split (and merge) rounds.
These quantities are inherently different: the number of features should be roughly cubic in the number of subcategories.
"However, we observe that the num-bers are very close, indicating that, due to the spar-sity of our productions, and the efficient multi-scale encoding, the number of grammar parameters grows linearly in the number of subcategories."
"Further-more, while most categories have similar complex-ity in those two cases, the complexity of the two most refined phrasal categories are flipped."
"Gener-ative grammars split NPs most highly, discrimina- tive grammars split the VP."
"This distinction seems to be because the complexity of VPs is more syntac-tic (e.g. complex subcategorization), while that of NPs is more lexical (noun choice is generally higher entropy than verb choice)."
It is also interesting to examine the automatically learned word class features.
Table 4 shows the suf-fixes with the highest weight for a few different cat-egories across the three languages that we experi-mented with.
The learning algorithm has selected discriminative suffixes that are typical derviational or inflectional morphemes in their respective lan-guages.
"Note that the highest weighted suffixes will typically not correspond to the most common suffix in the word class, but to the most discriminative."
"Finally, the span features also exhibit clear pat-terns."
"The highest scoring span features encourage the words between the last two punctuation marks to form a constituent (excluding the punctuation marks), for example ,[x+]. and :[x+]."
Words between quotation marks are also encouraged to form constituents: ‘‘[x+]’’ and x[‘‘x+’’]x.
Span features can also discourage grouping words into constituents.
"The features with the highest neg-ative weight involve single commas: x[x,x+], and x[x+,x+]x and so on (indeed, such spans were structurally disallowed by the[REF_CITE]parser)."
Discriminatively trained multi-scale grammars give state-of-the-art parsing performance on a variety of languages and corpora.
"Grammar size is dramati-cally reduced compared to the baseline, as well as to methods like split-and-merge[REF_CITE]."
"Because fewer parameters are estimated, multi-scale grammars may also be less prone to overfitting, as suggested by a cross-corpus evaluation experiment."
"Furthermore, the discriminative framework enables the seamless integration of additional, overlapping features, such as span features and unknown word features."
Such features further improve parsing per-formance and make the latent variable grammars very language independent.
"Our parser, along with trained grammars for a variety of languages, is available[URL_CITE]"
We show that jointly parsing a bitext can sub-stantially improve parse quality on both sides.
"In a maximum entropy bitext parsing model, we define a distribution over source trees, tar-get trees, and node-to-node alignments be-tween them."
Features include monolingual parse scores and various measures of syntac-tic divergence.
"Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likeli-hood of training tree pairs, with alignments treated as latent variables."
The resulting bi-text parser outperforms state-of-the-art mono-lingual parser baselines by 2.5 F 1 at predicting English side trees and 1.8 F 1 at predicting Chi-nese side trees (the highest published numbers on these corpora).
"Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation."
"Methods for machine translation (MT) have increas-ingly leveraged not only the formal machinery of syntax[REF_CITE], but also linguistic tree structures of either the source side[REF_CITE], the target side[REF_CITE], or both[REF_CITE]."
These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore im-pacted by parser quality.
"Unfortunately, parsing gen-eral bitexts well can be a challenge for newswire-trained treebank parsers for many reasons, including out-of-domain input and tokenization issues."
"On the other hand, the presence of translation pairs offers a new source of information: bilin-gual constraints."
"For example, Figure 1 shows a case where a state-of-the-art English parser[REF_CITE]has chosen an incorrect structure which is incompatible with the (correctly chosen) output of a comparable Chinese parser."
"In this paper, we show that bilin-gual constraints and reinforcement can be leveraged to substantially improve parses on both sides of a bitext, even for two resource-rich languages."
"Formally, we present a log-linear model over triples of source trees, target trees, and node-to-node tree alignments between them."
We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment.
Our model conditions on the input sen-tence pair and so features can and do reference input characteristics such as posterior distributions from a word-level aligner[REF_CITE].
"Our training data is the translated section of the Chinese treebank[REF_CITE], so at training time correct trees are observed on both the source and target side."
Gold tree align-ments are not present and so are induced as latent variables using an iterative training procedure.
"To make the process efficient and modular to existing monolingual parsers, we introduce several approxi-mations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k 2 com-binations, and Viterbi approximations to alignment posteriors."
We evaluate our system primarily as a parser and secondarily as a component in a machine translation pipeline.
"For both English and Chinese, we begin with the state-of-the-art parsers presented[REF_CITE]as a baseline."
Joint parse selection improves the English trees by 2.5 F [Footnote_1] and the Chi-nese trees by 1.8 F 1 .
"1 It is anticipated that in some applications, such as tree trans-ducer extraction, the alignments themselves may be of value, but in the present work they are not evaluated."
"While other Chinese treebank parsers do not have access to English side transla-tions, this Chinese figure does outperform all pub-lished monolingual Chinese treebank results on an equivalent split of the data."
"As MT motivates this work, another valuable evaluation is the effect of joint selection on down-stream MT quality."
"In an experiment using a syntactic MT system, we find that rules extracted from joint parses results in an increase of 2.4 BLEU points over rules extracted from independent parses. [Footnote_1]"
"1 It is anticipated that in some applications, such as tree trans-ducer extraction, the alignments themselves may be of value, but in the present work they are not evaluated."
"In sum, jointly parsing bitexts improves parses substantially, and does so in a way that that carries all the way through the MT pipeline."
"In our model, we consider pairs of sentences (s, s 0 ), where we use the convention that unprimed vari-ables are source domain and primed variables are target domain."
These sentences have parse trees t (respectively t 0 ) taken from candidate sets T (T 0 ).
Non-terminal nodes in trees will be denoted by n (n 0 ) and we abuse notation by equating trees with their node sets.
Alignments a are simply at-most-one-to-one matchings between a pair of trees t and t 0 (see Figure 2a for an example).
Note that we will also mention word alignments in feature definitions; a and the unqualified term alignment will always re-fer to node alignments.
Words in a sentence are de-noted by v (v 0 ).
"Our model is a general log-linear (maximum en-tropy) distribution over triples (t, a, t 0 ) for sentence pairs (s, s 0 ):"
"P(t, a, t|s, s 0 ) ∝ exp(w &gt; φ(t, a, t 0 ))"
"Features are thus defined over (t,a,t 0 ) triples; we discuss specific features below."
"To use our model, we need features of a triple (t, a, t 0 ) which encode both the monolingual quality of the trees as well as the quality of the alignment between them."
We introduce a variety of features in the next sections.
"To capture basic monolingual parse quality, we be-gin with a single source and a single target feature whose values are the log likelihood of the source tree t and the target tree t 0 , respectively, as given by our baseline monolingual parsers."
These two fea-tures are called S OURCE LL and T ARGET LL respec-tively.
"It is certainly possible to augment these sim-ple features with what would amount to monolin-gual reranking features, but we do not explore that option here."
"Note that with only these two features, little can be learned: all positive weights w cause the jointly optimal parse pair (t, t 0 ) to comprise the two top-1 monolingual outputs (the baseline)."
"All other features in our model reference the entire triple (t,a,t 0 )."
"In this work, such features are de-fined over aligned node pairs for efficiency, but gen-eralizations are certainly possible."
"Bias: The first feature is simply a bias feature which has value 1 on each aligned node pair (n, n 0 )."
This bias allows the model to learn a general prefer-ence for denser alignments.
"Alignment features: Of course, some alignments are better than others."
One indicator of a good node-to-node alignment between n and n 0 is that a good word alignment model thinks that there are many word-to-word alignments in their bispan.
"Similarly, there should be few alignments that violate that bis-pan."
"To compute such features, we define a(v,v 0 ) to be the posterior probability assigned to the word alignment between v and v 0 by an independent word aligner. [Footnote_2]"
"2 It is of course possible to learn good alignments using lexi-cal indicator functions or other direct techniques, but given our very limited training data, it is advantageous to leverage counts from an unsupervised alignment system."
"Before defining alignment features, we need to define some additional variables."
"For any node n ∈ t (n 0 ∈ t 0 ), the inside span i(n) (i(n 0 )) comprises the input tokens of s (s 0 ) dominated by that node."
"Similarly, the complement, the outside span, will be denoted o(n) (o(n 0 )), and comprises the tokens not dominated by that node."
"See Figure 2b,c for exam-ples of the resulting regions."
"I NSIDE B OTH = X X a(v, v 0 ) v∈i(n) v 0 ∈i(n 0 )"
"I N S RC O UT T RG = X X a(v, v 0 ) v∈i(n) v 0 ∈o(n 0 )"
"I N T RG O UT S RC = X X a(v, v 0 ) v∈o(n) v 0 ∈i(n 0 )"
"Hard alignment features: We also define the hard versions of these features, which take counts from the word aligner’s hard top-1 alignment output δ:"
"H ARD I NSIDE B OTH = X X δ(v, v 0 ) v∈i(n) v 0 ∈i(n 0 )"
"H ARD I N S RC O UT T RG = X X δ(v, v 0 ) v∈i(n) v 0 ∈o(n 0 )"
"H ARD I N T RG O UT S RC = X X δ(v, v 0 ) v∈o(n) v 0 ∈i(n 0 )"
"Scaled alignment features: Finally, undesirable larger bispans can be relatively sparse at the word alignment level, yet still contain many good word alignments simply by virtue of being large."
We therefore define a scaled count which measures den-sity rather than totals.
The geometric mean of span lengths was a superior measure of bispan “area” than the true area because word-level alignments tend to be broadly one-to-one in our word alignment model.
I NSIDE B OTH S CALED I NSIDE B OTH = p|i(n)| · |i(n 0 ) | I N S RC O UT T RG S CALED I N S RC O UT T RG = p|i(n)| · |o(n 0 ) | I N T RG O UT S RC S CALED I N T RG O UT S RC = p|o(n)| · |i(n 0 )|
"Head word alignment features: When consider-ing a node pair (n, n 0 ), especially one which dom-inates a large area, the above measures treat all spanned words as equally important."
"However, lex-ical heads are generally more representative than other spanned words."
Let h select the headword of a node according to standard head percolation rules[REF_CITE].
"A LIGN H EAD W ORD = a(h(n), h(n 0 ))"
"H ARD A LIGN H EAD W ORD = δ(h(n), h(n 0 ))"
We also consider features that measure correspon-dences between the tree structures themselves.
"Span difference: We expect that, in general, aligned nodes should dominate spans of roughly the same length, and so we allow the model to learn to penalize node pairs whose inside span lengths differ greatly."
S PAN D IFF = ||i(n)| − |i(n 0 )||
Number of children: We also expect that there will be correspondences between the rules of the CFGs that generate the trees in each language.
"To encode some of this information, we compute in-dicators of the number of children c that the nodes have in t and t 0 ."
"N UM C HILDREN h|c(n)|, |c(n 0 )|i = 1"
"Child labels: In addition, we also encode whether certain label pairs occur as children of matched nodes."
"Let c(n, `) select the children of n with la-bel `."
"C HILD L ABEL h`, ` 0 i = |c(n, `)| · |c(n 0 , ` 0 )|"
Note that the corresponding “self labels” feature is not listed because it arises in the next section as a typed variant of the bias feature.
"For each feature above (except monolingual fea-tures), we create label-specific versions by conjoin-ing the label pair (`(n),`(n 0 ))."
We use both the typed and untyped variants of all features.
"Recall that our data condition supplies sentence pairs (s, s 0 ) along with gold parse pairs (g, g 0 )."
We do not observe the alignments a which link these parses.
"In principle, we want to find weights which maximize the marginal log likelihood of what we do observe given our sentence pairs: [Footnote_3] w ∗ = arg max X P(g, a, g 0 |s, s 0 , w) (1) w a P exp(w &gt; φ(g, a, g 0 )) = arg max P a P (2) w (t,t 0 ) a exp(w &gt; φ(t, a, t 0 ))"
"3 In this presentation, we only consider a single sentence pair for the sake of clarity, but our true objective was multiplied over all sentence pairs in the training data."
There are several challenges.
"First, the space of symmetric at-most-one-to-one matchings is #P-hard to sum over exactly[REF_CITE]."
"Second, even without matchings to worry about, standard meth-ods for maximizing the above formulation would re-quire summation over pairs of trees, and we want to assume a fairly generic interface to independent monolingual parsers (though deeper joint modeling and/or training is of course a potential extension)."
"As we have chosen to operate in a reranking mode over monolingual k-best lists, we have another is-sue: our k-best outputs on the data which trains our model may not include the gold tree pair."
"We therefore make several approximations and modifi-cations, which we discuss in turn."
"Because summing over alignments a is intractable, we cannot evaluate (2) or its derivatives."
"However, if we restrict the space of possible alignments, then we can make this optimization more feasible."
"One way to do this is to stipulate in advance that for each tree pair, there is a canonical alignment a 0 (t, t 0 )."
"Of course, we want a 0 to reflect actual correspondences between t and t 0 , so we want a reasonable definition that ensures the alignments are of reasonable qual-ity."
"Fortunately, it turns out that we can efficiently optimize a given a fixed tree pair and weight vector: a ∗ = arg max P(a|t, t 0 , s, s 0 , w) a = arg max P(t, a, t 0 |s, s 0 , w) a = arg max exp(w &gt; φ(t, a, t 0 )) a"
This optimization requires only that we search for an optimal alignment.
"Because all our features can be factored to individual node pairs, this can be done with the Hungarian algorithm in cubic time. [Footnote_4] Note that we do not enforce any kind of domination con-sistency in the matching: for example, the optimal alignment might in principle have the source root aligning to a target non-root and vice versa."
"4 There is a minor modification to allow nodes not to match. Any alignment link which has negative score is replaced by a zero-score link, and any zero-score link in the solution is con-sidered a pair of unmatched nodes."
"We then define a 0 (t,t 0 ) as the alignment that maximizes w 0&gt; φ(t, a, t 0 ), where w 0 is a fixed initial weight vector with a weight of 1 for I NSIDE B OTH , -1 for I N S RC O UT T RG and I N T RG O UT S RC , and 0 for all other features."
"Then, we simplify (2) by fix-ing the alignments a 0 : exp(w &gt; φ(g, a 0 (g, g 0 ), g 0 )) w ∗ = arg max (3) (t,t 0 ) exp(w &gt; φ(t, a 0 (t, t 0 ), t 0 ))"
This optimization has no latent variables and is therefore convex and straightforward.
"However, while we did use this as a rapid training procedure during development, fixing the alignments a priori is both unsatisfying and also less effective than a pro-cedure which allows the alignments a to adapt dur-ing training."
"Again, for fixed alignments a, optimizing w is easy."
"Similarly, with a fixed w, finding the optimal a for any particular tree pair is also easy."
"Another option is therefore to use an iterative procedure that alternates between choosing optimal alignments for a fixed w, and then reoptimizing w for those fixed alignments according to (3)."
"By iterating, we per-form the following optimization: max a exp(w &gt; φ(g, a, g 0 )) w ∗ = arg max (4)P w (t,t 0 ) max a exp(w &gt; φ(t, a, t 0 ))"
Note that (4) is just (2) with summation replaced by maximization.
"Though we do not know of any guarantees for this EM-like algorithm, in practice it converges after a few iterations given sufficient training data."
We initialize the procedure by setting w 0 as defined above.
"When training our model, we approximate the sets of all trees with k-best lists, T and T 0 , produced by monolingual parsers."
"Since these sets are not guaranteed to contain the gold trees g and g 0 , our next approximation is to define a set of pseudo-gold trees, following previous work in monolingual parse reranking[REF_CITE]."
We define T̂ (T̂ 0 ) as the F 1 -optimal subset of T (T 0 ).
We then modify (4) to reflect the fact that we are seeking to maximize the likelihood of trees in this subset:
"X w ∗ = arg max P(t, t 0 |s, s 0 , w) (5) w (t,t 0 )∈(T̂,T̂ 0 ) where P(t, t 0 |s, s 0 , w) = max a exp(w &gt; φ(t, a, t 0 ))"
"P (6) (t̄,t̄ 0 )∈(T,T 0 ) max a exp(w &gt; φ(t̄, a, t̄ 0 ))"
"To reduce the time and space requirements for train-ing, we do not always use the full k-best lists."
"To prune the set T , we rank all the trees in T from 1 to k, according to their log likelihood under the base-line parsing model, and find the rank of the least likely pseudo-gold tree: r ∗ = min rank(t) t∈T̂"
"Finally, we restrict T based on rank:"
T pruned = {t ∈ T|rank(t) ≤ r ∗ + } where is a free parameter of the pruning procedure.
The restricted set T pruned0 is constructed in the same way.
"When training, we replace the sum over all tree pairs in (T, T 0 ) in the denominator of (6) with a sum over all tree pairs in (T pruned , T pruned0 )."
"The parameter can be set to any value from 0 to k, with lower values resulting in more efficient training, and higher values resulting in better perfor-mance."
We set by empirically determining a good speed/performance tradeoff (see §6.2).
"At test time, we have a weight vector w and so selecting optimal trees for the sentence pair (s, s 0 ) from a pair of k best lists, (T, T 0 ) is straightforward."
"We just find: (t ∗ , t 0∗ ) = arg max max P(t, a, t 0 |s, s 0 , w) (t,t 0 )∈(T,T 0 ) a = arg max max w &gt; φ(t, a, t 0 ) (t,t 0 )∈(T,T 0 ) a"
"Note that with no additional cost, we can also find the optimal alignment between t ∗ and t 0∗ : a ∗ = arg max w &gt; φ(t ∗ , a, t 0∗ ) a"
"Because the size of (T, T 0 ) grows as O(k 2 ), the time spent iterating through all these tree pairs can grow unreasonably long, particularly when reranking a set of sentence pairs the size of a typical MT corpus."
"To combat this, we use a simple pruning technique to limit the number of tree pairs under consideration."
"To prune the list of tree pairs, first we rank them according to the metric: w S OURCE LL ·"
S OURCE LL + w T ARGET LL · T ARGET LL
"Then, we simply remove all tree pairs whose rank-ing falls below some empirically determined cutoff."
"As we show in §6.3, by using this technique we are able to speed up reranking by a factor of almost 20 without an appreciable loss of performance."
"All the data used to train the joint parsing model and to evaluate parsing performance were taken from ar-ticles 1-325 of the Chinese treebank, which all have English translations with gold-standard parse trees."
"The articles were split into training, development, and test sets according to the standard breakdown for Chinese parsing evaluations."
"Not all sentence pairs could be included for various reasons, including one-to-many Chinese-English sentence alignments, sentences omitted from the English translations, and low-fidelity translations."
Additional sentence pairs were dropped from the training data because they had unambiguous parses in at least one of the two languages.
Table 1 shows how many sentences were included in each dataset.
We had two training setups: rapid and full.
"In the rapid training setup, only 1000 sentence pairs from the training set were used, and we used fixed align-ments for each tree pair rather than iterating (see §4.1)."
The full training setup used the iterative train-ing procedure on all 2298 training sentence pairs.
We used the English and Chinese parsers[REF_CITE][Footnote_5] to generate all k-best lists and as our evaluation baseline.
"Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270)."
This modified grammar was used to generate the k-best lists that we trained our model on.
"However, as we tested on the same set of articles used for monolingual Chi-nese parser evaluation, there was no need to use a modified grammar to generate k-best lists at test time, and so we used a regularly trained Chinese parser for this purpose."
"We also note that since all parsing evaluations were performed on Chinese treebank data, the Chi-nese test sentences were in-domain, whereas the English sentences were very far out-of-domain for the Penn Treebank-trained baseline English parser."
"Hence, in these evaluations, Chinese scores tend to be higher than English ones."
"Posterior word alignment probabilities were ob-tained from the word aligner[REF_CITE]and[REF_CITE][Footnote_6] , trained on approxi-mately 1.7 million sentence pairs."
"For our alignment model we used an HMM in each direction, trained to agree[REF_CITE], and we combined the pos-teriors using DeNero and Klein’s (2007) soft union method."
"Unless otherwise specified, the maximum value of k was set to 100 for both training and testing, and all experiments used a value of 25 as the parameter for training set pruning and a cutoff rank of 500 for test set pruning."
"To verify that all our features were contributing to the model’s performance, we did an ablation study, removing one group of features at a time."
"Table 2 shows the F 1 scores on the bilingual development data resulting from training with each group of fea-tures removed. [Footnote_7] Note that though head word fea-tures seemed to be detrimental in our rapid train-ing setup, earlier testing had shown a positive effect, so we reran the comparison using our full training setup, where we again saw an improvement when including these features."
"7 We do not have a test with the basic alignment features removed because they are necessary to compute a 0 (t, t 0 )."
"To find a good value of the parameter for train-ing set pruning we tried several different values, us-ing our rapid training setup and testing on the dev set."
The results are shown in Table 3.
"We also tried several different values of the rank cut-off for test set pruning, using the full training setup and testing on the dev set."
The results are in Table 4.
"For F 1 evaluation, which is on a very small set of sentences, we selected 500 as the value with the best speed/performance tradeoff."
"However, when rerank-ing our entire MT corpus, we used a value of 200, sacrificing a tiny bit of performance for an extra fac-tor of 2 in speed. [Footnote_8]"
"8 Using a rank cutoff of 200, the reranking step takes slightly longer than serially running both baseline parsers, and generat-ing k-best lists takes slightly longer than getting 1-best parses, so in total, joint parsing takes about 2.3 times as long as mono-lingual parsing. With a rank cutoff of 500, total parsing time is scaled by a factor of around 3.8."
"Since our bitext parser currently operates as a reranker, the quality of the trees is limited by the quality of the k-best lists produced by the baseline parsers."
"To test this limitation, we evaluated perfor-mance on the dev set using baseline k-best lists of varying length."
Training parameters were fixed (full training setup with k = 100) and test set pruning was disabled for these experiments.
The results are in Ta-ble 5.
"The relatively modest gains with increasing k, even as the oracle scores continue to improve, indi-cate that performance is limited more by the model’s reliance on the baseline parsers than by search errors that result from the reranking approach."
Our final evaluation was done using the full training setup.
"Here, we report F 1 scores on two sets of data."
"First, as before, we only include the sentence pairs from our bilingual corpus to fully demonstrate the gains made by joint parsing."
We also report scores on the full test set to allow easier comparison with past work on Chinese parsing.
"For the latter evalu-ation, sentences that were not in the bilingual cor-pus were simply parsed with the baseline parsers."
The results are in Table 6.
"Joint parsing improves F 1 by 2.5 points on out-of-domain English sentences and by 1.8 points on in-domain Chinese sentences; this represents the best published Chinese treebank parsing performance, even after sentences that lack a translation are taken into account."
"To test the impact of joint parsing on syntactic MT systems, we compared the results of training an MT system with two different sets of trees: those pro-duced by the baseline parsers, and those produced by our joint parser."
"For this evaluation, we used a syn-tactic system based[REF_CITE]and[REF_CITE], which extracts tree-to-string trans-ducer rules based on target-side trees."
"We trained the system on 150,000 Chinese-English sentence pairs from the training corpus[REF_CITE], and used a large (close to 5 billion tokens) 4-gram lan- guage model for decoding."
We tuned and evaluated BLEU[REF_CITE]on separate held-out sets of sentences of up to length 40 from the same corpus.
"The results are in Table 7, showing that joint parsing yields a BLEU increase of 2.4. [Footnote_9]"
9 Note that all numbers are single-reference BLEU scores and are not comparable to multiple reference scores or scores on other corpora.
"By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual con-straints that improve the quality of syntactic analy-ses over independent monolingual parsing."
"We pre-sented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list."
"On Chinese treebank data, this procedure improves F 1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences."
"Fur-thermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement."
"Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications."
"However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees."
"In this paper, we propose a technique that au-tomatically takes into account certain charac-teristics of the domains of interest, and ac-curately predicts parser performance on data from these new domains."
"As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a sta-tistical parser on any given domain."
Statistical natural language parsers have recently become more accurate and more widely available.
"As a result, they are being used in a variety of applications, such as question answering[REF_CITE], speech recogniti[REF_CITE], language modeling[REF_CITE], lan-guage generati[REF_CITE]and, most notably, machine translati[REF_CITE]."
"These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute."
"Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?”"
"The only recipe that is implicitly given in the large literature on parsing to date is to have human anno-tators build parse trees for a sample set from the do-main of interest, and consequently use them to com-pute a PARSEVAL[REF_CITE]score that is indicative of the intrinsic performance of the parser."
"Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human anno-tators to produce parse tree annotations, this recipe is, albeit precise, too expensive."
"The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ[REF_CITE], and assume that the accuracy measure will carry over to the domains of interest."
"This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding impor-tant decisions for the design of NLP systems that use a syntactic parser as an important component."
This paper proposes another method for measur-ing the performance of a parser on a given domain that is both cheap and effective.
It is a fully auto-mated procedure (no expensive annotation involved) that uses properties of both the domain of interest and the domain on which the parser was trained in order to measure the performance of the parser on the domain of interest.
"It is, in essence, a solution to the following prediction problem:"
"Input: (1) a statistical parser and its training data, (2) some chunk of text from a new domain or genre"
Output: an estimate of the accuracy of the parse trees produced for that chunk of text
Accurate estimations for this prediction problem will allow a system designer to make the right de-cisions for the given domain of interest.
"Such deci-sions include, but are not restricted to, the choice of the parser, the choice of the training data, the choice of how to implement various components such as the treatment of unknown words, etc."
"Altogether, a cor-rect estimation of the impact of such decisions on the resulting parse trees can guide a system designer in a hill-climbing scenario for which an extrinsic metric (such as the impact on the overall quality of the sys-tem) is usually too expensive to be employed often enough."
"To provide an example, a machine transla-tion engine that requires parse trees as training data in order to learn syntax-based translation rules[REF_CITE]needs to employ a syntactic parser as soon as the training process starts, but it can take up to hundreds and even thousands of CPU hours (for large training data sets) to train the engine be-fore translations can be produced and measured."
"Al-though a real estimate of the impact of a parser de-sign decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision."
"On the other hand, estimates using the so-lution proposed in this paper can be obtained fast, before submitting the parser output to a costly train-ing procedure."
"There have been previous studies which explored the problem of automatically predicting the task diffi-culty for various NLP applications.[REF_CITE]presented a regression based method for developing automatic evaluation metrics for ma-chine translation systems without directly relying on human reference translations.[REF_CITE]built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features."
"There have been a few studies of English parser accuracy in domains/genres other than WSJ[REF_CITE], but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non-"
WSJ domain of interest.
"He looked at sentences with 40 words or less.[REF_CITE]carried out a similar experiment on sentences of all lengths, and[REF_CITE]report additional re-sults."
"The table below shows results from our own measurements of Charniak parser [Footnote_1][REF_CITE]accuracy (F-measure on sentences of all lengths), which are consistent with these studies."
"For the Brown corpus, the test set was formed from every tenth sentence in the corpus[REF_CITE]."
"Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sen-tences, and whole corpora."
"We also investigate and contrast several scenarios for prediction: (1) the pre-dictor looks at the input text only, (2) the predictor looks at the input text and the output parse trees of P, and (3) the predictor looks at the input text, the output parse trees of P, and the outputs of other pro-grams, such as the output parse trees of a different parser P ref used as a reference."
Under none of these scenarios is the predictor allowed to look at gold-standard parses in the new domain/genre.
"The intuition behind what we are trying to achieve here can be compared to an analogous task—trying to assess the performance of a median student from a math class on a given test, without having access to the answer sheet."
"Looking at the test only, we could probably tell whether the test looks hard or not, and therefore whether the student will do well or not."
Looking at the student’s answers will likely give us an even better idea of the performance.
"Finally, the answers of a second student with similar proficiency will provide even better clues: if the students agree on every answer, then they probably both did well, but if they disagree frequently, then they (and hence our student) probably did not do as well."
Our first experiments are concerned with validat-ing the idea itself: can a predictor be trained such that it predicts the same F-scores as the ones ob-tained using gold-trees?
"We first validate this using the WSJ corpus itself, by dividing the WSJ treebank into several sections: 1."
The parser P is trained on this data. [Footnote_2].
2 Weka software ([URL_CITE]
We use this data for training our predictor. [Footnote_3].
3 We compared a few regression algorithms like SVM-Regression (using different kernels and parameter settings) and Multi-Layer Perceptron (neural networks) – we trained the al-gorithms separately on dev data and picked the one that gave the best cross-validation accuracy (F-measure).
We use this data for measuring our predictions.
"For each test sentence, we compute (1) the PARSEVAL F-measure score using the test gold standard, and (2) our predicted F-measure."
We report the correlation coefficient (r) between the actual F-scores and our predicted F-scores.
We will also use a root-mean-square error (rms error) metric to compare actual and predicted F-scores.
Section 3 describes the features used by our pre-dictor.
"Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights."
"To this end, we use SVM-Regression 2[REF_CITE]with an RBF kernel, to learn the feature weights and build our predictor system. 3 We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5)."
One hypothesis we explore is that (all other things being equal) longer sentences are harder to parse correctly than shorter sentences.
"When exposed to the development set, SVM-Regression learns weights to best predict F-scores using the values for this feature corresponding to each sentence in the corpus."
Does the predicted F-score correlate with actual F-score on a sentence by sentence basis?
There was a positive but weak correlation:
"Another hypothesis is that the parser performance is influenced by the number of UNKNOWN words in the sentence to be parsed, i.e., the number of words in the test sentence that were never seen be-fore in the training set."
"Training the predictor with this feature produces a positive correlation, slightly weaker compared to the Length feature."
Feature set dev (r) test (r) UNK 0.11 0.11
Unknown words are not the only ones that can in-fluence the performance of a parser.
"Rare words, for which statistical models do not have reliable es-timates, are also likely to impact parsing accuracy."
"To test this hypothesis, we add a language model perplexity–based (LM-PPL) feature."
"We extract the yield of the training trees, on which we train a tri-gram language model. [Footnote_4] We compute the perplexity of each test sentence with respect to this language model, and use it as feature in our predictor system."
"4 We trained using the SRILM language modeling toolkit, with default settings."
"Note that this feature is meant as a refinement of the previous UNK feature, in the sense that perplexity numbers are meant to signal the occurrence of un-known words, as well as rare (from the training data perspective) words."
"However, the correlation we ob-serve for this feature is similar to the correlation ob-served for the UNK feature, which seems to suggest that the smoothing techniques used by the parsers employed in these experiments lead to correct treat-ment of the rare words."
Feature set dev (r) test (r) LM-PPL 0.11 0.10
We also look at the possibility of automatically detecting certain “cue” words that are appropriate for our prediction problem.
"That is, we want to see if we can detect certain words that have a discrimi-nating power in deciding whether parsing a sentence that contains them is difficult or easy."
"To this end, we use a subset of the development data, which con-tains the 200 best-parsed and 200 worst-parsed sen-tences (based on F-measure scores)."
"For each word in the development dataset, we compute the infor-mation gain (IG)[REF_CITE]score for that word with respect to the best/worst parsed dataset."
"These words are then ranked by their IG scores, and the top 100 words are included as lex-ical features in our predictor system."
"As expected, the correlation on the development set is quite high (given that these lexical cues are extracted from this particular set), but a positive correlation holds for the test set as well."
Feature set dev (r) test (r) lex[REF_CITE]0.43 0.18
"Besides exploiting the information present in the in-put text, we can also inspect the output tree of the parser P for which we are interested in predicting accuracy."
"We create a rootSYN feature based on the syntactic category found at the root of the out-put tree (“is it S?”, “is it FRAG?”)."
"We also create a puncSYN feature based on the number of words labeled as punctuation tags (based on the intuition that heavy use of punctuation can be indicative of the difficulty of the input sentences), and a label- SYN feature in which we bundled together informa-tion regarding the number of internal nodes in the parse tree output that have particular labels (“how many nodes are labeled with PP?”)."
"In our predictor, we use 72 such labelSYN features corresponding to all the syntactic labels found in the parse tree out-put for the development set."
"The test set correlation given by the rootSYN and the labelSYN features are higher than some of the text-based features, whereas the puncSYN feature seems to have little discrimi-native power."
Feature set dev (r) test (r) rootSYN 0.21 0.17 puncSYN 0.09 0.01 labelSYN 0.33 0.28
"In addition to the text-based features and parser P– based features, we can bring in an additional parser P ref whose output is used as a reference against which the output of parser P is measured."
"For the reference parser feature, our goal is to measure how similar/different are the results from the two parsers."
"We find that if the parses are similar, they are more likely to be right."
"In order to compute similarity, we can compare the constituents in the two parse trees from P and P ref , and see how many constituents match."
This is most easily accomplished by consid-ering P ref to be a “gold standard” (even though it is not necessarily a correct parse) and computing the F-measure score of parser P against P ref .
We use this F-measure score as a feature for prediction.
"For the experiments presented in this section we use as P ref , the parser[REF_CITE]."
"Intu-itively, the requirement for choosing parser P ref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reli-able parse trees."
"The choice of P[REF_CITE]and P ref[REF_CITE]fits this bill, but many other choices can be made regarding P ref , such[REF_CITE]."
We leave the task of creating features based on the consensus of multiple parsers as future work.
The correlation given by the reference parser– based feature P ref on the test set is the highest among all the features we explored.
Feature set dev (r) test (r) P ref 0.40 0.36
"The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set."
Note how the lexical features tend to over-fit the development data—the words were specifically cho-sen for their discriminating power on that particular set.
"Hence, adding more lexical features to the pre-dictor system improves the correlation on develop-ment (due to over-fitting), but it does not produce consistent improvement on the test set."
"However, there is some indication that the counts of the lex-ical features are important, and count-based lexical features tend to have similar or better performance compared to their boolean-based counterparts."
"Since these features measure different but over-lapping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive."
"By taking the individual features that provide the best discriminative power, we are able to get a correla-tion score of 0.42 on the test set."
"If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method?"
"We implemented MCT, but obtained no better results."
"Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1)."
"The results for the scenario presented in Section 3 are encouraging, but other scenarios are also im-portant from a practical perspective."
"For instance, we are interested in predicting the performance of a particular parser not on a sentence-by-sentence ba-sis, but for a representative chunk of sentences from the new domain."
"In order to predict the F-measure on multiple sentences, we modify our feature set to generate information on a whole chunk of sentences rather than a single sentence."
"Predicting the corre-lation at chunk level is, not unexpectedly, an eas-ier problem than predicting correlation at sentence level, as the results in the first two columns of Ta-ble 2 show."
This scatterplot brings to light an artifact of using correlation metric (r) for evaluating our predictor’s performance.
"Although our objective is to improve correlation between ac-tual and predicted F-scores, the correlation metric (r) does not tell us directly how well the predictor is doing."
"In Figure 1, the system predicts that on an average, most sentence chunks can be parsed with an accuracy of 0.9085 (which is the mean pre-dicted F-score on WSJ-test)."
"But the range of pre-dictions from our system [0.89,0.92] is smaller than the actual F-score range [0.86,0.95]."
"Hence, even though the correlation scores are high, this does not necessarily mean that our predictions are on target."
"An additional metric, root-mean-square (rms) error, which measures the distance between actual and pre-dicted F-measures, can be used to gauge the qual-ity of our predictions."
"For a particular chunk-size, lowering the rms error translates into aligning the points of a scatterplot as the one in Figure 1, closer to the x=y line, implying that the predictor is getting better at exactly predicting the F-score values."
The third column in Table 2 shows the rms error for our predictor at different chunk sizes.
The results using this metric also show that the prediction problem be-comes easier as the chunk size increases.
"Assuming that we have the test set of WSJ sec-tion 23, but without the gold-standard trees, how can we get an approximation for the overall accu-racy of a parser P on this test set?"
"One possibility, which we use here as a baseline, is to compute the F-score on a set for which we do have gold-standard trees."
"If we use our development set[REF_CITE]for this purpose, and[REF_CITE]as the parser P, the baseline is an F-score of 90.48 (f d ), which is the actual Charniak parser accu-racy[REF_CITE]."
"Instead, if we run our pre-dictor on the test set (a single chunk containing all the sentences in the test set), it predicts an F-score of 90.85 (f p )."
These two predictions are listed as the first two rows in Table 3.
"Of course, having the actual gold-standard trees[REF_CITE]helps us decide which prediction is better: the actual ac-curacy of the Charniak parser[REF_CITE]is an F-score of 91.13 (f t ), which makes our prediction better than the baseline."
We correctly predict (in Table 3) that the WSJ-test is easier to parse than the WSJ-dev (90.85 &gt; 90.48).
"However, our predictor is too conservative—the WSJ-test is actually even easier to parse (91.13 &gt; 90.85)."
"We can fix this by shift-ing the mean predicted F-score (which is equal to f p ) further away from the dev F-measure (f d ), and closer to the actual F-measure (f t )."
This is achieved by shifting all the individual predictions by a certain amount as shown below.
Let p be an individual prediction from our system.
The shifted prediction p 0 is given by: p 0 = p + α(f p − f d ) (1)
We can tune α to make the new mean predic-tion (f p0 ) to be equal to the actual F-measure (f t ). f p0 = f p + α(f p − f d ) (2) f t − f p α = (3) f p − f d
"Using the F-score values from Table 3, we get an α = 0.757 and an exact prediction of 91.13."
"Of course, this is because we tune on test, so we need to validate this idea on a new test set to see if it leads to improved predictions (Section 5)."
Our predictor is also too conservative about its dis-tribution (see Figure 1).
"It knows (roughly) which chunks are easier to parse and which are harder, but its range of predictions is lower than the range of actual F-measure scores."
We can skew individual predictions so that sen-tences predicted to be easy are re-predicted to be even easier (and those that are hard to be even harder).
"For each prediction p 0 (from Equation 1), we compute p 00 = p 0 + β(p 0 − f p0 ) (4)"
"We simply set β to 1.0, doubling the distance of each prediction p 0 (in Equation 1) from the (ad-justed) mean prediction f p0 , to obtain the skewed pre-diction p 00 ."
Figure 2 shows how the points representing 100-sentence chunks in Figure 1 look after the predic-tions have been shifted (α = 0.757) and skewed (β = 1.0).
"These two operations have the desired effect of changing the range of predictions from [0.89,0.92] to [0.87,0.94], much closer to the actual range of [0.86,0.95]."
The points in the new plot (Fig-ure 2) also align closer to the “x=y” line than in the original graph (Figure 1).
"The rms error also drops from 0.015 to 0.014 (7% relative reduction), show-ing that the predictions have improved."
"Since we use the WSJ-test corpus to tune the pa-rameter values for shifting and skewing, we need to apply our predictor on a different test set to see if we get similar improvements by using these techniques, which we do in the next section."
"The Brown corpus represents a genuine challenge for our predictor, as it presents us with the oppor-tunity to test the performance of our predictor in an out-of-domain scenario."
"Our predictor, trained on WSJ data, is now employed to predict the per-formance of a WSJ-trained parser P on the Brown-test corpus."
"As in the previous experiments, we use[REF_CITE]trained on WSJ sec-tions 02-21 as parser P."
"The feature weights for our predictor are again trained on section 24 of WSJ, and the shifting and skewing parameters (α = 0.757, β = 1.0) are determined using section 23 of WSJ."
"The results on the Brown-test, both the origi-nal predictions and after they have been adjusted (shifted/skewed), are shown in Table 4, at different level of chunking."
"For chunks of size n &gt; 1, the shifting and skewing techniques help in lowering the rms error."
"In a similar vein with the evaluation done in Sec-tion 4, we are interested in estimating the overall ac-curacy of a WSJ-trained parser P given an out-of-domain set such as the Brown test set (for which, at least for now, we do not have access to gold-standard trees)."
"If we use[REF_CITE]as parser P, a cheap and readily-available answer is to approximate the performance using the Charniak parser performance[REF_CITE]which has an F-score of 91.13."
Another cheap and readily-available answer is to take the Charniak parser per-formance[REF_CITE]with an F-score of 90.48.
"Table 5 lists these baselines, along with the prediction made by our system when using a single chunk containing all the sentences in the Brown test set (both base predictions and adjusted predictions, i.e. shifting using α = 0.757)."
"Again, having gold-standard trees for the Brown test set helps us decide which prediction is better."
"Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34."
One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one pro-posed in this paper) is to compare parsing accu-racy when confronted with a choice between vari-ous parser deployments.
"Not only are there many parsing techniques available[REF_CITE], but recent annotation efforts in providing training material for statistical parsing[REF_CITE]have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”)."
"In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its applica-tion in an NLP task."
"For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser[REF_CITE], to which various speed-related enhancements[REF_CITE]have been applied."
"This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner."
"We use the more accurate, but slower Charniak parser[REF_CITE]as the reference parser P ref in our predictor (see Section 3.3)."
"In order to predict the Collins-style parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that was used for predicting Charniak parser behavior on the Brown corpus (Sec-tion 5)."
We compare three training scenarios that make for three different parsers: (1) P WSJ - trained on sections 02-21 of WSJ. (2) P News - trained on the union of the English
Chinese Translation Treebank[REF_CITE](news stories from Xinhua News Agency translated from Chinese into English) and the English Newswire Translation Treebank[REF_CITE](An-Nahar new stories translated from Arabic into English). (3) P WSJ−News - trained on the union of all the above training material.
"When comparing the performance of these three parsers on a development set from WSJ (section 0), we get the following F-scores. [Footnote_5]"
"5 Because of tokenization differences between the different treebanks involved in these experiments, we have to adopt a to-kenization scheme different from the one used in the Penn Tree-bank, and therefore the F-scores, albeit in the same range, are not directly comparable with the ones in the parsing literature."
Consider now that we are interested in compar-ing the parsing accuracy of these parsers on a do-main completely different from WSJ.
"The ranking P WSJ &gt;P WSJ−News &gt;P News , given by the evalua-tion above, provides some guidance, but is this guid-ance accurate?"
"The intuition here is that the in-formation that we already have about the new do-main of interest (which implicitly appears in texts extracted from this domain), can be used to bet-ter guide this decision."
"Our predictor is able to capitalize on this information, and provide domain-informed guidance for choosing the most accurate parser to use with the new data, which in this case relates to choosing the best training strategy for the parser P. If we consider as our domain of interest, news stories from Xinhua News Agency, then using our predictor on a chunk of 1866 sentences from this domain gives the F-scores shown in the second col-umn of Table 6."
"As with the previous experiments, we can com-pute the actual PARSEVAL F-scores (using gold-standard) for this particular 1866-sentence test set, as it happens to be part of the English Chinese Trans-lation Treebank[REF_CITE]."
These F-score fig-ures are shown in the third column of Table 6.
"As these results show, for this particular domain the cor-rect ranking is P WSJ−News &gt;P News &gt;P WSJ , which is exactly the ranking predicted by our method, with-out the aid of gold-standard trees."
"We observe that even though the system predicts the ranking correctly, the predictions in the Xinhua News domain might not be as accurate in compar-ison to the predictions on Brown corpus (predicted F-score = 86.96, actual F-score = 86.34)."
One pos-sible reason for this lower accuracy is that we use the same prediction model without optimizing for the particular parser on which we wish to make pre-dictions.
"Still, the model was able to make distinc-tions between multiple parsers for the ranking task correctly, and decide the best parser to use with the given data."
"We believe this to be useful in typical NLP applications which use parsing as a component, and where making the right choice between differ-ent parsers can affect the end-to-end accuracy of the system."
The steady advances in statistical parsing over the last years have taken this technology to the point where it is accurate enough to be useful in a va-riety of natural language applications.
"However, due to large variations in the characteristics of the domains for which these applications are devel-oped, estimating parsing accuracy becomes more involved than simply taking for granted accuracy estimates done on a certain well-studied domain, such as WSJ."
"As the results in this paper show, it is possible to take into account these variations in the domain characteristics (encoded in our predictor as text-based, syntax-based, and agreement-based features)—to make better predictions about the ac-curacy of certain statistical parsers (and under dif-ferent training scenarios), instead of relying on accu-racy estimates done on a standard domain."
"We have provided a mechanism to incorporate these domain variations for making predictions about parsing ac-curacy, without the costly requirement of creating human annotations for each of the domains of inter-est."
"The experiments shown in the paper were lim-ited to readily available statistical parsers (which are widely deployed in a number of applications), and certain domains/genres (because of ready access to gold-standard data on which we could verify predic-tions)."
"However, the features we use in our predic-tor are independent of the particular type of parser or domain, and the same technique could be applied for making predictions on other parsers as well."
There are many avenues for future work opened up by the work presented here.
The accuracy of the predictor can be further improved by incorporating more complex syntax-based features and multiple-agreement features.
"Moreover, rather than predict-ing an intrinsic metric such as the PARSEVAL F-score, the metric that the predictor learns to pre-dict can be chosen to better fit the final metric on which an end-to-end system is measured, in the style[REF_CITE]."
The end-result is a finely-tuned tool for predicting the impact of various parser design de-cisions on the overall quality of a system.
"We wish to acknowledge our colleagues at ISI, who provided useful suggestions and constructive criti-cism on this work."
We are also grateful to all the reviewers for their detailed comments.
This work was supported in part by NSF grant[REF_CITE].
"We address the task of computing vector space representations for the meaning of word oc-currences, which can vary widely according to context."
"This task is a crucial step towards a robust, vector-based compositional account of sentence meaning."
We argue that existing mod-els for this task do not take syntactic structure sufficiently into account.
We present a novel structured vector space model that addresses these issues by incorpo-rating the selectional preferences for words’ argument positions.
This makes it possible to integrate syntax into the computation of word meaning in context.
"In addition, the model per-forms at and above the state of the art for mod-eling the contextual adequacy of paraphrases."
"Semantic spaces are a popular framework for the rep-resentation of word meaning, encoding the meaning of lemmas as high-dimensional vectors."
"In the de-fault case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus."
These vectors are able to pro-vide a robust model of semantic similarity that has been used in NLP[REF_CITE]and to model experimental results in cognitive science[REF_CITE].
Semantic spaces are attractive because they provide a model of word meaning that is independent of dictio-nary senses and their much-discussed problems[REF_CITE].
"In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages[REF_CITE]."
"Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context."
There have been several approaches in the liter-ature[REF_CITE]that compute meaning in context from lemma vectors.
"Most of these studies phrase the prob-lem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b."
"The context b can consist of as little as one word, as shown in Example (1)."
"In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract."
"Conversely, verbs can influence the in-terpretation of nouns:"
"In (1a), ball is understood as a spherical object, and in (1c) as a dancing event. (1) a. catch a ball b. catch a disease c. attend a ball"
"In this paper, we argue that models of word mean-ing relying on this procedure of vector composition are limited both in their scope and scalability."
The underlying shortcoming is a failure to consider syntax in two important ways.
The syntactic relation is ignored.
"The first problem concerns the manner of vector composition, which ignores the relation between the target a and its con-text b."
"This relation can have a decisive influence on their interpretation, as Example (2) shows: (2) a. a horse draws b. draw a horse"
"In (2a), the meaning of the verb draw can be para-phrased as pull, while in (2b) it is similar to sketch."
"This difference in meaning is due to the difference in relation: in (2a), horse is the subject, while in (2b) it is the object."
"On the modeling side, however, a vector combination function that ignores the relation will assign the same representation to (2a) and (2b)."
"Thus, existing models are systematically unable to capture this class of phenomena."
Single vectors are too weak to represent phrases.
The second problem arises in the context of the im-portant open question of how semantic spaces can “scale up” to provide interesting meaning representa-tions for entire sentences.
"We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose."
One proposal for “scaling up” is to straightforwardly interpret c = a b as the meaning of the phrase a + b[REF_CITE].
"The problem is that the vector c can only encode a fixed amount of structural information if its dimen-sionality is fixed, but there is no upper limit on sen-tence length, and hence on the amount of structure to be encoded."
"It is difficult to conceive how c could encode deeper semantic properties, like predicate-argument structure (distinguishing “dog bites man” and “man bites dog”), that are crucial for sentence-level semantic tasks such as the recognition of textual entailment[REF_CITE]."
"An alternative ap-proach to sentence meaning would be to use the vec-tor space representation only for representing word meaning, and to represent sentence structure sepa-rately."
"Unfortunately, present models cannot provide this grounding either, since they compute a single vector c that provides the same representations for both the meanings of a and b in context."
"In this paper, we propose a new, structured vector space model for word meaning ( SVS ) that addresses these problems."
A SVS representation of a lemma comprises several vectors representing the word’s lexical meaning as well as the selectional preferences that it has for its argument positions.
"The meaning of word a in context b is computed by combining a with b’s selectional preference vector specific to the relation between a and b, addressing the first problem above."
"In an expression a + b, the meanings of a and b in this context are computed as two separate vectors a 0 and b 0 ."
"These vectors can then be combined with a representation of the structure’s expression (e.g., a parse tree), to address the second problem discussed above."
"We test the SVS model on the task of recognizing contextually appropriate paraphrases, finding that SVS performs at and above the state-of-the-art."
Plan of the paper.
Section 2 reviews related work.
Section 3 presents the SVS model for word meaning in context.
Sections 4 to 6 relate experiments on the paraphrase appropriateness task.
In this section we give a short overview over existing vector space based approaches to computing word meaning in context.
General context effects.
The first category of models aims at integrating the widest possible range of context information without recourse to linguistic structure.
The best-known work in this category is[REF_CITE].
He first computes “first-order” vec-tor representations for word meaning by collecting co-occurrence counts from the entire corpus.
"Then, he determines “second-order” vectors for individual word instances in their context, which is taken to be a simple surface window, by summing up all first-order vectors of the words in this context."
The resulting vectors form sense clusters.
They compute the expectation for a word w i in a sequence by summing the first-order vectors for the words w 1 to w i−1 and showed that the dis-tance between expectation and first-order vector for w i correlates with human reading times.
"The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a.[REF_CITE]uses vector representations of p and a to identify the set of words that are similar to both p and a. After this set has been narrowed down in a self-inhibitory network, the meaning of the predicate-argument combination is obtained by computing the centroid of its members’ vectors."
The procedure does not take the relation between p and a into account.
"R is the relation holding between p and a, and K additional knowledge."
This framework allows sen-sitivity to the relation.
"However, the concrete in-stantiations that Mitchell and Lapata consider disre-gards K and R, thus sharing the other models’ limi-tations."
"They focus instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication approach."
Tensor product-based models.
"The vector c is located in a very high-dimensional space and is thus capable of encoding the structure of the expression; however, this makes the model infeasible in practice, as dimensionality rises with every word added to the representation."
"Sim-ilar[REF_CITE], they predict most likely next words in a sequence, without taking syn-tax into account."
One of the main tests for the quality of models of word meaning in context is the ability to predict the appropriateness of paraphrases in given a context.
"Typically, a paraphrase applies only to some senses of a word, not all, as can be seen in the paraphrases “grab” and “contract” of “catch”."
Vector space models generally predict paraphrase ap-propriateness based on the similarity between vectors.
"This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation."
"Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, no-tably Information Retrieval and Textual Entailment."
"Nevertheless, they place their emphasis on different types of information."
"Current kernels are mostly tree kernels that compare syntactic structure, and use se-mantic information mostly for smoothing syntactic similarity[REF_CITE]."
"In con-trast, vector-space models focus on the interaction between the lexical meaning of words in composi-tion."
"In this section, we define the structured vector space ( SVS ) model of word meaning."
The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events.
"For example, in (1a), we assume that upon hearing the phrase “catch a ball”, the hearer will interpret the meaning of “catch” to match typical actions that can be performed with a ball."
"Similarly, the interpretation of “ball” will reflect the hearer’s expectations about typical things that can be caught."
This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds.
"In cognitive science, the central role of expecta-tions about typical events for human language pro-cessing is well-established."
"Expectations affect read-ing times[REF_CITE], the interpretation of participles[REF_CITE], and sentence pro-cessing generally[REF_CITE]."
Expectations exist both for verbs and nouns[REF_CITE].
"In linguistics, expectations, in the form of selec-tional restrictions and selectional preferences, have long been used in semantic theories[REF_CITE], and more recently induced from corpora[REF_CITE]."
"Attention has mostly been limited to selec-tional preferences of verbs, which have been used for example for syntactic disambiguati[REF_CITE], word sense disambiguati[REF_CITE]and semantic role label-ing[REF_CITE]."
"Recently, a vector-spaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen argu-ments[REF_CITE]."
We first present the SVS model of word meaning that integrates lexical information with selectional preferences.
"Then, we show how the SVS model pro-vides a new way of computing meaning in context."
Representing lemma meaning.
We abandon the traditional choice of representing word meaning as a single vector.
"Instead, we encode each word as a combination of (a) one vector that models the lexical meaning of the word, and (b) a set of vec-tors, each of which represents the semantic expecta-tions/selectional preferences for one particular rela-tion that the word supports. 1"
The idea is illustrated in Fig. 1.
"In the representa-tion of the verb catch, the central square stands for the lexical vector of catch itself."
"The three arrows link it to catch’s preferences for its subjects (subj), its objects (obj), and for verbs for which it appears as a complement (comp −1 )."
"The figure shows the se-lectional preferences as word lists for readability; in practice, each selectional preference is a single vector (cf. Section 4)."
"Likewise, ball is represented by one vector for ball itself, one for ball’s preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj −1 ), and one for the verbs of which is an object (obj −1 )."
"This representation includes selectional prefer-ences (like subj, obj, mod) exactly parallel to inverse selectional preferences (subj −1 , obj −1 , comp −1 )."
"To our knowledge, preferences of the lat-ter kind have not been studied in computational lin-guistics."
"However, their existence is supported in psycholinguistics by priming effects from nouns to typical verbs[REF_CITE]."
"Formally, let D be a vector space (the set of possi- ble vectors), and let R be some set of relation labels."
"In the structured vector space ( SVS ) model, we rep-resent the meaning of a lemma w as a triple w = (v, R, R −1 ) where v ∈ D is a lexical vector describing the word w itself, R : R → D maps each relation label onto a vector that describes w’s selectional preferences, and R −1 : R → D maps from role labels to vec-tors describing inverse selectional preferences of w. Both R and R −1 are partial functions."
"For example, the direct object preference would be undefined for intransitive verbs."
Computing meaning in context.
"The SVS model of lemma meaning permits us to compute the mean-ing of a word a in the context of another word b in a new way, via their selectional preferences."
"Let (v a , R a , R −1a ) and (v b , R b , R b−1 ) be the representa-tions of the two words, and let r ∈ R be the relation linking a to b."
"Then, we define the meaning of a and b in this context as a pair (a 0 , b 0 ) of vectors, where a 0 is the meaning of a in the context of b, and b 0 the meaning of b in the context of a: a 0 = v a R −1b (r), R a − {r}, R a−[Footnote_1] (4) b 0 = v b R a (r), R b , R b−1 − {r} where v 1 v 2 is a direct vector combination function as in traditional models, e.g. addition or component-wise multiplication."
1 We do not commit to a particular set of relations; see the discussion at the end of this section.
"If either R a (r) or R −1b (r) are not defined, the combination fails."
"Afterwards, the ar-gument position r is considered filled, and is deleted from R a and R b−1 ."
Figure 2 illustrates this procedure on the represen-tations from Figure 1.
The dotted lines indicate that the lexical vector for catch is combined with the in-verse object preference of ball.
"Likewise, the lexical vector for ball is combined with the object preference vector of catch."
Note that our procedure for computing meaning in context can be expressed within the framework of Mitchell and Lapata (Eq. (3)).
We can encode the expectations of a and b as additional knowledge K.
"The combined representation c is the pair (a 0 , b 0 ) that is computed according to our model (Eq. (4))."
"The SVS scheme we have proposed incorporates syntactic information in a more general manner than previous models, and thus addresses the issues we have discussed in Section 1."
"Since the representation retains individual selectional preferences for all rela-tions, combining the same words through different relations can (and will in general) result in different adapted representations."
"For instance, in the case of Example (2), we would expect the inverse subject preference of horse (“things that a horse typically does”) to push the lexical vector of draw into the di-rection of pulling, while its inverse object preference (“things that are done to horses”) suggest a different interpretation."
"Rather than yielding a single, joint vector for the whole expression, our procedure for computing mean-ing in context results in one context-adapted meaning representation per word, similar to the output of a WSD system."
"As a consequence, our model can be combined with any formalism representing the structure of an expression. (The formalism used then determines the set R of relations.)"
"For example, com-bining SVS with a dependency tree would yield a tree in which each node is labeled by a SVS tuple that represents the word’s meaning in context."
"This section provides the background to the following experimental evaluation of SVS , including parameters used for computing the SVS representations that will be used in the experiments."
"In this paper, we evaluate the SVS model against the task of predicting, given a predicate-argument pair, how appropriate a paraphrase (of either the predicate or the argument) is in that context."
"We perform two experiments that both use the paraphrase task, but differ in their emphasis."
Experiment 1 replicates an existing evaluation against human judgments.
"This evaluation uses synthetic dataset, limited to one par-ticular construction, and constructed to provide max-imally distinct paraphrase candidates."
Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness.
"In both experiments, we compare the SVS model against the state-of-the-art model[REF_CITE](henceforth M&amp;L; cf."
Sec. 2 for model details).
"In our parameterization of the vector space, we largely follow M&amp;L because their model has been rigorously evaluated and found to outper-form a range of other models."
"Our first space is a traditional “bag-of-words” vec-tor space (B OW ,[REF_CITE])."
"For each pair of a target word and context word, the B OW space records a function of their co-occurrence fre-quency within a surface window of size 10."
"The space is constructed from the British National Cor-pus (BNC), and uses the [Footnote_2],000 most frequent context words as dimensions."
"2 More specifically, we used the minimal context specification and plain weight function.[REF_CITE]."
"We also consider a “dependency-based” vector space (S YN ,[REF_CITE])."
"In this space, target and context words have to be linked by a “valid” dependency path in a dependency graph to count as co-occurring. 2"
This space was built from BNC de-pendency parses obtained from Minipar[REF_CITE].
"For both spaces, we used pre-experiments to com-pare two methods for the computation of vector com-ponents, namely raw co-occurrence counts, the stan-dard model, and the pointwise mutual information (PMI) definition employed by M&amp;L."
"We use a simple, knowledge-lean representation for selectional preferences inspired[REF_CITE], who models selectional preference through similarity to seen filler vectors v~ a : We compute the selectional preference vector for word b and relation r as the weighted centroid of seen filler vectors ~v a ."
We collect seen fillers from the Minipar-parse of the BNC.
"Let f(a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC, then"
"X R b (r) SELPREF = f(a, r, b) · v~ a (5) a:f(a,r,b)&gt;0"
We call this base model SELPREF .
"We will also study two variants of SELPREF , based on two dif-ferent hypotheses about what properties of the se-lectional preferences are particularly important for meaning adaption."
"The first model aims specifically at alleviating noise introduced by infrequent fillers, a common problem in data-driven approaches."
It only uses fillers seen more often than a threshold θ.
We call this model SELPREF - CUT :
"X R b (r) SELPREF - CUT = f(a, r, b) · ~v a (6) a:f(a,r,b)&gt;θ"
"Our second variant again aims at alleviating noise, but noise introduced by low-valued dimensions rather than infrequent fillers."
It achieves this by taking each component of the selectional preference vector to the nth power.
"In this manner, dimensions with high counts are further inflated, while dimensions with low counts are depressed. [Footnote_3] This model, SELPREF - POW , is defined as follows: If R b (r) SELPREF = hv 1 , . . . , v m i,"
"3 Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization."
"R b (r) SELPREF - POW = hv n1 , . . . , v nm i (7)"
The inverse selectional preferences R −1b are de-fined analogously for all three model variants.
"We instantiate the vector combination function as component-wise multiplication, following M&amp;L."
Baselines and significance testing.
All tasks that we consider below involve judgments for the mean-ing of a word a in the context of a word b.
A first baseline that every model must beat is simply using the original vector for a.
We call this baseline “target only”.
"Since we assume that the selectional prefer-ences of b model the expectations for a, we use b’s selectional preference vector for the given relation as a second baseline, “selpref only”."
Differences between the performance of mod-els were tested for significance using a stratified shuffling-based randomization test[REF_CITE]. [Footnote_4] .
4 The software is available[URL_CITE]∼ sebastian/sigf.html.
"In our first experiment, we attempt to predict human similarity judgments."
This experiment is a replication of the evaluation of M&amp;L on their dataset [Footnote_5] .
5 We thank J. Mitchell and M. Lapata for providing their data.
"The M&amp;L dataset comprises a total of 3,600 human similarity judgements for 120 experi-mental items."
"Each item, as shown in Figure 3, con-sists of an intransitive verb and a subject noun that are combined with a “landmark”, a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject."
The dataset was constructed by extracting pairs of subjects and intransitive verbs from a parsed ver-sion of the BNC.
"Each item was paired with two landmarks, chosen to be as dissimilar as possible ac-cording to a WordNet similarity measure."
"All nouns and verbs were subjected to a pretest, where only those with highly significant variations in human judgments across landmarks were retained."
"For each item of the final dataset, judgements on a 7-point scale were elicited."
"For example, judges considered the compatible landmark “slouch” to be much more similar to “shoulder slumps” than the incompatible landmark “decline”."
"In Figure 3, the column sim shows whether the experiment designers considered the respective landmark to have high or low similarity to the verb, and the column judgment shows a participant’s judgments."
"We used cosine to com-pute similarity to the lexical vector of the landmark. “Target only” compares the landmark against the lexi-cal vector of the verb, and “selpref only” compares it to the noun’s subj −1 preference."
"For the M&amp;L model, the comparison is to the combined lexical vectors of verb and noun."
"For our models SELPREF , SELPREF - CUT and SELPREF - POW , we combine the verb’s lexical vector with the subj −1 preference of the noun."
We used a held-out dataset of 10% of the data to optimize the parameters of θ of SELPREF - CUT and n of SELPREF - POW .
"Vectors with PMI compo-nents could model the data, while raw frequency components could not; we report only the former."
We use the same two evaluation scores as M&amp;L:
The first score is the average similarity to compatible landmarks (high) and incompatible landmarks (low).
"The second is Spearman’s ρ, a nonparametric corre-lation coefficient."
We compute ρ between individual human similarity scores and our predictions.
"Based on agreement between human judges, M&amp;L estimate an upper bound ρ of 0.4 for the dataset."
Results and discussion.
Table 1 shows the results of Exp. 1 on the test set.
"In the upper half (B OW ), we replicate M&amp;L’s main finding that simple component-wise multiplication of the predicate and argument vectors results in a highly significant correlation of ρ = 0.2, significantly outperforming both baselines."
"It is interesting, though, that the subj −1 preference itself (“Selpref only”) is already highly significantly correlated with the human judgments."
A comparison of the upper half (B OW ) with the lower half (S YN ) shows that the dependency-based space generally shows better correlation with human judgements.
This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces[REF_CITE].
All instances of the SELPREF model show highly significant correlations.
SELPREF and SELPREF - CUT show very similar performance.
"They do better than both baselines in the B OW space; however, in the cleaner S YN space, their performance is numerically lower than using selectional preferences only (ρ = 0.13 vs. 0.16)."
"SELPREF - POW is always significantly better than SELPREF and SELPREF - CUT , and shows the best result of all tested models (ρ = 0.27, B OW space)."
The performance is somewhat lower in the S YN space (ρ = 0.22).
"However, this difference, and the difference to the best M&amp;L model at ρ = 0.24, are not statistically significant."
The SVS model computes meaning in context by combining a word’s lexical representation with the preference vector of its context.
"In this, it differs from previous models, including that by M&amp;L, which used what we have been calling “direct combination”."
So it is important to ask to what extent this difference in method translate to a difference in predictions.
"We analyzed this by measuring the similarity by the nouns’ lexical vectors, used by direct combination methods, and their inverse subject preferences, which SVS uses."
"The result is shown in the first column in Table 2, computed as mean cosine similarities and standard deviations between noun vectors and selectional preferences."
"The table shows that these vectors have generally low similarity, which is further reduced by applying cutoff and potentiation."
"Thus, the predictions of SVS will differ from those of direct combination models like M&amp;L."
A related question is whether syntax-aware vec-tor combination makes a difference: Does the model encode different expectations for different syntactic relations (cf. Example 2)?
The second column of Ta-ble 2 explores this question by comparing inverse se-lectional preferences for the subject and object slots.
"We observe that the similarity is very high for raw preferences, but becomes lower when noise is elim-inated."
"Since the SELPREF - POW model performed best in our evaluation, we read this as evidence that potentiation helps to suppress noise introduced by mis-identified subject and object fillers."
"In Experiment 1, all experimental items were verbs, which means that all disambiguation was done through inverse selectional preferences."
"As inverse selectional preferences are currently largely unex-plored, it is interesting to note that the evidence that they provide for the paraphrase task is as strong as that of the context nouns themselves."
"This section reports on a second, more NLP-oriented experiment whose task is to distinguish between ap-propriate and inappropriate paraphrases on a broader range of constructions."
"For this experiment, we use the SemEval-1 lexical substitution (lexsub) dataset[REF_CITE], which contains 10 instances each of 200 target words in sentential contexts, drawn from Sharoff’s (2006) English Internet Corpus."
Contex-tually appropriate paraphrases for each instance of each target word were elicited from up to 6 partic-ipants.
Fig. 4 shows two instances for the verb to work.
The distribution over paraphrases can be seen as a characterization of the target word’s meaning in each context.
"In this paper, we pre-dict appropriate paraphrases solely on the basis of a single context word that stands in a direct predicate-argument relation to the target word."
We extracted all instances from the lexsub test data with such a relation.
"After parsing all sentences with verbal and nominal targets with Minipar, this resulted in three sets of sentences: (a), target intransitive verbs with noun subjects ( V - SUBJ , 48 sentences); (b), target tran-sitive verbs with noun objects ( V - OBJ , 213 sent.); and (c), target nouns occurring as objects of verbs ( N - OBJ , 102 sent.). [Footnote_6] Note that since we use only part of the lexical substitution dataset in this experiment, a di-rect comparison with results from the SemEval task is not possible."
6 The specification of this dataset will be made available.
"As in the original SemEval task, we phrase the task as a ranking problem."
"For each target word, the paraphrases given for all 10 instances are pooled."
"The task is to rank the list for each item so that appropriate paraphrases (such as “be employed” for # 2002) rank higher than paraphrases not given (e.g., “toil”)."
"Our model ranks paraphrases by their similarity to the following combinations (Eq. (4)): for V - SUBJ , verb plus the noun’s subj −1 preferences; for V - OBJ , verb plus the noun’s obj −1 preferences; and for N - OBJ , the noun plus the verb’s obj preferences."
"Our comparison model, M&amp;L, ranks all paraphrases by their similarity to the direct noun-verb combination."
"To avoid overfitting, we consider only the two mod-els that performed optimally in in the S YN space in Experiment 1 ( SELPREF - POW with n=30 and M&amp;L)."
"However, since we found that vectors with raw fre-quency components could model the data, while PMI components could not, we only report the former."
"For evaluation, we adopt the SemEval “out of ten” precision metric P OOT ."
It uses the model’s ten top-ranked paraphrases as its guesses for appropri-ate paraphrases.
"Let G i be the gold paraphrases for item i, M i the model’s top ten paraphrases for i, and f(s, i) the frequency of s as paraphrase for i:"
"P f(s, i) P OOT = 1/|I| X P s∈M i ∩G i (8) i s∈G i f(s, i)"
McCarthy and Navigli propose this metric for the dataset for robustness.
"Due to the sparsity of para-phrases, a metric that considers fewer guesses leads to artificially low results when a “good” paraphrase was not mentioned by the annotators by chance but is ranked highly by a model."
Results and discussion.
Table 6 shows the mean out-of-ten precision for all models.
The behavior is fairly uniform across all three datasets.
"Unsurpris-ingly, “target only”, which uses the same ranking for all instances of a target, yields the worst results. [Footnote_7]"
"7 “Target only” still does very much better than a random baseline, which performs at 22% P OOT ."
M&amp;L’s direct combination model outperforms “tar-get only” significantly (p &lt; 0.05).
"However, on both the V - SUBJ and the N - OBJ the “selpref only” baseline does better than direct combination."
The best results on all datasets are obtained by SELPREF - POW .
The difference between SELPREF - POW and the “target only” baseline is highly significant (p &lt; 0.01).
The difference to M&amp;L’s model is significant at p = 0.05.
We interpret these results as encouraging evidence for the usefulness of selectional preferences for judg-ing substitutability in context.
Knowledge about the selectional preferences of a single context word can already lead to a significant improvement in precision.
We find this overall effect even though the word is not informative in all cases.
"For instance, the subject of item 2002 in Fig. 4, “who”, presumably helps little in determining the verb’s context-adapted meaning."
It is interesting that the improvement of SELPREF - POW over “selpref only” is smallest for the N - OBJ dataset (1.9% P OOT ).
"N - OBJ uses selectional prefer-ences for nouns that may fill the direct object position, , while V - SUBJ and V - OBJ use inverse selectional preferences for verbs (cf. the two graphs in Fig. 1)."
"In this paper, we have considered semantic space models that can account for the meaning of word occurrences in context."
"Arguing that existing models do not sufficiently take syntax into account, we have introduced the new structured vector space ( SVS ) model of word meaning."
"In addition to a vector rep-resenting a word’s lexical meaning, it contains vec-tors representing the word’s selectional preferences."
These selectional preferences play a central role in the computation of meaning in context.
We have evaluated the SVS model on two datasets on the task of predicting the felicitousness of para-phrases in given contexts.
"On the M&amp;L dataset, SVS outperforms the state-of-the-art model of M&amp;L, though the difference is not significant."
"On the Lex-ical Substitution dataset, SVS significantly outper-forms the state-of-the-art."
"This is especially interest-ing as the Lexical Substitution dataset, in contrast to the M&amp;L data, uses “realistic” paraphrase candidates that are not necessarily maximally distinct."
The most important limitation of the evaluation that we have given in this paper is that we have only considered single words as context.
Our next step will be to integrate information from multiple rela-tions (such as both the subject and object positions of a verb) into the computation of context-specific meaning.
"Our eventual aim is a model that can give a compositional account of a word’s meaning in con-text, where all words in an expression disambiguate one another according to the relations between them."
"We will explore the usability of vector space mod-els of word meaning in NLP applications, formulated as the question of how to perform inferences on them in the context of the Textual Entailment task[REF_CITE]."
"Paraphrase-based inference rules play a large role in several recent approaches to Textual Entailment (e.g.[REF_CITE]); appropriate-ness judgments of paraphrases in context, the task of Experiments 1 and 2 above, can be viewed as testing the applicability of these inferences rules."
"Many thanks for helpful dis-cussion to Jason Baldridge, David Beaver, Dedre Gentner, James Hampton, Dan Jurafsky, Alexander Koller, Brad Love, and Ray Mooney."
"We consider a parsed text corpus as an in-stance of a labelled directed graph, where nodes represent words and weighted directed edges represent the syntactic relations be-tween them."
"We show that graph walks, com-bined with existing techniques of supervised learning, can be used to derive a task-specific word similarity measure in this graph."
"We also propose a new path-constrained graph walk method, in which the graph walk process is guided by high-level knowledge about mean-ingful edge sequences (paths)."
Empirical eval-uation on the task of named entity coordinate term extraction shows that this framework is preferable to vector-based models for small-sized corpora.
It is also shown that the path-constrained graph walk algorithm yields both performance and scalability gains.
Graph-based similarity measures have been used for a variety of language processing applications.
"In this paper we assume directed graphs, where typed nodes denote entities and labelled directed and weighted edges denote the relations between them."
"In this framework, graph walks can be ap-plied to draw a measure of similarity between the graph nodes."
"Previous works have applied graph walks to draw a notion of semantic similarity over such graphs that were carefully designed and man-ually tuned, based on WordNet reations[REF_CITE]."
"While these and other researchers have used WordNet to evaluate similarity between words, there has been much interest in extracting such a measure from text corpora (e.g.,[REF_CITE])."
"In this paper, we suggest pro-cessing dependency parse trees within the general framework of directed labelled graphs."
We construct a graph that directly represents a corpus of struc-tured (parsed) text.
"In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them."
We apply graph walks to derive an inter-word similarity mea-sure.
"We further apply learning techniques, adapted to this framework, to improve the derived corpus-based similarity measure."
"The learning methods applied include existing learning techniques, namely edge weight tuning, where weights are associated with the edge types, and discriminative reranking of graph nodes, using features that describe the possible paths between a graph node and the initial “query nodes”[REF_CITE]."
"In addition, we outline in this paper a novel method for learning path-constrained graph walks."
"While reranking allows use of high-level features that describe properties of the traversed paths, the suggested algorithm incorporates this information in the graph walk process."
"More specifically, we allow the probability flow in the graph to be conditioned on the history of the walk."
We show that this method results in improved performance as it directs proba-bility flow to meaningful paths.
"In addition, it leads to substantial gains in terms of runtime performance."
"The graph representation and the set of learning techniques suggested are empirically evaluated on the task of coordinate term extraction 1 from small to moderately sized corpora, where we compare them against vector-based models, including a state-of-the-art syntactic distributional similarity method[REF_CITE]."
"It is shown that the graph walk based approach gives preferable results for the smaller datasets (and comparable otherwise), where learning yields significant gains in accuracy."
There are several contributions of this paper.
"First, we represent dependency-parsed corpora within a general graph walk framework, and derive inter-word similarity measures using graph walks and learning techniques available in this framework."
"To our knowledge, the application of graph walks to parsed text in general, and to the extraction of coor-dinate terms in particular, is novel."
"Another main contribution of this paper is the path-constrained graph walk variant, which is a general learning tech-nique for calculating the similarity between graph nodes in directed and labelled graphs."
"Below we first outline our proposed scheme for representing a dependency-parsed text corpus as a graph, and provide some intuitions about the asso-ciated similarity metric (Section 2)."
"We then give an overview of the graph-walk based similarity met-ric (Section 3), as well as the known edge weight tuning and reranking learning techniques (Section 4)."
We next present the proposed algorithm of path-constrained graph walks (Section 5).
"The paper pro-ceeds with a review of related work (Section 6), a discussion of the coordinate term extraction task, empirical evaluation and our conclusions (Sections 7-9)."
"A typed dependency parse tree consists of directed links between words, where dependencies are la-belled with the relevant grammatical relation (e.g., nominal subject, indirect object etc.)."
"We suggest representing a text corpus as a connected graph of dependency structures, according to the scheme shown in Figure 1."
"The graph shown in the figure includes the dependency analysis of two sentences: “boys like playing with all kinds of cars”, and “girls like playing with dolls”."
"In the graph, each word mention is represented as a node, which includes the index of the sentence in which it appears, as well as its position within the sentence."
Word mentions are marked as circles in the figure.
The “type” of each word – henceforth a term node – is denoted by a square in the figure.
"Each word mention is linked to the corresponding term; for example, the nodes “like 1 ” and “like 2 ” represent distinct word mentions and both nodes are linked to the term “like”."
"For every edge in the graph, we add another edge in the opposite direction (not shown in the figure); for ex-ample, an inverse edge exists from “like 1 ” to “girls 1 ” with an edge labelled as “nsubj-inv”."
The resulting graph is highly interconnected and cyclic.
"We will apply graph walks to derive an extended measure of similarity, or relatedness, between word terms (as defined above)."
"For example, starting from the term “girls”, we will reach the semantically re-lated term “boys” via the following two paths: nsubj (1) girls −→ girls1 −→ like1 as−term −→ like mention −→ mention nsubj−inverse as−term like2 −→ boys2 −→ boys mention nsubj partmod (2) girls −→ girls1 −→ like1 −→ playing[Footnote_1] as−term −→ playing mention −→ playing2 partmod−inverse −→ like2 nsubj−inverse as−term −→ boys2 −→ boys ."
"1 In particular, we focus on the extraction of named entity classes."
"Intuitively, in a graph representing a large cor-pus, terms that are more semantically related will be linked by a larger number of connecting paths."
"In addition, shorter connecting paths may be in general more meaningful."
In the next section we show that the graph walk paradigm addresses both of these re-quirements.
"Further, different edge types, as well as the paths traversed, are expected to have varying im-portance in different types of word similarity (for ex-ample, verbs and nouns are associated with different connectivity patterns)."
These issues are addressed using learning.
This section provides a quick overview of the graph walk induced similarity measure.
"For details, the reader is referred to previous publications (e.g.,[REF_CITE])."
"In summary, similarity between two nodes in the graph is defined by a weighted graph walk process, where an edge of type ` is assigned an edge weight, θ ` , determined by its type. [Footnote_2]"
"2 In this paper, we consider either uniform edge weights; or, learn the set of weights Θ from examples."
"The transition proba-bility of reaching node y from node x over a single time step, Pr(x −→ y), is defined as the weight of their connecting edge, θ l , normalized by the to-tal outgoing weight from x."
"Given these transition probabilities, and starting from an initial distribu-tion V q of interest (a query), we perform a graph walk for a finite number of steps K. Further, at each step of the walk, a proportion γ of the probability mass at every node is emitted."
"Thus, this model ap-plies exponential decay on path length."
"The final probability distribution of this walk over the graph nodes, which we denote as R, is computed as fol-lows: R = P Ki γ i V q M i , where M is the transi- =1 tion matrix. [Footnote_3] The answer to a query, V q , is a list of nodes, ranked by the scores in the final distribution R."
"3 We tune K empirically and set γ = 0.5, as[REF_CITE]."
"In this multi-step walk, nodes that are reached from the query nodes by many shorter paths will be assigned a higher score than nodes connected over fewer longer paths."
"We consider a supervised setting, where we are given a dataset of example queries and labels over the graph nodes, indicating which nodes are relevant to which query."
"For completeness, we describe here two methods previously described by Minkov and"
Cohen[REF_CITE]: a hill-climbing method that tunes the graph weights; and a reranking method.
We also specify the feature set to be used by the reranking method in the domain of parsed text.
There are several motivations for learning the graph weights Θ in this domain.
"First, some dependency relations – foremost, subject and object – are in gen-eral more salient than others[REF_CITE]."
"In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity[REF_CITE])."
"Weight tuning allows the adaption of edge weights to each task (i.e., distribu-tion of queries)."
The weight tuning method implemented in this work is based on an error backpropagation hill climbing algorithm[REF_CITE].
The al-gorithm minimizes the following cost function:
"E = 1 X e z = 1 X 1(p z − p Optz ) 2 N z∈N N z∈N [Footnote_2] where e z is the error for a target node z defined as the squared difference between the final score assigned to z by the graph walk, p z , and some ideal score ac-cording to the example’s labels, p Optz . [Footnote_4] Specifically, p Optz is set to 1 in case that the node z is relevant or 0 otherwise."
"2 In this paper, we consider either uniform edge weights; or, learn the set of weights Θ from examples."
"4 For every example query, a handful of the retrieved nodes are considered, including both relevant and irrelevant nodes."
"The error is averaged over a set of example instantiations of size N. The cost function is minimized by gradient descent where the deriva-tive of the error with respect to an edge weight θ ` is derived by decomposing the walk into single time steps, and considering the contribution of each node traversed to the final node score."
Reranking of the top candidates in a ranked list has been successfully applied to multiple NLP tasks[REF_CITE].
"In essence, discriminative reranking allows the re-ordering of results obtained by methods that perform some form of local search, using features that encode higher level information."
"A number of features describing the set of paths from V q can be conveniently computed in the pro-cess of executing the graph walk, and it has been shown that reranking using these features can im-prove results significantly."
"It has also been shown that reranking is complementary to weight tuning[REF_CITE], in the sense that the two techniques can be usefully combined by tuning weights, and then reranking the results."
"In the reranking approach, for every training ex-ample i (1 ≤ i ≤ N), the reranking algorithm is provided with the corresponding output ranked list of l i nodes."
"Let z ij be the output node ranked at rank j in l i , and let p z ij be the probability assigned to z ij by the graph walk."
"Each output node z ij is repre-sented through m features, which are computed by pre-defined feature functions f 1 , . . . , f m ."
The rank-ing function for node z ij is defined as:
"F (z ij , ᾱ) = α 0 log(p z ij ) +"
X m α k f k (z ij ) k=1 where ᾱ is a vector of real-valued parameters.
"Given a new test example, the output of the model is the output node list reranked by F (z ij , ᾱ)."
"To learn the parameter weights ᾱ, we here applied a boosting method[REF_CITE](see also[REF_CITE])."
We evaluate the following feature templates.
"Edge label sequence features indicate whether a par-ticular sequence of edge labels ` i occurred, in a particular order, within the set of paths leading to the target node z ij ."
Lexical unigram feature indi-cate whether a word mention whose lexical value is t k was traversed in the set of paths leading to z ij .
"Finally, the Source-count feature indicates the number of different source query nodes that z ij was reached from."
"The intuition behind this last fea-ture is that nodes linked to multiple query nodes, where applicable, are more relevant."
"For exam-ple, for the query term “girl” in the graph depicted in Figure 1, the target node “boys” is described by the features (denoted as feature-name.feature-value): sequence.nsubj.nsubj-inv (where mention and as-term edges are omitted) , lexical. ’“like” etc."
"In this work, the features encoded are binary."
"However, features can be assugned numeric weights that corresponds to the probability of the indicator being true for any path between x and z ij[REF_CITE]."
"While node reranking allows the incorporation of high-level features that describe the traversed paths, it is desirable to incorporate such information ear-lier in the graph walk process."
"In this paper, we suggest a variant of a graph-walk, which is con-strained by path information."
"Assume that prelim-inary knowledge is available that indicates the prob-ability of reaching a relevant node after following a particular edge type sequence (path) from the query distribution V q to some node x. Rather than fix the edge weights Θ, we can evaluate the weights of the outgoing edges from node x dynamically, given the history of the walk (the path) up to this node."
"This should result in gains in accuracy, as paths that lead mostly to irrelevant nodes can be eliminated in the graph walk process."
"In addition, scalability gains are expected, for the same reason."
"We suggest a path-constrained graph walk algo-rithm, where path information is maintained in a compact path-tree structure constructed based on training examples."
Each vertex in the path tree de-notes a particular walk history.
"In applying the graph walk, the nodes traversed are represented as a set of node pairs, comprised of the graph node and the cor-responding vertices in the path tree."
The outgoing edge weights from each node pair will be estimated according to the respective vertex in the path tree.
This approach needs to address two subtasks: learn-ing of the path-tree; and updating of the graph walk paradigm to co-sample from the graph and the path tree.
We next describe these two components in de-tail.
Graph walks over typed graphs have been applied to derive semantic similarity for NLP problems us-ing WordNet as a primary information source.
"For instance,[REF_CITE]constructed a graph which represented various types of word re-lations from WordNet, and compared random-walk similarity to similarity assessments from human-subject trials."
Random-walk similarity has also been used for lexical smoothing for prepositional word attachment[REF_CITE]and query ex-pansi[REF_CITE].
"In contrast to these works, our graph representation de-scribes parsed text and has not been (consciously) engineered for a particular task."
"Instead, we in-clude learning techniques to optimize the graph-walk based similarity measure."
The learning meth-ods described in this paper can be readily applied to other directed and labelled entity-relation graphs. 7
"The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similar-ity from statistics associated with a parsed corpus[REF_CITE]."
"In most cases, these models construct vectors to represent each word w i , where each element in the vector for w i corresponds to particular “context” c, and represents a count or an indication of whether w i occurred in context c."
"A “context” can refer to simple co-occurrence with another word w j , to a particular syntactic relation to another word (e.g., a relation of “direct object” to w j ), etc."
"Given these word vectors, inter-word similarity is evaluated us-ing some appropriate similarity measure for the vec-tor space, such as cosine vector similarity, or Lin’s similarity[REF_CITE]."
"Recently, Padó and Lapata[REF_CITE]have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths."
"They considered two different weighting schemes: a length weight-ing scheme, assigning lower weight to longer con-necting paths; and an obliqueness weighting hierar-chy[REF_CITE], assigning higher weight to paths that include grammatically salient relations."
"In an evaluation of word pair similar-ity based on statistics from a corpus of about 100 million words, they show improvements over sev-eral previous vector space models."
Below we will compare our framework to that of Padó and Lap-ata.
"One important difference is that while Padó and Lapata make manual choices (regarding the set of paths considered and the weighting scheme), we ap-ply learning to adjust the analogous parameters."
We evaluate the text representation schema and the proposed set of graph-based similarity measures on the task of coordinate term extraction.
"In particular, we evaluate the extraction of named entities, includ-ing city names and person names from newswire data, using word similarity measures."
"Coordinate terms reflect a particular type of word similarity (relatedness), and are therefore an appropriate test case for our framework."
"While coordinate term ex-traction is often addressed by a rule-based (tem-plates) approach[REF_CITE], this approach was designed for very large corpora such as the Web, where the availability of many redundant documents allows use of high-precision and low-recall rules."
In this paper we focus on relatively small corpora.
"Small limited text collections may correspond to documents residing on a personal desktop, email collections, discussion groups and other specialized sets of documents."
The task defined in the experiments is to retrieve a ranked list of city or person names given a small set of seeds.
"This task is implemented in the graph as a query, where we let the query distribution V q be uniform over the given seeds (and zero elsewhere)."
"Ideally, the resulting ranked list will be populated with many additional city, or person, names."
"We compare graph walks to dependency vec-tors (DV)[REF_CITE], [Footnote_8] as well as to a vector-based bag-of-words co-occurrence model."
8 We used the code[URL_CITE]and converted the underlying syntactic patterns to the Stanford dependency parser conven-tions.
DV is a state-of-the-art syntactic vector-based model (see Section 6).
"The co-occurrence model represents a more traditional approach, where text is processed as a stream rather than syntactic structures."
"In ap-plying the vector-space based methods, we compute a similarity score between every candidate from the corpus and each of the query terms, and then aver-age these scores (as the query distributions are uni-form) to construct a ranked list."
"For efficiency, in the vector-based models we limit the considered set of candidates to named entities."
"Similarly, the graph walk results are filtered to include named entities. [Footnote_9]"
"9 In general, graph walk results can be filtered by various word properties, e.g., capitalization pattern, or part-of-speech."
"As the experimental corpora, we use the training set portion of the MUC-6 dataset[REF_CITE]as well as articles from the Associated Press (AP) extracted from the AQUAINT corpus[REF_CITE], all parsed using the Stanford depen-dency parser (de[REF_CITE]). 10"
"The MUC corpus provides true named entity tags, while the AQUAINT corpus includes automatically generated, noisy, named entity tags."
Statistics on the experi-mental corpora and their corresponding graph rep-resentation are detailed in Table 1.
"As shown, the MUC corpus contains about 140 thousand words, whereas the MUC+AP experimental corpus is sub-stantially larger, containing about 2.5 million words."
"Similarly, we generated a set of [Footnote_10] queries that include 4 per-son names selected randomly from the MUC corpus. (The MUC corpus was appended to AP, so that the same query sets are applicable in both cases.)"
10[URL_CITE]sen-tences longer than 70 words omitted.
"For each task, we use 5 queries for training and tuning and the remaining queries for testing."
We evaluated cross-validation performance over the training queries in terms of mean average precision for varying walk lengths K. We found that beyond K = 6 improvements were small (and in fact deteriorated for K = 9).
We there-fore set K = 6.
Weight tuning was trained using the training queries and two dozens of target nodes overall.
"In reranking, we set a feature count cutoff of 3, in order to avoid over-fitting."
Reranking was applied to the top 200 ranked nodes output by the graph walk using the tuned edge weights.
"Finally, path-trees were constructed using the top 20 correct nodes and 20 incorrect nodes retrieved by the uni-formly weighted graph walk."
"In the experiments, we apply a threshold of 0.5 to the path constrained graph walk method."
"We note that for learning, true labels were used for the fully annotated MUC corpus (we hand labelled all of the named entities of type location in the cor-pus as to whether they were city names)."
"However, noisy negative examples were considered for the larger automatically annotated AP corpus. (Specif-ically, for cities, we only considered city names in-cluded in the MUC corpus as correct answers.)"
A co-occurrence vector-space model was applied using a window of two tokens to the right and to the left of the focus word.
"Inter-word similarity was evaluated in this model using cosine similar-ity, where the underlying co-occurrence counts were normalized by log-likelihood ratio[REF_CITE]."
The parameters of the DV method were set based on a cross validation evaluation (using the MUC+AP corpus).
The medium set of dependency paths and the oblique edge weighting scheme were found to perform best.
We experimented with co-sine as well as Lin similarity measure in combina-tion with the dependency vectors representation.
"Fi-nally, given the large number of candidates in the MUC+AP corpus (Table 1), we show the results of applying the considered vector-space models to the top, high-quality, entities retrieved with reranking for this corpus. [Footnote_11]"
11 We process the union of the top 200 results per each query.
Test set results.
Figure 4 gives results for the city name (top) and the person name (bottom) extraction tasks.
"The left part of the figure shows results us-ing the MUC corpus, and its right part – using the MUC+AP corpus."
"The curves show precision as a function of rank in the ranked list, up to rank 100. (For this evaluation, we hand-labeled all the top-ranked results as to whether they are city names or person names.)"
"Included in the figure are the curves of the graph-walk method with uniform weights (G:Uw), learned weights (G:Lw), graph-walk with reranking (Rerank) and a path-constrained graph-walk (PCW)."
"Also given are the results of the co-occurrence model (CO), and the syntactic vector-space DV model, using the Lin similarity measure (DV:Lin)."
"Performance of the DV model using co-sine similarity was found comparable or inferior to using the Lin measure, and is omitted from the fig-ure for clarity."
Several trends can be observed from the results.
"With respect to the graph walk methods, the graph walk using the learned edge weights consistently outperforms the graph walk with uniform weights."
"Reranking and the path-constrained graph walk, however, yield superior results."
"Both of these learn-ing methods utilize a richer set of features than the graph walk and weight tuning, which can consider only local information."
"In particular, while the graph walk paradigm assigns lower importance to longer connecting paths (as described in Section 3), rerank-ing and the path-constrained walker allow to dis-card short yet irrelevant paths, and by that eliminate noise at the top ranks of the retrieved list."
"In gen-eral, the results show that edge sequences carry ad-ditional meaning compared with the individual edge label segments traversed."
"Out of the vector-based models, the co-occurrence model is preferable for the city name extraction task, and the syntactic dependency vec-tors model gives substantially better performance for person name extraction."
We conjecture that city name mentions are less structured in the underlying text.
"In addition, the syntactic weighting scheme of the DV model is probably not optimal for the case of city names."
"For example, a conjunction relation was found highly indicative for city names (see below)."
"However, this relation is not emphasized by the DV weighting schema."
"As expected, the performance of the vector-based models improves for larger corpora[REF_CITE]."
"These models demonstrate good performance for the larger MUC+AP corpus, but only mediocre performance for the smaller MUC corpus."
"Contrasting the graph-based methods with the vector-based models, the difference in performance in favor of reranking and PCW, especially for the smaller corpus, can be attributed to two factors."
"The first factor is learning, which optimizes performance for the underlying data."
"A second factor is the incor-poration of non-local information, encoding proper-ties of the traversed paths."
Following is a short description of the models learned by the different methods and tasks.
"Weight tuning assigned high weights to edge types such as conj-and, prep-in and prep-from, nn, ap-pos and amod for the city extraction task."
"For per- son extraction, prominent edge types included subj, obj, poss and nn. (The latter preferences are sim-ilar to the linguistically motivated weights of DV.)"
"High weight features assigned by reranking for city name extraction included, for example, lexical fea-tures such as “based” and “downtown”, and edge bi-grams such as “prep-in-Inverse→conj-and” or “nn- Inverse→nn”."
"Positive highly predictive paths in the constructed path tree included many symmetric paths, such as ...→conj andInverse...→.conj and..., ...→prep inInverse...→.prep in..., for the city name extraction task."
Figure 5 shows the number of graph nodes maintained in each step of the graph walk (logarithm scale) for a typical city extraction query and the MUC+AP corpus.
"As shown by the solid line, the number of graph nodes visited using the weighted graph walk paradigm grows exponentially with the length of the walk."
Applying a path-constrained walk with a threshold of 0 (PCW:0) re-duces the maximal number of nodes expanded (as paths not observed in the training set are discarded).
"As shown, increasing the threshold leads to signifi-cant gains in scalability."
"Overall, query processing time averaged at a few minutes, using a commodity PC."
In this paper we make several contributions.
"First, we have explored a novel but natural representation for a corpus of dependency-parsed text, as a labelled directed graph."
"We have evaluated the task of coor-dinate term extraction using this representation, and shown that this task can be performed using similar-ity queries in a general-purpose graph-walk based query language."
"Further, we have successfully ap-plied learning techniques that tune weights assigned to different dependency relations, and re-score can-didates using features derived from the graph walk."
"Another orthogonal contribution of this paper is a path-constrained graph walk variant, where the graph walk is guided by high level knowledge about meaningful paths, learned from training examples."
"This method was shown to yield improved perfor-mance for the suggested graph representation, and improved scalability compared with the local graph walk."
"The method is general, and can be readily ap-plied in similar settings."
"Empirical evaluation of the coordinate term ex-traction task shows that the graph-based framework performs better than vector-space models for the smaller corpus, and comparably otherwise."
"Over-all, we find that the suggested model is suitable for deep (syntactic) processing of small specialized cor-pora."
"In preliminary experiments where we evalu-ated this framework on the task of extracting general word synonyms, using a relatively large corpus of 15 million words, we found the graph-walk perfor-mance to be better than DV using cosine similarity measures, but second to DV using Lin’s similarity measure."
"While this set of results is incomplete, we find that it is consistent with the results reported in this paper."
The framework presented can be enhanced in sev-eral ways.
"For instance, WordNet edges and mor-phology relations can be readily encoded in the graph."
"We believe that this framework can be ap-plied for the extraction of more specialized no-tions of word relatedness, as in relation extracti[REF_CITE]."
"The path-constrained graph walk method proposed may be enhanced by learning edge probabilities, using a rich set of fea-tures."
We are also interested in exploring a possi-ble relation between the path-constrained walk ap-proach and reinforcement learning.
This paper presents a graph-theoretic model of the acquisition of lexical syntactic representa-tions.
The representations the model learns are non-categorical or graded.
We propose a new evaluation methodology of syntactic ac-quisition in the framework of exemplar theory.
"When applied to the CHILDES corpus, the evaluation shows that the model’s graded syn-tactic representations perform better than pre-viously proposed categorical representations."
"In recent years, exemplar theory has had great ex-planatory success in phonetics."
"Exemplar theory posits that linguistic production and perception are not mediated via abstract categories, but that instead each production and perception of a linguistic unit is stored and retained."
Linguistic inference then di-rectly operates on these stored exemplars.
"In this pa-per, we propose a new approach to lexical syntactic acquisition in the framework of exemplar theory."
Our approach uses an evaluation measure that is different from previous work.
Lexical syntac-tic acquisition is most often evaluated with respect to standard syntactic categories like verb and noun.
Our first contribution in this paper is that we instead evaluate learned representations in the context of a syntactic task.
This task is the determination of an aspect of grammaticality that we call local syntactic coherence.
Our second contribution is a graph-theoretic model of the acquisition of lexical syntactic rep-resentations that is more rigorous than previous heuristic proposals.
The graph-theoretic model can learn both categorical and non-categorical (or graded) representations.
"The model is also a unified framework for syntagmatic and paradigmatic rela-tions (as will be discussed below), and for lower- order syntactic relations (those that can be directly observed from the input) and higher-order syntac-tic relations (those that require some generalization from what is directly observable)."
"Our third contribu-tion is to show that, in the context of acquisition, graded representations are superior to standard cat-egorical representations in supporting judgments of local syntactic coherence."
"A graded representation formalism is one that, for any two words, can rep-resent a third word whose syntactic properties are intermediate between the two words[REF_CITE]."
Clearly exemplar theory is not the only frame-work in which lexical acquisition has been explored.
Our argument for the importance of distributional evidence does not call into question the large body of work in child language acquisition that demonstrates that “part of the capacity to learn languages must be ’innate’ ”[REF_CITE].
Tabula rasa learning is not possible.
Our goal is not to show that language acquisition pro-ceeds with a minimum of inductive bias.
"Rather, we attempt to formalize one aspect of language acquisi-tion, the use of distributional information."
The paper is organized as follows.
Section 2 moti-vates the exemplar-theoretic approach by reviewing its success in phonetics.
"Section 3 defines local syn-tactic coherence, which is the basis for a new evalu-ation methodology for the acquisition of lexical rep-resentations."
Section 4 develops the graph-theoretic model.
Section 5 compares graded and categorical representations for the task of inferring local syn- tactic coherence.
Section 6 presents our evaluation.
"Sections 7 and 8 discuss related and future work, and present our conclusions."
"The general idea of research into exemplars in speech production and perception is that encoun-tered items (segments, words, sentences etc.) are stored in great detail in memory along with rich linguistic and extra-linguistic context information."
These exemplars are organized into clouds of mem-ory traces with similar traces lying close to each other while dissimilar traces are more distant.
A number of such models have had great success in accounting for production and perception phenom-ena in phonetics.
"E.g.,[REF_CITE]offers an exemplar model which challenges the notion that speech is perceived through a process of normal-ization whereby a speaker-specific representation is mapped or normalized into a speaker-neutral cate-gorical abstraction."
"Johnson’s model successfully treats aspects of vowel perception, sex identifica-tion, and speaker variability."
"Crucially, no normal-ization of percepts into categorical representations takes place."
The correct identification of phonemes and words in his model is a function of direct com-parison to richly detailed exemplars stored in mem-ory.
"Other examples of exemplar-theoretic phonetic accounts include[REF_CITE],[REF_CITE], and our own work[REF_CITE]."
Ex-emplar theory’s success in phonetics motivates us to investigate its use as a model for local syntactic phe-nomena.
"In the context sequence model for exemplar-theoretic phonetics[REF_CITE], we represent speech using amplitude envelopes derived from the acoustic signal and then compute similarity as the integral over the correlation of the two acoustic sig-nals."
"For the syntactic level, we need a representa-tion that has two key properties of the represen-tation we use in phonetics in order to support an exemplar-theoretic account."
"First, the representa-tion must be directly derivable from the perceived input."
"In particular, it cannot rely on the results of any disambiguation that would occur either as part of exemplar-theoretic perception or in further down-stream processing."
"Second, it must support similar-ity computations."
"Accordingly, we first motivate the representation we use and then introduce a similarity measure on these representations."
"There are two main sources [Footnote_1] of directly observable information about the syntactic properties of words: semantic cues (e.g., things are often referred to with nouns) and the neighbors of a word in sentences that it is used in."
1 A comprehensive account of acquisition must also include morphology.[REF_CITE].
"In this pa-per, we only consider the second source of informa-tion for acquisition, lexical neighbors. [Footnote_2]"
2 Psycholinguistic evidence for the importance of neighbor information for learning categories includes[REF_CITE].
We further limit ourselves to the immediate left and right lexical neighbors (see discussion in Section 7).
"When using lexical neighbors as the basis of rep-resentation, we have to make a basic choice as to whether we look at left and right neighbors sepa-rately or whether we only look at the “correlated” neighborhood information of left and right neigh-bors jointly."
Our approach is based on the first alter-native: we separate the processing of left and right neighbors.
We do this for two reasons.
"First, gener-alization improves and model complexity decreases if left-neighbor information and right-neighbor in-formation are looked at separately."
"E.g., the right neighbors of to, might and not are similar because all three words can be followed by base verbs like dance: to dance, might dance, (might) not dance."
But their left neighbors are very different.
"Second, exemplar-theoretic similarity is best de-fined at the smallest possible scale in order to allow optimal matching between parts of the stimulus and parts of memory."
"In phonetics, we use a time scale of 10s of milliseconds or even less."
"Conceivably, one could also use segments (e.g., consonants and vowels) as the smallest unit; however, this would presume a segmented signal."
And segmentation is part of the perception task we want to explain in the first place.
Separating left and right neighbors – which amounts to looking at left and right local contexts of each word separately – is the smallest scale we can operate at when doing syntactic matching.
We choose this small scale for the same reasons as we choose a small scale in phonetics: to ensure maxi-mum flexibility when matching parts of the stimulus with exemplars in memory.
"Using words, bigrams or larger units would reduce the flexibility in matching and require a larger amount of experience (or train-ing data) to learn a particular generalization."
We refer to the representations of left and right contexts of a given word as half-words.
"In other words, we split a word into two entities, a left half-word that characterizes its behavior to the left and a right half-word that characterizes its behavior to the right."
"Thus left-context and right-context com-ponents of the representation of a given focus word are defined, where a left (right) half-word consists of a probability distribution over all words that oc-cur to the left (right) of the focus word and the dimensionality of the vector for each word is de-pendent on the number of distinct neighbors (left and right)."
"For example, having experienced take doll twice and drop doll once, then the left con-text distribution, or left half-word of doll, doll l , is P(take) = 2/3, P(drop) = 1/[Footnote_3]."
3[REF_CITE]have much the same motivation in introducing an evaluation measure of syntactic acquisition based on chunking.
"By extension, the phrase take the doll is represented as the following six half-words: take l , take r , the l , the r , doll l , and doll r ."
The basic intuition behind lo-cal syntactic coherence is that an important compo-nent of syntactic wellformedness – and a compo-nent that is of particular importance in acquisition – is whether a similar sequence has already been stored as grammatical in memory.
"The same way that a phonetic signal that is well-formed in a partic-ular language has many similar exemplars in mem-ory, a syntactic sequence should also be licensed by similar, previously perceived sequences in memory."
"To operationalize this notion, we need to be able to compute the similarity or distance between an in-put stimulus and exemplars in memory."
We do this by first defining a distance measure for sequences of fixed length.
"The distance ∆ between two sequences of half-words &lt; g 1 , . . . , g n &gt; and &lt; h 1 , . .. , h n &gt; is de-fined to be the sum of the distances of their half-words: ∆(&lt;g 1 , . . . , g n &gt;,&lt;h 1 , . .. , h n &gt;) = ni=1 ∆(g i , h i )"
This definition presupposes a definition P of the dis-tance of two half-words which will be given below.
"We then call a sequence of n half-words g 1 ,... ,g n locally coherent if there is a sequence h 1 , . . . , h n in memory with ∆(&lt; g 1 , . .. , g n &gt;, &lt; h 1 , . .. , h n &gt;) &lt; θ where θ is a parameter."
"Finally, we define a sentence to be locally n-coherent if all of its subsequences of length n are locally coherent."
The graph-theoretic model that is introduced in the next section will be evaluated with respect to how well it captures local syntactic coherence.
"This enables us to evaluate the model with respect to a task as opposed to its ability to reproduce a particu-lar linguistic representation of syntactic categories. 3 Obviously, the notion of local syntactic coherence only captures some aspects of syntax – e.g., it does not capture long-distance dependencies."
"However, it is a plausible component of syntactic competence and a plausible intermediate step in the acquisition of syntax."
"We briefly review the structuralist notions of syntag-matic and paradigmatic relationships that have been frequently used in prior work in NLP (e.g.,[REF_CITE])."
De Saussure defined a syntagmatic relationship between two words as their contigu-ous occurrence in a sentence and a paradigmatic re-lationship as mutual substitutability (de[REF_CITE]) (although he used the term rapport associ-atif instead of paradigmatic).
"E.g., brown and dog stand in a syntagmatic relationship with each other in the phrase brown dog; brown and black stand in a paradigmatic relationship with each other with re-spect to the position between the and dog in the phrase the X dog."
De Saussure’s conceptualization of syntactic relationships captures the fact that both admissible neighbors and admissible substitutes in language are an important part of the characteriza-tion of the syntactic properties of a word.
"We formalize the two relations as distribu-tions over words, where we assume a vocabulary {w 1 , . . . , w V } and V is the number of words in the vocabulary."
"We denote the left syntagmatic distribution of w i by p i,s,l,m where i is the vocabulary index of w i , s stands for syntagmatic, l for left and m is the order of the distribution as discussed below."
"Intuitively, p i,s,l,m (w j ) is the probability that word w j occurs to the left of w i ."
"Similarly, for the left paradigmatic distribution of w i , p i,p,l,m (w j ) is the probability that w j can be substituted for w i without changing local syntactic coherence as far as the context to the left is concerned."
Note that we distinguish between left and right paradigmatic distributions.
"A word w j can be a perfect substitute for w i as far as the context to the left is concerned, but a very unlikely substitute as far as the context to the right is concerned."
"E.g., in the phrase She loves her job, the word him is a good left-context substitute for her, but a terrible right-context substitute for her."
"We will now show how the syntag-matic/paradigmatic (henceforth: syn/para) dis-tributions are defined iteratively, based on the bigram distribution p ww , and grounded by defining p i,p,l,1 and p i,p,r,1 . p ww (w i w j ) is the probability that the bigram w i w j occurs, that is, that w i and w j occur next to each other (and in that order)."
We define the V × V joint probability matrix J by J ij = p ww (w i w j ).
Denote by N the diagonal V × V matrix that con-tains in N ii the reciprocal of p w (w i ) where p w is the marginal distribution of p ww :
V V 1 X p ww (w i w j ) =
X p ww (w j w i ) = p w (w i ) = N ii j=1 j=1
The conditional probability p left of the fol-lowing word and the conditional probability p right of the preceding word can be computed by multiplying (the transpose of) J and N: p left (w i |w j ) = p ww (w i w j )/p w (w j ) = (JN) ij ; and p right (w i |w j ) = (J T N) ij .
"The “grounding” paradigmatic distributions of or-der 1 are defined as follows. p i,p,l,1 (w j ) = p i,p,r,1 (w j ) = ( 10 ifif ww i =6 w j i = w j"
"In other words, each word has only one perfect left / right substitute and that perfect substitute is itself."
"We define the syn/para distributions of higher order recursively: p i,s,l,m = JNp i,p,l,m (1) p i,p,l,m = J T"
"Np i,s,l,m−1 (2) p i,s,r,m = J T Np i,p,r,m (3) p i,p,r,m = JNp i,s,r,m−1 (4)"
"Basic matrix arithmetic shows that p i,s,l,1 is sim-ply p left (.|w i ) and p i,s,r,1 is p right (.|w i )."
"For higher orders, the principle underlying Eq.s 1–4 is that when moving from left to right, we use p right (that is, J T N), the conditional distribution that characterizes right neighbors; when moving from right to left, we use p left (that is, JN), the condi-tional distribution that characterizes left neighbors."
This is graphically shown in Fig. 1.
"As illustrated by Fig. 1, the underlying graph for p i,s,r,m and p i,p,r,m is a weighted bipartite directed graph that connects the vocabulary on the left with the vocabulary on the right."
A directed edge from w i on the left to w j on the right is weighted with p ww (w i w j )/p w (w i ).
A directed edge from w j on the right to w i on the left (not shown) is weighted with p ww (w i w j )/p w (w j ).
"Eq.s 1–4 define four Markov chains: p i,s,l,m = (JNJ T N)p i,s,l,m−1 (5) p i,p,l,m = (J T NJN)p i,p,l,m−1 (6) p i,s,r,m = (J T NJN)p i,s,r,m−1 (7) p i,p,r,m = (JNJ T N)p i,p,r,m−1 (8)"
It is easy to see that p w is a stationary distribution for Eq. 1–4.
"Writing ~x for p w , we have:"
V p ww (w i w j ) (JNx~) i = X p (w ) p w (w j ) = p w (w i ) = x i w j j=1 V p ww (w j w i ) (J T N~x) i = X p (w ) p w (w j ) = p w (w i ) = x i w j j=1
Recall from Section 3 that our evaluation task is to discriminate sentences that exhibit local coherence from those that do not; that sentences are repre-sented as sequences of half-words; that syntactic co-herence of a sentence is defined as all subsequences of a given length n exhibiting local coherence; and that a subsequence is locally coherent if its distance from a sequence in memory is less than θ.
These definitions can be applied to the graph model as follows.
A left half-word is a left syntag-matic (or paradigmatic) distribution and a right half-word is a right syntagmatic (or paradigmatic) distri-bution.
We compute the distance of two half-words either as the Jensen-Shannon (JS) divergence[REF_CITE]or as (1 − cos(α)).
JS divergence is more ap-propriate for the comparison of probability distribu-tions.
But the cosine is more efficient when a sparse vector is compared to a dense vector. [Footnote_5] We therefore employ the cosine for the compute-intensive experi-ments in Section 6.
"5 This is so because, when computing the cosine, we can ig-nore all dimensions where one of the two vectors has a zero value."
The baseline representation is the categorical rep-resentation proposed[REF_CITE].
"A difficulty in replicating their experiments is that they use hierarchical agglomerative clustering (HAC), which eventually agglomerates all words in a sin-gle category."
"To circumvent the need for a stop-ping criterion, we represent each word as the tem-poral sequence of clusters it occurred in during ag-glomeration and define the distance of two words as the agglomeration step in which the two words are joined in a cluster."
"E.g., given the agglomeration se-quences {1}, {1, 2}, {1, 2, 4}, {1, 2, 3, 4} for w 1 and {4}, {4}, {1, 2, 4}, {1, 2, 3, 4} for w 4 , the distance between w 1 and w 4 is 3 since they are joined in step 3 when cluster {1, 2, 4} is created."
"For both graded (graph-theoretic) and categorical (cluster-based) representations, we need to set the parameter θ that is the boundary between locally co-herent and locally incoherent sentences."
This pa-rameter gives rise to a precision-recall tradeoff.
"A small θ will impose strict requirements on which se-quences in memory match, resulting in false nega-tive decisions for local grammaticality."
A large θ will incorrectly judge many locally incoherent se-quences to be grammatical.
We will pick the optimal θ in both cases.
"For categorical representations, this amounts to select-ing the HAC dendrogram with optimal performance."
The experiment below evaluates whether grammati-cal and ungrammatical sentences are well separated by the proposed measure. [Footnote_6]
"6 This evaluation of “separation” is not directly an evaluation of classification performance, but more similar to an evaluation of ranking using AUC or an evaluation of clustering using a measure like purity."
Experiment on CHILDES.
"We used the well-known CHILDES database[REF_CITE], a corpus of conversations between young children and their playmates, siblings, and caretakers."
"In order to avoid mixing varieties of English (e.g., British En-glish vs. American English), we selected the largest homogeneous subcorpus of CHILDES, the Manch-ester corpus."
"It contains roughly 350,000 sentences and 1.5 million words."
This is a conservative esti-mate of the amount of child-directed speech a child would receive annually[REF_CITE].
"All names in the corpus (i.e., all capitalized words) were replaced with a special word “ n ”."
A boundary symbol “ b ” was introduced to separate sentences.
The representation of the corpus is then a concate-nation of all its sentences.
The vocabulary consists of V = 8601 words.
Construction of the evaluation set.
"We tested the ability of the two models to distinguish locally coherent vs. incoherent sentences by selecting 100 unattested sentences from the corpus, which were not used to train the model."
"We only selected unat-tested sentences that were not a substring of a sen-tence in the training corpus since, presumably, any substring of a sentence in the training corpus is lo-cally coherent."
"A further constraint was that the unattested sentence was not allowed to contain a word that did not occur in the training corpus, the rationale being that we want to address the prob-lem of local coherence for known words only since unknown words present special challenges."
"Finally, we ensured that each unattested sentence contained a word that occurred in only one sentence type in the training corpus."
"In early experiments, we found that local grammatical inference for frequent words is easy as there is redundant evidence available that characterizes legal syntactic environments for fre-quent words."
"Since rare words are a key challenge in syntactic acquisition, we only selected sentences as unattested sentences that contained at least one rare word (where a rare word is defined as a word that occurs once in the training set). 100 ungrammatical sentences were generated by randomly selecting and concatenating words from the vocabulary."
"Ungrammatical sentences were matched in length to unattested sentences, so that both sets contained the same number of sentences of a given length."
"As with unattested sentences, un-grammatical sentences that were substrings of sen-tences in the training corpus were eliminated."
"As there are many more infrequent words than frequent words in the vocabulary, the construction ensured that, as with unattested sentences, infrequent words were overrepresented in ungrammatical sentences."
"To summarize, our setup consists of 348,463 training sentences, 100 unattested grammatical sen-tences and 100 ungrammatical sentences."
"The task of discriminating the 100 unattested from the 100 ungrammatical sentences cannot be solved perfectly as CHILDES contains ungrammat-ical sentences, a few of which were randomly se-lected as unattested sentences (e.g., yes pleas, which is missing the final letter)."
"Similarly, one or two of the automatically generated ungrammatical sen-tences were actually grammatical."
"Since the test set does not consist of a random sample of sentences, performance on the test set is not a direct indicator of the percentage of sentences that the model can correctly discriminate in a child’s typical input."
"A large proportion of sentences in child input are simple 1-word, 2-word, and 3-word sentences that even simplistic models can evaluate with high accuracy."
"However, the test set is appro-priate for a comparative evaluation of graded and categorical syntactic representations in language ac-quisition, which is one of the goals of the paper."
Dif-ficult sentences (those with rare words and greater length) are overrepresented in the test set as the dis-crimination of short sentences containing only fre-quent words can easily be done by simplistic mod-els.
"Thus, a test set of “easy” sentences would not distinguish good models from bad models."
"In order to train the graph model, the entries of matrix J were estimated using maximum likelihood based on the training corpus. p i,s,l,1 and p i,s,r,1 were then computed for all 8601 words."
"Replicating[REF_CITE], the most frequent 1000 words were clustered (using single-link HAC,[REF_CITE])."
"For each remaining word w, the closest neighbor w ′ in the 1000 most frequent words was determined and w was then assigned to the cluster of w ′ ."
"Fig. 3 shows the performance of graded and cat-egorical representations for different subsequence sizes n. To compute the accuracy for each n, the θ with optimal discrimination performance was cho-sen (for both graded and categorical)."
"For a subsequence of size n = 1, the performance is 0.5 in both cases since the 200-sentence test set does not contain unknown words."
"So for every half-word, there is a sequence of one half-word in the training corpus with distance 0."
"Thus, all sentences get the same local coherence scores, both for graded and categorical representations."
This argument does not apply to n = 2 since we earlier defined a sentence to be locally coherent if all of its subsequences are coherent.
"While subse-quences of 2 half-words that are part of the same word have local coherence score 0, this is not true of subsequences of 2 half-words that are part of differ-ent words, e.g., the subsequence &lt;black r ,dog l &gt; in black dog."
"If black dog does not occur in the train-ing set, then its local coherence score is &gt; 0."
"The main result of the experiment is that except for n=1 (p = 1) and n=2 (p = 0.39) the differences between categorical and graded representations are significant (χ 2 test, p &lt; 0.05 for 3 ≤ n ≤ 10)."
This is evidence that graded representations are more ac-curate when determining local syntactic coherence and grammaticality than categorical representations.
"The experimental results demonstrate that, for syntagmatic distributions of order 1, graded repre-sentations discriminate locally coherent vs. incoher-ent sentences better than categorical representations."
We attribute this to the ability of exemplar theory to incorporate rich context information into discrimi-nation decisions.
This is of particular importance for ambiguous words.
Categorical representations of ambiguous words are problematic because they are either too similar or not similar enough to the two alternatives.
"E.g., if a word with a verb/noun ambi-guity is represented as one of the alternatives, say, as a verb, then subsequences containing its noun use will no longer be similar to other subsequences with nouns."
"If a special conflation category noun/verb is introduced, then we are faced with the same prob-lem: subsequences containing the noun/verb cate-gory are not similar to subsequences containing ei-ther non-ambiguous verbs or non-ambiguous nouns."
The main motivation for higher-order distributions is that syntagmatic vectors of order 1 do not per-form well for some infrequent words.
"In the ele-phant/giraffe example above, the distance between the two words is close to maximum for order 1 repre-sentations because each occurs only once, in entirely different contexts."
"As we showed in Fig. 2, higher-order representations address this problem because they exploit indirect evidence about the syntactic properties of words."
"To evaluate higher-order representations on CHILDES, we used the same setup as before, but computed several additional iterations."
"We also lim-ited the experiments to a subset consisting of 60,000 words of the Manchester corpus."
"It contains only V =1666 different words, which reduces the storage requirements for the syn/para distributions (which is 2·V 2 for each order) and the cost of the matrix mul-tiplications."
We also used (1− cos(α)) instead of JS divergence as distance measure.
The results of the experiment are shown in Fig. 4.
"Higher-order representations are clearly superior for short subsequences, especially for n = 2 and n = 3 (and up to 5 half-words when comparing synt-1 and para-2)."
"However, for long subsequences, there is no consistent difference between the syntagmatic distri-bution of order 1 (synt-1) and higher order distribu-tions."
"Apparently, the generalized information avail-able in higher orders is not helpful in local grammat-ical inference if long contexts are considered."
"We were surprised that the best-performing dis-tribution for short sequences is para-2 (paradigmatic distribution of order 2), not a higher order distri-bution."
"E.g., para-3 performs worse than para-2."
We would expect the performance to decrease with higher order eventually since the distributions con-verge towards p w .
The fact that this happens so early in this experiment merits further investigation.
"Data-oriented parsing[REF_CITE]shares basic assumptions about linguistic inference with exemplar-based theory, but it does not model or use the similarity between input and stored exemplars."
Previous work on exemplar theory in syntax[REF_CITE]has not been computational or for-mal.
Previous work on non-categorical representa-tions of words has viewed these representations as an intermediate step for arriving at categorical parts of speech[REF_CITE].
"Consequently, all of these papers eval-uate their results by comparing induced categories to gold-standard parts of speech."
"The BEAGLE model[REF_CITE], and related work[REF_CITE], merges co-occurrence information and word order information into a single composite vector through a process of vector convolution."
Our model differs in that it ex-plicitly captures the recursive relationship between the orders in a unified framework.
Previous graph-theoretic work[REF_CITE]uses order 1 representations.
"Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains[REF_CITE]."
"In this paper, we have presented a graph-theoretic model of the acquisition of lexical syntactic rep-resentations and a new exemplar-based evaluation of lexical syntactic acquisition."
"When applied to the CHILDES corpus, the evaluation shows that the graded syntactic representations learned by the model perform significantly better than previously proposed categorical representations."
An initial evaluation of high-order representations showed lit-tle improvement over low-order representations.
"In future work, we intend to investigate the in-fluence of noise and ambiguity on the quality of the representations in order to characterize when higher order representations improve generalization and exemplar-theoretic inference."
We also want to address that the model as it currently stands is trained under the false assumption that the train-ing input is grammatical.
Ungrammatical test input which matches a learned ungrammatical sequence will be deemed grammatical.
"Future work will ex-amine how to best treat this challenge, e.g., by using an estimation of density instead of the simplistic “1 nearest neighbor” distance used here."
The most important future work concerns class-based language models.
The cognitive-linguistic tradition we have mainly addressed in this paper has focused on the task of learning traditional parts of speech and has usually not discussed the rele-vance of language models to acquisition.
"If, as we have argued, instead of learning traditional parts of speech the focus should be on performance in par-ticular language processing tasks (like grammatical-ity judgments), then language models are the nat-ural competing account that we must compare our work to."
"Of particular relevance are class-based lan-guage models (e.g.,[REF_CITE])."
"In ongoing work, we are attempting to show that the exemplar-theoretic model performs better on grammaticality judgments than class-based language models."
"This research was funded by the German Research Council (DFG,[REF_CITE])."
"We thank K. Rothenhäusler, H. Schmid and the reviewers for their valuable comments."
Question classification plays an important role in question answering.
Features are the key to obtain an accurate question classifier.
"In con-trast[REF_CITE]’s approach which makes use of very rich feature space, we pro-pose a compact yet effective feature set."
"In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet."
"In addition, Lesk’s word sense disambigua-tion (WSD) algorithm is adapted and the depth of hypernym feature is optimized."
"With fur-ther augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%."
An important step in question answering (QA) and other dialog systems is to classify the question to the anticipated type of the answer.
"For example, the question of Who discovered x-rays should be classi-fied into the type of human (individual)."
This infor-mation would narrow down the search space to iden-tify the correct answer string.
"In addition, this infor-mation can suggest different strategies to search and verify a candidate answer."
"For instance, the classifi-cation of question What is autism to a definition type question would trigger the search strategy specific for definition type (e.g., using predefined templates like: Autism is ... or Autism is defined as...)."
"In fact, the combination of QA and the named entity recog-nition is a key approach in modern question answer-ing systems[REF_CITE]."
The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results.
The difficulty lies in classify-ing the what and which type questions.
"Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type."
Considering also examples[REF_CITE]
"What tourist attractions are there in Reims, What are the names of the tourist attractions in Reims, What do most tourists visit in Reims, What attracts tourists to Reims, and What is worth seeing in Reims, all these reformulations are of the same answer type of location."
Different wording and syn-tactic structures make it difficult for classification.
"Many QA systems used manually constructed sets of rules to map a question to a type, which is not effi-cient in maintain and upgrading."
"With the increasing popularity of statistical approaches, machine learn-ing plays a more and more important role in this task."
"A salient advantage of machine learning ap-proach is that one can focus on designing insightful features, and rely on learning process to efficiently and effectively cope with the features."
"In addition, a learned classifier is more flexible to reconstruct than a manually constructed system because it can be trained on a new taxonomy in a very short time."
"Ear-lier question classification work includes[REF_CITE]and Radev et at. (2002), in which language model and Rappier rule learning were employed respectively."
"More recently,[REF_CITE]have developed a machine learning approach which uses the SNoW learning architecture[REF_CITE]."
They have compiled the UIUC question clas-sification dataset 1 which consists of 5500 training and 500 test questions.
"The questions in this dataset are collected from four sources: 4,500 English ques-tions published by USC[REF_CITE], about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions[REF_CITE]which serve as the test dataset."
"All questions in the dataset have been manually labeled by them according to the coarse and fine grained categories as shown in Table 3, with coarse classes (in bold) followed by their fine class refinements."
"In addition, the table shows the dis-tribution of the 500 test questions over such cate-gories."
"With a hand-built dictionary of semantically related words, their system is able to reach 84.2%."
The UIUC dataset has laid a platform for the follow-up research.
"With all these semantic features plus the syn-tactic ones, their model was trained on 21’500 ques-tions and was able to achieve the best accuracy of 89.3% on a test set of 1000 questions (taken[REF_CITE]) for 50 fine classes."
"Most recently,[REF_CITE]used a short (typ-ically one to three words) subsequence of question tokens as features for question classification."
"Their model can reach the accuracy of 86.2% using UIUC dataset over fine grained question categories, which is the highest reported accuracy on UIUC dataset."
"In contrast[REF_CITE]’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set."
"In particular, we propose head word feature and present two approaches to augment semantic fea-tures of such head words using WordNet."
"In addi-tion, Lesk’s word sense disambiguation (WSD) al-gorithm is adapted and the depth of hypernym fea-ture is optimized."
"With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% us-ing[REF_CITE]fine classes."
"In this section, we briefly present two classifiers, support vector machines and maximum entropy model, which will be employed in our experiments."
These two classifiers perform roughly identical in the question classification task.
Support vector machine[REF_CITE]is a useful technique for data classification.
"Given a training set of instance-labeled pairs (x i , y i ), i = 1, . . . , l where x i ∈ R n and y ∈ {[Footnote_1], −1} l , the support vector ma-chines (SVM) require the solution of the following optimization problem: min w,b,ξ 12 w T w+C P li=1 ξ i subject to y i (w T φ(x i ) + b) ≥ 1 − ξ i and ξ i ≥ 0."
Here training vectors x i are mapped into a higher (maybe infinite) dimensional space by the function φ.
Then SVM finds a linear separating hyperplane with the maximal margin in this higher dimensional space.
C &gt; 0 is the penalty parameter of the er-ror term.
"Furthermore, K(x i , x j ) ≡ φ(x i )"
T φ(x i ) is called the kernel function.
"There are four basic ker-nels: linear, polynomial, radial basis function, and sigmoid."
"In the question classification context, x i is represented by a set of binary features, for in-stance, the presence or absence of particular words. y i ∈ {1,−1} indicates wether a question is of a particular type or not."
"Due to the large number of features in question classification, one may not need to map data to a higher dimensional space."
"It has been commonly accepted that the linear kernel of K(x i ,x j ) = x iT x i is good enough for question classification."
"In this paper, we adopt the LIBSVM[REF_CITE]implementation in our exper-iments."
"Maximum entropy (ME) models[REF_CITE], also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been suc-cessfully applied to natural language processing in-cluding part of speech tagging, named entity recog-nition etc."
Maximum entropy models can inte-grate features from many heterogeneous informa-tion sources for classification.
Each feature corre-sponds to a constraint on the model.
"In the context of question classification, a sample feature could be the presence of a particular word associated with a par-ticular question type."
The maximum entropy model is the model with maximum entropy of all models that satisfy the constraints.
"In this paper, we adopt Stanford Maximum Entropy[REF_CITE]implementation in our experiments."
Each question is represented as a bag of features and is feeded into classifiers in training stage.
"We present five binary feature sets, namely question wh-word, head word, WordNet semantic features for head word, word grams, and word shape feature."
The five feature sets will be separately used by the classifiers to determine their individual contribution.
"In addition, these features are used in an incremental fashion in our experiments."
The wh-word feature is the question wh-word in given questions.
"For example, the wh-word of question What is the population of China is what."
"We have adopted eight question wh-words, namely what, which, when, where, who, how, why, and rest, with rest being the type does not belong to any of the previous type."
"For example, the question Name a food high in zinc is a rest type question."
Li and Roth (2002;2006) used head chunks as fea-tures.
The first noun chunk and the first verb chunk after the question word in a sentence are defined as head chunks in their approach.
"In both approaches, noisy information could be introduced."
"For example, considering the question of What is a group of turkeys called, both the head chunk and in- former span of this question is group of turkeys."
The word of turkeys in the chunk (or span) contributes to the classification of type ENTY:animal if the hyper-nyms of WordNet are employed (as described in next section).
"However, the extra word group would in-troduce ambiguity to misclassify such question into HUMAN:group, as all words appearing in chunk are treated equally."
"To tackle this problem, we pro-pose the feature of head word, which is one single word specifying the object that the question seeks."
"In the previous example What is a group of turkeys called, the head word is exactly turkeys."
"In doing so, no misleading word group is augmented."
An-other example is George Bush purchased a small in-terest in which baseball team.
"The head chunk, in-former span and head word are baseball team, base-ball team and team respectively."
The extra word baseball in the head chunk and informer span may lead the question misclassified as ENTY:sport rather than HUM:group.
"In most cases, the head chunk or informer span include head words."
The head chunk feature or informer span feature would be beneficiary so long as the useful information plays a stronger role than the misleading one.
"Nevertheless, this is not as effective as the introduction of one head word."
"To obtain the head word feature, a syntactic parser is required."
A syntactic parser is a model that out-puts the grammatical structure of given sentences.
"There are accurate parsers available such as Cha-niak parser[REF_CITE], Stan-ford parser[REF_CITE]and Berkeley parser[REF_CITE], among which we use the Berkeley parser [Footnote_2] to help identify the head word."
Figure 1 shows two example parse trees for questions What year did the Titanic sink and What is the sales tax in Minnesota respectively.
Collins rules[REF_CITE]can be applied to parse trees to extract the syntactic head words.
"For example, the WHNP phrase (Wh-noun phrase) in the top of Figure 1 takes its WP child as its head word, thus assigning the word what (in the bracket) which is associated with WP tag to the syntactic head word of WHNP phrase."
Such head word as-signment is carried out from the bottom up and the word did is extracted as the head word of the whole question.
"Similarly, the word is is extracted as the syntactic head word in the bottom of Figure 1."
Collins head words finder rules have been modi-fied to extract semantic head word[REF_CITE].
"To better cover the question sentences, we further re-define the semantic head finder rules to fit our needs."
"In particular, the rules to find the semantic head word of phrases SBARQ (Clause in-troduced by subordinating conjunction), SQ (sub-constituent of SBARQ excluding wh-word or wh-phrase), VP (verb phrase) and SINV (declarative sentence with subject-aux inversion) are redefined, with the head preference of noun or noun phrase rather than verb or verb phrase."
The new head word assignments for the previous two examples are shown in Figure 2.
"If the head word is any of name, type or kind etc, post fix is required to identify the real head word if necessary."
"In particular, we compile a tree pattern as shown in the left of Figure 3."
"If this pattern is matched against a given parse question parse tree, the head word is re-assigned to the head word of NP node in the tree pattern."
"For example, the initial head word extracted from parse tree of question What is the proper name for a female walrus is name."
"As such parse tree (as shown partially in the right of Figure 3) matches the compiled tree pattern, the post operation shall fix it to walrus, which is the head word of the NP in the tree pattern."
This post fix helps classify the question to ENTY:animal.
"In addition to the question head word as described above, we introduce a few regular expression pat-terns to help question head word identification."
Note that these patterns depend on the question type tax-onomy as shown in Table 3.
"For example, consid-ering the questions of What is an atom and What are invertebrates, the head word of atom and in-vertebrates do not help classify such questions to DESC:def."
"To resolve this, we create a binary fea-ture using a string regular expression which begins with what is/are and follows by an optional a, an, or the and then follows by one or two words."
"If a ques-tion matches this regular expression, a binary feature (a placeholder word is used in implementation, for instance DESC:def 1 in this case) would be inserted to the feature set of the question."
"This feature, if it is beneficial, would be picked up by the classifiers (SVMs or MEs) in training."
We list all regular ex-pression patterns which are used in our experiments as following:
DESC:def pattern 1
"The question begins with what is/are and follows by an optional a, an, or the and then follows by one or two words."
DESC:def pattern 2
The question begins with what do/does and ends with mean.
ENTY:substance pattern The question begins with what is/are and ends with composed of/made of/made out of.
DESC:desc pattern The question begins with what does and ends with do.
ENTY:term The question begins with what do you call.
DESC:reason pattern 1
The question begins with what causes/cause.
DESC:reason pattern 2
The question begins with What is/are and ends with used for.
ABBR: exp pattern The question begins with What does/do and ends with stand for.
HUM:desc pattern The question begins with Who is/was and follows by a word starting with a capital letter.
It is worth noting that all these patterns serve as feature generators for given questions: the feature becomes active if the pattern matches the ques-tions.
The algorithm to extract question head word is shown in Algorithm 1.
"There is no head word returned for when, where or why type questions, as these hw-words are informative enough; the inclu-sion of other words would introduce noisy informa-tion."
"If the question is of type how, the word follow-ing how is returned as head word."
The patterns are then attempted to match the question if it is of type what or who.
"If there is a match, the placehold word for such pattern (e.g., HUM:desc for HUM:desc pat-tern) is returned as head word."
"If none of the above condition is met, the candidate head word is ex-tracted from the question parse tree using the rede-fined head finder rules."
Such extracted head word is returned only if it has noun or noun phrase tag; oth-erwise the first word which has noun or noun phrase tag is returned.
The last step is a back up plan in case none of the previous procedure happens.
WordNet[REF_CITE]is a large English lexicon in which meaningfully related words are connected via cognitive synonyms (synsets).
The WordNet is a useful tool for word semantics analysis and has been widely used in question classificati[REF_CITE].
"A natural way to use WordNet is via hypernyms: Y is a hy-pernym of X if every X is a (kind of) Y. For exam-ple, the question of What breed of hunting dog did the Beverly Hillbillies own requires the knowledge of animal being the hypernym of dog."
"In this paper, we propose two approaches to augment WordNet se-mantic features, with the first augmenting the hyper-nyms of head words as extracted in previous section directly, and the second making use of a WordNet similarity package[REF_CITE], which implic-itly employs the structure of hypernyms."
"In WordNet, senses are organized into hierarchies with hypernym relationships, which provides a nat-ural way to augment hypernyms features from the original head word."
"For example, the hierarchies for a noun sense of domestic dog is described as: dog → domestic animal → animal, while another noun sense (a dull unattractive unpleasant girl or woman) is organized as dog → unpleasant woman → un-pleasant person."
"In addition, a verb sense of dog is organized as dog → pursue → travel."
"In our first ap-proach, we attempt to directly introduce hypernyms for the extracted head words."
"The augment of hyper-nyms for given head word can introduce useful in-formation, but can also bring noise if the head word or the sense of head word are not correctly identi-fied."
"To resolve this, three questions shall be ad-dressed: 1) which part of speech senses should be augmented? 2) which sense of the given word is needed to be augmented? and 3) how many depth are required to tradeoff the generality (thus more informative) and the specificity (thus less noisy)."
"The first question can be answered by mapping the Penn Treebank part of speech tag of the given head word to its WordNet part of speech tag, which is one of POS.NOUN, and POS.ADJECTIVE, POS.ADVERB and POS.VERB."
The second ques-tion is actually a word sense disambiguation (WSD) problem.
The Lesk algorithm[REF_CITE]is a clas-sical algorithm for WSD.
It is based on the assump-tion that words in a given context will tend to share a common topic.
A basic implementation of the The Lesk algorithm is described as following: 1.
Choosing pairs of ambiguous words within a context 2. Checks their definitions in a dictionary [Footnote_3]. Choose the senses as to maximize the number of common terms in the definitions of the cho-sen words
"In our head word sense disambiguation, the context words are words (except the head word itself) in the question, and the dictionary is the gloss of a sense for a given word."
Algorithm 2 shows the adapted Lesk algorithm which is employed in our system.
"Basically, for each sense of given head word, this 6: for each context word w in q do 7: int subMax = maximum number of common words in s definition (gloss) and definition of any sense of w 8: count = count + sumMax 9: end for 10: if count &gt; max[REF_CITE]: maxCount = count 12: optimum = s 13: end if 14: end for 15: return optimum algorithm computes the maximum number of com-mon words between gloss of this sense and gloss of any sense of the context words."
"Among all head word senses, the sense which results in the maxi-mum common words is chosen as the optimal sense to augment hypernyms later."
Finally the third ques-tion is answered via trail and error based on evaluat-ing randomly generated 10% data from the training dataset.
"Generally speaking, if the identification of the head word is not accurate, it would brings signif-icant noisy information."
Our experiments show that the use of depth six produces the best results over the validation dataset.
This indirectly proves that our head word feature is very accurate: the hypernyms introduction within six depths would otherwise pol-lute the feature space.
"In this approach, we make use of the WordNet Similarity package[REF_CITE], which im-plicitly employs WordNet hypernyms."
"In particu-lar, for a given pair of words, the WordNet similar-ity package models the length of path traveling from one word to the other over the WordNet network."
It then computes the semantic similarity based on the path.
"For example, the similarity between car and automobile is 1.0, while the similarity between film and audience is 0.38."
"For each question, we use the WordNet similarity package to compute the similarity between the head word of such question and each description word in a question categoriza-tion."
The description words for a question category are a few words (usually one to three) which explain the semantic meaning of such a question category [Footnote_3] .
"For example, the descriptions words for category ENTY:dismed are diseases and medicine."
The ques-tion category which has the highest similarity to the head word is marked as a feature.
This is equal to a mini question classifier.
"For example, as the head word walrus of question What is the proper name for a female walrus has the highest similarity mea-sure to animals, which is a description word of cat-egory ENTY:animal, thus the ENTY:animal is in-serted into the feature set of the given question."
An N-gram is a sub-sequence of N words from a given question.
"Unigram forms the bag of words feature, and bigram forms the pairs of words fea-ture, and so forth."
"We have considered unigram, bi-gram, and trigram features in our experiments."
"The reason to use such features is to provide word sense disambiguation for questions such as How long did Rip Van Winkle sleep, as How long (captured by wh-word and head word features) could refer to ei-ther NUM:dist or NUM:period."
The word feature of sleep help determine the NUM:period classification.
Word shape in a given question may be useful for question classification.
"For instance, the question Who is Duke Ellington has a mixed shape (begins a with capital letter and follows by lower case letters) for Duke, which roughly serves as a named entity recognizer."
"We use five word shape features, namely all upper case, all lower case, mixed case, all digits, and other."
The experiments show that this feature slightly boosts the accuracy.
We designed two experiments to test the accuracy of our classifiers.
The first experiment evaluates the individual contribution of different feature types to question classification accuracy.
"In particular, the SVM and ME are trained from the[REF_CITE]train-ing data using the following feature sets: 1) wh-word + head word, 2) wh-word + head word + direct hypernym, 3) wh-wod + head word + indirect hyper-nym, 4) unigram, 5) bigram, 6) trigram, and 7) word shape."
"We set up the tests of 1), 2) and 3) due to the fact that wh-word and head word can be treated as a unit, and hypernym depends on head word."
"In the second experiment, feature sets are incrementally feeded to the SVM and ME."
"The parameters for both SVM and ME classifiers (e.g., the C in the SVM) are all with the default values."
"In order to facilitate the comparison with previously reported results, the question classification performance is measured by accuracy, i.e., the proportion of the correctly classi-fied questions among all test questions."
Table 1 shows the question classification accuracy of SVM and ME using individual feature sets for 6 coarse and 50 fine classes.
"Among all feature sets, wh-word + head word proves to be very infor-mative for question classification."
"Our first Word-Net semantic feature augment, the inclusion of di-rect hypernym, can further boost the accuracy in the fine classes for both SVM and ME, up to four per- cent."
This phenomena conforms[REF_CITE]that WordNet hypernym benefits mainly on the 50 fine classes classification.
Their system managed to improve around 4 percent with the help of those semantic features.
"They reported that WordNet didn’t contribute much to the system, while our results show that the WordNet signifi-cantly boosts the accuracy."
"The reason may be that their system expanded the hypernyms for each word in the question, while ours only expanded the head word."
"In doing so, the augmentation does not intro-duce much noisy information."
Notice that the inclu-sion of various depth of hypernyms results in differ-ent accuracy.
"The depth of six brings the highest ac-curacy of 85.4% and 85.6%[REF_CITE]classes, which is very competitive to the previ-ously reported best accuracy of 86.2%[REF_CITE]."
"Our second proposed WordNet semantic feature, the indirect use of hypernym, does not perform as good as the first approach; it only contributes the accuracy gain of 1.8 and 1.6 in the fine classes for SVM and ME respectively."
"The reason may be two fold: 1) the description words (usually one to three words) of question categories are not representative enough, and 2) the indirect use of hypernyms via the WordNet similarity package is not as efficient as direct use of hypernyms."
"Among the surface words features, unigram fea-ture perform the best with accuracy of 80.4%[REF_CITE]classes, and 88.0% for SVM under 6 classes."
"It is not surprising that the word shape feature only achieves small gain in question classi-fication, as the use of five shape type does not pro-vide enough information for question classification."
"However, this feature is treated as an auxiliary one to boost a good classifier, as we will see in the second experiment."
"Based on the individual feature contribution, we then trained the SVMs and MEs using wh-word, head word, direct hypernyms (with depth 6) of head word, unigram, and word shape incrementally."
Table 2 shows the question classification accuracy (bro-ken down by question types) of SVM and ME for 6 coarse and 50 fine classes.
"As can be seen, the main difficulty for question classification lies in the what type questions."
SVM and ME perform roughly iden-tical if they use the same features.
"For both SVM and ME, the baseline using the wh-head word and head word results in 81.4% and 82.0% respectively for 50 fine class classification (92.0% and 92.2% for 6 coarse classes)."
"The incremental use of hypernym feature within 6 depths boost about four percent for both[REF_CITE]classes, while slight gain or slight loss for SVM and ME for 6 coarse classes."
The further use of unigram feature leads to another three percent gain for both[REF_CITE]classes.
"Finally, the use of word shape leads to another 0.6% accuracy increase for both[REF_CITE]classes."
The best accuracy achieved for 50 classes is 89.2%[REF_CITE].0% for ME.
"For 6 coarse classes, SVM and ME achieve the best accuracy of 93.4% and 93.6% respectively."
Our best result feature space only consists of 13’697 binary features and each question has 10 to 30 active features.
"Compared to the over feature size of 200’000[REF_CITE], our feature space is much more compact, yet turned out to be more informative as suggested by the experiments."
"Note that if we replace the bigram with unigram, SVM and ME achieve the overall accuracy of 88.4% and 88.0% respectively for 50 fine classes, and the use of trigram leads[REF_CITE].6% and 86.8% respectively."
"The inclusion of unigram, bi-gram and trigram together won’t boost the accu-racy, which reflects the fact that the bigram and tri-gram features cannot bring more information given that unigram, wh-word and head word features are present."
This is because the useful information which are supposed to be captured by bigram or tri-gram are effectively captured by wh-word and head word features.
The unigram feature thus outper-forms bigram and trigram due to the fact that it is less sparse.
"In addition, if we replace the indirect use of hypernym with the direct use of hypernym, the overall accuracy is 84.6% and 84.8% for SVM and ME respectively."
All these experiments conform to the individual features contributions as shown in Table 1.
"For a better understanding of the error distribu-tion with respect to the 50 question categories, Ta-ble 3 shows the precision and recall for each ques-tion type in the best result (89.2%) using SVM."
"It is not surprising that some of the categories are more difficult to predict such as ENTY:other and ENTY:product, while others are much easier such as HUMAN:individual, since the former are more semantically ambiguous than the latter."
Table 4 shows the summary of the classification accuracy of all models which were applied to UIUC dataset.
Note (1) that SNoW accuracy without the related word dictionary was not reported.
"With the semantically related word dictionary, it achieved 91%."
Note (2) that SNoW with a semantically re-lated word dictionary achieved 84.2% but the other algorithms did not use it.
Our results are summa-rized in the last two rows.
Our classifiers are able to classify some chal- lenge questions.
"For instance, the question What is the proper name for a female walrus has been correctly classified as ENTY:animal."
"However, it still has nearly ten percent error rate for 50 fine classes."
The reason is three fold: 1) there are in-herently ambiguity in classifying a question.
"For instance, the question What is mad cow disease, it could be either of type DESC:def or ENTY:dismed; 2) there are inconsistent labeling in the training data and test data."
"For instance, What is the popula-tion of Kansas is labeled with the type NUM:other while What is the population of Arcadia , Florida is labeled with type NUM:count."
"Another exam-ple, What county is Chicago in is labeled with type LOC:other while What county is Phoenix , AZ in is labeled with type LOC:city; and 3) The parser can produce incorrect parse tree which would result in wrong head word extraction."
"For instance, the head word extracted from What is the speed humming-birds fly is hummingbirds (the correct one should be speed), thus leading to the incorrect classification of ENTY:animal (rather than the correct NUM:speed)."
"In contrast[REF_CITE]’s approach which makes use of very rich feature space, we proposed a compact yet effective feature set."
"In particular, we proposed head word feature and presented two approaches to augment semantic features of such head words using WordNet."
"In addition, Lesk’s word sense disambiguation algorithm was adapted and the depth of hypernym feature was optimized through cross validation, which was to introduce useful information while not bringing too much noise."
"With further augment of wh-word, unigram feature, and word shape feature, we can obtain ac-curacy of 89.2% using linear SVMs, or 89.0% using[REF_CITE]fine classes."
This research was supported by British Telecom grant[REF_CITE]and BISC Program of UC Berkeley.
An increasingly popular method for finding information online is via the Community Question Answering (CQA) portals such as Yahoo!
"An-swers, Naver, and Baidu Knows."
"Searching the CQA archives, and rank-ing, filtering, and evaluating the sub-mitted answers requires intelligent processing of the questions and an-swers posed by the users."
"One impor-tant task is automatically detecting the question’s subjectivity orientation: namely, whether a user is searching for subjective or objective information."
"Unfortunately, real user questions are often vague, ill-posed, poorly stated."
"Furthermore, there has been little la-beled training data available for real user questions."
"To address these prob-lems, we present CoCQA, a co-training system that exploits the association be-tween the questions and contributed answers for question analysis tasks."
The co-training approach allows CoCQA to use the effectively unlim-ited amounts of unlabeled data readily available in CQA archives.
In this pa-per we study the effectiveness of CoCQA for the question subjectivity classification task by experimenting over thousands of real users’ questions.
"Automatic question answering (QA) has been one of the long-standing goals of natural lan-guage processing, information retrieval, and artificial intelligence research."
"For a natural language question we would like to respond with a specific, accurate, and complete an-swer that addresses the question."
"Although much progress has been made, answering complex, opinion, and even many factual questions automatically is still beyond the current state-of-the-art."
"At the same time, the rise of popularity in social media and collabo-rative content creation services provides a promising alternative to web search or com-pletely automated QA."
"The explicit support for social interactions between participants, such as posting comments, rating content, and responding to questions and comments makes this medium particularly amenable to Ques-tion Answering."
Some very successful exam-ples of Community Question Answering (CQA) sites are Yahoo!
"Answers [URL_CITE] and Naver [URL_CITE] , and Baidu Knows [URL_CITE] ."
Answers alone has already amassed hundreds of mil-lions of answers posted by millions of par-ticipants on thousands of topics.
"The questions posted to such CQA portals are typically complex, subjective, and rely on human interpretation to understand the corre-sponding information need."
"At the same time, the questions are also usually ill-phrased, vague, and often subjective in nature."
"Hence, analysis of the questions (and of the corre-sponding user intent) in this setting is a par-ticularly difficult task."
"At the same time, CQA content incorporates the relationships between questions and the corresponding an-swers."
"Because of the various incentives pro-vided by the CQA sites, answers posted by users tend to be, at least to some degree, re-sponsive to the question."
This observation suggests investigating whether the relation- ship between questions and answers can be exploited to improve automated analysis of the CQA content and the user intent behind the questions posted.
"To this end, we exploit the ideas of co-training, a general semi-supervised learning approach naturally applicable to cases of com-plementary views on a domain, for example, web page links and content[REF_CITE]."
"In our setting, we focus on the complimentary views for a question, namely the text of the question and the text of the as-sociated answers."
As a concrete case-study of our approach we focus on one particularly important aspect of intent detection: the subjectivity orientation.
We attempt to predict whether a question posted in a CQA site is subjective or objective.
"Objective questions are expected to be an-swered with reliable or authoritative informa-tion, typically published online and possibly referenced as part of the answer, whereas sub-jective questions seek answers containing pri-vate states, e.g. personal opinions, judgment, experiences."
"If we could automatically predict the orientation of a question, we would be able to better rank or filter the answers, improve search over the archives, and more accurately identify similar questions."
"For example, if a question is objective, we could try to find a few highly relevant articles as references, whereas if a question is subjective, useful an-swers are not expected to be found in authori-tative sources and tend to rank low with cur-rent question answering and CQA search tech-niques."
"Finally, learning how to identify ques-tion orientation is a crucial component of in-ferring user intent, a long-standing problem in web information access settings."
"In particular, we focus on the following re-search questions: • Can we utilize the inherent structure of the CQA interactions and use the unlimited amounts of unlabeled data to improve classi-fication performance, and/or reduce the amount of manual labeling required? • Can we automatically predict question sub-jectivity in Community Question Answering – and which features are useful for this task in the real CQA setting?"
The rest of the paper is structured as fol-lows.
"We first overview the community ques-tion answering setting, and state the question orientation classification problem, which we use as the motivating application for our sys-tem, more precisely."
We then introduce our CoCQA system for semi-supervised classifi-cation of questions and answers in CQA com-munities (Section 3).
"We report the results of our experiments over thousands of real user questions in Section 4, showing the effective-ness of our approach."
"Finally, we review re-lated work in Section 5, and discuss our con-clusions and future work in Section 6."
We first briefly describe the essential features of question answering communities such as Yahoo!
Answers or Naver.
"Then, we formally state the problem addressed in this paper, and the features used for this setting."
Online social media content and associated services comprise one of the fastest growing segments on the Web.
"The explicit support for social interactions between participants, such as posting comments, rating content, and re-sponding to questions and comments makes the social media unique."
Question answering has been particularly amenable to social media by directly connecting information seekers with the community members willing to share the information.
"Answers, with mil-lions of users and hundreds of millions of an-swers for millions of questions is a very suc-cessful implementation of CQA."
"For example, consider two example user-contributed questions, objective and subjective respectively:"
What’s the difference between chemotherapy and radiation treat-ments?
Has anyone got one of those home blood pressure monitors? and if so what make is it and do you think they are worth getting?
Figure 1 shows an example of community interactions in Yahoo!
Answers around the question Q2 above.
"A user posted the question in the Health category of the site, and was able to obtain 10 responses from other users."
"Even-tually, the asker chooses the best answer."
"Fail-ing that, as shown in the example, the best an-swer can also be chosen according to the votes from other users."
"Many of the interactions de-pend on the perceived goals of the asker: if the participants interpret the question as subjec-tive, they will tend to share their experiences and opinions, and if they interpret the question as objective, they may still share their experi-ences but may also provide more factual in-formation."
We now state our problem of question orienta-tion more precisely.
"We consider question ori-entation from the perspective of user goals: authors of objective questions request authori-tative, objective information (e.g., published literature or expert opinion), whereas authors of subjective questions seek opinions or judg- ments of other users in the community."
We state our problem as follows.
"Question Subjectivity Problem: Given a question Q in a question answering com-munity, predict whether Q has objective or subjective orientation, based on ques-tion and answer text as well as the user and community feedback."
In the CQA setting we could easily obtain thousands or millions of unlabeled examples from the online CQA archives.
"On the other hand, it is difficult to create a labeled dataset with a reasonable size, which could be used to train a perfect classifier to analyze ques-tions from different domains and sub-domains."
"Therefore, semi-supervised learning[REF_CITE]is a natural approach for this setting."
"Intuitively, we can consider the text of the question itself or answers to it."
"In other words, we have multiple (at least two) natural views of the data, which satisfies the condi-tions of the co-training approach[REF_CITE]."
"In co-training, two separate classifiers are trained on two sets of features, respectively."
"By automatically labeling the unlabeled examples, these two classifiers it-eratively “teach” each other by giving their partners a newly labeled data that they can predict with high confidence."
"Based on the original co-training algorithm[REF_CITE]and other implementations, we develop our algorithm CoCQA shown in Figure 2."
"At Steps 1 and 2, the K examples are com-ing from different feature spaces, and each category (for example, Subjective and Objec-tive) has top K j most confident examples cho-sen, where K j corresponds to the distribution of class in the current set of labeled examples L. CoCQA will terminate when the incre-ments of both classifiers are less than a speci-fied threshold X or the maximum number of iterations are exceeded."
"Following the co-training approach, we include the most confi-dently predicted examples as additional “la-beled” data."
"The SVM output margin value was used to estimate confidence; alternative methods (including reliability of this confi-dence prediction) could further improve per-formance, and we will explore these issues in future work."
"Finally, the next question is how to estimate classification performance with training data."
"For each pass, we randomly split the original training data into N folds (N=10 in our experiments), and keep one part for valida-tion and the rest, augmented with the newly added examples, as the expanded training set."
"After CoCQA terminates, we obtain two classifiers."
"When a new example arrives, we will classify it with these two classifiers based on both of the feature sets, and combine the predictions of these two classifiers."
"We ex-plored two strategies to make the final deci-sion based on the confidence values given by two classifiers:  Choose the class with higher confidence  Multiply the confidence values, and choose the class that has the highest product."
We found the second heuristic to be more effective than the first in our experiments.
"As the base classifier we use SVM in the current implementation, but other classifiers could be incorporated as well."
We experiment with supervised and semi-supervised methods on a relatively large data set from Yahoo!
"To our knowledge, there is no standard data-set of real questions and answers posted by online users, labeled for subjectivity orienta-tion."
"Hence, we had to create a dataset our-selves."
"To create our dataset, we downloaded more than 30,000 resolved questions from each of the following top-level categories of Yahoo!"
"Answers: Arts, Education, Health, Science, and Sports."
"We randomly chose 200 questions from each category to create a raw dataset with 1,000 questions total."
"Then, we labeled the dataset with annotators from the Amazon’s Mechanical Turk service [URL_CITE] ."
"For annotation, each question was judged by 5 Mechanical Turk workers who passed a qualification test of 10 questions (labeled by ourselves) with at least 9 of them correctly marked."
The qualification test was required to ensure that the raters were sufficiently com-petent to make reasonable judgments.
"We grouped the tasks into 25 question batches, where the whole batch was submitted as the Mechanical Turk’s Human Intelligence Task (HIT)."
The batching of questions was done to easily detect the “random” ratings produced by irresponsible workers.
"That is, each worker rated a batch of 25 questions."
"While precise definition of subjectivity is elusive, we decided to take the practical per-spective, namely the &quot;majority&quot; interpreta-tion."
The annotators were instructed to guess orientation according to how the question would be answered by most people.
"We did not deal with multi-part questions: if any part of question was subjective, the whole ques-tion was labeled as subjective."
"The gold stan-dard was thus derived with the majority strat-egy, followed by manual inspection as a “san-ity check”."
"At this stage we removed 22 ques-tions with undeterminable meaning, including gems such as “Upward Soccer"
Shorts?” [Footnote_5] and “1+1=?fdgdgdfg?” [URL_CITE] .
"Fi-nally, we create a labeled dataset consisting of 978 resolved questions, available online [Footnote_7] ."
Table 1 reports the statistics of the annotated dataset.
"The overall inter-annotator percentage agreement between Mechanical Turk workers and final annotation is 0.777, indicating that the task is difficult, but feasible for humans to annotate manually."
The statistics of our labeled sample show that the vast majority of the questions in all categories except for Science are subjective in nature.
The relatively high ratio of subjective questions in the Science category is surprising.
"However, we find that users often post polem-ics and statements instead of questions, using CQA as a forum to share their opinions on controversial topics."
"Overall, we were struck by the expressed need in Subjective informa-tion, even for categories such as Health and Education, where objective information would intuitively seem more desirable."
"For the subjectivied experiments to follow, we attempt to capture the linguistic characteristics identified in previous work (Section 5) in a lightweight and robust manner, due to the informal and noisy nature of CQA."
"In particular, we use the following feature classes, computed separately over question and answer content:  Character 3-grams  Words  Word with Character 3-grams  Word n-grams (n&lt;=3, i.e. W i , W i W i+1 , W i"
"W i+2 )  Word and POS n-gram (n&lt;=3, i.e. W i , W i W i+1 , W i POS i+1 ,"
"POS i W i+1 , POS i POS i+1 , etc.)."
"We use the character 3-grams features to overcome spelling errors and problems of ill-formatted or ungrammatical questions, and the POS information to capture common pat-terns across domains, as words, especially the content words, are quite diverse in different topical domains."
"For word and character 3-gram features, we consider two different ver-sions: case-sensitive and case-insensitive."
Case-insensitive features are assumed to be helpful for mitigating negative effects of ill-formatted text.
"Moreover, we experimented with three term weighting schemes: Binary, TF, and TF*IDF."
"Term frequency (TF) exhibited bet-ter performance in our development experi-ments, so we use this weighting scheme for all the experiments in Section 4."
"Regarding features: both words and structure of the text (e.g., word order) can be used to infer subjec-tivity."
"Therefore, the features we employ, such as words and word n-grams, are ex-pected to be useful as a (coarse) proxy to grammatical and phrase features."
"Unlike tra-ditional work on news-like text, the text of CQA and has poor spelling, grammar, and heavily uses non-standard abbreviations, hence our decision to use character n-grams."
"Metrics: Since the prediction on both sub-jective questions and objective questions is equally important, we use the macro-averaged F1 measure as the evaluation met-ric."
This is computed as the macro average of F1 measures computed for the Subjective and Objective classes individually.
The F1 meas-ure for either class is computed as 2⋅ Precision⋅ Recall .
"Methods compared: We compare our ap-proach with both the base supervised learning, as well as GE, a state-of-the-art semi-supervised method:  Supervised: we use the LibSVM im-plementati[REF_CITE]with linear kernel.  GE:"
"This is a state-of-the-art semi-supervised learning algorithm, General-ized Expectation (GE), introduced[REF_CITE]that incorporates model expectations into the objective functions for parameter estimation.  CoCQA:"
Our method (Section 3).
"For semi-supervised learning experiments, we selected a random subset of 2,000 unla-beled questions for each of the topical catego-ries, for the total of 10,000 unlabeled questions."
"First we report the performance of our Super-vised baseline system with a variety of fea-tures, reporting the average results of 5-fold cross validation."
Then we investigate the per-formance to our new CoCQA framework under a variety of settings.
"Table 2 reports the classification perform-ance for varying units of representation (e.g., question text vs. answer text) and the varying feature sets."
"We used case-insensitive features and TF (term frequency within the text unit) as feature weights, as these two settings achieved the best results in our development experi-ments."
"The rows show performance consider-ing only the question text (question), the best answer (best_ans), text of all answers to a question (all_ans), the text of the question and the best answer (q_bestans), and the text of the question with all answers (q_allans), re-spectively."
"In particular, using the words in the question alone achieves F1 of 0.717, com-pared to using words in the question and the best answers text (F1 of 0.695)."
"For compari-son, a naïve baseline that always guesses the majority class (Subjective) obtains F1 of 0.398."
"With character 3-gram, our system achieves performance comparable with words as fea-tures, but combining them together does not improve performance."
"We observe a slight gain with more complicated features, e.g. word n-gram, or word and POS n-grams, but the gain is not significant, and hence not worth the increased complexity of the feature generation."
"Finally, combining question text with answer text does not improve performance."
"Interestingly, the best answer itself is not as effective as the question for subjectivity analysis, nor is using all of the answers sub-mitted."
"One possible reason is that approxi-mately 40% of the best answers were chosen by the community and not the asker herself, are hence not necessarily representative of the asker intent."
Table 3 reports the supervised subjectivity classification performance for each question category with word features.
"The overall clas-sification results are significantly lower com-pared to training and testing on the mixture of the questions drawn from all categories, likely caused by the small amount of labeled training data for each category."
"Another pos-sibility is that the subjectivity clues are not topical and hence are not category dependent, with the possible exception of the questions in the Health domain."
"As words are simple and effective features in this experiment, we will use them in the subsequent experiments."
"Furthermore, the feature set using the words in the question with best answer together (q_bestans) exhibit higher performance than question with all answers (q_allans)."
"Thus, we will only con-sider questions and best answers in the fol-lowing experiments with GE and CoCQA."
"We now focus on investigating the effec-tiveness of CoCQA, our co-training-based framework for community question answer-ing analysis."
Table 4 summarizes the main results of this section.
"The values for CoCQA are derived with the parameter settings: K=100, X=0.001."
"These optimal settings are chosen after comprehensive experiments with differ-ent combinations, described later in this sec-tion."
GE does not exhibit a significant im-provement over Supervised.
"In contrast, CoCQA performs significantly better than the purely supervised method, with F1 of 0.745 compared to the F1 of 0.717 for Supervised."
"While it may seem surprising that a semi-supervised method outperforms a supervised one, note that we use all of the available la-beled data as provided to the Supervised method, as well as a large amount of unlabeled data, that is ultimately responsible for the per-formance improvement."
"As an added advantage, CoCQA approach is also practical."
"In a realistic application, we have two different situations: offline and online."
"With online processing, we may not have best answers available to predict ques-tion’s orientation, whereas we can employ in-formation from best answers in offline setting."
Co-training is a solution that is applicable to both situations.
"With CoCQA, we have two classifiers using the question text and the best answer text, respectively."
"We can use both of them to obtain better results in the offline set-ting, while in online setting, we can use the text of the question alone."
"In contrast, GE may not have this flexibility."
We now analyze the performance of CoCQA under a variety of settings to derive optimal parameters and to better understand the performance.
Figure 3 reports the perform-ance of CoCQA with varying the K parameter from 20 to 200.
"In this experiment, we fix X to be 0.001."
The combination of question and best answer is superior to that of question and all answers.
"When K is 100, the system obtains the best result, 0.745."
Figure 4 reports the number of co-training iterations needed to converge to optimal per-formance.
"While a vali-dation set should have been used for CoCQA parameter tuning, Figures 3 and 4 indicate that CoCQA is not sensitive to the specific parameter settings."
"In particular, we observe that any K is greater than 100, and for any number of iterations R is greater than 10, CoCQA exhibits in effectively equivalent per-formance."
"Figure 5 reports the performance of CoCQA for varying the number of labeled examples from 50 to 400 (that is, up to 50% of the available labeled training data)."
"Note that for this comparison we use the same fea-ture sets (words in question and best answer text), but using only the (varying) fractions of the manually labeled data."
"Surprisingly, CoCQA exhibits comparable performance of F1=0.685 with only 200 labeled examples are used, compared to the F1=0.695 for Super-vised with all 800 labeled training examples on this feature set."
"In other words, CoCQA is able to achieve comparable performance to supervised SVM with only one quarter of the labeled training data."
"Question analysis, especially question classifi-cation, has been long studied in the question answering research community."
"However, most of the previous research primarily con-sidered factual questions, with the notable ex-ception of the most recent TREC opinion QA track."
"Furthermore, the questions were specifi-cally designed for benchmark evaluation."
"A related thread of research considered deep analysis of the questions (and corresponding sentences) by manually classifying questions along several orientation dimensions, notably[REF_CITE]."
"In contrast, our work focuses on analyzing real user questions posted in a question answering community."
"These questions are often complex or subjec-tive, and are typically difficult to answer automatically as the question author probably was not able to find satisfactory answers with quick web search."
"Automatic complex question answering has been an active area of research, ranging from simple modification to factoid QA techniques (e.g.,[REF_CITE]) to knowledge intensive approaches for specific domains (e.g.,[REF_CITE])."
"However, the technology does not yet exist to automatically answer open-domain complex and subjective questions."
"While there has been some recent research (e.g.,[REF_CITE]) on retrieving high quality answers from CQA archives, the subjectivity orientation of the questions has not been considered as a feature for ranking."
A related corresponding problem is com-plex QA evaluation.
"Recent efforts at auto-matic evaluation show that even for well-defined, objective, complex questions, evaluation is extremely labor-intensive and introduces many challenges ([REF_CITE])."
As part of our contribution we showed that it is feasi-ble to use the Amazon Mechanical Turk ser-vice for evaluation by combining large degree of annotator redundancy (5 annotators per question) with more sparse but higher-quality expert annotation.
"The problem of automatic subjective ques-tion answering has recently started to be ad-dressed in the question answering commu-nity, most recently as the first opinion QA track[REF_CITE]."
"Unlike the con-trolled TREC opinion track (introduced in 2007), many of the questions in Yahoo!"
"An-swers community are inherently subjective, complex, ill-formed, or all of the above."
"To our knowledge, this paper is the first large-scale study of subjective/objective orientation of information needs, and certainly the first in the CQA environment."
A closely related research thread is subjec-tivity analysis at document and sentence level.
"For example, reference (Yu, H., and Hatzivassiloglou, V. 2003;[REF_CITE]) attempted to classify sentences into those reporting facts or opinions."
"Also related is research on sentiment analysis (e.g.,[REF_CITE]) where the goal is to classify a sen-tence or text fragment as being overall positive or negative."
"More generally,[REF_CITE]and subsequent work focused on the analysis of subjective language in narrative text, primarily news."
Our problem is quite dif-ferent in the sense that we are trying to iden-tify the orientation of a question.
"Nevertheless, our baseline method is similar to the methods and features used for sentiment analysis, and one of our contributions is evaluating the use-fulness of the established features and tech-niques to the new CQA setting."
"In order to predict question orientation, we build on co-training, one of the known semi-supervised learning techniques."
"Many models and techniques have been proposed for classi-fication, including support vector machines, decision tree based techniques, boosting-based techniques, and many others."
We use LIBSVM[REF_CITE]as a robust implemen-tation of SVM algorithms.
"In summary, while we draw on many tech-niques in question answering, natural language processing, and text classification, our work differs from previous research in that a) de-velop a novel co-training based algorithm for question and answer classification; b) we ad-dress a relatively new problem of automatic question subjectivity prediction; c) demon-strate the effectiveness of our techniques in the new CQA setting and d) explore the character-istics unique to CQA – while showing good results for a quite difficult task."
"We presented CoCQA, a co-training frame-work for modeling the textual interactions in question answer communities."
"Unlike previous work, we have focused on real user questions (often noisy, ungrammatical, and vague) sub-mitted in Yahoo!"
"Answers, a popular commu-nity question answering portal."
"We demon-strated CoCQA for one particularly important task of automatically identifying question sub-jectivity orientation, showing that CoCQA is able to exploit the structure of questions and corresponding answers."
"Despite the inherent difficulties of subjectivity analysis for real user questions, we have shown that by applying CoCQA to this task we can significantly im-prove prediction performance, and substan-tially reduce the size of the required training data, while outperforming a general state-of-the-art semi-supervised algorithm that does not take advantage of the CQA characteris-tics."
"In the future we plan to explore more so-phisticated features such semantic concepts and relationships (e.g., derived from WordNet or Wikipedia), and richer syntactic and lin-guistic information."
We also plan to explore related variants of semi-supervised learning such as co-boosting methods to further im-prove classification performance.
We will also investigate other applications of our co-training framework to tasks such as sentiment analysis in community question answering and similar social media content.
This paper explores the use of set expan-sion (SE) to improve question answering (QA) when the expected answer is a list of entities belonging to a certain class.
"Given a small set of seeds, SE algorithms mine textual re-sources to produce an extended list including additional members of the class represented by the seeds."
We explore the hypothesis that a noise-resistant SE algorithm can be used to extend candidate answers produced by a QA system and generate a new list of answers that is better than the original list produced by the QA system.
We further introduce a hybrid ap-proach which combines the original answers from the QA system with the output from the SE algorithm.
Experimental results for several state-of-the-art QA systems show that the hy-brid system performs better than the QA sys-tems alone when tested on list question data from past TREC evaluations.
Question answering (QA) systems are designed to retrieve precise answers to questions posed in nat-ural language.
"A list question expects a list as its answer, e.g. Name the coffee-producing countries in South America."
The ability to answer list questions has been tested as part of the yearly TREC QA eval-uati[REF_CITE].
This paper focuses on the use of set expansion to improve list question answering.
"A set expansion (SE) algo-rithm receives as input a few members of a class or set, and mines various textual resources (e.g. web pages) to produce an extended list including addi-tional members of the class or set that are not in the input."
A well-known online SE system is Google Sets 1 .
"This system is publicly accessible, but since it is a proprietary system that might be changed at any time, its results cannot be replicated reliably."
"We ex-plore the hypothesis that a SE algorithm, when care-fully designed to handle noisy inputs, can be applied to the output from a QA system to produce an overall list of answers for a given question that is better than the answers produced by the QA system itself."
"We propose to exploit large, redundant sources of struc-tured and/or semi-structured data and use linguistic analysis to seed a shallow analysis of these sources."
This is a hard problem since the linguistic evidence used as seeds is noisy.
"More precisely, we combine the QA system Ephyra[REF_CITE]with the SE system SEAL[REF_CITE]to create a hybrid approach that performs better than either system by itself when tested on data from the[REF_CITE]-15 evaluations."
"In addition, we apply our SE algorithm to answers generated by the five QA systems that performed the best on the list questions in the[REF_CITE]evaluation and report improvements in F [URL_CITE] scores for four of these systems."
Section 2 of the paper gives an overview of the QA and SE systems used for our experiments.
"Sec-tion 3 describes how the SE system was adapted to deal with noisy seeds produced by QA systems, and Section 4 presents the details of the experimental de-sign."
"Experimental results are discussed in Section 5, and the paper concludes in Section 6 with a dis-cussion of planned future work."
Ephyra[REF_CITE]is a QA system that has been evaluated in the TREC QA track[REF_CITE].
The system combines three answer extrac-tion techniques for factoid and list questions: (1) an answer type classification approach; (2) a syntactic pattern learning and matching approach; and (3) a semantic extractor that uses a semantic role label-ing system.
The answer type based extractor clas-sifies questions by their answer types and extracts candidates of the expected types.
The Ephyra pat-tern matching approach learns textual patterns that relate question key terms to possible answers and applies these patterns to candidate sentences to ex-tract factoid answers.
The semantic approach gener-ates a semantic representation of the question that is based on predicate-argument structures and extracts answer candidates from similar structures in the cor-pus.
"The source code of the answer extractors is in-cluded in OpenEphyra, an open source release of the system. [URL_CITE]"
"The answer candidates from these extractors are combined and ranked by a statistical answer selec-tion framework[REF_CITE], which estimates the probability of an answer based on a number of answer validation and similarity features."
"Valida-tion features use resources such as gazetteers and Wikipedia to verify an answer, whereas similarity features measure the syntactic and semantic simi-larity to other candidates, e.g. using string distance measures and WordNet relations."
Set expansion (SE) refers to expanding a given par-tial set of objects into a more complete set.
SEAL [URL_CITE][REF_CITE]is a SE system which ac-cepts input elements (seeds) of some target set S t and automatically finds other probable elements of S t in semi-structured documents such as web pages.
"SEAL also works on unstructured text, but its ex-traction mechanism benefits from structuring ele-ments such as HTML tags."
"The algorithm is in-dependent of the human language from which the seeds are taken, and also independent of the markup language used to annotate the documents."
Examples of SEAL’s input and output are shown in Figure 1.
"In more detail, SEAL comprises three major com-ponents: the Fetcher, the Extractor, and the Ranker."
The Fetcher focuses on retrieving web pages.
The URLs of the web pages come from top results re-trieved from Google and Yahoo! using the concate-nation of all seeds as the query.
"The Extractor au-tomatically constructs page-specific extraction rules, or wrappers, for each page that contains the seeds."
"Every wrapper is defined by two character strings, which specify the left-context and right-context nec-essary for an entity to be extracted from a page."
These strings are chosen to be maximally-long con-texts that bracket at least one occurrence of every seed string on a page.
"Most of the wrappers con- tain HTML tags, which illustrates the importance of structuring information in the source documents."
All entity mentions bracketed by these contextual strings derived from a particular page are extracted from the same page.
"Finally, the Ranker builds a graph, and then ranks the extracted mentions glob-ally based on the weights computed by performing a random graph walk."
"An example graph is shown in Figure 2, where each node d i represents a document, w i a wrapper, and m i an extracted entity mention."
"The graph mod-els the relationship between documents, wrappers, and mentions."
"In order to measure the relative im-portance of each node within the graph, the Ranker performs a graph walk until all node weights con-verge."
The idea is that nodes are weighted higher if they are connected to many other highly weighted nodes.
We apply this SE algorithm to answer candidates for list questions generated by Ephyra and other TREC QA systems to find additional instances of correct answers that were not in the original candi-date set.
SEAL was originally designed to handle only rele-vant input seeds.
"When provided with a mixture of relevant and irrelevant answers from a QA system, the performance would suffer."
"In this section, we propose three modifications to SEAL to improve its ability to handle noisy input seeds."
"For each expansion, SEAL’s fetcher concatenates all seeds and sends them as one query to the search engines."
"However, when the seeds are noisy, the documents fetched are constrained by the irrele-vant seeds, which decreases the chance of finding good documents."
"To overcome this problem, we designed an aggressive fetcher (AF) that increases the chance of composing queries containing only relevant seeds."
It sends a two-seed query for ev-ery possible pair of seeds to the search engines.
"If there are n input seeds ¡ , ¢ then the total number of queries sent would be n2 ."
"For example, suppose SEAL is given a set of noisy seeds: Boston, Seattle and Carnegie-Mellon (assuming Carnegie-Mellon is irrelevant), then by using AF, one query will contain only relevant seeds (as shown in Table 1)."
The docu-ments are then collected and sent to SEAL’s extrac-tor for learning wrappers.
SEAL’s extractor requires the longest common contexts to bracket at least one instance of every seed per web page.
"However, when seeds are noisy, such common contexts usually do not exist or are too short to be useful."
"To solve this problem, we propose a lenient extractor (LE) which only requires the contexts to bracket at least one in-stance of a minimum of two seeds, instead of every seed."
This increases the chance of finding longest common contexts that bracket only relevant seeds.
"For instance, suppose SEAL is given the seeds from the previous example (Boston, Seattle and Carnegie-Mellon) and the passage below."
"Then the extractor would learn the wrappers shown in Table 2. “While attending a hearing in Boston City Hall, Alan, a professor at Boston University, met Tina, his former student at Seattle Univer-sity, who is studying at Carnegie-Mellon University Art School and will be working in Seattle City Hall.”"
"As illustrated, with lenient extraction, SEAL is now able to learn the second wrapper because it brackets one instance of at least two seeds (Boston and Seattle)."
This can be very helpful if the list question is asking for city names rather than univer-sity names.
"The extractor then uses these wrappers to extract additional answer candidates, by search-ing for other strings that fit into the placeholders of the wrappers."
Note that the example was simplified for ease of presentation.
The wrappers are actually character-based (as opposed to word-based) and are likely to contain HTML tags when generated from real web pages.
Most QA systems use keywords from the question to guide the retrieval of relevant documents and the ex-traction of answer candidates.
We believe these key-words are also important for SEAL to identify ad-ditional instances of correct answers.
"For example, if the seeds are George Washington, John Adams, and Thomas Jefferson, then without using any con-text from the question, SEAL would output a mix-ture of founding fathers and presidents of the U.S.A. To solve this problem, we devised a hinted expan-sion (HE) technique that utilizes the context given in the question to constrain SEAL’s search space on the Web."
This is achieved by appending keywords from the question to every query that is sent to the search engines.
"The rationale is that the retrieved documents will also match the keywords, which may increase the chance of finding those documents that contain our desired set of answers."
We conducted experiments in two phases.
"In the first phase, we evaluated the SE approach by apply-ing SEAL to answers generated by Ephyra."
"In the second phase, we evaluated the approach by apply-ing SEAL to the output from QA systems that per-formed the best on the list questions in the[REF_CITE]evaluation."
"In both phases, the answers found by SEAL were retrieved from the Web instead of the AQUAINT newswire corpus used in the TREC eval-uations."
"However, we rejected answers if they could only be found in the Web and not in the AQUAINT corpus to avoid an unfair advantage over the QA systems: TREC participants were allowed to extract candidates from the Web (or any other source), but they had to identify a supporting document in the AQUAINT corpus for each answer and thus could not return answers that were not covered by the cor-pus."
Preliminary experiments showed that we can ob-tain a good balance between the amount and quality of the documents fetched by using only rare ques-tion terms as hint words.
"In particular, we select the three question words that occur least frequently in a sample of the AQUAINT corpus as hints."
"The can-didate answers were evaluated by using the answer keys, composed of regular expression patterns, ob-tained from the TREC website."
We did not extend the patterns with additional correct answers found in our experiments.
These answer keys were not offi-cially used in the TREC evaluation; thus the baseline scores we computed for Ephyra and other QA sys-tems in our experiments are slightly different from those officially reported.
"We evaluated our SE approach on Ephyra using the list questions[REF_CITE]and 15 (55, 93, and 89 questions, respectively)."
"For each question, the top four answer candidates from Ephyra were given as input seeds to SEAL."
"Initial experiments showed that by adding additional seeds, the effectiveness of our approach can be improved at the expense of a longer runtime."
We report both mean average precision (MAP) and F 1 scores.
"For the F 1 scores, we drop answer candidates with low confidence scores by applying a relative cut-off threshold: an answer candidate is dropped if the ratio of its confidence score and the score of the top answer is below a threshold."
An optimal threshold for a question is a threshold that maximizes the F 1 score for that particular question.
"For each TREC dataset, we conducted three ex-periments: (1) evaluation of answer candidates us-ing MAP; (2) evaluation using average F 1 with an optimal threshold for each question; and (3) eval-uation using average F 1 with thresholds trained by 5-fold cross validation."
"For each of those 5-fold val-idations, only one threshold was determined for all questions in the training folds."
"We evaluated two SE approaches, SEAL and Google Sets, on the five QA systems that performed the best on the list questions[REF_CITE]."
"For each question, the top four answer candidates [Footnote_4] from those systems were given as input seeds to SEAL and Google Sets."
"Unlike the candidates found by Ephyra, these can-didates were provided without confidence scores; hence, we assumed they all have a score of 1.0."
"In our experiments with SEAL, we first determined a single threshold that optimizes the average of the F 1 scores of the top five systems in both[REF_CITE]and 14."
We then obtained evaluation results for the top systems[REF_CITE]by using this trained threshold.
"When performing hinted expansion, the keywords (or hint words) for each question were extracted by Ephyra’s question analysis component."
"In our exper-iments with Google Sets, we requested Small Sets of items and again measured the performance in terms of F 1 scores."
We also tried requesting Large Sets but the results were worse.
"In Tables 3 and 4, we present evaluation results for all answers from Ephyra, only the top four answers, and various configurations of SEAL using the top four answers as seeds."
"Table 3 shows the MAP for each dataset ([REF_CITE]and 15), and Table 4 shows for each dataset the average F 1 score when using optimal per-question thresholds."
The results indicate that SEAL achieves the best performance when configured with all three proposed extensions.
"In terms of MAP, the best-configured SEAL im-proves the quality of the input answers (relatively) by 65%, 116%, 110% for each dataset respectively, and improves Ephyra’s overall performance by 36%, 30%, 41%."
"In terms of optimal F 1 , SEAL improves the quality of the input answers by 55%, 77%, 76% and Ephyra’s overall performance by 14%, 9%, 14% respectively."
"These results illustrate that a SE sys-tem is capable of improving a QA system’s perfor-mance on list questions, if we know how to select good thresholds."
"In practice, the thresholds are unknown and must be estimated from a training set."
"Table 5 shows eval-uation results using 5-fold cross validation for each dataset ([REF_CITE]and 15) independently, and the combination of all three datasets (All)."
"For each validation, we determine the threshold that maxi-mizes the F 1 score on the training folds, and we also determine the F 1 score on the test fold by ap-plying the trained threshold."
We repeat this valida-tion for each of the five test folds and present the av-erage threshold and F 1 score for each configuration and dataset.
The F 1 scores give an estimate of the performance on unseen data and allow a fair com- parison across systems.
"Here, we also introduce a hybrid system (Hybrid) that intersects the answers found by both systems by multiplying their proba-bilistic scores."
"Tables 3, 4, and 5 show that the effectiveness of the SE approach depends on the quality of the initial answer candidates."
"The improvements are most ap-parent for the[REF_CITE]dataset, where Ephyra has a much higher performance compared[REF_CITE]and 15."
"However, the best-configured SEAL did not improve the F 1 score[REF_CITE]as reported in Table 5."
We suspect that this is due to the compar-atively low quality of Ephyra’s top four answers for this dataset.
"The experiments also illustrate that by intersecting the answer candidates found by Ephyra and SEAL, we can eliminate poor answer candi-dates and partially compensate for the low preci-sion of Ephyra on the harder TREC datasets."
"How-ever, this comes at the expense of a lower recall, which slightly hurts the performance on the compar-atively easier[REF_CITE]questions."
"We also evaluated Google Sets on top four answers from Ephyra[REF_CITE]-15 and obtained F 1 scores of 12%, 11%, and 9% respectively (compared to 29%, 17%, and 16% for our hybrid approach with trained thresh-olds)."
Table 6 shows F 1 scores for the SE approach applied to the output from the five QA systems with the highest performance on the list questions[REF_CITE].
"Again, Hybrid intersects the answers found by the QA system and SEAL by multiplying their confidence scores."
Two thresholds were trained separately on the top five systems in both[REF_CITE]and 14; one for SEAL (0.2376) and another for Hy-brid (0.2463).
"As shown, the performance of Google Sets is worse than SEAL and Hybrid, but better than the top four answers on average."
"We believe our SE system outperforms Google Sets because we have methods to handle noisy inputs (i.e. AF, LE) and a method for guiding the SE algorithm to search in the right space on the Web (i.e. HE)."
The results show that both SEAL and Hybrid are capable of improving four out of the five systems.
We observed that one reason why SEAL did not im-prove “lcc[REF_CITE]” was the incompleteness of the an-swer keys.
Table 7 shows one of many examples where SEAL was penalized for finding additional correct answers.
"As illustrated, Hybrid improved all systems except “NUSCHUAQA1”."
"The reason is that even though SEAL improved the baseline, their overlapping answer set is too small; thus hurt-ing the recall of Hybrid substantially."
"Unfortunately, for the top[REF_CITE]systems we only had access to the answers that were actually submitted by the par-ticipants, whereas for Ephyra we could utilize the entire list of generated answer candidates, includ-ing those that fell below the cutoff threshold for list questions."
"Nevertheless, the hybrid approach could improve the baseline by more than 2% on average in terms of F 1 score."
Table 8 shows that the best-configured SEAL is capable of expanding only the relevant seeds even when given a set of noisy seeds.
Neither Google Sets nor the original SE algorithm without the proposed extensions could expand these seeds with additional candidates.
"On average, SEAL required about 5 seconds for querying the search engines, 10 seconds for crawl-ing the[REF_CITE]seconds for extracting answer can-didates from the web pages, and 5 seconds for rank-ing the candidates."
Note that the SE system has not been optimized extensively.
The runtime of the web page retrieval step and much of the search is due to network latency and can be reduced if the search is performed locally.
We have shown that our SE approach is capable of improving the performance of QA systems on list questions by utilizing only their top four answer can-didates as seeds.
We have also illustrated a feasible and effective method for integrating a SE approach into any QA system.
"We would like to emphasize that for each of the experiments we conducted, all that the SE system received as input were the top four noisy answers from a QA system and three key-words from the TREC questions."
We have shown that higher quality candidates support more effec-tive set expansion.
"In the future, we will investigate how to utilize more answer candidates from the QA system and determine the minimal quality of those candidates required for SE approach to make an im-provement."
"We have also shown that, in terms of F 1 scores with trained thresholds, the hybrid method improves the Ephyra QA system on all datasets and also im-proves four out of the five systems that performed the best on the list questions[REF_CITE]."
"How-ever, the final list of answers only comprises candi-dates found by both the QA system and the SE al-gorithm."
"In future experiments, we will investigate other methods of merging answer candidates, such as taking the union of answers from both systems."
"We expect further improvements from adding can-didates that are found only by the QA system, but it is unclear how the confidence measures from the two systems can be combined effectively."
"We would also like to emphasize that the SE ap-proach is entirely language independent, and thus can be readily applied to answer candidates in other languages."
"In future experiments, we will investi-gate its performance on question answering tasks in languages such as Chinese and Japanese."
"As pointed out previously, the performance of the SE approach highly depends on the accuracy of the seeds."
"However, QA systems are usually not op-timized to provide few high-precision results, but treat precision and recall as equally important."
"This leaves room for further improvements, e.g. by ap-plying stricter answer validation techniques to the seeds used for SE."
"We also plan to analyze the effectiveness of our approach across different question types and evalu-ate it on more complex questions such as the rigid list questions in the new TAC QA evaluation, which ask for opinion holders and subjects."
We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data.
The key aspects of this problem are the design of a di-alog information representation and a learning approach that supports capture of domain in-formation from in-domain dialogs.
"To represent a dialog for a learning purpose, we based our representation, the form-based di-alog structure representation, on an observa-ble structure."
"We show that this representation is sufficient for modeling phenomena that oc-cur regularly in several dissimilar task-oriented domains, including information-access and problem-solving."
"With the goal of ultimately reducing human annotation effort, we examine the use of unsupervised learning techniques in acquiring the components of the form-based representation (i.e. task, subtask, and concept)."
"These techniques include statis-tical word clustering based on mutual infor-mation and Kullback-Liebler distance, TextTiling, HMM-based segmentation, and bisecting K-mean document clustering."
"With some modifications to make these algorithms more suitable for inferring the structure of a spoken dialog, the unsupervised learning algo-rithms show promise."
"In recent dialog management frameworks, such as RavenClaw[REF_CITE]and Col-lagen[REF_CITE], domain-dependent com-ponents of a dialog manager are clearly separated from domain-independent components."
"This sepa- ration allows rapid development of a dialog man-agement module in a new task-oriented domain as dialog system developers can focus only on speci-fying domain-specific dialog information (e.g. the Dialog Task Specification in RavenClaw and Task Models in Collagen) while general dialog beha-viors (e.g. turn-taking, confirmation mechanism, and generic help) are provided by the framework."
"For task-oriented domains, the domain-specific dialog information is equivalent to task-specific information."
Examples of the task-specific infor-mation are steps in a task and domain keywords.
Specifying task-specific knowledge by hand is still a time consuming process[REF_CITE].
"Furthermore, the hand-crafted knowledge may not reflect users’ perceptions of a task[REF_CITE]."
"To reduce the subjectivity of system devel-opers, recorded conversations of humans perform-ing a similar task as a target dialog system have been used to help the developers design the task specification."
"Nevertheless, analyzing a corpus of dialogs by hand requires a great deal of human ef-fort[REF_CITE]."
This paper investi-gates the feasibility of automating this dialog analysis process through a machine-learning ap-proach.
"By inferring the task-specific dialog in-formation automatically from human-human interaction data, the knowledge engineering effort could be reduced as the developers need to only revise learned information rather than analyzing a large amount of data."
"Acquiring the task-specific knowledge from a corpus of human-human dialogs is considered a knowledge acquisition process, where the target task structure has not yet been specified but will be explored from data before a dialog system is built."
This is contrasted with a dialog structure recogni-tion process ([REF_CITE];
We use an unsupervised learning approach in our knowledge acquisition process as it can freely explore the structure in the data without any influ-ence from human supervision.
It is also interesting to see how well a machine-learning approach can perform on the problem of task-specific knowledge acquisition when no assump-tion about the domain is made and no prior know-ledge is used.
"Examination of task-oriented human-human di-alogs show that task-specific information can be observed in dialog transcription; therefore, it should be feasible to be infer it through an unsu-pervised learning approach."
Figure 1 (a) shows a dialog in an air travel domain.
"This dialog is orga-nized into three parts according to the three steps (i.e. reserve a flight, reserve a car, reserve a hotel) required to accomplish the task, creating a travel itinerary."
Domain keywords (highlighted in bold) required to accomplish each step are clearly com-municated.
"To infer task-specific knowledge from data us-ing an unsupervised learning approach, two prob-lems need to be addressed: 1) choosing an appropriate dialog representation that captures ob-servable task-specific knowledge in a dialog, and 2) developing an unsupervised learning approach that infers the task-specific knowledge modeled by this representation from in-domain human-human dialogs."
The first problem is discussed in Section 3 where a form-based dialog structure representa-tion is proposed.
"After describing the definition of each component in the form-based dialog structure representation, examples of how a domain expert models the task-specific information in a dialog with the form-based representation are given Sec-tion 3.1."
Then the annotation experiment which was used to verify that the form-based representa-tion can be understood and applied by other human annotators is discussed in Section 3.2.
"For the second problem, we modify existing unsupervised learning approaches to make them suitable for in-ferring the structure of a spoken dialog."
Section 4 describes these modifications and their perfor-mances when inferring the components of the form-based dialog structure representation from interaction data.
Automatic task-specific knowledge acquisition for configuring a dialog system is a relatively new re-search area.
Supervised learning approaches were used to acquire a task model for a collaborative agent[REF_CITE]and task-specific in-formation for a customer care service[REF_CITE].
These supervised algorithms were trained on rich knowledge sources (examples described in a specific annotation language and a well-organized website respectively) annotated by do-main experts.
"In contrast, the unsupervised concep-tual clustering algorithm in DIA-MOLE[REF_CITE]requires no additional human annotation to infer a set of domain-specific dialog acts from in-domain dialogs."
"The motivation behind the use of an unsupervised approach is similar to ours, to re-duce human effort in creating a new dialog system."
Many models have been proposed to account for the structure of a human-human conversation.
"Many such models focus on other aspects of a di-alog such as coordinated activities, i.e. turn-taking and grounding,[REF_CITE]and regular patterns in the dialog[REF_CITE]rather than the domain-specific information com-municated by participants."
More complicated di-alog representations[REF_CITE]model several aspects of a dialog including domain-specific information.
"However, additional components in these models, such as beliefs and intentions, are difficult to ob-serve directly from a conversation and, as for the current technology, may not be learnable through an unsupervised learning approach."
"Since the task-specific information that we would like to model will be used for configuring a dialog system, we can view this information from a dialog system perspective."
"Our dialog representa-tion is based on form, a data representation used in a form-based (or frame-based) dialog system."
A form is a simple representation that captures neces-sary task-specific information communicated through dialog.
This information is observable from dialog transcription (see below) and thus could be inferred through an unsupervised learning approach.
"Typically, a form corresponds to a database query form while slots in the form represent search criteria."
"Nevertheless, a form can represent related pieces of information required to perform any do-main action not just a database query action."
"With this more general definition of a form, a form-based dialog structure representation can be ap-plied to various types of task-oriented domains where dialog participants have to gather pieces of information, analogous to search criteria, through dialog in order to perform domain actions that ful-fill a dialog goal."
"In the form-based dialog structure representa-tion, task-specific information in each dialog is organized into a three-level structure of concept, subtask and task."
A concept is a word or a group of words which captures a piece of information re-quired to perform a domain action.
A subtask is a subset of a dialog which contains sufficient con-cepts to execute a domain action that advances a dialog toward its goal.
A task is a subset of a di-alog (usually the entire dialog) which contains all the subtasks that belong to the same goal.
A sub-task can also be considered as a step in a task.
"In terms of representation, a task is represented by a set of forms, one for each of its subtasks."
A con-cept is a slot in a form.
"To model the structure of a dialog in a new do-main with the form-based dialog structure repre-sentation, a list of tasks, subtasks, and concepts in that domain has to be specified."
This list is consi-dered a domain-specific tagset.
"The form-based dialog structure framework only provides the defi-nitions of these components (i.e. task, subtask, and concept), which can be regarded as meta-tags and are domain-independent."
"A list of tasks, subtasks, and concepts can be identified manually as shown in Section 3.1 or automatically through a machine-learning approach as discussed in Section 4."
"Sec-tion 3.1 illustrates how a domain expert models the task-specific information in two task-oriented do-mains, air travel planning (information-accessing) and map reading (problem-solving), with the form-based representation."
These examples also show that the form-based dialog structure representation is sufficient for modeling task-specific information in dissimilar domains.
"Nonetheless, by focusing on observable task-specific information and describing this informa-tion using a simple model, the form-based dialog structure representation cannot model the informa-tion that is not clearly expressed in a dialog."
Ex-ample of such information in an air travel domain is the pickup date of a car rental which may not be discussed in a dialog as it can be inferred from the arrival date of the corresponding flight.
"Further-more, the form-based representation is not well suited for modeling a complex dialog that has a dynamic structure such as a tutoring dialog."
Figure 1 illustrates how a dialog in the air travel doma[REF_CITE]can be represented with the form-based dialog structure representa-tion.
"A dialog in this domain usually has a single goal, to create an air-travel itinerary which may include hotel and car reservations."
"Thus, the entire dialog corresponds to one task."
"The dialog in Fig-ure 1 (a) contains three subtasks, one for each make_•_reservation action."
The forms that represent these subtasks are shown in Figure 1 (b) – (d).
Each form contains a set of concepts neces-sary for making the corresponding reservation.
"For a display purpose, the values of these slots are omitted."
A subtask can be further decomposed.
"For ex-ample, to reserve a round trip ticket, two database lookup actions, one for each leg, are required."
A reserve_flight subtask in Figure 1 is decomposed into two query_flight_info subtasks.
The corres-ponding forms of these subtasks are illustrated in Figure 2.
Each FlighInfo concept in the flight res- ervation form is a result of a database lookup ac-tion that corresponds to each flight query form.
Figure 3 show a dialog in the map reading do-ma[REF_CITE]and its corresponding form-based dialog structure representation.
The goal of a dialog in this domain is to have a route follower draw a route on his/her map according to a description given by a route giver.
"Since drawing an entire route involves several drawing strokes, a draw_a_route task is divided into several draw_a_segment subtasks, one for each drawing action."
This action required a set of concepts that describe a segment as shown in a segment descrip-tion form.
"Since the landmarks on the giver’s map can be different from those in the follower’s map, the participants have to explicitly define the Loca-tion of a mismatched Landmark before using it in a segment description."
In this case grounding be-comes another subtask and can be represented by a form.
This type of grounding is not necessary in the air travel domain.
"The goal of this annotation experiment is to verify that the form-based dialog structure framework can be understood by human annotators other than its developers, and that they can consistently apply the framework to model task-specific information in a dialog."
"In this experiment, each annotator had to design a form-based dialog structure representation for a given task-oriented domain by specifying a hierarchical structure of tasks, sub-tasks and con-cepts in that domain."
"Note that we are interested in the process of designing a domain-specific tagset from the definitions of task, subtask, and concept provided by the framework, not in the process of using an existing tagset to annotate data (see for example[REF_CITE])."
The description of the framework is provided in annotation guidelines along with examples from the domains that were not used in the experiment.
"The experimental procedure is as follows: the subjects first developed their own tagset according to the guidelines by analyzing a set of in-domain dialogs, and then annotate those dialogs with the tagset they had designed."
"To obtain enough anno-tated instances for each dialog structure component and to make the annotation simple, the dialog structure annotation part of the experiment was divided into two sub-parts: concept annotation and task/sub-task annotation."
"Two domains were used in the experiment, air travel planning and map reading."
Four subjects were assigned to each do-main.
None had used the scheme previously.
The average number of tags that each subject annotated is shown in the first row of Table 1.
"Since some variations in tagset designs are ac-ceptable as long as they conform to the guidelines, each subject’s annotation is judged against the guidelines rather than one specific reference anno-tation."
An annotation instance is marked as incor-rect only when it does not conform to the guidelines.
Each subject’s annotation was eva-luated by both a coding scheme expert and by oth-er subjects.
Accuracy is computed from the expert’s judgment while acceptability is computed from peers’ judgments.
Acceptability scores shown in Table 1 were averaged from all other subjects in the same group.
Please note that the result pre-sented in this table should not be compared to the results from machine-learning approaches pre- sented in Table 2 and Table 3 as the evaluation procedures and data sets are different.
Both accuracy and acceptability are high for all annotation tasks except for the accuracy of task/subtask annotation in the map reading domain.
Most of the errors come from the annotation of the grounding subtasks.
"Since its corresponding ac-tion is quite difficult to observe, subjects may not have a concrete definition of grounding and were more likely to produce errors."
"In addition, they were less critical when judging other subjects’ an-notations."
Consistency in applying the form-based dialog structure representation shows that the re-presentation is unambiguous and could potentially be identified through a machine-learning approach.
"When comparing among components, concepts were annotated more consistently than tasks and subtasks in terms of both accuracy and acceptabili-ty."
"One possible reason is that, a concept is easier to observe as its unit is smaller than a task or a sub-task."
"Moreover, dialog participants have to clearly communicate the concepts in order to execute a domain action."
"The subjects usually agreed on tasks and top-level subtasks, but did not quite agree on low-level subtasks."
"The low-level sub-tasks are correlated with the implementation of a dialog system; hence, the designs of these subtasks are more subjective and likely to be different."
This section describes machine-learning approach-es for inferring the task-specific information mod-eled by the form-based dialog structure representation from human-human conversations.
"Specifically, the learning approach has to infer a list of tasks, sub-tasks and concepts in a given do-main from in-domain dialogs similar to what a human does in Section 3."
"To make the problem tractable, components in the form-based represen-tation are acquired separately."
"For most task-oriented dialogs that we encountered, each dialog corresponds to one task."
"Hence, the learning effort can be focused on identifying concept and subtask."
"Since we can only observe instances or values of these components in a dialog, we have to first iden-tify these instances and then make a generalization for its type."
"For instant, to infer that there is a con-cept City in the air travel domain, a set of city names has to be identified and grouped together."
"To identify a set of domain concepts from the transcription of in-domain dialogs, we follow the algorithm described[REF_CITE]."
"This algorithm utilizes an unsuper-vised clustering algorithm which clusters words based on context similarity, e.g. mutual informa-tion-based and Kullback-Liebler-based clustering, since the members of the same domain concept are usually used in similar contexts in a particular do-main."
Examples of the clusters obtained from the KL-based clustering algorithm are shown in Figure 4.
"These clusters represent Hour, RentalCompa-ny, and City respectively."
Underlined cluster members belong to other concepts.
The clustering algorithm can identify all 12 members of Hour and about half of RentalCompany.
"In the third clus-ter, some airport names got merged with city names because they occur in quite similar context."
The rest of this section describes an approach for identifying subtasks and their corresponding forms in a given domain.
"We decided to simplify the form-learning problem by first segmenting a dialog into form-filling episodes (which are equiv-alent to sub-tasks), then grouping the ones that cor-respond to the same form together so that we can determine a set of necessary slots in each form from the concepts present in its corresponding cluster."
We further simplify the problem by con-centrating on the domains that have only one top-level task (though in principle the approach can be extended to the domains that have multiple top-level tasks).
"Since we utilize well-known unsuper-vised algorithms, only the modifications which are applied to make these algorithms suitable for infer-ring the structure of a spoken dialog are discussed."
Two unsupervised discourse segmentation algo-rithms are investigated: TextTiling[REF_CITE]and Hidden Markov Modeling[REF_CITE].
These algorithms only recover the sequence of subtasks but not the hierarchical structure of subtasks similar to Bangalore et al.’s (2006) chunk-based model.
"Nevertheless, this simplifica-tion is sufficient when a subtask is embedded at the beginning or the end of the higher-level subtask which is the case for most embedded structures we have found."
"Both algorithms, while performing well with expository text, require modifications when applying to a fine-grained segmentation problem of spoken dialogs."
"In WSJ text, the aver-age topic length is 428 words[REF_CITE]while in the air travel domain the average subtask length is 84 words (10 utterances)."
"For TextTiling, the modifications include a dis-tance weight and a data-driven stop word list."
"For the subtasks that are much shorter than the average length, distant words in the context window can be irrelevant."
A distance weight demotes the impor-tance of the context word that is far away from the considered boundary by giving it a lower weight.
"A manually prepared stop word list, containing common words, may not be suitable for every ap-plication domain."
We propose a novel approach that determines a list of stop words directly from word distribution in each data set.
TextTiling as-sumes that words that occur regularly throughout a dialog are not informative.
"However, the regularity of a particular word is determined from its distribu-tion over the dialog rather than from its frequency."
A high frequency word is useful if its instances occur only in a specific location.
"For example, the word “delta” which occurs many times in a re-serve_flight subtask but does not occur in other subtasks is undoubtedly useful for determining subtask boundaries while the word “you” which can occur anywhere in a dialog is not useful."
"Spe-cifically, a regularity count of word w is defined as the number of sliding context windows in the simi-larity score calculation of TextTiling that contain the word w in each dialog."
A data-driven stop word list contains words that have a regularity count greater than a pre-defined threshold.
"For HMM-based segmentation, we modified Barzilay and Lee’s (2004) content models by using larger text spans when inducing the HMM states."
HMM states are created automatically by cluster-ing similar text spans together.
"When using an ut- terance as a text span, it may not contain enough information to indicate its relevant subtask as some utterances in a task-oriented dialog are very short and can occur in any subtask (e.g. acknowledge-ments and yes/no responses)."
"Larger text spans, reference topics, were used[REF_CITE]."
"Nevertheless, this approach requires true segment boundaries."
"To eliminate the need of an-notated data in our algorithm, HMM states are in-duced from predicted segments generated by TextTiling instead."
"After segmenting all dialogs into sequences of subtasks, the bisecting K-means clustering algo-rithm[REF_CITE]is used to group the segments that belong to the same type of subtask together as they represent the same form type."
The clustering is done based on cosine similarity be-tween segments.
This unsupervised clustering al-gorithm is also used to infer a set of HMM states in the HMM-based segmentation described above.
Words are used as features for both segmenta-tion and clustering algorithms.
"If a set of domain concepts has already been identified, we can use this information to enhance the features."
"When concept annotation is available, we can incorporate a concept label into a representation of a concept word."
A Label+Word representation joins a word string and its label and can help disambiguate be-tween similar words that belong to different con-cepts.
"For instance, “one” in “that one” is not the same token as “[Hour]:one”."
"A Label representa-tion, on the other hand, only represents a concept word by its label."
"This representation is based on the assumption that a list of concepts occurring in one subtask is distinguishable from a list of con-cepts occurring in other subtasks regardless of the values of the concepts; hence, a concept label is more informative than its value."
This representa-tion provides an abstraction over all different val-ues of the same concept type.
"For example, [Airline]:northwest and [Airline]:delta are represented with the same token [Airline]."
"In all experiments, concept labels are provided by a do-main expert as we assume that a set of domain concepts has already been identified."
"To evaluate dialog segmentation performance, we compare predicted boundaries against subtask boundaries annotated by a domain expert."
Subtask boundaries could occur only at utterance bounda-ries.
Two metrics are used: P k[REF_CITE]and concept-based f-measure (C. F-1).
P k measures the probability of misclassifying two ut-terances that are k utterances apart as belonging to the same sub-task or different sub-tasks. k is set to half the average sub-task length.
C. F-1 is a mod-ification of the standard f-measure (a harmonic mean of precision and recall) that gives credit to some near misses.
"Since the segmented dialogs will later be used to identify a set of forms and their associated slots, the segment that contains the same set of concepts as the reference segment is acceptable even if its boundaries are slightly dif-ferent from the reference."
"For this reason, a near-miss counts as a match if there is no concept be-tween the near-miss boundary and the reference boundary."
We evaluated the proposed dialog segmentation algorithms with 24 dialogs from the air travel do-main and 20 dialogs from the map reading domain.
The window size for TextTiling was set to 4 utter-ances.
"The cut-off threshold for choosing subtask boundaries was set to μ - σ/2; where μ is the mean of the depth scores[REF_CITE], the relative change in word co-occurrence similarities on both sides of a candidate boundary, in each dialog and σ is their standard deviation."
We found that a small window size and a low cut-off threshold are more suitable for identifying fine-grained segments as in the case of subtasks.
"However, we also found that TextTiling is quite robust as varying these two pa-rameters doesn’t severely degrade its performance[REF_CITE]."
The threshold for selecting data-driven stop words was set to μ + 2*σ; where μ is the mean of the regularity counts of all the words in a given dialog and σ is their standard deviation.
The performance of TextTiling and HMM-based segmentation algorithm is shown in Table 2.
"Augmented TextTiling, which uses a data-driven stop word list, distance weights, and the Label+Word representation, performed significant-ly better than the baseline in both domains."
Each of these augmenting techniques can on their own im-prove segmentation performance but not signifi-cantly.
"Unsurprisingly, the proposed regularity counts discover stop words that are specific to spo- ken dialogs, but are absent from the hand-crafted list 1 , e.g. “okay” and “yeah”."
"For HMM-based segmentation, the segmenta-tion result obtained when modeling the HMM states from predicted subtasks generated by Text-Tiling (4 th row) is better than the result obtained when modeling the HMM states from utterances (3 rd row)."
Predicted segments provide more context to the clustering algorithm that induces the HMM states.
As a result a more robust state representa-tion is obtained.
A more efficient clustering algo-rithm can also improve the performance of the HMM-based segmentation algorithm since it pro-vides a state representation that better differentiates among dialog segments which belong to dissimilar subtasks.
"When the Label representation which yielded a better subtask clustering result (see Sec-tion 4.2) was used, HMM-based segmentation pro-duced a better result (5 th row) especially in the map reading domain."
These numbers may appear mod-est compared to the numbers obtained when seg-menting expository text.
"However, predicting the boundaries of fine-grained subtasks is more diffi-cult even with a supervised learning approach[REF_CITE]."
Our results are comparable to Arguello and Rosé’s (2006) results.
"Between the two segmentation algorithms, the HMM-based algorithm performed slightly worse than TextTiling in the air travel domain but per-formed significantly better in the map reading do-main."
"The HMM-based algorithm can identify more boundaries between fine-grained subtasks, which occur more often in the map reading do-main."
"TextTiling, which relies on local lexical co-hesion, is unlikely to find two significant drops in lexical similarity that are only a couple of utter-ances apart, and thus fails to detect boundaries of short segments."
"However, HMM-based segmenta- tion misses more boundaries between two subtask occurrences of the same type, which occurs more often in the air travel domain, as they are usually represented by the same state."
We evaluated the subtask clustering algorithm on the same data set used in the dialog segmentation evaluation.
Table 3 presents the quality score (QS) for each clustering result.
These QSs were obtained by comparing the output clusters against a set of reference subtasks.
See[REF_CITE]for the definition of QS.
"When predicted segments were clustered, the quality of the output (2 nd row) is not as good as when the reference segments were used ([URL_CITE] st row) as inaccurate segment boundaries affected the per-formance of the clustering algorithm."
"However, the qualities of subtasks that occur frequently are not much different."
"In terms of feature representation, the clustering algorithm that uses the Label repre-sentation achieved better performance in both do-mains."
"When the sets of concepts in all of the subtasks are disjoint, the clustering algorithm that uses the Label representation can achieve a very good result as in the map reading domain."
This result is even better than the result obtained when the reference segments were clustered by the algo-rithm that uses the Label+Word representation.
These results demonstrate that an appropriate fea-ture representation provides more useful informa-tion to the clustering algorithm than accurate segment boundaries.
"However, when the subtasks contain overlapping sets of concepts as in the air travel domain, the performance gain obtained from the Label representation is quite small."
Figure 5 shows four types of forms in the air travel domain that were acquired by the proposed form identification approach.
The slot names are taken from concept labels.
The number in paren-theses is slot frequency in the corresponding clus-ter.
The underlined slots are the ones that belong to other forms.
Some slots in the car query form are missing as some instances of its corresponding subtask get merged into other clusters.
The results presented in the previous sections show that existing unsupervised learning algorithms are able to identify components of the form-based di-alog structure representation..
"However, some modifications are required to make these algo-rithms more suitable for inferring the structure of a spoken dialog."
The advantages of different learn-ing algorithms can be combined to improve per-formance.
"For example, TextTiling and HMM-based segmentation are good at detecting different types of boundaries; therefore, combining the pre-dictions made by both algorithms could improve segmentation performance."
Additional features such as prosodic features could also be useful.
Subsequent steps in the learning process are sub-jected to propagation errors.
"However, the pro-posed learning algorithms, which are based on generalization of recurring patterns, are able to learn from inaccurate information given that the number of errors is moderate, so that there are enough correct examples to learn from."
"Given re-dundant information in dialog corpora, a domain knowledge acquisition process does not require high learning accuracy and an unsupervised learn-ing approach is reasonable."
The overall quality of the learning result is acceptable.
The proposed un-supervised learning approach can infer much use-ful task-specific dialog information needed for automatically configuring a task-oriented dialog system from data.
"To represent a dialog for a learning purpose, we based our representation, the form-based dialog structure representation, on observable informa-tion."
Components of the form-based representation can be acquired with acceptable accuracy from observable structures in dialogs without requiring human supervision.
We show that this dialog re-presentation can capture task-specific information in dissimilar domains.
"Additionally, it can be un-derstood and applied by annotators other than the developers."
Our investigation shows that it is feasible to au-tomatically acquire the domain-specific dialog in-formation necessary for configuring a task-oriented dialog system from a corpus of in-domain dialogs.
This corpus-based approach could potentially re-duce human effort in dialog system development.
A limitation of this approach is that it can discover only information present in the data.
"For instance, the corpus-based approach cannot identify city names absent in the corpus while a human devel-oper would know to include these."
Revision may be required to make learned information more ac-curate and complete before deployment; we expect that this effort would be less than the one required for manual analysis.
A detailed evaluation of cor-rection effort would be desirable.
"In this paper, task-specific knowledge was ac-quired from in-domain dialogs without using any prior knowledge about the domain."
"In practice, existing knowledge sources about the world and the domain, such as WordNet, could be used to improve learning."
Some human supervision can be valuable particularly in the form of semi-supervised learning and active learning.
In particu-lar a process that integrates human input at appro-priate times (for example seeding or correction) is likely to be part of a successful approach.
We introduce the relative rank differential sta-tistic which is a non-parametric approach to document and dialog analysis based on word frequency rank-statistics.
"We also present a simple method to establish semantic saliency in dialog, documents, and dialog segments using these word frequency rank statistics."
"Applica-tions of our technique include the dynamic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detec-tion in conversational speech and text."
"Our ap-proach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outper-formed term-frequency and TF-IDF cosine dis-tance approaches in several experiments con-ducted."
"Existing research in dialog analysis has focused on several specific problems including dialog act de-tection (e.g.,[REF_CITE]), segmenta-tion and chunking (e.g.,[REF_CITE]), topic detec-tion (e.g.,[REF_CITE]), distillation and summarization (e.g.,[REF_CITE]) etc."
The breath of this research reflects the increasing im-portance that dialog analysis has for multiple do-mains and applications.
"While historically, dialog analysis research has initially leveraged the corre-sponding techniques originally intended for textual document analysis, techniques tailored specifically for dialog processing eventually should be able to address the sparseness, noise, and time considera-tions intrinsic to dialog and conversations."
The approach proposed in this paper focuses on the relative change of rank ordering of words occur-ring in a conversation according to their frequen-cies.
Our approach emphasizes relatively improb-able terms by focusing on terms that are relatively unlikely to appear frequently and thus weighting their change in rank more once they are observed.
"Our technique achieves this in a non-parametric fashion without explicitly computing probabilities, without the assumption of an underlying distribu-tion, and without the computation of likelihoods."
"In general, non-parametric approaches to data analysis are well known and present several attrac-tive characteristics (as a general reference see[REF_CITE])."
Non-parametric ap-proaches require few assumptions about the data analyzed and can present computational advan-tages over parametric approaches especially when the underlying distributions of the data are not normal.
"In specific, our approach uses rank order statistics of word-feature frequencies to compute a relative rank-differential statistic."
This paper is organized as follows: in Section 2 we introduce and describe our basic approach (the relative rank differential RRD function and its sorted list).
In Section 3 we address the temporal nature of dialogs and describe considerations to dynamically update the RRD statistics in an on-line fashion especially for the case of shifting tem-poral windows of analysis.
In Section 4 we relate the RRD approach to relevant existing and previ-ous dialog and text analysis approaches.
In Section 5 we illustrate the usefulness of our metric by ana-lyzing a set of conversations in various ways using the RRD.
"Specifically, in that section we will em-pirically demonstrate its robustness to noise and data sparseness compared to the popular term fre-quency and TF-IDF cosine distance approaches in a dialog classification task."
"And finally, in Section 6, we present some concluding remarks and future directions"
"Let d u = {d [Footnote_1]u , d [Footnote_2]u , d [Footnote_3]u ...} denote the ranked dictionary of a language (i.e., the ordered list of words sorted in decreasing order of frequency)."
"1 We only consider at this point the case in which both speak-ers’ parts in a dialog interaction are considered jointly (i.e., single channel), however, our method can be easily extended to separate conversation channels. Also, for simplicity we consider at this point only words (or phrases) as features."
"2 For brevity, we refer to the Relative Rank Differential of a word given two utterances as a statistic. It is not, strictly speaking, a metric or a distance, but rather a function ."
"3 For simplicity, c is written without subscripts when these are apparent from the context ."
The superscript u denotes that this ranked list is based on the universal language.
"Specifically, the u word d i is the i th entry in d u if its frequency of u occurrence in the language denoted by f (d i ) is larger than f (d j ) for every j where i &lt; j (for u notational simplicity we assume that no two words share the same frequency)."
"In the case where want to relax this assumption we simply allow i &lt; j when f ( d i ) = f ( d ju ) as long as d iu precedes u u d j lexicographically, or under any other desired precedence criteria."
"For d u we assume that f (d i ) &gt; 0 for every entry (i.e., each word has u been observed at least once in the language)."
"Similarly, let now d S = {d 1 S , d 2 S , d 3 S ...} de-note the corresponding ranked dictionary for a dia-log, or dialog segment, S (ordered, as in the case of the language dictionary, in decreasing order of frequency) 1 ."
The superscript S denotes that this
S ranked list is based on the dialog S.
The word d i is the i th entry in d s if its frequency of occurrence
S in the conversation segment S denoted by f ( d i ) is larger than f ( d j ) for every j where i &lt; j .
In this case we allow f ( d i ) ≥ 0 for every i so
S that the cardinality of d u is the same as d s .
"Let r d ( w ) denote the rank of word w in the ranked dictionary d so that, for example, r (d u ) = i . du i"
"Based now on a dialog segment and a universal language, any given word w will be associated with a rank in d u (the universal ranked dictionary) and a rank in d s , the dialog segment ranked dic-tionary."
"Let us define now for every word the relative rank differential (RRD) function or statistic 2 given by: r u (w) − r (w) c , (w) = d ( r d s d u (w) ) α d s d u"
The relative rank-differential is the ratio of the absolute difference (or change) in rank between the word’s original position in the universal dictionary and the segment s. The exponent α in the de-nominator allows us to emphasize or deemphasize changes in terms according to their position or rank in the language (or universal) dictionary.
"Typically we will want to increase the denominator’s value (i.e., deemphasize) for terms that have very low frequency (and their rank value in the universal dictionary is large) so that only relatively big changes in rank will result in substantial values of this function."
"When alpha is zero, the RRD focuses on every word identically as we consider only the absolute change in rank."
"For alpha equal to 1.0 the relative change in rank gets scaled down linearly according to its rank, while for alphas larger than 1.0 the nu-merator will scale down or reduce to a larger extent the value of relative rank differential for words that have large rank value."
Based on each word’s relative rank differential we can compute the ranked list of words sorted in decreasing order by their corresponding value of relative rank differential.
"Let this sorted list of words be denoted by R ( d u , d S ) = { w 1 , w 2 ,...} ."
So that c ( w i ) is larger than c(w j ) 3 for every j where i &lt; j .
We now provide some intuition on the ranked RRD lists and the RRD function.
"The ranked dictionary of a language contains information about the frequency of all words in a language (i.e., across the universe of conversations) while the segment counterpart pertains a single conversation or segment thereof."
"The relative rank differential tells us how different a word is ranked in a conversation segment from the universal language, but this difference is normalized by the universal rank of the word."
"Intuitively, and especially when alpha equals 1.0, the RRD denotes some sort of percent change in rank."
This also means that this function is less sensitive to small changes in frequency in the case of frequent words and to small changes in rank in case of infrequent words.
"Finally, the sorted list R ( d u , d S ) contains in order of importance the most relatively salient terms of a dialog segment, as measured by relative changes or differences in rank."
"We now discuss how to extend the metrics de-scribed in the previous section to consider finite-time sliding windows of analysis, that is, we de-scribe how to update rank statistics, specifically the ranked lists and relative rank differential informa-tion for every feature in an on-line fashion."
"This is useful when tracking the evolution of single dia-logs, when focusing the analysis to span shorter regions, as well as to supporting dynamic real-time analytics of large number of dialogs."
"To approach this, we decompose the word events (words as they occur in time) into arriving and departing events."
"An arriving event at time t is a word that is covered by the analysis window at its specific time as the finite length window slides in time, and a departing word at time t is a feature that stops falling within the window of analysis."
"For simplicity, and without loss of generality, we now assume that we are performing the analysis in real time and that the sliding window of analysis spans from current time t back to (t-T), where T is the length of the window of analysis."
An arriving word at current time t falls into our current window of analysis and thus needs to be processed.
"To account for these events efficiently, we need a new structure: the temporal event FIFO list (i.e., a queue where events get registered) that keeps track of events as they arrive in time."
As an event (word w t ) arrives it is registered and proc-essed as follows: 1. Find the corresponding identifier of w t in the universal ranked dictionary and add it u as d i at the end of the temporal event list together with its time stamp. 2.
"The corresponding entry in d s , the ranked segment dictionary, is located through an index list that maps d i → d ks and the u segment frequency associated is incre-mented f (d ks ) = f (d ks ) +1 3. Verify if the rank of the feature needs to be updated in the segment rank list."
In other words evaluate whether f (d ks−1 ) &gt; f (d ks ) still holds true after the update.
If this is not true then shift feature up in the rank list (to a higher rank) and shift down the predecessor feature in the rank list.
"In this single shift-up-down operation, update the index list and the value of k. 4."
"For every feature shifted down in 3 down re-compute the relative rank differential RRD function and verify if its position needs to be modified in R ( d u , d S ) (a sec-ond index list is needed to compute this ef-ficiently). 5."
Repeat step 3 iteratively until feature is not able to push up any further in the ranked list.
The process for dealing with departing events is quite similar to the arriving process just described.
"Of course, as the analysis window slides in time, it is necessary to keep track of the temporal event FIFO list to make sure that the events at the top are removed as soon as they fall out of the analysis window."
The process is then: 1.
The departing event is identified and its corresponding identifier in the universal u ranked dictionary d i is removed from the top of the temporal event list. 2.
Its location in d s the ranked segment dic-tionary is located through the index list.
The corresponding segment frequency as-sociated is decreased as follows: f (d ks ) = f (d sk ) −1 . 3. Verify if the rank of the feature needs to be updated in the segment rank list.
In other words evaluate if f (d ks+1 ) &lt; f (d ks ) still holds true after the update.
"If not shift fea-ture down in rank (to a lower rank, denot-ing less frequent occurrence) and shift the successor feature up in the rank list."
"In this single shift up-down operation, update the index list and the value of k. 4."
"For every feature shifted up in step 3 re-compute the relative rank differential and verify if its location needs to be modified in R ( d u , d S ) 5."
Repeat step 3 until the feature is not able to shift down any further in the ranked list.
"The procedures just described are efficiently implementable as they simply identify entries in rank lists through index lists, update values by in-crementing and decrementing variables, and per-formed some localized and limited re-sorting."
"Ad-ditionally, simple operations like adding data at the end and removing data at the beginning of the FIFO list are needed making it altogether computa-tionally inexpensive."
Our work relates to several existing techniques as follows.
"Many techniques of text and dialog analy-sis utilize a word frequency vector based approach (e.g.,[REF_CITE]) in which lexical fea-tures counts (term frequencies) are used to popu-late the vector."
Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF).
"The TF-IDF and TF metrics are the base of other ap-proaches like discriminative classification ([REF_CITE]; and[REF_CITE]), Text Till-ing or topic chains[REF_CITE], and latent semantic indexing[REF_CITE]."
"Ultimately, these types of approaches are the foun-dation of complex classification and document un-derstanding systems which use these features to-gether with possibly more sophisticated classifica-tion algorithms (e.g., D’[REF_CITE])."
"When using TF and TF-IDF approaches, it is im-portant to notice that by normalizing the term fre-quency by the document length, TF-based ap-proaches are effectively equivalent to estimation of a multinomial distribution."
The variance of the es-timate will be larger as the number of observations decreases.
"Recently, approaches that explicitly es- tablish this parametric assumption and perform parameter inference have been presented[REF_CITE]."
This work is an example of the potential complexity associated when performing parameter inference.
"The area of adaptation of frequency parameters for ASR, specifically the work[REF_CITE], is relevant to our work in the sense that both ap-proaches emphasize the importance of and present a method to update the lexical or semantic feature statistics on-line."
"In the area of non-parametric processing of dialog and text, the work[REF_CITE], is very close to the work in this paper as it deals with non-parametric statistics of the word frequencies (rank of occurrences) and uses the Spearman’s Correla-tion Coefficient."
"Our work differs from this ap-proach in two ways: first, the Relative Rank Dif-ferential tells us about the relative change in rank (while SCC focuses in the absolute change) and secondly, from the ranked RDD list, we can iden-tify the saliency of each term (as opposed to sim-ply computing the overall similarity between two passages)."
"In order to illustrate the application of the RRD statistic, we conducted two sets of experiments based on conversations recorded in a large cus-tomer contact center for an American car manufac-turer."
In the first group of experiments we took a corpus of 258 hand transcribed dialogs and con-ducted classification experiments using the basic RRD statistic as feature.
We compared its per-formance against term frequency and TF-IDF based cosine distance approaches.
The second set of experiments is based on ASR transcribed speech and for this we used a second corpus consisting of a set of 44 conversations spanning over 3 hours of conversational speech.
"In the first set of experiments we intend to illus-trate two things: first the usefulness of RRD as a feature in terms of representational accuracy and second, its robustness to noise and data sparseness compared to other popular features."
In the second set of experiments we illustrate the versatility and potential of our technique to be applied in dialog-oriented analysis.
For this set of experiments we used a corpus of 258 hand transcribed conversations.
Each dialog was treated like a single document.
"Using the set of dialogs we constructed different query vectors and affected these queries using various noise condi-tions, and then we utilized these vectors to perform a simple document query classification experiment."
We measured the cosine distance between the noisy query vector and the document vector of each document in this corpus.
A noisy query is constructing by adding zero mean additive gaus-sian noise to the query vector with amplitude pro-portional to the value of a parameter N and with floor value of zero to avoid negative valued fea-tures.
"We allow, in these experiments, for counts to have non-integer values; as the dialog becomes larger, the Gaussian assumption holds true due to the Central Limit Theorem, independently of the actual underlying distribution of the noise source."
"This distortion is intended to mimic the variation between two similar dialogs (or utterances) that are essentially similar, except for a additive zero mean random changes."
A good statistic should be able to show robustness to these types of distortions.
A correct match is counted when the closest match for each query is the generating document.
"Table 1 shows the percent correct matches for the TF, TF-IDF and Relative Rank Differential fea-tures, under various levels of query noise."
"As we can see, in clean conditions the accuracy of the 3 features is quite high but as the noise conditions increase the accuracy of the 3 techniques decreases substantially."
"However, the TF feature is much more sensitive to noise than the other two tech-niques."
We can see that our technique is better than both TF and TF-IDF in noisy conditions.
We also conducted experiments to test the com-parative robustness or the RRD feature to query data sparseness.
"To measure this, we evaluated the accuracy in query-document match when using a random subset of the document as query."
"Figure 1 show the results of this experiment using the RRD feature, the Term Frequency, and the TF-IDF fea-ture vectors."
"We can see that with as little as 5% of the document size as query, the RRD achieves close to 90% accuracy while the TF-IDF feature needs up to 20% to achieve the same performance, and the TF counts only need close to 70%."
These results empirically demonstrate that RRD statistics are more robust to noise and to term cov-erage sparseness than TF and TF-IDF.
For the experiments of this section we used 44 dia-logs.
Manual transcriptions for these 44 conversa-tions were obtained in order to evaluate the speech recognition accuracy.
"While we could have used the manual transcripts to perform the analysis, the results reported here are based on the recognition output."
The reason for using ASR transcripts as opposed to human transcription is that we wanted to evaluate how useful our approach would be in a real ASR based solution dealing with large amounts of noisy data at this level of ASR error.
Each dialog was recorded in two separate channels (one for the agent and one for the customer) and automatically transcribed separately using a large vocabulary two-stage automatic speech recognizer system.
"In the first stage, a speaker independent recognition pass is performed after which the re-sulting hypothesis is used to compensate and adapt feature and models."
Using the adapted feature and models the second stage recognition is performed.
"After recognition, the single best hypothesis with time stamps for the agent and customer are weaved back together."
The overall Word Error Rate is about 24% and var-ies significantly between the set of agents and the set of customers (the set of agents being more ac-curate).
The universal dictionary we used consists exclu-sively of the words occurring in the corpus which total 2046 unique words.
Call length ranged from just less than 1 minute to more than 20 minutes with most of the calls lasting between 2 and 3 min-utes.
The corpus consists of close to 30k tokens and does not distinguish between agent channel and customer channel.
A universal dictionary of ranked words is built from the set of dialogs and each dialog is treated as a segment.
Dialog Tagging and Topic Saliency In this analysis we look at complete dialogs.
"A use-ful application of the methods we describe in this work is to identify and separate calls that are inter-esting from non-interesting calls [Footnote_4] , furthermore, one could also be interested in singling out which spe-cific terms make this dialog salient."
"4 For the purpose of this work, we simply define as an inter-esting call a call that deals with an infrequent or rare topic which influences the distribution of keywords and key-phrases. Examples of calls in our domain meeting this criterion are calls dealing with accidents and airbags."
"An application of this approach is the automatic generation of tags (e.g., social-network style of document tagging)."
"In our approach, we will identify calls whose top en-tries in their sorted relative rank differential lists are above a certain threshold and deem these calls as semantically salient."
We now describe in detail how an interesting call can be distinguished from a non-interesting call using the relative rank differential statistic.
"Figure 2 below shows the ranked dictionary d S = {d 1S , d 2S , d 3S ...} (i.e., the universal rank id’s as a function of their observed ranks) and Figure 3 shows the plot of the sorted relative rank differen-tial list R ( d u , d S ) for when the segment corre-sponds to an interesting call (as defined above)."
"The chosen call, specifically shows as topic AIR-BAG deployment in the context of a car accident."
"Specifically, Figure 2 shows the corresponding rank in the universal ranked dictionary versus the rank in the dialog or segment."
"We can see that the right-most part of the plot is largely monotonic, meaning that most entries of lesser frequency occur in the same ranked order both in the universal and the specific dialog (including zero times for the segment), while a subset across the whole range of the universal dictionary were substantially relo-cated up in the rank (i.e., occurred more frequently in the dialog than in the language)."
If the plot was a single straight line each word would have the same rank both in the language and in the dialog.
"We argue that while the terms of interest lie in that subset of interest in the graph (the terms whose rank increased substantially), not all of those words are equally interesting or important and rather than simply looking at absolute changes in rank we fo-cus on the relative-rank differential RRD metric."
"Thus, Figure 3 shows the sorted values of the rela-tive rank differential list (with α =1.3 )."
"The top entries and their rank in the universal dictionary (in parentheses) are:[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE]."
"As we can see, the top entries are distributed across a broad range of ranks in the universal dictionary and relate to the topic of the conversation, which from the top ranked entries are evidently the deployment of front and side airbags during an accident, and thus, for this call were able to identify its semantic saliency from the corpus of conversations."
Other interesting or salient calls also showed a similar this profile in the RRD curve.
The question now is what the behavior of our ap-proach for uninteresting calls is.
"We repeated the procedure above for a call which we deemed se-mantically un-interesting (i.e., dealing with a common topic like call transfer and other routine procedures)."
"Figure 4 shows the sorted relative rank differential values and, especially when com-pared with Figure 2, we see a large monotonic component on the higher ranked terms and not so marked discontinuities in the low and mid-range part of the curve."
"We computed the relative rank differential RRD metric for each feature similarly as with the inter-esting call, and ranked the words based on these values."
The distribution of the ranked values is shown in Figure 5.
"The resulting words with top values are[REF_CITE],[REF_CITE], and[REF_CITE]."
From these words we cannot really tell what is the spe-cific topic of the conversation is as easily as with the interesting call.
"More importantly, we can now compare Figures 3 and 5 and see that the highest relative rank differential value of the top entry in Figure 3 (larger than 10) is significantly larger than the largest relative rank differential value in Figure 5 (just above 7) reflecting the fact that the relative rank differential metric could be a useful parameter in evaluating semantic saliency of a segment using a static threshold."
"As an interesting point, con-ceivably the highly ranked features based on RRD could reflect language utilization idiosyncrasies. of R ( d , d ) for a non-interesting (semantically non-salient) call."
In this paper we presented a novel non parametric rank-statistics based method for the semantic analysis of conversations and dialogs.
"Our method is implementable in segment-based or dialog-based modalities, as well as in batch form and in on-line or dynamic form."
"Applications of our method in-clude topic detection, event tracking, story/topic monitoring, new-event detection, summarization, information filtering, etc."
"Because our approach is based on non-parametric statistics it has favorable intrinsic benefits, like making no assumptions about the underlying data, which makes it suitable for the use of both lexical semantic features as well as classifier-based semantic features."
"Furthermore, our approach could, in the future, benefit from classical non-parametric approaches like block-treatment analysis etc."
"We demonstrated that our approach is as effective in query classification as TF and TF-IDF in low noise and no noise (i.e., distortion) conditions, and consistently better than those techniques in noisy conditions."
We also found RRD to be more robust to query data sparseness than TF and TF-IDF.
"These results provide a motivation to combine our statistic with other techniques like topic chains, textilling, latent semantic indexing, and discrimi-nant classification approaches; specifically RRD could replace TF and TF-IDF based features."
Future work could focus on applying ranking sta-tistics to techniques for mining and tracking tem-poral and time-changing parameters in conjunction with techniques[REF_CITE].
Another area of possible future work is the detec-tion and separation of multiple underlying trends in dialogs.
"Our approach is also suited for the analy-sis of large streams of real time conversations, and this is a very important area of focus as presently more and more conversational data gets generated through channels like chat, mobile telephony, VoIP etc."
"Predicting possible code-switching points can help develop more accurate methods for au-tomatically processing mixed-language text, such as multilingual language models for speech recognition systems and syntactic an-alyzers."
We present in this paper exploratory results on learning to predict potential code-switching points in Spanish-English.
We trained different learning algorithms using a transcription of code-switched discourse.
"To evaluate the performance of the classifiers, we used two different criteria: 1) measuring pre-cision, recall, and F-measure of the predic-tions against the reference in the transcrip-tion, and 2) rating the naturalness of artifi-cially generated code-switched sentences."
Av-erage scores for the code-switched sentences generated by our machine learning approach were close to the scores of those generated by humans.
"Multilingual speakers often switch back and forth between languages when speaking or writing, mostly in informal settings."
The mixing of lan-guages involves very elaborated patterns and forms and we usually use the term Code-Switching (CS) to encompass all of them[REF_CITE].
"Before the Internet era, CS was mainly used in its spoken form."
"But with so many different informal interaction set-tings, such as chats, forums, blogs, and web sites like Myspace and Facebook, CS is being used more and more in written form."
"For English and Spanish,"
CS has taken a step further.
It has become a hall-mark of the chicano culture as it is evident by the growing number of chicano writers publishing work in Spanish-English CS.
"We have not completely discovered the process of human language acquisition, especially dual lan-guage acquisition."
"Findings in linguistics, soci-olinguistics, and psycholinguistics show that the production of code-switched discourse requires a very sophisticated knowledge of the languages be-ing mixed."
Some theories suggest bilingual speak-ers might have a third grammar for processing this type of discourse.
The general agreement regarding CS is that switches do not take place at random and instead it is possible to identify rules that bilingual speakers adhere to.
"Understanding the CS process can lead to accu-rate methods for the automatic processing of bilin-gual discourse, and corpus-driven studies about CS can also inform linguistic theories."
In this paper we present exploratory work on learning to predict CS points using a machine learning approach.
Such an approach can be used to reduce perplexity of lan-guage models for bilingual discourse.
We believe that CS behavior can be learned by a classifier and the results presented in this paper support our belief.
One of the difficult aspects of trying to predict CS points is how to evaluate the performance of the learner since switching is intrinsically motivated and there are no forced switches[REF_CITE].
"Therefore, standard classification measures for this task such as precision, recall, F-measure, or ac-curacy, are not the best approach for measuring the effectiveness of a CS predictor."
"To comple- ment the evaluation of our approach, we designed a task involving human judgements on the naturalness of automatically generated code-switched sentences."
Both evaluations yielded encouraging results.
The next section discusses theories explaining the CS production process.
Then in Section 3 we present our framework for learning to predict CS points.
Section 4 discusses the empirical evaluation of the classifiers compared to the human reference.
In Section 5 we present results of human evalua-tions on automatically generated code-switched sen-tences.
Section 6 describes previous work related to the processing of code-switched text.
"Finally, we conclude in Section 7 with a summary of our find-ings and directions for future work."
The combination of languages can be considered to be a continuous spectrum where on each end of the spectrum we have one of the standard languages and no blending.
As one moves closer to the mid-dle of the spectrum the amount and complexity of the blending pattern increases.
"The blending pattern most widely known, and studied, is code-switching, which refers to the mixing of words from two lan-guages, but the words themselves do not suffer any syntactic or phonological alterations."
"The CS points can lie at sentence boundaries, but very often we will also observe CS inside sentences."
"According[REF_CITE]when CS is used inside a sentence, it can only happen at syntactic boundaries shared by both languages, and the resulting monolingual fragments will conform to the grammar of the corresponding language."
In this CS theory the relationship between both languages is symmetric –lexical items from one language can be replaced by the corresponding items in the sec-ond language and vice versa.
"Another prevalent lin-guistic theory argues the contrary: there is an asym-metric relation where the changes can occur only in one direction, which reflects the existence of a Ma-trix Language (ML), the dominant language, and an Embedded Language (EL), or subordinate language[REF_CITE]."
"The Matrix Language Frame model, proposed and extended by Scotton-Myers, supports this asymmetric relation theory."
This formalism pre-scribes that content morphemes can come from the
"ML or the EL, whereas late system morphemes, the elements that indicate grammatical relations, can only be provided by the ML[REF_CITE]."
"Until an empirical evaluation is carried out on large representative samples of discourse involving a large number of different speakers, and different language-pairs, the production of CS discourse will not be explained satisfactorily."
The goal of this work is to move closer to a better understanding of CS by learning from corpora to predict possible CS points.
We recorded a conversation among three English-Spanish bilingual speakers that code-switch regu-larly when speaking to each other.
"The conversa-tion lasts for about 40 minutes (∼8k words, 922 sentences)."
It was manually transcribed and anno-tated with Part-of-Speech (POS) tags.
A total of 239 switches were identified manually.
"English is the predominant language used, with a total of 576 monolingual sentences."
We refer to this transcrip-tion as the Spanglish data set.
We are currently in the process of collecting new transcriptions of this con-versation in order to measure inter annotator agree-ment.
"Machine learning algorithms have proven to be sur-prisingly good at language processing tasks, in-cluding optical character recognition, text classifica-tion, named entity extraction, and many more."
The premise of our paper is that machine learning al-gorithms can also be successful at learning how to code-switch as well as humans.
At the very least we want to provide encouraging evidence that this is possible.
"To the best of our knowledge, there is no previous work related to the problem of auto-matically predicting CS points."
Our machine learn-ing framework then is inspired by existing theories of CS and existing work on part-of-speech tagging code-switched text[REF_CITE].
"In our approach, each word boundary is a poten-tial point for switching – an instance of the learning task."
It should be noted that we can only rely on the history of words preceding potential CS points in or- der to extract meaningful features.
"Otherwise, if we look also into the future, we could just do language identification to extract the CS points."
"However, our goal is to provide methods that can be used in real time applications, where we do not have access to observations beyond the point of interest."
Another restriction we imposed on the method is related to the size of the context used.
"A sentence can be code-switched in different ways, with all different ver-sions adhering to the CS “grammar”."
The number of permissible CS sentences grows almost exponen-tially with the length of the sentence 1 .
"By limiting the length of the context to at most two words we are trying to avoid some sort of over fitting by hav-ing the model making assumptions over the interac-tion of the two languages that will be too weak, or speaker-dependent."
Previous studies have identified several socio-pragmatic functions of code-switching.
"The most common include direct quotation, emphasis, clari-fication, parenthetical comments, tags, and trigger switches."
"Other characteristics relevant to CS be-havior are the topic being discussed, the speakers involved, the setting where the conversation is tak-ing place, and the level of familiarity between the speakers."
Having encoded information regarding the CS function and the aforementioned relevant factors might help in predicting upcoming CS points.
"How-ever, annotating this information in the transcription can be time consuming and very often this informa- tion is not readily available."
"Therefore, at the ex-pense of making this task even more difficult, we de-cided against trying to include this type of informa-tion and include only lexical and syntactic features, to evaluate a practical and cost effective method for this task."
Table 1 shows the list of features.
"All of these features are associated with word w n , the word immediately preceding boundary n. Feature [Footnote_1] is the word form 2 ."
1 Almost exponentially because not all sentences will be con-sidered grammatical.
Feature [Footnote_2] is language identifica-tion.
"2 Strictly speaking these should be called tokens, not words since punctuation marks are considered as well."
"If the production of CS discourse adheres to the matrix language frame model, then knowledge of the language can potentially be a good source of information."
Feature [URL_CITE] is the gold-standard POS tag.
We also include as a feature the position of the word relative to the phrase constituent using a Beginning-Inside-Outside (BIO) scheme.
"For in-stance, the word at the beginning of the verb phrase will be labeled as B, the following words inside this verb phrase will be tagged as I, and words that were not identified as part of a phrase constituent were labeled as O. This chunking information was ex-tracted using the English and Spanish versions of FreeLing 3 ."
We did not measure accuracy on the chunking information.
Features 5 to 9 were gener-ated by tagging the Spanglish conversation using the Spanish and the English versions of the Tree Tagger[REF_CITE].
"Attributes 5 to 7 are extracted from the English version, which include the POS tag, the confidence, and the lemma for that word."
"Similarly, features 8 to 10 were taken from the Spanish mono-lingual tree tagger."
Features from the monolingual taggers will have some noisy labels when tagging fragments of the other language.
"However, consider-ing that our feature set is small we want to explore if adding these features, which include the lemmas and probability estimates, can contribute to the learning task."
We also explored using a larger context.
"In this case, we extract the same features shown in Table 1 for the two words preceding the word boundary, resulting in 20 attributes representing each instance."
Evaluation for this task is not straightforward.
"Within a sentence, there are several CS points that will result in a natural sounding code-switched sen-tence, but none of these CS points are mandatory."
"CS has a lot to do with the speaker’s preferences, the topic being discussed, and the background of the participants involved."
"Using the standard approach for measuring performance of classifiers can be mis-leading, especially if the reference data set is small and/or has only a small number of speakers."
"It is un-realistic to just consider F-measure, or accuracy, as truthfully reflecting how well the learners generalize to the task."
"Therefore, we evaluated the classifier’s performance using two different criteria, which are discussed in the next sections."
This is the standard evaluation of machine learning classifiers.
We randomly divided the data into sen-tences and grouped them into 10 subsets to perform a cross-validation.
Tables 2 and 3 show results for Naive Bayes (NB) and Value Feature Interval (VFI)[REF_CITE].
"Using WEKA[REF_CITE], we experimented with differ-ent subsets of the attributes and two context win-dows: using only the preceding word and using the previous two words."
The results presented here are overall averages of 10-fold cross validation.
We also report standard deviations.
"It should be noted that the Spanglish data set is highly imbalanced, around 96% of the instances belong to the negative class."
"Therefore, our comparisons are based on Precision, Recall, and F-measure, leaving accuracy aside, since a weak classifier predicting that all instances belong to the negative class will reach an accuracy of 96%."
The performance measures shown on Tables 2 and 3 show that NB outperforms VFI in most of the con-figurations tested.
"In particular, NB yields the best results when using a 1 word context with no lexical forms nor lemmas as attributes (see Table 2 row 3)."
This is a fortunate finding –for most practical prob-lems there will always be words in the test set that have not been observed in the training set.
For our small Spanglish data set that will certainly be the case.
"In contrast, VFI achieves higher F-measures when using a context of two words and all the fea-tures are used."
"Analyzing the predictions of the learners we noted that the NB classifier is heavily biased by the lan-guage attribute, close to 80% of the positive predic- tions made by NB are after seeing a word in Span-ish."
This preference seems to support the assump-tion of the asymmetry between the two languages and the existence of an ML [Footnote_4] .
4 We remind the reader that in this paper ML stands for Ma-trix Language.
"This however is not the case for VFI, only a little over 50% of the posi-tive predictions belong to this scenario."
"Another in-teresting finding is the learner’s tendency to predict a code-switch after observing words like “Yeah”, “anyway”, “no”, and “shower”."
The first two seem to fit the pattern of idiomatic expressions.
"Accord-ing to Montes-Alcalá this type of CS includes lin-guistic routines and fillers that are difficult to trans-late accurately[REF_CITE], which might be the case of “anyway”, and unconscious changes, which can explain the case of “Yeah”."
"The case of “shower” and “no” are more difficult to explain, they might be overfitting patterns from the learners."
We also found out that VFI learned to predict that a CS will take place right after seeing the sequence of words le dije (I said).
"This sequence of words is frequently used when the speaker is about to quote his/herself, and this quotation is one of the well-documented CS functions[REF_CITE]."
"A greedy search approach for attribute selection using WEKA showed that out of the 20 attributes (when using a two word context), the subset with the highest predictive value included the language identification for word w n−1 and w n−2 , the confi-dence threshold from the English tagger for word w n−2 , the lemma from the Spanish Tree tagger for w n−1 , and the lexical form of the word w n−1 ."
We expected the chunk information to be useful and this does not seem to be the case.
Another unexpected outcome is that higher F-measures are reached by adding features generated by the monolingual Tree taggers.
"Even though these features are noisy, they still carry useful information."
We only show results from NB and VFI.
Initial experiments with a subset of the data showed that these algorithms were the most promising for this task.
"They both yielded higher F-measures, even when compared against Support Vector Machines (SVMs), C4.5, and neural networks."
"On this ex-periment all the discriminative classifiers reached a classification accuracy close to 96%, but an F- measure on the positive class of around 0%."
"NB and VFI estimate predictions for each class sepa-rately, which makes them robust to imbalanced data sets."
"In addition, generative models are known to be better for smaller data sets since they reach their higher asymptotic error much faster than discrimi-native models[REF_CITE]."
This might explain why Naive Bayes outperformed strong clas-sifiers such as SVMs by a large margin.
The overall prediction performance is not very high.
"However, we should remark that for this par-ticular task expecting a high F-measure is unrealis-tic."
"Consider for example, a case where the learners predict a CS point where the speaker decided not to switch, this does not imply that particular point is not a good CS point."
"And similarly, if the classifier missed an existing CS point in the reference data set the resulting sentence might still be grammatical and natural sounding."
"This motivated the use of an alter-native evaluation, which we discuss below."
"The goal of this evaluation is to explore how humans perceive our automatically generated CS sentences, and in particular, how do they compare to the orig-inal sentences and to the randomly generated ones."
"Some of them were selected from the Spanglish Times Magazine [URL_CITE] , some others from blogs found[REF_CITE]."
Other sentences were taken from a paper discussing CS on e-mails[REF_CITE].
"All of the sentences are true occurrences of writ-ten CS, from speakers different from the ones in the Spanglish data set."
The sentences were translated to standard English and Spanish and were manually aligned.
We will use this parallel set of sentences to predict CS points with our models.
Based on the model predictions we will generate code-switched sentences by combining monolingual fragments.
It should be noted that the Spanglish data set is a transcription of spoken CS.
"In contrast, this new evaluation set contains only written CS."
"Recent stud-ies suggest written CS will adhere to the rules of spoken CS[REF_CITE], but there is still some controversy on this issue."
"From our perspec-tive, both samples come from informal conversa-tional interactions."
It is expected that both will have similar patterns and therefore will provide a good source for our evaluation.
In this subsection we describe how to generate code-switched sentences randomly and with the learned models described in the previous sections.
"For the classifier-based approach , we POS tagged each par-allel set of sentences, with the monolingual English and Spanish Tree Taggers, and we extracted the same set of features described shown in Table 1."
"We decided to train the models with a context size of one word, even though both learners reached higher F-measures when using a two-word context."
"This decision was based on the observation that having a two-word context will pose restrictions on possible CS points, since we would not be able to switch un-less we have inserted into the sentence at least two tokens from the same language."
"We trained the NB and VFI models with the Span-glish data set (using features 2–6, 8, and 9, see Ta-ble 1) and generated CS predictions for each paral-lel file."
"A code-switched sentence is generated by adding the first token of the sentence in language 1 (L1), and continue adding more tokens from L1 until a CS point is found."
"When a CS prediction is found, the following tokens are selected from the second language (L2), and we continue adding tokens from L2 until the classifier has predicted a change."
Differ-ent versions of the sentences are generated by chang-ing the definition of L1 and L2.
"For the randomly generated CS sentences, switch-ing decisions are made randomly with a probability proportional to the positive predictions made by the classifiers (in this case NB)."
"That is, for the Spanish sentences switch points are predicted randomly with a 30% chance of switching while for English switch points are predicted with a 10% chance."
"In total we generated 180 CS sentences: 30 sen-tences per generator scheme (we have three genera-tors: NB, VFI, and random), and two versions from each generator corresponding to the two possible configurations of L1-L2 (Spanish-English, English-Spanish)."
We noticed that in some cases same sen-tences are generated by different methods and some-times there are no switches.
We narrowed down the sentences by randomly choosing the combination of L1-L2 for each generator.
"This reduced the num-ber of sentences from having 6 versions, to having only 3 versions of each sentence."
"From the resulting 30 sets, we removed 2 sets because one or more of the generator schemes produced a monolingual sen-tence."
"Therefore, we used 28 sets for human evalua-tions."
We had a total of 18 subjects participating in the ex-periment.
"All of them identified themselves as be-ing able to read and write Spanish and English, and the majority of them said to have used CS at least some times."
We showed to the human subjects the 28 sets of sentences.
This time we included the orig-inal version of the sentence.
"Therefore, each judge was given 4 versions of each of the 28 code-switched sentences: the one generated from NB predictions, the one from VFI, the randomly generated, and the original one."
Then we asked them to rate each sen-tence with a number from 1 to 5 indicating how nat-ural and human-like the sentence sounds.
"A rating of 5 means that they strongly agree, 4 means they agree, 3 not sure, 2 disagree, 1 strongly disagree."
The average results are presented in Table 4.
"The sentences generated by NB were scored consider-ably higher than those from VFI and random, and closer to the human sentences."
According to the paired t-test the difference between the NB score and the random one is significant (p=0.01).
However the average score for VFI is lower than random.
More experiments are needed to see if by choosing the set-ting where VFI had the highest F-measure would make a difference in this respect.
"Overall the sub-jects rated the human-generated CS sentences lower than what we were expecting, although it is clear that they consider these sentences more natural sound-ing than the rest."
This low rating might be related to the attitude several evaluators expressed toward CS.
In the evaluation form we asked the judges to ex-press their opinion on CS and several of them indi-cated feelings along the lines of “we shouldn’t code-switch”.
"There are several ways in which two parallel sen-tences can be combined in CS, and possibly several will sound natural, but from our results, it is clear that the NB algorithm was indeed able to generate a human-like CS behavior that was successfully dif-ferentiated from randomly-generated sentences."
"By looking at the set of automatically generated code-switched sentences, we realized that the ma-jority of the sentences are grammatical and natural sounding."
We believe that for a large number of the sentences it would be hard for a human to distin-guish the sentences that were automatically gener-ated from the human-generated ones.
"One of the give away clues is when a multi-word expression is CS, or a tag line."
Table 5 shows three examples from the sentences evaluated.
"In the table there is an example in sentence 1c where the noun phrase is code-switched, the sentence is grammatical accord- ing to Spanish rules, but it sounds very odd to have the noun carta followed by the adjective in English, “astrological”."
"Other interesting features are present in example 3 where for the same noun phrase “pro-duce section” we have both, the female marking de-terminer la and the masculine el."
The same thing happens for the noun phrase “check-out line”.
We would need to have a larger occurrence of these in-stances in our test set to determine if on average one form is preferred over the other.
"In another experiment, we measured the predic-tion performance of NB and VFI on the 30 code-switched sentences used in this part of the evalua-tion."
"The best results, an F-measure of 0.418, were achieved by NB when a context of 1 word was used, and no words, nor lemmas were included as features."
This is the same setting used for the generation pro-cess.
"In contrast, VFI reached an F-measure of 0.351 on this same setting. 30 sentences represent a very small dataset but the results are very promising since the speakers are different in the training and testing dataset."
"Moreover, these results support the claim that written and spoken CS obey similar rules."
There is little prior work on computational linguis-tic approaches to code-switched discourse.
"Most of the previous work includes formalisms to pars-ing and generating mixed sentences, for example for Marathi and English[REF_CITE], or Hindi and En-glish[REF_CITE]."
Sankoff proposed a pro-duction model of bilingual discourse that accounts for the equivalence constraint and the unpredictabil-ity of code-switching[REF_CITE].
His real-time production model draws on the alternation of fragments from two virtual monolingual sentences.
But no statistical assessment has been conducted on real corpora.
Another related work deals with language iden-tification on English-Maltese code-switched SMS messages[REF_CITE].
"What the au-thors found to work best for language identification in this noisy domain is a combination of a bigram Hidden Markov Model, trained on language tran-sitions, and a trigram character Markov Model for handling unknown words."
We presented preliminary results on learning to pre-dict CS points with machine learning.
"One of the possible applications of our method involves fine-tuning the weights in a multilingual language model, for instance, as part of a speech recognizer for Span-glish."
"With this in mind, we restricted the possible features in the learning scenario allowing only lexi-cal and syntactic features that could be automatically generated from the text."
"Empirical evaluations on a Spanglish conversation showed that Naive Bayes and VFI can predict with acceptable F-measures possible CS points, considering the difficulty of the task."
Prediction of CS points can help improve mul-tilingual language models.
Evaluation of our approach cannot be done based only on the gold-standard set since there is no sin- gle right answer in this task.
"Therefore, we comple-mented the evaluation by involving judgements from bilingual speakers."
We generated CS sentences by taking the predictions from the classifiers to merge parallel sentences.
"On average, the sentences gen-erated from the NB model were rated closer to the original sentences, and a lot higher than the ones from a random generator."
Most of the sentences sounded human-like.
"But because the process is au-tomatic we did find some awkward constructions, for example plural vs singular noun-verb agreement, or multi-word phrases that were code-switched in the middle."
Perhaps a multi-word recognition fea-ture could improve results.
"One of the advantages of technological develop-ment and economic globalization is that more peo-ple from different regions of the world with differ-ent cultures, and therefore, different languages will be in closer contact."
"As a result, code-switching will become more popular."
It is important to start ad-dressing this type of bilingual communication from a computational linguistics point of view.
This work is one of the few attempts to fill the gap.
Some directions for future work include: explor-ing the extent to which our results can be improved by including a multi-word expression recognition system.
We also want to investigate the integration of our approach to multilingual language models and move beyond CS to address other deeper linguistic phenomena.
"Lastly, we would like to explore similar approaches in other popular language combinations."
Knowing the degree of antonymy between words has widespread applications in natural language processing.
Manually-created lexi-cons have limited coverage and do not include most semantically contrasting word pairs.
We present a new automatic and empirical mea-sure of antonymy that combines corpus statis-tics with the structure of a published the-saurus.
"The approach is evaluated on a set of closest-opposite questions, obtaining a preci-sion of over 80%."
"Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances."
"Native speakers of a language intuitively recog-nize different degrees of antonymy—whether two words are strongly antonymous (hot–cold, good– bad, friend–enemy), just semantically contrasting (enemy–fan, cold–lukewarm, ascend–slip) or not antonymous at all (penguin–clown, cold–chilly, boat–rudder)."
"Over the years, many definitions of antonymy have been proposed by linguists[REF_CITE], cognitive scien-tists[REF_CITE], psycholinguists[REF_CITE], and lexicographers[REF_CITE], which differ from each other in small and large respects."
"In its strictest sense, antonymy applies to gradable adjec-tives, such as hot–cold and tall–short, where the two words represent the two ends of a semantic dimension."
"In a broader sense, it includes other adjectives, nouns, and verbs as well (life–death, ascend–descend, shout–whisper)."
"In its broadest sense, it applies to any two words that represent contrasting meanings."
We will use the term de-gree of antonymy to encompass the complete se-mantic range—a combined measure of the contrast in meaning conveyed by two words and the tendency of native speakers to call them opposites.
"The higher the degree of antonymy between a target word pair, the greater the semantic contrast between them and the greater their tendency to be considered antonym pairs by native speakers."
Automatically determining the degree of antonymy between words has many uses includ-ing detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions[REF_CITE](Kyoto has a predominantly wet climate /
It is mostly dry in Kyoto).
"Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements."
Antonyms often indicate the discourse relation of contrast[REF_CITE].
"They are also useful for detecting humor[REF_CITE], as satire and jokes tend to have contradictions and oxymorons."
"Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out."
"For example, in the automatic creation of a thesaurus it is necessary to distinguish near-synonyms from word pairs that are semantically contrasting."
Measures of distributional similarity fail to do so.
"Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component."
"Lexicons of pairs of words that native speakers consider antonyms have been created for certain lan-guages, but their coverage has been limited."
"Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered antonym pairs, and they remain unrecorded."
"Even though a number of computational approaches have been proposed for semantic closeness, and some for hypernymy–hyponymy[REF_CITE], measures of antonymy have been less successful."
"To some ex-tent, this is because antonymy is not as well under-stood as other classical lexical-semantic relations."
"We first very briefly summarize insights and in-tuitions about this phenomenon, as proposed by lin-guists and lexicographers (Section 2)."
We discuss related work (Section 3).
We describe the resources we use (Section 4) and present experiments that ex-amine the manifestation of antonymy in text (Sec-tions 5 and 6).
We then propose a new empirical approach to determine the degree of antonymy be-tween two words (Section 7).
"We compiled a dataset of 950 closest-opposite questions, which we used for evaluation (Section 8)."
We conclude with a discus-sion of the merits and limitations of this approach and outline future work.
"Antonymy, like synonymy and hyponymy, is a lexical-semantic relation that, strictly speaking, ap-plies to two lexical units—combinations of surface form and word sense. (That said, for simplicity and where appropriate we will use the term “antonymous words” as a proxy for “antonymous lexical units”.)"
"However, accepting this leads to two interesting and seemingly paradoxical questions (described below in the two subsections)."
"Native speakers of a language consider certain con-trasting word pairs to be antonymous (for example, large–small), and certain other seemingly equivalent word pairs as less so (for example, large–little)."
"A number of reasons have been suggested: (1)[REF_CITE]observes that if the meaning of the target words is completely defined by one semantic dimen-sion and the words represent the two ends of this se- mantic dimension, then they tend to be considered antonyms."
"We will refer to this semantic dimension as the dimension of opposition. (2) If on the other hand,[REF_CITE]point out, there is more to the meaning of the antonymous words than the dimension of opposition—for example, more se-mantic dimensions or added connotations—then the two words are not so strongly antonymous."
"Most people do not think of chubby as a direct antonym of thin because it has the additional connotation of being cute and informal. (3)[REF_CITE]also pos-tulates that word pairs are not considered strictly antonymous if it is difficult to identify the dimension of opposition (for example, city–farm). (4)[REF_CITE]claim that two contrasting words are identified as antonyms if they occur together in a sentence more often than chance."
"However,[REF_CITE]claim that the greater-than-chance co-occurrence of antonyms in sentences is because together they convey contrast well, which is rhetorically useful, and not really the reason why they are considered antonyms in the first place."
"Two words (more precisely, two lexical units) are considered to be close in meaning if there is a lexical-semantic relation between them."
Lexical-semantic relations are of two kinds: classical and non-classical.
"Examples of classical rela-tions include synonymy, hyponymy, troponymy, and meronymy."
"Non-classical relations, as pointed out[REF_CITE], are much more com-mon and include concepts pertaining to another con-cept (kind, chivalrous, formal pertaining to gentle-manly), and commonly co-occurring words (for ex-ample, problem–solution pairs such as homeless, shelter)."
Semantic distance (or closeness) in this broad sense is known as semantic relatedness.
"Two words are considered to be semantically similar if they are associated via the synonymy, hyponymy– hypernymy, or the troponymy relation."
"So terms that are semantically similar (plane–glider, doctor– surgeon) are also semantically related, but terms that are semantically related may not always be semanti-cally similar (plane–sky, surgeon–scalpel)."
Antonymy is unique among these relations be-cause it simultaneously conveys both a sense of closeness and of distance[REF_CITE].
Antony-mous concepts are semantically related but not se-mantically similar.
This is known as the co-occurrence hypothesis.
They also showed that this was empirically true for four adjective antonym pairs.
All of these pairs were adjectives.
"How-ever, non-antonymous semantically related words such as hypernyms, holonyms, meronyms, and near-synonyms also tend to occur together more often than chance."
"Thus, separating antonyms from them has proven to be difficult."
They eval-uated their method on 80 pairs of antonyms and 80 pairs of synonyms taken from the Webster’s Colle-giate Thesaurus[REF_CITE].
"In this paper, we pro-pose a method to determine the degree of antonymy between any word pair and not just those that are distributionally similar."
"However, the Turney method is super-vised whereas the method proposed in this paper is completely unsupervised."
"Unfortunately, they evalu-ated their method on only 18 word pairs."
Neither of these methods determines the degree of antonymy between words and they have not been shown to have substantial coverage.
"The closer this vector is to the context vectors of the other target word, the more antonymous the two tar-get words are."
"However, the antonymous vectors are manually created."
"Further, the approach is not eval-uated beyond a handful of word pairs."
Work in sentiment detection and opinion mining aims at determining the polarity of words.
"For ex-ample, Pang,[REF_CITE]detect that adjectives such as dazzling, brilliant, and grip-ping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the noun negatively."
Many of these gradable adjec-tives have antonyms. but these approaches do not attempt to determine pairs of positive and negative polarity words that are antonyms.
"Published thesauri, such as the Roget’s and Mac-quarie, divide the vocabulary into about a thousand categories."
Words within a category tend to be near-synonymous or semantically similar.
"One may also find antonymous and semantically related words in the same category, but this is rare."
The intuition is that words within a category represent a coarse concept.
Words with more than one meaning may be found in more than one category; these repre-sent its coarse senses.
"Within a category, the words are grouped into paragraphs."
Words in the same paragraph tend to be closer in meaning than those in different paragraphs.
We will take advantage of the structure of the thesaurus in our approach.
"Unlike the traditional approach to antonymy, Word-Net encodes antonymy as a lexical relationship—a relation between two words (not concepts)[REF_CITE]."
"Even though a synset (a WordNet con-cept) may be represented by more than one word, individual words across synsets are marked as (di- rect) antonyms."
Gross et al. argue that other words in the synsets form “indirect antonyms”.
"Even after including the indirect antonyms, Word-Net’s coverage is limited."
"Further, the noun–noun, verb–verb, and adjective–adjective antonym pairs of WordNet largely ignore near-opposites as revealed by our experiments (Section 8 below)."
"Also, Word-Net (or any other manually-created repository of antonyms for that matter) does not encode the de-gree of antonymy between words."
"Nevertheless, we investigate the usefulness of WordNet as a source of seed antonym pairs for our approach."
The distributional hypothesis of closeness states that words that occur in similar contexts tend to be semantically close[REF_CITE].
"Distributional measures of distance, such as those proposed[REF_CITE], quantify how similar the two sets of contexts of a target word pair are."
Equation 1 is a modified form of Lin’s measure that ignores syntactic depen-dencies and hence it estimates semantic relatedness rather than semantic similarity:
Lin w 1 w 2  ∑ w T w 1 T w 2
I w 1 w I w 2 w (1) ∑ w  T w 1 I w 1 w  ∑ w  T w 2
I w 2 w
Here w 1 and w 2 are the target words; I x y is the pointwise mutual information between x and y; and T x is the set of all words y that have positive point-wise mutual information with the word x (I x y  0).
They argued that this is because the word-distance measures clump to-gether the contexts of the different senses of the tar-get words.
"They proposed a way to obtain distri-butional distance between word senses, using any of the distributional measures such as cosine or that proposed by Lin, and showed that this approach per-formed markedly better than the traditional word-distance approach."
They used thesaurus categories as very coarse word senses.
Equation 2 shows how Lin’s formula is used to determine distributional dis-tance between two thesaurus categories c 1 and c 2 :
Lin c 1 c 2  ∑ w T c 1 T c 2
I c 1 w  I c 2 w (2) ∑ w  T c 1 I c 1 w ∑ w  T c 2
I c 2 w
Here T c is the set of all words w that have posi-tive pointwise mutual information with the thesaurus category c (I c w  0).
We adopt this method for use in our approach to determine word-pair antonymy.
"As a first step towards formulating our approach, we investigated the co-occurrence hypothesis on a significantly larger set of antonym pairs than those studied before."
"We randomly selected a thousand antonym pairs (nouns, verbs, and adjectives) from WordNet and counted the number of times (1) they occurred individually and (2) they co-occurred in the same sentence within a window of five words, in the British National Corpus (BNC)[REF_CITE]."
We then calculated the mutual information for each of these word pairs and averaged it.
"We randomly gen-erated another set of a thousand word pairs, without regard to whether they were antonymous or not, and used it as a control set."
The average mutual infor-mation between the words in the antonym set was 0.94 with a standard deviation of 2.27.
The average mutual information between the words in the con-trol set was 0.01 with a standard deviation of 0.37.
Thus antonymous word pairs occur together much more often than chance irrespective of their intended senses (p 0 01).
"Of course, a number of non-antonymous words also tend to co-occur more of-ten than chance—commonly known as collocations."
"Thus, strong co-occurrence is not a sufficient condi-tion for detecting antonyms, but these results show that it can be a useful cue."
"The meaning of the utterance will be inverted, of course, but the sentence will remain grammatical and lin-guistically plausible."
This came to be known as the substitutability hypothesis.
"However, their exper-iments did not support this claim."
"They found that given a sentence with the target adjective removed, most people did not confound the missing word with its antonym."
"From this (and to some extent from the co-occurrence hypothesis), we can derive the distri-butional hypothesis of antonyms: antonyms occur in similar contexts more often than non-antonymous words."
We used the same set of one thousand antonym pairs and one thousand control pairs as in the pre-vious experiment to gather empirical proof of the distributional hypothesis.
"For each word pair from the antonym set, we calculated the distributional dis-tance between each of their senses using Moham-mad and Hirst’s (2006) method of concept distance along with the modified form of Lin’s (1998) dis-tributional measure (equation 2)."
The distance be-tween the closest senses of the word pairs was av-eraged for all thousand antonyms.
The process was then repeated for the control set.
The control set had an average semantic close-ness of 0.23 with a standard deviation of 0.11 on a scale from 0 (unrelated) to 1 (identical).
"On the other hand, antonymous word pairs had an average semantic closeness of 0.30 with a standard devia-tion of 0.23. 1"
"This demonstrates that relative to other word pairs, antonymous words tend to occur in simi-lar contexts (p 0 [Footnote_1])."
"1 It should be noted that absolute values in the range between 0 and 1 are meaningless by themselves. However, if a set of word pairs is shown to consistently have higher values than an-other set, then we can conclude that the members of the former set tend to be semantically closer than those of the latter."
"However, near-synonymous and similar word pairs also occur in similar contexts. (the distributional hypothesis of closeness)."
"Thus, just like the co-occurrence hypothesis, occurrence in similar contexts is not sufficient, but rather yet another useful cue towards detecting antonyms."
We now present an empirical approach to determine the degree of antonymy between words.
"In order to maximize applicability and usefulness in natural language applications, we model the broad sense of antonymy."
"Given a target word pair, the approach determines whether they are antonymous or not, and if they are antonymous whether they have a high, medium, or low degree of antonymy."
"More pre-cisely, the approach presents a way to determine whether one word pair is more antonymous than an-other."
The approach relies on the structure of the pub-lished thesaurus as well as the co-occurrence and distributional hypotheses.
"As mentioned earlier, a thesaurus organizes words in sets representing con-cepts or categories."
We first determine pairs of the-saurus categories that are contrasting in meaning (Section 7.1).
We then use the co-occurrence and distributional hypotheses to determine the degree of antonymy (Section 7.2).
We propose two ways of detecting thesaurus cate-gory pairs that represent contrasting concepts (we will call these pairs contrasting categories): (1) us-ing a seed set of antonyms and (2) using a simple heuristic that exploits how thesaurus categories are ordered.
"Affix-generated seed set Antonym pairs such as hot–cold and dark–light occur frequently in text, but in terms of type-pairs they are outnumbered by those created using affixes, such as un- (clear– unclear) and dis- (honest–dishonest)."
"Further, this phenomenon is observed in most languages[REF_CITE]."
Table 1 lists sixteen morphological rules that tend to generate antonyms in English.
"These rules were applied to each of the words in the Macquarie The-saurus and if the resulting term was also a valid word in the thesaurus, then the word-pair was added to the affix-generated seed set."
"These sixteen rules generated 2,734 word pairs."
"Of course, not all of them are antonymous, for example sect–insect and coy–decoy."
"However, these are relatively few in number and were found to have only a small impact on the results."
"WordNet seed set We compiled a list of 20,611 semantically contrasting word pairs from WordNet."
"If two words from two synsets in WordNet are con-nected by an antonymy link, then every possible word pair across the two synsets was considered to be semantically contrasting."
A large number of them include multiword expressions.
We will refer to them as the WordNet seed set.
"Then, given these two seed sets, if any word in thesaurus category C 1 is antonymous to any word in category C 2 as per a seed antonym pair, then the two categories are marked as contrasting."
"It should be noted, however, that the seed antonym pair may be antonymous only in certain senses."
"For example, consider the antonym pair work–play."
"Here, play is antonymous to work only in its ACTIVITY FOR FUN sense and not its DRAMA sense."
"In such cases, we employ the distributional hypothesis of closeness: two words are antonymous to each other in those senses which are closest in meaning to each other."
"Since the thesaurus category pertaining to WORK is relatively closer in meaning to the ACTIVITY FOR FUN sense than the DRAMA sense, those two cat-egories will be considered contrasting and not the categories pertaining to WORK and DRAMA ."
"If no word in C 1 is antonymous to any word in C 2 , then the categories are considered not contrasting."
"As the seed sets, both automatically generated and manually created, are relatively large in comparison to the total number of categories in the[REF_CITE], this simple approach has reason-able coverage and accuracy."
Most published thesauri are ordered such that contrasting categories tend to be adjacent.
"This is not a hard-and-fast rule, and often a category may be contrasting in meaning to several other categories."
"Further, often adjacent categories are not semanti-cally contrasting."
"However, since this was an easy-enough heuristic to implement, we investigated the usefulness of considering adjacent categories as con-trasting."
We will refer to this as the adjacency heuristic.
"Once we know which category pairs are contrast-ing (using the methods from the previous subsec-tion), we determine the degree of antonymy be-tween the two categories (Section 7.2.1)."
The aim is to assign contrasting category pairs a non-zero value signifying the degree of contrast.
"In turn, we will use that information to determine the degree of antonymy between any word pair whose members belong to two contrasting categories (Sections 7.2.2 and 7.2.3)."
"Using the distributional hypothesis of antonyms, we claim that the degree of antonymy between two contrasting concepts (thesaurus categories) is di-rectly proportional to the distributional closeness of the two concepts."
"In other words, the more the words representing two contrasting concepts occur in sim-ilar contexts, the more the two concepts are consid-ered to be antonymous."
Again we used Mohammad and Hirst’s (2006) method along with Lin’s (1998) distributional mea-sure to determine the distributional closeness of two thesaurus concepts.
Co-occurrence statistics re-quired for the approach were computed from the
Words that occurred within a window of 5 words were considered to co-occur.
"Recall that strictly speaking, antonymy (like other lexical-semantic relations) applies to lexical units (a combination of surface form and word sense)."
"If two words are used in senses pertaining to contrast-ing categories (as per the methods described in Sec-tion 7.1), then we will consider them to be antony-mous (degree of antonymy is greater than zero)."
"If two words are used in senses pertaining to non-contrasting senses, then we will consider them to be not antonymous (degree of antonymy is equal to 0)."
"If the target words belong to the same thesaurus paragraphs as any of the seed antonyms linking the two contrasting categories, then the words are con-sidered to have a high degree of antonymy."
This is because words that occur in the same thesaurus para-graph tend to be semantically very close in mean-ing.
"Relying on the co-occurrence hypothesis, we claim that for word pairs listed in contrasting cate-gories, the greater their tendency to co-occur in text, the higher their degree of antonymy."
We use mutual information to capture the tendency of word–word co-occurrence.
"If the target words do not both belong to the same paragraphs as a seed antonym pair, but occur in con-trasting categories, then the target words are consid-ered to have a low or medium degree of antonymy (less antonymous than the word pairs discussed above)."
"Such word pairs that have a higher tendency to co-occur are considered to have a medium degree of antonymy, whereas those that have a lower ten-dency to co-occur are considered to have a low de-gree of antonymy."
Co-occurrence statistics for this purpose were col-lected from the Google n-gram corpus[REF_CITE]. [Footnote_2] Words that occurred within a window of 5 words were considered to be co-occurring.
"2 We used the Google n-gram corpus is created from a text collection of over 1 trillion words. We intend to use the same corpus (and not the BNC) to determine semantic distance as well, in the near future."
"Even though antonymy applies to pairs of word and sense combinations, most available texts are not sense-annotated."
"If antonymous occurrences are to be exploited for any of the purposes listed in the be-ginning of this paper, then the text must be sense disambiguated."
"However, word sense disambigua-tion is a hard problem."
"Yet, and to some extent be-cause unsupervised word sense disambiguation sys-tems perform poorly, much can be gained by using simple heuristics."
"For example, it has been shown that cohesive text tends to have words that are close in meaning rather than unrelated words."
"This, along with the distributional hypothesis of antonyms, and the findings[REF_CITE](antony-mous concepts tend to occur more often than chance in the same sentence), suggests that if we find a word pair in a sentence such that two of its senses are strongly contrasting (as per the algorithm described in Section 7.2.2), then it is probable that the two words are used in those contrasting senses."
"In order to best evaluate a computational measure of antonymy, we need a task that not only requires knowing whether two words are antonymous but also whether one word pair is more antonymous than another pair."
"Therefore, we evaluated our system on a set of closest-opposite questions."
Each question has one target word and five alternatives.
The objec-tive is to identify that alternative which is the closest opposite of the target.
"For example, consider: adulterate: a. renounce b. forbid c. purify d. criticize e. correct"
Here the target word is adulterate.
"One of the al-ternatives provided is correct, which as a verb has a meaning that contrasts with that of adulterate; how-ever, purify has a greater degree of antonymy with adulterate than correct does and must be chosen in order for the instance to be marked as correctly answered."
"This evaluation is similar to how oth-ers have evaluated semantic distance algorithms on TOEFL synonym questions[REF_CITE], except that in those cases the system had to choose the al-ternative which is closest in meaning to the target."
We looked on the World Wide Web for large sets of closest antonym questions.
We found two inde-pendent sets of questions designed to prepare stu- dents for the Graduate Record Examination. 3 The first set consists of 162 questions.
We used this set to develop our approach and will refer to it as the de-velopment set.
"Even though the algorithm does not have any tuned parameters per se, the development set helped determine which cues of antonymy were useful and which were not."
The second set has 1208 closest-opposite questions.
We discarded questions that had a multiword target or alternative.
"After re-moving duplicates we were left with 950 questions, which we used as the unseen test set."
"Interestingly, the data contains many instances that have the same target word used in different senses."
For example: (1) obdurate: a. meager b. unsusceptible c. right d. tender e. intelligent (2) obdurate: a. yielding b. motivated c. moribund d. azure e. hard ([Footnote_3]) obdurate: a. transitory b. commensurate c. complaisant d. similar e. uncommunicative
3 Both datasets are apparently in the public domain and will be made available on request.
"In (1), obdurate is used in the HARDENED IN FEEL - INGS sense and the closest opposite is tender."
"In (2), it is used in the RESISTANT TO PERSUASION sense and the closest opposite is yielding."
"In (3), it is used in the PERSISTENT sense and the closest opposite is transitory."
The datasets also contain questions in which one or more of the alternatives is a near-synonym of the target word.
For example: astute: a. shrewd b. foolish c. callow d. winning e. debating
Observe that shrewd is a near-synonym of astute.
The closest-opposite of astute is foolish.
"A man-ual check of a randomly selected set of 100 test-set questions revealed that, on overage, one in four had a near-synonym as one of the alternative."
We used the algorithm proposed in Section 7 to auto-matically solve the closest-opposite questions.
"Since individual words may have more than one mean-ing, we relied on the hypothesis that the intended sense of the alternatives are those which are most antonymous to one of the senses of the target word. (This follows from the discussion earlier in Section 7.2.3.)"
So for each of the alternatives we used the target word as context (but not the other alterna-tives).
We think that using a larger context to de-termine antonymy will be especially useful when the target words are found in sentences and natural text—something we intend to explore in the future.
Table 2 presents results obtained on the develop-ment and test data using different combinations of the seed sets and the adjacency heuristic.
"If the sys-tem did not find any evidence of antonymy between the target and any of its alternatives, then it refrained from attempting that question."
"We therefore report precision (number of questions answered correctly / number of questions attempted), recall (number of questions answered correctly / total number  of ques-tions), and F-score values (2 P R P R )."
Observe that all results are well above the ran-dom baseline of 0.20 (obtained when a system ran-domly guesses one of the five alternatives to be the answer).
"Also, using only the small set of sixteen affix rules, the system performs almost as well as when it uses 10,807 WordNet antonym pairs."
"Using both the affix-generated and the WordNet seed sets, the system obtains markedly improved precision and coverage."
Using only the adjacency heuristic gave best precision values (upwards of 0.8) with substan- tial coverage (attempting close to half the questions).
"However, best overall performance was obtained us-ing both seed sets and the adjacency heuristic (F-score of 0.7)."
"These results show that, to some degree, the auto-matic approach does indeed mimic human intuitions of antonymy."
"In tasks that require higher precision, using only the adjacency heuristic is best, whereas in tasks that require both precision and coverage, the seed sets may be included."
"Even when both seed sets were included, only four instances in the develop-ment set and twenty in the test set had target–answer pairs that matched a seed antonym pair."
"For all re-maining instances, the approach had to generalize to determine the closest opposite."
"This also shows that even the seemingly large number of direct and in-direct antonyms from WordNet (more than 10,000) are by themselves insufficient."
"The comparable performance obtained using the affix rules alone suggests that even in languages without a wordnet, substantial accuracies may be achieved."
"Of course, improved results when using WordNet antonyms as well suggests that the infor-mation they provide is complementary."
Error analysis revealed that at times the system failed to identify that a category pertaining to the target word contrasted with a category pertaining to the answer.
Additional methods to identify seed antonym pairs will help in such cases.
Certain other errors occurred because one or more alternatives other than the official answer were also antonymous to the target.
"For example, the system chose accept as the opposite of chasten instead of reward."
We have proposed an empirical approach to antonymy that combines corpus co-occurrence statistics with the structure of a published thesaurus.
The method can determine the degree of antonymy or contrast between any two thesaurus categories (sets of words representing a coarse concept) and between any two word pairs.
We evaluated the ap-proach on a large set of closest-opposite questions wherein the system not only identified whether two words are antonymous but also distinguished be- tween pairs of antonymous words of different de-grees.
It achieved an F-score of 0.7 in this task where the random baseline was only 0.2.
"When aiming for high precision it scores over 0.8, but there is some drop in the number of questions attempted."
"In the process of developing this approach we validated the co-occurrence hypothesis proposed[REF_CITE]on a large set of 1000 noun, verb, and adjective pairs."
We also gave empirical proof that antonym pairs tend to be used in similar contexts— the distributional hypothesis for antonyms.
Our future goals include porting this approach to a cross-lingual framework in order to determine antonymy in a resource-poor language by combin-ing its text with a thesaurus from a resource-rich language.
We will use antonym pairs to identify contrast relations between sentences to in turn im-prove automatic summarization.
"We also intend to use the approach proposed here in tasks where key-word matching is especially problematic, for exam-ple, separating paraphrases from contradictions."
"Some phrases can be interpreted either id-iomatically (figuratively) or literally in con-text, and the precise identification of idioms is indispensable for full-fledged natural lan-guage processing (NLP)."
"To this end, we have constructed an idiom corpus for Japanese."
This paper reports on the corpus and the re-sults of an idiom identification experiment us-ing the corpus.
"The corpus targets 146 am-biguous idioms, and consists of 102,846 sen-tences, each of which is annotated with a lit-eral/idiom label."
"For idiom identification, we targeted 90 out of the 146 idioms and adopted a word sense disambiguation (WSD) method using both common WSD features and idiom-specific features."
"The corpus and the experi-ment are the largest of their kind, as far as we know."
"As a result, we found that a standard supervised WSD method works well for the idiom identification and achieved an accuracy of 89.25% and 88.86% with/without idiom-specific features and that the most effective idiom-specific feature is the one involving the adjacency of idiom constituents."
Some phrases like kick the bucket are ambiguous with regard to whether they carry literal or idiomatic meaning in a certain context.
This ambiguity needs to be resolved in the same manner as ambiguous words that have been dealt with in the WSD liter-ature.
"We term the resolution of the literal/idiomatic ambiguity as idiom identification, hereafter."
Idiom identification is classified into two kinds; one is for idiom types and the other is for idiom to- kens.
"With the former, phrases that can be inter-preted as idioms are found in text corpora, typically for compiling idiom dictionaries."
"On the other hand, the latter helps identify a phrase in context as a true idiom or a phrase that should be interpreted literally (a literal phrase, henceforth)."
"In this paper, we deal with the latter, i.e., idiom token identification."
"Despite the recent enthusiasm for multiword ex-pressions (MWEs)[REF_CITE], the idiom token identification is in an early phase of its development."
"Given that many NLP tasks like machine translation or parsing have been developed as a result of the availability of lan-guage resources, idiom token identification should also be developed when adequate idiom resources are provided."
"To this end, we have constructed a Japanese idiom corpus."
We have also conducted an idiom identification experiment using the corpus that we hope will be a good reference point for fu-ture studies on the task.
We drew on a standard WSD framework with machine learning exploiting both features commonly used in the WSD studies and idiom-specific features.
"This paper reports in detail the corpus and the result of the experiment; herein, it must be noted that to the best of our knowl-edge, the corpus and the experiment are the largest ever of their kind."
We only deal with the ambiguity between lit-eral and idiomatic interpretations.
"However, some phrases have two or more idiomatic meanings with-out context."
"For example, a Japanese idiom te-o dasu (hand- ACC stretch) [Footnote_1] can be interpreted as ei- ther “punch,” “steal” or “make moves on.”"
1 ACC is the accusative case marker. Likewise we use the following notation in this paper; NOM for the nominative case
This kind of ambiguity should be placed on the agenda.
We do not tackle the problem of what constitutes the notion of “idiom.”
We simply regard phrases listed[REF_CITE]as idioms.
The reminder of this paper is organized as fol-lows.
In §2 we present related works. §3 shows the target idioms.
"After the idiom corpus is described in §4, we detail our idiom identification method and experiment in §5."
Finally §6 concludes the paper.
There have only been a few works on the con-struction of an idiom corpus.
"In this regard,[REF_CITE]and[REF_CITE]are no-table exceptions."
"They call the corpus TroFi Exam-ple Base, which is available on the Web. [Footnote_2][REF_CITE]compiled a corpus of English verb-noun combinations (VNCs) tokens."
"Their corpus deals with 53 VNC expressions and consists of about [URL_CITE],000 example sentences."
"Like ours, they assigned each example with a label indicating whether an ex-pression in the example is used literally or idiomati-cally."
Our corpus can be regarded as the Japanese idiom counterpart of these works.
"However, note that our corpus targets 146 idioms and consists of as many as 102,846 example sentences."
"Another ex-ception is[REF_CITE], who manually con-structed an example database of Japanese compound functional expressions named MUST."
They provide it on the Web. [URL_CITE]
Some of the compound functional expressions in Japanese are ambiguous like idioms are. [Footnote_4]
"4 For example, (something)-ni-atatte ((something)- DAT -run.into) means either “run into (something)” or “on the occa-sion of (something).” The former is the literal interpretation and the latter is the idiomatic interpretation of the compound func-tional expression."
The SAID dataset [Footnote_5] provides data about the syn-tactic flexibility of English idioms.
It does not con-cern itself with idiom token identification.
Previous studies have mostly focused on the id-iom type identificati[REF_CITE].
"However, there has been a growing interest in idiom token identification in re-cent times[REF_CITE]."
"Although their task is ex-actly the same as ours and we draw on the gram-matical knowledge provided by them, the scale of their experiment is very small, since only 108 sen-tences were used for idiom identification in their pa-per."
"Further, unlike HSU, we employ matured WSD technologies."
These studies used only the characteristics of id-ioms (or MWEs).
"On the other hand, we exploit a WSD method, for which there have been many studies and matured technologies, in addition to the characteristics of idioms."
"However, they employed an unsu-pervised method, while ours is a completely super-vised one."
"Apart from idioms,[REF_CITE]con-ducted the token classification of Japanese com-pound verbs exploiting supervised method."
"For this study, we selected 146 idioms through the following procedure. 1 We extracted basic idioms[REF_CITE]."
"Sato compiled about 3,600 basic idioms of Japanese from five books: two dictionaries for elementary school, two idiom dictionaries, and one linguistics book on idioms."
We extracted those idioms that were described in more than two of these five books.
The total number of such idioms added up to 926. 2
"From among these idioms, we chose ambiguous ones. [Footnote_6] As a result, 146 idioms were se-lected."
6 Some idioms like by and large do not have a literal mean-ing. They are not dealt with in this paper.
"As for 2 , sometimes it is not trivial to determine if an idiom is ambiguous or not."
"Some idioms are rarely interpreted literally, while others, in all likeli-hood, take on the literal meaning."
Is it meaningful to regard them as ambiguous and deal with them in this study?
"If not, how does one assuredly distinguish truly ambiguous idioms from those that are mostly interpreted either literally or figuratively?"
This can only be done if there is an accurate idiom identifica-tion system.
"After all, we asked two native speakers of Japanese (Group A) to classify idioms into two classes: 1) truly ambiguous ones and 2) completely unambiguous or practically unambiguous ones."
"On the basis of the classification, one of the authors made final judgments."
"To verify how stable this ambiguity endorsement was, we asked another two other native speakers of Japanese (Group B) to perform the same task and calculated the Kappa statistic between the two speakers."
"First, we sampled 101 idioms from the 926 chosen earlier."
"Then, the two members of Group B classified the sampled idioms into the two classes."
"The Kappa statistic was found to be 0.6576, which indicates middling stability."
The corpus is designed for the idiom token iden-tification task.
"That is, each example sentence in the corpus is annotated with a label that indicates whether the corresponding phrase in the example is used as an idiom or a literal phrase."
We call the for-mer the positive example and the latter the negative example.
"More specifically, the corpus consists of lines that each represent one example."
A line con-sists of four fields as follows: 1 Label indicates whether the example is positive or negative.
Label i is used for positive examples and l for negative ones. 2 ID denotes the idiom that is included in the exam-ple.
"In this study, each idiom has a unique num-ber, which is based[REF_CITE]. 3 Lemma also shows the idiom in the example."
We assigned each idiom its canonical (or standard) form on the basis[REF_CITE]. 4 Example is the example itself.
Given below is a sample of a negative example of goma-o suru (sesame- ACC crush) ’flatter’. • l 1417 !&quot;#$% $&amp;&apos;(!&quot;#$&amp; ···
The third field is the lemma of the idiom.
The last one is the example that says ’crushing sesame in a mortar...’
"Before working on the corpus construction, we prepared a reference by which human annotators could consistently distinguish between the literal and figurative meanings of idioms."
"To be more pre-cise, this reference specified literal and idiomatic meanings for each idiom like dictionaries do."
"For example, the entry for goma-o suru in the reference is as follows."
Idiom: To flatter people.
Literal: To crush sesame.
"As for the corpus size, we continued to anno-tate examples for each idiom, regardless of the pro-portion of idioms and literal phrases, until the total number of examples for each idiom reached 1,000. [Footnote_7]"
"7 For idioms that we sampled for preliminary annotation, we annotated more than 1,000 examples."
"In the case of a shortage of original data, we anno-tated as many examples as possible."
The original data were sourced from the Japanese Web corpus[REF_CITE].
"We constructed the corpus in the following man-ner: 1 From the Web corpus, we collected exam-ple sentences that contained one of our target id-ioms whichever meaning (positive or negative) they take on."
"Concretely speaking, we automatically col-lected sentences in which constituent words of one of our targets appeared in a canonical dependency relationship by using KNP [URL_CITE] , a Japanese dependency parser. 2"
We classified the collected examples as positive and negative.
This was done by human an-notators and was based on the reference to distin-guish the two meanings.
"For annotation, longer ex-amples were given higher priority than shorter ex-amples."
Note that we discarded examples that were collected by mistake due to dependency parsing er-rors and those that lacked a context that could help them be interpreted correctly.
This was done by the two members of Group A and took 230 hours.
"The corpus consists of 102,846 examples. [Footnote_9] Figure 1 shows the distribution of the number of examples."
9 Note that the figures reported here are for the corpus of the 2008-06-25 version and will be slightly changed over time.
"However, we annotated less than 100 examples for 17 idioms because of inadequate original data."
The average number of words in a sentence is 46.
Idiom in Figure 2 shows the distribution of sen-tence length (the number of words) in the corpus.
"Web and News indicate the sentence length in the Web and a newspaper corpora, respectively."
This is drawn[REF_CITE].
"As you see, our corpus contains many more long sen-tences."
"This is because longer sentences were given priority for annotation, as stated in §4.2."
Figure 3 shows the longest and shortest examples each for lit-eral and idiomatic meanings of goma-o suru drawn from the corpus.
"To determine how consistent the positive/negative annotation is across different human annotators, we sampled 1,421 examples from the corpus, asked the two members of Group B to do the same annota-tion, and calculated the Kappa statistic between the two."
"The value was 0.8519, which indicates very high agreement."
The corpus is available on the Web. [URL_CITE]
"Currently we provide the list of the basic Japanese idioms we are dealing with, the idiom corpus, and the vector representation data used for the idiom identification experiment."
The corpus is protected under the BSD license.
We adopted a standard WSD method using machine learning.
"More specifically, we used SVM[REF_CITE]with a quadratic kernel implemented in TinySVM. [Footnote_11]"
"The features we used are classified into either those that have been commonly used in WSD on the lines[REF_CITE](LN hereafter), or those that have been designed for Japanese idiom identification proposed by HSU. 12 • Common WSD Features f1:"
POS of three words on the left side of idiom and three words on the right side f2:
Local collocations f3:
Single words in the surrounding context f4a: Lemma of the rightmost word among those words that are the dependents of the leftmost constituent word of idiom [Footnote_13] f4b:
13 Note that Japanese is a head final language.
POS of the rightmost word among those words that are the dependents of the left-most constituent word of idiom f5a: Lemma of the word which the rightmost constituent word of idiom is the dependent of f5b:
POS of the word which the rightmost con-stituent word of idiom is the dependent of f6: Hypernyms of words in the surrounding context f7: Domains of words[REF_CITE]in the surrounding context • Idiom-Specific Features f8:
Adnominal modification flag f9:
Topic case marking flag f10: Voice alternation flag f11: Negation flag f[Footnote_12]: Volitional modality flag f13: Adjacency flag
12 Remember that HSU implemented them in handcrafted rules. We adapted them to a machine learning framework.
"We used[REF_CITE]a morphological analyzer of Japanese, and KNP to extract these features. 14[URL_CITE]f2 and f3 are the same as those described in LN."
But f1 is slightly different in that we did not use the P 0 of LN. f4 and f5 roughly correspond to the syn-tactic relations of LN.
We adapted it to Japanese id-ioms along with some simplifications.
"In the case of the example of mune-o utu (chest- ACC hit) ‘impress’ below, [Footnote_15] f4 is the POS and lemma of tyousyu and f5 corresponds to those of uta. [Footnote_16] • tyousyu-no mune-o utu utukusi uta audience- GEN chest- ACC hit beautiful song ‘A beautiful song that impresses the audience’ f6 and f7 are available from JUMAN’s output."
15 The arrows indicate dependency relations.
"16 Functional words attaching to either the f4 word or the f5 word are ignored. In the example, no ( GEN ) is ignored."
"For example, the hypernym of tyousyu (audience) is human and its domain is culture/media."
Those of uta (song) are abstract-thing and culture/recreation.
"They are not used in LN, but they are known to be useful for WSD[REF_CITE]. f8 indicates whether a nominal constituent of an idiom, if any, undergoes adnominal modification. f9 indicates whether one of Japanese topic case mark-ers is attached to a nominal constituent of an idiom, if any. f10 is turned on when a passive or causative suffix is attached to a verbal constituent of an idiom, if any. [Footnote_17] f11 and f12 are similar to f10."
"17 Passivization is indicated by the suffix (r)are in Japanese. But the same suffix is also used for honorification, potentials and spontaneous potentials. Since it is beyond the current tech-nology, we gave up distinguishing them."
"The former is used for negated forms and the latter for volitional modality suffixes of a predicate part of an idiom, if any. [Footnote_18] Volitional modality includes expressions like order, request, permission, prohibition, and volition."
"18 Note that f10, f11 and f12 are applied to only those idioms that can be used as predicates."
"Finally, f13 indicates whether the constituents of an idiom is adjacent to each other."
"As discussed in HSU, the idiom-specific fea-tures are effective to distinguish idioms from lit-eral phrases."
"For example, the idiom goma-o suru does not allow adnominal modification, while its lit-eral counterpart does."
"Similarly, the idiom mune-o utu cannot take volitional modality unlike its literal counterpart."
"In the experiment, we dealt with 90 idioms for which more than 50 examples for both idiomatic and literal usages were available. [Footnote_19]"
"19 Some examples were unavailable due to the feature extrac-tion failure. Thus, examples used for the experiment are fewer in number than those included in the corpus."
We conducted experiments for each idiom.
The performance measure is the accuracy. # of examples correctly identified Accuracy = # of all example
The baseline system uniformly regards all ex-amples as either positive or negative depending on which is more dominant in the idiom corpus.
"Natu-rally, this is prepared for each idiom. max(# of positive, # of negative) Baseline = # of all example"
The accuracy and the baseline accuracy for each idiom are calculated in a 10-fold cross validation style; we split examples of an idiom into 10 pieces in advance of the experiment.
"Also, we calculated the overall accuracy and baseline accuracy from the individual results."
"We summed up all accuracy scores of all the 90 idioms and then divided it by 90, which is called the macro-average."
"We did this for the baseline accuracy, too."
Another performance measure is the relative error reduction (RER). [Footnote_20]
20 ER stands for Error Rate in the formula.
ER of baseline − ER of system RER = ER of baseline
The overall RER is calculated from the overall ac-curacy and baseline by the above formula.
Table 1 shows the overall performance.
The first col-umn is the baseline accuracy (%).
The second col-umn is the accuracy (%) and relative error reduction (%) of the system without the idiom-specific fea-tures.
The third column is those of the system with the idiom features.
Tables 2 and 3 show the individ-ual results of the 90 idioms.
The first column shows the target idioms.
The second column shows base-line accuracy (%) and the numbers of positive and negative examples for each idiom.
The accuracy (%) and relative error reduction (%) of the system with-out the idiom-specific features are described in the third column.
The fourth column is those of the sys-tem with the idiom features.
Bold face indicates a better performance.
"All in all, we see relatively high baseline perfor-mances."
"Nevertheless, both systems outperformed the baseline."
"Especially, the system without the idiom-specific features has a noticeable lead over the baseline, showing that WSD technologies are effec-tive in the idiom identification."
"Incorporating the id-iom features into the system improved the overall performance, which is statistically significant (Mc-Nemar test, p&lt;0.01)."
But performances of some id-ioms slightly degraded by the incorporation of the idiom features.
"Table 4 shows overall results without using one of the idiom features. [Footnote_21] As you see, the adjacency flag (f13) contributes to idiom identification accuracy the most. [Footnote_22]"
"21 The first row shows the result with all idiom features used, just for ease of reference."
22 Note that greater performance drop indicates greater con-tribution.
"On the other hand, the adnominal modifica-tion flag (f8) contributes to the task only slightly. [Footnote_23]"
"23 This result is inconsistent with the result obtained in HSU, where they reported that grammatical constraints involving ad-nominal modification was most effective. This inconsistency might be attributed to the differences of datasets being used for idiom identification experiment. HSU used only 108 sentences"
Table 5 shows the results reported in CFS.
Their baseline system regards all instances as idioms.
The performance of the supervised one is obtained by the method[REF_CITE].
"Though we cannot simply compare this with our results due to the difference in experimental conditions, this im-plies that our WSD-based method was equally good or possibly better than their methods that are tailored to MWEs."
"In this paper, we reported on the idiom corpus we have constructed and the idiom identification exper-iment using the corpus."
"As mentioned in §4.3, some idioms are short of examples in the current idiom corpus."
We plan to collect more examples by using different characters.
"In the Japanese language, there are basically three character systems: Hiragana, Katakana, and Chinese characters."
"Thus, you can write an idiom in different characters."
"For example, mune-o utu (chest- ACC hit) ‘impress’ can be either 8#:ƒ or 8#Xƒ ."
"In spite of its imperfection, we are sure that we can learn a lot about the idiom identification from the corpus, since, as far as we know, it is the largest-ever one, and so is the idiom identification experi-ment reported in §5."
"Also, we showed that a standard supervised WSD method works well for the idiom identification."
Our system achieved the accuracy of 89.25% and 88.86% with/without idiom-specific features.
"Though we dealt with as many as 90 idioms, prac-tical NLP systems are required to deal with many more idioms."
"Toward a scalable idiom identifica-tion, we have to develop an unsupervised or semi-supervised method."
"The unsupervised method of for the experiment, while 75,011 sentences were used for our experiment."
"Also, the dataset of HSU came from newspaper articles, while our dataset came from the web."
"Fortu-nately, the Japanese WordNet is now available[REF_CITE], thus we can try their method."
"Also, CFS propose a language-independent unsu-pervised method."
These could be of help.
"At any rate, our idiom corpus will play an im-portant role in the development of unsupervised or semi-supervised methods, and the experimental re-sults obtained in this study will be a good reference point to evaluate those methods."
The accuracy of current word sense disam-biguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples.
"Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evalua-tion involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory."
"We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain."
"To address this issue, we propose combining a domain adaptation tech-nique using feature augmentation with active learning."
Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new do-main.
"Finally, we propose that one can maxi-mize the dual benefits of reducing the annota-tion effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types."
"In language, many words have multiple meanings."
"The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD)."
WSD is one of the funda-mental problems in natural language processing and is important for applications such as machine trans-lation (MT) ([REF_CITE];
WSD is typically viewed as a classification prob-lem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process.
"In current WSD research, WordNet[REF_CITE]is usually used as the sense inventory."
"WordNet, however, adopts a very fine level of sense granularity, thus restricting the accu-racy of WSD systems."
"Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data."
"To provide a standardized test-bed for evalua-tion of WSD systems, a series of evaluation exer-cises called SENSEVAL were held."
"In the English all-words task of SENSEVAL-2 and SENSEVAL-3[REF_CITE], no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses."
"In SENSEVAL-2, the best per-forming system[REF_CITE]in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best perform-ing system[REF_CITE]achieved an accu-racy of 65.2%."
"In[REF_CITE]which was the most recent SENSEVAL evaluation, a similar En-glish all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in run-ning English texts."
"For this task, the best perform-ing system[REF_CITE]achieved an accuracy of 59.1%."
"Results of these evaluations showed that state-of-the-art English all-words WSD systems per-formed with an accuracy of 60%–70%, using the fine-grained sense inventory of WordNet."
"The low level of performance by these state-of-the-art WSD systems is a cause for concern, since WSD is supposed to be an enabling technology to be incorporated as a module into applications such as MT and IR."
"As mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too fine-grained."
"As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s."
"To address this issue, a coarse-grained English all-words task[REF_CITE]was conducted[REF_CITE]."
This task used a coarse-grained version of WordNet and reported an ITA of around 90%.
"We note that the best performing system[REF_CITE]of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropri-ate level of sense granularity."
Another issue faced by current WSD systems is the lack of training data.
We note that the top per-forming systems mentioned in the previous para-graphs are all based on supervised learning.
"With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data."
"Since it is time consuming to per-form sense annotation of word occurrences, only a handful of sense-tagged corpora are publicly avail-able."
"Among the existing sense-tagged corpora, the S EM C OR corpus[REF_CITE]is one of the most widely used."
"In S EM C OR , content words have been manually tagged with WordNet senses."
"Cur-rent supervised WSD systems (which include all the top-performing systems in the English all-words task) usually rely on this relatively small manually annotated corpus for training examples, and this has inevitably affected the accuracy and scalability of current WSD systems."
"Related to the problem of a lack of training data for WSD, there is also a lack of test data."
Having a large amount of test data for evaluation is impor-tant to ensure the robustness and scalability of WSD systems.
"Due to the expensive process of manual sense-tagging, the SENSEVAL English all-words task evaluations were conducted on relatively small sets of evaluation data."
"For instance, the evaluation data of SENSEVAL-2 and SENSEVAL-3 English all-words task consists of 2,473 and 2,041 test exam-ples respectively."
"In[REF_CITE]the fine-grained English all-words task consists of only 465 test ex- amples, while the[REF_CITE]coarse-grained En-glish all-words task consists of 2,269 test examples."
"Hence, it is necessary to address the issues of sense granularity, and the lack of both training and test data."
"To this end, a recent large-scale anno-tation effort called the OntoNotes project[REF_CITE]was started."
"Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank[REF_CITE], the project added several new layers of semantic annotations, such as coreference information, word senses, etc."
"In its first release[REF_CITE]through the Lin-guistic Data Consortium (LDC), the project man-ually sense-tagged more than 40,000 examples be-longing to hundreds of noun and verb types with an[REF_CITE]%, based on a coarse-grained sense inven-tory, where each word has an average of only 3.2 senses."
"Thus, besides providing WSD examples that were sense-tagged with a high ITA, the project also addressed the previously discussed issues of a lack of training and test data."
"In this paper, we use the sense-tagged data pro-vided by the OntoNotes project to investigate the accuracy achievable by current WSD systems when adopting a coarse-grained sense inventory."
"Through our experiments, we then highlight that domain adaptation for WSD is an important issue as it sub-stantially affects the performance of a state-of-the-art WSD system which is trained on S EM C OR but evaluated on sense-tagged examples in OntoNotes."
"To address this issue, we then show that by com-bining a domain adaptation technique using feature augmentation with active learning, one only needs to annotate a small amount of in-domain examples to obtain a substantial improvement in the accuracy of the WSD system which is previously trained on out-of-domain examples."
The contributions of this paper are as follows.
"To our knowledge, this is the first large-scale WSD evaluation conducted that involves hundreds of word types and tens of thousands of sense-tagged exam-ples, and that is based on a coarse-grained sense in-ventory."
"The present study also highlights the practi-cal significance of domain adaptation in word sense disambiguation in the context of a large-scale empir-ical evaluation, and proposes an effective method to address the domain adaptation problem."
"In the next section, we give a brief description of our WSD system."
"In Section 3, we describe exper-iments where we conduct both training and evalu-ation using data from OntoNotes."
"In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a differ-ent domain as compared to the OntoNotes evalua-tion data."
"In Section 5, we perform domain adapta-tion experiments using a recently introduced feature augmentation technique."
"In Section 6, we investi-gate the use of active learning to reduce the annota-tion effort required to adapt our WSD system to the domain of the OntoNotes data, before concluding in Section 7."
"For the experiments reported in this paper, we fol-low the supervised learning approach[REF_CITE], by training an individual classifier for each word using the knowledge sources of local col-locations, parts-of-speech (POS), and surrounding words."
"For local collocations, we use 11 features: C −1,−1 , C 1,1 , C −2,−2 , C 2,2 , C −2,−1 , C −1,1 , C [Footnote_1],2 , C −3,−1 , C −2,1 , C −1,2 , and C [Footnote_1],3 , where C i,j refers to the ordered sequence of tokens in the local context of an ambiguous word w. Offsets i and j denote the starting and ending position (relative to w) of the se-quence, where a negative (positive) offset refers to a token to its left (right)."
"1 We removed erroneous examples which were simply tagged with ‘XXX’ as sense-tag, or tagged with senses that were not found in the sense-inventory provided. Also, since we will be comparing against training on S EM C OR later (which was tagged using WordNet senses), we removed examples tagged with OntoNotes senses which were not mapped to WordNet senses. On the whole, about 7% of the original OntoNotes ex-amples were removed as a result."
"1 We removed erroneous examples which were simply tagged with ‘XXX’ as sense-tag, or tagged with senses that were not found in the sense-inventory provided. Also, since we will be comparing against training on S EM C OR later (which was tagged using WordNet senses), we removed examples tagged with OntoNotes senses which were not mapped to WordNet senses. On the whole, about 7% of the original OntoNotes ex-amples were removed as a result."
"For parts-of-speech, we use 7 features: P −3 , P −2 , P −1 , P 0 , P 1 , P 2 , P 3 , where P 0 is the POS of w, and P −i (P i ) is the POS of the ith token to the left (right) of w."
"For surrounding words, we consider all unigrams (single words) in the surrounding context of w."
These words can be in a different sentence from w.
"For our experiments re-ported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance[REF_CITE]."
"The annotated data of OntoNotes is drawn from the Wall Street Journal (WSJ) portion of the Penn Tree-bank corpus, divided into sections 00-24."
"These WSJ documents have been widely used in various NLP tasks such as syntactic parsing[REF_CITE]and semantic role labeling (SRL) (Carreras and Mar- quez, 2005)."
"In these tasks, the practice is to use documents[REF_CITE]-21 as training data and[REF_CITE]as test data."
"Hence for our ex-periments reported in this paper, we follow this con-vention and use the annotated instances[REF_CITE]-21 as our training data, and instances[REF_CITE]as our test data."
"As mentioned in Section 1, the OntoNotes data provided WSD examples for a large number of nouns and verbs, which are sense-tagged accord-ing to a coarse-grained sense inventory."
"In Table 1, we show the amount of sense-tagged data available from OntoNotes, across the various WSJ sections. 1"
"In the table, for each WSJ section, we list the num-ber of word types, the number of sense-tagged ex-amples, and the cumulative count on the number of sense-tagged examples."
"From the table, we see that sections 02-21, which will be used as training data in our experiments, contain a total of slightly over 31,000 sense-tagged examples."
"Using examples from sections 02-21 as training data, we trained our WSD system and evaluated on the examples from section 23."
"In our experiments, if a word type in section 23 has no training exam-ples from sections 02-21, we randomly select an OntoNotes sense as the answer."
"Using these ex-perimental settings, our WSD system achieved an accuracy of 89.1%."
We note that this accuracy is much higher than the 60%–70% accuracies achieved by state-of-the-art English all-words WSD systems which are trained using the fine-grained sense inven-tory of WordNet.
"Hence, this highlights the impor-tance of having an appropriate level of sense granu-larity."
"Besides training on the entire set of examples from sections 02-21, we also investigated the per-formance achievable from training on various sub-sections of the data and show these results as “ON” in Figure 1."
"From the figure, we see that WSD accu-racy increases as we add more training examples."
"The fact that current state-of-the-art WSD sys-tems are able to achieve a high level of perfor-mance is important, as this means that WSD systems will potentially be more usable for inclusion in end-applications."
"For instance, the high level of perfor-mance by syntactic parsers allows it to be used as an enabling technology in various NLP tasks."
"Here, we note that the 89.1% WSD accuracy we obtained is comparable to state-of-the-art syntactic parsing ac-curacies, such as the 91.0% performance by the sta-tistical parser[REF_CITE]."
"Although our WSD system had achieved a high accuracy of 89.1%, this was achieved by train-ing on a large amount (about 31,000) of manually sense annotated examples from sections 02-21 of the OntoNotes data."
"Further, all these training data and test data are gathered from the same domain of WSJ."
"In reality, however, since manual sense annotation is time consuming, it is not feasible to collect such a large amount of manually sense-tagged data for ev- ery domain of interest."
"Hence, in this section, we in-vestigate the performance of our WSD system when it is trained on out-of-domain data."
"In the English all-words task of the previous SEN-SEVAL evaluations (SENSEVAL-2, SENSEVAL-3,[REF_CITE]), the best performing English all-words task systems with the highest WSD ac-curacy were trained on S EM C OR[REF_CITE]."
"Hence, we similarly trained our WSD sys-tem on S EM C OR and evaluated on section 23 of the OntoNotes corpus."
"For those word types in section 23 which do not have training examples from S EM - C OR , we randomly chose an OntoNotes sense as the answer."
"In training on S EM C OR , we have also ensured that there is a domain difference between our training and test data."
"This is because while the OntoNotes data was gathered from WSJ, which contains mainly business related news, the S EM C OR corpus is the sense-tagged portion of the Brown Cor-pus (BC), which is a mixture of several genres such as scientific texts, fictions, etc."
"Evaluating on the section 23 test data, our WSD system achieved only 76.2% accuracy."
"Compared to the 89.1% accuracy achievable when we had trained on examples from sections 02-21, this is a substan-tially lower and disappointing drop of performance and motivates the need for domain adaptation."
The need for domain adaptation is a general and important issue for many NLP tasks[REF_CITE].
"For instance, SRL systems are usu-ally trained and evaluated on data drawn from the WSJ."
"In the[REF_CITE]shared task on SRL[REF_CITE], however, a task of train-ing and evaluating systems on different domains was included."
"For that task, systems that were trained on the PropBank corpus[REF_CITE](which was gathered from the WSJ), suffered a 10% drop in accuracy when evaluated on test data drawn from BC, as compared to the performance achievable when evaluated on data drawn from WSJ."
"More re-cently,[REF_CITE]included a shared task on de-pendency parsing[REF_CITE]."
"In this task, systems that were trained on Penn Treebank (drawn from WSJ), but evaluated on data drawn from a different domain (such as chemical abstracts and parent-child dialogues) showed a similar drop in per-formance."
"For research involving training and eval- uating WSD systems on data drawn from different domains, several prior research efforts[REF_CITE]observed a similar drop in performance of about 10% when a WSD system that was trained on the BC part of the DSO corpus was evaluated on the WSJ part of the corpus, and vice versa."
"In the rest of this paper, we perform domain adap-tation experiments for WSD, focusing on domain adaptation methods that use in-domain annotated data."
"In particular, we use a feature augmentation technique recently introduced[REF_CITE], and active learning[REF_CITE]to per-form domain adaptation of WSD systems."
"In this section, we will first introduce the A UGMENT technique[REF_CITE], before showing the performance of our WSD system with and without using this technique."
The A UGMENT technique introduced[REF_CITE]is a simple yet very effective approach to per-forming domain adaptation.
This technique is appli-cable when one has access to training data from the source domain and a small amount of training data from the target domain.
The technique essentially augments the feature space of an instance.
"Assuming x is an instance and its original feature vector is Φ(x), the augmented feature vector for instance x is"
"Φ ′ (x) = ( &lt; Φ(x), Φ(x), 0 &gt; if x ∈ D s , &lt; Φ(x), 0, Φ(x) &gt; if x ∈ D t where 0 is a zero vector of size |Φ(x)|, D s and D t are the sets of instances from the source and target domains respectively."
We see that the tech-nique essentially treats the first part of the aug-mented feature space as holding general features that are not meant to be differentiated between different domains.
"Then, different parts of the augmented fea-ture space are reserved for holding source domain specific, or target domain specific features."
"Despite its relative simplicity, this A UGMENT technique has been shown to outperform other domain adaptation techniques on various tasks such as named entity recognition, part-of-speech tagging, etc."
"As mentioned in Section 4, training our WSD sys-tem on S EM C OR examples gave a relatively low ac-curacy of 76.2%, as compared to the 89.1% accuracy obtained from training on the[REF_CITE]- 21 examples."
"Assuming we have access to some in-domain training data, then a simple method to poten-tially obtain better accuracies is to train on both the out-of-domain and in-domain examples."
"To investi-gate this, we combined the S EM C OR examples with various amounts of OntoNotes examples to train our WSD system and show the resulting “SC+ON” ac-curacies obtained in Figure 1."
"We also performed another set of experiments, where instead of simply combining the S EM C OR and OntoNotes examples, we applied the A UGMENT technique when combin-ing these examples, treating S EM C OR examples as out-of-domain (source domain) data and OntoNotes examples as in-domain (target domain) data."
We similarly show the resulting accuracies as “SC+ON Augment” in Figure 1.
"Comparing the “SC+ON” and “SC+ON Aug-ment” accuracies in Figure 1, we see that the A UG - MENT technique always helps to improve the ac-curacy of our WSD system."
"Further, notice from the first few sets of results in the figure that when we have access to limited in-domain training exam-ples from OntoNotes, incorporating additional out-of-domain training data from S EM C OR (either using the strategies “SC+ON” or “SC+ON Augment”) achieves better accuracies than “ON”."
Significance tests using one-tailed paired t-test reveal that these accuracy improvements are statistically significant at the level of significance 0.01 (all significance tests in the rest of this paper use the same level of signif-icance 0.01).
These results validate the contribution of the SemCor examples.
This trend continues till the result for sections 02-06.
"The right half of Figure 1 shows the accuracy trend of the various strategies, in the unlikely event that we have access to a large amount of in-domain training examples."
"Although we observe that in this scenario, “ON” performs better than “SC+ON”, “SC+ON Augment” continues to perform better than “ON” (where the improvement is statistically significant) till the result for sections 02-09."
"Beyond that, as we add more OntoNotes examples, signif-icance testing reveals that the “SC+ON Augment” and “ON” strategies give comparable performance."
"This means that the “SC+ON Augment” strategy, besides giving good performance when one has few in-domain examples, does continue to perform well even when one has a large number of in-domain ex-amples."
"So far in this paper, we have seen that when we have access to some in-domain examples, a good strategy is to combine the out-of-domain and in-domain ex-amples via the A UGMENT technique."
"This suggests that when one wishes to apply a WSD system to a new domain of interest, it is worth the effort to an-notate a small number of examples gathered from the new domain."
"However, instead of randomly se-lecting in-domain examples to annotate, we could use active learning[REF_CITE]to help select in-domain examples to annotate."
"By doing so, we could minimize the manual annotation effort needed."
"In WSD, several prior research efforts have suc-cessfully used active learning to reduce the annota-tion effort required[REF_CITE]."
"With the exception[REF_CITE]which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain."
"Also, these prior research efforts only experimented with a few word types."
"In contrast, we perform active learning experiments on the hundreds of word types in the OntoNotes data, with the aim of adapting our WSD system trained on S EM C OR to the WSJ domain represented by the OntoNotes data."
"For our active learning experiments, we use the uncertainty sampling strategy[REF_CITE], as shown in Figure 2."
"For our experiments, the S EM C OR examples will be our initial set of training examples, while the OntoNotes examples from sections 02-21 will be used as our pool of adaptation examples, from which we will select ex-amples to annotate via active learning."
"Also, since we have found that the A UGMENT technique is use-ful in increasing WSD accuracy, we will apply the"
A UGMENT technique during each iteration of active learning to combine the S EM C OR examples and the selected adaptation examples.
"As shown in Figure 2, we train an initial WSD system using only the set D S of S EM C OR exam-ples."
We then apply our WSD system on the set D A of OntoNotes adaptation examples.
The example in D A which is predicted with the lowest confidence will be removed from D A and added to the set D T of in-domain examples that have been selected via active learning thus far.
"We then use the A UGMENT technique to combine the set of examples in D S and D T to train a new WSD system, which is then ap-plied again on the set D A of remaining adaptation examples, and this active learning process continues until we have used up all the adaptation examples."
"Note that because we are using[REF_CITE]-21 (which have already been sense-tagged be-forehand) as our adaptation data, the annotation of the selected example during each active learning it-eration is simply simulated by referring to its tagged sense."
"As mentioned earlier, we use the examples[REF_CITE]-21 as our adaptation exam- ples during active learning."
"Hence, we perform active learning experiments on all the word types that have sense-tagged examples[REF_CITE]-21, and show the evaluation results[REF_CITE]as the topmost “all” curve in Figure 3."
"Since our aim is to reduce the human an-notation effort required in adapting a WSD system to a new domain, we may not want to perform active learning on all the word types in practice."
"Instead, we can maximize the benefits by performing active learning only on the more frequently occurring word types."
"Hence, in Figure 3, we also show via var-ious curves the results of applying active learning only to various sets of word types, according to their frequency, or number of sense-tagged examples[REF_CITE]-21."
Note that the various ac-curacy curves in Figure 3 are plotted in terms of evaluation accuracies over all the test examples[REF_CITE]hence they are directly com-parable to the results reported thus far in this pa-per.
"Also, since the accuracies for the various curves stabilize after 35 active learning iterations, we only show the results of the first 35 iterations."
"From Figure 3, we note that by performing ac-tive learning on the set of 150 most frequently oc-curring word types, we are able to achieve a WSD accuracy of 82.6% after 10 active learning iterations."
"Note that in Section 4, we mentioned that training only on the out-of-domain S EM C OR examples gave an accuracy of 76.2%."
"Hence, we have gained an accuracy improvement of 6.4% (82.6% − 76.2%) by just using 1,500 in-domain OntoNotes examples."
"Compared with the 12.9% (89.1% − 76.2%) im-provement in accuracy achieved by using all 31,114[REF_CITE]-21 examples, we have ob-tained half of this maximum increase in accuracy, by requiring only about 5% (1,500/31,114) of the total number of sense-tagged examples."
"Based on these results, we propose that when there is a need to apply a previously trained WSD system to a different do-main, one can apply the A UGMENT technique with active learning on the most frequent word types, to greatly reduce the annotation effort required while obtaining a substantial improvement in accuracy."
"Using the WSD examples made available through OntoNotes, which are sense-tagged according to a coarse-grained sense inventory, we show that our WSD system is able to achieve a high accuracy of 89.1% when we train and evaluate on these ex-amples."
"However, when we apply a WSD system that is trained on S EM C OR , we suffer a substan-tial drop in accuracy, highlighting the need to per-form domain adaptation."
"We show that by com-bining the A UGMENT domain adaptation technique with active learning, we are able to effectively re-duce the amount of annotation effort required for do-main adaptation."
"Bootstrapping has a tendency, called seman-tic drift, to select instances unrelated to the seed instances as the iteration proceeds."
"We demonstrate the semantic drift of bootstrap-ping has the same root as the topic drift of Kleinberg’s HITS, using a simplified graph-based reformulation of bootstrapping."
"We confirm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task."
"Proposed algorithms achieve superior perfor-mance to Espresso and previous graph-based WSD methods, even though the proposed al-gorithms have less parameters and are easy to calibrate."
In recent years machine learning techniques be-come widely used in natural language processing (NLP).
These techniques offer various ways to ex-ploit large corpora and are known to perform well in many tasks.
"However, these techniques often re-quire tagged corpora, which are not readily available to many languages."
"So far, reducing the cost of hu-man annotation is one of the important problems for building NLP systems."
"To mitigate the problem of hand-tagging re-sources, semi(or minimally)-supervised and unsu-pervised techniques have been actively studied."
"Boot-strapping has been widely adopted in NLP applica-tions such as word sense disambiguati[REF_CITE], named entity recogniti[REF_CITE]and relation extracti[REF_CITE]."
"However, it is known that bootstrapping often ac-quires instances not related to seed instances."
"For example, consider the task of collecting the names of common tourist sites from web corpora."
"Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic pat-terns such as “pictures” and “photos,” which also co-occur with many other unrelated instances."
"The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.”"
This phenomenon is called semantic drift[REF_CITE].
"A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by."
"The recently pro-posed Espresso[REF_CITE]al-gorithm incorporates sophisticated scoring functions to cope with generic patterns, but[REF_CITE]pointed out, Espresso still shows se-mantic drift unless iterations are terminated appro-priately."
"Another deficiency in bootstrapping is its sensi-tivity to many parameters such as the number of seed instances, the stopping criterion of iteration, the number of instances and patterns selected on each it-eration, and so forth."
These parameters also need to be calibrated for each task.
"In this paper, we present a graph-theoretic anal-ysis of Espresso-like bootstrapping algorithms."
"We argue that semantic drift is inherent in these algo-rithms, and propose to use two graph-based algo-rithms that are theoretically less prone to semantic drift, as an alternative to bootstrapping."
"After a brief review of related work in Section 2, we analyze in Section 3 a bootstrapping algorithm (Simplified Espresso) which can be thought of as a degenerate version of Espresso."
"Simplified Espresso is simple enough to allow an algebraic treatment, and its equivalence to Kleinberg’s HITS algorithm[REF_CITE]is shown."
An implication of this equivalence is that semantic drift in this bootstrap-ping algorithm is essentially the same phenomenon as topic drift observed in link analysis.
Another im-plication is that semantic drift is inevitable in Sim-plified Espresso as it converges to the same score vector regardless of seed instances.
The original Espresso also suffers from the same problem as its simplified version does.
"It incorpo-rates heuristics not present in Simplified Espresso to reduce semantic drift, but these heuristics have lim-ited effect as we demonstrate in Section 3.3."
"In Section 4, we propose two graph-based algo-rithms to reduce semantic drift."
These algorithms are used in link analysis community to reduce the effect of topic drift.
In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed re-duce semantic drift.
"Finally, we conclude our work in Section 6."
Bootstrapping (or self-training) is a general frame-work for reducing the requirement of manual an-notation.
The idea of learning with a bootstrapping method was adopted for many tasks.
He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathe-matically well understood.
"It makes use of a translation dictionary and a comparable corpus to help disam-biguate word senses in the source language, by ex-ploiting the asymmetric many-to-many sense map-ping relationship between words in two languages."
They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes.
Stop classes are sets of terms known to cause semantic drift in particular seman-tic classes.
"However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes."
A major drawback of bootstrapping is the lack of principled method for selecting optimal param-eter values[REF_CITE].
"Also, there is an issue of generic patterns which deteriorates the quality of acquired instances."
Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift.
We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail.
What distinguishes Espresso from other bootstrapping algorithms is that it benefits from generic patterns by using a principled measure of instance and pattern reliability.
The key idea of Espresso is recursive definition of pattern-instance scoring metrics.
"The reliability scores of pattern p and instance i, denoted respectively as r π (p) and r ι (i), are given as follows: ∑ pmi(i,p) i∈I max pmi r ι (i) r π (p) = (1) |I| ∑ pmi(i,p) p∈P max pmi r π (p) r ι (i) = (2) |P | where |i, p| (3) pmi(i, p) = log 2 |i, ∗||∗, p| is pointwise mutual information between i and p, P and I are sets of patterns and instances, and |P | and |I| are the numbers of patterns and instances, respec-tively. |i, ∗| and |∗, p| are the frequencies of pattern p and instance i in a given corpus, respectively, and |i, p| is the frequency of pattern p which co-occurs with instance i. max pmi is a maximum value of the pointwise mutual information over all instances and patterns."
"The intuition behind these definitions is that a reliable pattern co-occurs with many reli-able instances, and a reliable instance co-occurs with many reliable patterns."
"Espresso and other bootstrapping methods iterate the following three phases: pattern induction, pat-tern ranking/selection, and instance extraction."
"We describe these phases below, along with the parameters that controls each phase."
Pattern Induction Induce patterns from a corpus given seed instances.
"Patterns may be sur-face text patterns, lexico-syntactic patterns, and/or just features."
Pattern Ranking/Selection Create a pattern ranker from a corpus using instances as fea-tures and select patterns which co-occur with seed instances for the next instance extraction phase.
The main issue here is to avoid ranking generic patterns high and to choose patterns with high relatedness to the seed instances.
Parameters and configurations: (a) a pattern scoring metrics and (b) the number of patterns to use for extraction of instances.
Instance Extraction Select high-confidence instances to the seed instance set.
"It is desirable to keep only high-confidence instances at this phase, as they are used as seed instances for the next iteration."
"Optionally, instances can be cumula-tively obtained on each iteration to retain highly rel-evant instances learned in early iterations."
"Parame-ters and configurations: (c) instance scoring metrics, (d) whether to retain extracted instances on each it-eration or not, and (e) the number of instances to pass to the next iteration."
Bootstrapping iterates the above three phases sev-eral times until stopping criteria are met.
"Acquired instances tend to become noisy as the iteration pro-ceeds, so it is important to terminate before semantic drift occurs."
"Thus, we have another configuration: (f) stopping criterion."
"Espresso uses Equations (1) for (a) and (2) for (c) respectively, whereas other parameters rely on the tasks and need calibration."
"Even though Espresso greatly improves recall while keeping high precision by using these pattern and instance scoring metrics,[REF_CITE]observed that extracted instances matched against generic patterns may be-come erroneous after tens of iterations, showing the difficulty of applying bootstrapping methods to dif-ferent domains."
"Let us consider a simple bootstrapping algorithm illustrated in Figure 1, in order to elucidate the cause of semantic drift."
"As before, let |I| and |P| be the numbers of instances and patterns, respectively."
"The algo-rithm takes a seed vector i 0 , and a pattern-instance co-occurrence matrix M as input. i 0 is a |I|-dimensional vector with 1 at the position of seed in-stances, and 0 elsewhere."
"M is a |P| × |I|-matrix whose (p,i)-element [M] pi holds the (possibly re-weighted) number of co-occurrence of pattern p and instance i in the corpus."
"If both i and p have con-verged, the algorithm returns the pair of i and p as output."
"This algorithm, though simple, can encode Espresso’s update formulae ([Footnote_1]) and (2) as Steps 3 through 6 if we pose pmi(i, p) (4) [M] pi = max pmi, and normalize p and i in Steps 4 and 6 by p ← p/|I| and i ← i/|P |, (5) respectively."
"1 As long as the relative magnitude of the components of vec-tor i n is preserved, the vector can be normalized in any way on each iteration. Hence HITS and Simplified Espresso use differ-ent normalization but both converge to the principal eigenvector of A."
"This specific instance of the algorithm of Fig-ure 1, obtained by specialization through Equations (4) and (5), will be henceforth referred to as Simpli-fied Espresso."
"Indeed, it is an instance of the origi-nal Espresso in which the iteration is not terminated until convergence, all instances are carried over to the next iteration, and instances are not cumulatively learned."
Let n denote the number of times Steps 2–10 are iterated.
"Plugging (4) and (5) into Steps 3–6, we see that the score vector of instances after the nth iteration is i n = A n i 0 (6) where 1 T M. (7) A = |I||P |M"
"Suppose matrix A is irreducible; i.e., the graph induced by taking A as the adjacency matrix is con-nected."
"If n is increased and i n is normalized on each iteration, i n tends to the principal eigenvec-tor of A. This implies that no matter what seed in-stances are input, the algorithm will end up with the same ranking of instances, if it is run until conver-"
T M gence.
"Because A = M|I||P| , the principal eigen-vector of A is identical to the authority vector of HITS[REF_CITE]algorithm run on the graph induced by M. 1 This similarity of Equations (1), (2) and HITS is not discussed[REF_CITE]."
"As a consequence of the above discussion, se-mantic drift in simplified Espresso seems to be in-evitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector i 0 ."
"A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query.[REF_CITE]"
"Unlike HITS and Simplified Espresso, how-ever, Espresso and other bootstrapping algo-rithms[REF_CITE], incorporate heuristics so that only patterns and in-stances with high confidence score are carried over to the next iteration."
"To investigate the effect of semantic drift on Espresso with and without the heuristics of selecting the most confident instances on each iteration (i.e., the original Espresso and Simplified Espresso of Section 3.2), we apply them to the task of word sense disambiguation of word “bank” in the Senseval-3 Lexical Sample (S3LS) Task data. [URL_CITE]"
"Of the ten senses of bank, the most frequent is the bank as in “bank of the river.”"
We use the standard training-test split provided with the data set.
We henceforth denote Espresso with the follow-ing filtering strategy as Filtered Espresso to stress the distinction from Simplified Espresso.
"For Fil-tered Espresso, we cleared all but the 100 top-scoring instances in the instance vector on each iter-ation, and the number of non-zeroed instance scores grows by 100 on each iteration."
"On the other hand, we cleared all but the 20 top-scoring patterns in the pattern vector on each iteration, and the number of non-zeroed pattern scores grows by 1 on each iter-ation following[REF_CITE]. [Footnote_3]"
"3 We conducted preliminary experiment to find these param-eters to maximize the performance of Filtered Espresso. (These numbers are different from the original Espresso[REF_CITE].) The number of initial patterns is rel-atively large because of a data sparseness problem in WSD, unlike relation extraction and named entity recognition. Also, WSD basically uses more features than relation extraction and thus it is hard to determine the stopping criterion based on the number and scores of patterns,[REF_CITE]does."
"The values of other parameters (b), (d), (e) and (f) remains the same as those for simplified Espresso in Section 3.1."
"The task of WSD is to correctly predict the senses of test instances whose true sense is hidden from the system, using training data and their true senses."
"To predict the sense of a given instance i, we apply k-nearest neighbor algorithm."
"Given a test instance i, its sense is predicted with the following procedure:"
Figure 2 shows the convergence process of Simplified- and Filtered Espresso.
"X-axis indicates the number of bootstrapping iterations and Y-axis indicates the recall, which in this case equals pre-cision, as the coverage is 100% in all cases."
"Simplified Espresso tends to select the most fre-quent sense as the iteration proceeds, and after nine iterations it selects the most frequent sense (“the bank of the river”) regardless of the seed instances."
"As expected from the discussion in Section 3.2, generic patterns gradually got more weight and se-mantic drift occurred in later iterations."
"Indeed, the ranking of the instances after convergence was iden-tical to the HITS authority ranking computed from instance-pattern matrix M (i.e., the ranking induced by the dominant eigenvector of M T M)."
"On the other hand, Filtered Espresso suffers less from semantic drift."
"The final recall achieved was 0.773 after convergence on the 20th iteration, outperforming the most-frequent sense baseline by 0.10."
"However, a closer look reveals that the filter-ing heuristics is limited in effectiveness."
Figure 3 plots the learning curve of Filtered Espresso on the set of test instances.
We show re-call ( |total|correcttrueinstancesinstances| | ) of each sense to see how Filtered Espresso tends to select the most frequent sense.
"If semantic drift takes place, the number of instances predicted as the most frequent sense should increase as the iteration proceeds, resulting in increased recall on the most frequent sense and decreased recall on other senses."
"Figure 3 exactly exhibit this trend, meaning that Filtered Espresso is not completely free from semantic drift."
Figure 2 also shows that the recall of Filtered Espresso starts to decay after the seventh iteration.
We explore two graph-based methods which have the advantage of Espresso to harness the property of generic patterns by the mutual recursive definition of instance and pattern scores.
"They also have less parameters than bootstrapping, and are less prone to semantic drift."
"If we apply the von Neumann kernels to the pattern-instance co-occurrence matrix instead of the document-word matrix, the relative importance of an instance to seed instances can be estimated."
"Let A = M T M be the instance similarity matrix obtained from pattern-instance matrix M, and λ be the principal eigenvalue of A."
The von Neumann kernel matrix K β with diffusion factor β (0 ≤ β &lt; λ −1 ) is defined as follows: ∑ ∞ K β = A β n A n = A(I − βA) −1 . (8) n=0
"The similarity between two instances i, j is given by the (i,j) element of K β ."
"Hence, the i-th column vector can be used as the score vector for seed in-stance i."
"They compute the weighted sum of all paths between two nodes in the co-citation graph induced by A = M T M. The (M T M) n term of smaller n corre-sponds to the relatedness to the seed instances, and the (M T M) n term of larger n corresponds to HITS importance."
"The von Neumann kernels calculate the weighted sum of (M T M) n from n = 1 to ∞, and therefore smaller diffusion factor β results in rank-ing by relatedness, and larger β returns ranking by HITS importance."
"In NLP literature,[REF_CITE]introduced the notion of first- and second-order co-occurrence."
"First-order co-occurrence is a context which directly co-occurs with a word, whereas second-order co-occurrence is a context which occurs with the (con-textual) words that co-occur with a word."
"Higher-order co-occurrence information is less sparse and more robust than lower-order co-occurrence, and thus is useful for a proximity measure."
"Given these definitions, we see that the (M T M) n term of smaller n corresponds to lower-order co-occurrence, which is accurate but sparse, and the (M T M) n term of larger n corresponds to higher-order co-occurrence, which is dense but possibly giving too much weight on unrelated instances ex-tracted by generic patterns."
"As a result, it is expected that setting diffusion factor β to a small value prevents semantic drift and also takes higher order pattern vectors into account."
We verify this claim in Section 5.3.
"The von Neumann kernels can be regarded as a mix-ture of relatedness and importance, and diffusion factor β controls the trade-off between relatedness and importance."
"In practice, however, setting the right parameter value becomes an issue."
"We solve this problem by the regularized Laplacian[REF_CITE], which are stable across diffusion factors and can safely benefit from generic patterns."
Let G be a weighted undirected graph whose adja-cency (weight) matrix is a symmetric matrix A. The (combinatorial) graph Laplacian L of a graph G is defined as follows:
"L = D − A (9) where D is a diagonal matrix, and the ith diagonal element [D] ii is given by ∑ [D] ii = [A] ij . (10) j"
"Here, [A] ij stands for the (i, j) element of A. By re-placing A with −L in Equation (8) and deleting the first A, we obtain a regularized Laplacian kernel [Footnote_4] . ∑ ∞ R β = β n (−L) n = (I + βL) −1 (11) n=0"
"4 It has been reported that normalization of A improves per-formance in applicati[REF_CITE], so we nor-malize L by L = I − D − 21 AD − 12 ."
"We test the von Neumann kernels and the regular-ized Laplacian on the same task as we used in Sec-tion 3.3; i.e., word sense disambiguation of word “bank.”"
"During the training phase, a pattern-instance matrix M was constructed using the training and testing data from Senseval-3 Lexical Sample (S3LS) Task."
"The (i, j) element of M of both kernels is set to pointwise mutual information of a pattern i and an instance j, just the same as in Espresso."
"Recall is used in evaluation. 5 The diffusion parameter β is set to 10 −[Footnote_5] and 10 −2 for the von Neumann kernels and the regularized Laplacian, respectively."
"5 Again, recall equals precision in this case as the coverage is 100% in all cases."
"Table 1 illustrates how well the proposed meth-ods reduce semantic drift, just the same as the ex-periment of Figure 3 in Section 3.3."
We evalu-ate the recall on predicting the most frequent sense (MFS) and the recall on predicting other less fre-quent senses (others).
"For Filtered Espresso, two results are shown: the result on the seventh iter-ation, which maximizes the performance (Filtered Espresso (optimal stopping)), and the one after con-vergence."
"As in Section 3.3, if semantic drift oc-curs, recall of prediction on the most frequent sense increases while recall of prediction on other senses declines."
"Even Filtered Espresso was affected by se-mantic drift, which is again a consequence of the inherent graphical nature of Espresso-like bootstrap-ping algorithms."
"On the other hand, both proposed methods succeeded to balance the most frequent sense and other senses."
Filtered Espresso at the op-timal number of iterations achieved the best perfor-mance.
"Nevertheless, the number of iterations has to be estimated separately."
"We conducted experiments on the task of word sense disambiguation of S3LS data, this time not just on the word “bank” but on all target nouns in the data, following[REF_CITE]."
We used two types of patterns.
Unordered single words (bag-of-words)
We used all single words (unigrams) in the provided context from S3LS data sets.
Each word in the con-text constructs one pattern.
The pattern correspond-ing to a word w is set to 1 if it appears in the con-text of instance i. Words were lowercased and pre-processed with the Porter Stemmer [URL_CITE] .
"Local collocations A local collocation refers to the ordered sequence of tokens in the local, narrow context of the target word."
We allowed a pattern to have wildcard expressions like “sale of * interest in * *” for the target word interest.
We set the window size to ±3 by a preliminary experiment.
"We report the results of Filtered Espresso both af-ter convergence, and with its optimal number of iter-ations to show the upper bound of its performance."
Table 2 compares proposed methods with Espresso with various configurations.
The proposed methods outperform by a large margin the most fre-quent sense baseline and both Simplified- and Fil-tered Espresso.
This means that the proposed meth-ods effectively prevent semantic drift.
"Also, Filtered Espresso without early stopping shows more or less identical performance to Sim-plified Espresso."
"It is implied that the heuristics of filtering and early stopping is a crucial step not to select generic patterns in Espresso, and the result is consistent with the experiment of convergence pro-cess of Espresso in Section 3.3."
Filtered Espresso halted after the seventh itera-tion (Filtered Espresso (optimal stopping)) is com-parable to the proposed methods.
"However, in boot-strapping, not only the number of iterations but also a large number of parameters must be adjusted for each task and domain."
This shortcoming makes it hard to adapt bootstrapping in practical cases.
One of the main advantages of the proposed methods is that they have only one parameter β and are much easier to tune.
It is suggested in Sections 3.3 and 4.1 that
"Espresso and the von Neumann kernel with large β converge to the principal eigenvector of A, though the result does not seem to support this claim (both Simplified- and Filtered[REF_CITE]points lower than the most frequent sense baseline)."
The reason seems to be because Espresso and the von Neumann kernels use pointwise mutual information as a weighting factor so that the principal eigenvec-tor of A may not always represent the most frequent sense. [Footnote_7]
7 A similar but more extreme case is described[REF_CITE]in which the use of a normalized weight matrix M results in an unintuitive principal eigenvector.
"We also show the results of previous graph-based methods[REF_CITE], based on Hyper-Lex[REF_CITE]and PageRank[REF_CITE]."
"The experimental set-up is the same as ours in that they do not use the sense tags of training cor-pus to construct a co-occurrence graph, and they use the sense tags of all the S3LS training corpus for mapping senses to clusters."
"However, these meth-ods have seven parameters to tune in order to achieve the best performance, and hence are difficult to opti-mize."
Figure 4 shows the performance of the von Neu-mann kernels with a diffusion factor β.
"As ex-pected, smaller β leads to relatedness to seed in-stances, and larger β asymptotically converges to the HITS authority ranking (or equivalently, Simplified"
One of the disadvantages of the von Neumann kernels over the regularized Laplacian is their sen-sitivity to parameter β.
Figure 5 illustrates the per-formance of the regularized Laplacian with a diffu-sion factor β.
"The regularized Laplacian is stable for various values of β, while the von Neumann kernels change their behavior drastically depending on the value of β."
"However, β in the von Neumann kernels is upper-bounded by the reciprocal 1/λ of the prin-cipal eigenvalue of A, and the derivatives of kernel matrices with respect to β can be used to guide sys-tematic calibration of β (see[REF_CITE]for detail)."
This paper gives a graph-based analysis of seman-tic drift in Espresso-like bootstrapping algorithms.
We indicate that semantic drift in bootstrapping is a parallel to topic drift in HITS.
We confirm that the von Neumann kernels and the regularized Laplacian reduce semantic drift in the Senseval-3 Lexical Sam-ple task.
Our proposed methods have only one pa-rameters and are easy to calibrate.
"Beside the regularized Laplacian, many other ker-nels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community[REF_CITE]."
One such kernel is the commute-time kernel[REF_CITE]defined as the pseudo-inverse of Laplacian.
"Despite having no parameters at all, it has been re-ported to perform well in many collaborative filter-ing tasks[REF_CITE]."
We plan to test these kernels in our task as well.
Another research topic is to investigate other semi-supervised learning techniques such as co-training[REF_CITE].
"As we have described in this paper, self-training can be thought of a graph-based algorithm."
It is also interesting to analyze how co-training is related to the proposed algorithm.
Bootstrapping algorithms have been used in many NLP applications.
Two major tasks of bootstrap-ping are word sense disambiguation and named en-tity recognition.
"In named entity recognition task, instances are usually retained on each iteration and added to seed instance set."
This seems to be be-cause named entity recognition suffers from seman-tic drift more severely than word sense disambigua-tion.
"Even though this problem setting is different from ours, it needs to be verified that the graph-based approaches presented in this paper are also ef-fective in named entity recognition."
"Web-search queries are known to be short, but little else is known about their structure."
In this paper we investigate the applicability of part-of-speech tagging to typical English-language web search-engine queries and the potential value of these tags for improving search results.
We begin by identifying a set of part-of-speech tags suitable for search queries and quantifying their occurrence.
"We find that proper-nouns constitute 40% of query terms, and proper nouns and nouns together constitute over 70% of query terms."
"We also show that the majority of queries are noun-phrases, not unstructured collections of terms."
We then use a set of queries manually la-beled with these tags to train a Brill tag-ger and evaluate its performance.
"In addi-tion, we investigate classification of search queries into grammatical classes based on the syntax of part-of-speech tag sequences."
We also conduct preliminary investigative experi-ments into the practical applicability of lever-aging query-trained part-of-speech taggers for information-retrieval tasks.
"In particular, we show that part-of-speech information can be a significant feature in machine-learned search-result relevance."
"These experiments also in-clude the potential use of the tagger in se-lecting words for omission or substitution in query reformulation, actions which can im-prove recall."
"We conclude that training a part-of-speech tagger on labeled corpora of queries significantly outperforms taggers based on tra-ditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience."
"Web-search queries are widely acknowledged to be short (2.8 words[REF_CITE]) and to be fre-quently reformulated, but little else is understood about their grammatical structure."
"Since search queries are a fundamental part of the information retrieval task, it is essential that we interpret them correctly."
"However, the variable forms queries take complicate interpretation significantly."
We hypoth-esize that elucidating the grammatical structure of search queries would be highly beneficial for the as-sociated information retrieval task.
Previous work with queries[REF_CITE]considered that short queries may be ambigu-ous in their part of speech and that different docu-ments are relevant depending on how this ambigu-ity is resolved.
"For example, the word “boat” in a query may be intended as subject of a verb, object of a verb, or as a verb, with each case reflecting a distinct intent."
"To distinguish between the possi-bilities, Allan and Raghavan[REF_CITE]propose eliciting feedback from the user by showing them possible contexts for the query terms."
"In addition to disambiguating query terms for re-trieval of suitable documents, part-of-speech tag-ging can help increase recall by facilitating query reformulation."
"Zukerman and Raskutti[REF_CITE]part-of-speech tag well-formed questions, and use the part-of-speech tags to substi-tute synonyms for the content words."
"Several authors have leveraged part-of-speech tagging towards improved index construction for information retrieval through part-of-speech- based weighting schemas and stopword detecti[REF_CITE],[REF_CITE],[REF_CITE]."
Their exper-iments show degrees of success.
"Recently, along with term weighting, Lioma has been using part-of-speech n-grams for noise and content detection in indexes[REF_CITE]."
"Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents."
"Similarly,[REF_CITE]added a layer of natural language processing using part-of-speech tags and syntactical parsing to the common statis-tical information-retrieval framework, much like experiments detailed in sections 4 and 5."
Our system differs in that our syntactic parsing system was applied to web-search queries and uses rules derived from the observed linguistic structure of queries as opposed to natural-language corpora.
"By focusing on the part-of-speech distribution and syntactic structure of queries over tagged indexed documents, with a simple bijection mapping our query tags to other tag sets, our system offers a complementary approach that can be used in tandem with the techniques referenced above."
Lima and Pederson (de[REF_CITE]) conducted related work in which part-of-speech tagging using morphological analysis was used as a preprocessing step for labeling tokens of web-search queries before being parse by a probabilis-tic context-free grammar tuned to query syntax.
We believe this technique and others relying on part-of-speech tagging of queries could benefit from using a query-trained tagger prior to deeper linguistic anal-ysis.
Pasca[REF_CITE]showed that queries can be used as a linguistic resource for discovering named entities.
"In this paper we show that the majority of query terms are proper nouns, and the majority of queries are noun-phrases, which may explain the success of this data source for named-entity discov-ery."
"In this work, we use metrics that assume a unique correct part-of-speech tagging for each query, im-plicitly addressing the disambiguation issue through inter-annotator-agreement scores and tagger gener-alization error."
"To identify these tags, we first ana-lyze the different general forms of queries."
In Sec-tion 2 we determine a suitable set of part-of-speech labels for use with search queries.
We then use man-ually labeled query data to train a tagger and eval-uate its performance relative to one trained on the Brown corpus in Section 3.
"We make observations about the syntactic structure of web-search queries in Section 4, showing that the majority (70%) of queries are noun-phrases, in contrast with the com-monly held belief that queries consist of unstruc-tured collections of terms."
"Finally, we examine the potential use of tagging in the tasks of search rele-vance evaluation and query reformulation in Section 5."
We sampled queries from the Yahoo! search en-gine recorded[REF_CITE].
Queries were sys-tematically lower-cased and white-spaced normal-ized.
We removed any query containing a non- ASCII character.
"Queries were then passed through a high-precision proprietary query spelling correc-tor, followed by the Penn Treebank tokenizer."
No other normalization was carried out.
"Despite Penn-tokenization, queries were typical in their average length[REF_CITE]."
"We sampled 3,283 queries from our dataset to label, for a total of 2,508 unique queries comprised of 8,423 individual to-kens."
"The sparse textual information in search queries presents difficulties beyond standard corpora, not only for part-of-speech tagging software but also for human labelers."
To quantify the level of these diffi-culties we measured inter-rater agreement on a set of 100 queries labeled by each editor.
"Since one labeler annotated 84.4% of the queries, we used a non-standard metric to determine agreement."
One hundred queries were selected at random from each of our secondary labelers.
Our primary labeler then re-labeled these queries.
"Accuracy was then calcu-lated as a weighted average, specifically the mean of the agreement between our primary labeler and sec-ondary labelers, weighted by the number of queries contributed by each secondary labeler."
"Measuring agreement with respect to the individual part-of-speech tag for each token, our corpus has an inter-rater agreement of 79.3%."
"If we require agreement between all tokens in a query, agreement falls to 65.4%."
"Using Cohen’s kappa coefficient, we have that token-level agreement is a somewhat low 0.714 and query-level agreement is an even lower 0.641."
We attempted to accurately quantify token-level ambiguity in queries by examining queries where chosen labels differ.
An author-labeler examined conflicting labels and made a decision whether the difference was due to error or genuine ambigu-ity.
"Error can be a result of accidentally select-ing the wrong label, linguistic misunderstanding (e.g., “chatting” labeled as a verb or gerund), or lack of consensus between editors (e.g., model num-bers could be nouns, proper nouns, or even num-bers)."
"Examples of genuinely ambiguous queries in-clude “download” and “rent,” both of which could be a noun or verb."
Another major source of gen-uine token-level ambiguity comes from strings of proper nouns.
"For example, some editors consid-ered “stillwater chamber of commerce” one entity and hence four proper-noun tokens while others con-sidered only the first token a proper noun."
This left us with a metric in-dicating query ambiguity accounts for 69.7% of la-beling error.
In preliminary labeling experiments we found many standard part-of-speech tags to be extremely rare in web-search queries.
Adding them to the set of possi-ble tags made labeling more difficult without adding any necessary resolution.
In Table 1 we give the set of tags we used for labeling.
"In general, part-of-speech tags are defined according to the distribu-tional behavior of the corresponding parts of speech."
Our tag set differs dramatically from the Brown or Penn tag sets.
"Perhaps most noticeably, the sizes of the tag sets are radically different."
The Brown tag set contains roughly 90 tags.
"In addition, several tags can be appended with additional symbols to indicate negation, genitives, etc."
Our tag set contains just 19 unique classes.
Our contrasting tag sets reflect an extremely dif-ferent use of the English language and correspond-ing part-of-speech distribution.
"For example, the Brown tag set contains unique tags for 35 types of verbs."
We use a single label to indicate all cases of verbs.
"However, the corpora the Brown tag set was designed for consists primarily of complete, natural-language sentences."
"Essentially, every sentence con-tains at least one verb."
"In contrast, a verb of any type accounts for only 2.35% of our tags."
"Similarly, the Brown corpus contains labels for 15 types of deter-miners."
This class makes up just 0.66% of our data.
"Our most common tag is the proper noun, which constitutes 40% of all query terms, and proper nouns and nouns together constitute 71% of query terms."
"In the Brown corpus, by contrast, the most common tag, noun, constitutes about 13% of terms."
"Thus the distribution of tag types in queries is quite different from typical edited and published texts, and in par-ticular, proper nouns are more common than regular nouns."
"Although we have chosen to work with lowercase data, web search queries sometimes contain capi- talization information."
"Since capitalization is fre-quently used in other corpora to identify proper nouns, we reviewed its use in web-search queries."
We found that the use of capitalization is inconsis-tent.
"On a sample of 290,122 queries[REF_CITE]only 16.8% contained some capitaliza-tion, with 3.9% of these all-caps."
"To review the use of capitalization, we hand-labeled 103 queries con-taining capital letters (Table 2)."
Neither all-lowercase (83.2%) nor all-caps (3.9%) queries can provide us with any part-of-speech clues.
But we would like to understand the use of capitalization in queries with varied case.
"In par-ticular, how frequently does first-letter capitalization indicate a proper noun?"
"We manually part-of-speech tagged 75 mixed-case queries, which contained 289 tokens, 148 of which were proper nouns."
The base-line fraction of proper nouns in this sample is thus 51% (higher than the overall background of 40.2%).
"A total of 176 tokens were capitalized, 125 of them proper nouns."
"Proper nouns thus made up 73.3% of capitalized tokens, which is larger than the back-ground occurrence of proper nouns."
We can con-clude from this that capitalization in a mixed-case query is a fair indicator that a word is a proper noun.
"However, the great majority of queries contain no informative capitalization, so the great majority of proper nouns in search queries must be uncapital-ized."
"We cannot, therefore, rely on capitalization to identify proper nouns."
"With this knowledge of the infrequent use of capi-tal letters in search queries in mind, we will examine the effects of ignoring or using a query’s capitaliza-tion for part-of-speech tagging in Section 3.4.2."
"To investigate automation of the tagging process, we trained taggers on our manually labeled query set."
"In the sections below, we used two datasets."
The first consists of 1602 manually labeled queries.
"For the experiments in Section 3.5 we la-beled additional queries, for a total of 2503 manu-ally labeled queries."
We experimented with two freely available part-of-speech taggers: The Brill Tagger[REF_CITE]and The Stanford Tagger[REF_CITE].
The Brill tagger works in two stages.
The initial tagger queries a lexicon and labels each token with its most common part-of-speech tag.
"If the token is not in the lexicon, it labels the token with a de-fault tag, which was “proper noun” in our case."
"In the second stage, the tagger applies a set of lexical rules which examine prefixes, suffixes, and infixes."
The tagger may then exchange the default tag based on lexical characteristics common to particular parts of speech.
"After application of lexical rules, a set of contextual rules analyze surrounding tokens and their parts of speech, altering tags accordingly."
"We chose to experiment primarily with the Brill tagger because of its popularity, the human-readable rules it generates, and its easily modifiable code base."
"In addition, the clearly defined stages and in-corporation of the lexicon provide an accessible way to supply external lexicons or entity-detection rou-tines, which could compensate for the sparse con-textual information of search queries."
"We also experimented with the Stanford Log-Linear Part-of-Speech Tagger, which presently holds the best published performance in the field at 96.86% on the Penn Treebank corpus."
It achieves this accuracy by expanding information sources for tagging.
"In particular, it provides “(i) more exten-sive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating par-ticles from prepositions and adverbs.”"
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence[REF_CITE].
"With proper nouns dominating the distribution, we first considered using the accuracy of labeling all to-kens “proper noun” as a baseline."
"In this case, we labeled 1953 of 4759 (41.0%) tokens correctly."
"This is a significant improvement over the accuracy of tagging all words as “noun” on the Brown corpus (approximately 13%), reflecting the frequent occur-rence of proper nouns in search queries."
"However, to examine the grammatical structure of search queries we must demonstrate that they are not simply col-lections of words."
"With this in mind, we chose in-stead to use the most common part-of-speech tag for a word as a baseline."
"We evaluated the baseline performance on our manually labeled dataset, with URLs removed."
"Each token in the set was assigned its most common part of speech, according to the Brill lexicon."
"In this case, 4845 of 7406 tokens were tagged correctly (65.42%)."
"The Brill tagger software is pre-trained on the stan-dard Wall Street Journal corpus, so the simplest pos-sible approach is to apply it directly to the query data set."
"We evaluated this “out-of-the-box” performance on our 1602 manually labeled queries, after mapping tags to our reduced tag set. (Our effective training-set size is 1440 queries, since 10% were held out to measure accuracy through cross validation.)"
"The WSJ-trained tagger labeled 2293 of 4759 (48.2%) tags correctly, a number well below the baseline performance, demonstrating that application of the contextual rules that Brill learns from the syntax of natural-language corpora has a negative effect on ac-curacy in the context of queries."
"When we re-trained Brill’s tagger on a manually labeled set of queries, we saw accuracy increase to 69.7%."
The data used to train the tagger therefore has a significant effect on its accuracy (Table 3).
"The accuracy of the tag-ger trained on query data is above the baseline, in-dicating that search queries are somewhat more than collections of words."
"We conducted several experiments in improving tag-ger accuracy, summarized in Table 3 and described in detail below."
"With a training-set size of 1500 queries, compris-ing a lexicon of roughly 4500 words, it is natural to question if expanding the lexicon by incorporating external sources boosts performance."
"To this end, we lower-cased the lexicon of 93,696 words pro-vided by the Brill tagger, mapped the tags to our own tag set, and merged our lexicon from queries."
"This experiment resulted in an accuracy of 71.1%, a 1.4% increase."
One explanation for the limited increase is that this lexicon is derived from the Brown corpus and the Penn Treebank tagging of the Wall Street Jour-nal.
These corpora are based on works published in 1961 and 1989-1992 respectively.
"As shown in Table 1, proper nouns dominate the distribution of search-engine queries."
"Many of these queries will involve recent products, celebrities, and other time-sensitive proper nouns."
"We speculate that Web-based information resources could be leveraged to expand the lexicon of timely proper nouns, thereby enhancing performance."
"The overall performance of the pre-trained Brill tagger on our query set may be due to its poor per-formance on proper nouns, our most frequent part of speech."
"In the WSJ newspaper training data, proper-nouns always start with a capital letter."
"As discussed in Section 2.3, capitalization is rare in web-search queries."
"To examine the effect of the missing capitalization of proper nouns, we evaluated a pre-trained Brill tagger on our previously men-tioned manually labeled corpus of 1602 queries al-tered such that only the proper nouns were capital-ized."
"In this case, the tagger reached an extraordi-nary 89.4% accuracy (Table 3)."
"Unfortunately, the vast majority of queries do not contain capitalization information and those that do often contain mislead-ing information."
"The pre-trained tagger achieved only a 45.6% accuracy on non-lowercased queries, performing even worse than on the set with no capi-talization at all."
We saw in Section 2.3 that web searchers rarely use capitalization.
We have also seen that a pre-trained Brill tagger run on queries with perfect capitalization (“oracle” capitalization) can achieve 89.4% accuracy.
We now look at how performance might be affected if we used an imperfect algorithm for capitalization.
"In order to attempt to capitalize the proper nouns in queries, we used a machine-learned system which searches for the query terms and examines how of-ten they are capitalized in the search results, weight-ing each capitalization occurrence by various fea-tures[REF_CITE]."
"Though the capitalization system provides 79.3% accuracy, using this system we see an only a small increase of accuracy in part-of-speech tagging at 70.9%."
This system does not improve significantly over the tagger trained on the lower-cased corpus.
One explanation is that cap-italization information of this type could only be obtained for 81.9% of our queries.
"Multiplied by accuracy, this implies that roughly 81.9% * 79.3% = 65.0% of our proper nouns are correctly cased."
This suggests that any technique for proper-noun de-tection in search-engine queries must provide over 65.0% accuracy to see any performance increase.
Finally we looked at the capitalization as input by searchers.
"We trained on the oracle-capitalized cor-pus, and tested on raw queries without normaliza-tion."
We saw an accuracy of just 45.6%.
Thus using the capitalization input by web searchers is mislead-ing and actually hurts performance.
It is important to understand whether tagger accu-racy is limited by the small size of our manually la-beled dataset.
"To examine the effect of dataset size, we trained Brill’s tagger with increasing numbers of labeled queries and evaluated accuracy with each set size."
"In the interim between conducting the experi-ments of sections 3.1 through 3.3 and those of sec-tion 3.5, we were able to obtain 1120 new labeled queries, allowing us to extend the learning curve."
"With our complete corpus of 2722 labeled exam-ples (for a cross-validated training-set size of 2450 labeled examples, URLs omitted), we see an accu-racy of 78.6% on a per-token basis."
"We see the most significant gains in performance with the first few hundred labeled examples, but even after 2500 ex-amples are labeled, more labeled data continues to improve performance."
In Table 4 we see a comparison of Brill’s tagger to the Stanford tagger trained on 2450 labeled queries.
The 0.3% performance increase is not statistically significant.
"As listed in Section 3, the features the Stanford tagger adds to achieve high accuracy in traditional natural-language corpora are not in- formative in the domain of search-engine queries."
We believe greater performance on our data will be achieved primarily through examination of common sources of inter-rater disagreement (such as consis-tent handling of ambiguity) and incorporation of ex-ternal sources to detect proper nouns not in the lexi-con.
"To validate our intuition that expanding the lexi-con will boost performance, we obtained a propri-etary list of 7385 known trademarked terms used in the sponsored-search industry."
"Treating these phrases as proper nouns and adding them to the lex-icon from the Wall Street Journal supplied with the Brill tagger, we see our cross validated accuracy im-prove to 80.2% (with a standard deviation of 1.85%), the highest score achieved in our experiments."
We find it likely that incorporation of more external lex-ical sources will result in increased performance.
Our experiments also support our hypothesis that addressing inter-annotator agreement will boost per-formance.
We can see this by examining the results of the experiments in section 3.3 verses section 3.5.
"In section 3.3, we see the accuracy on the query-trained Brill tagger is 69.7%."
"As mentioned, for the experiment in section 3.5, we were able to ob-tain 1120 new queries."
"Each of these newly labeled queries came from the same labeler, who believes their handling of the ambiguities inherent in search queries became more consistent over time."
"With the same training-set size of 1440 used in section 3.3, Figure 1 shows performance at 1440 queries is roughly 6% higher."
We believe this significant im-provement is a result of more consistent handling of query ambiguity obtained through labeling experi-ence.
The above-baseline performance of the Brill tagger trained on web-search queries suggests that web-search queries exhibit some degree of syntactical structure.
"With a corpus of queries labeled with part-of-speech information, we are in a position to ana-lyze this structure and characterize the typical pat-terns of part-of-speech used by web searchers."
"To this end, we randomly sampled and manually la-beled a set of 222 queries from the part-of-speech dataset used for tagger training mentioned above."
Each query was labeled with a single meta-tag in-dicating query type.
"Two author-judges simultane-ously labeled queries and created the set of meta-tags during much discussion, debate, and linguistic research."
A list of our meta-tags and the distribu-tion of each are provided in Table 5.
"We can see that queries consisting of a noun-phrase dominate the distribution of query types, in contrast with the popularly held belief that queries consist of unstruc-tured collections of terms."
"To determine how accurately a meta-tag can be determined based on part-of-speech labels, we cre-ated a grammar consisting of a set of rules to rewrite part-of-speech tags into higher-level grammatical structures."
These higher-level grammatical struc-tures are then rewritten into one of the seven classes of meta-tags seen in Table 5.
Our grammar was con-structed by testing the output of our rewrite rules on queries labeled with par-of-speech tags that were not part of the 222 queries sampled for meta-tag label-ing.
Grammar rules were revised until the failure rate on previously untested part-of-speech-labeled queries stabilized.
Failure was evaluated by two means.
"In the first case, the grammar rules failed to parse the sequence of part-of-speech tags."
"In the second case, the grammar rules led to an inappro-priate classification for a query type."
"As during the labeling phase, the two author-labelers simultane-ously reached a consensus on whether a parse failed or succeeded, rendering an inter-annotator score in-applicable."
The resulting grammar was then tested on the 222 queries with query-type meta-tags.
Our rules function much like production rules in context-free grammars.
"As an example, the two-tag sequence “determiner noun” will be rewritten as “noun phrase.”"
"This in turn could be re-written into a larger structure, which will then be rewritten into a meta-tag of query type."
The primary differ-ence between a context-free grammar or probabilis-tic context-free grammar (such as that employed by Lima and Pederson (de[REF_CITE])) and our grammar is that our rules are applied itera-tively as opposed to recursively.
"As such, our gram-mar yields a single parse for each input."
Some of our rules reflect the telegraphic nature of web queries.
"For example, it is much more com-mon to see an abbreviated noun-phrase consisting of adjective-noun, than one consisting of determiner-adjective-noun."
"Examining the Table 5, we see that just label-ing a query “noun-phrase” results in an accuracy of 69.8%."
Our grammar boosted this high baseline by 14% to yield an final accuracy result of 83.3% at la-beling queries with their correct meta-type.
These meta-types could be useful in deciding how to han-dle a query.
Further enhancements to the grammar would likely yield a performance increase.
"How-ever, we feel accuracy is currently high enough to continue with experiments towards application of leveraging grammar-deduced query types for infor-mation retrieval."
We can think of some of these meta-types as elided sentences.
"For example, the noun-phrase queries could be interpreted as requests of the form “how can I obtain X” or “where can I get informa-tion on X”, while the verb-phrase queries are re-quests of the form ”I would like to DO-X”."
"Since search queries are part of an information re-trieval task, we would like to demonstrate that part-of-speech tagging can assist with that task."
We con-ducted two experiments with a large-scale machine-learned web-search ranking system.
"In addition, we considered the applicability of part-of-speech tags to the question of query reformulation."
We worked with a proprietary experimental testbed in which features for predicting the relevance of a query to a document can be tested in a machine-learning framework.
"Features can take a wide va-riety of forms (boolean, real-valued, relational) and apply to a variety of scopes (the page, the query, or the combination)."
These features are evaluated against editorial judgements and ranked according to their significance in improving the relevance of results.
We evaluated two part-of-speech tag-based features in this testbed.
The first experiment involved a simple query-level feature indicating whether the query contained a noun or a proper noun.
This feature was evaluated on thousands of queries for the test.
"At the conclu-sion of the test, this feature was found to be in the top 13% of model features, ranked in order of signif-icance."
"We believe this significance represents the importance of recognizing the presence of a noun in a query and, of course, matching it."
Within this experimental testbed a statistically significant im-provement of information-retrieval effectiveness is notoriously difficult to attain.
We did not see a sig-nificant improvement in this metric.
"However, we feel that our feature’s high ranking warrants report-ing and hints at a potentially genuine boost in re-trieval performance in a system less feature-rich."
The second experiment was more involved and re-flected more of our intuition about the likely applica-tion of part-of-speech tagging to the improvement of search results.
"In this experiment, we part-of-speech tagged both queries and documents."
Documents were tagged with a conventionally trained Brill tag-ger with the resulting Penn-style tags mapped to our tag set.
Many thousands of query-document pairs were processed in this manner.
The feature was based on the percent of times the part-of-speech tag of a word in the query matched the part-of-speech tag of the same word in the document.
"This feature was ranked in the top 12% by significance, though we again saw no statistically significant increase in overall retrieval performance."
"We considered the application of part-of-speech tag-ging to the problem of query reformulation, in which a single word in the query is altered within the same user session."
We used a set of automatically tagged queries to calculate change probabilities of each word by part-of-speech tag and the results are shown in Table 6.
The type of word most likely to be reformu-lated is “number.”
"Examples included changing a year (“most popular baby names 2007” → “most popular baby names 2008”), while others included model, version and edition numbers (“harry potter 6” → “harry potter 7”) most likely indicating that the user is looking at variants on a theme, or cor-recting their search need."
Typically a number is a modifier of the core search meaning.
"The next most commonly changed type was “adjective,” perhaps indicating that adjectives can be used to refine, but not fundamentally alter, the search intent."
"Nouns and proper nouns are the next most commonly mod-ified types, perhaps reflecting user modification of their search need, refining the types of documents retrieved."
"Other parts of speech are relatively sel-dom modified, perhaps indicating that they are not viewed as having a large impact on the documents retrieved."
We can see from the impact of the search engine ranking features and from the table of query refor-mulation likelihood that making use of the grammat-ical structure of search queries can have an impact on result relevance.
"It can also assist with tasks as-sociated with improving recall, such as query refor-mulation."
"We have quantified, through a lexicostatistical anal-ysis, fundamental differences between the natural language used in standard English-language corpora and English search-engine queries."
These differ-ences include reduced granularity in part-of-speech classes as well as the dominance of the noun classes in queries at the expense of classes such as verbs frequently found in traditional corpora.
"In addi-tion, we have demonstrated the poor performance of taggers trained on traditional corpora when applied to search-engine queries, and how this poor perfor-mance can be overcome through query-based cor-pora."
We have suggested that greater improvement can be achieved by proper-noun detection through incorporation of external lexicons or entity detec-tion.
"Finally, in preliminary investigations into ap-plications of our findings, we have shown that query part-of-speech tagging can be used to create signif-icant features for improving the relevance of web search results and may assist with query reformu-lation."
Improvements in accuracy can only increase the value of POS information for these applications.
We believe that query grammar can be further ex-ploited to increase query understanding and that this understanding can improve the overall search expe-rience.
We present a novel method for discovering and modeling the relationship between in-formal Chinese expressions (including collo-quialisms and instant-messaging slang) and their formal equivalents.
"Specifically, we pro-posed a bootstrapping procedure to identify a list of candidate informal phrases in web corpora."
"Given an informal phrase, we re-trieve contextual instances from the web us-ing a search engine, generate hypotheses of formal equivalents via this data, and rank the hypotheses using a conditional log-linear model."
"In the log-linear model, we incorpo-rate as feature functions both rule-based intu-itions and data co-occurrence phenomena (ei-ther as an explicit or indirect definition, or through formal/informal usages occurring in free variation in a discourse)."
"We test our system on manually collected test examples, and find that the (formal-informal) relation-ship discovery and extraction process using our method achieves an average [Footnote_1]-best preci-sion of 62%."
"1 For clarity, we represent Chinese words in the format: Chi-nese characters (optional PinYin equivalent in parentheses and optional English gloss in brackets)."
"Given the ubiquity of informal conversational style on the internet, this work has clear applications for text normalization in text-processing systems including machine translation aspiring to broad coverage."
"Informal text (e.g., newsgroups, online chat, blogs, etc.) is the majority of all text appearing on the Inter-net."
"Informal text tends to have very different style from formal text (e.g., newswire, magazine, etc.)."
"In particular, they are different in vocabulary, syn-tactic structure, semantic interpretation, discourse 



 structure, and so on."
"On the other hand, certain re-lations exist between the informal and formal text, and informal text often has a viable formal equiva-lent."
"Table 1 shows several naturally occurring ex-amples of informal expressions in Chinese, and Ta-ble 2 provides a more detailed inventory and charac-terization of this phenomena 1 ."
The first example of informal phrase “88” is used very often in Chinese on-line chat when a person wants to say “bye-bye” to the other person.
This can be explained as fol-lows.
"In Chinese, the standard equivalent to “bye-bye” is “dd” whose PinYin is “BaiBai”."
"Coin-cidentally, the PinYin of “88” is “BaBa”."
"Because “BaBa” and “BaiBai” are near homophones, people often use “88” to represent “dd”, either for input convenience or just for fun."
The other relations in Table 1 are formed due to similar processes as will be described later.
"Due to the often substantial divergence between informal and formal text, a text-processing system trained on formal text does not typically work well on informal genres."
"For example, in a machine translation system[REF_CITE], if the bilin-gual training data does not contain the word “d d” (the second example in Table 1), it leaves the word untranslated."
"On the other hand, if the word “dd” does appear in the training data but it has only a translation “gruel” as that is the meaning in the formal text, the translation system may wrongly translate “dd” into “gruel” for the informal text where the word “dd” is more likely to mean “like”."
"Therefore, as a text-normalization step, it is desirable to transform the informal text into its standard formal equivalent before feeding it into a general-purpose text-processing system."
"Unfortu-nately, there are many processes for generating in-formal expressions in common use today."
"Such transformations are highly flexible/diverse, and new phrases are invented on the Internet every day due to major news events, popular movies, TV shows, ra-dio talks, political activities, and so on."
"Therefore, it is of great interest to have a data-driven method that can automatically find the relations between in-formal and formal expressions."
"In this paper, we present a novel method for dis-covering and modeling the relationship between in-formal Chinese expressions found in web corpora and their formal equivalents."
"Specifically, we im-plement a bootstrapping procedure to identify a list of candidate informal phrases."
"Given an indi-vidual informal phrase, we retrieve contextual in-stances from the web using a search engine (in this case,[URL_CITE]generate hypotheses of for-mal equivalents via this data, and rank the hypothe-ses using a conditional log-linear model."
"In the log-linear model, we incorporate as feature functions both rule-based intuitions and data co-occurrence phenomena (either as an explicit or indirect defini-tion, or through formal/informal usages occurring in free variation in a discourse)."
"We test our system on manually collected test examples [Footnote_2] , and find that the (formal-informal) relationship discovery and extrac-tion process using our method achieves an average precision of more than 60%."
2 The training and test examples are freely available[URL_CITE]
"This work has applica- tions for text normalization in many general-purpose text-processing tasks, e.g., machine translation."
"To the best of our knowledge, our work is the first published machine-learning approach to pro-ductively model the broad types of relationships be-tween informal and formal expressions in Chinese using web corpora."
"In this section, we describe the phenomena and pro-vide examples of the relations between formal and informal expressions in Chinese (we refer to the relation as formal-informal phrases hereafter, even in the case of single-word expressions)."
"We man-ually collected 908 formal-informal relations, and classified these relations into four categories."
"We collected these pairs by investigating multiple web-pages where the formal-informal relations are man-ually compiled, and then merged these seed relations and removed duplicates."
"In this way, the 908 exam-ples should give good coverage on the typical cat-egories in the formal-informal relations."
"Also, the distribution of the categories found in the 908 exam-ples should be representative of the actual distribu-tion of the formal-informal relations occurring in the real text."
Table 2 presents these categories and ex-amples in each category.
"In the last column, the table also shows the relative frequency of each category, computed based on the 908 examples."
Recall that we represent Chinese words in the format: Chinese characters (optional PinYin equivalent in parenthe-ses and optional English gloss in brackets).
"In general, a homophone is a word that is pro-nounced the same as another word but differs in meaning and/or written-form."
"Here, we use the word “homophone” in a loose way."
"In particular, we re-fer an informal phrase as a homophone of a formal phrase if its pronunciation is the same or similar to the formal phrase."
"In the three examples belonging to the homophone category in Table 2, the first ex-ample is a true homophone, while the other two are loose homophones."
"The third example represents a major sub-class where the informal phrase is a num-ber (e.g., 88)."
"For illustrative purposes, we can present the transformation path showing how the informal phrase is obtained from the formal phrase."
"In par-ticular, the transformation path for this category is “Formal → PinYin → Informal (similar or same PinYin as the formal phrase)”."
"A Chinese abbreviation of a formal phrase is ob-tained by selecting one or more characters from this formal phrase, and the selected characters can be at any position in the formal phrase[REF_CITE]."
"In comparison, an acronym is a special form of abbreviation, where only the first character of each word in the formal phrase is selected to form the informal phrase."
Table 2 presents three examples belonging to this category.
"While the first example is an abbreviation, and the other two examples are acronyms."
"The transformation path for the second exam-ple is “Formal → PinYin → Acronym”, and the transformation path for the third example is “For-mal → English → Acronym”."
"Clearly, they differ in whether PinYin or English is used as a bridge."
A transliteration is transcribing a word or text writ-ten in one writing system into another writing sys-tem.
Table 2 presents examples belonging to this category.
"In the first example, the Chinese infor-mal phrase “dd (FenSi)[a Chinese food]” can be thought as a transliteration of the English phase “fans” as the pronunciation of “fans” is quite sim-ilar to the PinYin “FenSi”."
The transformation path for this category is “For-mal → English → Chinese Transliteration”.
"Due to the inherently informal and flexible nature of expressions in informal genre, the formation of an informal phrase can be very complex or ad-hoc."
"For example, an informal phrase can be generated by ap-plying the above transformation rules jointly."
"More importantly, many relations cannot be described us-ing a simple set of rules."
"Table 2 presents three such examples, where the first two examples are gener-ated by applying rules jointly and the third example is created by decomposing the Chinese characters in the formal form."
The statistics collected from the 904 examples tells us that about 45% of the relations belonging to this category.
This motivates us to use a data-driven method to automatically discover the relations between informal and formal phrases.
"In natural language, related words tend to appear to-gether (i.e., co-occurrence)."
"For example, Bill Gates tends to appear together with Microsoft more of-ten than expected by chance."
"Such co-occurrence may imply the existence of a relationship, and is ex-ploited in formal-informal relation discovery under different conditions."
"In general, for many informal phrases in popular use, there is likely to be an explicit definition somewhere that provides or paraphrases its meaning for an unfa-miliar audience."
People have created dedicated def-inition web-pages to explain the relations between formal and informal phrases.
"For example, the first example in Table 3 is commonly explained in many dedicated definition web-pages on the Internet."
"On the other hand, in some formal text (e.g., research papers), people tend to define the informal phrase before it is used frequently in the later part of the text."
The second example of Table 3 illustrates this phenomena.
"Clearly, the definition text normally contains salient patterns."
"For example, the first ex-ample follows the “informaldformalddd” defi-nition pattern, while the second example follows the pattern “formal (informal)”."
This gives us a reliable way to seed and bootstrap a list of informal phrases as will be discussed in Section 4.1.
Informal phrases appear in online chat very often for input convenience or just for fun.
"Since different people may have different ways or traditions to ex-press semantically-equivalent phrases, one may find many nearby data co-occurrence examples in chat text."
"For example, in Table 4, after a series of mes-sage exchanges, person A wants to end the conver-sation and types “dd” (meaning “bye-bye”), per-son B later includes the same semantic content, but in a different (more or less formal) expression (e.g. “88”)."
"For some formal-informal relations, since both of the informal and formal phrases have been used in public very often and people are normally aware of these relations, an author may use the informal and formal phrases interchangeably without bother-ing to explain the relations."
This is particularly true in news articles for some well-known relations.
"Ta-ble 5 shows an example, where the abbreviation “d dd” (meaning “winter olympics”) appears in the title and its full-form “ddddd” appears in the text of the same document."
"In general, the relative distance between an informal phrase and its formal phrase varies."
"For example, they may appear in the same sentence, or in neighboring sentences."
"In this section, we describe an approach that auto-matically discovers the relation between a formal phrase and an informal phrase from web corpora."
"Specifically, we propose a bootstrapping procedure to identify a list of candidate informal phrases."
"Given a target informal phrase, we retrieve a large set of of instances in context from the Web, generate candidate hypotheses (i.e, candidate formal phrases) from the data, and rank the hypotheses by using a conditional log-linear model."
"The log-linear model is very flexible to incorporate both the rule- and data- driven intuitions (described in Sections 2 and 3, re-spectively) into the model as feature functions."
"Before finding the formal phrase corresponding to an informal phrase, we first need to identify infor-mal phrases of interest."
"For example, one can collect informal phrases manually."
"However, this is too ex-pensive as new relations between informal and for-mal phrases emerge every day on the Internet."
"Alter-natively, one can employ a large amount of formal text (e.g., newswire) and informal text (e.g., Inter-net blogs) to derive such a list as follows."
"Specifi-cally, from the informal corpus we can extract those phrases whose frequency in the informal corpus is significantly different from that in the formal cor-pus."
"However, such a list may be quite noisy, i.e., many of them are not informal phrases at all."
"An alternative approach to extracting the infor-mal phrases is to use a bootstrapping algorithm (e.g.,[REF_CITE])."
"Specifically, we first manually collect a small set of example relations."
"Then, using these relations as a seed set, we extract the text pat-terns (e.g., the definition pattern showing how the informal and formal phrases co-occur in the data as discussed in Section 3.1)."
"With these patterns, we identify many more new relations from the data and augment them into the seed set."
The procedure it-erates.
"Using such an approach, we should be able to extract a large list of formal-informal relations."
"Clearly, the list extracted in this way may be quite noisy, and thus it is important to exploit both the data- and rule-driven intuitions to rank these rela-tions properly."
"Given an informal phrase, we retrieve training data from the web on the fly."
"Specifically, we first use a search engine to identify a set of hyper-links that point to web pages containing contexts relevant to the informal phrase, and then follow the hyper-links to download the web pages."
The input to the search engine is a text query.
One can simply use the infor-mal phrase as a query.
"However, this may lead to a set of pages that have nothing to do with the infor-mal phrase."
"For example, if we search the informal phrase “88” (the third example in Table 2) using the well-known Chinese search engine[URL_CITE]none of the top-10 pages are related to the infor-mal phrase “88”."
"To avoid this situation, one can use a search engine that is dedicated to informal text search (e.g., blogsearch.baidu.com)."
"Alternatively, one can use the general-purpose search engine but expanding the query with domain information."
"For example, for the informal phrase “88”, we can use a query “88 dddd”, where “dddd” means internet language."
"Given an informal phrase, we generate a set of hy-potheses which are candidate formal phrases corre-sponding to the informal phrase."
We considered two general approaches to the generation of hypotheses.
Rule-driven Hypothesis Generation: One can use the rules described in Section 2 to generate a set of hypotheses.
"However, with this approach, one may generate an exponential number of hypotheses."
"For example, assuming the number of English words starting with a given letter is O(|V |), we can generate O(|V | n ) hypotheses given an acronym containing n letters."
Another problem with this approach is that a relation between an informal phrase and a formal phrase may not be explained by a specific rule.
"In fact, as shown in the last row of Table 2, such rela-tions consist of 44.8% of all corpus instances."
"Data-driven Hypothesis Generation: With data retrieved from the Web, we can generate hypotheses by enumerating the frequent n-grams co-occurring with the informal phrase within certain distance."
"This exploits the data co-occurrence phenomena de-scribed in Section 3, that is, the formal phrase tends to co-occur with the informal phrase nearby in the data, for the multiple reasons described above."
This can deal with the cases where the relation between an informal phrase and a formal phrase cannot be explained by a rule.
"However, it also suffers from the over-generation problem as in the rule-driven ap-proach."
"In this paper, we use the data-driven method to generate hypotheses, and rank the hypotheses using a conditional log-linear model that incorporates both the rule and data intuitions as feature functions."
Log-linear models are known for flexible incorpora-tion of features into the model.
Each feature func-tion reflects a hint/intuition that can be used to rank the hypotheses.
"In this subsection, we develop a conditional log-linear model that incorporates both the rule and data intuitions as feature functions."
"Given an informal phrase (say x) and a candidate formal phrase (say y), the model assigns the pair a score (say s(x, y)), which will be used to rank the hypothesis y."
"The score s(x, y) is a linear combina-tion of the feature scores (say Φ i (x, y)) over a set of feature functions indexed by i. Formally,"
"K s(x, y) = X Φ i (x, y) × α i (1) i=1 where K is the number of feature functions defined and α i is the weight assigned to the i-th feature func-tion (i.e., Φ i )."
"To learn the weight vector ~α, we first define a probability measure, 1 s(x,y)"
"P α~ (y|x) = Z(x, ~α)e (2) where Z(x, ~α) is a normalization constant."
"Now, we define the regularized log-likelihood (LL R ) of the training data (i.e, a set of pairs of (x, y)), as follows,"
"N LL R (~α) = X log P α~ (y j |x j ) − ||α~|| 2 (3) 2σ 2 j=1 where N is the number of training examples, and the regularization term ||α~|| 2 is a Gaussian prior with a 2σ 2 variance σ 2[REF_CITE]."
"The optimal weight vector α~ ∗ is obtained by maximizing the regularized log-likelihood (LL R ), that is, α = arg max LL R (α~)~ ∗ (4) α~"
"To maximize the above function, we use a limited-memory variable method[REF_CITE]that is implemented in the TAO package[REF_CITE]and has been shown to be very effective in various natural language processing tasks[REF_CITE]."
"During test time, the following decision rule is normally used to predict the optimal formal phrase y ∗ for a given informal phrase x, y ∗ = arg max s(x, y). (5) y"
"As mentioned before, we incorporate both the rule- and data-driven intuitions as feature functions in the log-linear model."
"Rule-driven feature functions: Clearly, if a pair (x, y) matches the rule patterns described in Table 2, the pair has a high possibility to be a true formal-informal relation."
"To reflect this intuition, we de-velop several feature functions as follows. • LD-PinYin(x,y): the Levenshtein distance on PinYin of x and y."
"The distance between two PinYin characters is weighted based on the similarity of pronunciation, for example, the weight w(l,n) is smaller than the weight w(a, z). • LEN-PinYin(x,y): the difference in the num-ber of PinYin characters between x and y. • Is-PinYin-Acronym(x, y): is x a PinYin acronym of y?"
"For example, Is-PinYin-Acronym(GG, dd)=1, Is-PinYin-Acronym(GG, dd)=0. • Is-CN-Abbreviation(x,y): is x a Chinese ab-breviation of y?"
"For example, Is-CN-Abbreviation(dd,dddd)=1, Is-CN-Abbreviation(dd,dddd)=0."
"Data-driven feature functions: As described in Section 3, the informal and formal phrases tends to co-occur in the data."
"Here, we develop several fea-ture functions to reflect this intuition. • n-gram co-occurrence relative frequency : we collect the n-grams that occur in the data within a window of the occurrence of the informal phrase, and compute their relative frequency as feature values."
"Since different orders of grams will have quite different statistics, we define 7 features in this category: 1-gram, 2-gram, 3-gram, 4-gram, 5-gram, 6to10-gram, and 11to15-gram."
Note that the order n of a n-gram is in terms of number of Chinese char-acters instead of words. • Features on a definition pattern: we have dis-cussed definition patterns in Section 3.1.
"For each definition pattern, we can define a feature function saying that if the co-occurrence of x and y satisfies the definition pattern, the feature value is one, otherwise is zero. • Features on the number of relevant web-pages: another interesting feature function can be de-fined as follows."
"For each candidate relation (x, y), we use the pair as a query to search the web, and treat the number of pages returned by the search engine as a feature value. [Footnote_3]"
3 Note that the number of pages relevant to a query can be easily obtained as most search engines return this number.
"However, these features are quite expensive as millions of queries may need to be served."
Recall that in Section 2 we categorize the formal-informal relations based on the manually collected relations.
"In this section, we use a subset of them for training and testing."
"In particular, we use 252 exam-ples to train the log-linear model that is described in Section 4, and use 249 examples as test data to compute the precision. [Footnote_4]"
"4 Again, the training and test examples are freely available[URL_CITE]"
Table 6 shows the weights [Footnote_5] learned for the var-ious feature functions described in Section 4.4.
"5 Note that we do not use the features on definition patterns and on the number of relevant web pages, for efficiency."
"Clearly, different feature functions get quite differ-ent weights."
This is intuitive as the feature functions may differ in the scale of the feature values or in their importance in ranking the hypotheses.
"In fact, this shows the importance of using the log-linear model to learn the optimal weights in a principled and automatic manner, instead of manually tuning the weights in an ad-hoc way."
"Tables 7-9 show the precision results for different categories as described in Section 2, using the rule-driven, data-driven, or both rule and data-driven fea-tures, respectively."
"In the tables, the precision corre-sponding to the “top-N” is computed in the follow-ing way: if the true hypothesis is among the top-N hypotheses ranked by the model, we tag the classi-fication as correct, otherwise as wrong."
"Clearly, the larger the N is, the higher the precision is."
"Comput-ing the top-N precision (instead of just computing the usual top-1 precision) is meaningful especially when we consider our relation extractor as an inter-mediate step in an end-to-end text-processing sys-tem (e.g., machine translation) since the final deci-sion can be delayed to later stage based on more ev-idence."
"In general, our model gets quite respectably high precision for such a task (e.g., more than 60% for top-1 and more than 85% for top-100) when us-ing both data and rule-driven features, as shown in Table 9."
"Moreover, the data-driven features are more helpful than the rule-driven features (e.g, 25.3% ab-solute improvement in 1-best precision), while the combination of these features does boost the perfor-mance of any individual feature set (e.g., 10.4% ab-solute improvement in 1-best precision over the case using data-driven features only)."
We also carried out experiments (see[REF_CITE]) in the bootstrapping procedure described in Section 4.1.
"In particular, we start from a seed set having 130 relations."
We identify the frequent patterns from the data retrieved from the web for these seed exam-ples.
"Then, we use these patterns to identify many more new possible formal-informal relations."
"After the first iteration, we select the top 3000 pairs of re-lations matched by the patterns."
"The recall of a man-ually collected test set (having 750 pairs) on these 3000 pairs is around 30%, which is quite promising given the highly noisy data."
"Automatically extracting the relations between full-form Chinese phrases and their abbreviations is an interesting and important task for many NLP appli-cations (e.g., machine translation, information re-trieval, etc.)."
"Recently,[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE]have investigated this task."
"Specifically,[REF_CITE]describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treat-ing the abbreviation as the observation and the full-form words as states in the model."
"Using a set of manually-created full-abbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa)."
"Clearly, the method[REF_CITE]is super-vised because it requires the full-abbreviation rela-tions as training data."
"They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper."
"Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system."
"Other interesting work that addresses a similar task as ours includes the work on homophones (e.g.,[REF_CITE]), abbreviations with their defi-nitions (e.g.,[REF_CITE]), abbreviations and acronyms in the medical doma[REF_CITE], and transliteration (e.g., ([REF_CITE];"
"While all the above work deals with the rela-tions occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the rela-tions from the web corpora, instead from just formal text."
"Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena."
"In this paper, we have first presented a taxonomy of the formal-informal relations occurring in Chinese text."
We have then proposed a novel method for discovering and modeling the relationship between informal Chinese expressions (including colloqui-alisms and instant-messaging slang) and their formal equivalents.
"Specifically, we have proposed a boot-strapping procedure to identify a list of candidate informal phrases in web corpora."
"Given an infor-mal phrase, we retrieved contextual instances from the web using a search engine, generated hypothe-ses of formal equivalents via this data, and ranked the hypotheses using a conditional log-linear model."
"In the log-linear model, we incorporated as feature functions both rule-based intuitions and data co-occurrence phenomena (either as an explicit or in-direct definition, or through formal/informal usages occurring in free variation in a discourse)."
"We tested our system on manually collected test examples, and found that the (formal-informal) relationship discovery and extraction process using our method achieves an average 1-best precision of 62%."
"Given the ubiquity of informal conversational style on the internet, this work has clear applications for text nor-malization in text-processing systems including ma-chine translation aspiring to broad coverage."
We demonstrate the effectiveness of multilin-gual learning for unsupervised part-of-speech tagging.
"The key hypothesis of multilin-gual learning is that by combining cues from multiple languages, the structure of each be-comes more apparent."
We formulate a hier-archical Bayesian model for jointly predicting bilingual streams of part-of-speech tags.
The model learns language-specific features while capturing cross-lingual patterns in tag distri-bution for aligned words.
"Once the parame-ters of our model have been learned on bilin-gual parallel data, we evaluate its performance on a held-out monolingual test set."
Our evalu-ation on six pairs of languages shows consis-tent and significant performance gains over a state-of-the-art monolingual baseline.
"For one language pair, we observe a relative reduction in error of 53%."
"In this paper, we explore the application of multilin-gual learning to part-of-speech tagging when no an-notation is available."
This core task has been studied in an unsupervised monolingual framework for over a decade and is still an active area of research.
"In this paper, we demonstrate the effectiveness of multilin-gual learning when applied to both closely related and distantly related language pairs."
We further ana-lyze the language features which lead to robust bilin-gual performance.
The fundamental idea upon which our work is based is that the patterns of ambiguity inherent in part-of-speech tag assignments differ across lan-guages.
"At the lexical level, a word with part-of-speech tag ambiguity in one language may corre-spond to an unambiguous word in the other lan-guage."
"For example, the word “can” in English may function as an auxiliary verb, a noun, or a regular verb."
"However, each of the corresponding functions in Serbian is expressed with a distinct lexical item."
Languages also differ in their patterns of structural ambiguity.
"For example, the presence of an article in English greatly reduces the ambiguity of the suc-ceeding tag."
"In Serbian, a language without articles, this constraint is obviously absent."
"The key idea of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent."
"While multilingual learning can address ambigu-ities in each language, it must be flexible enough to accommodate cross-lingual variations such as tag inventory and syntactic structure."
"As a result of such variations, two languages often select and order their tags differently even when expressing the same meaning."
A key challenge of multilingual learning is to model language-specific structure while allow-ing information to flow between languages.
We jointly model bilingual part-of-speech tag se-quences in a hierarchical Bayesian framework.
"For each word, we posit a hidden tag state which gen-erates the word as well as the succeeding tag."
"In addition, the tags of words with common seman-tic or syntactic function in parallel sentences are combined into bilingual nodes representing the tag pair."
These joined nodes serve as anchors that cre-ate probabilistic dependencies between the tag se- quences in each language.
"We use standard tools from machine translation to discover aligned word-pairs, and thereafter our model treats the alignments as observed data."
Our model structure allows language-specific tag inventories.
"Additionally, it assumes only that the tags at joined nodes are correlated; they need not be identical."
We factor the conditional probabilities of joined nodes into two individual transition probabil-ities as well as a coupling probability.
"We define priors over the transition, emission, and coupling parameters and perform Bayesian inference using Gibbs sampling and the Metropolis-Hastings algo-rithm."
"We evaluate our model on a parallel corpus of four languages: English, Bulgarian, Serbian, and Slovene."
"For each of the six language pairs, we train a bilingual model on this corpus, and evaluate it on held-out monolingual test sets."
Our results show consistent improvement over a monolingual baseline for all languages and all pairings.
"In fact, for one language pair – Serbian and Slovene – the error is reduced by over 53%."
"Moreover, the multilingual model significantly reduces the gap between unsu-pervised and supervised performance."
"For instance, in the case of Slovene this gap is reduced by 71%."
We also observe significant variation in the level of improvement across language pairs.
We show that a cross-lingual entropy measure corresponds with the observed differentials in performance.
"Multilingual Learning A number of approaches for multilingual learning have focused on induc-ing cross-lingual structures, with applications to machine translation."
Examples of such efforts include work on the induction of synchronous grammars[REF_CITE]and learning multilingual lexical resources[REF_CITE].
"Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their trans-lations[REF_CITE]."
"When annotations for a task of interest are avail-able in a source language but are missing in the target language, the annotations can be projected across a parallel corpus[REF_CITE]."
"In fact, projection methods have been used to train highly accurate part-of-speech taggers[REF_CITE]."
"In contrast, our own work assumes that an-notations exist for neither language."
"Finally, there has been recent work on applying unsupervised multilingual learning to morphologi-cal segmentati[REF_CITE]."
"In this paper, we demonstrate that unsupervised mul-tilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging."
"Unsupervised Part-of-Speech Tagging Since the work[REF_CITE], the HMM has been the model of choice for unsupervised tagging[REF_CITE]."
Recent advances in these approaches include the use of a fully Bayesian HMM[REF_CITE].
"In very recent work,[REF_CITE]depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features."
"In addition, a number of approaches have focused on develop-ing discriminative approaches for unsupervised and semi-supervised tagging[REF_CITE]."
Our focus is on developing a simple model that effectively incorporates multilingual evidence.
We view this direction as orthogonal to refining mono-lingual tagging models for any particular language.
We propose a bilingual model for unsupervised part-of-speech tagging that jointly tags parallel streams of text in two languages.
"Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set."
"Our key hypothesis is that the patterns of ambigu-ity found in each language at the part-of-speech level will differ in systematic ways; by considering multi-ple language simultaneously, the total inherent am-biguity can be reduced in each language."
"The model is designed to permit information to flow across the language barrier, while respecting language-specific idiosyncrasies such as tag inventory, selection, and order."
"We assume that for pairs of words that share similar semantic or syntactic function, the associ-ated tags will be statistically correlated, though not necessarily identical."
"We use such word pairs as the bilingual anchors of our model, allowing cross-lingual information to be shared via joint tagging de-cisions."
"We use standard tools from machine trans-lation to identify these aligned words, and thereafter our model treats them as fixed and observed data."
"To avoid cycles, we remove crossing edges from the alignments."
"For unaligned parts of the sentence, the tag and word selections are identical to standard monolin-gual HMM’s."
"Figure 1 shows an example of the bilingual graphical structure we use, in comparison to two independent monolingual HMM’s."
We formulate a hierarchical Bayesian model that exploits both language-specific and cross-lingual patterns to explain the observed bilingual sentences.
We present a generative story in which the observed words are produced by the hidden tags and model parameters.
"In Section 4, we describe how to in-fer the posterior distribution over these hidden vari-ables, given the observations."
"Our generative model assumes the existence of two tagsets, T and T ′ , and two vocabularies W and W ′ , one of each for each language."
"For ease of exposi-tion, we formulate our model with bigram tag de- pendencies."
"However, in our experiments we used a trigram model, which is a trivial extension of the model discussed here and in the next section. 1."
"For each tag t ∈ T, draw a transition distri-bution φ t over tags T, and an emission distri-bution θ t over words W , both from symmetric Dirichlet priors. [Footnote_1] 2."
"1 The Dirichlet is a probability distribution over the simplex, and is conjugate to the multinomial[REF_CITE]."
"For each tag t ∈ T ′ , draw a transition distri-bution φ ′t over tags T ′ , and an emission distri-bution θ ′t over words W ′ , both from symmetric Dirichlet priors. 3. Draw a bilingual coupling distribution ω over tag pairs T × T ′ from a symmetric Dirichlet prior. 4."
For each bilingual parallel sentence: (a) Draw an alignment a from an alignment distribution
"A (see the following para-graph for formal definitions of a and A), (b) Draw a bilingual sequence of part-of-speech tags (x 1 , ..., x m ), (y 1 , ..., y n ) ac-cording to:"
"P (x 1 , ..., x m , y 1 , ..., y n |a, φ, φ ′ , ω). [Footnote_2]"
2 Note that we use a special end state rather than explicitly modeling sentence length. Thus the values of m and n depend on the draw.
"This joint distribution is given in equa-tion 1. (c) For each part-of-speech tag x i in the first language, emit a word from W : e i ∼ θ x i , (d) For each part-of-speech tag y j in the sec-ond language, emit a word from W ′ : f j ∼ θ y′ j ."
We define an alignment a to be a set of one-to-one integer pairs with no crossing edges.
"Intuitively, each pair (i, j) ∈ a indicates that the words e i and f j share some common role in the bilingual paral-lel sentences."
"In our experiments, we assume that alignments are directly observed and we hold them fixed."
"From the perspective of our generative model, we treat alignments as drawn from a distribution A, about which we remain largely agnostic."
We only require that A assign zero probability to alignments which either: (i) align a single index in one language to multiple indices in the other language or (ii) con-tain crossing edges.
"The resulting alignments are thus one-to-one, contain no crossing edges, and may be sparse or even possibly empty."
Our technique for obtaining alignments that display these properties is described in Section 5.
"Given an alignment a and sets of transition param-eters φ and φ ′ , we factor the conditional probability of a bilingual tag sequence (x 1 , ...x m ), (y 1 , ..., y n ) into transition probabilities for unaligned tags, and joint probabilities over aligned tag pairs:"
"P (x 1 , ..., x m , y 1 , ..., y n |a, φ, φ ′ , ω) ="
Y φ x i−1 (x i ) ·
"Y φ ′ y j−1 (y j ) · unaligned i unaligned j (1) Y P (x i , y j |x i−1 , y j−1 , φ, φ ′ , ω) (i,j)∈a"
"Because the alignment contains no crossing edges, we can model the tags as generated sequen-tially by a stochastic process."
We define the dis-tribution over aligned tag pairs to be a product of each language’s transition probability and the cou-pling probability:
"P (x i , y j |x i−1 , y j−1 , φ, φ ′ , ω) = φ x i−1 (x i ) φ y′ j−1 (y j ) ω(x i , y j ) (2) Z"
The normalization constant here is defined as:
"Z = X φ x i−1 (x) φ y′ j−1 (y) ω(x, y) x,y"
This factorization allows the language-specific tran-sition probabilities to be shared across aligned and unaligned tags.
"In the latter case, the addition of the coupling parameter ω gives the tag pair an addi-tional role: that of multilingual anchor."
"In essence, the probability of the aligned tag pair is a product of three experts: the two transition parameters and the coupling parameter."
"Thus, the combination of a high probability transition in one language and a high probability coupling can resolve cases of inher-ent transition uncertainty in the other language."
"In addition, any one of the three parameters can “veto” a tag pair to which it assigns low probability."
"To perform inference in this model, we predict the bilingual tag sequences with maximal probabil-ity given the observed words and alignments, while integrating over the transition, emission, and cou-pling parameters."
"To do so, we use a combination of sampling-based techniques."
The core element of our inference procedure is Gibbs sampling[REF_CITE].
"Gibbs sampling begins by randomly initializing all unob-served random variables; at each iteration, each ran-dom variable z i is sampled from the conditional dis-tribution P (z i |z −i ), where z −i refers to all variables other than z i ."
"Eventually, the distribution over sam-ples drawn from this process will converge to the unconditional joint distribution P(z) of the unob-served variables."
"When possible, we avoid explic-itly sampling variables which are not of direct inter-est, but rather integrate over them—this technique is known as “collapsed sampling,” and can reduce variance[REF_CITE]."
"We sample: (i) the bilingual tag sequences (x, y), (ii) the two sets of transition parameters φ and φ ′ , and (iii) the coupling parameter ω."
"We integrate over the emission parameters θ and θ ′ , whose priors are Dirichlet distributions with hyperparameters θ 0 and θ 0′ ."
"The resulting emission distribution over words e i , given the other words e −i , the tag sequences x and the emission prior θ 0 , can easily be derived as:"
"P (e i |x, e −i , θ 0 ) ="
Z θ x i (e i )
"P (θ x i |θ 0 ) dθ x i θ xi (3) n(x i , e i ) + θ 0 = n(x i ) +"
W x i θ 0
"Here, n(x i ) is the number of occurrences of the tag x i in x −i , n(x i , e i ) is the number of occurrences of the tag-word pair (x i , e i ) in (x −i , e −i ), and W x i is the number of word types in the vocabulary W that can take tag x i ."
The integral is tractable due to Dirichlet-multinomial conjugacy[REF_CITE].
"We will now discuss, in turn, each of the variables that we sample."
"Note that in all cases we condi-tion on the other sampled variables as well as the observed words and alignments, e, f and a, which are kept fixed throughout."
This section presents the conditional distributions that we sample from to obtain the part-of-speech tags.
"Depending on the alignment, there are several scenarios."
"In the simplest case, both the tag to be sampled and its succeeding tag are not aligned to any tag in the other language."
"If so, the sampling distribution is identical to the monolingual case, in-cluding only terms for the emission (defined in equa-tion 3), and the preceding and succeeding transi-tions:"
"P (x i |x −i , y, e, f, a, φ, φ ′ , ω, θ 0 , θ ′0 ) ∝ P (e i |x, e −i , θ 0 ) φ x i−1 (x i ) φ x i (x i+1 )."
"For an aligned tag pair (x i ,y j ), we sample the identity of the tags jointly."
By applying the chain rule we obtain terms for the emissions in both lan-guages and a joint term for the transition probabili-ties:
"P (x i , y j |x −i , y −j , e, f, a, φ, φ ′ , ω, θ 0 , θ 0′ ) ∝ P (e i |x, e −i , θ 0 )"
"P (f j |y, f −j , θ 0′ )"
"P (x i , y j |x −i , y −j , a, φ, φ ′ , ω)"
The expansion of the joint term depends on the alignment of the succeeding tags.
"In the case that the successors are not aligned, we have a product of the bilingual coupling probability and four transition probabilities (preceding and succeeding transitions in each language):"
"P(x i , y j |x −i , y −j , a, φ, φ ′ , ω) ∝ ω(x i , y j )φ x i−1 (x i ) φ y′ j−1 (y j ) φ x i (x i+1 ) φ ′y j (y j+1 )"
"Whenever one or more of the succeeding tags is aligned, the sampling formulas must account for the effect of the sampled tag on the joint probability of the succeeding tags, which is no longer a sim-ple multinomial transition probability."
"We give the formula for one such case—when we are sampling an aligned tag pair (x i , y j ), whose succeeding tags (x i+1 , y j+1 ) are also aligned to one another:"
"P(x i , y j |x −i , y −j , a, φ, φ ′ , ω) ∝ ω(x i , y j ) x,y φ x i (x) φ ′y j (y) ω(x, y)# · φ x i−1 (x i ) φ ′y j−1 (y j ) &quot;P φ x i (x i+1 ) φ ′y j (y j+1 )"
"Similar equations can be derived for cases where the succeeding tags are not aligned to each other, but to other tags."
"When computing the joint probability of an aligned tag pair (Equation 2), we employ the transition pa-rameters φ, φ ′ and the coupling parameter ω in a nor-malized product."
"Because of this, we can no longer regard these parameters as simple multinomials, and thus can no longer sample them using the standard closed formulas."
"Instead, to resample these parameters, we re-sort to the Metropolis-Hastings algorithm as a sub-routine within Gibbs sampling[REF_CITE]."
Metropolis-Hastings is a Markov chain sampling technique that can be used when it is impossible to directly sample from the posterior.
"Instead, sam-ples are drawn from a proposal distribution and then stochastically accepted or rejected on the basis of: their likelihood, their probability under the proposal distribution, and the likelihood and proposal proba-bility of the previous sample."
We use a form of Metropolis-Hastings known as an independent sampler.
"In this setup, the proposal distribution does not depend on the value of the previous sample, although the accept/reject decision does depend on the previous model likelihood."
"More formally, if we denote the proposal distribution as Q(z), the target distribution as P (z), and the previ-ous sample as z, then the probability of accepting a new sample z ∗ ∼ Q is set at:"
"P (z ∗ ) Q(z) min 1, P (z) Q(z ∗ )"
Theoretically any non-degenerate proposal distri-bution may be used.
"However, a higher acceptance rate and faster convergence is achieved when the proposal Q is a close approximation of P ."
"For a par-ticular transition parameter φ x , we define our pro-posal distribution Q to be Dirichlet with parameters set to the bigram counts of the tags following x in the sampled tag data."
"Thus, the proposal distribu-tion for φ x has a mean proportional to these counts, and is thus likely to be a good approximation to the target distribution."
"Likewise for the coupling parameter ω, we de-fine a Dirichlet proposal distribution."
"This Dirichlet is parameterized by the counts of aligned tag pairs (x,y) in the current set of tag samples."
"Since this sets the mean of the proposal to be proportional to these counts, this too is likely to be a good approxi-mation to the target distribution."
After every iteration of Gibbs sampling the hyper-parameters θ 0 and θ 0′ are re-estimated using a single Metropolis-Hastings move.
The proposal distribu-tion is set to a Gaussian with mean at the current value and variance equal to one tenth of the mean.
Our evaluation framework follows the standard pro-cedures established for unsupervised part-of-speech tagging.
"Given a tag dictionary (i.e., a set of possi-ble tags for each word type), the model has to select the appropriate tag for each token occurring in a text."
We also evaluate tagger performance when only in-complete dictionaries are available[REF_CITE].
"In both scenarios, the model is trained only using untagged text."
"In this section, we first describe the parallel data and part-of-speech annotations used for system eval-uation."
Next we describe a monolingual base-line and our procedures for initialization and hyper-parameter setting.
"Data As a source of parallel data, we use Orwell’s novel “Nineteen Eighty Four” in the original English as well as translations to three Slavic languages — Bulgarian, Serbian and Slovene."
This data is dis-tributed as part of the Multext-East corpus which is publicly available.
"The corpus provides detailed morphological annotation at the world level, includ-ing part-of-speech tags."
In addition a lexicon for each language is provided.
We obtain six parallel corpora by considering all pairings of the four languages.
We compute word level alignments for each language pair using Giza++.
"To generate one-to-one alignments at the word level, we intersect the one-to-many alignments going in each direction and automatically remove crossing edges in the order in which they appear left to right."
This process results in alignment of about half the tokens in each bilingual parallel corpus.
We treat the alignments as fixed and observed variables throughout the training procedure.
"The corpus consists of 94,725 English words (see Table 2)."
"For every language, a random three quar-ters of the data are used for learning the model while the remaining quarter is used for testing."
"In the test set, only monolingual information is made available to the model, in order to simulate future performance on non-parallel data."
Tagset The Multext-East corpus is manually an-notated with detailed morphosyntactic information.
"In our experiments, we focus on the main syntac-tic category encoded as a first letter of the labels."
"The annotation distinguishes between 13 parts-of-speech, of which 11 are common for all languages"
"In the Multext-East corpus, punctuation marks are not annotated."
We expand the tag repository by defining a separate tag for all punctuation marks.
This allows the model to make use of any transition or coupling patterns involving punctuation marks.
We do not consider punctuation tokens when com-puting model accuracy.
Table 2 shows the tag/token ratio for these lan-guages.
"For Slavic languages, we use the tag dic-tionaries provided with the corpus."
"For English, we use a different process for dictionary construc-tion."
"Using the original dictionary would result in the tag/token ratio of 1.5, in comparison to the ra-tio of 2.[Footnote_3] observed in the Wall Street Journal (WSJ) corpus."
"3 The remaining two tags are Particle and Determiner; The English tagset does not include Particle while the other three languages Serbian, Slovene and Bulgarian do not have Deter-miner in their tagset."
"To make our results on English tagging more comparable to previous benchmarks, we expand the original dictionary of English tags by merging it with the tags from the WSJ dictionary."
"This process results in a tag/token ratio of 2.58, yielding a slightly more ambiguous dictionary than the one used in pre-vious tagging work. [Footnote_4]"
4 We couldn’t perform the same dictionary expansion for the Slavic languages due to a lack of additional annotated resources.
Monolingual Baseline As our monolingual base-line we use the unsupervised Bayesian HMM model[REF_CITE](BHMM1).
This model modifies the standard HMM by adding pri-ors and by performing Bayesian inference.
Its is in line with state-of-the-art unsupervised models.
"This model is a particulary informative baseline, since our model reduces to this baseline model when there are no alignments in the data."
This implies that any performance gain over the baseline can only be at- tributed to the multilingual aspect of our model.
We used our own implementation after verifying that its performance on WSJ was identical to that reported[REF_CITE].
"Supervised Performance In order to provide a point of comparison, we also provide supervised re-sults when an annotated corpus is provided."
We use the standard supervised HMM with Viterbi decod-ing.
"Training and Testing Framework Initially, all words are assigned tags randomly from their tag dictionaries."
"During each iteration of the sam-pler, aligned tag pairs and unaligned tags are sam-pled from their respective distributions given in Sec-tion 4.1 above."
The hyperparameters θ 0 and θ 0′ are initialized with the values learned during monolin-gual training.
They are re-estimated after every iter-ation of the sampler using the Metropolis Hastings algorithm.
The parameters φ and φ ′ are initially set to trigram counts and the ω parameter is set to tag pair counts of aligned pairs.
"Overall, the algorithm is run for 1000 iterations of tag sampling, by which time the resulting log-likelihood converges to stable values."
"Each Metropolis Hastings subroutine sam-ples 20 values, with an acceptance ratio of around 1/6, in line with the standard recommended values."
"After training, trigram and word emission prob-abilities are computed based on the counts of tags assigned in the final iteration."
"For smoothing, the final sampled values of the hyperparameters are used."
The highest probability tag sequences for each monolingual test set are then predicted using trigram Viterbi decoding.
We report results averaged over five complete runs of all experiments.
"Complete Tag Dictionary In our first experiment, we assume that a complete dictionary listing the pos-sible tags for every word is provided in each lan-guage."
"Table 1 shows the monolingual results of a random baseline, an unsupervised Bayesian HMM and a supervised HMM."
Table 3 show the results of our bilingual models for different language pair-ings while repeating the monolingual unsupervised results from Table 1 for easy comparison.
The final column indicates the absolute gain in performance over this monolingual baseline.
"Across all language pairs, the bilingual model consistently outperforms the monolingual baseline."
All the improvements are statistically significant by a Fisher sign test at p &lt; 0.05.
"For some lan-guage pairs, the gains are quite high."
"For instance, the pairing of Serbian and Slovene (two closely re-lated languages) yields absolute improvements of 6.7 and 7.7 percentage points, corresponding to rel-ative reductions in error of 51.4% and 53.2%."
"Pair-ing Bulgarian and English (two distantly related lan-guages) also yields large gains: 5.6 and 1.3 percent-age points, corresponding to relative reductions in error of 50% and 14%, respectively. [Footnote_5]"
"5 The accuracy of the monolingual English tagger is rela-tively high compared to the 87% reported[REF_CITE]on the WSJ corpus. We attribute this discrep-ancy to the slight differences in tag inventory used in our data-set. For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our tag inventory and corpus), the performance of Goldwater’s model on WSJ is similar to what we report here."
"When we compare the best bilingual result for each language (Table 3, in bold) to the monolin-gual supervised results (Table 1), we find that for all languages the gap between supervised and un-supervised learning is reduced significantly."
"For En-glish, this gap is reduced by 21%."
"For the Slavic lan-guages, the supervised-unsupervised gap is reduced by even larger amounts: 57%, 69%, and 78% for Serbian, Bulgarian, and Slovene respectively."
"While all the languages benefit from the bilin-gual learning framework, some language combina-tions are more effective than others."
"Slovene, for in-stance, achieves a large improvement when paired with Serbian (+7.7), a closely related Slavic lan-guage, but only a minor improvement when coupled with English (+1.3)."
"On the other hand, for Bulgar-ian, the best performance is achieved when coupling with English (+5.6) rather than with closely related Slavic languages (+3.1 and +2.4)."
"As these results show, an optimal pairing cannot be predicted based solely on the family connection of paired languages."
"To gain a better understanding of this variation in performance, we measured the internal tag en-tropy of each language as well as the cross-lingual tag entropy of language pairs."
"For the first measure, we computed the conditional entropy of a tag de-cision given the previous two tags."
"Intuitively, this should correspond to the inherent structural uncer-tainty of part-of-speech decisions in a language."
"In fact, as Table 1 shows, the trigram entropy is a good indicator of the relative performance of the mono-lingual baseline."
"To measure the cross-lingual tag entropies of language pairs, we considered all bilin-gual aligned tag pairs, and computed the conditional entropy of the tags in one language given the tags in the other language."
This measure should indi-cate the amount of information that one language in a pair can provide the other.
The results of this anal- ysis are given in the first column of Table 3.
"We ob-serve that the cross-lingual entropy is lowest for the Serbian and Slovene pair, corresponding with their large gain in performance."
"Bulgarian, on the other hand, has lowest cross-lingual entropy when paired with English."
This corresponds with the fact that English provides Bulgarian with its largest perfor-mance gain.
"In general, we find that the largest per-formance gain for any language is achieved when minimizing its cross-lingual entropy."
Reduced Tag Dictionary We also conducted ex-periments to investigate the impact of the dictio-nary size on the performance of the bilingual model.
"Here, we provide results for the realistic scenario where only a very small dictionary is present."
Ta-ble 4 shows the performance when a tag dictionary for the 100 most frequent words is present in each language.
The bilingual model’s results are consis-tently and significantly better than the monolingual baseline for all language pairs.
We have demonstrated the effectiveness of multilin-gual learning for unsupervised part-of-speech tag-ging.
"The key hypothesis of multilingual learn- ing is that by combining cues from multiple lan-guages, the structure of each becomes more appar-ent."
We formulated a hierarchical Bayesian model for jointly predicting bilingual streams of tags.
The model learns language-specific features while cap-turing cross-lingual patterns in tag distribution.
Our evaluation shows significant performance gains over a state-of-the-art monolingual baseline.
Code-switching is an interesting linguistic phenomenon commonly observed in highly bilingual communities.
It consists of mixing languages in the same conversational event.
This paper presents results on Part-of-Speech tagging Spanish-English code-switched dis-course.
"We explore different approaches to exploit existing resources for both languages that range from simple heuristics, to language identification, to machine learning."
The best results are achieved by training a machine learning algorithm with features that combine the output of an English and a Spanish Part-of-Speech tagger.
"Worldwide the percentage of bilingual speakers is fairly large, and it keeps increasing at a high rate."
"In the U.S., 18% of the total population speaks a language other than English at home, the major-ity of which speaks Spanish (U.S.[REF_CITE])."
"A significant percentage of this Spanish-English bilingual population code-switch between the two languages in what is often referred as Span-glish, the mix of Spanish and English."
Spanish and English are not the only occurrence of language mixtures.
"Examples of other popular combinations include Arabic dialects, French and German, Span-ish and Catalan, Maltese and English, and English and French."
"Typically when there are linguistic bor-ders, or when the country has more than one official language, we can find instances of code-switching."
"Despite the wide use of code-switched discourse among bilinguals, this linguistic phenomenon has received little attention in the fields of Natural Lan-guage Processing and Computational Linguistics."
Part-of-Speech (POS) tagging is a well studied prob-lem in these fields.
"For languages such as English, German, Spanish, and Chinese there are several dif-ferent POS taggers that reach high accuracies, espe-cially in news text corpora."
"However, to our knowl-edge, there is no previous work on developing a POS tagger for text with mixes of languages."
In this paper we present results on the problem of POS tagging English-Spanish code-switched dis-course by taking advantage of existing taggers for both languages.
"This rationale follows the evi-dence from studies of code-switching on different language pairs, which have shown code-switching to be grammatical according to both languages be-ing switched."
We use different heuristics to combine POS tag information from existing monolingual tag-gers.
We also explore the use of different language identification methods to select POS tags from the appropriate monolingual tagger.
"However, the best results are achieved by a machine learning approach using features generated by the monolingual POS taggers."
"The next section presents the facts about code-switching, including some previous work done mainly in linguistics."
Then in Section 3 we dis-cuss previous work related to the automated pro-cessing of code-switched discourse.
In Section 4 we describe the English-Spanish code-switched data set gathered for the experimental evaluation.
Sec-tion 5 presents the heuristics-based approaches for POS tagging that we explored.
In Section 6 we describe our machine learning approach and show results on POS tagging code-switched text.
"An in depth analysis of results is presented in Section 7, and we conclude this paper with a summary of the findings and directions for future work in Section 8."
"In the linguistic, sociolinguistic, psychology, and psycholinguistic literature, bilingualism and the in-herent phenomena it exhibits have been studied for nearly a century[REF_CITE]."
"Despite the numerous previ-ous studies of linguistic characteristics of bilingual-ism, there is no clear consensus on the terminol-ogy related to language alternation patterns in bilin-gual speakers."
"The alternation of languages within a sentence is known as code-mixing, but it has also been referred as intrasentential code-switching, and intrasentential alternati[REF_CITE]."
"Alternation across sen-tence boundaries is known as intersentential code-switching, or just code-switching."
In the rest of this paper we will refer to the mixing of languages as code-switching.
"When necessary, we will differen-tiate the type of code-switching by referring to al-ternations within sentences as intrasentential code-switching and alternations across sentence bound-aries as intersentential code-switching."
"Linguistic phenomena in bilingual speakers have been analyzed on different language pairs, includ-ing English-French, English-Dutch, Finish-English, Arabic-French, and Spanish-English, to name a few."
"There is a general agreement that code-switched pat-terns are not generated randomly; according to these studies, they follow specific grammatical rules."
"Fur-thermore, some studies suggest that, if these rules are violated, the resulting discourse will sound un-natural[REF_CITE]."
The fol-lowing shows the rules governing code-switching discourse described in several studies[REF_CITE]. • Switches can take place only between full word boundaries.
This is also known as the free mor-pheme constraint. • Monolingual constructs within the sentence will follow the grammatical rules of the mono-lingual fragment. • Permissible switch points are those that do not violate the order of adjacent constituents on both sides of the switch point of either of the languages.
This is called the equivalence con-straint.
"Although these rules are somewhat controversial, and most of the studies on this area have been con-ducted on small samples, we cannot ignore the fact that patterns bearing the above rules have emerged in different bilingual communities with different back-grounds."
A previous work related to the processing of code-switched text deals with language identification on English-Maltese code-switched SMS messages[REF_CITE].
"In addition to deal-ing with intrasentential code-switching, they have to deal with text where misspellings and ad hoc word contractions abound."
"What Rosner and Far-rigua have found to work best for language identi-fication in this noisy domain is a combination of a bigram Hidden Markov Model, trained on language transitions, and a trigram character Markov Model for handling unknown words."
"In another related work, Franco and Solorio present preliminary results on training a language model for Spanish-English code-switched text[REF_CITE]."
"To evaluate their language model, they asked a human subject to judge sentences generated by a PCFG in-duced from training data and the language model."
"However, they only used one human judge."
Regarding the automated POS tagging and pars-ing of code-mixed utterances there is little prior work.
"To the best of our knowledge, there is no parser, nor POS tagger, currently available for the syntactic analysis of this type of discourse."
"There are theoretical approaches that propose formalisms to represent the structure of code-switched utter-ances and describe a framework for parsing and gen-erating mixed sentences, for example for Marathi and English[REF_CITE], or Hindi and English[REF_CITE]."
Sankoff proposed a production model of bilingual discourse that accounts for the equivalence constraint and the unpredictability of code-switching[REF_CITE].
His real-time production model draws on the alter-nation of fragments from two virtual monolingual sentences.
It also accounts for other types of code-switching such as repetition-translation and inser-tional code-switching.
But no statistical assessment has been conducted on real corpora.
"Our goal is to develop a POS tagger for code-switched utterances, which is the first step of the syntactic analysis of any language."
Among the chal-lenges we face is the lack of a representative sam-ple of code-mixed discourse.
"Most POS taggers are built using large collections, usually at least a mil-lion words, such as the Brown corpus[REF_CITE], the Wall Street Journal corpus[REF_CITE], or the Switchboard corpus[REF_CITE]."
"Currently, there is no annotation of code-switched text of comparable size."
"But in contrast to the lack of linguistic resources available for Spanish-English code-mixed discourse, English and Spanish have sufficient resources, especially En-glish."
"Thus, rather than starting from scratch, we will draw on existing taggers for both languages, which will reduce the amount of code-switched data needed."
Some examples of POS taggers that per-form reasonably well on monolingual text of each language can be found[REF_CITE].
"However, these tools are designed to work on monolingual text, therefore if applied as they are to code-switched text, their ac-curacy will decrease by a large margin."
In the fol-lowing sections we will explore different methods for combining monolingual taggers.
"Data collections that have instances of Spanish-English code-switching, Spanglish for short, are not easily found since code-switching is primarily used in spoken form."
To gather data we recorded a con-versation among three staff members of a southwest university in the U.S.
"The three speakers come from a highly bilingual background and code-switch reg-ularly when speaking among themselves, or other bilingual speakers."
"This recording session has around 39 minutes of continuous speech (922 sentences, about 8k words) and was transcribed and annotated with POS tags by a human annotator."
The annotations were later re-vised by a different annotator but no inter-annotator agreement was measured.
The POS tag set used in the annotation is the combination of the tag sets from the English and the Spanish Tree Taggers (see Sec-tion 5).
"The vocabulary of the transcription has a to-tal of 1,516 different word forms [Footnote_1] ."
1 This transcription and the audio file are freely available for research purposes by contacting the first author.
"In the conversa-tion a total of 239 switches were identified manually, out of which 129 are intrasentential code-switches, and the rest are intersentential."
"English is the pre-dominant language used, with a total of 6,020 tokens and 576 monolingual sentences."
"In contrast, the transcription has close to 2k tokens in Spanish."
"Ta-ble 1 shows examples of code-switching taken from the recorded conversation; (a) and (b) are instances of intrasentential code-switching, and (c) shows in-tersentential code-switching."
In this section we present several heuristics-based methods for POS tagging code-switched text.
"First, we describe the monolingual taggers used in this work."
Then we present the different approaches ex-plored and contrast their performance.
We used the Tree Tagger[REF_CITE]for this work because of the following considerations: 1.
It has both English and Spanish versions.
"The English tagger uses a slightly modified version of the Penn Treebank tag set and was trained and eval-uated on different portions of the Penn Treebank, reaching a POS tagging accuracy of 96.36%."
The Spanish one uses a different tagset with 75 different POS tags [Footnote_2] and was trained on the Spanish CRATER corpus. 2.
2 The authors were unable to identify the source of the Span-ish tagset.
"The transition probabilities are estimated using a modified version of the ID3 decision tree algorithm[REF_CITE], which provides more freedom to learn contextual cues than n-grams. 3."
"Both taggers include a special tag for foreign words, P E for Spanish and F W for English."
"We do not expect this tag to identify correctly all foreign words, but when available this information will be exploited. 4."
The Tree tagger generates probability estimates on the tags that can be used as features. 5.
"Finally, when the tagger fails to lemmatize a word it outputs the special token hunknowni."
This information can be used as a hint of words that do not belong to that particular language.
"For all heuristics the complete Spanglish data set was given to both taggers as a single text, then the final tag for each word was selected from the output of the taggers according to the different heuristics."
"Table 2 shows the tagging accuracies of the different heuristics we explored, which are explained below. 1. Using the monolingual tagger."
Here we simply give the Spanglish text to the Spanish and the English tree tagger.
"We expect from both taggers a performance degradation due to the inclusion of for-eign words in code-switching, as compared against their accuracy on monolingual texts."
Another complicating factor to keep in mind is that we are dealing with spoken language.
"Hesitations, fillers, disfluencies, and interruption points, such as Umm, Mmmhmm, and Uh-huh, are frequently observed in speech and it is well known that they complicate the POS tagging task."
"The tagging accuracy from using the individual taggers is rather low, 26% for the Spanish tagger and 54% for the English one."
The large difference between the two taggers can be attributed to the fact that the majority of the words in the corpus are in English. 2.
Using confidence thresholds.
"The Tree Tagger can output probabilities for each tag, showing the confidence of the tagger on each particular tag."
To use this information we choose for each word the tag from the tagger with the highest confidence.
When there is a tie we use either the English or the Spanish tag.
Table 2 shows the results for the two cases.
"The “Highest prob tag or English” heuristic gives an ac-curacy of 51%, which is almost as accurate as using only the English tagger."
"The “Highest prob tag or Spanish” achieves an accuracy of 49%, which is an improvement over using only the monolingual Span-ish tagger, but it is still below the accuracy of the En-glish monolingual tagger."
This is also possibly due to the task being easier for the English tagger. 3.
Combining confidence thresholds with knowl-edge from special tags and lemmas.
"This heuristic uses confidence thresholds combined with decisions based on the special tags, described in Section 5.1, and the unknown lemmas found."
"Let POS E (w i ) and POS S (w i ) be the POS tags assigned to word w i by the English and Spanish tagger respectively; and let Prob E (w i ) and Prob S (w i ) be the confi-dence scores of POS tags for word w i computed by the English and Spanish tree taggers, respectively."
"For each word w i in the text, the final POS tag,"
"P OS F (w i ), will be assigned as follows: 1."
If POS E (w i ) =
"FW, then POS F (w i ) ← POS S (w i ) 2."
Else if POS S (w i ) =
"PE, then POS F (w i ) ← POS E (w i ) 3."
"Else if POS E (w i ) = hunknowni, then POS F (w i ) ← POS S (w i ) 4."
"Else if POS S (w i ) = hunknowni, then POS F (w i ) ← POS E (w i ) 5."
"Else if Prob E (w i ) &gt; Prob S (w i ), then POS F (w i ) ← POS E (w i ) 6."
Else POS F (w i ) ← POS S (w i )
"This heuristic performs better than the other meth-ods explored so far, yielding an accuracy of 64.27%."
It seems that knowledge of the taggers can be used to improve results.
"However, POS tagging accuracy is still poor. 4."
Selecting POS tags based on automated language identification.
We used two different strategies for automatically identifying the language at the word level.
One is based on dictionary look-up and the other is character-based language models.
"For the first approach, every word in the text is searched in the English and Spanish dictionaries."
"If a word is found in the English dictionary, then we identify that word as belonging to English and the POS tags from the English tagger are used for that word and the following ones, until a word is found in the Span-ish dictionary."
"Similarly, for a word not found in the English dictionary, but found in the Spanish dic-tionary, we use the Spanish tags until an English word is found."
"Note that this simple heuristic will always label words that belong to both languages as English, which is also the case for words not found in either dictionary."
This dictionary-based method has a language identification accuracy of 94% on the Spanglish corpus.
"The character language models were trained on the Agence France Presse (AFP) portions of the Gi-gaword for English and Spanish, respectively."
"For each of the words in the Spanglish corpus, we first decide its language by choosing the one with the lowest perplexity, calculated using character n-gram language models, then we use the corresponding"
"We experimented with different language model orders, with n ranging from 2 to 6, and found that we achieve the highest accuracy, 81.46%, on POS tagging using a 5-gram language model."
This 5-gram method reached a language identification ac-curacy of 85% for the Spanglish corpus.
"However, the language identification method using dictionary look-up achieved the best POS tagging result so far: 86.03%."
"The Spanglish conversation is dominated by every-day language that is easily found in dic-tionaries, while the text used to train the charac-ter based n-gram language models includes vocab-ulary that is not commonly used in conversations."
This can explain why the simple dictionary look-up approach yielded better results for our corpus.
"Performing manual identification of the language and sending to the appropriate tagger just the corre-sponding fragments yields a very high POS tagging accuracy, 89.72%."
This shows that it is important to deal with the language switches for boosting ac-curacy.
However relying on human annotated lan-guage tags would be expensive and for some tasks unfeasible.
"From Table 2 we can see that, with the exception of the language identification heuristic, accuracies are low for the previous experiments."
"However, we be-lieve that we can improve results further by using Machine Learning (ML) algorithms trained specif-ically for this task."
In this section we describe the ML setting and present a comparison of the differ-ent algorithms we tested.
The key point is that the features selected for de-scribing the learning instances are the output from the English and the Spanish taggers.
"This scheme is similar to a stacked classifier approach[REF_CITE], where the final classifier takes as input the predictions made by the different learners on the first pass and is trained to select the right tag from them, or a different one if the right answer is not available."
"The gold-standard POS tags are used as the class label, and instances in this learning task are described by the following attributes:"
Feature 1 is just the lexical word form as it ap-pears in the transcript.
"Features 2 to 4 are generated by the English Tree tagger, while features 5 to 7 are generated by the Spanish Tree tagger."
Thus all fea-tures are automatically extracted.
We evaluated experimentally the idea of using ML with different learning algorithms in WEKA[REF_CITE].
"We selected some of the most widely known algorithms, including Support Vector Machines (SVM) with a polynomial kernel of ex-ponent one[REF_CITE], Weka’s modified version of Quinlan’s C4.5 (J48)[REF_CITE], Additive Logistic Regression with Decision Stumps (Logit Boost)[REF_CITE]and Naive Bayes."
The only parameter we modified was for J48 –we enabled the option for reducing error pruning.
Table 3 shows the average accuracy of 10-fold cross-validation for each classifier together with the variance.
SVM and Logit Boost performed the best and the difference between the two algorithms is not significant according to the paired t-test (P-value = 0.1).
"For comparison, we show the accuracy of the language identification approach together with the oracle accuracy."
"The oracle is the accuracy achieved when always selecting the right POS tag, when it is available, from the output of both Tree Taggers."
We did not expect the oracle’s accuracy to be an up-per bound on the accuracy for the ML learning al-gorithm.
Our intuition is that the ML algorithm can be trained to identify when the taggers have made a mistake and what the right answer should be.
"As the results show, the ML approach can indeed out-perform the oracle, and the language identification method."
In Figure 1 we show the effect of the amount of training data on the accuracy using Logit Boost.
We selected Logit Boost for this and the follow-ing experiments since its accuracy is comparable to SVMs but it is computationally less expensive.
We randomly partitioned the transcription into 10 sub-groups.
Then we used one subgroup as the test set and the rest for training.
"Starting with one subgroup in the training set, we incrementally added one sub-group to the training set and evaluate the tagging performance of the test set."
"We repeated this pro-cess several times, choosing randomly a new test set each time."
The percentages shown are the average over all the experiments.
The curve flattens after 60% of the training data is used.
We do not gain much by adding more training data after this.
"Results shown in Table 3 demonstrate that POS tagging can be learned effectively based on the at-tributes described in Subsection 6.1, even if we are not explicitly adding contextual information."
"To de-termine the extent to which each attribute is con-tributing to the learning task, we performed another set of experiments where we selected different sub-sets of the attributes."
Table 4 shows the results with Logit Boost.
"Overall, the attributes taken from the English POS tagger are more valuable for this learn-ing task."
If we only take the word form and the fea-tures from the English Tree tagger (first row in Ta-ble 4) we are reaching an accuracy that outperforms all heuristics.
"Still, there is some valuable infor-mation provided by the Spanish POS tagger output since the highest accuracy is achieved by including the Spanish-based attributes in combination with the English-based ones."
"Surprisingly, we can manage to outperform the oracle by using only three attributes: the lexical word form and the POS tags from the English and Spanish tagger (see row 7 in table), or the POS tags from the monolingual taggers together with the lemma from the Spanish tagger (see row 4 from bottom to top)."
"We also experimented adding as an attribute the output of the language identifica-tion method, but found no significant changes in the accuracy."
We analyzed the different results gathered through the experiments and we present here the most rele-vant insights.
"The first discovery, is that a lot of the errors made by the oracle, and the other methods as well, are due to the difficulties inherent in dealing with sponta-neous speech where fillers, interruption points, hes-itations, and the like abound."
About as much as 20% of the errors made by the oracle are due to these features.
For the rest of the analysis we decided to ignore these types of mistakes for all methods and focus only on the remaining mistakes.
In the case of the oracle we are left with 445 erroneously POS tagged words.
"From those, about 50%, or 233 to be exact, are errors in sentences with code-switches."
We consider this to be a strong indication of the complexity that intrasentential switches add to the task of POS tagging.
"For the taggers, these sentences are incomplete, or ill-formed, since they have frag-ments with foreign words and thus, they fail to iden-tify them."
The rest of the oracle mistakes can not be attributable to a single cause.
"Some are fragmented sentences, and some are due to errors inherent of the tagger, but nothing is particulary salient about them."
"The language identification methods share, of course, the same mistakes made by the oracle, plus 342 more, for a total of 787 (in the case of the dictionary-based language identification)."
The chal-lenge of POS tagging code-switched text is more ev-ident for this method.
"Out of the mistakes made by the language identification method, 540 lie in sen-tences with code-switching, that is, nearly 70% of the mistakes."
POS tag was available from one of the taggers.
"Some typical examples of these errors are words that belong to both languages, such as “a”, “no”, “me” and “con”."
The ML approach outperformed both the lan-guage identification method and the oracle.
"Analyz-ing the predictions made by SVM we verified that out of the 445 errors made by the oracle, SVM cor-rectly tagged 223, the majority of which are words in sentences with code-switching (142 words)."
"When compared against the errors from the method based on language identification, SVM correctly tagged 481 words out of the 787, 374 of which are words in sentences with code-switches."
"In summary, the ML approach is more robust to code-switched sen-tences."
"Note that we did find some errors made by the ML approach that are not shared by the oracle or the language identification method, a total of 105."
Some of these mistakes are due to inconsistencies on the human-annotated tags.
"For instance, in most cases slang words such as “gonna” and “wanna” are labeled as unknown words, but we found that these words were labeled as verbs in a few cases."
"Not sur-prisingly this caused the ML algorithm to fail, since these class labels were misleading."
"The majority of the mistakes, however, seem to be due to systematic mistakes by the POS taggers."
One last remark is regarding our decision to find a method for successfully exploiting the existing tag-gers for POS tagging Spanglish text.
Our origi-nal motivation came from the lack of linguistic re-sources to process Spanglish text.
"However, we did train from scratch a sequential model for POS tag-ging Spanglish, namely Conditional Random Fields (CRFs)[REF_CITE]."
We used MALLET[REF_CITE]for this experiment and the same training/testing partitions used in the experiment re-ported in Table 3.
The CRF POS tagger was trained using capitalization information and the previous to-ken as context.
"The average accuracy of this[REF_CITE]%, which is lower than the language identifi-cation heuristic."
We believe that this low accuracy is due to the lack of a representative sample of anno-tated Spanglish.
It will be interesting to see if when more data becomes available the ML algorithms still yield the best results.
Code-switching is a fresh and exciting research area that has received little attention in the language pro-cessing community.
"Research on this topic has many interesting applications, including automatic speech recognition, machine translation, and computer as-sisted language learning."
"In this paper we present preliminary work towards developing a POS tagger for English-Spanish code-switched text that, to the best of our knowledge, is the first effort towards this end."
We explored different heuristics for taking advan-tage of existing linguistic resources for English and Spanish with unimpressive results.
A simple word-level language identification strategy outperformed all heuristics tested.
"But the best results, even bet-ter than the oracle, were achieved by using machine learning using the output of monolingual POS tag-gers as input features."
"In the error analysis we showed that most of the mistakes made by the language identification method, and the oracle itself, occur in sentences with intrasentential code-switching, showing the diffi-culty of the task."
"In contrast, our machine learning approach was less sensitive to the complexity of this alternation pattern."
There is still a lot of work to do in this area.
"Our ongoing efforts include gathering a larger corpus, with different speakers and conversational styles, as well as written forms of code-switching from blogs and Internet forums."
"In addition, we are exploring the use of context information."
The features we are currently using to represent each word do not take into account the context surrounding the word.
We want to test if by using contextual features we can further improve our results.
"In this study we focused on code-switching, but borrowing is another complex language alternation pattern that we want the POS tagger to handle."
We are working on developing a special method for identification and morphological analysis of borrow-ings.
This method will help increase the accuracy of the POS tagger.
Spanish-English is not the only popular combi-nation of languages.
An interesting line of future work would be to explore if the method presented here can be adapted to different language combi- nations.
"Moreover, multilingual communities will code-switch among more than two codes and this poses fascinating research challenges as well."
"This paper presents a novel, ranking-style word segmentation approach, called RSVM-Seg, which is well tailored to Chinese informa-tion retrieval(CIR)."
This strategy makes seg-mentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence.
"On the training corpus composed of query items, a ranking model is learned by a widely-used tool Ranking SVM, with some useful statistical features, such as mutual information, differ-ence of t-test, frequency and dictionary infor-mation."
"Experimental results show that, this method is able to eliminate overlapping am-biguity much more effectively, compared to the current word segmentation methods."
"Fur-thermore, as this strategy naturally generates segmentation results with different granular-ity, the performance of CIR systems is im-proved and achieves the state of the art."
"To improve information retrieval systems’ perfor-mance, it is important to comprehend both queries and corpus precisely."
"Unlike English and other western languages, Chinese does not delimit words by white-space."
Word segmentation is therefore a key preprocessor for Chinese information retrieval to comprehend sentences.
"Due to the characteristics of Chinese, two main problems remain unresolved in word segmentation: segmentation ambiguity and unknown words, which are also demonstrated to affect the performance of Chinese information retrieval[REF_CITE]."
Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity.
The first one refers to that ABC can be segmented into AB C or A BC.
"The second one refers to that string AB can be a word, or A can be a word and B can be a word."
"In CIR, the combinatory ambiguity is also called segmentation granularity problem[REF_CITE]."
There are many researches on the relationship between word segmentation and Chi-nese information retrieval[REF_CITE].
Their studies show that the segmentation accuracy does not monotonically influence subse-quent retrieval performance.
"Especially the overlap-ping ambiguity, as shown in experiments[REF_CITE], will cause more performance decrement of CIR."
"Thus a CIR system with a word segmenter bet-ter solving the overlapping ambiguity, may achieve better performance."
"Besides, it also showed that the precision of new word identification was more im-portant than the recall."
"There are some researches show that when com-pound words are split into smaller constituents, bet-ter retrieval results can be achieved[REF_CITE]."
"On the other hand, it is reasonable that the longer the word which co-exists in query and cor-pus, the more similarity they may have."
"A hypothe-sis, therefore, comes to our mind, that different seg-mentation granularity can be incorporated to obtain better CIR performance."
"In this paper we present a novel word segmenta-tion approach for CIR, which can not only obviously reduce the overlapping ambiguity, but also introduce different segmentation granularity for the first time."
"In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Rank-ing SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granu-larity by cutting adjacent character pairs according to this rank."
Other machine-learning based segmen-tation algorithms[REF_CITE]treat segmentation prob-lem as a character sequence tagging problem based on classification.
"However, these methods cannot di-rectly obtain different segmentation granularity."
Ex-periments show that our method can actually im-prove information retrieval performance.
This paper is structured as follows.
It starts with a brief introduction of the related work on the word segmentation approaches.
"Then in Section 3, we in-troduce our segmentation method."
Section 4 evalu-ates the method based on experimental results.
"Fi-nally, Section 5 makes summary of this whole paper and proposes the future research orientation."
Various methods have been proposed to address the word segmentation problem in previous studies.
"They fall into two main categories, rule-based ap-proaches that make use of linguistic knowledge and statistical approaches that train on corpus with ma-chine learning methods."
"In rule-based approaches, algorithms of string matching based on dictionary are the most commonly used, such as maximum matching."
They firstly segment sentences accord-ing to a dictionary and then resort to some rules to resolve ambiguities[REF_CITE].
"These rule-based methods are fast, how-ever, their performances depend on the dictionary which cannot include all words, and also on the rules which cost a lot of time to make and must be up-dated frequently."
Recent years statistical approaches became more popular.
These methods take advan-tage of various probability information gained from large corpus to segment sentences.
"Among them, Wang’s work[REF_CITE]is the most similar to our method, since both of us apply statistics infor-mation of each gap in the sentence to eliminate over-lapping ambiguity in methods."
"However, when com-bining different statistics, Wang decided the weight by a heuristic way which was too simply to be suit-able for all sentences."
"In our method, we employ a machine-learning method to train features’ weights."
"Many machine-learning methods, such as HMM[REF_CITE], CRF[REF_CITE], Maximum Entropy[REF_CITE], have been exploited in segmentation task."
"To our knowledge, machine-learning methods used in seg-mentation treated word segmentation as a character tagging problem."
"According to the model trained from training corpus and features extracted from the context in the sentence, these methods assign each character a positional tag, indicating its relative po-sition in the word."
These methods are difficult to get different granularity segmentation results directly.
Our method has two main differences with them.
"Firstly, we tag the gap between characters rather than characters themselves."
"Secondly, our method is based on ranking rather than classification."
"Then, we will present our ranking-based segmen-tation method, RSVM-Seg."
Traditional segmentation methods always take the segmentation problem as classification problem and give a definite segmentation result.
"In our approach, we try to solve word segmentation problem from the view of ranking."
"For easy understanding, let’s rep-resent a Chinese sentence S as a character sequence:"
C 1:n = C 1 C 2 . . .
We also explicitly show the gap G i (i = 1 . . . n − 1) between every two adjacent characters C i and C i+1 :
C 1:n |G 1:n−1 = C 1 G 1 C 2 G 2 . . .
G n−1 C n
"IAS i (i = 1...n) is corresponding to G i (i = 1 . . . n), reflecting the internal association strength between C i and C i+1 ."
"The higher the IAS value is, the stronger the associative between the two charac-ters is."
"If the association between two characters is weak, then they can be segmented."
"Otherwise, they should be unsegmented."
That is to say we could make segmentation based on the ranking of IAS value.
"In our ranking-style segmentation method, Ranking SVM is exploited to predict IAS ranking."
"In next subsections, we will introduce how to take advantage of Ranking SVM model to solve our problem."
"Then, we will describe features used for training the Ranking SVM model."
"Finally, we will give a scheme how to get segmentation result from predicted ranking result of Ranking SVM."
"Ranking SVM is a classical algorithm for ranking, which formalizes learning to rank as learning for classification on pairs of instances and tackles the classification issue by using SVM[REF_CITE]."
"Suppose that X²R d is the feature space, where d is the number of features, and Y = r 1 , r 2 , . . . , r K is the set of labels representing ranks."
"And there exists a total order between ranks r 1 &gt; r 2 &gt; ... &gt; r K , where &gt; denotes the order relationship."
"The actual task of learning is formalized as a Quadratic Pro-gramming problem as shown below: min ω,ξ ı 1kωk 2 +"
"CΣξ ı 2 (1) s.t.hω, x ı − x  i &gt; 1 − ξ ı , ∀x ı Â x  , ξ ı ≥ 0 where kωk denotes l 2 norm measuring the margin of the hyperplane and ξ ij denotes a slack variable. x i Â x j means the rank class of x i has an order prior to that of x j , i.e. Y (x i ) &gt; Y (x j )."
"Suppose that the solution to (1) is ω ∗ , then we can make the ranking function as f(x) = hω ∗ , xi."
"When applying Ranking SVM model to our prob-lems, an instance (feature vector x) is created from all bigrams (namely C i C i+1 ,i = 1...n − 1) of a sentence in the training corpus."
Each feature is defined as a function of bigrams (we will de-scribe features in detail in next subsection).
The instances from all sentences are then combined for training.
And Y refers to the class label of the IAS degree.
"As we mentioned above, segmenta-tion decision is based on IAS value."
"Therefore, the number of IAS degree’s class label is also cor-respondent to the number of segmentation class la-bel."
"In traditional segmentation algorithms, they al-ways label segmentation as two classes, segmented and unsegmented."
"However, for some phrases, it is a dilemma to make a segmentation decision based on this two-class scheme."
"For example, Chinese phrase ”笔记本电脑(Notepad)” can be segmented as ”笔记本(Note)” and ”电脑(computer)” or can be viewed as one word."
We cannot easily classify the gap between ”本” and ”脑” as segmented or un-segmented.
"Therefore, beside these two class la-bels, we define another class label, semisegmented, which means that the gap between two characters could be segmented or unsegmented, either will be right."
"Correspondingly, IAS degree is also divided into three classes, definitely inseparable (marked as 3), partially inseparable (marked as 2), and sepa-rable (marked as 1). ”Separable” corresponds to be segmented”; ”partially inseparable” corresponds to semisegmented; ”definitely inseparable” corre-sponds to be unsegmented."
"Obviously, there exists orders between these labels’ IAS values, namely IAS(1) &lt; IAS(2) &lt; IAS(3),IAS(∗) represents the IAS value of different labels."
"Next, we will describe the features used to train Ranking SVM model."
"Mutual Information: Mutual information, mea-suring the relationship between two variables, has been extensively used in computational language re-search."
"Given a Chinese character string ’xy’ (as mentioned above, in our method, ’xy’ refers to bi-gram in a sentence), mutual information between characters x and y is defined as follows: p(x, y) mi(x, y) = log 2 p(x)p(y) (2) where p(x, y) is the co-occurrence probability of x and y, namely the probability that bigram ’xy’ oc-curs in the training corpus, and p(x),p(y) are the independent probabilities of x and y respectively."
"From (2), we conclude that mi(x,y) À 0 means that IAS is strong; mi(x,y) ≈ 0 means that it is indefinite for IAS between characters x and y; mi(x,y) ¿ 0 means that there is no association been characters x and y."
"However, mutual infor-mation has no consideration of context, so it can-not solve the overlapping ambiguity effectively[REF_CITE]."
"To remedy this defect, we introduce another statistics measure, difference of t-test."
Difference of t-score (DTS): Difference of t-score is proposed on the basis of t-score.
"Given a Chinese character string ’xyz’, the t-score of the character y relevant to character x and z is defined as: t x,z (y) = p p(z|y) − p(y|x) (3) σ 2 (p(z|y)) + σ 2 (p(y|x)) where p(y|x) is the conditional probability of y given x, and p(z|y), of z given y, and σ 2 (p(y|x)), σ 2 (p(z|y)) are variances of p(y|x) and of p(z|y) re-spectively."
"Sun et al. gave the derivation formula of σ 2 (p(y|x)), σ 2 (p(z|y))[REF_CITE]as σ 2 (p(z|y)) ≈ r(y, z) σ 2 (p(y|x)) ≈ r(x, y) (4) r 2 (y) r 2 (x) where r(x, y), r(y, z), r(y), r(z) are the frequency of string xy, yz, y, and z respectively."
"Thus formula (3) is deducted as r(y,z) − rr((xx,y)) t x,z (y) = q r(y) (5) r(y,z) + rr( 2 x(,xy))r 2 (y) t x,z (y) indicates the binding tendency of y in the context of x and z: if t x,z (y) &gt; 0 then y tends to be bound with z rather than with x; if t x,z (y) &lt; 0, they y tends to be bound with x rather than with z."
"To measure the binding tendency between two ad-jacent characters ’xy’ (also, it refers to bigram in a sentence in our method), we use difference of t-score (DT S)[REF_CITE]which is defined as dts(x, y) = t v,y (x) − t x,w (y) (6)"
"Higher dts(x,y) indicates stronger IAS between adjacent characters x and y."
Dictionary Information: Both statistics mea-sures mentioned above cannot avoid sparse data problem.
Then Dictionary Information is used to compensate for the shortage of statistics informa-tion.
The dictionary we used includes 75784 terms.
We use binary value to denote the dictionary feature.
"If a bigram is in the dictionary or a part of dictionary term, we label it as ”1”, otherwise, we label is as ”0”."
Frequency: An important characteristic of new word is its repeatability.
"Thus, we also use fre-quency as another feature to train Ranking SVM model."
"Here, the frequency is referred to the number of times that a bigram occurs in the training corpus."
We give a training sentence for a better under-standing of features mentioned above.
"Algorithm 1 : Generate various granularity terms 1: Input: A Chinese sentence S = C 1 : C n IAS = IAS 1:n−1 LB = 1; RB = n 2: Iterative(S,IAS): 3: while length(S) ≥ 3 do 4: MB = F indMinIAS(IAS) 5: SL = C LB:MB 6: SR = C MB+1:RB 7: IAS L = IAS LB:MB 8: IAS R = IAS MB+1:RB 9: Iterative(SL, IAS L ) 10: Iterative(SR, IAS R ) 11: end while is ”中国建设银行网(China Construction Bank net-work)”"
"We extract all bigrams in this sentence, com-pute the four above features and give the IAS a la-bel for each bigram."
The feature vectors of all these bigrams for training are shown in Table 1.
"In order to compare with other segmentation meth-ods, which give a segmentation result based on two class labels, segmented and unsegmented, it is nec-essary to convert real numbers result given by Rank-ing SVM to these two labels."
"Here, we make a heuristic scheme to segment the sentence based on IAS ranking result predicted by Ranking SVM."
The scheme is described in Algorithm 1.
In each itera-tion we cut the sentence at the gap with minimum IAS value.
Nie et.al. pointed out that the average length of words in usage is 1.59[REF_CITE].
"Therefore, we stop the segmentation iterative when the length of sub sentence is 2 or less than 2."
"By this method, we could represent the segmentation re-sult as a binary tree."
Figure 1 shows an example of this tree.
"With this tree, we can obtain various gran-ularity segmentations easily, which could be used in CIR."
This segmentation scheme may cause some combinatory ambiguity.
"However, Nie et.al.[REF_CITE]also pointed out that there is no accu-rate word definition, thus whether combinatory am-biguity occurs is uncertain."
"What’s more, compared to overlapping ambiguity, combinatory ambiguity is not the fatal factor for information retrieval perfor-mance as mentioned in introduction."
"Therefore, this scheme is reasonable for Chinese information re- trieval."
"Since the label scheme and evaluation measure (de-scribed in next subsection) of our segmentation method are both different from the traditional seg-mentation methods, we did not carry out experi-ments on SIGHAN."
"Instead, we used two query logs (QueryLog[Footnote_1] and QueryLog2) as our experiment cor-pus, which are from two Chinese search engine com-panies. 900 queries randomly from QueryLog[Footnote_1] were chosen as training corpus. 110 Chinese queries from PKU Tianwang 1 , randomly selected 150 queries[REF_CITE]queries from QueryLog2 were used as test corpus."
1 Title field[REF_CITE]web retrieval TD task topics.[URL_CITE]
1 Title field[REF_CITE]web retrieval TD task topics.[URL_CITE]
The train and test cor-pus have been tagged by three people.
"They were given written information need statements, and were asked to judge the IAS of every two adjacent char-acters in a sentence on a three level scale as men-tioned above, separable, partially inseparable, and definitely inseparable."
"The assessors agreed in 84% of the sentences, the other sentences were checked by all assessors, and a more plausible alternative was selected."
We exploited SV M light[URL_CITE] as the toolkit to implement Ranking SVM model.
"Since our approach is based on the ranking of IAS values, it is inappropriate to evaluate our method by the traditional method used in other segmentation algorithms."
"Here, we proposed an evaluation mea-sure RankPrecision based on Kendall’s τ[REF_CITE], which compared the similarity between the predicted ranking of IAS values and the rankings of these tags as descending order."
RankPrecision formula is as follows:
"RankP recision = Σ ni=1 InverseCount(s i ) (7) 1 − Σ ni=1 CompInverseCount(s i ) where s i represents the ith sentence (unsegmented string), InverseCount(s i ) represents the number of discordant pairs inversions in the ranking of the predicted IAS value compared to the correct labeled ranking."
CompInverseCount(s i ) represents the number of discordant pairs inversions when the la-bels totally inverse.
Contributions of the Features: We investi-gated the contribution of each feature by gen-erating many versions of Ranking SVM model.
RankPrecision as described above was used for evaluations in these and following experiments.
"We used Mutual Information(MI); Difference of T-Score(DTS); Frequency(F); mutual informa-tion and difference of t-score(MI+DTS); mu- tual information, difference of t-score and Fre-quency(MI+DTS+F); mutual information, differ-ence of t-score and dictionary(MI+DTS+D); mutual information, difference of t-score, frequency and Dictionary(MI+DTS+F+D) as features respectively."
The results are shown in Table 2 and Figure 2.
"From the results, we can see that: • Using all described features together, the Rank-ing SVM achieved a good performance."
"And when we added MI, DTS, frequency, dictio-nary as features one by one, the RankPrecision improved step by step."
"It demonstrates that the features we selected are useful for segmenta-tion. 



 frequency, the RankPrecision was hurt on three test corpus, even worse than dts feature."
"The reason is supposed that some non-meaning but common strings, such as ”的人” would be took for a word with high IAS values."
"To correct this error, we could build a stop word list, and when we meet a character in this list, we treat them as a white-space."
Effects of corpus size:We trained different Rank-ing SVM models with different corpus size to in-vestigate the effects of training corpus size to our method performance.
The results are shown in Ta-ble 3 and Figure 3.
"From the results, we can see that the effect of corpus size to the performance of our approach is minors."
"Our segmentation approach can achieve good performance even with small training corpus, which indicates that Ranking SVM has gen-eralization ability."
"Therefore we can use a relative small corpus to train Ranking SVM, saving labeling effort."
"Effects on Finding Boundary: In algorithm 1, we could get different granularity segmentation words when we chose different length as stop condition."
Figure 4 shows the ”boundary precision” at each stop condition.
"Here, ”boundary precision” is defined as"
No.of right cut boundaries (8) No.of all cut boundaries
"From the result shown in figure 4, we can see that as the segmentation granularity gets smaller, the boundary precision gets lower."
"The reason is obvi-ous, that we may segment a whole word into smaller parts."
"However, as we analyzed in introduction, in CIR, we should judge words boundaries correctly to avoid overlapping ambiguity."
"As for combinatory ambiguity, through setting different stop length con-dition, we can obtain different granularity segmen-tation result."
"Effects on Overlapping Ambiguity: Due to the inconsistency of train and test corpus, it is difficult to keep fair for Chinese word segmentation evaluation."
Since ICTCLAS is considered as the best Chinese word segmentation systems.
We chose ICTCLAS as the comparison object.
"Moreover, we chose Maximum Match segmentation algorithm, which is rule-based segmentation method, as the baseline."
"We compared the number of overlapping ambigu-ity(NOA) among these three approaches on test cor-pus QueryLog1, QueryLog2 and TianWang."
The re-sult is shown in Table 4.
"On these three test cor-pus, the NOA of our approach is smallest, which indicates our method resolve overlapping ambiguity more effectively."
"For example, the sentence ”基础 课件(basic notes)”, the segmentation result of ICT-CLAS is ”基础课(basic class)/件(article)”, the word ”课件(notes)” is segmented, overlapping ambiguity occurring."
"However, with our method, the predicted IAS value rank of positions between every two ad-jacent characters in this sentence is ”基3础1课2件”, which indicates that the character ”课” has stronger internal associative strength with the character ”件” than with the character ”础”, eliminating overlap-ping ambiguity according to this ISA rank results."
Effects on Recognition Boundaries of new word:
"According to the rank result of all IAS values in a sentence, our method can recognize the bound-aries of new words precisely, avoiding the overlap-ping ambiguity caused by new words."
"For example, the phrase ”海南中招录取(Hainan High School’s Entry Recruitment)”, the ICTCLAS segmentation result is ”海南/中/招录/取”, because the new word ”中招” cannot be recognized accurately, thus the character ”招” is combined with its latter charac-ter ”录”, causing overlapping ambiguity."
"By our method, the segmentation result is shown as figure 5, in which no overlapping ambiguity occurs."
"Performance of Chinese Information Re-trieval: To evaluate the effectiveness of RSVM-Seg method on CIR, we compared it with the FMM seg-mentation."
"Our retrieval system combines differ-ent query representations obtained by our segmen-tation method, RSVM-Seg."
"In previous TREC Tere-byte Track, Markov Random Field(MRF)[REF_CITE]model has displayed better perfor-mance than other information retrieval models, and it can much more easily include dependence fea-tures."
"There are three variants of MRF model, full independence(FI), sequential dependence(SD), and full dependence(FD)."
"We chose SD as our retrieval model, since Chinese words are composed by char-acters and the adjacent characters have strong de-pendence relationship."
"We evaluated the CIR per-formance on the Chinese Web[REF_CITE]provided by Tianwang [URL_CITE] , which, as we know, is the largest publicly available Chinese web corpus till now."
"It consists of 37,482,913 web pages with total size of 197GB."
We used the topic set[REF_CITE]Topic Distillation (TD) task which contains 121 topics.
"MAP, R-Precision and GMAP[REF_CITE]were as main evaluation metrics."
"GMAP is the geometric mean of AP(Average Precision) through different queries, which was introduced to concentrate on dif-ficult queries."
The result is shown in 5.
"From the table, we can see that our segmentation method im-prove the CIR performance compared to FMM."
"From what we have discussed above, we can safely draw the conclusion that our work includes several main contributions."
"Firstly, to our best known, this is the first time to take the Chinese word segmenta-tion problem as ranking problem, which provides a new view for Chinese word segmentation."
This ap-proach has been proved to be able to eliminate over-lapping ambiguity and also be able to obtain various segmentation granularities.
"Furthermore, our seg-mentation method can improve Chinese information retrieval performance to some extent."
"As future work, we would search another more encouraging method to make a segmentation deci-sion from the ranking result."
"Moreover, we will try to relabel SIGHAN corpus on our three labels, and do experiments on them, which will be more con-venient to compare with other segmentation meth-ods."
"Besides, we will carry out more experiments to search the effectiveness of our segmentation method to CIR."
"Active learning is well-suited to many prob-lems in natural language processing, where unlabeled data may be abundant but annota-tion is slow and expensive."
This paper aims to shed light on the best active learning ap-proaches for sequence labeling tasks such as information extraction and document segmen-tation.
"We survey previously used query selec-tion strategies for sequence models, and pro-pose several novel algorithms to address their shortcomings."
"We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed meth-ods advance the state of the art."
Traditional supervised learning algorithms use whatever labeled data is provided to induce a model.
"By contrast, active learning gives the learner a de-gree of control by allowing it to select which in-stances are labeled and added to the training set."
"A typical active learner begins with a small labeled set L, selects one or more informative query instances from a large unlabeled pool U, learns from these la-beled queries (which are then added to L), and re-peats."
"In this way, the learner aims to achieve high accuracy with as little labeling effort as possible."
"Thus, active learning can be valuable in domains where unlabeled data are readily available, but ob-taining training labels is expensive."
Such is the case with many sequence labeling tasks in natural language domains.
"For example, part-of-speech tagging[REF_CITE], information extracti[REF_CITE], and document segmentati[REF_CITE]are all typically treated as sequence labeling problems."
"The source data for these tasks (i.e., text documents in electronic form) are often easily ob-tained."
"However, due to the nature of sequence la-beling tasks, annotating these texts can be rather te-dious and time-consuming, making active learning an attractive technique."
"While there has been much work on active learn-ing for classificati[REF_CITE], active learning for sequence labeling has received considerably less attention."
"A few meth-ods have been proposed, based mostly on the con-ventions of uncertainty sampling, where the learner queries the instance about which it has the least cer-tainty[REF_CITE], or query-by-committee, where a “committee” of models selects the instance about which its members most disagree[REF_CITE]."
We provide more detail on these and the new strategies we propose in Section 3.
"The comparative effectiveness of these ap-proaches, however, has not been studied."
"Further-more, it has been suggested that uncertainty sam-pling and query-by-committee fail on occasi[REF_CITE]by query-ing outliers, e.g., instances considered informative in isolation by the learner, but containing little infor-mation about the rest of the distribution of instances."
Proposed methods for dealing with these shortcom-ings have so far only considered classification tasks.
This paper presents two major contributions for active learning and sequence labeling tasks.
"First, we motivate and introduce several new query strate-gies for probabilistic sequence models."
"Second, we conduct a thorough empirical analysis of previously proposed methods with our algorithms on a variety of benchmark corpora."
The remainder of this pa-per is organized as follows.
Section 2 provides a brief introduction to sequence labeling and condi-tional random fields (the sequence model used in our experiments).
Section 3 describes in detail all the query selection strategies we consider.
Section 4 presents the results of our empirical study.
Section 5 concludes with a summary of our findings.
"In this paper, we are concerned with active learn-ing for sequence labeling."
"Figure 1 illustrates how, for example, an information extraction prob-lem can be viewed as a sequence labeling task."
"Let x = hx 1 , . . . , x T i be an observation sequence of length T with a corresponding label sequence y = hy 1 , . .. , y T i. Words in a sentence corre-spond to tokens in the input sequence x, which are mapped to labels in y."
"These labels indicate whether the word belongs to a particular entity class of inter-est (in this case, org and loc) or not (null)."
"These labels can be assigned by a sequence model based on a finite state machine, such as the one shown to the right in Figure 1."
"We focus our discussion of active learning for sequence labeling on conditional random fields, or CRFs[REF_CITE]."
The rest of this sec-tion serves as a brief introduction.
CRFs are sta-tistical graphical models which have demonstrated state-of-the-art accuracy on virtually all of the se-quence labeling tasks mentioned in Section 1.
"We use linear-chain CRFs, which correspond to condi-tionally trained probabilistic finite state machines."
A linear-chain CRF model with parameters θ de-fines the posterior probability of y given x to be 1 :
"T K ! 1 P(y|x; θ) = exp X X θ k f k (y t−1 , y t , x t ) ."
Z(x) t=[Footnote_1] k=1 (1)
"1 Our discussion assumes, without loss of generality, that each label is uniquely represented by one state, thus each label sequence y corresponds to exactly one path through the model."
"Here Z(x) is a normalization factor over all pos-sible labelings of x, and θ k is one of K model parameter weights corresponding to some feature f k (y t−1 , y t , x t )."
"Each feature f k describes the se-quence x at position t with label y t , observed along a transition from label states y t−1 to y t in the finite state machine."
Consider the example text from Fig-ure 1.
"Here, f k might be the feature W ORD =ACME and have the value f k = 1 along a transition from the null state to the org state (and 0 elsewhere)."
Other features set to 1 here might be A LL C APS and N EXT W ORD =Inc.
The weights in θ are set to max-imize the conditional log likelihood ` of training se-quences in the labeled data set L:
"L K θ 2 `(L; θ) = X log P (y (l) |x (l) ; θ) − X k , 2σ 2 l=[Footnote_1] k=1 where L is the size of the labeled set L, and the sec-ond term is a Gaussian regularization penalty on kθk to prevent over-fitting."
"1 Our discussion assumes, without loss of generality, that each label is uniquely represented by one state, thus each label sequence y corresponds to exactly one path through the model."
"After training, labels can be predicted for new sequences using the Viterbi algo-rithm."
"For more details on CRFs and their training procedures, see[REF_CITE]."
"Note that, while we describe the active learning algorithms in the next section in terms of linear-chain CRFs, they have analogs for other kinds of sequence models, such as hidden Markov models, or HMMs[REF_CITE], probabilistic context-free grammars[REF_CITE], and general CRFs[REF_CITE]."
"In order to select queries, an active learner must have a way of assessing how informative each instance is."
"Let x ∗ be the most informative instance according to some query strategy φ(x), which is a function used to evaluate each instance x in the unlabeled pool U."
"Given: Labeled set L, unlabeled pool U, query strategy φ(·), query batch size B repeat // learn a model using the current L θ = train(L) ; for b = 1 to B do // query the most informative instance x ∗b = arg max x∈U φ(x) ; // move the labeled query from U to L L = L ∪ hx ∗b , label(x ∗b )i ; U = U − x ∗b ; end until some stopping criterion ;"
Algorithm 1: Pool-based active learning.
Algorithm 1 provides a sketch of the generic pool-based active learning scenario.
"In the remainder of this section, we describe var-ious query strategy formulations of φ(·) that have been used for active learning with sequence mod-els."
"We also point out where we think these ap-proaches may be flawed, and propose several novel query strategies to address these issues."
"One of the most common general frameworks for measuring informativeness is uncertainty sampling[REF_CITE], where a learner queries the instance that it is most uncertain how to la-bel."
"Here, y ∗ is the most likely label sequence, i.e., the Viterbi parse."
This approach queries the instance for which the current model has the least confidence in its most likely labeling.
"For CRFs, this confi-dence can be calculated using the posterior proba-bility given by Equation (1)."
We call this approach margin (M): φ M (x) = − P (y 1∗ |x; θ) − P (y ∗2 |x; θ) .
"Here, y 1∗ and y ∗2 are the first and second best la-bel sequences, respectively."
"These can be efficiently computed using the N-best algorithm[REF_CITE], a beam-search generalization of Viterbi, with N = 2."
The minus sign in front is sim-ply to ensure that φ M acts as a maximizer for use with Algorithm 1.
Another uncertainty-based measure of informa-tiveness is entropy[REF_CITE].
"For a dis-crete random variable Y , the entropy is given by H(Y ) = − P i P (y i ) log P (y i ), and represents the information needed to “encode” the distribution of outcomes for Y ."
"As such, is it often thought of as a measure of uncertainty in machine learning."
"In active learning, we wish to use the entropy of our model’s posteriors over its labelings."
One way this has been done with probabilistic sequence models is by computing what we call token entropy (TE):
"T M φ TE (x) = − 1 X X P θ (y t = m) log P θ (y t = m), T t=1 m=1 (2) where T is the length of x, m ranges over all pos-sible token labels, and P θ (y t = m) is shorthand for the marginal probability that m is the label at position t in the sequence, according to the model."
"For CRFs and HMMs, these marginals can be effi-ciently computed using the forward and backward algorithms[REF_CITE]."
"The summed token en-tropies have typically been normalized by sequence length T , to avoid simply querying longer sequences[REF_CITE]."
"How-ever, we argue that querying long sequences should not be explicitly discouraged, if in fact they contain more information."
"Thus, we also propose the total token entropy (TTE) measure: φ TTE (x) ="
T × φ TE (x).
"For most sequence labeling tasks, however, it is more appropriate to consider the entropy of the la-bel sequence y as a whole, rather than some aggre-gate of individual token entropies."
"Thus an alternate query strategy is sequence entropy (SE): φ SE (x) = − X P (ŷ|x; θ) log P (ŷ|x; θ), (3) ŷ where ŷ ranges over all possible label sequences for input sequence x. Note, however, that the number of possible labelings grows exponentially with the length of x. To make this feasible, previous work[REF_CITE]has employed an approximation we call N-best sequence entropy (NSE): φ NSE (x) = − X P (ŷ|x; θ) log P (ŷ|x; θ), ŷ∈N where N = {y 1∗ , . . . , y N∗ }, the set of the N most likely parses, and the posteriors are re-normalized (i.e., Z(x) in Equation (1) only ranges over N )."
"For N = 2, this approximation is equivalent to φ M , thus N-best sequence entropy can be thought of as a gen-eralization of the margin approach."
"Recently, an efficient entropy calculation via dy-namic programming was proposed for CRFs in the context of semi-supervised learning[REF_CITE]."
We use this algorithm to compute the true sequence entropy (3) for active learning in a constant-time factor of Viterbi’s complexity.
Another general active learning framework is the query-by-committee (QBC) approach[REF_CITE].
"In this setting, we use a committee of models C = {θ (1) , . . . , θ (C) } to represent C different hy-potheses that are consistent with the labeled set L."
"The most informative query, then, is the instance over which the committee is in most disagreement about how to label."
"In particular, we use the query-by-bagging ap-proach[REF_CITE]to learn a com-mittee of CRFs."
"In each round of active learning, L is sampled (with replacement)"
"L times to create a unique, modified labeled set L (c) ."
Each model θ (c) ∈ C is then trained using its own corresponding labeled set L (c) .
"To measure disagreement among committee members, we consider two alternatives."
"T M φ VE (x) = − 1 X X V (y t , m) log V (y t , m), T C C t=1 m=1 where V (y t , m) is the number of “votes” label m re-ceives from all the committee member’s Viterbi la-belings at sequence position t."
The most informative query is considered to be the one with the largest average KL divergence between a committee member’s posterior label dis-tribution and the consensus.
"We modify this ap-proach for sequence models by summing the average KL scores using the marginals at each token position and, as with vote entropy, normalizing for length."
We call this approach Kullback-Leibler (KL):
"T C φ KL (x) = 1 X 1 XD(θ (c) kC), T C t=1 c=1 where (using shorthand again):"
M D(θ (c) kC) = X P θ (c) (y t = m) log P θ (c) (y t = m). m=1 P C (y t = m)
Here P C (y t = m) =
"C1 P Cc=1 P θ (c) (y t = m), or the “consensus” marginal probability that m is the label at position t in the sequence."
"Both of these disagreement measures are normal-ized for sequence length T. As with token en-tropy (2), this may bias the learner toward query-ing shorter sequences."
"To study the effects of nor-malization, we also conduct experiments with non-normalized variants φ TV E and φ TKL ."
"Additionally, we argue that these token-level dis-agreement measures may be less appropriate for most tasks than measuring the committee’s disagree-ment about the label sequence y as a whole."
"There-fore, we propose sequence vote entropy (SVE): φ SV E (x) = − X P (ŷ|x; C) log P (ŷ|x; C), ŷ∈N C where N C is the union of the N-best parses from all models in the committee C, and P(ŷ|x;C) ="
"C1 P C P(ŷ|x; θ (c) ), or the “consensus” posterior c=1 probability for some label sequence ŷ."
"This can be thought of as a QBC generalization of N-best en-tropy, where each committee member casts a vote for the posterior label distribution."
We also explore a sequence Kullback-Leibler (SKL) variant:
C φ SKL (x) = 1 X X P(ŷ|x;θ (c) )log P(ŷ|x;θ (c) ).
C P(ŷ|x; C) c=1 ŷ∈N C
A third general active learning framework we con-sider is to query the instance that would impart the greatest change to the current model if we knew its label.
"Since we train discriminative models like CRFs using gradient-based optimization, this in-volves querying the instance which, if labeled and added to the training set, would create the greatest change in the gradient of the objective function (i.e., the largest gradient vector used to re-estimate pa-rameter values)."
"Let ∇`(L; θ) be the gradient of the log-likelihood ` with respect to the model parameters θ, as given[REF_CITE]."
"Now let ∇`(L +hx,yi ;θ) be the new gradient that would be obtained by adding the training tuple hx,yi to L. Since the query algorithm does not know the true la-bel sequence y in advance, we instead calculate the expected gradient length (EGL): φ EGL (x) = X P (ŷ|x; θ) ∇`(L +hx,ŷi ; θ) , ŷ∈N approximated as an expectation over the N-best la-belings, where k · k is the Euclidean norm of each resulting gradient vector."
"We first introduced this ap-proach in previous work on multiple-instance active learning[REF_CITE], and adapt it to query selection with sequences here."
"Note that, at query time, ∇`(L;θ) should be nearly zero since ` con-verged at the previous round of training."
"Thus, we can approximate ∇`(L +hx,ŷi ;θ) ≈ ∇`(hx,ŷi;θ) for computational efficiency, because the training in-stances are assumed to be independent."
It has been suggested that uncertainty sampling and QBC are prone to querying outliers[REF_CITE].
Figure 2 illus-trates this problem for a binary linear classifier us-ing uncertainty sampling.
"The least certain instance lies on the classification boundary, but is not “rep-resentative” of other instances in the distribution, so knowing its label is unlikely to improve accuracy on the data as a whole."
"QBC and EGL exhibit similar behavior, by spending time querying possible out-liers simply because they are controversial, or are expected to impart significant change in the model."
We argue that this phenomenon can occur with se-quence labeling tasks as well as with classification.
"To address this, we propose a new active learning approach called information density (ID):"
U ! β φ
"ID (x) = φ SE (x) × 1 X sim(x, x (u) ) ."
"That is, the informativeness of x is weighted by its average similarity to all other sequences in U, sub-ject to a parameter β that controls the relative im-portance of the density term."
"In the formulation pre-sented above, sequence entropy φ SE measures the “base” informativeness, but we could just as easily use any of the instance-level strategies presented in the previous sections."
This density measure requires us to compute the similarity of two sequences.
"To do this, we first transform each x, which is a sequence of feature vectors (tokens), into a single kernel vector ~x: &quot; T # T ~x = X f 1 (x t ), . . . , X f J (x t ) , t=1 t=1 where f j (x t ) is the value of feature f j for token x t , and J is the number of features in the input represen-tation [Footnote_2] ."
"2 Note that J =6 K, and f j (x t ) here differs slightly from the feature definition given in Section 2. Since the labels y t−1 and y t are unknown before querying, the K features used for model training are reduced down to the J input features here, which factor out any label dependencies."
"In other words, sequence x is compressed into a fixed-length feature vector ~x, for which each element is the sum of the corresponding feature’s values across all tokens."
"We can then use cosine similarity on this simplified representation: ~x · ~x (u) sim cos (x, x (u) ) = . k~xk × k~x (u) k"
"We have also investigated similarity functions based on exponentiated Euclidean distance and KL-divergence, the latter of which was also employed[REF_CITE]for density-weighting QBC in text classification."
"However, these measures show no improvement over cosine similarity, and re-quire setting additional hyper-parameters."
"One potential drawback of information density is that the number of required similarity calculations grows quadratically with the number of instances in U. For pool-based active learning, we often as-sume that the size of U is very large."
"However, these densities only need to be computed once, and are independent of the base information measure."
"Thus, when employing information density in a real-world interactive learning setting, the density scores can simply be pre-computed and cached for efficient lookup during the actual active learning process."
"We also introduce a query selection strategy for se-quence models based on Fisher information, build-ing on the theoretical framework[REF_CITE]."
"Fisher information I(θ) represents the over-all uncertainty about the estimated model parame-ters θ, as given by:"
Z ∂ 2 Z I(θ) = − P (x) P (y|x; θ) x ∂θ 2 log P (y|x; θ). y
"For a model with K parameters, the Fisher infor-mation takes the form of a K × K covariance ma-trix."
Our goal in active learning is to select the query that most efficiently minimizes the model variance reflected in I(θ).
This can be accomplished by op-timizing the Fisher information ratio (FIR): φ FIR (x) = −tr
"I x (θ) −1 I U (θ) , (4) where I x (θ) and I U (θ) are Fisher information ma-trices for sequence x and the unlabeled pool U, re-spectively."
The leading minus sign again ensures that φ FIR is a maximizer for use with Algorithm 1.
"Previously, Fisher information for active learning has only been investigated in the context of simple binary classification."
"When employing FIR with se-quence models like CRFs, there are two additional computational challenges."
"First, we must integrate over all possible labelings y, which can, as we have seen, be approximated as an expectation over the N-best labelings."
"Second, the inner product in the ratio calculation (4) requires inverting a K × K matrix for each x."
"In most interesting natural language ap-plications, K is very large, making this algorithm intractable."
"However, it is common in similar situ-ations to approximate the Fisher information matrix with its diagonal[REF_CITE]."
"Thus we estimate I x (θ) using: ∂ log P(ŷ|x; θ) 2 &quot; I x (θ) = X P(ŷ|x; θ) + δ, . . . , ∂θ 1 ŷ∈N # ∂ log P(ŷ|x; θ) 2 + δ , ∂θ K and I U (θ) using:"
U I U (θ) = 1 XI x (u) (θ).
"For CRFs, the partial derivative at the root of each element in the diagonal vector is given by:"
"T ∂ log P(ŷ|x; θ) = X f k (ŷ t−1 , ŷ t , x t ) ∂θ k t=1"
"T − X X P(y, y 0 |x)f k (y, y 0 , x t ), t=1 y,y 0 which is similar to the equation used to compute the training gradient, but without a regularization term."
A smoothing parameter δ 1 is added to prevent division by zero when computing the ratio.
"Notice that this method implicitly selects repre-sentative instances by favoring queries with Fisher information I x (θ) that is not only high, but similar to that of the overall data distribution I U (θ)."
"This is in contrast to information density, which tries to query representative instances by explicitly model-ing the distribution with a density weight."
In this section we present a large-scale empirical analysis of the query strategies described in Sec-tion 3 on eight benchmark information extraction and document segmentation corpora.
The data sets are summarized in Table 1.
"CoNLL-03[REF_CITE]is a col-lection of newswire articles annotated with four en-tities: person, organization, location, and misc."
"NLPBA[REF_CITE]is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type."
"BioCreative[REF_CITE]and FlySlip[REF_CITE]also comprise texts in the biomedical domain, annotated for gene entity mentions in arti-cles from the human and fruit fly literature, respec-tively."
"CORA[REF_CITE]consists of two collections: a set of research paper headers annotated for entities such as title, author, and insti-tution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher."
The Sig+Reply corpus[REF_CITE]is a set of email messages annotated for signature and quoted reply line segments.
"SigIE is a subset of the signature blocks from Sig+Reply which we have enhanced with several address book fields such as name, email, and phone."
All corpora are format-ted in the “IOB” sequence representati[REF_CITE].
"We implement all fifteen query selection strate-gies described in Section 3 for use with CRFs, and evaluate them on all eight data sets."
"We also com-pare against two baseline strategies: random in-stance selection (i.e., passive learning), and naı̈vely querying the longest sequence in terms of tokens."
"We use a typical feature set for each corpus based on the cited literature (including words, orthographic patterns, part-of-speech, lexicons, etc.)."
"Where the N-best approximation is used N = 15, and for all QBC methods C = 3; these figures exhibited a good balance of accuracy and training speed in prelimi-nary work."
"For information density, we arbitrarily set β = 1 (i.e., the information and density terms have equal weight)."
"In each experiment, L is ini-tialized with five random labeled instances, and up to 150 queries are subsequently selected from U in batches of size B = 5."
All results are averaged across five folds using cross-validation.
We evaluate each query strategy by constructing learning curves that plot the overall F 1 measure (for all entities or segments) as a function of the num-ber of instances queried.
"Due to lack of space, we cannot show learning curves for every experiment."
"Instead, Table 2 summarizes our results by reporting the area under the learning curve for all strategies on all data."
Figure 3 presents a few representative learning curves for six of the corpora.
The first conclusion we can draw from these results is that there is no single clear winner.
"However, in-formation density (ID), which we introduce in this paper, stands out."
"It usually improves upon the base sequence entropy measure, never performs poorly, and has the highest average area under the learning curve across all tasks."
"It seems particularly effective on large corpora, which is a typical assumption for the active learning setting."
"Sequence vote entropy (SVE), a QBC method we propose here, is also note-worthy in that it is fairly consistently among the top three strategies, although never the best."
"Second, the top uncertainty sampling strategies are least confidence (LC) and sequence entropy (SE), the latter being the dominant entropy-based method."
"Among the QBC strategies, sequence vote entropy (SVE) is the clear winner."
We conclude that these three methods are the best base information measures for use with information density.
"Third, query strategies that evaluate the en-tire sequence (SE, SVE, SKL) are generally su-perior to those which aggregate token-level infor-mation."
"Furthermore, the total token-level strate-gies (TTE, TVE, TKL) outperform their length- normalized counterparts (TE, VE, KL) in nearly all cases."
"In fact, the normalized variants are often in-ferior even to the baselines."
"While an argument can be made that these shorter sequences might be eas-ier to label from a human annotator’s perspective, our ongoing work indicates that the relationship be-tween instance length and actual labeling costs (e.g., elapsed annotation time) is not a simple one."
"Anal-ysis of our experiment logs also shows that length-normalized methods are occasionally biased toward short sequences with little intuitive value (e.g., sen-tences with few or no entities to label)."
"In addition, vote entropy appears to be a better disagreement measure for QBC strategies than KL divergence."
"Finally, Fisher information (FIR), while theoreti-cally sound, exhibits behavior that is difficult to in-terpret."
"It is sometimes the winning strategy, but oc-casionally only on par with the baselines."
"When it does show significant gains over the other strategies, these gains appear to be only for the first several queries (e.g., NLPBA and BioCreative in Figure 3)."
This inconsistent performance may be a result of the approximations made for computational efficiency.
"Expected gradient length (EGL) also appears to ex-hibit mediocre performance, and is likely not worth its additional computational expense."
Here we discuss the execution times for each query strategy using current hardware.
"The uncertainty sampling methods are roughly comparable in run time (token-based methods run slightly faster), each routinely evaluating tens of thousands of sequences in under a minute."
"The QBC methods, on the other hand, must re-train multiple models with each query, resulting in a lag of three to four minutes per query batch (and up to 20 minutes for corpora with more entity labels)."
"The expected gradient length and Fisher informa-tion methods are the most computationally expen-sive, because they must first perform inference over the possible labelings and then calculate gradients for each candidate label sequence."
"As a result, they take eight to ten minutes (upwards of a half hour on the larger corpora) for each query."
"Unlike the other strategies, their time complexities also scale linearly with the number of model parameters K which, in turn, increases as new sequences are added to L."
"As noted in Section 3.4, information density in-curs a large computational cost to estimate the den-sity weights, but these can be pre-computed and cached for efficient lookup."
"In our experiments, this pre-processing step takes less than a minute for the smaller corpora, about a half hour for CoNLL-03 and BioCreative, and under two hours for NLPBA."
The density lookup causes no significant change in the run time of the base information measure.
"Given these results, we advocate information density with an uncertainty sampling base measure in practice, particularly for active learning with large corpora."
"In this paper, we have presented a detailed analy-sis of active learning for sequence labeling tasks."
"In particular, we have described and criticized the query selection strategies used with probabilistic se- quence models to date, and proposed several novel strategies to address some of their shortcomings."
Our large-scale empirical evaluation demonstrates that some of these newly proposed methods advance the state of the art in active learning with sequence models.
"These methods include information density (which we recommend in practice), sequence vote entropy, and sometimes Fisher information."
String-to-string transduction is a central prob-lem in computational linguistics and natural language processing.
"It occurs in tasks as di-verse as name transliteration, spelling correc-tion, pronunciation modeling and inflectional morphology."
"We present a conditional log-linear model for string-to-string transduction, which employs overlapping features over la-tent alignment sequences, and which learns la-tent classes and latent string pair regions from incomplete training data."
"We evaluate our ap-proach on morphological tasks and demon-strate that latent variables can dramatically improve results, even when trained on small data sets."
"On the task of generating mor-phological forms, we outperform a baseline method reducing the error rate by up to 48%."
"On a lemmatization task, we reduce the error rates[REF_CITE]by 38–92%."
"A recurring problem in computational linguistics and language processing is transduction of charac-ter strings, e.g., words."
"That is, one wishes to model some systematic mapping from an input string x to an output string y. Applications include: • phonology: underlying representation ↔ surface representation • orthography: pronunciation ↔ spelling • morphology: inflected form ↔ lemma, or differ-ently inflected form • fuzzy name matching (duplicate detection) and spelling correction: spelling ↔ variant spelling • lexical translation (cognates, loanwords, translit-erated names): English word ↔ foreign word"
We present a configurable and robust framework for solving such word transduction problems.
Our results in morphology generation show that the pre-sented approach improves upon the state of the art.
A weighted edit distance model[REF_CITE]would consider each character in isola-tion.
"To consider more context, we pursue a very natural generalization."
"Given an input x, we evalu-ate a candidate output y by moving a sliding window over the aligned (x,y) pair."
"More precisely, since many alignments are possible, we sum over all these possibilities, evaluating each alignment separately. [Footnote_1]"
"1 At the other extreme,[REF_CITE]use no alignment; each feature takes its own view of how (x, y) relate."
"At each window position, we accumulate log-probability based on the material that appears within the current window."
"The window is a few charac-ters wide, and successive window positions over-lap."
"This stands in contrast to a competing approach[REF_CITE]that is inspired by phrase-based machine translati[REF_CITE], which segments the input string into substrings that are transduced independently, ig-noring context. [Footnote_2]"
"2 We feel that this independence is inappropriate. By anal-ogy, it would be a poor idea for a language model to score a string highly if it could be segmented into independently fre-quent n-grams. Rather, language models use overlapping n-grams (indeed, it is the language model that rescues phrase-based MT from producing disjointed translations). We believe phrase-based MT avoids overlapping phrases in the channel model only because these would complicate the modeling of reordering (though see, e.g.,[REF_CITE]and[REF_CITE]). But in the problems of section 1, letter reordering is rare and we may assume it is local to a window."
"Figure 1: One of many possible alignment strings A for the observed pair breaking/broke, enriched with latent strings ` 1 and ` 2 ."
Observed letters are shown in bold.
The box marks a trigram to be scored.
See Fig. 2 for features that fire on this trigram.
"Joint n-gram models over the input and output di-mensions have been used before, but not for mor-phology, where we will apply them. [Footnote_3] Most notable is the local log-linear grapheme-to-phoneme model[REF_CITE], as well as generative models for that task ([REF_CITE],[REF_CITE],[REF_CITE])."
3[REF_CITE]does use pair HMMs for morphology.
"We advance that approach by adding new latent dimensions to the (input, output) tuples (see Fig. 1). [Footnote_4]"
"4[REF_CITE]similarly added extra dimensions. However, their added dimensions were supervised, not latent, and their model was a standard generative n-gram model whose generalization was limited to standard n-gram smoothing."
This enables us to use certain linguistically inspired features and discover unannotated information.
Our features consider less or more than a literal n-gram.
"On the one hand, we generalize with features that abstract away from the n-gram window contents; on the other, we specialize the n-gram with features that make use of the added latent linguistic structure."
"In section 5, we briefly sketch our framework for concisely expressing and efficiently implementing models of this form."
"Our framework uses familiar log-linear techniques for stochastic modeling, and weighted finite-state methods both for implementa-tion and for specifying features."
It appears general enough to cover most prior work on word transduc-tion.
"We imagine that it will be useful for future work as well: one might easily add new, linguisti-cally interesting classes of features, each class de-fined by a regular expression."
We use an input alphabet Σ x and output alphabet Σ y .
We conventionally use x ∈ Σ ∗x to denote the input string and y ∈
Σ ∗y to denote the output string.
"There are many possible alignments between x and y. We represent each as an alignment string A ∈ Σ ∗ , over an alignment alphabet of ordered xy def pairs, Σ xy = ((Σ x ∪ { }) × (Σ y ∪ { })) − {( , )}."
"For example, one alignment of x = breaking with y = broke is the 9-character string A = (b,b)(r,r)(e,o)(a, )(k,k)( ,e)(i, )(n, )(g, )."
It is pictured in the first two lines of Fig. 1.
"The remainder of Fig. 1 shows how we intro-duce latent variables, by enriching the alignment def characters to be tuples rather than pairs."
"Let Σ = (Σ xy × Σ ` 1 × Σ ` 2 × · · · × Σ ` K ), where Σ ` i are al-phabets used for the latent variables ` i ."
"FSA and FST stand for “finite-state acceptor” and “finite-state transducer,” while WFSA and WFST are their weighted variants."
The ◦ symbol denotes composition.
Let T be a relation and w a string.
"We write T [w] to denote the image of w under T (i.e., range(w ◦ T)), a set of 0 or more strings."
"Similarly, if W is a weighted language (typically encoded by a WFSA), we write W [w] to denote the weight of w in W ."
"Let π x ⊆ Σ ∗ × Σ ∗x denote the deterministic reg-ular relation that projects an alignment string to its corresponding input string, so that π x [A] = x. Sim-ilarly, define π y ⊆ Σ ∗ × Σ ∗y so that π y [A] = y. Let A xy be the set of alignment strings"
"A compatible with x and y; formally, A xy = {A ∈ Σ ∗ : π x [A] = def x ∧ π y [A] = y}."
"This set will range over all possible alignments between x and y, and also all possible configurations of the latent variables."
"We use a standard log-linear model whose features are defined on alignment strings A ∈ A xy , allow-ing them to be sensitive to the alignment of x and y. Given a collection of features f i :"
"Σ ∗ → R with as-sociated weights θ i ∈ R, the conditional likelihood of the training data is"
P A∈A xy exp P i θ i f i (A) p θ (y | x) =
P P (1) y 0 A∈A xy 0 exp P i θ i f i (A)
"Given a parameter vector θ, we compute equa-tion (1) using a finite-state machine."
"We define a WFSA, U θ , such that U θ [A] yields the unnormalized def probability u θ (A) = expP i θ i f i (A) for any A ∈ Σ ∗ . (See section 5 for the construction.)"
"To obtain the numerator of equation (1), with its P A∈A xy , we sum over all paths in U θ that are compatible with x and y. That is, we build x ◦ π x−1 ◦"
U θ ◦ π y ◦ y and sum over all paths.
For the denominator we build the larger machine x ◦ π x−1 ◦
U θ and again compute the pathsum.
We use standard algorithms[REF_CITE]to compute the pathsums as well as their gradients with respect to θ for optimization (section 4.1).
"Below, we will restrict our notion of valid align-ment strings in Σ ∗ ."
"U θ is constructed not to accept invalid ones, thus assigning them probability 0."
"Note that the possible output strings y 0 in the de-nominator in equation (1) may have arbitrary length, leading to an infinite summation over alignment strings."
"Thus, for some values of θ, the sum in the denominator diverges and the probability dis-tribution is undefined."
There exist principled ways to avoid such θ during training.
"However, in our current work, we simply restrict to finitely many alignment strings (given x), by prohibiting as invalid those with &gt; k consecutive insertions (i.e., charac-ters like ( , a)). [Footnote_5][REF_CITE]and others have similarly bounded unary rule cycles in PCFGs."
"5 We set k to a value between 1 and 3, depending on the tasks, always ensuring that no input/output pairs observed in training are excluded. The insertion restriction does slightly enlarge the FSA U θ : a state must keep track of the number of consecutive symbols in the immediately preceding x input, and for a few states, this cannot be determined just from the immediately pre-ceding (n − 1)-gram. Despite this, we found empirically that our approximation is at least as fast as the exact method[REF_CITE], who sums around cyclic subnetworks to numerical convergence. Furthermore, our approximation does not require us to detect divergence during training."
The alignment between x and y is a latent ex-planatory variable that helps model the distribution p(y | x) but is not observed in training.
Other latent variables can also be useful.
Morphophonological changes are often sensitive to phonemes (whereas x and y may consist of graphemes); syllable bound-aries; a conjugation class; morpheme boundaries; and the position of the change within the form.
"Thus, as mentioned in section 2.1, we enrich the alignment string A so that it specifies additional la-tent variables to which features may wish to refer."
"In Fig. 1, two latent strings are added, enabling the features in Fig. 2(a)–(h)."
"The first character is not just an input/output pair, but the 4-tuple (b, b, 2, 1)."
"Here, ` 1 indicates that this form pair (breaking / broke) as a whole is in a particular cluster, or word class, labeled with the arbitrary number 2."
Notice in Fig. 1 that the class 2 is visible in all local windows throughout the string.
"It allows us to model how cer-tain phenomena, e.g. the vowel change from ea to o, are more likely in one class than in another."
"Form pairs in the same class as the breaking / broke ex-ample might include the following Germanic verbs: speak, break, steal, tear, and bear."
"Of course, word classes are latent (not labeled in our training data)."
"Given x and y, A xy will include alignment strings that specify class 1, and others that are identical except that they specify class 2; equation (1) sums over both possibilities. [Footnote_6]"
6 The latent class is comparable to the latent variable on the tree root symbol S[REF_CITE].
"In a valid alignment string A, ` 1 must be a constant string such as 111... or 222..., as in Fig. 1, so that it spec-ifies a single class for the entire form pair."
See sec-tions 4.2 and 4.3 for examples of what classes were learned in our experiments.
The latent string ` 2 splits the string pair into num-bered regions.
"In a valid alignment string, the re-gion numbers must increase throughout ` 2 , although numbers may be skipped to permit omitted regions."
"To guide the model to make a useful division into regions, we also require that identity characters such as (b, b) fall in even regions while change charac-ters such as (e, o) (substitutions, deletions, or inser- tions) fall in odd regions. 7 Region numbers must not increase within a sequence of consecutive changes or consecutive identities. 8"
"In Fig. 1, the start of re-gion 1 is triggered by e:o, the start of region 2 by the identity k:k, region 3 by :e."
Allowing region numbers to be skipped makes it possible to consistently assign similar labels to sim-ilar regions across different training examples.
"Ta-ble 2, for example, shows pairs that contain a vowel change in the middle, some of which contain an ad-ditional insertion of ge in the begining (verbinden / verbunden, reibt / gerieben)."
"We expect the model to learn to label the ge insertion with a 1 and vowel change with a 3, skipping region 1 in the examples where the ge insertion is not present (see section 4.2, Analysis)."
In the next section we describe features over these enriched alignment strings.
One of the simplest ways of scoring a string is an n-gram model.
"In our log-linear model (1), we include ngram features f i (A), each of which counts the oc-currences in A of a particular n-gram of alignment characters."
"The log-linear framework lets us include ngram features of different lengths, a form of back-off smoothing[REF_CITE]."
"We use additional backoff features on alignment strings to capture phonological, morphological, and orthographic generalizations."
Examples are found in features (b)-(h) in Fig. 2. Feature (b) matches vowel and consonant character classes in the input and output dimensions.
"In the id/subst ngram feature, we have a similar abstraction, where the character classes ins, del, id, and subst are defined over in-put/output pairs, to match insertions, deletions, iden-tities (matches), and substitutions."
"In string transduction tasks, it is helpful to in-clude a language model of the target."
"While this can be done by mixing the transduction model with a separate language model, it is desirable to in-clude a target language model within the transduc- tion model."
"We accomplish this by creating target language model features, such as (c) and (g) from Fig. 2, which ignore the input dimension."
We also have features which mirror features (a)-(d) but ig-nore the latent classes and/or regions (e.g. features (e)-(h)).
"Notice that our choice of Σ only permits mono-tonic, 1-to-1 alignments, following[REF_CITE]."
"We may nonetheless favor the 2-to-1 alignment (ea,o) with bigram features such as (e,o)(a, )."
"A “collapsed” version of a feature will back off from the specific alignment of the characters within a win-dow: thus, (ea,o) is itself a feature."
"Currently, we only include collapsed target language model fea-tures."
"These ignore epsilons introduced by deletions in the alignment, so that collapsed ok fires in a win-dow that contains ."
We evaluate our model on two tasks of morphol-ogy generation.
Predicting morphological forms has been shown to be useful for machine translation and other tasks. [Footnote_9]
"9 E.g.,[REF_CITE]improve MT performance by selecting correct morphological forms from a knowledge source. We instead focus on generalizing from observed forms and generating new forms (but see with rootlist in Table 3)."
"Here we describe two sets of exper-iments: an inflectional morphology task in which models are trained to transduce verbs from one form into another (section 4.2), and a lemmatization task (section 4.3), in which any inflected verb is to be re-duced to its root form."
We train θ to maximize the regularized [Footnote_10] conditional log-likelihood [Footnote_11]
10 The variance σ 2 of the L 2 prior is chosen by optimizing on development data. We are also interested in trying an L 1 prior.
"11 Alternatives would include faster error-driven methods (perceptron, MIRA) and slower max-margin Markov networks."
"X log p θ (y ∗ | x) + ||θ|| 2 /2σ 2 , (2) (x,y ∗ )∈C where C is a supervised training corpus."
"To max-imize (2) during training, we apply the gradient-based optimization method L-BFGS[REF_CITE]. [Footnote_12]"
12 This worked a bit better than stochastic gradient descent.
"To decode a test example x, we wish to find ŷ = argmax y∈Σ ∗y p θ ( y | x)."
"Constructively, ŷ is the highest-probability string in the WFSA T [x], where T = π x−1 ◦U θ ◦π y is the trained transducer that maps x nondeterministically to y. Alas, it is NP-hard to find the highest-probability string in a WFSA, even an acyclic one[REF_CITE]."
The problem is that the probability of each string y is a sum over many paths in T [x] that reflect differ-ent alignments of y to x.
"Although it is straightfor-ward to use a determinization constructi[REF_CITE][Footnote_13] to collapse these down to a single path per y (so that ŷ is easily read off the single best path), determinization can increase the WFSA’s size expo-nentially."
"13 Weighted determinization is not always possible, but it is in our case because our limit to k consecutive insertions guar-antees that T[x] is acyclic."
We approximate by pruning T[x] back to its 1000-best paths before we determinize. [Footnote_14]
"14 This value is high enough; we see no degradations in per-formance if we use only 100 or even 10 best paths. Below that, performance starts to drop slightly. In both of our tasks, our conditional distributions are usually peaked: the 5 best output candidates amass &gt; 99% of the probability mass on average. Entropy is reduced by latent classes and/or regions."
"Since the alignments, classes and regions are not observed in C, we do not enjoy the convex objec-tive function of fully-supervised log-linear models."
Training equation (2) therefore converges only to some local maximum that depends on the starting point in parameter space.
"To find a good starting point we employ staged training, a technique in which several models of ascending complexity are trained consecutively."
The parameters of each more complex model are initialized with the trained pa-rameters of the previous simpler model.
Our training is done in four stages.
"All weights are initialized to zero. ¬ We first train only fea-tures that fire on unigrams of alignment charac-ters, ignoring features that examine the latent strings or backed-off versions of the alignment characters (such as vowel/consonant or target language model features)."
"The resulting model is equivalent to weighted edit distance[REF_CITE]. ­[REF_CITE]we train all n-grams of alignment charac-ters, including higher-order n-grams, but no backed-off features or features that refer to latent strings. ® Next, we add backed-off features as well as all collapsed features. ¯"
"Finally, we train all features."
"In our experiments, we permitted latent classes 1– 2 and, where regions are used, regions 0–6."
"For speed, stages ­–¯ used a pruned Σ that included only “plausible” alignment characters: a may not align to b unless it did so in the trained stage-(1) model’s optimal alignment of at least one training pair (x, y ∗ )."
We conducted several experiments on the CELEX morphological database.
"We arbitrarily consid-ered mapping the following German verb forms: [Footnote_16] 13SIA → 13SKE, 2PIE → 13PKE, 2PKE → z, and rP → pA. [Footnote_17] We refer to these tasks as 13SIA, 2PIE, 2PKE and rP. Table 2 shows some examples of regular and irregular forms."
"16 From the available languages in CELEX (German, Dutch, and English), we selected German as the language with the most interesting morphological phenomena, leaving the mul-tilingual comparison for the lemmatization task (section 4.3), where there were previous results to compare with. The 4 Ger-man datasets were picked arbitrarily."
17 A key to these names: 13SIA=1st/3rd sg. ind. past; 13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.; 13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd pl. subjunct. pres.; z=infinitive; rP=imperative pl.; pA=past part.
"Common phenomena include stem changes (ei:ie), prefixes inserted af-ter other morphemes (abzubrechen) and circumfixes (gerieben)."
We compile lists of form pairs from CELEX.
"For each task, we sample 2500 data pairs without re-placement, of which 500 are used for training, 1000 as development and the remaining 1000 as test data."
We train and evaluate models on this data and repeat the process 5 times.
All results are averaged over these 5 runs.
"Table 1 and Fig. 3 report separate results after stages ­, ®, and ¯ of training, which include suc-cessively larger feature sets."
"These are respectively labeled ngrams, ngrams+x, and ngrams+x+latent."
"In Table 1, the last row in each section shows the full feature set at that stage (cf. Fig. 3), while earlier rows test feature subsets. [Footnote_18]"
18 The number k of consecutive insertions was set to 3.
Our baseline is the SMT toolkit Moses[REF_CITE]run over letter strings rather than word strings.
It is trained (on the same data splits) to find substring-to-substring phrase pairs and translate from one form into another (with phrase reordering turned off).
"Results reported as moses3 are obtained from Moses runs that are constrained to the same context windows that our models use, so the maxi-mum phrase length and the order of the target lan-guage model were set to 3."
"We also report results using much larger windows, moses9 and moses15."
The results in Table 1 show that including latent classes and/or regions improves the results dramatically.
Compare the last line in ngrams+x to the last line in ngrams+x+latent.
"The accuracy numbers improve from 82.8 to 87.5 (13SIA), from 88.7 to 93.4 (2PIE), from 74.7 to 87.4 (2PKE), and from 69.9 to 84.9 (rP). [Footnote_19]"
19 All claims in the text are statistically significant under a paired permutation test (p &lt; .05).
This shows that error re-ductions between 27% and 50% were reached.
"On 3 of 4 tasks, even our simplest ngrams method beats the moses3 method that looks at the same amount of context. [Footnote_20] With our full model, in particular using latent features, we always outperform moses3—and even outperform moses15 on 3 of the 4 datasets, re-ducing the error rate by up to 48.3% (rP)."
20 This bears out our contention in footnote 2 that a “segment-ing” channel model is damaging. Moses cannot fully recover by using overlapping windows in the language model.
"On the fourth task (2PIE), our method and moses15 are sta-tistically tied."
"While the gains from backoff features in Table 1 were modest (significant gains only on 13SIA), the learning curve in Fig. 3 suggests that they were help-ful for smaller training sets on 2PKE (see ngrams vs ngrams+x on 50 and 100) and helped consistently over different amounts of training data for 13SIA."
The types of errors that our system (and the moses baseline) make differ from task to task.
"Due to lack of space, we mainly focus on the com-plex rP task."
"Here, most errors come from wrongly copying the input to the output, without making a change (40-50% of the errors in all models, except for our model with latent classes and no regions, where it accounts for only 30% of the errors)."
This is so common because about half of the training ex-amples contain identical inputs and outputs (as in the imperative berechnet and the participle (ihr habt) berechnet).
Another common error is to wrongly as-sume a regular conjugation (just insert the prefix ge-at the beginning).
"Interestingly, this error by sim-plification is more common in the Moses models (44% of moses3 errors, down to 40% for moses15) than in our models, where it accounts for 37% of the errors of our ngrams model and only 19% if la-tent classes or latent regions are used; however, it goes up to 27% if both latent classes and regions are used. [Footnote_21]"
21 We suspect that training of the models that use classes and regions together was hurt by the increased non-convexity; an-nealing or better initialization might help.
"All models for rP contain errors where wrong analogies to observed words are made (ver-schweisst/verschwissen in analogy to the observed durchweicht/durchwichen, or bebt/geboben in anal-ogy to hebt/gehoben)."
"In the 2PKE task, most errors result from inserting the zu morpheme at a wrong place or inserting two of them, which is always wrong."
"This error type was greatly reduced by la-tent regions, which can discover different parame-ters for different positions, making it easier to iden-tify where to insert the zu."
Analysis of the 2 latent classes (when used) shows that a split into regular and irregular conjugations has been learned.
"For the rP task we compute, for each data pair in development data, the poste-rior probabilities of membership in one or the other class. 98% of the regular forms, in which the past participle is built with ge- . . . -t, fall into one class, which in turn consists nearly exclusively (96%) of these forms."
Different irregular forms are lumped into the other class.
The learned regions are consistent across different pairs.
"On development data for the rP task, 94.3% of all regions that are labeled 1 are the insertion se-quence ( ,ge), region 3 consists of vowel changes 93.7% of the time; region 5 represents the typical suffixes (t, en), (et, en), (t, n) (92.7%)."
"In the 2PKE task, region 0 contains different prefixes (e.g. entgegen in entgegenzutreten), regions 1 and 2 are empty, region 3 contains the zu affix, region 4 the stem, and region 5 contains the suffix."
The pruned alignment alphabet excluded a few gold standard outputs so that the model contains paths for 98.9%–99.9% of the test examples.
We verified that the insertion limit did not hurt oracle accuracy.
"We apply our models to the task of lemmatization, where the goal is to generate the lemma given an in-flected word form."
"We compare our model to Wicen-towski (2002, chapter 3), an alternative supervised approach."
"Wicentowski’s Base model simply learns how to replace an arbitrarily long suffix string of an input word, choosing some previously observed suf-fix → suffix replacement based on the input word’s final n characters (interpolating across different val-ues of n)."
His Affix model essentially applies the Base model after stripping canonical prefixes and suffixes (given by a user-supplied list) from the input and output.
"Finally, his WFAffix uses similar meth-ods to also learn substring replacements for a stem vowel cluster and other linguistically significant re-gions in the form (identified by a deterministic align-ment and segmentation of training pairs)."
This ap-proach is a bit like our change regions combined with Moses’s region-independent phrase pairs.
We compare against all three models.
"Note that Affix and WFAffix have an advantage that our mod-els do not, namely, user-supplied lists of canonical affixes for each language."
It is interesting to see how our models with their more non-committal tri-gram structure compare to this.
"Table 3 reports re-sults on the data sets used[REF_CITE], for Basque, English, Irish, and Tagalog."
Follow-ing[REF_CITE]-fold cross-validation was used.
"The columns n+x and n+x+l mean ngram+x and ngram+x+latent, respectively."
"As latent variables, we include 2 word classes but no change regions. [Footnote_22]"
22 The insertion limit k was set to 2 for Basque and 1 for the other languages.
"For completeness, Table 3 also compares[REF_CITE]on a selection (rather than genera-tion) task."
"Here, at test time, the lemma is selected from a candidate list of known lemmas, namely, all the output forms that appeared in training data. [Footnote_23] These additional results are labeled with rootlist in the right half of Table 3."
"23 Though test data contained no (input, output) pairs from training data, it reused many of the output forms, since many inflected inputs are to be mapped to the same output lemma."
"On the supervised generation task without rootlist, our models outperform[REF_CITE]by a large margin."
Comparing our results that use la-tent classes (n+x+l) with Wicentowski’s best mod-els we observe error reductions ranging from about 38% (Tagalog) to 92% (Irish).
"On the selection task with rootlist, we outperform[REF_CITE]in English, Irish, and Tagalog."
We examined the classes learned on En-glish lemmatization by our ngrams+x+latent model.
"For each of the input/output pairs in development data, we found the most probable latent class."
"For the most part, the 2 classes are separated based on whether or not the correct output ends in e. This use of latent classes helped address many errors like wronging / wronge or owed / ow)."
"Such missing or surplus final e’s account for 72.5% of the errors for ngrams and 70.6% of the errors for ngrams+x, but only 34.0% of the errors for ngrams+x+latent."
"The test oracles are between 99.8% – 99.9%, due to the pruned alignment alphabet."
"As on the inflec-tion task, the insertion limit does not exclude any gold standard paths."
"We used the OpenFST library[REF_CITE]to implement all finite-state computations, using the expectation semiring[REF_CITE]for training."
"Our model is defined by the WFSA U θ , which is used to score alignment strings in Σ ∗ (section 2.2)."
We now sketch how to construct U θ from features. n-gram construction The construction that we currently use is quite simple.
All of our current features fire on windows of width ≤ 3.
We build a WFSA with the structure of a 3-gram language model over Σ ∗ .
"Each of the |Σ| 2 states remembers two previous alignment characters ab of history; for each c ∈ Σ, it has an outgoing arc that accepts c (and leads to state bc)."
The weight of this arc is the total weight (from θ) of the small set of features that fire when the trigram window includes abc.
"By conven-tion, these also include features on bc and c (which may be regarded as backoff features ?bc and ??c)."
"Since each character in Σ is actually a 4-tuple, this trigram machine is fairly large."
"We build it lazily (“on the fly”), constructing arcs only as needed to deal with training or test data."
"Feature templates Our experiments use over 50,000 features."
How do we specify these features to the above construction?
"Rather than writing ordi-nary code to extract features from a window, we find it convenient to harness FSTs as a “little language”[REF_CITE]for specifying entire sets of features."
"A feature template T is an nondeterministic FST that maps the contents of the sliding window, such as abc, to one or more features, which are also described as strings. [Footnote_24] The n-gram machine de-scribed above can compute T[((a ? b) ? c) ? ] to find out what features fire on abc and its suffixes."
"24 Formally, if i is a string naming a feature, then f i (A) counts the number of positions in A that are immediately pre-ceded by some string in T −1 [i]."
"One simple feature template performs “vowel/consonant backoff”; e.g., it maps abc to the feature named VCC."
Fig. 2 showed the result of applying several actual feature templates to the window shown in Fig. 1.
The extended regular expression calculus provides a flexible and concise notation for writ-ing down these FSTs.
"As a trivial example, the tri-gram “vowel/consonant backoff” transducer can be described as T = V V V , where V is a transducer that performs backoff on a single alignment charac-ter."
Feature templates should make it easy to experi-ment with adding various kinds of linguistic knowl-edge.
"We have additional algorithms for compiling U θ from a set of arbitrary feature templates, [Footnote_25] in-cluding templates whose features consider windows of variable or even unbounded width."
25 Provided that the total number of features is finite.
"The details are beyond the scope of this paper, but it is worth point-ing out that they exploit the fact that feature tem-plates are FSTs and not arbitrary code."
"The modeling framework we have presented here is, we believe, an attractive solution to most string transduction problems in NLP."
"Rather than learn the topology of an arbitrary WFST, one specifies the topology using a small set of feature templates, and simply trains the weights."
We evaluated on two morphology generation tasks.
"When inflecting German verbs we, even with the simplest features, outperform the moses3 base-line on 3 out of 4 tasks, which uses the same amount of context as our models."
"Introducing more sophis-ticated features that have access to latent classes and regions improves our results dramatically, even on small training data sizes."
"Using these we outper-form moses9 and moses15, which use long context windows, reducing error rates by up to 48%."
On the lemmatization task we were able to improve the re-sults reported[REF_CITE]on three out of four tested languages and reduce the error rates by 38% to 92%.
"The model’s errors are often reason-able misgeneralizations (e.g., assume regular con-jugation where irregular would have been correct), and it is able to use even a small number of latent variables (including the latent alignment) to capture useful linguistic properties."
"In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks."
"We would like to use features that look at wide context on the input side, which is inexpen-sive[REF_CITE]."
Latent variables we wish to consider are an increased number of word classes; more flexible regions—see[REF_CITE]on learning a state transition diagram for acoustic regions in phone recognition—and phono-logical features and syllable boundaries.
"Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists[REF_CITE]."
"Finally, rather than de-fine a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific back-off patterns and adaptively sized n-gram windows."
"Many of these enhancements will increase the com-putational burden, and we are interested in strategies to mitigate this, including approximation methods."
We propose a new graph-based semi-supervised learning (SSL) algorithm and demonstrate its application to document categorization.
Each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted Kullback-Leibler divergence between distributions that encode the class membership probabilities of each vertex.
The proposed objective is convex with guaranteed convergence using an alternating minimiza-tion procedure.
"Further, it generalizes in a straightforward manner to multi-class problems."
"We present results on two stan-dard tasks, namely[REF_CITE]and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art."
Semi-supervised learning (SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers.
"In many problems, such as speech recognition, doc-ument classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily ob-tained thus making these problems useful appli-cations of SSL."
Classic examples of SSL algo-rithms include self-training[REF_CITE]and co-training[REF_CITE].
Graph-based SSL algorithms are an important class of SSL techniques that have attracted much of attention of late[REF_CITE].
Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph.
"In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a mea-sure of similarity between vertices."
Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples[REF_CITE]and those that optimize a loss function based on smoothness constraints derived from the graph[REF_CITE].
Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective[REF_CITE].
"In general graph-based SSL algorithms are non-parametric and transductive. [Footnote_1] A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training."
1 Excluding Manifold Regularizati[REF_CITE].
"In practice, however, transductive learners can be modified to handle unseen data[REF_CITE]."
"A common drawback of many graph-based SSL algorithms (e.g.[REF_CITE]) is that they assume binary classification tasks and thus require the use of sub-optimal (and often com-putationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone struc-tured domains such as strings and trees."
There are also issues related to degenerate solutions (all un-labeled samples classified as belonging to a single class)[REF_CITE].
"For more background on graph-based and general SSL and their applica-tions, see[REF_CITE]."
In this paper we propose a new algorithm for graph-based SSL and use the task of text classifica-tion to demonstrate its benefits over the current state-of-the-art.
Text classification involves automatically assigning a given document to a fixed number of se-mantic categories.
"Each document may belong to one, many, or none of the categories."
"In general, text classification is a multi-class problem (more than 2 categories)."
Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive[REF_CITE].
As a result there has been interest is us-ing SSL techniques for text classificati[REF_CITE].
However past work in semi-supervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem.
"We believe such an approach may be sub-optimal because, disregard-ing data overlap, the different classifiers have train-ing procedures that are independent of one other."
In order to address the above drawback we pro-pose a new framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-divergence)[REF_CITE]terms between probability distributions defined for each graph vertex.
"The use of probability distributions, rather than fixed integer labels, not only leads to a straightforward multi-class generalization, but also allows us to exploit other well-defined functions of distributions, such as entropy, to improve system performance and to allow for the measure of uncer-tainty."
"For example, with a single integer, at most all we know is its assignment."
"With a distribution, we can continuously move from knowing an assignment with certainty (i.e., an entropy of zero) to expres-sions of doubt or multiple valid possibilities (i.e., an entropy greater than zero)."
This is particularly use-ful for document classification as we will see.
"We also show how one can use the alternating minimiza-ti[REF_CITE]algorithm to op-timize our objective leading to a relatively simple, fast, easy-to-implement, guaranteed to converge, it-erative, and closed form update for each iteration."
"We consider the transductive learning problem, i.e., given a training set D = {D l , D u }, where D l and D u are the sets of labeled and unlabeled samples respec-tively, the task is to infer the labels for the samples in D u ."
"In other words, D u is the “test-set.”"
"Here D l = {(x i ,y i )} li=1 , D u = {x i } li+=ul+1 , x i ∈ X (the input space of the classifier, and corresponds to vec-tors of features) and y i ∈ Y (the space of classifier outputs, and for our case is the space of non-negative integers)."
Thus |Y| = 2 yields binary classifica-tion while |Y| &gt; 2 yields multi-class.
"We define n = l + u, the total number of samples in the train-ing set."
"Given D, most graph-based SSL algorithms utilize an undirected weighted graph G = (V,E) where V = {1,...,n} are the data points in D and E = V × V are the set of undirected edges between vertices."
"We use w ij ∈ W to denote the weight of the edge between vertices i and j. W is referred to as the weight (or affinity) matrix of G. As will be seen shortly, the input features x i effect the final classification results via W, i.e., the graph."
Thus graph construction is crucial to the success of any graph-based SSL algorithm.
"Graph construction “is more of an art, than science”[REF_CITE]and is an active research area[REF_CITE]."
"In general the weights are formed as w ij = sim(x i , x j )δ(j ∈ K(i))."
"Here K(i) is the set of i’s k-nearest-neighbors (KNN), sim(x i , x j ) is a given measure of similarity between x i and x j , and δ(c) returns a 1 if c is true and 0 otherwise."
Getting the similarity measure right is crucial for the success of any SSL algorithm as that is what determines the graph.
Note that setting K(i) = |V | = n results in a fully-connected graph.
"Some popular similarity measures include kxi−xjk22 sim(x i , x j ) = e − σ2 or hx i , x j i sim(x i , x j ) = cos(x i , x j ) = k x i k 22 k x j k 22 where k x i k 2 is the L 2 norm, and hx i ,x j i is the inner product of x i and x j ."
The first similarity mea-sure is an RBF kernel applied on the squared Eu-clidean distance while the second is cosine similar-ity.
In this paper all graphs are constructed using cosine similarity.
We next introduce our proposed approach.
"For every i ∈ V , we define a probability distribution p i over the elements of Y."
"In addition let r j , j = 1 . . . l be another set of probability distributions again over the elements of Y (recall, Y is the space of classi-fier outputs)."
Here {r j } j represents the labels of the supervised portion of the training data.
"If the label for a given labeled data point consists only of a sin-gle integer, then the entropy of the corresponding r j is zero (the probability of that integer will be unity, with the remaining probabilities being zero)."
"If, on the other hand, the “label” for a given labeled data point consists of a set of integers (e.g., if the object is a member of multiple classes), then r j is able to represent this property accordingly (see below)."
"We emphasize again that both p i and r j are probability distributions, with r j fixed throughout training."
"The goal of learning in this paper is to find the best set of distributions p i , ∀i that attempt to: 1) agree with the labeled data r j wherever it is available; 2) agree with each other (when they are close according to a graph); and 3) be smooth in some way."
"These cri-teria are captured in the following new multi-class SSL optimization procedure: &quot; l min p C 1 (p), where C 1 (p) = X D KL r i ||p i i=1 n  n +µ X X w ij"
"D KL p i ||p j − ν X H(p i ) , i j i=1 (1) and where p , (p 1 ,...,p n ) denotes the en-tire set of distributions to be learned, H(p i ) = − P y p i (y) log p i (y) is the standard Shannon en-tropy function of p i , D KL ( p i ||q j ) is the KL-divergence between p i and q j , and µ and ν are hy-perparameters whose selection we discuss in section 5."
"The distributions r i are derived from D l (as men-tioned above) and this can be done in one of the fol-lowing ways: (a) if ŷ i is the single supervised label for input x i then r i (y) = δ(y = ŷ i ), which means that r i gives unity probability for y equaling the la-bel ŷ i ; (b) if ŷ i = {ŷ i(1) , . . . , ŷ i(k) }, k ≤ |Y| is a set of possible outputs for input x i , meaning an object validly falls into all of the corresponding categories, we set r i (y) = (1/k)δ(y ∈ ŷ i ) meaning that r i is uniform over only the possible categories and zero otherwise; (c) if the labels are somehow provided in the form of a set of non-negative scores, or even a probability distribution itself, we just set r i to be equal to those scores (possibly) normalized to be-come a valid probability distribution."
"Among these three cases, case (b) is particularly relevant to text classification as a given document many belong to (and in practice may be labeled as) many classes."
"The final classification results, i.e., the final labels for D u , are then given by ŷ = argmax p i (y). y∈Y"
We next provide further intuition on our objective function.
SSL on a graph consists of finding a la-beling D u that is consistent with both the labels pro-vided in D l and the geometry of the data induced by the graph.
"The first term of C 1 will penalize the solution p i i ∈ {1, . . . , l}, when it is far away from the labeled training data D l , but it does not in-sist that p i = r i , as allowing for deviations from r i can help especially with noisy labels[REF_CITE]or when the graph is extremely dense in cer-tain regions."
"As explained above, our framework al-lows for the case where supervised training is uncer-tain or ambiguous."
"We consider it reasonable to call our approach soft-supervised learning, generalizing the notion of semi-supervised learning, since there is even more of a continuum here between fully su-pervised and fully unsupervised learning than what typically exists with SSL."
Soft-supervised learning allows uncertainty to be expressed (via a probability distribution) about any of the labels individually.
The second term of C 1 penalizes a lack of con-sistency with the geometry of the data and can be seen as a graph regularizer.
"If w ij is large, we prefer a solution in which p i and p j are close in the KL-divergence sense."
"While KL-divergence is asym-metric, given that G is undirected implies W is sym-metric (w ij = w ji ) and as a result the second term is inherently symmetric."
The last term encourages each p i to be close to the uniform distribution if not preferred to the con-trary by the first two terms.
This acts as a guard against degenerate solutions commonly encountered in SSL[REF_CITE].
"For example, consider the case where part of the graph is almost completely disconnected from any labeled vertex (which is possible in the k-nearest neighbor case)."
"In such situations the third term en- sures that the nodes in this disconnected region are encouraged to yield a uniform distribution, validly expressing the fact that we do not know the labels of these nodes based on the nature of the graph."
"More generally, we conjecture that by maximizing the en-tropy of each p i , the classifier has a better chance of producing high entropy results in graph regions of low confidence (e.g. close to the decision boundary and/or low density regions)."
This overcomes a com-mon drawback of a large number of state-of-the-art classifiers that tend to be confident even in regions close to the decision boundary.
We conclude this section by summarizing some of the features of our proposed framework.
It should be clear that C 1 uses the “manifold assumption” for SSL (see chapter 2[REF_CITE]) — it assumes that the input data can be embed-ded within a low-dimensional manifold (the graph).
"As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers[REF_CITE]), the framework general-izes in a straightforward manner to multi-class prob-lems."
"Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y| independent problems)."
"Fur-thermore, the objective is capable of handling label training data uncertainty[REF_CITE]."
"Of course, this objective would be useless if it wasn’t possible to efficiently and easily optimize it on large data sets."
We next describe a method that can do this.
"As long as µ,ν ≥ 0, the objective C 1 (p) is con-vex."
"This follows since D KL (p i ||p j ) is convex in the pair (p i ,p j )[REF_CITE], nega-tive entropy is convex, and a positive-weighted lin-ear combination of a set of convex functions is con-vex."
"Thus, the problem of minimizing C 1 over the space of collections of probability distributions (a convex set) constitutes a convex programming prob-lem[REF_CITE]."
This property is extremely beneficial since there is a unique global optimum and there are a variety of methods that can be used to yield that global optimum.
One possible method might take the derivative of the objective along with Lagrange multipliers to ensure that we stay within the space of probability distributions.
This method can sometimes yield a closed form single-step an-alytical expression for the globally optimum solu-tion.
"Unfortunately, however, our problem does not admit such a closed form solution because the gra-dient of C 1 (p) with respect to p i (y) is of the form, k 1 p i (y) log p i (y) + k 2 p i (y) + k 3 (where k 1 , k 2 , k 3 are fixed constants)."
"Sometimes, optimizing the dual of the objective can also produce a solution, but un-fortunately again the dual of our objective also does not yield a closed form solution."
"The typical next step, then, is to resort to iterative techniques such as gradient descent along with modifications to en-sure that the solution stays within the set of proba-bility distributions (the gradient of C 1 alone will not necessarily point in the direction where p is still a valid distribution) - one such modification is called the method of multipliers (MOM)."
Another solu-tion would be to use computationally complex (and complicated) algorithms like interior point methods (IPM).
"While all of the above methods (described in detail[REF_CITE]) are feasible ways to solve our problem, they each have their own draw-backs."
"Using MOM, for example, requires the care-ful tuning of a number of additional parameters such as learning rates, growth factors, and so on."
IPM in-volves inverting a matrix of the order of the number of variables and constraints during each iteration.
We instead adopt a different strategy based on al-ternating minimizati[REF_CITE].
"This approach has a single additional optimization parameter (contrasted with MOM), admits a closed form solution for each iteration not involving any matrix inversion (contrasted with IPM), and yields guaranteed convergence to the global optimum."
"In order to render our approach amenable to AM, how-ever, we relax our objective C 1 by defining a new (third) set of distributions for all training samples q i , i = 1, . . . , n denoted collectively like the above us-ing the notation q , (q 1 , . .. , q n )."
"We define a new objective to be optimized as follows: &quot; l min p,q C 2 (p, q), where C 2 (p, q) = X D KL r i ||q i i=1 n  n +µ X X w ij0 D KL p i ||q j − ν X H(p i ) . i=1 j∈N(i) i=1"
"Before going further, the reader may be wondering at this juncture how might it be desirable for us to have apparently complicated the objective function in an attempt to yield a more computationally and methodologically superior machine learning proce-dure."
This is indeed the case as will be spelled out below.
"First, in C 2 we have defined a new weight matrix [W 0 ] ij = w 0ij of the same size as the original where W 0 = W + αI n , where I n is the n × n iden-tity matrix, and where α ≥ 0 is a non-negative con-stant (this is the optimization related parameter men-tioned above)."
This has the effect that w ii0 ≥ w ii .
"In the original objective C 1 , w ii is irrelevant since D KL (p||p) = 0 for all p, but since there are now two distributions for each training point, there should be encouragement for the two to approach each other."
"Like C 1 , the first term of C 2 ensures that the la-beled training data is respected and the last term is a smoothness regularizer, but these are done via dif-ferent sets of distributions, q and p respectively — this choice is what makes possible the relatively sim-ple analytical update equations given below."
"Next, we see that the two objective functions in fact have identical solutions when the optimization enforces the constraint that p and q are equal: (p,q):p=q min C 2 (p, q) = min p C 1 (p)."
"Indeed, as α gets large, the solutions considered vi-able are those only where p = q. We thus have that: α→∞ p,q lim min C 2 (p, q) = min p C 1 (p)."
"Therefore, the two objectives should yield the same solution as long as α ≥ w ij for all i, j. A key advan-tage of this relaxed objective is that it is amenable to alternating minimization, a method to produce a se-quence of sets of distributions (p n , q n ) as follows: p n = argminC 2 (p, q n−1 ), q n = argminC 2 (p n , q). p q"
"It can be shown (we omit the rather lengthy proof due to space constraints) that the sequence gener-ated using the above minimizations converges to the minimum of C 2 (p, q), i.e., n→∞ lim C 2 (p (n) , q (n) ) = inf p,q C 2 (p, q), provided we start with a distribution that is initial-ized properly q (0) (y) &gt; 0 ∀ y ∈ Y."
The update equations for p (n) and q (n) are given by βi(n−1)(y) p (in) (y) =
"Z1 exp , γi i q (in) (y) = r i (y)δ(i ≤ l) + µ P j w ji 0 p (jn) (y), δ(i ≤ l) + µ P j w 0 ji where γ i = ν + µ X w 0 ij , j β i(n−1) (y) = −ν + µ X w ij 0 (log q j(n−1) (y) − 1) j and where Z i is a normalizing constant to ensure p i is a valid probability distribution."
"Note that each it-eration of the proposed framework has a closed form solution and is relatively simple to implement, even for very large graphs."
Henceforth we refer to the proposed objective optimized using alternating min-imization as AM.
Label propagation (LP)[REF_CITE]is a graph-based SSL algorithms that per-forms Markov random walks on the graph and has a straightforward extension to multi-class problems.
The update equations for LP (which also we use for our LP implementations) may be written as p (in) (y) = r i (y)δ(i ≤ l) + δ(i &gt; l) P j w ij p (jn−1) (y) δ(i ≤ l) + δ(i &gt; l) P j w ij
Note the similarity to the update equation for q i(n) in our AM case.
It has been shown that the squared-loss based SSL algorithm[REF_CITE]and LP have similar updates[REF_CITE].
The proposed objective C 1 is similar in spirit to the squared-loss based objective[REF_CITE].
"Our method, however, differs in that we are optimizing the KL-divergence over probability distributions."
We show in section 5 that KL-divergence based loss significantly outperforms the squared-loss.
"We believe that this could be due to the following: 1) squared loss is appropriate un-der a Gaussian loss model which may not be opti-mal under many circumstances (e.g. classification); 2) KL-divergence D KL (p||q) is based on a relative (relative to p) rather than an absolute error; and 3) under certain natural assumptions, KL-divergence is asymptotically consistent with respect to the under-lying probability distributions."
AM is also similar to the spectral graph trans-ducer[REF_CITE]in that they both attempt to find labellings over the unlabeled data that re-spect the smoothness constraints of the graph.
"While spectral graph transduction is an approximate solu-tion to a discrete optimization problem (which is NP hard), AM is an exact solution obtained by optimiz-ing a convex function over a continuous space."
"Fur-ther, while spectral graph transduction assumes bi-nary classification problems, AM naturally extends to multi-class situations without loss of convexity."
Entropy Minimization (EnM)[REF_CITE]uses the entropy of the unlabeled data as a regularizer while optimizing a parametric loss function defined over the labeled data.
"While the objectives in the case of both AM and EnM make use of the entropy of the unlabeled data, there are several important differences: (a) EnM is not graph-based, (b) EnM is parametric whereas our proposed approach is non-parametric, and most importantly, (c) EnM attempts to minimize entropy while the pro-posed approach aims to maximize entropy."
"While this may seem a triviality, it has catastrophic conse-quences in terms of both the mathematics and mean-ing."
"The objective in case of EnM is not convex, whereas in our case we have a convex formulation with simple update equations and convergence guar-antees.[REF_CITE]is a graph-based SSL al-gorithm that also employs alternating minimiza-tion style optimization."
"However, it is inherently squared-loss based which our proposed approach out-performs (see section 5)."
"Further, they do not provide or state convergence guarantees and one side of their update approximates an NP-complete optimization procedure."
The information regularization (IR)[REF_CITE]algorithm also makes use of a KL-divergence based loss for SSL.
Here the in-put space is divided into regions {R i } which might or might not overlap.
"For a given point x i ∈ R i , IR attempts to minimize the KL-divergence between p i (y i |x i ) and p̂ R i (y), the agglomerative distribution for region R i ."
"Given a graph, one can define a re-gion to be a vertex and its neighbor thus making IR amenable to graph-based SSL."
"In[REF_CITE], the agglomeration is performed by a simple averaging (arithmetic mean)."
"While IR sug-gests (without proof of convergence) the use of al-ternating minimization for optimization, one of the steps of the optimization does not admit a closed-form solution."
This is a serious practical drawback especially in the case of large data sets.[REF_CITE](hereafter referred to as PD) is an extension of the IR algorithm to hypergraphs where the agglom-eration is performed using the geometric mean.
This leads to closed form solutions in both steps of the al-ternating minimization.
"There are several important differences between IR and PD on one side and our proposed approach: (a) neither IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13[REF_CITE]) is actually a spe-cial case of our update equation for p i (y) and may be obtained by setting w ij = 1/2."
"Further, our work here may be easily extended to hypergraphs."
"We compare our algorithm (AM) with other state-of-the-art SSL-based text categorization al-gorithms, namely, (a) SVM[REF_CITE], (b) Transductive-SVM (TSVM)[REF_CITE], (c) Spectral Graph Transduction (SGT)[REF_CITE], and (d) Label Propagation (LP)[REF_CITE]."
"Note that only SGT and LP are graph-based algorithms, while SVM is fully-supervised (i.e., it does not make use of any of the unlabeled data)."
"We implemented SVM and TSVM using SVM Light (Joachims, b) and SGT using SGT Light (Joachims, a)."
"In the case of SVM, TSVM and SGT we trained |Y| classifiers (one for each class) in a one vs. rest manner precisely following[REF_CITE]."
We used the “ModApte” split of the[REF_CITE]dataset collected from the Reuters newswire in 1987[REF_CITE].
"The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u )."
Categories outside the 10 most frequent were collapsed into one class and assigned a label “other”.
"For each document i in the training and test sets, we extract features x i in the following manner: stop-words are removed fol-lowed by the removal of case and information about inflection (i.e., stemming)[REF_CITE]."
We then compute TFIDF features for each document[REF_CITE].
All graphs were constructed us-ing cosine similarity with TFIDF features.
"For this task Y = { earn, acq, money, grain, crude, trade, interest, ship, wheat, corn, average}."
"For LP and AM, we use the output space Y 0 ="
Y∪{ other }.
"For documents in D l that are labeled with multiple categories, we initialize r i to have equal non-zero probability for each such category."
"For example, if document i is annotated as belonging to classes { acq, grain, wheat}, then r i (acq) = r i (grain) = r i (wheat) = 1/3."
These samples constitute D l .
All algo-rithms used the same transduction sets.
"In the case of SGT, LP and AM, the first transduction set was used to tune the hyperparameters which we then held fixed for all the remaining 20 transduction sets."
"For all the graph-based approaches, we ran a search over K ∈ {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note K = n represents a fully connected graph)."
"In addi-tion, in the case of AM, we set α = 2 for all exper-iments, and we ran a search over µ ∈ {1e–8, 1e–4, 0.01, 0.1, 1, 10, 100} and ν ∈ {1e–8, 1e–6, 1e–4, 0.01, 0.1}, for SGT the search was over c ∈ {3000, 3200, 3400, 3800, 5000, 100000} (see[REF_CITE])."
"We report precision-recall break even point (PRBEP) results on the 3,299 test documents in Ta-ble 1."
PRBEP has been a popular measure in infor-mation retrieval (see e.g.[REF_CITE]).
It is defined as that value for which precision and recall are equal.
Results for each category in Ta-ble 1 were obtained by averaging the PRBEP over the 20 transduction sets.
The final row “average” was obtained by macro-averaging (average of av-erages).
"The optimal value of the hyperparame-ters in case of LP was K = 100; in case of AM, K = 2000, µ = 1e–4, ν = 1e–2; and in the case of SGT, K = 100, c = 3400."
The results show that AM outperforms the state-of-the-art on 6 out of 10 categories and is competitive in 3 of the remain-ing 4 categories.
Further it significantly outperforms all other approaches in case of the macro-averages.
AM is significant over its best competitor SGT at the 0.0001 level according to the difference of pro-portions significance test.
Figure 1 shows the variation of “average” PRBEP against the number of labeled documents (l).
"For each value of l, we tuned the hyperparameters over the first transduction set and used these values for all the other 20 sets."
Figure 1 also shows error-bars (± standard deviation) all the experiments.
"As expected, the performance of all the approaches improves with increasing number of labeled docu-ments."
"Once again in this case, AM, outperforms the other approaches for all values of l."
World Wide Knowledge Base (WebKB) is a collec-tion of 8282 web pages obtained from four academic domains.
The web pages in the WebKB set are la-beled using two different polychotomies.
The first is according to topic and the second is according to web domain.
"In our experiments we only consid-ered the first polychotomy, which consists of 7 cat-egories: course, department, faculty, project, staff, student, and other."
"Following[REF_CITE]we only use documents from categories course, de-partment, faculty, project which gives 4199 docu-ments for the four categories."
"Each of the documents is in HTML format containing text as well as other information such as HTML tags, links, etc."
We used both textual and non-textual information to construct the feature vectors.
In this case we did not use ei-ther stop-word removal or stemming as this has been found to hurt performance on this task[REF_CITE].
As in the the case of the Reuters data set we extracted TFIDF features for each document and constructed the graph using cosine similarity.
"As[REF_CITE], we created four roughly-equal random partitions of the data set."
"In order to obtain D l , we first randomly choose a split and then sample l documents from that split."
The other three splits constitute D u .
We believe this is more realistic than sampling the labeled web-pages from a single university and testing web-pages from the other universities[REF_CITE].
This method of creating transduction sets allows us to better eval-uate the generalization performance of the various algorithms.
Once again we create 21 transduction sets and the first set was used to tune the hyperpa-rameters.
"Further, we ran a search over the same grid as used in the case of Reuters."
"We report precision- recall break even point (PRBEP) results on the 3,148 test documents in Table 2."
"For this task, we found that the optimal value of the hyperparameter were: in the case of LP, K = 1000; in case of AM, K = 1000, µ = 1e–2, ν = 1e–4; and in case of SGT, K = 100, c = 3200."
"Once again, AM is sig-nificant at the 0.0001 level over its closest competi-tor LP."
Figure 2 shows the variation of PRBEP with number of labeled documents (l) and was generated in a similar fashion as in the case of the Reuters data set.
"We note that LP may be cast into an AM-like frame-work by using the following sequence of updates, p (in) (y) = δ(i ≤ l)r i (y) + δ(i &gt; l)q (in−1) , P w p (n) (y) j ij i q (in) (y) ="
P w j ij
"To compare the behavior of AM and LP, we ap-plied this form of LP along with AM on a simple 5-node binary-classification SSL graph where two nodes are labeled (node 1 and 2) and the remaining nodes are unlabeled (see Figure 3, top)."
"Since this is binary classification (|Y | = 2), each distribution p i or q i can be depicted using only a single real num-ber between 0 and 1 corresponding to the probability that each vertex is class 2 (yes two)."
"We show how both LP and AM evolve starting from exactly the same random starting point q 0 (Figure 3, bottom)."
"For each algorithm, the figure shows that both algo-rithms clearly converge."
"Each alternate iteration of LP is such that the labeled vertices oscillate due to its clamping back to the labeled distribution, but that is not the case for AM."
"We see, moreover, qualitative differences in the solutions as well – e.g., AM’s so-lution for the pendant node 5 is less confident than is LP’s solution."
More empirical comparative analysis between the two algorithms of this sort will appear in future work.
We have proposed a new algorithm for semi-supervised text categorization.
Empirical results show that the proposed approach significantly out-performs the state-of-the-art.
In addition the pro-posed approach is relatively simple to implement and has guaranteed convergence properties.
"While in this work, we use relatively simple features to construct the graph, use of more sophisticated fea-tures and/or similarity measures could lead to further improved results."
