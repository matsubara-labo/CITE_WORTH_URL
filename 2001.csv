sentence
"The 3 billion base pair sequence of the human genome is now available, and attention is focusing on"
"This paper addresses recent progress in speaker-independent, large vocabulary, continuous speech recognition, which has opened up a wide range of near and mid-term applications."
One rapidly ex-panding application area is the process-ing of broadcast audio for information access.
"At L IMSI , broadcast news tran-scription systems have been developed for English, French, German, Mandarin and Portuguese, and systems for other languages are under development."
"Au-dio indexation must take into account the specificities of audio data, such as needing to deal with the continuous data stream and an imperfect word tran-scription."
"Some near-term applications areas are audio data mining, selective dissemination of information and me-dia monitoring."
A major advance in speech processing technology is the ability of todays systems to deal with non-homogeneous data as is exemplified by broadcast data.
"With the rapid expansion of different me-dia sources, there is a pressing need for automatic processing of such audio streams."
"Broadcast au-dio is challenging as it contains segments of vari-ous acoustic and linguistic natures, which require appropriate modeling."
A special section in the Communications of the ACM devoted to “News on Demand”[REF_CITE]includes contribu-tions from many of the sites carrying out active research in this area.
"Via speech recognition, spoken document re-trieval (SDR) can support random access to rel-evant portions of audio documents, reducing the time needed to identify recordings in large multi-media databases."
The TREC (Text REtrieval Con-ference) SDR evaluation showed that only small differences in information retrieval performance are observed for automatic and manual transcrip-tions[REF_CITE].
Large vocabulary continuous speech recogni-tion (LVCSR) is a key technology that can be used to enable content-based information access in au-dio and video documents.
"Since most of the lin-guistic information is encoded in the audio chan-nel of video data, which once transcribed can be accessed using text-based tools."
This research has been carried out in a multilingual environment in the context of several recent and ongoing Euro-pean projects.
We highlight recent progress in LVCSR and describe some of our work in de-veloping a system for processing broadcast au-dio for information access.
"The system has two main components, the speech transcription com-ponent and the information retrieval component."
"Versions of the L IMSI broadcast news transcrip-tion system have been developed in American En-glish, French, German, Mandarin and Portuguese."
Substantial advances in speech recognition tech-nology have been achieved during the last decade.
Only a few years ago speech recognition was pri- marily associated with small vocabulary isolated word recognition and with speaker-dependent (of-ten also domain-specific) dictation systems.
"The same core technology serves as the basis for a range of applications such as voice-interactive database access or limited-domain dictation, as well as more demanding tasks such as the tran-scription of broadcast data."
"With the exception of the inherent variability of telephone channels, for most applications it is reasonable to assume that the speech is produced in relatively stable envi-ronmental and in some cases is spoken with the purpose of being recognized by the machine."
"The ability of systems to deal with non-homogeneous data as is found in broadcast au-dio (changing speakers, languages, backgrounds, topics) has been enabled by advances in a vari-ety of areas including techniques for robust signal processing and normalization; improved training techniques which can take advantage of very large audio and textual corpora; algorithms for audio segmentation; unsupervised acoustic model adap-tation; efficient decoding with long span language models; ability to use much larger vocabularies than in the past - 64 k words or more is common to reduce errors due to out-of-vocabulary words."
"With the rapid expansion of different media sources for information dissemination including via the internet, there is a pressing need for au-tomatic processing of the audio data stream."
"The vast majority of audio and video documents that are produced and broadcast do not have associ-ated annotations for indexation and retrieval pur-poses, and since most of today’s annotation meth-ods require substantial manual intervention, and the cost is too large to treat the ever increasing volume of documents."
"Broadcast audio is chal-lenging to process as it contains segments of vari-ous acoustic and linguistic natures, which require appropriate modeling."
"Transcribing such data re-quires significantly higher processing power than what is needed to transcribe read speech data in a controlled environment, such as for speaker adapted dictation."
"Although it is usually as-sumed that processing time is not a major issue since computer power has been increasing con-tinuously, it is also known that the amount of data appearing on information channels is increasing at a close rate."
Therefore processing time is an important factor in making a speech transcription system viable for audio data mining and other re-lated applications.
Transcription word error rates of about 20% have been reported for unrestricted broadcast news data in several languages.
As shown in Figure 1 the L IMSI broadcast news transcription system for automatic indexa-tion consists of an audio partitioner and a speech recognizer.
"The goal of audio partitioning is to divide the acoustic signal into homogeneous segments, la-beling and structuring the acoustic content of the data, and identifying and removing non-speech segments."
The L IMSI BN audio partitioner re-lies on an audio stream mixture model[REF_CITE].
"While it is possible to transcribe the continuous stream of audio data without any prior segmentation, partitioning offers several advan-tages over this straight-forward solution."
"First, in addition to the transcription of what was said, other interesting information can be extracted such as the division into speaker turns and the speaker identities, and background acoustic con-ditions."
This information can be used both di-rectly and indirectly for indexation and retrieval purposes.
"Second, by clustering segments from the same speaker, acoustic model adaptation can be carried out on a per cluster basis, as opposed to on a single segment basis, thus providing more adaptation data."
"Third, prior segmentation can avoid problems caused by linguistic discontinu-ity at speaker changes."
"Fourth, by using acoustic models trained on particular acoustic conditions (such as wide-band or telephone band), overall performance can be significantly improved."
"Fi-nally, eliminating non-speech segments substan-tially reduces the computation time."
"The result of the partitioning process is a set of speech seg-ments usually corresponding to speaker turns with speaker, gender and telephone/wide-band labels (see Figure 2)."
"For each speech segment, the word recognizer de-termines the sequence of words in the segment, associating start and end times and an optional confidence measure with each word."
"The L IMSI system, in common with most of today’s state-of-the-art systems, makes use of statistical models of speech generation."
"From this point of view, message generation is represented by a language model which provides an estimate of the probabil-ity of any given word string, and the encoding of the message in the acoustic signal is represented by a probability density function."
"The speaker-independent 65k word, continuous speech rec-ognizer makes use of 4-gram statistics for lan-guage modeling and of continuous density hidden Markov models (HMMs) with Gaussian mixtures for acoustic modeling."
Each word is represented by one or more sequences of context-dependent phone models as determined by its pronunciation.
"The acoustic and language models are trained on large, representative corpora for each task and language."
Processing time is an important factor in mak-ing a speech transcription system viable for au-tomatic indexation of radio and television broad-casts.
"For many applications there are limita-tions on the response time and the available com-putational resources, which in turn can signifi-cantly affect the design of the acoustic and lan-guage models."
Word recognition is carried out in one or more decoding passes with more accurate acoustic and language models used in successive passes.
"A 4-gram single pass dynamic network decoder has been developed[REF_CITE]which can achieve faster than real-time de-coding with a word error under 30%, running in less than 100 Mb of memory on widely available platforms such Pentium III or Alpha machines."
"A characteristic of the broadcast news domain is that, at least for what concerns major news events, similar topics are simultaneously covered in dif-ferent emissions and in different countries and languages."
Automatic processing carried out on contemporaneous data sources in different lan-guages can serve for multi-lingual indexation and retrieval.
"Multilinguality is thus of particular in-terest for media watch applications, where news may first break in another country or language."
"At L IMSI broadcast news transcription systems have been developed for the American English,"
"French, German, Mandarin and Portuguese lan-guages."
"The Mandarin language was chosen be-cause it is quite different from the other lan-guages (tone and syllable-based), and Mandarin resources are available via the LDC as well as ref-erence performance results."
Our system and other state-of-the-art sys-tems can transcribe unrestricted American En-glish broadcast news data with word error rates under 20%.
Our transcription systems for French and German have comparable error rates for news broadcasts[REF_CITE].
The character error rate for Mandarin is also about 20%[REF_CITE].
"Based on our expe-rience, it appears that with appropriately trained models, recognizer performance is more depen-dent upon the type and source of data, than on the language."
"For example, documentaries are partic-ularly challenging to transcribe, as the audio qual-ity is often not very high, and there is a large pro-portion of voice over."
The automatically generated partition and word transcription can be used for indexation and in-formation retrieval purposes.
Techniques com-monly applied to automatic text indexation can be applied to the automatic transcriptions of the broadcast news radio and TV documents.
"These techniques are based on document term frequen-cies, where the terms are obtained after standard text processing, such as text normalization, tok-enization, stopping and stemming."
Most of these preprocessing steps are the same as those used to prepare the texts for training the speech recog-nizer language models.
"While this offers advan-tages for speech recognition, it can lead to IR er-rors."
"For better IR results, some words sequences corresponding to acronymns, multiword named-entities (e.g. Los Angeles), and words preceded by some particular prefixes (anti, co, bi, counter) are rewritten as a single word."
Stemming is used to reduce the number of lexical items for a given word sense.
"The stemming lexicon contains about 32000 entries and was constructed using Porter’s algorithm[REF_CITE]on the most frequent words in the collection, and then manually cor-rected."
The information retrieval system relies on a un- mean average precision using using a 1-gram doc-ument model.
"The document collection contains 557 hours of broadcast news from the period of February[REF_CITE]. (21750 stories, 50 queries with the associated relevance judgments.) igram model per story."
The score of a story is ob-tained by summing the query term weights which are simply the log probabilities of the terms given the story model once interpolated with a general English model.
This term weighting has been shown to perform as well as the popular TF IDF weighting scheme[REF_CITE].
The text of the query may or may not include the index terms associated with relevant docu-ments.
"One way to cope with this problem is to use query expansion (Blind Relevance Feedback, BRF (Walker and de[REF_CITE])) based on terms present in retrieved contemporary texts."
"The system was evaluated in the TREC SDR track, with known story boundaries."
The SDR data collection contains 557 hours of broadcast news from the period of February[REF_CITE].
This data includes 21750 stories and a set of 50 queries with the associated relevance judg-ments[REF_CITE].
In order to assess the effect of the recogni-tion time on the information retrieval results we transcribed the 557 hours of broadcast news data using two decoder configurations: a single pass 1.4xRT system and a three pass 10xRT system.
The word error rates are measured on a 10h test subset[REF_CITE].
"The information retrieval results are given in terms of mean av-erage precision (MAP), as is done for the TREC benchmarks in Table 1 with and without query ex-pansion."
"For comparison, results are also given for manually produced closed captions."
"With query expansion comparable IR results are ob-tained using the closed captions and the 10xRT transcriptions, and a moderate degradation (4% absolute) is observed using the 1.4xRT transcrip-tions."
The broadcast news transcription system also pro-vides non-lexical information along with the word transcription.
"This information is available in the partition of the audio track, which identifies speaker turns."
"It is interesting to see whether or not such information can be used to help locate story boundaries, since in the general case these are not known."
Statistics were made on 100 hours of radio and television broadcast news with man-ual transcriptions including the speaker identities.
This means that using only speaker change information for detect-ing document boundaries would miss 40% of the boundaries.
"With automatically detected speaker changes, the number of missed boundaries would certainly increase."
"At the same time, 11,160 of the 12,439 speaker turns occur in the middle of a document, resulting in a false alarm rate of almost 90%."
"A more detailed analysis shows that about 50% of the sections involve a single speaker, but that the distribution of the number of speaker turns per section falls off very gradually (see Fig-ure 3)."
"False alarms are not as harmful as missed detections, since it may be possible to merge ad-jacent turns into a single document in subsequent processing."
These results show that even perfect speaker turn boundaries cannot be used as the pri-mary cue for locating document boundaries.
"They can, however, be used to refine the placement of a document boundary located near a speaker change."
We also investigated using simple statistics on the durations of the documents.
A histogram of the 2096 sections is shown in Figure 4.
One third of the sections are shorter than 30 seconds.
"The histogram has a bimodal distribution with a sharp peak around 20 seconds, and a smaller, flat peak around 2 minutes."
"Very short documents are typical of headlines which are uttered by sin-gle speaker, whereas longer documents are more likely to contain data from multiple talkers."
This distribution led us to consider using a multi-scale segmentation of the audio stream into documents.
Similar statistics were measured on the larger cor-pus (Figure 4 bottom).
"As proposed[REF_CITE], we segment the audio stream into overlapping documents of a fixed duration."
"As a result of optimization, we chose a 30 sec-ond window duration with a 15 second overlap."
Since there are many stories significantly shorter than 30s in broadcast shows (see Figure 4) we conjunctured that it may be of interest to use a double windowing system in order to better tar-get short stories[REF_CITE].
The win-dow size of the smaller window was selected to be 10 seconds.
"So for each query, we indepen-dently retrieved two sets of documents, one set for each window size."
"Then for each document set, document recombination is done by merging overlapping documents until no further merges are possible."
The score of a combined document is set to maximum score of any one of the com-ponents.
"For each document derived from the 30s windows, we produce a time stamp located at the center point of the document."
"However, if any smaller documents are embedded in this document, we take the center of the best scor-ing document."
This way we try to take advantage of both window sizes.
The MAP using a single 30s window and the double windowing strategy are shown in Table 2.
"For comparison, the IR re-sults using the manual story segmentation and the speaker turns located by the audio partitioner are also given."
All conditions use the same word hy-potheses obtained with a speech recognizer which had no knowledge about the story boundaries.
From these results we can clearly see the inter-est of using a search engine specifically designed to retrieve stories in the audio stream.
"Using an a priori acoustic segmentation, the mean aver-age precision is significantly reduced compared to a “perfect” manual segmentation, whereas the window-based search engine results are much closer."
Note that in the manual segmentation all non-story segments such as advertising have been removed.
This reduces the risk of having out-of-topic hits and explains part of the difference be-tween this condition and the other conditions.
"The problem of locating story boundaries is be-ing further pursued in the context of the A LERT project, where one of the goals is to identify “doc-uments” given topic profiles."
This project is in-vestigating the combined use of audio and video segmentation to more accurately locate document boundaries in the continuous data stream.
The work presented in this paper has benefited from a variety of research projects both at the Eu-ropean and National levels.
These collaborative efforts have enabled access to real-world data al-lowing us to develop algorithms and models well-suited for near-term applications.
The European project LE-4 O LIVE : A Multilingual Indexing Tool for Broadcast Material Based on Speech Recognition[URL_CITE]olive/) addressed methods to automate the disclosure of the infor-mation content of broadcast data thus allowing content-based indexation.
"Speech recognition was used to produce a time-linked transcript of the audio channel of a broadcast, which was then used to produce a concept index for retrieval."
Broadcast news transcription systems for French and German were developed.
The French data come from a variety of television news shows and radio stations.
The German data consist of TV news and documentaries from ARTE.
"O LIVE also developed tools for users to query the database, as well as cross-lingual access based on off-line machine translation of the archived documents, and online query translation."
The European project IST A LERT : Alert sys-tem for selective dissemination[URL_CITE]aims to associate state-of-the-art speech recognition with audio and video segmentation and automatic topic index-ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications.
"The targeted languages are French, German and Portuguese."
Major media-monitoring companies in Europe are participating in this project.
Two other related FP5 IST projects are: C ORE -
TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line.
"C ORETEX[URL_CITE]aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology."
"In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions."
The E CHO project[URL_CITE]aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives.
"The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation."
This paper has described some of the ongoing re-search activites at L IMSI in automatic transcrip-tion and indexation of broadcast data.
"Much of this research, which is at the forefront of todays technology, is carried out with partners with real needs for advanced audio processing technolo-gies."
Automatic speech recognition is a key tech-nology for audio and video indexing.
"Most of the linguistic information is encoded in the au-dio channel of video data, which once transcribed can be accessed using text-based tools."
This is in contrast to the image data for which no common description language is widely adpoted.
"A va-riety of near-term applications are possible such as audio data mining, selective dissemination of information (News-on-Demand), media monitor-ing, content-based audio and video retrieval."
"It appears that with word error rates on the order of 20%, comparable IR results to those obtained on text data can be achieved."
"Even with higher word error rates obtained by run-ning a faster transcription system or by transcrib-ing compressed audio data ([REF_CITE]; J.M.[REF_CITE]) (such as that can be loaded over the Internet), the IR performance re-mains quite good."
"In this paper, we propose adding long-term grammatical information in a Whole Sentence Maximun Entropy Language Model (WSME) in order to improve the performance of the model."
The grammatical information was added to the WSME model as fea-tures and were obtained from a Stochas-tic Context-Free grammar.
"Finally, ex-periments using a part of the Penn Tree-bank corpus were carried out and sig-nificant improvements were acheived."
"Language modeling is an important component in computational applications such as speech recog-nition, automatic translation, optical character recognition, information retrieval etc.[REF_CITE]."
Statistical language models have gained considerable acceptance due to the efficiency demonstrated in the fields in which they have been applied[REF_CITE].
"Traditional statistical language models calcu-late the probability of a sentence using the chain rule: where &quot; # $ , which is usually known as the history of ."
The effort in the language modeling techniques is usually directed to the es-timation of   &quot; .
The language model defined by the expression   &quot; is named the condi-tional language model.
"In principle, the deter-mination of the conditional probability in (1) is expensive, because the possible number of word sequences is very great."
"Traditional conditional language models assume that the probability of the word does not depend on the entire history, and the history is limited by an equivalence rela-tion % , and (1) is rewritten as: &amp;   &amp;&apos; ( % (2)"
The most commonly used conditional language model is the n-gram model.
"In the n-gram model, the history is reduced (by the equivalence rela-tion) to the last )+-* , words."
"The power of the n-gram model resides in: its consistence with the training data, its simple formulation, and its easy implementation."
"However, the n-gram model only uses the information provided by the last )./* , words to predict the next word and so only makes use of local information."
"In addition, the value of n must be low ( 021 ) because for )43-1 there are problems with the parameter estimation."
"Hybrid models have been proposed, in an at-tempt to supplement the local information with long-distance information."
"They combine dif-ferent types of models, like n-grams, with long-distance information, generally by means of lin-ear interpolation, as has been shown in (Belle- garda, 1998;[REF_CITE])."
A formal framework to include long-distance and local information in the same language model is based on the Maximum Entropy principle (ME).
"Using the ME principle, we can combine information from a variety of sources into the same language model[REF_CITE]."
"The goal of the ME principle is that, given a set of features (pieces of desired informa-tion contained in the sentence), a set of functions 5  5 7 (measuring the contribution of each feature to the model) and a set of constraints [Footnote_1] , we have to find the probability distribution that satis-fies the constraints and minimizes the relative en-tropy (Divergence of Kullback-Leibler) 8  (;: , with respect to the distribution ;: ."
1 The constraints usually involve the equality between theoretical expectation and the empirical expectation over the training corpus.
"The general Maximum Entropy probability dis-tribution relative to a prior distribution : is given by the expression:  &lt;&amp ; =, ;:&gt; &lt;@?BADCE9FHG I EKJLENM(OQP (3) = where is the normalization constant and R are parameters to be found."
The R represent the con-tribution of each feature to the distribution.
"From (3) it is easy to derive the Maximum Entropy conditional language model[REF_CITE]: if S is the context space and T is the vocabulary   ,xthen T then S x: T is the states space, and if  W U [ = , U ?BADCE9F&quot;GBI EKJLENM]\&lt;^_`P (4) and = U :  U [2c BADdE9F&quot;GBI E JQEeM9\&lt;^`P_? (5) _ where  U is the normalization constant depend-ing on the context U ."
"Although the conditional ME language model is more flexible than n-gram models, there is an important obstacle to its gen-eral use: conditional ME language models have a high computational cost[REF_CITE], spe-cially the evaluation of the normalization constant (5)."
"Although we can incorporate local information (like n-grams) and some kinds of long-distance information (like triggers) within the conditional ME model, the global information contained in the sentence is poorly encoded in the ME model, as happens with the other conditional models."
There is a language model which is able to take advantage of the local information and at the same time allows for the use of the global properties of the sentence: the Whole Sentence Maximum En-tropy model (WSME)[REF_CITE].
"We can include classical information such us n-grams, distance n-grams or triggers and global proper-ties of the sentence, as features into the WSME framework."
"Besides the fact that the WSME model training procedure is less expensive than the conditional ME model, the most important training step is based on well-developed statisti-cal sampling techniques."
"In recent works[REF_CITE], WSME models have been successfully trained using features of n-grams and distance n-grams."
"In this work, we propose adding information to the WSME model which is provided by the gram-matical structure of the sentence."
The informa-tion is added in the form of features by means of a Stochastic Context-Free Grammar (SCFG).
The grammatical information is combined with features of n-grams and triggers.
"In section 2, we describe the WSME model and the training procedure in order to estimate the pa-rameters of the model."
"In section 3, we define the grammatical features and the way of obtaining them from the SCFG."
"Finally, section 4 presents the experiments carried out using a part of the Wall Street Journal in order evalute the behavior of this proposal."
The whole sentence Maximum Entropy model di-rectly models the probability distribution of the complete sentence [Footnote_2] .
"2 By sentence, we understand any sequence of linguistic units that belongs to a certain vocabulary."
The WSME language model has the form of (3).
"In order to simplify the notation we write f #g ? I E , and define: 7 h &lt;[ f JLENMiOLP (6) so (3) is written as:  & lt;[ =, ;: &lt; h (7) where is a sentence and the f are now the pa-rameters to be learned."
The training procedure to estimate the parame-ters of the model is the Improved Iterative Scaling algorithmn (IIS)[REF_CITE].
"IIS is based on the change of the log-likelihood over the training corpus k , when each of the parameters changes from R to R . ,  .X o ."
Mathematical considerations on the change in the log-likelihood give the training equation:  & lt; 5 @mp? EKJrqVM(OQP * sVtc &lt;uwv &lt; 5 &lt;[yx c (8) O {z &lt; A 7 5 &lt; 5 where .
"In each iteration of the IIS, we have to find the value of the improve-ment n in the parameters, solving (8) with respect to  for each | }, ~ ."
The main obstacle in the WSME training pro-cess resides in the calculation of the first sum in (8).
The sum extends over all the sentences of a given length.
"The great number of such sen-tences makes it impossible, from computing per-spective, to calculate the sum, even for a moderate length [Footnote_3] ."
3 the number of sentences  of length  is   
"Nevertheless, such a sum is the statisti-cal expected value of a function 5 of with respect to the distribution :  ? p EKJ qV ."
"As is well known, it could be estimated using the sampling expectation as:   5 ? p E J q  &apos; ,  5   JrqMiO P (9) where # is a random sample from and  ? p E .  ="
"Note that in (7) the constant is unknown, so direct sampling from is not possible."
"In sampling from such types of probability distribu-tions, the Monte Carlo Markov Chain (MCMC) sampling methods have been successfully used when the distribition is not totally known[REF_CITE]."
MCMC are based on the convergence of certain Markov Chains to a target distribution .
"In MCMC, a path of the Markov chain is ran for a long time, after which the visited states are considered as a sampling element."
"The MCMC sampling methods have been used in the param-eter estimation of the WSME language models, specially the Independence Metropolis-Hasting (IMH) and the Gibb’s sampling algorithms[REF_CITE]."
The best results have been obtainded using the (IMH) algorithm.
"Although MCMC performs well, the distribu-tion from which the sample is obtained is only an approximation of the target sampling distribution."
"Therefore samples obtained from such distribu-tions may produce some bias in sample statis-tics, like sampling mean."
"Recently, another sam-pling technique which is also based on Markov Chains has been developed by Propp and Wils[REF_CITE], the Perfect Sampling (PS) technique."
PS is based on the concept of Coupling From the Past.
"In PS, several paths of the Markov chain are running from the past (one path in each state of the chain)."
"In all the paths, the transition rule of the Markov chain uses the same set of random numbers to transit from one state to another."
"Thus if two paths coincide in the same state in time  , they will remain in the same states the rest of the time."
"In such a case, we say that the two paths are collapsed."
"Now, if all the paths collapse at any given time, from that point in time, we are sure that we are sampling from the true target distribution ."
"The Coupling From the Past algorithm, systematically goes to the past and then runs paths in all states and repeats this procedure until a time  has been found."
"Once  has been found, the paths that be-gin in time Y* all paths collapse at time x ."
"Then we run a path of the chain from the state at time * to the actual time ( x ), and the last state arrived is a sample from the target distribution."
"The reason for going from past to current time is technical, and is detailed[REF_CITE]."
"If the state space is huge (as is the case where the state space is the set of all sentences), we must define a stochastic order over the state space and then run only two paths: one beginning in the minimum state and the other in the maximum state, following the same mecha-nism described above for the two paths until they collapse."
"In this way, it is proved that we get a sample from the exact target distribution and not from an approximate distribution as in MCMC algorithms[REF_CITE]."
"Thus, we hope that in samples generated with perfect sam-pling, statistical parameter estimators may be less biased than those generated with MCMC."
"Recently[REF_CITE], the PS was successfully used to estimate the param-eters of a WSME language model ."
"In that work, a comparison was made between the per-formance of WSME models trained using MCMC and WSME models trained using PS."
"Features of n-grams and features of triggers were used In both kinds of models, and the WSME model trained with PS had better performance."
We then consid-ered it appropriate to use PS in the training proce-dure of the WSME.
The model parameters were completed with the estimation of the global normalization = constant h &lt; =.
"Using (7), we can = deduce that y r&amp; and thus estimate using the sampling expecta-tion.  h &lt; , h  ` c where  is a random sample from : . ; Because : , is easywetohavesample  totalfromcontrolit inoverthe traditionalthe distribitionway."
The main goal of this paper is the incorporation of gramatical features to the WSME.
Grammatical information may be helpful in many aplications of computational linguistics.
"The grammatical structure of the sentence provides long-distance information to the model, thereby complementing the information provided by other sources and im-proving the performance of the model."
"Grammat-ical features give a better weight to such param-eters in grammatically correct sentences than in grammatically incorrect sentences, thereby help-ing the model to assign better probabilities to cor-rect sentences from the language of the applica- tion."
"To capture the grammatical information, we use Stochastic Context-Free Grammars (SCFG)."
"Over the last decade, there has been an increas-ing interest in Stochastic Context-Free Grammars (SCFGs) for use in different tasks (K., 1979;[REF_CITE])."
"The reason for this can be found in the capa-bility of SCFGs to model the long-term depen-dencies established between the different lexical units of a sentence, and the possibility to incor-porate the stochastic information that allows for an adequate modeling of the variability phenom-ena."
"Thus, SCFGs have been successfully used on limited-domain tasks of low perplexity."
"However, SCFGs work poorly for large vocabulary, general-purpose tasks, because the parameter learning and the computation of word transition probabilities present serious problems for complex real tasks."
"To capture the long-term relations and to solve the main problem derived from the use of SCFGs in large-vocabulary complex tasks,we consider the proposal[REF_CITE]: de-fine a category-based SCFG and a probabilistic model of word distribution in the categories."
"The use of categories as terminal of the grammar re-duces the number of rules to take into account and thus, the time complexity of the SCFG learning procedure."
"The use of the probabilistic model of word distribution in the categories, allows us to obtain the best derivation of the sentences in the application."
"Actually, we have to solve two problems: the estimation of the parameters of the models and their integration to obtain the best derivation of a sentence."
The parameters of the two models are esti-mated from a training sample.
Each word in the training sample has a part-of-speech tag (POStag) associated to it.
These POStags are considered as word categories and are the terminal symbols of our SCFG.
"Given a category, the probability distribution of a word is estimated by means of the relative fre-quency of the word in the category, i.e. the rela-tive frequency which the word has been labeled with a POStag (a word may belong to different categories)."
"To estimate the SCFG parameters, several al-gorithms have been presented (K. and S.J., 1991;"
"Taking into account the good results achieved on real tasks[REF_CITE], we used them to learn our category-based SCFG."
"To solve the integration problem, we used an algorithm that computes the probability of the best derivation that generates a sentence, given the category-based grammar and the model of word distribution into categories[REF_CITE]."
This algorithm is based on the well-known Viterbi-like scheme for SCFGs.
"Once the grammatical framework is defined, we are in position to make use of the informa-tion provided by the SCFG."
"In order to define the grammatical features, we first introduce some no-tation.  A  Context-Free, whereGrammar  is the finiteG isseta offour-tuplenon ter-minals, ¡ is a finite set of terminals ( }¥¡}§¦ &gt; , ¤ X  is the initial symbol of the grammar and £ ¨ª© is the finite « set of ¨ productions X  and « or X rules  of the @¯ .formWewhere consider only context-free grammars in Chomsky normal ¨°© form ±² , that is ¨°© grammars ³ with ¨ 6 rules ± 6 ² of X the  or where form ³ and X ¡ ."
A Stochastic Context-Free Gramar ´
O is a pair ´  where ´ is a context-free grammar and is a probability distribution over the grammar rules.
"The grammatical features are defined as fol-lows: let   , a sentence of the train-ing set."
"As mentioned above, we can compute the best derivation of the sentence , using the defined SCFG and obtain the parse tree of the sentence."
"Once we have the parse tree of all the sentences in the training corpus, we can collect the set of all the production rules used in the derivation of the sentences in the corpus. ¶  Formally a : © UbW we ;¸ , wheredefine  the set X   .  &lt; is the set of all grammatical rules used in the derivation of . ¨ To X  includeand ³ the X ¡ rules, in theof setthe  form, ¨y©º³ , where we make use of a special symbol $ which is not in the terminals ¨»©½¼ nor in the non-terminals."
"If a rule of the form occurs in the derivation tree of ¨ , the 6 ¼ 6¢¾ corresponding."
"The set ¿ element ¬ O t&lt;u  in  &lt; is written as (where k is the corpus), is the set of grammatical features.  is the set representation of the grammati-cal information contained in the derivation trees of the sentences and may be incorporated to the WSME model by means of the characteristic functions defined as: if ; .  5 M]\&lt;^ r_ ^ ÀP &amp;¿Á ,x (10) Othewise"
"Thus, whenever the WSME model processes a sentence , if it is looking for a specific gram-matial feature, say ; , we get the derivation tree for and the set  &lt; is calculated from the derivation tree."
"Finally, the model asks if the the tuple ; is an element of  feature is active; if not, the feature  . ; If  it is,doesthe not contribute to the sentence probability."
"There-fore, a sentence may be a grammatically incorrect sentence (relative to the SCFG used), if deriva-tions with low frequency appears."
A part of the Wall Street Journal (WSJ) which had been processed in the Penn Treebanck Project[REF_CITE]was used in the experiments.
This corpus was automatically labelled and man-ually checked.
There were two kinds of labelling: POStag labelling and syntactic labelling.
The POStag vocabulary was composed of 45 labels.
The syntactic labels are 14.
The corpus was di-vided into sentences according to the bracketing.
"The sets are described as follow: the training cor-pus has 11,201 sentences; the test set has 6,350 sentences and the held-out set has 5,796 sen-tences."
A base-line Katz back-off smoothed trigram model was trained using the CMU-Cambridge statistical Language Modeling Toolkit [Footnote_4] and used as prior distribution in (3) i.e. ;: .
4 Available at[URL_CITE]
The vocabu-lary generated by the trigram model was used as vocabulary of the WSME model.
"The size of the vocabulary was 19,997 words."
The estimation of the word-category probabil-ity distribution was computed from the training corpus.
"In order to avoid null values, the unseen events were labeled with a special “unknown” symbol which did not appear in the vocabulary, so that the probabilitie of the unseen envent were positive for all the categories."
The SCFG had the maximum number of rules which can be composed of 45 terminal symbols (the number of POStags) and 14 non-terminal symbols (the number of syntactic labels).
The initial probabilities were randomly generated and three different seeds were tested.
"However, only one of them is here given that the results were very similar."
"The size of the sample used in the ISS was es-timated by means of an experimental procedure and was set at 10,000 elements."
"The procedure used to generate the sample made use of the “di-agnosis of convergence”[REF_CITE], a method by means of which an inicial portion of each run of the Markov chain of sufficient length is dis-carded."
"Thus, the states in the remaining portion come from the desired equilibrium distribution."
"In this work, a discarded portion of 3,000 ele-ments was establiched."
"Thus in practice, we have to generate 13,000 instances of the Markov chain."
"During the IIS, every sample was tagged using the grammar estimated above, and then the gram-matical features were extracted, before combining them with other kinds of features."
The adequate number of iterations of the IIS was established ex-perimentally in 13.
We trained several WSME models using the Perfect Sampling algorithm in the IIS and a dif-ferent set of features (including the grammatical features) for each model.
"The different sets of features used in the models were: n-grams (1-grams,2-grams,3-grams); triggers; n-grams and grammatical features; triggers and grammatical feautres; n-grams, triggers and grammatical fea-tures."
"The ) -gram features,(N), was selected by means of its frequency in the corpus."
"We select all the unigrams, the bigrams with frequency greater than 5 and the trigrams with frequency greater than 10, in order to mantain the proportion of each type of ) -gram in the corpus."
"The triggers, (T), were generated using a trig- models with grammatical features and models without grammatical features for WSME mod-els over part of the WSJ corpus."
"N means fea-tures of n-grams, T means features of Triggers."
The perplexity of the trained n-gram model was PP=162.049 ger toolkit developed by Adam Berger [Footnote_5] .
5 Available at: htpp://[URL_CITE]
The triggers were selected in acordance with de mu-tual information.
The triggers selected were those with mutual information greater than 0.0001.
"The grammatical features, (G), were selected using the parser tree of all the sentences in the training corpus to obtain the sets  and their union  as defined in section 3."
"The size of the initial set of features was: 12,023 ) -grams, 39,428 triggers and 258 gramati-cal features, in total 51,709 features."
"At the end of the training procedure, the number of active fea-tures was significantly reduced to 4,000 features on average."
"During the training procedure, some of the f &apos; x and, so, we smooth the model."
We smoothed it using a gaussian prior technique.
"In the gaussian technique, we assumed that the f paramters had a gaussian (normal) prior probabil-ity distributi[REF_CITE]and found the maximum aposteriori parameter distri-bution."
"The prior distribution was f #Å Æ x 6 Ç , and we used the held-out data to find the Ç pa-rameters."
Table 1 shows the experimental results: the first row represents the set of features used.
The second row shows the perplexity of the models without using grammatical features.
The third row shows the perplexity of the models using grammatical features and the fourth row shows the improvement in perplexity of each model us-ing grammatical features over the corresponding model without grammatical features.
"As can be seen in Table 1, all the WSME models performed better than the ) -gram model, however that is nat-ural because, in the worst case (if all f È, ), the WSME models perform like the ) -gram model."
"In Table 1, we see that all the models us-ing grammatical features perform better than the models that do not use it."
"Since the training pro-cedure was the same for all the models described and since the only difference between the two kinds of models compared were the grammatical features, then we conclude that the improvement must be due to the inclusion of such features into the set of features."
The average percentage of im-provement was about 13%.
"Also, although the model N+T performs bet-ter than the other model without grammatical fea-tures (N,T), it behaves worse than all the models with grammatical features ( N+G improved 2.9% and T+G improvd 5.9% over N+T)."
"In this work, we have sucessfully added gram-matical features to a WSME language model us-ing a SCFG to extract the grammatical informa-tion."
We have shown that the the use of gram-matical features in a WSME model improves the performance of the model.
Adding grammatical features to the WSME model we have obtained a reduction in perplexity of 13% on average over models that do not use grammatical features.
Also a reduction in perplexity between approximately 22% and 28% over the n-gram model has been obtained.
We are working on the implementation of other kinds of grammatical features which are based on the POStags sentences obtained using the SCFG that we have defined.
The prelimary experiments have shown promising results.
We will also be working on the evaluation of the word-error rate (WER) of the WSME model.
In the case of WSME model the WER may be evaluated in a type of post-procesing using the n-best utterances.
"In this paper, we compare the rela-tive effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system."
"We take a selec-tion of both bag-of-words and segment order-sensitive string comparison meth-ods, and run each over both character-and word-segmented data, in combina-tion with a range of local segment con-tiguity models (in the form of N-grams)."
"Over two distinct datasets, we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models."
"Further, in their optimum configuration, bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy, but much faster."
We also pro-vide evidence that our findings are scal-able.
"Translation memories (TMs) are a list of translation records (source language strings paired with a unique target language translation), which the TM system accesses in suggesting a list of target language (L2) translation candi-dates for a given source language (L1) input[REF_CITE]."
Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.
"Typi-cally in example-based machine translation, either a single TRec is retrieved from the TM based on a match with the overall L1 input, or the input is partitioned into coherent segments, and indi-vidual translations retrieved for each[REF_CITE]; this is the first step toward generating a customised transla-tion for the input."
"With stand-alone TM systems, on the other hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the final translation."
"A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the final retrieval accu-racy will become."
"Naturally, any appreciation in retrieval complexity comes at a price in terms of computational overhead."
We thus follow the lead[REF_CITE]in asking the ques-tion: what is the empirical effect on retrieval per-formance of different match approaches?
"Here, retrieval performance is defined as the combina-tion of retrieval speed and accuracy, with the ideal method offering fast response times at high accu-racy."
"In this paper, we choose to focus on retrieval performance within a Japanese–English TR con-text."
One key area of interest with Japanese is the effect that segmentation has on retrieval performance.
"As Japanese is a non-segmenting language (does not explicitly delimit words or-thographically), we can take the brute-force ap-proach in treating each string as a sequence of characters (character-based indexing), or al-ternatively call upon segmentation technology in partitioning each string into words (word-based indexing)."
Orthogonal to this is the question of sensitivity to segment order.
"That is, should our match mechanism treat each string as an unor-ganised multiset of terms (the bag-of-words ap-proach), or attempt to find the match that best preserves the original segment order in the in-put (the segment order-sensitive approach)?"
We tackle this issue by implementing a sample of representative bag-of-words and segment order-sensitive methods and testing the retrieval per-formance of each.
"As a third orthogonal param-eter, we consider the effects of segment contigu-ity."
"That is, do matches over contiguous segments provide closer overall translation correspondence than matches over displaced segments?"
"Segment contiguity is either explicitly modelled within the string match mechanism, or provided as an add-in in the form of segment N-grams."
"To preempt the major findings of this pa-per, over a series of experiments we find that character-based indexing is consistently superior to word-based indexing."
"Furthermore, the bag-of-words methods we test are equivalent in re-trieval accuracy to the more expensive segment order-sensitive methods, but superior in retrieval speed."
"Finally, segment contiguity models provide benefits in terms of both retrieval accuracy and retrieval speed, particularly when coupled with character-based indexing."
"We thus provide clear evidence that high-performance TR is achievable with naive methods, and moreso that such meth-ods outperform more intricate, expensive meth-ods."
"That is, the dumber the retrieval mechanism, the better."
"Below, we review the orthogonal parameters of segmentation, segment order and segment conti-guity (§ 2)."
We then present a range of both bag- of-words and segment order-sensitive string com-parison methods (§ 3) and detail the evaluation methodology (§ 4).
"Finally, we evaluate the dif-ferent methods in a Japanese–English TR context (§ 5), before concluding the paper (§ 6)."
"In this section, we review three parameter types that we suggest impinge on TR performance, namely segmentation, segment order, and segment contiguity."
"Despite non-segmenting languages such as Japanese not making use of segment delimiters, it is possible to artificially partition off a given string into constituent morphemes through the process of segmentation."
We will collectively term the resultant segments as words for the remainder of this paper.
"Looking to past research on string compari-son methods for TM systems, almost all sys-tems involving Japanese as the source lan-guage rely on segmentati[REF_CITE],[REF_CITE]and[REF_CITE]providing rare in-stances of character-based systems."
This is despite[REF_CITE]providing evi-dence from Japanese information retrieval that character-based indexing performs comparably to word-based indexing.
"In analogous research,[REF_CITE]compared character-and word-based indexing within a Japanese– English TR context and found character-based in-dexing to hold a slight empirical advantage."
The most obvious advantage of character-based indexing over word-based indexing is that there is no pre-processing overhead.
Other arguments for character-based indexing over word-based in-dexing are that we: (a) avoid the need to com-mit ourselves to a particular analysis type in the case of ambiguity or unknown words; (b) avoid the need for stemming/lemmatisation; and (c) to a large extent get around problems related to the normalisation of lexical alternation.
Note that all methods described below are ap-plicable to both word- and character-based index-ing.
"To avoid confusion between the two lexeme types, we will collectively refer to the elements of indexing as segments."
Our expectation is that TRecs that preserve the segment order observed in the input string will provide closer-matching translations than TRecs containing those same segments in a different or-der.
"As far as we are aware, there is no TM sys-tem operating from Japanese that does not rely on word/segment/character order to some degree."
"Given the input α 1 α 2 α 3 α 4 , we would expect that of α 1 β 1 α 2 β 2 α 3 β 3 α 4 and α 1 α 2 α 3 α 4 β 1 β 2 β 3 , the latter would provide a translation more reflective of the translation for the input."
"This intuition is captured either by embedding some contiguity weighting facility within the string match mecha-nism (in the case of weighted sequential correspon-dence — see below), or providing an independent model of segment contiguity in the form of seg-ment N-grams."
"The particular N-gram orders we test are simple unigrams (1-grams), pure bigrams (2-grams), and mixed unigrams/bigrams."
"These N-gram models are implemented as a pre-processing stage, fol-lowing segmentation (where applicable)."
"All this involves is mutating the original strings into N-grams of the desired order, while preserving the original segment order and segmentation schema."
"From the Japanese string 夏 · の · 雨 [natu·no·ame] “summer rain”, 1 for example, we would generate the following variants (common to both character-and word-based indexing): [Footnote_1]-gram: 夏 · の · 雨 2-gram: 夏の · の雨"
1 Character boundaries (which double as word boundaries in this case) indicated by “·”.
Mixed [Footnote_1]/2-gram: 夏 · 夏の · の · の雨 · 雨
1 Character boundaries (which double as word boundaries in this case) indicated by “·”.
"As the starting point for evaluation of the three parameter types targeted in this re-search, we take two bag-of-words (segment order-oblivious) and three segment order-sensitive meth-ods, thereby modelling the effects of segment or-der (un)awareness."
"We then run each method over both segmented and unsegmented data in combi-nation with the various N-gram models proposed above, to capture the full range of parameter set-tings."
The particular bag-of-word approaches we tar-get are the vector space model ([REF_CITE]p300) and “token intersection”.
"For segment order-sensitive approaches, we test 3-operation edit distance and similarity, and also “weighted sequential correspondence”."
"All methods are formulated to operate over an arbitrary wt schemata, although in L1 string com-parison throughout this paper, we assume that any segment made up entirely of punctuation is given a wt of 0, and any other segment a wt of 1."
"All methods are subject to a threshold on translation utility, and in the case that the threshold is not achieved, the null string is re-turned."
"The various thresholds are as follows: where IN is the input string, and len is the con-ventional segment length operator."
"Various optimisations were made to each string comparison method to reduce retrieval time, of the type described[REF_CITE]."
"While the details are beyond the scope of this pa-per, suffice to say that the segment order-sensitive methods benefited from the greatest optimisation, and that little was done to accelerate the already quick bag-of-words methods."
"As our main dataset, we used 3033 unique Japanese–English TRecs extracted from construc-tion machinery field reports for the purposes of this research."
"Most TRecs comprise a single sen-tence, with an average Japanese character length of 27.7 and English word length of 13.3."
"Impor-tantly, our dataset constitutes a controlled lan-guage, that is, a given word will tend to be trans-lated identically across all usages, and only a lim-ited range of syntactic constructions are employed."
"In secondary evaluation of retrieval performance over differing data sizes, we extracted 61,236 Japanese–English TRecs from the JEIDA parallel corpus[REF_CITE], which is made up of gov-ernment white papers."
"The alignment granular-ity of this second corpus is much coarser than for the first corpus, with a single TRec often extend-ing over multiple sentences."
"The average Japanese character length of each[REF_CITE].3, and the av-erage English word length is 35.7."
"The language used in the JEIDA corpus is highly constrained, although not as controlled as that in the first cor-pus."
"The construction of TRecs from both corpora was based on existing alignment data, and no fur-ther effort was made to subdivide partitions."
"For Japanese word-based indexing, segmenta-tion was carried out primarily with ChaSen v2.0[REF_CITE], and where specifically mentioned, JUMAN v3.5[REF_CITE]and ALTJAWS [URL_CITE] were also used."
Retrieval accuracy was determined by way of 10-fold semi-stratified cross validation over the dataset.
"As part of this, all Japanese strings of length 5 characters or less were extracted from the dataset, and cross validation was performed over the residue, including the shorter strings in the training data (i.e. TM) on each iteration."
"In N-fold stratified cross validation, the dataset is divided into N equally-sized partitions of uni-form class distribution."
"Evaluation is then carried out N times, taking each partition as the held-out test data, and the remaining partitions as the training data on each iteration; the overall accu-racy is averaged over the N data configurations."
"As our dataset is not pre-classified according to a discrete class description, we are not able to per-form true data stratification over the class distri-bution."
"Instead, we carry out “semi-stratification” over the L1 segment lengths of the TRecs."
Evaluation of retrieval accuracy is carried out ac-cording to a modified version of the method pro-posed[REF_CITE].
"The first step in this process is to determine the set of “op-timal” translations by way of the same basic TR procedure as described above, except that we use the held-out translation for each input to search through the L2 component of the TM."
"As for L1 TR, a threshold on translation utility is then ap-plied to ascertain whether the optimal translations are similar enough to the model translation to be of use, and in the case that this threshold is not achieved, the empty string is returned as the sole optimal translation."
"Next, we proceed to ascertain whether the ac-tual system output coincides with one of the opti-mal translations, and rate the accuracy of each method according to the proportion of optimal outputs."
"If multiple outputs are produced, we se-lect from among them randomly."
"This guaran-tees a unique translation output and differs from the methodology[REF_CITE], who judged the system output to be “correct” if the potentially multiple set of top-ranking outputs contained an optimal translation, placing methods with greater fan-out of outputs at an advantage."
"So as to filter out any bias towards a given string comparison method in TR, we determine transla-tion optimality based on both 3-operation edit dis-tance (operating over English word bigrams) and also weighted sequential correspondence (operat-ing over English word unigrams)."
We then de-rive the final translation accuracy as the average of the accuracies from the respective evaluation sets.
"Here again, our approach differs from that[REF_CITE], who based deter-mination of translation optimality exclusively on 3-operation edit distance (operating over word un-igrams), a method which we found to produce a strong bias toward 3-operation edit distance in L1 TR."
"In determining translation optimality, all punc-tuation and stop words were first filtered out of each L2 (English) string, and all remaining seg-ments scored at a wt of 1."
Stop words are defined as those contained within the SMART[REF_CITE]stop word list. [URL_CITE]
"Perhaps the main drawback of our approach to evaluation is that we assume a unique model translation for each input, where in fact, multiple translations of equivalent quality could reasonably be expected to exist."
"In our case, however, both corpora represent relatively controlled languages and language use is hence highly predictable."
The proposed evaluation methodology is thus justified.
"In this section, we test our five string comparison methods over the construction machinery corpus, under both character- and word-based indexing, and with each of unigrams, bigrams and mixed unigrams/bigrams."
"The retrieval accuracies and times for the different string comparison meth-ods are presented in Figs. 1 and 2, respectively."
"Here and in subsequent graphs, “VSM” refers to the vector space model, “TINT” to token inter-section, “3opD” to 3-op edit distance, “3opS” to 3-op edit similarity, and “WSC” to weighted se-quential correspondence; the bag-of-words meth-ods are labelled in italics and the segment order-sensitive methods in bold."
"In Figs. 1 and 2, results for the three N-gram models are presented sepa-rately, within each of which, the data is sectioned off into the different string comparison methods."
"Weighted sequential correspondence was tested with a unigram model only, due to its inbuilt mod-elling of segment contiguity."
Bars marked with an asterisk indicate a statistically significant [Footnote_5] gain over the corresponding indexing paradigm (i.e. character-based indexing vs. word-based indexing for a given string comparison method and N-gram order).
5 As determined by the paired t test (p &lt; 0.05).
"Times in Fig. 2 are calibrated relative to 3-operation edit distance with word unigrams, and plotted against a logarithmic time axis."
"Results to come from these figures can be sum-marised as follows: • Character-based indexing is consistently su-perior to word-based indexing, particularly when combined with bigrams or mixed uni-grams/bigrams. • In terms of raw translation accuracy, there is very little to separate the best of the bag-of-words methods from the best of the segment order-sensitive methods. • With character-based indexing, bigrams offer tangible gains in translation accuracy at the same time as greatly accelerating the retrieval process."
"With word-based indexing, mixed unigrams/bigrams offer the best balance of translation accuracy and computational cost. • Weighted sequential correspondence is mod-erately successful in terms of accuracy, but grossly expensive."
"Based on the above results, we judge bi-grams to be the best segment contiguity model for character-based indexing, and mixed uni-grams/bigrams to be the best segment contiguity model for word-based indexing, and for the re-mainder of this paper, present only these two sets of results."
"While we have been able to confirm the find-ing[REF_CITE]that character-based indexing is superior to word-based indexing, we are no closer to determining why this should be the case."
"In the following sections we look to shed some light on this issue by considering each of: (i) the retrieval accuracy for other segmentation sys-tems, (ii) the effects of lexical normalisation, and (iii) the scalability and reproducibility of the given results over different datasets."
"Finally, we present a brief qualitative explanation for the overall re-sults."
"Above, we observed that segmentation consis-tently brought about a degradation in translation retrieval for the given dataset."
"Automated seg-mentation inevitably leads to errors, which could possibly impinge on the accuracy of word-based indexing."
"Alternatively, the performance drop could simply be caused somehow by our particular choice of segmentation module, that is ChaSen."
"First, we used JUMAN to segment the con-struction machinery corpus, and evaluated the re-sultant dataset in the exact same manner as for the ChaSen output."
"Similarly, we ran a devel-opment version of ALTJAWS over the same cor-pus to produce two datasets, the first simply seg-mented and the second both segmented and lex-ically normalised."
"By lexical normalisation, we mean that each word is converted to its canonical form."
"The main segment types that normalisation has an effect on are verbs and adjectives (conju-gating words), and also loan-word nouns with an optional long final vowel (e.g. monitā “monitor” ⇒ monita) and words with multiple inter-replaceable kanji realisations (e.g. 充分 [zyūbuN] “sufficient” ⇒ 十分 )."
"The retrieval accuracies for JUMAN, and ALT-JAWS with and without lexical normalisation are presented in Fig. 3, juxtaposed against the retrieval accuracies for character-based in-dexing (bigrams) and also ChaSen (mixed uni-grams/bigrams) from Section 5.1."
Asterisked bars indicate a statistically significant gain in accuracy over ChaSen.
"Looking first to the results for JUMAN, there is a gain in accuracy over ChaSen for all string com-parison methods."
"With ALTJAWS, also, a con-sistent gain in performance is evident with simple segmentation, the degree of which is significantly higher than for JUMAN."
The addition of lexi-cal normalisation enhances this effect marginally.
Notice that character-based indexing (based on character bigrams) holds a clear advantage over the best of the word-based indexing results for all string comparison methods.
"Based on the above, we can state that the choice of segmentation system does have a modest im-pact on retrieval accuracy, but that the effects of lexical normalisation are highly localised."
"In the following, we look to quantify the relationship be-tween retrieval and segmentation accuracy."
"In the next step of evaluation, we took a random sample of 200 TRecs from the original dataset, and ran each of ChaSen, JUMAN and ALTJAWS over the Japanese component of each."
"We then man-ually evaluated the output in terms of segment precision and recall, defined respectively as: # correct segs in output Segment precision ="
Total # segs in output # correct segs in output Segment recall =
Total # segs in model data
One slight complication in evaluating the out-put of the three systems is that they adopt in-congruent models of conjugation.
"We thus made allowance for variation in the analysis of verb and adjective complexes, and focused on the segmen-tation of noun complexes."
"A performance breakdown for ChaSen (CS), JUMAN (JM) and ALTJAWS (AJ) is presented in Tab. 1. ALTJAWS was found to outperform the remaining two systems in terms of segment pre-cision, while ChaSen and JUMAN performed at the exact same level of segment precision."
"Look-ing next to segment recall, ChaSen significantly outperformed both ALTJAWS and JUMAN."
"The source of almost all errors in recall, and roughly half of errors in precision for both ChaSen and"
"JUMAN was katakana sequences such as gēto-rokku-barubu “gate-lock valve”, transcribed from English."
"ALTJAWS, on the other hand, was re-markably successful at segmenting katakana word sequences, achieving a segment precision of 100% and segment recall approaching 99%."
"This is thought to have been the main cause for the dis-parity in retrieval accuracy for the three systems, aggravated by the fact that most katakana se-quences were key technical terms."
"To gain an insight into consistency in the case of error, we further calculated the total number of segment types in the output, expecting to find a core set of correctly-analysed segments, of rel-atively constant size across the different systems, plus an unpredictable component of segment er-rors, of variable size."
The system generating the fewest segment types can thus be said to be the most consistent.
"Based on the segment type counts in Tab. 1, ALTJAWS errs more consistently than the re-maining two systems, and there is very little to separate ChaSen and JUMAN."
This is thought to have had some impact on the inflated retrieval ac-curacy for ALTJAWS.
"To summarise, there would seem to be a di-rect correlation between segmentation accuracy and retrieval performance, with segmentation ac-curacy on key terms (katakana sequences) having a particularly keen effect on translation retrieval."
"In this respect, ALTJAWS is superior to both ChaSen and JUMAN for the target domain."
"Ad-ditionally, complementing segmentation with lex-ical normalisation would seem to produce meager performance gains."
"Lastly, despite the slight gains to word-based indexing with the different segmen-tation systems, it is still significantly inferior to character-based indexing."
All results to date have arisen from evaluation over a single dataset of fixed size.
"In order to validate the basic findings from above and observe how increases in the data size affect retrieval perfor-mance, we next ran the string comparison meth-ods over differing-sized subsets of the JEIDA cor-pus."
"We simulate TMs of differing size by randomly splitting the JEIDA corpus into ten partitions, and running the various methods first over par-tition 1, then over the combined partitions 1 and 2, and so on until all ten partitions are combined together into the full corpus."
We tested all string comparison methods other than weighted sequen-tial correspondence over the ten subsets of the JEIDA corpus.
Weighted sequential correspon-dence was excluded from evaluation due to its overall sub-standard retrieval performance.
"The translation accuracies for the different methods over the ten datasets of varying size, are indicated in Fig. 4, with each string comparison method tested under character bigrams (“2-gram −seg”) and mixed word unigrams/bigrams (“1/2-gram +seg”) as above."
The results for token intersec-tion have been omitted from the graph due to their being almost identical to those for VSM.
"A striking feature of the graph is that it is right-decreasing, which is essentially an artifact of the inflated length of each TRec (see Section 4.1) and resultant data sparseness."
"That is, for smaller datasets, in the bulk of cases, no TRec in the TM is similar enough to the input to warrant consid-eration as a translation candidate (i.e. the trans-lation utility threshold is generally not achieved)."
"For larger datasets, on the other hand, we are hav-ing to make more subtle choices as to the final translation candidate."
"One key trend in Fig. 4 is the superiority of character- over word-based indexing for each of the three string comparison methods, at a rela-tively constant level as the TM size grows."
Also of interest is the finding that there is very little to distinguish bag-of-words from segment order-sensitive methods in terms of retrieval accuracy in their respective best configurations.
"As with the original dataset from above, 3-operation edit similarity was the strongest per-former just nosing out (character bigram-based) VSM for line honours, with 3-operation edit dis-tance lagging well behind."
"Next, we turn to consider the mean unit re-trieval times for each method, under the two in-dexing paradigms."
"Times are presented in Fig. 5, plotted once again on a logarithmic scale in order to fit the full fan-out of retrieval times onto a single graph."
"VSM and 3-operation edit distance were the most consistent performers, both maintaining retrieval speeds in line with those for the original dataset at around or under 1.0 (i.e. the same re-trieval time per input as 3-operation edit distance run over word unigrams for the construction ma-chinery dataset)."
"Most importantly, only minor increases in retrieval speed were evident as the TM size increased, which were then reversed for the larger datasets."
"All three string comparison methods displayed this convex shape, although the final running time for 3-operation edit simi-larity under character- and word-based indexing was, respectively, around 10 and 100 times slower than that for VSM or 3-operation edit distance over the same dataset."
"To combine the findings for accuracy and speed, VSM under character-based indexing suggests it-self as the pick of the different system configura-tions, combining both speed and consistent accu-racy."
"That is, it offers the best overall retrieval performance."
"Above, we established that character-based index-ing is superior to word-based indexing for distinct datasets and a range of segmentation modules, even when segmentation is coupled with lexical normalisation."
"Additionally, we provided evidence to the effect that bag-of-words methods offer supe-rior translation retrieval performance to segment order-sensitive methods."
"We are still no closer, however, to determining why this should be the case."
"Here, we seek to provide an explanation for these intriguing results."
"First comparing character- and word-based in-dexing, we found that the disparity in retrieval accuracy was largely related to the scoring of katakana words, which are significantly longer in character length than native Japanese words."
"For the construction machinery dataset as analysed with ChaSen, for example, the average charac-ter length of katakana words is 3.62, as com-pared to 2.05 overall."
"Under word-based index-ing, all words are treated equally and character length does not enter into calculations."
Thus a katakana word is treated identically to any other word type.
"Under character-based index-ing, on the other hand, the longer the word, the more segments it generates, and a single matching katakana sequence thus tends to contribute more heavily to the final score than other words."
"Ef-fectively, therefore, katakana sequences receive a higher score than kanji and other sequences, pro-ducing a preference for TRecs which incorporate the same katakana sequences as the input."
"As noted above, katakana sequences generally repre-sent key technical terms, and such weighting thus tends to be beneficial to retrieval accuracy."
We next examine the reason for the high corre-lation in retrieval accuracy between bag-of-words and segment order-sensitive methods in their op- timum configurations (i.e. when coupled with character bigrams).
"Essentially, the probabil-ity of a given segment set permuting in differ-ent string contexts diminishes as the number of co-occurring segments decreases."
"That is, for a given string pair, the greater the segment over-lap between them (relative to the overall string lengths), the lower the probability that those seg-ments are going to occur in different orderings."
"This is particularly the case when local segment contiguity is modelled within the segment de-scription, as occurs for the character bigram and mixed word uni/bigram models."
"For high-scoring matches, therefore, segment order sensitivity be-comes largely superfluous, and the slight edge in retrieval accuracy for segment order-sensitive methods tends to come for mid-scoring matches, in the vicinity of the translation utility threshold."
"This research has been concerned with the rela-tive import of segmentation, segment order and segment contiguity on translation retrieval per-formance."
We simulated the effects of word or-der sensitivity vs. bag-of-words word order insen-sitivity by implementing a total of five compar-ison methods: two bag-of-words approaches and three word order-sensitive approaches.
"Each of these methods was then tested under character-based and word-based indexing and in combina-tion with a range of N-gram models, and the rel-ative performance of each such system configu-ration evaluated."
"Character-based indexing was found to be superior to word-based indexing, par-ticularly when supplemented with a character bi-gram model."
"We went on to discover a strong correlation be-tween retrieval accuracy and segmentation accu-racy/consistency, and that lexical normalisation produces marginal gains in retrieval performance."
"We further tested the effects of incremental in-creases in data on retrieval performance, and con-firmed our earlier finding that character-based in-dexing is superior to word-based indexing."
"At the same time, we discovered that in their best con-figurations, the retrieval accuracies of our bag-of-words and segment order sensitive string compar-ison methods are roughly equivalent, but that the computational overhead for bag-of-words methods to achieve that accuracy is considerably lower than that for segment order sensitive methods."
The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.
"Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less."
"In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used."
"We are fortunate that for this particular application, correctly labeled training data is free."
"Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
"Machine learning techniques, which automatically learn linguistic information from online text corpora, have been applied to a number of natural language problems throughout the last decade."
A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.
"While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not."
"In part, this is due to the standardization of data sets used within the field, as well as the potentially large cost of annotating data for those learning methods that rely on labeled text."
"The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets."
"Yet since we now have access to significantly more data, one has to wonder what conclusions that have been drawn on small data sets may carry over when these learning methods are trained using much larger corpora."
"In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation."
"In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem."
First we show learning curves for four different machine learning algorithms.
"Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost."
"Confusion set disambiguation is the problem of choosing the correct use of a word, given a set of words with which it is commonly confused."
"Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}."
Numerous methods have been presented for confusable disambiguation.
"The more recent set of techniques includes multiplicative weight-update algorithms[REF_CITE], latent semantic analysis[REF_CITE], transformation-based learning[REF_CITE], differential grammars[REF_CITE], decision lists[REF_CITE], and a variety of Bayesian classifiers ([REF_CITE])."
"In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose."
Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.
"Other such problems include word sense disambiguation, part of speech tagging and some formulations of phrasal chunking."
"One advantageous aspect of confusion set disambiguation, which allows us to study the effects of large data sets on performance, is that labeled training data is essentially free, since the correct answer is surface apparent in any collection of reasonably well-edited text."
This work was partially motivated by the desire to develop an improved grammar checker.
"Given a fixed amount of time, we considered what would be the most effective way to focus our efforts in order to attain the greatest performance improvement."
"Some possibilities included modifying standard learning algorithms, exploring new learning techniques, and using more sophisticated features."
"Before exploring these somewhat expensive paths, we decided to first see what happened if we simply trained an existing method with much more data."
"This led to the exploration of learning curves for various machine learning algorithms: winnow 1 , perceptron, naïve Bayes, and a very simple memory-based learner."
"For the first three learners, we used the standard collection of features employed for this problem: the set of words within a window of the target word, and collocations containing words and/or parts of speech."
The memory-based learner used only the word before and word after as features.
"We collected a [Footnote_1]-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose."
1 Thanks to Dan Roth for making both Winnow and Perceptron available.
This training corpus is three orders of magnitude greater than the largest training corpus previously used for this problem.
"We used 1 million words of Wall Street Journal text as our test set, and no data from the Wall Street Journal was used when constructing the training corpus."
"Each learner was trained at several cutoff points in the training corpus, i.e. the first one million words, the first five million words, and so on, until all one billion words were used for training."
"In order to avoid training biases that may result from merely concatenating the different data sources to form a larger training corpus, we constructed each consecutive training corpus by probabilistically sampling sentences from the different sources weighted by the size of each source."
"In Figure 1, we show learning curves for each learner, up to one billion words of training data."
Each point in the graph is the average performance over ten confusion sets for that size training corpus.
Note that the curves appear to be log-linear even out to one billion words.
"Of course for many problems, additional training data has a non-zero cost."
"However, these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development."
"At least for the problem of confusable disambiguation, none of the learners tested is close to asymptoting in performance at the training corpus size commonly employed by the field."
"Such gains in accuracy, however, do not come for free."
Figure 2 shows the size of learned representations as a function of training data size.
"For some applications, this is not necessarily a concern."
"But for others, where space comes at a premium, obtaining the gains that come with a billion words of training data may not be viable without an effort made to compress information."
"In such cases, one could look at numerous methods for compressing data (e.g.[REF_CITE])."
"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van[REF_CITE]), parsing[REF_CITE], and word sense disambiguati[REF_CITE]."
"By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used."
Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner.
"When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective."
But does voting still offer performance gains when classifiers are trained on much larger corpora?
"The complementarity between two learners was defined[REF_CITE]in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy."
"As training size increases significantly, we would expect complementarity between classifiers to decrease."
This is due in part to the fact that a larger training corpus will reduce the data set variance and any bias arising from this.
"Also, some of the differences between classifiers might be due to how they handle a sparse training set."
"As a result of comparing a sample of two learners as a function of increasingly large training sets, we see in Table 1 that complementarity does indeed decrease as training size increases."
Next we tested whether this decrease in complementarity meant that voting loses its effectiveness as the training set increases.
"To examine the impact of voting when using a significantly larger training corpus, we ran 3 out of the 4 learners on our set of 10 confusable pairs, excluding the memory-based learner."
Voting was done by combining the normalized score each learner assigned to a classification choice.
"In Figure 3, we show the accuracy obtained from voting, along with the single best learner accuracy at each training set size."
"We see that for very small corpora, voting is beneficial, resulting in better performance than any single classifier."
"Beyond 1 million words, little is gained by voting, and indeed on the largest training sets voting actually hurts accuracy."
"While the observation that learning curves are not asymptoting even with orders of magnitude more training data than is currently used is very exciting, this result may have somewhat limited ramifications."
Very few problems exist for which annotated data of this size is available for free.
Surely we cannot reasonably expect that the manual annotation of one billion words along with corresponding parse trees will occur any time soon (but see[REF_CITE]for a discussion that this might not be completely infeasible).
"Despite this pitfall, there are techniques one can use to try to obtain the benefits of considerably larger training corpora without incurring significant additional costs."
"In the sections that follow, we study two such solutions: active learning and unsupervised learning."
Active learning involves intelligently selecting a portion of samples for annotation from a pool of as-yet unannotated training samples.
Not all samples in a training set are equally useful.
"By concentrating human annotation efforts on the samples of greatest utility to the machine learning algorithm, it may be possible to attain better performance for a fixed annotation cost than if samples were chosen randomly for human annotation."
Most active learning approaches work by first training a seed learner (or family of learners) and then running the learner(s) over a set of unlabeled samples.
A sample is presumed to be more useful for training the more uncertain its classification label is.
Uncertainty can be judged by the relative weights assigned to different labels by a single classifier[REF_CITE].
"Another approach, committee-based sampling, first creates a committee of classifiers and then judges classification uncertainty according to how much the learners differ among label assignments."
"For example,[REF_CITE]describe a committee-based sampling technique where a part of speech tagger is trained using an annotated seed corpus."
"A family of taggers is then generated by randomly permuting the tagger probabilities, and the disparity among tags output by the committee members is used as a measure of classification uncertainty."
"Sentences for human annotation are drawn, biased to prefer those containing high uncertainty instances."
"While active learning has been shown to work for a number of tasks, the majority of active learning experiments in natural language processing have been conducted using very small seed corpora and sets of unlabeled examples."
"Therefore, we wish to explore situations where we have, or can afford, a non-negligible sized training corpus (such as for part-of-speech tagging) and have access to very large amounts of unlabeled data."
"We can use bagging[REF_CITE], a technique for generating a committee of classifiers, to assess the label uncertainty of a potential training instance."
"With bagging, a variant of the original training set is constructed by randomly sampling sentences with replacement from the source training set in order to produce N new training sets of size equal to the original."
"After the N models have been trained and run on the same test set, their classifications for each test sentence can be compared for classification agreement."
"The higher the disagreement between classifiers, the more useful it would be to have an instance manually labeled."
"We used the naïve Bayes classifier, creating 10 classifiers each trained on bags generated from an initial one million words of labeled training data."
We present the active learning algorithm we used below.
Initialize: Training data consists of X words correctly labeled Iterate: 1) Generate a committee of classifiers using bagging on the training set 2) Run the committee on unlabeled portion of the training set 3) Choose M instances from the unlabeled set for labeling - pick the M/2 with the greatest vote entropy and then pick another M/2 randomly – and add to training set
"We initially tried selecting the M most uncertain examples, but this resulted in a sample too biased toward the difficult instances."
"Instead we pick half of our samples for annotation randomly and the other half from those whose labels we are most uncertain of, as judged by the entropy of the votes assigned to the instance by the committee."
"This is, in effect, biasing our sample toward instances the classifiers are most uncertain of."
We show the results from sample selection for confusion set disambiguation in Figure 4.
"The line labeled &quot;sequential&quot; shows test set accuracy achieved for different percentages of the one billion word training set, where training instances are taken at random."
"We ran three active learning experiments, increasing the size of the total unlabeled training corpus from which we can pick samples to be annotated."
"In all three cases, sample selection outperforms sequential sampling."
"At the endpoint of each training run in the graph, the same number of samples has been annotated for training."
"However, we see that the larger the pool of candidate instances for annotation is, the better the resulting accuracy."
"By increasing the pool of unlabeled training instances for active learning, we can improve accuracy with only a fixed additional annotation cost."
"Thus it is possible to benefit from the availability of extremely large corpora without incurring the full costs of annotation, training time, and representation size."
"While the previous section shows that we can benefit from substantially larger training corpora without needing significant additional manual annotation, it would be ideal if we could improve classification accuracy using only our seed annotated corpus and the large unlabeled corpus, without requiring any additional hand labeling."
In this section we turn to unsupervised learning in an attempt to achieve this goal.
"Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated, e.g. Marialdo&apos;s HMM part-of-speech tagger training (1994), Charniak&apos;s parser retraining experiment (1996), Yarowsky&apos;s seeds for word sense disambiguation (1995) and Nigam et al&apos;s (1998) topic classifier learned in part from unlabelled documents."
A nice discussion of this general problem can be found[REF_CITE].
The question we want to answer is whether there is something to be gained by combining unsupervised and supervised learning when we scale up both the seed corpus and the unlabeled corpus significantly.
"We can again use a committee of bagged classifiers, this time for unsupervised learning."
"Whereas with active learning we want to choose the most uncertain instances for human annotation, with unsupervised learning we want to choose the instances that have the highest probability of being correct for automatic labeling and inclusion in our labeled training data."
"In Table 2, we show the test set accuracy (averaged over the four most frequently occurring confusion pairs) as a function of the number of classifiers that agree upon the label of an instance."
"For this experiment, we trained a collection of 10 naïve Bayes classifiers, using bagging on a 1-million- word seed corpus."
"As can be seen, the greater the classifier agreement, the more likely it is that a test sample has been correctly labeled."
"Since the instances in which all bags agree have the highest probability of being correct, we attempted to automatically grow our labeled training set using the 1-million-word labeled seed corpus along with the collection of naïve Bayes classifiers described above."
"All instances from the remainder of the corpus on which all 10 classifiers agreed were selected, trusting the agreed-upon label."
The classifiers were then retrained using the labeled seed corpus plus the new training material collected automatically during the previous step.
In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.
"In both cases we gain from unsupervised training compared to using only the seed corpus, but only up to a point."
"At this point, test set accuracy begins to decline as additional training instances are automatically harvested."
"We are able to attain improvements in accuracy for free using unsupervised learning, but unlike our learning curve experiments using correctly labeled data, accuracy does not continue to improve with additional data."
Doing so gave a small improvement over just using the manually parsed data.
"We repeated this experiment with our data, and show the outcome in Table 4."
Choosing only the labeled instances most likely to be correct as judged by a committee of classifiers results in higher accuracy than using all instances classified by a model trained with the labeled seed corpus.
"In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline."
This is likely due to eventually having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
"In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available."
"We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets."
"We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation."
"We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora."
"While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing."
In this paper we argue that comparative evaluation in anaphora resolution has to be performed using the same pre-processing tools and on the same set of data.
The paper proposes an evaluation environment for comparing anaphora resolution algorithms which is illustrated by presenting the results of the comparative evaluation of three methods on the basis of several evaluation measures.
"The evaluation of any NLP algorithm or system should indicate not only its efficiency or performance, but should also help us discover what a new approach brings to the current state of play in the field."
"To this end, a comparative evaluation with other well-known or similar approaches would be highly desirable."
"We have already voiced concern[REF_CITE],[REF_CITE]that the evaluation of anaphora resolution algorithms and systems is bereft of any common ground for comparison due not only to the difference of the evaluation data, but also due to the diversity of pre-processing tools employed by each anaphora resolution system."
The evaluation picture would not be accurate even if we compared anaphora resolution systems on the basis of the same data since the pre-processing errors which would be carried over to the systems’ outputs might vary.
As a way forward we have proposed the idea of the evaluation workbench[REF_CITE]- an open-ended architecture which allows the incorporation of different algorithms and their comparison on the basis of the same pre-processing tools and the same data.
"Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common ”knowledge-poor philosophy”: Kennedy and Boguraev’s (1996) parser-free algorithm, Baldwin’s (1997)"
CogNiac and Mitkov’s (1998b) knowledge-poor approach.
"In order to secure a ”fair”, consistent and accurate evaluation environment, and to address the problems identified above, we have developed an evaluation workbench for anaphora resolution which allows the comparison of anaphora resolution approaches sharing common principles (e.g. similar pre-processing or resolution strategy)."
The workbench enables the ”plugging in” and testing of anaphora resolution algorithms on the basis of the same pre-processing tools and data.
"This development is a time-consuming task, given that we have to re-implement most of the algorithms, but it is expected to achieve a clearer assessment of the advantages and disadvantages of the different approaches."
Developing our own evaluation environment (and even reimplementing some of the key algorithms) also alleviates the impracticalities associated with obtaining the codes of original programs.
Another advantage of the evaluation workbench is that all approaches incorporated can operate either in a fully automatic mode or on human annotated corpora.
"We believe that this is a consistent way forward because it would not be fair to compare the success rate of an approach which operates on texts which are perfectly analysed by humans, with the success rate of an anaphora resolution system which has to process the text at different levels before activating its anaphora resolution algorithm."
"In fact, the evaluations of many anaphora resolution approaches have focused on the accuracy of resolution algorithms and have not taken into consideration the possible errors which inevitably occur in the pre-processing stage."
"In the real-world, fully automatic resolution must deal with a number of hard pre-processing problems such as morphological analysis/POS tagging, named entity recognition, unknown word recognition, NP extraction, parsing, identification of pleonastic pronouns, selectional constraints, etc."
Each one of these tasks introduces errors and thus contributes to a drop in the performance of the anaphora resolution system. [Footnote_1]
"1 For instance, the accuracy of tasks such as robust parsing and identification of pleonastic pronouns is far below 100% See[REF_CITE]for a detailed discussion."
"As a result, the vast majority of anaphora resolution approaches rely on some kind of pre-editing of the text which is fed to the resolution algorithm, and some of the methods have only been manually simulated."
"By way of illustration, Hobbs’ naive approach (1976; 1978) was not implemented in its original version."
"In[REF_CITE]pleonastic pronouns are removed manually [Footnote_2] , whereas[REF_CITE]the outputs of the part-of-speech tagger and the NP extractor/ partial parser are post-edited similarly[REF_CITE]where the output of the Slot Unification Grammar parser is corrected manually."
"2 In addition,[REF_CITE]undertook additional pre-editing such as the removal of sentences for which the parser failed to produce a reasonable parse, cases where the antecedent was not an NP etc.;[REF_CITE]manually removed 30 occurrences of pleonastic pronouns (which could not be recognised by their pleonastic recogniser) as well as 6 occurrences of it which referred to a VP or prepositional constituent."
"Finally, Ge at al’s (1998) and Tetrault’s systems (1999) make use of annotated corpora and thus do not perform any pre-processing."
"One of the very few systems [Footnote_3] that is fully automatic is MARS, the latest version of Mitkov’s knowledge-poor approach implemented by Evans."
3 Apart from MUC coreference resolution systems which operated in a fully automatic mode.
Recent work on this project has demonstrated that fully automatic anaphora resolution is more difficult than previous work has suggested[REF_CITE].
The three algorithms implemented receive as input a representation of the input file.
This representation is generated by running an XML parser over the file resulting from the pre-processing phase.
A list of noun phrases is explicitly kept in the file representation.
Each entry in this list consists of a record containing: • the word form • the lemma of the word or of the head of the noun phrase • the starting position in the text • the ending position in the text • the part of speech • the grammatical function • the index of the sentence that contains the referent • the index of the verb whose argument this referent is
Each of the algorithms implemented for the workbench enriches this set of data with information relevant to its particular needs.
"Apart from the pre-processing tools, the implementation of the algorithms included in the workbench is built upon a common program-ming interface, which allows for some basic processing functions to be shared as well."
An example is the morphological filter applied over the set of possible antecedents of an anaphor.
The evaluation workbench is easy to use.
The user is presented with a friendly graphical interface that helps minimise the effort involved in preparing the tests.
The only information she/he has to enter is the address (machine and directory) of the FDG parser and the file annotated with coreferential links to be processed.
"The results can be either specific to each method or specific to the file submitted for processing, and are displayed separately for each method."
These include lists of the pronouns and their identified antecedents in the context they appear as well as information as to whether they were correctly solved or not.
"In addition, the values obtained for the four evaluation measures (see section 3.2) and several statistical results characteristic of each method (e.g. average number of candidates for antecedents per anaphor) are computed."
"Separately, the statistical values related to the annotated file are displayed in a table."
We should note that (even though this is not the intended usage of the workbench) a user can also submit unannotated files for processing.
"In this case, the algorithms display the antecedent found for each pronoun, but no automatic evaluation can be carried out due to the lack of annotated testing data."
"While the workbench is based on the FDG shallow parser at the moment, we plan to update the environment in such a way that two different modes will be available: one making use of a shallow parser (for approaches operating on partial analysis) and one employing a full parser (for algorithms making use of full analysis)."
Future versions of the workbench will include access to semantic information (WordNet) to accommodate approaches incorporating such types of knowledge.
knowledge-poor anaphora resolution approaches
The first phase of our project included comparison of knowledge-poorer approaches which share a common pre-processing philosophy.
"We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev’s parser-free version of Lappin and Leass’ RAP[REF_CITE], Baldwin’s pronoun resolution method[REF_CITE]and Mitkov’s knowledge-poor pronoun resolution approach[REF_CITE]."
All three of these algorithms share a similar pre-processing methodology: they do not rely on a parser to process the input and instead use POS taggers and NP extractors; nor do any of the methods make use of semantic or real-world knowledge.
We re-implemented all three algorithms based on their original description and personal consultation with the authors to avoid misinterpretations.
"Since the original version of CogNiac is non-robust and resolves only anaphors that obey certain rules, for fairer and comparable results we implemented the ”resolve-all” version as described[REF_CITE]."
"Although for the current experiments we have only included three knowledge-poor anaphora resolvers, it has to be emphasised that the current implementation of the workbench does not restrict in any way the number or the type of the anaphora resolution methods included."
"Its modularity allows any such method to be added in the system, as long as the pre-processing tools necessary for that method are available."
"All three approaches fall into the category of factor-based algorithms which typically employ a number of factors (preferences, in the case of these three approaches) after morphological agreement checks."
The workbench incorporates an automatic scoring system operating on an XML input file where the correct antecedents for every anaphor have been marked.
"The annotation scheme recognised by the system at this moment is MUC, but support for the MATE annotation scheme is currently under developement as well."
We have implemented four measures for evaluation: precision and recall as defined[REF_CITE][Footnote_4] as well as success rate and critical success rate as defined[REF_CITE].
4 This definition is slightly different from the one used[REF_CITE]and[REF_CITE]. For more discussion on this see[REF_CITE].
These four measures are calculated as follows: • Precision = number of correctly resolved anaphor / number of anaphors attempted to be resolved • Recall = number of correctly resolved anaphors / number of all anaphors identified by the system • Success rate = number of correctly resolved anaphors / number of all anaphors • Critical success rate = number of correctly resolved anaphors / number of anaphors with more than one antecedent after a morphological filter was applied
The last measure is an important criterion for evaluating the efficiency of a factor-based anaphora resolution algorithm in the ”critical cases” where agreement constraints alone cannot point to the antecedent.
It is logical to assume that good anaphora resolution approaches should have high critical success rates which are close to the overall success rates.
"In fact, in most cases it is really the critical success rate that matters: high critical success rates naturally imply high overall success rates."
"Besides the evaluation system, the workbench also incorporates a basic statistical calculator which addresses (to a certain extent) the question as to how reliable or realistic the obtained performance figures are - the latter depending on the nature of the data used for evaluation."
"Some evaluation data may contain anaphors which are more difficult to resolve, such as anaphors that are (slightly) ambiguous and require real-world knowledge for their resolution, or anaphors that have a high number of competing candidates, or that have their antecedents far away both in terms of sentences/clauses and in terms of number of ”intervening” NPs etc."
"Therefore, we suggest that in addition to the evaluation results, information should be provided in the evaluation data as to how difficult the anaphors are to resolve. [Footnote_5] To this end, we are working towards the development of suitable and practical measures for quantifying the average ”resolution complexity” of the anaphors in a certain text."
"5 To a certain extent, the critical success rate defined above addresses this issue in the evaluation of anaphora resolution algorithms by providing the success rate for the anaphors that are more difficult to resolve."
"For the time being, we believe that simple statistics such as the number of anaphors with more than one candidate, and more generally, the average number of candidates per anaphor, or statistics showing the average distance between the anaphors and their antecedents, could serve as initial quantifying measures (see Table 2)."
"We believe that these statistics would be more indicative of how ”easy” or ”difficult” the evaluation data is, and should be provided in addition to the information on the numbers or types of anaphors (e.g. intrasentential vs. intersentential) occurring or coverage (e.g. personal, possessive, reflexive pronouns in the case of pronominal anaphora) in the evaluation data."
We have used a corpus of technical texts manually annotated for coreference.
We have decided on this genre because both Kennedy&amp;Boguraev and Mitkov report results obtained on technical texts.
"The corpus contains 28,272 words, with 19,305 noun phrases and 422 pronouns, out of which 362 are anaphoric."
"The files that were used are: ”Beowulf HOW TO” (referred in Table 1 as BEO), ”Linux CD-Rom HOW TO” (CDR), ”Access HOW TO” (ACC), ”Windows Help file” (WIN)."
"The evaluation files were pre-processed to remove irrelevant information that might alter the quality of the evaluation (tables, sequences of code, tables of contents, tables of references)."
The texts were annotated for full coreferential chains using a slightly modified version of the MUC annotation scheme.
All instances of identity-of-reference direct nominal anaphora were annotated.
The annotation was performed by two people in order to minimize human errors in the testing data (see[REF_CITE]for further details).
Table 1 describes the values obtained for the success rate and precision [Footnote_6] of the three anaphora resolvers on the evaluation corpus.
"6 Note that, since the three approaches are robust, recall is equal to precision."
"The overall success rate calculated for the 422 pronouns found in the texts was 56.9% for Mitkov’s method, 49.72%[REF_CITE].6% for Kennedy and Boguraev’s method."
"Table 2 presents statistical results on the evaluation corpus, including distribution of pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors. [Footnote_7]"
"7 In Tables 1 and 2, only pronouns that are treated as anaphoric and hence tried to be resolved by the three methods are included. Therefore, pronouns in first and second person singular and plural and demonstratives do not appear as part of the number of pronouns."
"As expected, the results reported in Table 1 do not match the original results published[REF_CITE],[REF_CITE]and[REF_CITE]where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison."
"By contrast, the evaluation workbench enables a uniform and balanced comparison of the algorithms in that (i) the evaluation is done on the same data and (ii) each algorithm employs the same pre-processing tools and performs the resolution in fully automatic fashion."
"Our experiments also confirm the finding of Orasan,[REF_CITE]that fully automatic resolution is more difficult than previously thought with the performance of all the three algorithms essentially lower than originally reported."
"We believe that the evaluation workbench for anaphora resolution proposed in this paper alleviates a long-standing weakness in the area of anaphora resolution: the inability to fairly and consistently compare anaphora resolution algorithms due not only to the difference of evaluation data used, but also to the diversity of pre-processing tools employed by each system."
"In addition to providing a common ground for comparison, our evaluation environment ensures that there is fairness in terms of comparing approaches that operate at the same level of automation: formerly it has not been possible to establish a correct comparative picture due to the fact that while some approaches have been tested in a fully automatic mode, others have benefited from post-edited input or from a pre- (or manually) tagged corpus."
"Finally, the evaluation workbench is very helpful in analysing the data used for evaluation by providing insightful statistics."
The theoretical study of the range concatenation grammar [RCG] formal-ism has revealed many attractive prop-erties which may be used in NLP.
"In particular, range concatenation lan-guages [RCL] can be parsed in poly-nomial time and many classical gram-matical formalisms can be translated into equivalent RCGs without increas-ing their worst-case parsing time com-plexity."
"For example, after transla-tion into an equivalent RCG, any tree adjoining  grammar can be parsed intime."
"In this paper, we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers."
The non-deterministic parsing choices of the main parser for a lan-guage are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable su-perset of .
The results of a practi-cal evaluation of this method on a wide coverage English grammar are given.
"Usually, during a nondeterministic process, when a nondeterministic choice occurs, one explores all possible ways, either in parallel or one after the other, using a backtracking mechanism."
"In both cases, the nondeterministic process may be as-sisted by another process to which it asks its way."
This assistant may be either a guide or an oracle.
"An oracle always indicates all the good ways that will eventually lead to success, and those good ways only, while a guide will indicate all the good ways but may also indicate some wrong ways."
"In other words, an oracle is a perfect guide[REF_CITE], and the worst guide indicates all possi-and  and ble ways."
"Given two problems their respective solutions  and  , if they are such that  , any algorithm which solves is a candidate guide for nondeterministic al-gorithms solving  ."
"Obviously, supplementary conditions have to be fulfilled for to be a guide."
The first one deals with relative efficiency: it as-sumes that problem can be solved more effi-ciently than problem .
"Of course, parsers are privileged candidates to be guided."
In this pa-per we apply this technique to the parsing of a subset of RCLs that are the languages defined by RCGs.
The syntactic formalism of RCGs is pow-erful while staying computationally tractable.
"In-deed, the positive version of RCGs [PRCGs] de-fines positive RCLs [PRCLs] that exactly cover the class PTIME of languages recognizable in de-terministic polynomial time."
"For example, any mildly context-sensitive language is a PRCL."
"In Section 2, we present the definitions of PRCGs and PRCLs."
"Then, in Section 3, we de-sign an algorithm which transforms any PRCL ,  such that the (the-into another PRCL oretical) parse time for is less than or equal to the parse time for : the parser for will be guided by the parser for ."
"Last, in Section 4, we relate some experiments with a wide coverage tree-adjoining grammar [TAG] for English."
"This section only presents the basics of RCGs, more details can be found[REF_CITE]."
A positive &quot;!$ range &amp;# %&apos;#)*( # concatenation # grammar [ !
"PRCG] is a 5-tuple where called predicate names ),is a finite set of nonterminal % ( symbols (alsoand are finite, dis-joint sets of terminal symbols ! and variable sym-bols respectively, ,+ is the start predicate name , and is a finite set of clauses - *.0/1-   where 5 617 and each of . # - # 32 232 #  is a predicate of the form 8 :9 # 23232 #;=9 &lt;&amp;# 23232 ;# ?9 &gt;@ where %AB6DC(H&amp;I is its arity , 8 &lt; + ,  , is an + ! , and each of argument ."
Each occurrence of a predicate in the LHS (resp. RHS) of a clause is a predicate defini-tion (resp 8 . call).
Clauses 8 which define predicate name 8 are + ! calledhas a fixed-clausesarity.
Eachwhose predicate name  value is . 8
By definition arity arity .
"The ar- , and the arity P ity of an -clause is arity of a grammar (we have a P -PRCG) is the max-imum 8 arity . 23232 of / its 4 clauses 32 232 8 &lt;&amp; . 23232"
"For a given :# fg string `_[REF_CITE]2bda J ec + % I , a pair of integers L s.t. 7eJhLiJfgmon is called a f range , and is denoted Ej Lk2l2 fGp : L L is its lower bound , is its size . n For a is its upper bound and given _ frmon , the set of all ranges is noted q ."
"In fact Ws&lt;  , jEL;232l2b2 aut denotes the occurrence frmon of the string mon a in _ ."
"Two ranges jELk2l2 and f j:P?2l2wv can be concatenated iff the two bounds m n and P are currences or more generally strings inequal, the result is the range jELk2l2wv ."
Variable xFN% (H oc- &amp;I can be instantiated to ranges.
"However, an oc-currence of fHp the CZ2l2 terminal fgmon iff y {|a t can be instantiated to the range j."
"That is, in a clause, several occurrences of the same terminal may well be instantiated to different ranges while several occurrences of the same variable can only be instantiated to the same range."
"Of course, the concatenation on strings matches the concatena-tion on ranges. 8 &quot;} # 23232 k# |} ~&gt;"
"We say that 8 :9 # 23232 #;9?@&gt; iffis } an ~&lt; + instantiation q n # CK9 J&lt; of the predicate A and each symbol (terminal or variable n ) of =9 , &lt; is instantiated }d&lt; to a range in q s.t. is instantiated to ."
"If, in a clause, all predicates are instantiated, we have an instantiated clause ."
"A binary relation derive, denoted   n , is de-fined on strings of instantiated predicates."
"If  is a string of instantiated predicates and / if  is the LHS of some instantiated n = . clause  , "
"An input string _+  I% then we have , R _ is a sen- tence iff the empty string (of instantiated  predi-j&quot;7r2l2 cates) can be derived from , the instan-tiation of the start predicate on the whole source called a complete derivation .   , the PRCL de-text."
"Such a sequence of instantiated predicates is fined by a PRCG , is the set of all its sentences."
"For a given sentence _ , as in the context-free [CF] case, a single complete derivation can be represented by a parse tree and the (unbounded) set of complete derivations by a finite structure, the parse forest ."
"All possible derivation strategies (i.e., top-down, bottom-up, . . . ) are encompassed within both parse trees and parse forests."
A clause is:  combinatorial if at least one argument of its RHS predicates does not consist of a single variable;  bottom-up erasing (resp. top-down erasing ) if there is at least one variable occurring in its RHS (resp.
"LHS) which does not appear in its LHS (resp. RHS);  erasing if there exists a variable appearing only in its LHS or only in its RHS;  linear if none of its variables occurs twice in its LHS or twice in its RHS;  simple if it is non-combinatorial, non-erasing and linear."
"These definitions extend naturally from clause to set of clauses (i.e., grammar)."
"In this paper we will not consider negative RCGs, since the guide construction algorithm presented is Section 3 is not valid for this class."
"Thus, in the sequel, we shall assume that RCGs are PRCGs."
"In[REF_CITE]is presented a parsing al-gorithm which, for any RCG and any input string  of length , produces a parse forest in   time."
"The exponent  , called degree of , is the maximum number of free (indepen-dent) bounds in a clause."
"For a non-bottom-up-erasing RCG,  is less than or equal to the max-imum value, for all Q clauses, of the sum A [xZ[ where, for a clause , A [ is its arity and [ is the number of (different) variables in its LHS predi-cate."
"The purpose of this section is to present a transfor-mation algorithm which takes as input any PRCG and generates as output a 1-PRCG  , such that"
Let  &quot;!$#&amp;&quot;%&apos;! ) # (*&amp;# #%x#)# ( # . and let  be # the  initialbe thePRCGgen-erated 1-PRCG 8 .
"Informally, to each A -ary predi-cate name 8 &lt; we shall associate A unary predicate 8 names , each corresponding to one argument of ."
"F  Z]  8 &lt; R 8 + !$# ~uLyo 8 ) ! % % ,  ( , and ( and the set of clauses is generated in the way described be- 9 low."
"We say that two strings and ¢ , on some al-phabet :# , share a common 9 substring , and we write 9  ¢  _ , iff either , or ¢ or both are empty or, if and Q   *- 8 t :9 t # 23232#;9 t*§t )23232# 7¨J , we . ¦ have / - R  Rg6¦C- ."
"For any clause - t f J©5 in , such #  that :yª 8# t , # we Q 4¬  5 . clauses « [ way."
The clause Qb­ Q #[REF_CITE]J®PxJ¯° 5 . generate the set of in the following 8 .­ :9 .­ /±°- ­ where the RHS ­ ishasconstructedthe 8 &lt; :9 t&lt; form from ° the ­ iff t ’s as follows.
"A 9 &lt; predicate 9 .­ call t share &lt; a com-thearguments t  and :9 .­ #;9 t is in mon substring (i.e., we have )."
"As an example, the following set of clauses, in which ² , ³ and ´ are variables and a and µ are terminal symbols, defines the 3-copy language  _&apos;0_ ¶Ru_ _·+  a # µ I which is not a CF language [CFL] and even lies beyond the formal power of"
This PRCG is transformed by the above algorithm into a 1-PRCG whose // 88
This transformation algorithm works for any PRCG.
"Moreover, if we restrict ourselves to the class of PRCGs that are non-combinatorial and non-bottom-up-erasing, it is easy to check that the constructed 1-PRCG is also non-combinatorial and non-bottom-up-erasing."
It has been shown[REF_CITE]that non-combinatorial and non-bottom-up-erasing 1-RCLs can be parsed in cubic time after a simple grammatical transformation.
"In order to reach this cubic parse time, we as-sume in the sequel that any RCG at hand is a non-combinatorial and non-bottom-up-erasing PRCG."
"However, even if this cubic time transformation is not performed, we can show that the (theoreti-cal) throughput of the parser for cannot be less than the throughput of the parser for ."
"In other words, if we consider the parsers for and and if we recall the end of Section 2, it is easy to show that the degrees, say  and g J¼ , of their polynomial parse times are such that ."
The equality is reached iff the maximum value  in is produced by a unary clause which is kept unchanged by our transformation algorithm.
The starting RCG is called the initial gram-mar and it defines the initial language .
The cor-responding 1-PRCG ¸ constructed by our trans-formation algorithm is called the guiding gram-mar and its language is the guiding language .
"If the algorithm to reach a cubic parse time is ap-alentplied to  the guiding grammar  , we get an equiv- -guiding grammar (it also defines )."
"The various RCL parsers associated with these grammars are respectively  called initial parser , of a ( -) guiding parser is called a ( -) guidingguiding  parser and -guiding parser  ."
The output structure .
"The term guide is used for the process which, with the help of a guiding structure, an-swers ‘yes’ or ‘no’ to any question asked by the guided process."
"In our case, the guided processes are the  RCL parsers for called guided parser and -guided parser ."
Parsing with a guide proceeds as follows.
The guided process is split in two phases.
"First, the source text is parsed by the guiding parser which builds the guiding structure  ."
"Of course, if the source  text is parsed by the -guiding parser, the-guiding structure is then translated into a guid-ing structure, as if the source text had been parsed by the guiding parser."
"Second, the guided parser proper is launched, asking the guide to help (some of) its nondeterministic choices."
"Our current implementation of RCL parsers is like a (cached) recursive descent parser in which the nonterminal calls are replaced by instantiated predicate calls 8 . &quot;} Assume k# } that, at some place in anis an instantiated predicateRCL parser, call."
"In a corresponding guided parser, this 8 call } can be } guarded by a call to a guide, with , and 8 &quot;} as parameters 8 &quot;} , that will check that bothand are instantiated predicates in the guiding structure."
"Of course, various actions in a guided parser can be guarded by guide calls, but the guide can only answer questions that, in some sense, have been registered into the guiding structure."
"The guiding structure may thus con-tain more or less complete information, leading to several guide levels ."
"For example, one of the simplest levels one may think of, is to only register in the guiding structure the (numbers of the) clauses of the guid-ing grammar for which at least one instantiation occurs in their parse forest."
"In such a case, dur-ing the second phase, when Q the guided parser tries guide to know whether or not can be valid."
"Theto instantiate some clause of Q , it can call the guide will answer ‘yes’ iff the guiding structure contains Q the set « [ of clauses in  generated from by the transformation algorithm."
"At the opposite, we can register in the guid-ing structure the full parse forest output by the guiding parser."
"This parse forest is, for a given sentence, the set of all instantiated clauses of the guiding grammar that are used in all complete derivations."
"During the second phase, when Q theguided parser has instantiated some clause of the initial grammar, it builds the set of the cor-responding instantiations of all clauses in « [ and asks the guide to check that this set is a subset of the guiding structure."
"During our experiment, several guide levels have been considered, however, the results in Sec-tion 5 are reported with a restricted guiding struc-ture which only contains the set of all (valid) clause numbers and for each clause the set of its LHS instantiated predicates."
The goal of a guided parser is to speed up a parsing process.
"However, it is clear that the the-oretical parse time complexity is not improved by this technique and even that some practical parse time will get worse."
"For example, this is the case for the above 3-copy language."
"In that case, it is not % difficult I to check that the guiding languageis , and that the guide will always answer ‘yes’ to any question asked by the guided parser."
Thus the time taken by the guiding parser and by the guide itself is simply wasted.
"Of course, a guide that always answer ‘yes’ is not a good one and we should note that this case may % I happen,even when the guiding language is not ."
"Thus, from a practical point of view the question is sim-ply “will the time spent in the guiding parser and in the guide be at least recouped by the guided parser?”"
"Clearly, in the general case, no definite answer can be brought to such a question, since the total parse time may depend not only on the input grammar, the (quality of) the guiding gram-mar (e.g., is not a too “large” superset of ), the guide level, but also it may depend on the parsed sentence itself."
"Thus, in our opinion, only the results of practical experiments may globally decide if using a guided parser is worthwhile ."
Another potential problem may come from the size of the guiding grammar itself.
"In partic-ular, experiments with regular approximation of"
"CFLs related[REF_CITE]show that most reported methods are not practical for large CF grammars, because of the high costs of obtaining the minimal DFSA."
"In our case, it can easily be shown that the in-crease in size of the guiding grammars is bounded by a constant factor and thus seems a priori ac-ceptable from a practical point of view."
The next section depicts the practical exper-iments we have performed to validate our ap-proach.
"In order to compare a (normal) RCL parser and its guided versions, we looked for an existing wide-coverage grammar."
"We chose the grammar for English designed for the XTAG system[REF_CITE], because it both is freely available and seems rather mature."
"Of course, that grammar uses the TAG formalism. [Footnote_1]"
1 We assume here that the reader has at least some cursory notions of this formalism. An introduction to TAG can be found[REF_CITE].
"Thus, we first had to transform that English TAG into an equiva-lent RCG."
"To perform this task, we implemented the algorithm described[REF_CITE](see also[REF_CITE]), which allows to transform any TAG into an equivalent simple PRCG. [Footnote_2]"
2 We first stripped the original TAG of its feature struc-tures in order to get a pure featureless TAG.
"However, Boullier’s algorithm was designed for pure TAGs, while the structures used in the XTAG system are not trees, but rather tree schemata, grouped into linguistically pertinent tree families, which have to be instantiated by in-flected forms for each given input sentence."
That important difference stems from the radical dif-ference in approaches between “classical” TAG parsing and “usual” RCL parsing.
"In the former, through lexicalization, the input sentence allows the selection of tree schemata which are then in-stantiated on the corresponding inflected forms, thus the TAG is not really part of the parser."
"While in the latter, the (non-lexicalized) grammar is pre-compiled into an optimized automaton. [Footnote_3]"
"3 The advantages of this approach might be balanced by the size of the automaton, but we shall see later on that it can be made to stay reasonable, at least in the case at hand."
"Since the instantiation of all tree schemata by the complete dictionary is impracticable, we designed a two-step process."
"For example, from the sentence “George loved himself  .”, a lexer first produces  the sequence  “George phase, this sequence is used as actual input to our parsers."
The names between braces are pre-terminals.
"We assume that each terminal leaf v of every elementary tree schema ½ has  QÀ LÁ - - where ¾ is the family of ½ , Q is the been labeled by a pre-terminal name of the form category of v (verb, noun, . . . ) and L is an optional occurrence index. [Footnote_4]"
"4 The usage of Â as component of Ã is due to the fact that in the XTAG syntactic dictionary, lemmas are associ-ated with tree family names."
"Thus  , the association George “  n-n nxn-n nn-n ” means that the inflected form “George” is a noun (suffix -n ) that can occur in all trees of the “ n ”, “ nxn ” or “ nn ” families (everywhere a ter-minal leaf of category noun occurs)."
"Since, in this two-step process, the inputs are not sequences of terminal symbols but instead simple DAG structures, as the one depicted in Figure 1, we have accordingly implemented in our RCG system the ability to handle inputs that are simple DAGs of tokens. [Footnote_5]"
5 This is done rather easily for linear RCGs. The process-ing of non-linear RCGs with lattices as input is outside the scope of this paper.
"In Section 3, we have seen that the language defined by a guiding grammar for some RCG , is a superset of , the language defined is a simple PRCG,  is a simple by ."
"If 1-PRCG, and thus is a CFL (see[REF_CITE])."
"In other words, in the case of TAGs, our transformation algorithm approximates the initial tree-adjoining language by a CFL, and the steps of CF parsing performed by the guiding parser can well be understood in terms of TAG parsing."
"The original algorithm[REF_CITE]per-forms a one-to-one mapping between elementary trees and clauses, initial trees generate simple unary clauses while auxiliary trees generate sim-ple binary clauses."
Our transformation algorithm leaves unary clauses unchanged (simple unary clauses 8 are in fact CF productions).
"For binary-clauses, our algorithm generates two clauses, -clause which corresponds to the part of 8 the an auxiliary tree to the left of the spine and an -clause for the part to the right of the spine."
Both are CF clauses that the guiding parser calls inde-pendently.
"Therefore, for a TAG, the associated guiding parser performs substitutions as would a TAG parser, while each adjunction is replaced by two independent substitutions, such 8 that there 8 is no guarantee that any couple of -tree and -tree 8 can glue together to form a valid (adjoinable)-tree."
"In fact, guiding parsers perform some kind of (deep-grammar based) shallow parsing."
"For our experiments, we first transformed the English XTAG into an equivalent simple PRCG: the initial grammar ."
"Then, using the algorithms of Section 3, we built, from , the correspond-  ing guiding grammar  , and from  the -guiding grammar."
Table 1 gives some information on these grammars. [Footnote_6]
"6 Note that the worst-case parse time for both the initial tion 3, this identical polynomial degrees ÌÍ Ì| comes from an untransformed unary clause which itself is the result of the translation of an initial tree."
"For our experiments, we have used a test suite distributed with the XTAG system."
"All measures have been per-formed on a 800 MHz Pentium[REF_CITE]MB of memory, running Linux."
All parsers have been and the guiding parsers is Å0ÆlÇ@&quot;È ÉËÊ .
"As explained in Sec- compiled with gcc without any optimization flag. produce the guiding structures, both by theWe have first compared the total time taken  to-guiding parser and by the guiding  -guidingparser (seeparserTa-is twice as fast as theble 2)."
"On this sample set  , the-guiding parser."
"We guess that, on such short sentences, the benefit yielded by the lowest degree has not yet offset the time needed to handle a much greater num-ber of clauses."
"To validate this guess, we have tried longer sentences.  "
The sizes of these RCL parsers (load modules) are in Table 3 while their parse times are in Ta-ble 4. [Footnote_7]
7 The time taken by the lexer phase is linear in the length of the input sentences and is negligible.
"We have also noted in the last line, for reference, the times of the latest XTAG parser[REF_CITE], [Footnote_8] on our sample set and on the 35-word sentence. [Footnote_9]"
"8 It implements a chart-based head-corner parsing algo-rithm for lexicalized TAGs, see[REF_CITE]. This parser can be run in two phases, the second one being devoted to the evaluation of the features structures on the parse forest built during the first phase. Of course, the times reported in that paper are only those of the first pass. Moreover, the various parameters have been set so that the resulting parse trees and ours are similar. Almost half the sample sentences give identical results in both that system and ours. For the other half, it seems that the differences come from the way the co-anchoring problem is handled in both systems. To be fair, it must be noted that the time taken to output a complete parse forest is not included in the parse times reported for our parsers. Outputing those parse forests, similar to Sarkar’s ones, takes one second on the whole sample set and 80 sec-onds for the 35-word sentence (there are more than 3 600000 instantiated clauses in the parse forest of that last sentence)."
"9 Considering the last line of Table 2, one can notice that and the Ç~Ó -guided parser are noticeably different, when they the times taken by the guided phases of the guided parser should be the same. This anomaly, not present on the sample set, is currently under investigation."
"In[REF_CITE], there is some evidence to in-dicate that in LTAG parsing the number of trees selected by the words in a sentence (a measure of the syntactic lexical ambiguity of the sentence) is a better predictor of complexity than the num-ber of words in the sentence."
"Thus, the accuracy of the tree selection process may be crucial for parsing speeds."
"In this section, we wish to briefly compare the tree selections performed, on the one hand by the words in a sentence and, on the other hand, by a guiding parser."
"Such filters can be used, for example, as pre-processors in classical [L]TAG parsing."
"With a guiding parser as tree fil-ter, a tree (i.e., a clause) is kept, not because it has been selected by a word in the input sentence, but because an instantiation of that clause belongs to the guiding structure."
"The recall of both filters is 100%, since all per-tinent trees are necessarily selected by the input words and present in the guiding structure."
"On the other hand, for the tree selection by the words in a sentence, the precision measured on our sam- ple set is 15.6% on the average, while it reaches 100% for the guiding parser (i.e., each and every selected tree is in the final parse forest)."
The experiment related in this paper shows that some kind of guiding technique has to be con-sidered when one wants to increase parsing effi-ciency.
"With a wide coverage English TAG, on a small sample set of short sentences, a guided parser is on the average three times faster than its non-guided counterpart, while, for longer sen-tences, more than one order of magnitude may be expected."
"However, the guided parser speed is very sensi-tive to the level of the guide, which must be cho-sen very carefully since potential benefits may be overcome by the time taken by the guiding struc-ture book-keeping procedures."
"Of course, the filtering principle related in this paper is not novel (see for example[REF_CITE]for deductive databases) but, if we consider the various attempts of guided pars-ing reported in the literature, ours is one of the very few examples in which important savings are noted."
One reason for that seems to be the extreme simplicity of the interface between the guiding and the guided process: the guide only performs a direct access into the guiding struc-ture.
"Moreover, this guiding structure is (part of) the usual parse forest output by the guiding parser, without any transduction (see for example[REF_CITE]how a FSA can guide a CF parser)."
"As already noted by many authors (see for ex-ample[REF_CITE]), the choice of a (parsing) algorithm, as far as its throughput is concerned, cannot rely only on its theoretical complexity but must also take into account practical experi-ments."
"Complexity analysis gives worst-case up-per bounds which may well not be reached, and which implies constants that may have a prepon-derant effect on the typical size ranges of the ap-plication."
"We have also noted that guiding parsers can be used in classical TAG parsers, as efficient and (very) accurate tree selectors."
"More generally, we are currently investigating the possibility to use guiding parsers as shallow parsers."
The above results also show that (guided) RCL parsing is a valuable alternative to classical (lex-icalized) TAG parsers since we have exhibited parse time savings of several orders of magnitude over the most recent XTAG parser.
These savings even allow to consider the parsing of medium size sentences with the English XTAG.
"The global parse time for TAGs might also be further improved using the transformation de-scribed[REF_CITE]which, starting from any TAG, constructs  an equivalent RCG that can."
"However, this improvementbe parsed in is not definite, since, on typical input sentences, the increase in size of the resulting grammar may well ruin the  expected practical benefits, as inthe case of the -guiding parser processing short sentences."
We must also note that a (guided) parser may also be used as a guide for a unification-based parser in which feature terms are evaluated (see the experiment related[REF_CITE]).
"Although the related practical experiments have been conducted on a TAG, this guide tech-nique is not dedicated to TAGs, and the speed of all PRCL parsers may be thus increased."
"This per-tains in particular to the parsing of all languages whose grammars can be translated into equivalent PRCGs — MC-TAGs, LCFRS, . . ."
"While paraphrasing is critical both for interpretation and generation of natu-ral language, current systems use man-ual or semi-automatic methods to col-lect paraphrases."
We present an un-supervised learning algorithm for iden-tification of paraphrases from a cor-pus of multiple English translations of the same source text.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
Paraphrases are alternative ways to convey the same information.
A method for the automatic acquisition of paraphrases has both practical and linguistic interest.
"From a practical point of view, diversity in expression presents a major challenge for many NLP applications."
"In multidocument summarization, identification of paraphrasing is required to find repetitive information in the in-put documents."
"In generation, paraphrasing is employed to create more varied and fluent text."
"Most current applications use manually collected paraphrases tailored to a specific application, or utilize existing lexical resources such as Word-Net[REF_CITE]to identify paraphrases."
"However, the process of manually collecting para-phrases is time consuming, and moreover, the col-lection is not reusable in other applications."
Ex-isting resources only include lexical paraphrases; they do not include phrasal or syntactically based paraphrases.
"From a linguistic point of view, questions concern the operative definition of paraphrases: what types of lexical relations and syntactic mechanisms can produce paraphrases?"
"Many linguists ([REF_CITE]; de[REF_CITE]) agree that paraphrases retain “ap-proximate conceptual equivalence”, and are not limited only to synonymy relations."
But the ex-tent of interchangeability between phrases which form paraphrases is an open questi[REF_CITE].
A corpus-based approach can provide in-sights on this question by revealing paraphrases that people use.
This paper presents a corpus-based method for automatic extraction of paraphrases.
We use a large collection of multiple parallel English trans-lations of novels [Footnote_1] .
1 Foreign sources are not used in our experiment.
"This corpus provides many instances of paraphrasing, because translations preserve the meaning of the original source, but may use different words to convey the mean-ing."
An example of parallel translations is shown in Figure 1.
"It contains two pairs of para-phrases: (“burst into tears”, “cried”) and (“com-fort”, “console”)."
Our method for paraphrase extraction builds upon methodology developed in Machine Trans-lation (MT).
"In MT, pairs of translated sentences from a bilingual corpus are aligned, and occur-rence patterns of words in two languages in the text are extracted and matched using correlation measures."
"However, our parallel corpus is far from the clean parallel corpora used in MT."
"The rendition of a literary text into another language not only includes the translation, but also restruc-turing of the translation to fit the appropriate lit-erary style."
This process introduces differences in the translations which are an intrinsic part of the creative process.
"This results in greater dif-ferences across translations than the differences in typical MT parallel corpora, such as the Cana-dian Hansards."
We will return to this point later in Section 3.
"Based on the specifics of our corpus, we de-veloped an unsupervised learning algorithm for paraphrase extraction."
"During the preprocessing stage, the corresponding sentences are aligned."
We base our method for paraphrasing extraction on the assumption that phrases in aligned sen-tences which appear in similar contexts are para-phrases.
"To automatically infer which contexts are good predictors of paraphrases, contexts sur-rounding identical words in aligned sentences are extracted and filtered according to their predic-tive power."
"Then, these contexts are used to ex-tract new paraphrases."
"In addition to learning lex-ical paraphrases, the method also learns syntactic paraphrases, by generalizing syntactic patterns of the extracted paraphrases."
"Extracted paraphrases are then applied to the corpus, and used to learn new context rules."
This iterative algorithm con-tinues until no new paraphrases are discovered.
A novel feature of our approach is the ability to extract multiple kinds of paraphrases:
Identification of lexical paraphrases.
"In con-trast to earlier work on similarity, our approach allows identification of multi-word paraphrases, in addition to single words, a challenging issue for corpus-based techniques."
Extraction of morpho-syntactic paraphrasing rules.
Our approach yields a set of paraphras-ing patterns by extrapolating the syntactic and morphological structure of extracted paraphrases.
This process relies on morphological information and a part-of-speech tagging.
Many of the rules identified by the algorithm match those that have been described as productive paraphrases in the linguistic literature.
"In the following sections, we provide an overview of existing work on paraphrasing, then we describe data used in this work, and detail our paraphrase extraction technique."
"We present re- sults of our evaluation, and conclude with a dis-cussion of our results."
Many NLP applications are required to deal with the unlimited variety of human language in ex-pressing the same information.
"So far, three major approaches of collecting paraphrases have emerged: manual collection, utilization of exist-ing lexical resources and corpus-based extraction of similar words."
Manual collection of paraphrases is usually used in generati[REF_CITE].
"Paraphrasing is an inevitable part of any generation task, because a semantic con-cept can be realized in many different ways."
Knowledge of possible concept verbalizations can help to generate a text which best fits existing syn-tactic and pragmatic constraints.
"Traditionally, al-ternative verbalizations are derived from a man-ual corpus analysis, and are, therefore, applica-tion specific."
"The second approach — utilization of existing lexical resources, such as WordNet — overcomes the scalability problem associated with an appli-cation specific collection of paraphrases."
"Lexical resources are used in statistical generation, sum-marization and question-answering."
The ques-tion here is what type of WordNet relations can be considered as paraphrases.
"In some appli-cations, only synonyms are considered as para-phrases[REF_CITE]; in others, looser definitions are used[REF_CITE]."
"These definitions are valid in the context of particular applications; however, in general, the correspondence between paraphrasing and types of lexical relations is not clear."
The same ques-tion arises with automatically constructed the-sauri[REF_CITE].
"While the extracted pairs are indeed similar, they are not paraphrases."
"For example, while “dog” and “cat” are recognized as the most similar concepts by the method described[REF_CITE], it is hard to imagine a context in which these words would be interchangeable."
"The first attempt to derive paraphrasing rules from corpora was undertaken[REF_CITE], who investigated morphological and syntactic variants of technical terms."
"While these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing yet."
Sta-tistical techniques were also successfully used[REF_CITE]to identify paraphrases of adjective-noun phrases.
"In contrast, our method is not limited to a particular paraphrase type."
The corpus we use for identification of para-phrases is a collection of multiple English trans-lations from a foreign source text.
"Specifically, we use literary texts written by foreign authors."
"Many classical texts have been translated more than once, and these translations are available on-line."
"In our experiments we used 5 books, among them, Flaubert’s Madame Bovary, Ander-sen’s Fairy Tales and Verne’s Twenty Thousand Leagues Under the Sea."
Some of the translations were created during different time periods and in different countries.
"In total, our corpus contains 11 translations [Footnote_2] ."
2 Free of copyright restrictions part of our corpus(9 translations) is available[URL_CITE]
"At first glance, our corpus seems quite simi-lar to parallel corpora used by researchers in MT, such as the Canadian Hansards."
The major dis-tinction lies in the degree of proximity between the translations.
"Analyzing multiple translations of the literary texts, critics (e.g.[REF_CITE]) have observed that translations “are never iden-tical”, and each translator creates his own inter-pretations of the text."
Clauses such as “adorning his words with puns” and “saying things to make her smile” from the sentences in Figure 1 are ex-amples of distinct translations.
"Therefore, a com-plete match between words of related sentences is impossible."
"This characteristic of our corpus is similar to problems with noisy and comparable corpora[REF_CITE], and it prevents us from using methods developed in the MT community based on clean parallel corpora, such[REF_CITE]."
"Another distinction between our corpus and parallel MT corpora is the irregularity of word matchings: in MT, no words in the source lan-guage are kept as is in the target language trans-lation; for example, an English translation of a French source does not contain untranslated French fragments."
"In contrast, in our corpus the same word is usually used in both transla-tions, and only sometimes its paraphrases are used, which means that word–paraphrase pairs will have lower co-occurrence rates than word– translation pairs in MT."
"For example, consider oc-currences of the word “boy” in two translations of “Madame Bovary” — E. Marx-Aveling’s transla-tion and Etext’s translation."
"The first text contains 55 occurrences of “boy”, which correspond to 38 occurrences of “boy” and 17 occurrences of its paraphrases (“son”, “young fellow” and “young-ster”)."
This rules out using word translation meth-ods based only on word co-occurrence counts.
"On the other hand, the big advantage of our cor-pus comes from the fact that parallel translations share many words, which helps the matching pro-cess."
"We describe below a method of paraphrase extraction, exploiting these features of our corpus."
"During the preprocessing stage, we perform sen-tence alignment."
"Sentences which are translations of the same source sentence contain a number of identical words, which serve as a strong clue to the matching process."
Alignment is performed using dynamic programming[REF_CITE]with a weight function based on the num-ber of common words in a sentence pair.
"This simple method achieves good results for our cor-pus, because 42% of the words in corresponding sentences are identical words on average."
"Align-ment produces 44,562 pairs of sentences with 1,798,526 words."
"To evaluate the accuracy of the alignment process, we analyzed 127 sentence pairs from the algorithm’s output. 120(94.5%) alignments were identified as correct alignments."
We then use a part-of-speech tagger and chun-ker[REF_CITE]to identify noun and verb phrases in the sentences.
These phrases become the atomic units of the algorithm.
"We also record for each token its derivational root, using the CELEX[REF_CITE]database."
"Given the aforementioned differences between translations, our method builds on similarity in the local context, rather than on global alignment."
Consider the two sentences in Figure 2.
"Analyzing the contexts surrounding “ ? ”-marked blanks in both sentences, one expects that they should have the same meaning, because they have the same premodifier “empty” and relate to the same preposition “in” (in fact, the first “ ? ” stands for “sky”, and the second for “heavens”)."
"Generalizing from this example, we hypothesize that if the contexts surrounding two phrases look similar enough, then these two phrases are likely to be paraphrases."
The definition of the context depends on how similar the translations are.
"Once we know which contexts are good paraphrase pre-dictors, we can extract paraphrase patterns from our corpus."
"Examples of such contexts are verb-object re-lations and noun-modifier relations, which were traditionally used in word similarity tasks from non-parallel corpora[REF_CITE]."
"However, in our case, more indirect relations can also be clues for paraphrasing, because we know a priori that input sentences convey the same information."
"For example, in sentences from Figure 3, the verbs “ringing” and “sounding” do not share identical subject nouns, but the modifier of both subjects “Evening” is identical."
Can we conclude that identical modifiers of the subject imply verb sim-ilarity?
"To address this question, we need a way to identify contexts that are good predictors for paraphrasing in a corpus."
"To find “good” contexts, we can analyze all contexts surrounding identical words in the pairs of aligned sentences, and use these contexts to learn new paraphrases."
This provides a basis for a bootstrapping mechanism.
"Starting with identi-cal words in aligned sentences as a seed, we can incrementally learn the “good” contexts, and in turn use them to learn new paraphrases."
"Iden-tical words play two roles in this process: first, they are used to learn context rules; second, iden-tical words are used in application of these rules, because the rules contain information about the equality of words in context."
"This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguati[REF_CITE], lexicon construction for information ex-tracti[REF_CITE], and named en-tity classificati[REF_CITE]."
"In our case, the co-training process creates a binary classifier, which predicts whether a given pair of phrases makes a paraphrase or not."
"Our model is based on the DLCoTrain algo-rithm proposed[REF_CITE], which applies a co-training procedure to decision list classifiers for two independent sets of fea-tures."
"In our case, one set of features describes the paraphrase pair itself, and another set of features corresponds to contexts in which paraphrases oc-cur."
These features and their computation are de-scribed below.
Our paraphrase features include lexical and syn-tactic descriptions of the paraphrase pair.
The lexical feature set consists of the sequence of to-kens for each phrase in the paraphrase pair; the syntactic feature set consists of a sequence of part-of-speech tags where equal words and words with the same root are marked.
"For example, the value of the syntactic feature for the pair (“the vast chimney”, “the chimney”) is (“DT JJ NN ”, “DT NN ”), where indices indicate word equali-ties."
"We believe that this feature can be useful for two reasons: first, we expect that some syntac-tic categories can not be paraphrased in another syntactic category."
"For example, a determiner is unlikely to be a paraphrase of a verb."
"Second, this description is able to capture regularities in phrase level paraphrasing."
"In fact, a similar rep-resentation was used[REF_CITE]to describe term variations."
The contextual feature is a combination of the left and right syntactic contexts surrounding actual known paraphrases.
"There are a num- ber of context representations that can be con-sidered as possible candidates: lexical n-grams, POS-ngrams and parse tree fragments."
"The nat-ural choice is a parse tree; however, existing parsers perform poorly in our domain [Footnote_3] ."
"3 To the best of our knowledge all existing statistical parsers are trained on WSJ or similar type of corpora. In the experiments we conducted, their performance significantly degraded on our corpus — literary texts."
"Part-of-speech tags provide the required level of ab-straction, and can be accurately computed for our data."
"The left (right) context is a sequence of part-of-speech tags of words, occurring on the left (right) of the paraphrase."
As in the case cal words are marked.
"For example, when  of syntactic paraphrase features, tags of identi- , the contextual feature for the paraphrase pair (“comfort”, “console”) from Figure 1 sentences is left =“VB TO ”, (“tried to”), left =“VB TO ”, (“tried to”), right =“PRP$ , ”, (“her,”) right context$ =“PRP$ , ”, (“her,”)."
"In the next section, we describe how the classifiers for con-textual and paraphrasing features are co-trained."
"Our co-training algorithm has three stages: ini-tialization, training of the contextual classifier and training of the paraphrasing classifiers."
Initialization Words which appear in both sen-tences of an aligned pair are used to create the ini-tial “seed” rules.
"Using identical words, we cre-ate a set of positive paraphrasing examples, such as word =tried, word =tried."
"However, train-ing of the classifier demands negative examples as well; in our case it requires pairs of words in aligned sentences which are not paraphrases of each other."
"To find negative examples, we match identical words in the alignment against all different words in the aligned sentence, as-suming that identical words can match only each other, and not any other word in the aligned sen-tences."
"For example, “tried” from the first sen-tence in Figure 1 does not correspond to any other word in the second sentence but “tried”."
"Based on this observation, we can derive negative ex-amples such as word =tried, word =Emma and word =tried, word =console."
"Given a pair of identical words from two sentences of length and , the algorithm produces one positive ex- ample and   negative examples."
"Training of the contextual classifier Using this initial seed, we record contexts around pos-itive and negative paraphrasing examples."
From all the extracted contexts we must identify the ones which are strong predictors of their category.
"Following[REF_CITE], filtering is based on the strength of the context and its fre-fined as  !&quot;#%$ !&quot; , where &amp;()* +, quency."
The strength of positive context is de-tive examples (paraphrase pairs) and &amp;()* + is is the number of times context surrounds posi-the frequency of the context .
Strength of the negative context is defined in a symmetrical man-we select - rules ( -./ in our experiments) ner.
For the positive and the negative categories with the highest frequency and strength higher than the predefined threshold of 95%.
Examples of selected context rules are shown in Figure 4.
The parameter of the contextual classifier is a context length.
In our experiments we found that a maximal context length of three produces best results.
We also observed that for some rules a shorter context works better.
"Therefore, when recording contexts around positive and negative examples, we record all the contexts with length smaller or equal to the maximal length."
"Because our corpus consists of translations of several books, created by different translators, we expect that the similarity between translations varies from one book to another."
This implies that contextual rules should be specific to a particular pair of translations.
"Therefore, we train the con-textual classifier for each pair of translations sep-arately."
Training of the paraphrasing classifier Con-text rules extracted in the previous stage are then applied to the corpus to derive a new set of pairs of positive and negative paraphrasing examples.
Applications of the rule performed by searching sentence pairs for subsequences which match the less than 5 tokens apart.
"For example, apply-left and right parts of the contextual rule, and are ing the first rule from Figure 4 to sentences from Figure 1 yields the paraphrasing pair (“comfort”, “console”)."
"Note that in the original seed set, the left and right contexts were separated by one to-ken."
This stretch in rule application allows us to extract multi-word paraphrases.
"For each extracted example, paraphrasing rules are recorded and filtered in a similar manner as contextual rules."
Examples of lexical and syntac-tic paraphrasing rules are shown in Figure 5 and in Figure 6.
"After extracted lexical and syntactic paraphrases are applied to the corpus, the contex-tual classifier is retrained."
"New paraphrases not only add more positive and negative instances to the contextual classifier, but also revise contex-tual rules for known instances based on new para-phrase information."
The iterative process is terminated when no new paraphrases are discovered or the number of iterations exceeds a predefined threshold.
Our algorithm produced 9483 pairs of lexical paraphrases and 25 morpho-syntactic rules.
"To evaluate the quality of produced paraphrases, we picked at random 500 paraphrasing pairs from the lexical paraphrases produced by our algorithm."
These pairs were used as test data and also to eval-uate whether humans agree on paraphrasing judg-ments.
"The judges were given a page of guide-lines, defining paraphrase as “approximate con-ceptual equivalence”."
The main dilemma in de-signing the evaluation is whether to include the context: should the human judge see only a para-phrase pair or should a pair of sentences contain-ing these paraphrases also be given?
In a simi-lar MT task — evaluation of word-to-word trans-lation — context is usually included[REF_CITE].
"Although paraphrasing is considered to be context dependent, there is no agreement on the extent."
"To evaluate the influence of context on paraphrasing judgments, we performed two experiments — with and without context."
"First, the human judge is given a paraphrase pair with-out context, and after the judge entered his an-swer, he is given the same pair with its surround-ing context."
Each context was evaluated by two judges (other than the authors).
The agreement was measured using the Kappa coefficient[REF_CITE].
"Complete agreement be-tween judges would correspond to K equals ; equals 0 . if there is no agreement among judges, then K judgment without context was 7 The judges agreement on the paraphrasing 0&apos;8:9&apos;; which is substantial agreement[REF_CITE]."
"The first judge found 439(87.8%) pairs as correct paraphrases, and the second judge — higher agreement ( 7&lt;.0&apos;8:&apos;= &gt; ), and judges identi- 426(85.2%)."
Judgments with context have even fied 459(91.8%) and 457(91.4%) pairs as correct paraphrases.
The recall of our method is a more problematic issue.
"The algorithm can identify paraphrasing re-lations only between words which occurred in our corpus, which of course does not cover all English tokens."
"Furthermore, direct comparison with an electronic thesaurus like WordNet is impossible, because it is not known a priori which lexical re-lations in WordNet can form paraphrases."
"Thus, we can not evaluate recall."
"We hand-evaluated the coverage, by asking a human judges to extract paraphrases from 50 sentences, and then counted how many of these paraphrases where predicted by our algorithm."
"In addition to evaluating our system output through precision and recall, we also compared our results with two other methods."
The first of these was a machine translation technique for de-riving bilingual lexicons[REF_CITE]includ-ing detection of non-compositional compounds [Footnote_4] .
4 The equivalences that were identical on both sides were removed from the output
We did this evaluation on 60% of the full dataset; this is the portion of the data which is pub-licly available.
"Our system produced 6,826 word pairs from this data and Melamed provided the top 6,826 word pairs resulting from his system on this data."
We randomly extracted 500 pairs each from both sets of output.
"We mixed this output and gave the re-sulting, randomly ordered 1000 pairs to six eval-uators, all of whom were native speakers."
Each evaluator provided judgments on 500 pairs with-out context.
Precision for our system was 71.6% and for Melamed’s was 52.7%.
"This increased precision is a clear advantage of our approach and shows that machine translation techniques cannot be used without modification for this task, par-ticularly for producing multi-word paraphrases."
"There are three caveats that should be noted; Melamed’s system was run without changes for this new task of paraphrase extraction and his sys-tem does not use chunk segmentation, he ran the system for three days of computation and the re-sult may be improved with more running time since it makes incremental improvements on sub-sequent rounds, and finally, the agreement be-tween human judges was lower than in our pre-vious experiments."
We are currently exploring whether the information produced by the two dif-ferent systems may be combined to improve the performance of either system alone.
Another view on the extracted paraphrases can be derived by comparing them with the Word-Net thesaurus.
This comparison provides us with quantitative evidence on the types of lexical re-lations people use to create paraphrases.
We se-lected 112 paraphrasing pairs which occurred at least 20 times in our corpus and such that the words comprising each pair appear in WordNet.
We use the frequency threshold to select paraphrases which are not tailored to one context.
Examples of paraphrases and their WordNet relations are shown in Figure 7.
These figures quantitatively validate our intuition that synonymy is not the only source of paraphras-ing.
One of the practical implications is that us-ing synonymy relations exclusively to recognize paraphrasing limits system performance.
"In this paper, we presented a method for corpus-based identification of paraphrases from multi-ple English translations of the same source text."
We showed that a co-training algorithm based on contextual and lexico-syntactic features of para-phrases achieves high performance on our data.
"The wide range of paraphrases extracted by our algorithm sheds light on the paraphrasing phe-nomena, which has not been studied from an em-pirical perspective."
"Future work will extend this approach to ex-tract paraphrases from comparable corpora, such as multiple reports from different news agencies about the same event or different descriptions of a disease from the medical literature."
This exten-sion will require using a more selective alignment technique (similar to that[REF_CITE]).
We will also investigate a more pow-erful representation of contextual features.
"Fortu-nately, statistical parsers produce reliable results on news texts, and therefore can be used to im-prove context representation."
This will allow us to extract macro-syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.
"This paper presents a formal analysis for a large class of words called alternative markers, which includes other(than), such(as), and besides."
"These words appear frequently enough in dialog to warrant serious attention, yet present natural language search engines perform poorly on queries containing them."
I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine’s operational seman-tics.
"The value of this approach is that as the operational semantics of natural lan-guage applications improve, even larger improvements are possible."
"Consider the following examples discovered in a corpus of queries submitted to the Electric Knowl-edge search engine 12 , the successor of the On Point natural language search system described[REF_CITE]."
"Each consists of a query, a re-sponse (not shown), and then a follow-up query. ([Footnote_1]) What is the drinking age in Afghanistan?"
1 formerly known as The Electric Monk
What is the drinking age in other countries? ([URL_CITE]) Where can I find web browsers for download?
Where can I find other web browsers than Netscape for download? (3) Where can I find a list of all the shoe manu-facturers in the world?
"Where can I find shoes made by Buffalino, such as the Bushwackers? (4) Where are online auctions indexed?"
Are there other auction search engines be-sides BidFind?
"In each case, particular words are used to con-strain the space of appropriate answers: e.g. such (as), other (than), and besides."
"I call these words, and others like them, alternative mark-ers, and alternative markers along with their syn-tactic argument (e.g. other countries), I call al-ternative phrases."
"Alternative phrases that are closely bound to the noun phrase to which they refer, like those above, I call connected alterna-tive phrases (modeled after similar terminology[REF_CITE])."
"There are also free alternative phrases, such as Other than Fido, every dog likes going for walks, which are not discussed here but are discussed in depth[REF_CITE]."
"I have found queries containing all these forms despite the fact that no current NLIR system can han-dle them, either ignoring the alternative phrase or, worse, treating the queries as if the alternative marker were absent."
The fact that these phrases define the space of correct answers makes it absolutely necessary to correctly interpret them in an IR application.
"The user requires countries that are not Afghanistan, web browsers that are not Netscape, shoes with similar properties to Bushwackers, and auction search engines that are not BidFind."
Answers that do not conform are wrong.
Another feature of alternative markers are their presuppositions.
"Through these presuppositions, alternative markers can provide a rich source of knowledge about the world,[REF_CITE]has already recognized."
"For example, these queries im-ply that Afghanistan is a country, Netscape is a web browser, Bushwackers are shoes, and BidFind is an auction search engine."
Anaphoric resolu-tion can sometimes be critical for these inferences.
"In (1), for instance, other countries anaphorically depends on Afghanistan in the previous query."
"While not as obviously important to natural lan-guage information retrieval, this property of al-ternative phrases can be used to improve future queries (see Section 4.1)."
"The purpose of this paper is, first, to briefly provide a well-founded semantic analysis for al-ternative phrases that is amenable to computa-tion."
"The main thrust is then to show that a motivated approximation of this analysis can dra-matically improve the results of a practical natu-ral language application—in particular, a natural language search engine."
"The value of this two-step approach is that as the operational semantics of practical applications are gradually extended, progressively more extensive approximations can be transparently incorporated."
Work on alternative phrases has previously been limited to Hearst’s work in connection with knowl-edge base construction and formal semantics by Hoeksema and von Fintel.
"This technique is adequate for the purpose of acquiring hyponyms from large corpora because the goal is to take advantage of easily available information, not to handle a wide variation of lin-guistic phenomena."
Handling a full range of nat-ural language queries requires a deeper approach.
"For example, pattern matching alone cannot ac-count for anaphoric reference such as in (1) or ex-amples where discourse knowledge invalidates the hyponym relation as in (6). (6) John’s pet dog, Fido, was sick."
So John and the other dogs went for a walk.
"In v[REF_CITE]andcontrast,[REF_CITE]provide in-depth semantic analyses of a limited class of alternative phrases; namely they focus entirely on exceptive phrases (ways of referring to exceptions) illustrated by the lexical items but and except (for)."
The thrust of their analyses is directed towards how these words interact with quantifiers to determine the final set of entities.
"Since in queries, quantifiers rarely interact with alternative phrases in the manner discussed in Hoeksema and von Fintel’s work, their analyses have not been carried over into the present work."
I present a formal approach to alternative phrases that is wider in scope than the alternatives re-viewed in Section 2 (although less detailed in some respects than von Fintel and Hoeksema’s work).
"In my analysis of alternative phrases, I make use of the pragmatic view of presuppositions explored[REF_CITE]and[REF_CITE]which, stated loosely, sees them as proposi-tions that must be true for an utterance to make sense. (For an overview of presup-position, see[REF_CITE].)"
The semantics of lexical entries are separated into assertion and presupposition as[REF_CITE]and[REF_CITE].
"The idea is also used[REF_CITE]to capture anaphoric (non-structural) links between discourse con-nectives and material derivable from previous discourse, and[REF_CITE]and[REF_CITE]for natural language generation."
"Lexical entries are written in the following form, where the semantic parameters scope both the assertion and presuppositions: word `  syn : syntactic category sem : λ... assert : proposition  presup : proposition* "
The concept of alternative sets plays an impor-tant role in the semantics of alternative phrases.
An alternative set is a set of propositions which differ with respect to how one or more argu-ments are filled.
"For example, the alternative set {like(mary, jen), like(mary, bob), ...} repre-sents the entities that Mary likes."
An early discussion of these structures is provided[REF_CITE]where an analysis is given for the focus particle even.
"Alternative sets are also used[REF_CITE]and[REF_CITE]to develop a detailed account of focus, particularly with the focus particle only."
"My analysis approximates this set of proper-ties as a pair consisting of a set of entities (e.g. {jen,bob,...}) and the property they share (e.g. λx.like(mary,x))."
"My analyses of alternative phrases uses the relation alts(p,q) which, intu-itively, specifies that the two sets of entities de-noted by p and q can be found together in at least one alternative set in the knowledge base."
The description component of the alternative set (i.e. the property) need not be known.
"It is important to note that although here I focus on unifying such structures, I also make use of the fact that alts is a relation that is symmetric and reflexive, but not transitive."
The alternative phrases I have analyzed fall into two classes: those that assemble a set from ele-ments and those that excise a set from a larger set (as in exceptive phrases).
"In either case, one particular set of elements is of interest, the fig-ure."
"With assembly words, the figure is either admitted into the set or combined with a com-plement to form a set."
"With excision, the figure is explicitly excluded from the ground."
"The figure may derive from structurally-related constituents (as with besides), or it may be presupposed (as with other)."
I implement my analyses with Combinatory Cat-egorial Grammar[REF_CITE].
CCG is a lexicalized grammar that encodes both the syntactic and semantic properties of a word in the lexicon.
"For the analyses presented in this paper, standard Categorial Grammar suffices."
"A minor variation is that rather than having the basic categories N and NP, I simply use NP."
Noun phrases with and without determiners are distinguished with the bare feature.
"In this section, I provide an analysis of one syntac-tic form of other in order to illustrate the semantic technique described above."
Discussion of alternate syntactic forms and other alternative markers can be found[REF_CITE].
The semantic analysis below defines other as an excision word that excludes the figure from the ground.
"The figure is a free variable that must be provided from the common ground or discourse, as is the case in (1). other `  syn : NP comp+,eq− /NP bare+ assert : λx.g(x) ∧ ¬f(x) sem : λg presup : ∀x.f(x) → g(x)  alts(f,λx.g(x) ∧ ¬f(x))"
The analysis allows the derivation in Figure 1.
"At this point, the semantics is dependent on the free variable f, the figure."
"This is reflected by the fact that, in isolation, other countries does not make sense."
"Although such anaphoric reference is difficult to resolve, in some constructions we can identify the figure without bringing full resolution techniques to bear—as we would have to in (1)."
"Some of these constructions, as in Figure 2, are those that contain the word than, whose analysis is given below. syn :"
"NP\NP eq−,comp+ /NP than ` sem : λxλy assert : y presup : alts(x, y) "
"The presupposition set in Figure 2 is the union of the presuppositions of other and than, as bound during the derivation."
"The remaining variable, f, can be determined solely from the derivation’s presupposition set using the old AI planning heuristic “use existing objects”[REF_CITE]to avoid inventing new objects when others are already available."
"In particular, we can unify alts(f,λx.browser(x) ∧ ¬f(x)) and alts(netscape, λx.browser(x) ∧ ¬f(x)), discover-ing that f, the figure, is netscape."
"This then instantiates the remaining presupposition, yield-ing ∀x.netscape(x) → browser(x): i.e. Netscape is a browser."
"Unifying logical forms to instantiate variables in this way follows the “interpretation as abduction” paradigm[REF_CITE], where this merging is performed to exploit redundancy for “getting a minimal, and hence a best, inter-pretation.”"
Similar analyses in terms of alternative sets have been developed for many other alternative phrases[REF_CITE].
In the next section I show that practical applica-tions such as natural language search engines can benefit from appropriate approximations of this kind of analysis.
There are a variety of techniques for allowing nat-ural language queries in information retrieval sys-tems.
The simplest approach is simply to re-move the “function words” from the query and use the remaining words in a standard keyword search (Alta Vista).
"In more complex approaches, pattern matching (the EK search engine), pars-ing (Ask Jeeves), and machine learning[REF_CITE]techniques can support the associ-ation of more appropriate keywords with a query."
I will concentrate on the pattern matching tech-nique of the Electric Knowledge search engine and shown how a theory of alternative phrases can drastically improve results.
The Electric Knowledge search engine uses pat-tern recognition to transform a natural language question into a series of increasingly more general Boolean queries that can be used with standard back-end retrieval techniques.
"The question is fil-tered through a hierarchy of regular expressions, and hand-crafted rules are used to extract infor-mation from the question into Boolean expres-sions."
The regular expression matching is aided by an ISA hierarchy such that generalizations of keywords can be captured.
"As mentioned in Sec-tion 1, the fact that the presuppositions of alterna-tive phrases encode hyponym information can be useful in augmenting this aspect of systems like the EK search engine."
"This technique suffers from the fact that, in or-der to be tractable, this set of patterns is limited and important information in the query can be lost."
"In particular, the Electric Knowledge search engine does not have patterns that attempt to as-sociate alternative phrases with appropriate pieces of boolean query."
"To overcome this, an appropriate approxima-tion of the semantic result of my analysis that is compatible with the back end search system must be found."
"For the Electric Knowledge search en-gine (similar approaches are certainly possible for other NLIR systems), a hybrid query has been in-troduced to account for alternative phrases, which combines a natural language query with further restrictions added in a system-specific language."
The syntax is shown in (7). (7) Query :|: ANSWER NOT NEAR (| word list) (8) What are some web browsers? :|: ANSWER NOT NEAR (| netscape)
The natural language query is separated from the restrictions by the :|: symbol.
The restrictions specify that the answer to the query must not be near to certain words.
"The hybrid query in (8), for example, is a trans-formation of the original query What are some other web browsers than Netscape?."
The EK search engine uses the natural language part of the query to initially locate possible answering documents.
The rest of the query is used when gathering evi-dence for including a document in the final results.
The EK search engine finds a location in the docu-ment that should answer the query and then com-pares it against the criteria appended to the end of the query.
"If it does not meet the criteria (that it not be near the word Netscape), another location is tried."
"If there are no more possible answers, the document is rejected."
"This is, of course, not exactly what the original query meant."
"However, it is su-perior to queries like ‘‘browsers’’ AND NOT ‘‘netscape’’ which rejects all pages containing Netscape, even if they also contain other browsers."
The evaluation in Section 5 shows that this op-erational semantics is sufficient to dramatically improve the results for queries with alternative phrases.
"Instead of using the EK search engine’s pattern matching on the initial question, the algorithm does a post-analysis of the syntactic structure pro-duced by parsing the question with my analysis."
"The algorithm recursively descends the deriva-tion, searching for semantic forms that result from alternative phrases."
The information from the al-ternative phrase is removed and then appended to the end of the hybrid query in a different form.
"In Figure 2, for example, the information other than Netscape is removed, leaving only web browsers, to form the hybrid query in (8)."
"First, it is useful to determine how many queries contain alternative phrases in order to judge how large a problem this really is."
"Unfortunately, this is complicated by the fact that users, in general, know that such constructions are not understood by search engines, so they avoid them."
"In fact, even in NLIR systems, users often use keywords even though doing so performs worse than asking natural language questions."
They do this because they do not trust the system.
Work will be neces-sary to improve users’ awareness of NL capabili-ties through advertising and by implementing new user interfaces.
Work will also be needed to take more account of the fact that search is often an iterative and even interactive process.
"As noted[REF_CITE], the results of large-scale document-retrieval competitions such as the Text Retrieval Conference[REF_CITE]do not necessarily reflect the experience many users have with retrieval systems."
"In the meantime, I have attempted to find a baseline for this number by considering two cor-pora of human/human dialogs."
"The corpora are both from tutorial situations where a mentor helps a student through problems in the subject of Physics for one corpus ([REF_CITE]; Van-Lehn et al., in press), and Basic Electricity and Electronics[REF_CITE], for the other."
Tu-toring dialogs are an ideal place to look for data relevant to NLIR because they consist entirely of one party attempting to elicit information from the other.
"In some cases, it is the tutor eliciting information from the student, and in others it is the other way around."
Table 1 shows the frequencies of some alterna-tive phrases.
A more illuminating statistic is how often alternative phrases appear in a single dialog.
I consider a dialog to be a single session between the student and tutor where the discussion of each problem is considered a separate session.
"The ta-ble shows that in 269 total dialogs, each dialog contained, on average, 3.65 alternative phrases."
"If one only considers alternative phrases that occur in the context of a question, there are, on average, 1.54 alternative phrases per dialog."
I consider an alternative phrase to be in a question context if it is in the same dialog turn as a question.
"Because tutors ask questions in order to lead the student to the answer, it is perhaps better to consider just the student data, where a quarter of the dialogs con-tained question contexts with alternative phrases."
"This data is not meant to be considered a rig-orous result, but it is a strong indication that any query-answering system is likely to confront an al-ternative phrase during the course of interacting with a user."
"Furthermore, the data shows that during the course of the interaction it will be ap-propriate for the system to respond using an al-ternative phrase, which is important when con-sidering more responsive search engines than are available today."
It is also interesting to note that a wide vari-ety of alternative phrases occur in this data. (9) contains some examples.
"Because excision words, especially other, are by far the most frequent, I have only considered them in the evaluation pre-sented here. (9) a."
The battery is the green cylinder right?
I don’t see anything negative other than #5. b.
And what do you have in addition to voltage? c. Are there any other forces on that knob besides that one you’ve labeled W1?
Of interest is the potential for improving various search engines through a better way of treating alternative phrases.
"To start with, given a set of queries containing alternative phrases, it is impor-tant to see how well these systems perform with-out enhancement."
"Table 2 shows the performance of Alta Vista, Ask Jeeves, and the EK search en-gine on eight excision examples taken from the corpus of EK search engine queries."
"For a data entry “x/y, z”, y is the total number of returned documents and x are those which contain an an-swer to the query. z are the number of answers that are wrong because they are about the subject that was explicitly being excluded in the query."
"As the data shows, none of the search engines fare particularly well."
The precision for all three is around 20%.
"That is, only about one in five of the responses contain an answer to the query."
"Fur-thermore, from a half to two thirds of the incorrect responses were specifically about the subject the query wanted to exclude, displaying little or no understanding of excision alternative phrases."
The point is not to draw any conclusions about the relative merits of the search engines from this test.
"Rather, it is that each NLIR system shows room for improvement."
"Since I will demonstrate that improvement only for the Electric Knowledge search engine, next, this shows that that improve-ment is not due to exceptionally bad prior perfor-mance by the EK search engine."
"Table 3 shows the results of asking the EK search engine questions in three different forms: without an alternative phrase, with an alternative phrase that has not been translated, and with the alterna-tive phrase translated as described in Section 4.2."
"The first row of the table, for instance, refers to the questions in (10)."
The remaining sentences can be found[REF_CITE].
"Although an im-plementation exists that is capable of performing the translation in Section 4.2, this was done by hand for this evaluation to abstract away from parsing issues. (10) What are some works by Edgar Allan Poe?"
What are some works by Edgar Allan Poe other than the Raven?
What are some works by Edgar Allan Poe? :|: ANSWER NOT NEAR (| raven)
"Unfortunately, at the time of this evaluation, Electric Knowledge had taken down their public portal in favor of providing search for the web pages of specific clients."
"This means that the large index used to process the queries in Table 2 was no longer available, and I therefore used different indices and different queries for this evaluation."
"I used indices of pages about American history and literature—each about 11,000 pages."
"These new, more specialized, indices have the benefit of abstracting this evaluation away from coverage issues (explaining the differences in precision be-tween Table 2 and Table 3)."
I created the questions in two ways.
"For sev-eral, I began by asking a question without an alternative phrase."
I then added an alterna-tive phrase in order to remove some responses I was not interested in.
"For example, when I asked Who are the romantic poets?, all re-sponses were about female romantic poets."
I therefore used the query Who are the romantic poets not including women? in the evaluation.
"Some queries were made without first trying the non-alternative phrase version: What are some epics besides Beowulf?, for example."
"This variation reflects the fact that it is unclear which is more common, excluding clutter from a set of re-sponses or a priori excluding cases that the ques-tioner is not interested in."
"The queries also vary in their syntactic structure, information requested, and alternative phrase used."
The purpose of vary-ing the queries in these ways is to ensure that the results do not simply reflect a quirk in the EK search engine’s implementation.
"For each query, Table 3 shows total, the number of documents returned; good, the number of true positives; and top 5, the number of true positives in the top five returned documents."
"A true pos-itive was given to a document if it contained an answer to the question, and half a point was given to a document that contained an obvious link to a document with an answer to the question."
Pre-cision is computed for all documents and for the top five.
It is important to note that the scores for the queries without alternative phrases are still computed with respect to the alternative phrase.
"That is, documents only about the “Raven” are considered false positives."
"In this way, we can view these scores as a baseline—what would have hap-pened had the system simply removed the alter-native phrase."
"This should be taken with a grain of salt because, in many cases, I chose the query because there were documents to remove (as in the Romantics example)."
"However, a concern was that the transformed query would cause numerous false negatives."
This is not the case as seen by the fact that the precision of the transformed query is not lower than the baseline.
"In fact, in no exam-ple was the precision less than the baseline, and at worse, the precision remained the same."
"Performance on questions containing alterna-tive phrases was quite poor, with an average of 15% precision."
"This is significantly worse than the transformed query, and even the baseline."
The performance drop is due to the fact that the com-plex syntax of the query confuses the EK search engine’s analysis.
"The EK search engine is forced to reduce the query to a simple set of keywords in-cluding the figure, which we were trying to avoid."
"Thus as predicted in the discussion of poten-tial improvement, not accounting for alternative phrases can greatly increase the number of false positives containing the figure (FPF), the very thing the query is attempting to exclude."
"Table 4 shows that in the baseline case, where there is no alternative phrase, on average for 28% of the re-turned documents the only answer was the one we wanted to exclude."
Adding the alternative phrase has the opposite of the intended effect as the per-centage of FPFs increases to 44% for reasons de-scribed above.
"Transforming the query, on the other hand, causes the desired effect, more than halving the percentage of FPFs of the baseline."
"In this paper, I briefly presented a formal anal-ysis for a large class of words called alterna-tive markers."
The analysis elegantly captures the desired phenomena by separating assertion and presupposition and making use of alternative phrases and the “use existing objects” heuristic.
A simple pattern matching approach would fail to capture examples requiring knowledge and those with anaphora.
"Furthermore, sophisticated noun phrase detection would be required for many ex-amples."
"I then show that alternative phrases appear fre-quently enough in dialog to warrant serious atten-tion, yet present natural language search engines perform poorly on queries containing alternative phrases."
"However, by approximating my seman-tic analysis into a form understood by a natural language search engine, I showed that the perfor-mance of that search engine improved dramati-cally."
"As the operational semantics of natural lan-guage applications improves, even larger improve-ments are possible."
Further improvement is also possible by ap-proaching the problem from the other direction.
Alternative phrases can be used to help charac-terize documents beyond the simple word indices used in many current systems.
This richer data can be used in the retrieval process to more effec-tively identify appropriate documents.
We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing.
"Expe-riments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precis-ion of 90.8% and a recall of 90.6%)."
We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.
One of the goals in statistical natural language parsing is to find the minimal set of statistical dependencies (between words and syntactic structures) that achieves maximal parse accuracy.
"Many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents[REF_CITE], leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies."
"The Data Oriented Parsing (DOP) model, on the other hand, takes a rather extreme view on this issue: given an annotated corpus, all fragments (i.e. subtrees) seen in that corpus, regardless of size and lexicalization, are in principle taken to form a grammar (see[REF_CITE]; Sima&apos;an 1999)."
The set of subtrees that is used is thus very large and extremely redundant.
"Both from a theoretical and from a computational perspective we may wonder whether it is possible to impose constraints on the subtrees that are used, in such a way that the accuracy of the model does not deteriorate or perhaps even improves."
That is the main question addressed in this paper.
We report on experiments carried out with the Penn Wall Street Journal (WSJ) treebank to investigate several strategies for constraining the set of subtrees.
We found that the only constraints that do not decrease the parse accuracy consist in an upper bound of the number of words in the subtree frontiers and an upper bound on the depth of unlexicalized subtrees.
"We also found that counts of subtrees with several nonheadwords are important, resulting in improved parse accuracy over previous parsers tested on the WSJ."
"To-date, the Data Oriented Parsing model has mainly been applied to corpora of trees whose labels consist of primitive symbols (but see Bod &amp;[REF_CITE])."
"Let us illus-trate the original DOP model presented[REF_CITE], called DOP1, with a simple example."
Assume a corpus consisting of only two trees:
"New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as °."
"Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree)."
Thus a new sentence such as Mary likes Susan can be derived by combining subtrees from this corpus:
"Other derivations may yield the same tree, e.g.:"
"DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is equal to the number of occurrences of t, | t |, divided by the total number of occurrences of all subtrees t&apos; with the same root label as t. Let r(t) return the root label of t."
Then we may write: | t | P(t) =
Σ t&apos;: r(t&apos;)=r(t) |t&apos; |
"In most applications of DOP1, the subtree probabilities are smoothed by the technique described[REF_CITE]which is based on Good-Turing. (The subtree probabilities are not smoothed by backing off to smaller subtrees, since these are taken into account by the parse tree probability, as we will see.)"
The probability of a derivation t 1 °...°t n is computed by the product of the probabilities of its subtrees ti:
P(t 1 °...°t n ) = Π i P(t i )
"As we have seen, there may be several distinct derivations that generate the same parse tree."
The probability of a parse tree T is thus the sum of the probabilities of its distinct derivations.
"Let t id be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by"
Σ d Π i P(t id )
Thus the DOP1 model considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.
This means that the model is sensitive to the frequency of large subtrees while taking into account the smoothing effects of counts of small subtrees.
Note that the subtree probabilities in DOP1 are directly estimated from their relative frequen-cies.
"A number of alternative subtree estimators have been proposed for DOP1 (cf.[REF_CITE]), including maximum likelihood estimati[REF_CITE]."
"But since the relative frequency estimator has so far not been outper -formed by any other estimator for DOP1, we will stick to this estimator in the current paper."
Each corpus-subtree t is converted into a context-free rule r where the lefthand side of r corresponds to the root label of t and the righthand side of r corresponds to the frontier labels of t. Indices link the rules to the original subtrees so as to maintain the subtree&apos;s internal structure and probability.
"These rules are used to create a derivation forest for a sentence (using a CKY parser), and the most probable parse is computed by sampling a sufficiently large number of random derivations from the forest (&quot;Monte Carlo disambiguation&quot;, see[REF_CITE])."
"While this technique has been successfully applied to parsing the ATIS portion in the Penn Treebank[REF_CITE], it is extremely time consuming."
This is mainly because the number of random derivations that should be sampled to reliably estimate the most probable parse increases exponentially with the sentence length (see[REF_CITE]).
It is therefore questionable whether Bod&apos;s sampling technique can be scaled to larger domains such as the WSJ portion in the Penn Treebank.
"Goodman (1996, 1998) showed how DOP1 can be reduced to a compact stochastic context-free grammar (SCFG) which contains exactly eight SCFG rules for each node in the training set trees."
"Although Goodman&apos;s method does still not allow for an efficient computation of the most probable parse (in fact, the problem of computing the most probable parse in DOP1 is NP-hard --see Sima&apos;an 1999), his method does allow for an efficient computation of the &quot;maximum constit-uents parse&quot;, i.e. the parse tree that is most likely to have the largest number of correct constituents."
Goodman has shown on the ATIS corpus that the maximum constituents parse performs at least as well as the most probable parse if all subtrees are used.
"Unfortunately, Goodman&apos;s reduction method is only beneficial if indeed all subtrees are used."
"Sima&apos;an (1999: 108) argues that there may still be an isomorphic SCFG for DOP1 if the corpus-subtrees are restricted in size or lexicalization, but that the number of the rules explodes in that case."
In this paper we will use Bod&apos;s subtree-to-rule conversion method for studying the impact of various subtree restrictions on the WSJ corpus.
"However, we will not use Bod&apos;s Monte Carlo sampling technique from complete derivation forests, as this turned out to be prohibitive for WSJ sentences."
"Instead, we employ a Viterbi n-best search using a CKY algorithm and estimate the most probable parse from the 1,000 most probable derivations, summing up the probabilities of derivations that generate the same tree."
"Although this heuristic does not guarantee that the most probable parse is actually found, it is shown[REF_CITE]to perform at least as well as the estimation of the most probable parse with Monte Carlo techniques."
"However, in computing the 1,000 most probable derivations by means of Viterbi it is prohibitive to keep track of all subderivations at each edge in the chart (at least for such a large corpus as the WSJ)."
As in most other statistical parsing systems we therefore use the pruning technique described[REF_CITE]and Collins (1999: 263-264) which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability.
Any item with a score less than 10 −5 times of that of the best item is pruned from the chart.
"For our base line parse accuracy, we used the now standard division of the WSJ (see[REF_CITE]) with sections 2 through 21 for training (approx. 40,000 sentences) and section 23 for testing (2416 sentences ≤ 100 words); section 22 was used as development set."
"All trees were stripped off their semantic tags, co-reference information and quotation marks."
"We used all training set subtrees of depth 1, but due to memory limitations we used a subset of the subtrees larger than depth 1, by taking for each depth a random sample of 400,000 subtrees."
These random subtree samples were not selected by first exhaustively computing the complete set of subtrees (this was computationally prohibit -ive).
"Instead, for each particular depth &gt; 1 we sampled subtrees by randomly selecting a node in a random tree from the training set, after which we selected random expansions from that node until a subtree of the particular depth was obtained."
"We repeated this procedure 400,000 times for each depth &gt; 1 and ≤ 14."
Thus no subtrees of depth &gt; 14 were used.
"This resulted in a base line subtree set of 5,217,529 subtrees which were smoothed by the technique described[REF_CITE]based on Good-Turing."
"Since our subtrees are allowed to be lexicalized (at their frontiers), we did not use a separate part-of-speech tagger: the test sentences were directly parsed by the training set subtrees."
"For words that were unknown in our subtree set, we guessed their categories by means of the method described[REF_CITE]which uses statistics on word-endings, hyphenation and capitalization."
The guessed category for each unknown word was converted into a depth-1 subtree and assigned a probability by means of simple Good-Turing estimation (see[REF_CITE]).
"The most probable parse for each test sentence was estimated from the 1,000 most probable derivations of that sentence, as described in section 3."
We used &quot;evalb&quot; 1 to compute the standard PARSEVAL scores for our parse results.
"We focus on the Labeled Precision (LP) and Labeled Recall (LR) scores only in this paper, as these are commonly used to rank parsing systems."
"Table 1 shows the LP and LR scores obtained with our base line subtree set, and compares these scores with those of previous stochastic parsers tested on the WSJ (respectively[REF_CITE]and[REF_CITE])."
"The table shows that by using the base line subtree set, our parser outperforms most previous parsers but it performs worse than the parser[REF_CITE]."
We will use our scores of 89.5%[REF_CITE].3% LR (for test sentences ≤ 40 words) as the base line result against which the effect of various subtree restrictions is investigated.
"While most subtree restrictions diminish the accuracy scores, we will see that there are restrictions that improve our scores, even beyond those[REF_CITE]."
"We will initially study our subtree restrictions only for test sentences ≤ 40 words (2245 sentences), after which we will give in 4.6 our results for all test sentences ≤ 100 words (2416 sentences)."
"While we have tested all subtree restrictions initially on the development set (section 22 in the WSJ), we believe that it is interesting and instructive to report these subtree restrictions on the test set (section 23) rather than reporting our best result only."
Our first subtree restriction is concerned with subtree size.
We therefore performed experi-ments with versions of DOP1 where the base line subtree set is restricted to subtrees with a certain maximum depth.
Table 2 shows the results of these experiments.
Our scores for subtree-depth 1 are comparable to Charniak&apos;s treebank grammar if tested on word strings (see[REF_CITE]).
"Our scores are slightly better, which may be due to the use of a different unknown word model."
Note that the scores consistently improve if larger subtrees are taken into account.
"The highest scores are obtained if the full base line subtree set is used, but they remain behind the results[REF_CITE]."
One might expect that our results further increase if even larger subtrees are used; but due to memory limitations we did not perform experiments with subtrees larger than depth 14.
"The more words a subtree contains in its frontier, the more lexical dependencies can be taken into account."
"To test the impact of the lexical context on the accuracy, we performed experiments with different versions of the model where the base line subtree set is restricted to subtrees whose frontiers contain a certain maximum number of words; the subtree depth in the base line subtree set was not constrained (though no subtrees deeper than 14 were in this base line set)."
Table 3 shows the results of our experiments. lexicalizations (for test sentences ≤ 40 words)
"We see that the accuracy initially increases when the lexical context is enlarged, but that the accuracy decreases if the number of words in the subtree frontiers exceeds 12 words."
Our highest scores of 90.8%[REF_CITE].5% LR outperform the scores of the best previously published parser[REF_CITE]who obtains 90.1% for both LP and LR.
"Moreover, our scores also outper-form the reranking technique[REF_CITE]who reranks the output of the parser[REF_CITE]using a boosting method based on Schapire &amp;[REF_CITE], obtaining 90.4%[REF_CITE].1% LR."
We have thus found a subtree restriction which does not decrease the parse accuracy but even improves it.
"This restriction consists of an upper bound of 12 words in the subtree frontiers, for subtrees ≤ depth 14. (We have also tested this lexical restriction in combination with subtrees smaller than depth 14, but this led to a decrease in accuracy.)"
"Instead of investigating the impact of lexical context, we may also be interested in studying the importance of structural context."
"We may raise the question as to whether we need all unlexica-lized subtrees, since such subtrees do not contain any lexical information, although they may be useful to smooth lexicalized subtrees."
"We accom-plished a set of experiments where unlexicalized subtrees of a certain minimal depth are deleted from the base line subtree set, while all lexicalized subtrees up to 12 words are retained."
"Table 4 shows that the accuracy increases if unlexicalized subtrees are retained, but that unlexicalized subtrees larger than depth 6 do not contribute to any further increase in accuracy."
"On the contrary, these larger subtrees even slightly decrease the accuracy."
The highest scores obtained are: 90.8% labeled precision and 90.6% labeled recall.
We thus conclude that pure structural context without any lexical information contributes to higher parse accuracy (even if there exists an upper bound for the size of structural context).
The importance of structural context is consonant[REF_CITE]who showed that structural context from higher nodes in the tree (i.e. grandparent nodes) contributes to higher parse accuracy.
This mirrors our result of the importance of unlexicalized subtrees of depth 2.
But our results show that larger structural context (up to depth 6) also contributes to the accuracy.
We may also raise the question as to whether we need almost arbitrarily large lexicalized subtrees (up to 12 words) to obtain our best results.
"It could be the case that DOP&apos;s gain in parse accuracy with increasing subtree depth is due to the model becoming sensitive to the influence of lexical heads higher in the tree, and that this gain could also be achieved by a more compact model which associates each nonterminal with its headword, such as a head-lexicalized SCFG."
Head-lexicalized stochastic grammars have recently become increasingly popular (see[REF_CITE]).
These grammars are based on Magerman&apos;s head-percolation scheme to determine the headword of each nonterminal[REF_CITE].
Unfortunat-ely this means that head-lexicalized stochastic grammars are not able to capture dependency relations between words that according to Magerman&apos;s head-percolation scheme are &quot;nonheadwords&quot; -- e.g. between more and than in the WSJ construction carry more people than cargo where neither more nor than are head-words of the NP constituent more people than cargo.
"A frontier-lexicalized DOP model, on the other hand, captures these dependencies since it includes subtrees in which more and than are the only frontier words."
"One may object that this example is somewhat far-fetched, but[REF_CITE]notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left."
This is because Magerman&apos;s head-percolation scheme makes should and have the heads of their respective VPs so that there is no dependency relation between the verb left and its subject John.
Chiang observes that almost a quarter of all nonempty subjects in the WSJ appear in such a configuration.
"In order to isolate the contribution of nonheadword dependencies to the parse accuracy, we eliminated all subtrees containing a certain maximum number of nonheadwords, where a nonheadword of a subtree is a word which according to Magerman&apos;s scheme is not a headword of the subtree&apos;s root nonterminal (although such a nonheadword may of course be a headword of one of the subtree&apos;s internal nodes)."
"In the following experiments we used the subtree set for which maximum accuracy was obtained in our previous experiments, i.e. containing all lexicalized subtrees with maximally 12 frontier words and all unlexicalized subtrees up to depth 6."
Table 5 shows that nonheadwords contribute to higher parse accuracy: the difference between using no and all nonheadwords is 1.2% in LP and 1.0% in LR.
"Although this difference is relatively small, it does indicate that nonhead-word dependencies should preferably not be discarded in the WSJ."
"We should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers (see[REF_CITE])."
"But our parser is the first parser that also includes counts between two or more non-headwords, to the best of our knowledge, and these counts lead to improved performance, as can be seen in table 5."
"We have seen that for test sentences ≤ 40 words, maximal parse accuracy was obtained by a subtree set which is restricted to subtrees with not more than 12 words and which does not contain unlexicalized subtrees deeper than 6. [Footnote_2]"
"2 It may be noteworthy that for the development set (section 22 of WSJ), maximal parse accuracy was obtained with exactly the same subtree restrictions. As explained in 4.1, we initially tested all restrictions on the development set, but we preferred to report the effects of these restrictions for the test set."
We used these restrictions to test our model on all sentences ≤ 100 words from the WSJ test set.
This resulted in an[REF_CITE].7% and an[REF_CITE].7%.
"These scores slightly outperform the best previously published parser[REF_CITE], who obtained 89.5%[REF_CITE].6% LR for test sentences ≤ 100 words."
"Only the reranking technique proposed[REF_CITE]slightly outperforms our precision score, but not our recall score: 89.9%[REF_CITE].6% LR."
The main goal of this paper was to find the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing.
We have found that this minimal set of fragments is very large and extremely redundant: highest parse accuracy is obtained by employing only two constraints on the fragment set: a restriction of the number of words in the fragment frontiers to 12 and a restriction of the depth of unlexicalized fragments to 6.
No other constraints were warranted.
There is an important question why maximal parse accuracy occurs with exactly these constraints.
"Although we do not know the answer to this question, we surmise that these constraints differ from corpus to corpus and are related to general data sparseness effects."
In previous experiments with DOP1 on smaller and more restricted domains we found that the parse accuracy decreases also after a certain maximum subtree depth (see[REF_CITE]; Sima&apos;an 1999).
"We expect that also for the WSJ the parse accuracy will decrease after a certain depth, although we have not been able to find this depth so far."
A major difference between our approach and most other models tested on the WSJ is that the DOP model uses frontier lexicalization while most other models use constituent lexicalization (in that they associate each constituent non-terminal with its lexical head -- see[REF_CITE]).
The results in this paper indicate that frontier lexicalization is a promising alternative to constituent lexicaliza-tion.
Our results also show that the linguistically motivated constraint which limits the statistical dependencies to the locality of headwords of constituents is too narrow.
"Not only are counts of subtrees with nonheadwords important, also counts of unlexicalized subtrees up to depth 6 increase the parse accuracy."
"The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is[REF_CITE]who extracts a stochastic tree-insertion grammar or STIG (Schabes &amp;[REF_CITE]) from the WSJ, obtaining 86.6%[REF_CITE].9% LR for sentences ≤ 40 words."
"However, Chiang&apos;s approach is limited in at least two respects."
"First, each elementary tree in his STIG is lexicalized with exactly one lexical item, while our results show that there is an increase in parse accuracy if more lexical items and also if unlexicalized trees are included (in his conclusion Chiang acknowledges that &quot;multiply anchored trees&quot; may be important)."
"Second, Chiang computes the probability of a tree by taking into account only one derivation, while in STIG, like in DOP1, there can be several derivations that generate the same tree."
Another difference between our approach and most other models is that the underlying grammar of DOP is based on a treebank grammar (cf.
"While a treebank grammar only assigns probabilities to rules or subtrees that are seen in a treebank, a markov grammar assigns proba-bilities to any possible rule, resulting in a more robust model."
We expect that the application of the markov grammar approach to DOP will further improve our results.
"Research in this direction is already ongoing, though it has been tested for rather limited subtree depths only (see Sima&apos;an 2000)."
"Although we believe that our main result is to have shown that almost arbitrary fragments within parse trees are important, it is surprising that a relatively simple model like DOP1 outperforms most other stochastic parsers on the WSJ."
"Yet, to the best of our knowledge, DOP is the only model which does not a priori restrict the fragments that are used to compute the most probable parse."
"Instead, it starts out by taking into account all fragments seen in a treebank and then investigates fragment restrictions to discover the set of relevant fragments."
"From this perspective, the DOP approach can be seen as striving for the same goal as other approaches but from a dif-ferent direction."
"While other approaches usually limit the statistical dependencies beforehand (for example to headword dependencies) and then try to improve parse accuracy by gradually letting in more dependencies, the DOP approach starts out by taking into account as many dependencies as possible and then tries to constrain them without losing parse accuracy."
"It is not unlikely that these two opposite directions will finally converge to the same, true set of statistical dependencies for natural language parsing."
"As it happens, quite some convergence has already taken place."
The history of stochastic parsing models shows a consistent increase in the scope of statistical dependencies that are captured by these models.
Figure 4 gives a (very) schematic overview of this increase (see Carroll &amp;[REF_CITE]for a more detailed account of a subsumption lattice where SCFG is at the bottom and DOP at the top).
Thus there seems to be a convergence towards a maximalist model which &quot;takes all fragments [...] and lets the statistics decide&quot; ([REF_CITE]: 5).
"While early head-lexicalized grammars restricted the fragments to the locality of headwords (e.g.[REF_CITE]), later models showed the importance of including context from higher nodes in the tree[REF_CITE]."
This mirrors our result of the utility of (unlexicalized) fragments of depth 2 and larger.
"The importance of including single nonhead-words is now also uncontroversial (e.g.[REF_CITE]), and the current paper has shown the importance of including two and more nonheadwords."
"Recently,[REF_CITE]observed that &quot;In an ideal situation we would be able to encode arbitrary features hs, thereby keeping track of counts of arbitrary fragments within parse trees&quot;."
This is in perfect correspondence with the DOP philosophy.
"For ambiguous sentences, traditional semantics construction produces large numbers of higher-order formulas, which must then be -reduced individ-ually."
"Underspecified versions can pro-duce compact descriptions of all read-ings, but it is not known how to perform -reduction on these descriptions."
We show how to do this using -reduction constraints in the constraint language for -structures (CLLS).
Traditional approaches to semantics constructi[REF_CITE]employ formu-las of higher-order logic to derive semantic rep-resentations compositionally; then -reduction is applied to simplify these representations.
"When the input sentence is ambiguous, these approaches require all readings to be enumerated and -reduced individually."
"For large numbers of read-ings, this is both inefficient and unelegant."
Existing underspecification approaches ([REF_CITE]; van[REF_CITE]) provide a partial solution to this problem.
"They delay the enumeration of the read-ings and represent them all at once in a single, compact description."
"An underspecification for-malism that is particularly well suited for describ-ing higher-order formulas is the Constraint Lan-guage for Lambda Structures, CLLS[REF_CITE]."
CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena[REF_CITE].
They are based on dominance constraints[REF_CITE]and extend them with parallelism[REF_CITE]and binding constraints.
"However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done."
Such an operation – which we will call un-derspecified -reduction – would essentially -reduce all described formulas at once by deriv-ing a description of the reduced formulas.
"In this paper, we show how underspecified -reductions can be performed in the framework of CLLS."
"Our approach extends the work presented[REF_CITE], which defines -reduction constraints and shows how to obtain a complete solution procedure by reducing them to paral-lelism constraints in CLLS."
The problem with this previous work is that it is often necessary to perform local disambiguations.
"Here we add a new mechanism which, for a large class of de-scriptions, permits us to perform underspecified -reduction steps without disambiguating, and is still complete for the general problem."
"We start with a few examples to show what underspecified -reduction should do, and why it is not trivial."
We then introduce CLLS and -reduction constraints.
In the core of the paper we present a procedure for underspecified -reduction and apply it to illustrative examples.
"In this section, we show what underspecified -reduction should do, and why the task is nontriv-ial."
Consider first the ambiguous sentence Every student didn’t pay attention.
"In first-order logic, the two readings can be represented as pay attention’  &amp ; 5 6  &apos; 598 8 : &amp; 56  &apos; 598 8"
A classical compositional semantics construction first derives these two readings in the form of two
HOL-formulas: & amp;6 . /  8 ;5 6 &lt;=  6 &amp;6 . 0/ &gt; &amp; 8 ;5 6 &lt;=  8 where . 0/  is an abbreviation for the term . / @?   6: A 5 D  8
An underspecified description of both readings is shown in Figure 2.
"For now, notice that the graph has all the symbols of the two HOL formulas as node labels, that variable binding is indicated by dashed arrows, and that there are dotted lines indi-cating an “outscopes” relation; we will fill in the details in Section 3."
Now we want to reduce the description in Fig-ure 2 as far as possible  .
"The first -reduction step, with the redex at is straightforward."
"Even though the description is underspecified, the re-ducing part is a completely known -term."
The result is shown on the left-hand side of Figure 1.
"Here we have just one redex, starting at , which binds a single variable."
The next reduction step is less obvious:
The operator could @ either % be- long to the context (the part between and ) or to the argument (below ).
"Still, it is not dif-ficult to give a correct description of the result: it is shown in the middle of Fig. 1."
"For the final step, which takes us ,) * to the rightmost description, the redex starts at ."
Note that now the might be part of the body or part of the context of this redex.
The end result is precisely a description of the two readings as first-order formulas.
"So far, the problem does not look too difficult."
"Twice, we did not know what exactly the parts of the redex were, but it was still easy to derive cor-rect descriptions of the reducts."
But this is not always the case.
"Consider Figure 3, an abstract but simple example."
"In the left description, 3 there are two possible positions for the : above or below ."
"Proceeding naı̈vely as above, we arrive scription is also satisfied by the termat the right-hand description in Fig. 3. E But 6  this G 8 de- 8 8 , which cannot be obtained by reducing any of the terms described on the left-hand side."
"More gen-erally, the naı̈ve “graph rewriting” approach is unsound; the resulting descriptions can have too many readings."
"Similar problems arise in (more complicated) examples from semantics, such as the coordination in Fig. 8."
The underspecified -reduction operation we propose here does not rewrite descriptions.
"In-stead, we describe the result of the step using a “ -reduction constraint” that ensures that the re-duced terms are captured correctly."
Then we use a saturation calculus to make the description more explicit.
"In this section, we briefly recall the definition of the constraint language for -structures (CLLS)."
A more thorough and complete introduction can be found[REF_CITE]. ?
L %E M&amp;N#MPOPOPORQ
"We assume a signature K of function  6 E 8TSVU symbols, each equipped with an arity ."
"A tree W consists of a finite set of bol _^ \ X nodes XZY6 ;8 ][ Y`K\ , each."
Eachof whichnode X ishaslabeleda sequenceby a sym-of children +X a MPOPOPO7M []\ where b ? &amp;6 e^ \ 6 X 8 8 is the arity of the label of X .
"A single node f , the root of W , is not the child of any other node."
The idea behind -structures is that a -term can be considered as a pair of a tree which represents the structure of the term and a binding function encoding variable  binding.
"We assume K contains symbols (arity 0, for variables), (arity 1, for abstraction), (arity 2, for application), and analogous labels for the logical connectives."
"A -structure h is a pair 6 W M 8 of a tree W and a binding  function that maps  every node X with label to a node with label , , or i dominating X ."
The binding function explicitly maps nodes representing variables to the nodes representing their binders.
"E  When we draw -structures, we rep-resent the binding function using dashed arrows, as in the picture 5[REF_CITE]to the right, which represents the -term ."
A -structure corresponds uniquely to a closed -term modulo l -renaming.
We will freely consider -structures as first-order model struc-tures with domain []\ .
This structure defines the E 6 following MPOPOPO M relations 8 .
The labeling 6 8p? relation E XBqXnm ?
X  for all #X o atsur holds ]sub in W . ifThe ^_\ dominance X andre-lation ?
X9vxwX &lt;y .
Inequalityholds iff there ? z isissimplya pathinequality  y such thatof X9X y y nodes; disjointness X { holds iff neither X9vxw&lt; nor X y v w X .
Now we define the constraint language for -structures  M ) (CLLS) to talk about these relations. 3 are variables that will denote nodes of a -structure. 3 v E w 6~}=3MPOPOPO?z ~M 3 =} 3 8 { ~} 6|ttE +8 ? | y 8 3 m1m }? | 6 m3 8 ? }   6 o 8 ?@L  MPOPOPORMb 3 o Q }
"A constraint | is a conjunction of literals (for dominance 3 , labeling 3 , etc). "
We 3 use ? the abbrevi- 3 ? ations 3 vxv w  for v wvxw3 .
The z -bindingand literal 63 for 8 ? expresses that denotes 3 a node which the binding function 6  maps 8 @?
L to  MPOPOPO .
The M 3 o Q inversestates -binding 3 MPOPOPO literal M 3 o  denote  that the entire 6 set M  8 of vari- .
"A pair h  able nodes bound by of a -structure h and a variable assignment satisfies a -structure iff it satisfies each literal, in the obvi-ous way."
Figure 6 3 8 C? 4 L : 3 M 34
Q  3 constraint vxw   3 graph xv w 34! of 
We draw constraints as graphs (Fig. 4) in which nodes represent variables.
"Labels and solid lines indicate labeling literals, while dotted lines repre-sent dominance."
Dashed arrows indicate the bind-ing relation; disjointness and inequality literals are not represented.
"The informal diagrams from Section 2 can thus be read as constraint graphs, which gives them a precise formal meaning."
"Finally, we define segments of -structures and correspondences between segments."
This allows us to define parallelism and -reduction con-straints.
A segment is a contiguous part of a -structure that is delineated by several nodes of the structure.
"Intuitively, it is a tree from which some subtrees have been cut out, leaving behind holes. structure W Definition 6 2 M (Segments 8 )."
A  segment OPOPO M #X ol ofofnodesa -is a tuple X X in []\ such ? that X v w #X q and #X qJ{2X6 hold 8 in W for  6 l#asrz8? X MPOPOPOsbM XBo .
"Theis itsroot(possibly  l isempty X ,)andse-quence of holes."
The set  6 l 8 of nodes of l is  6 l 8_?dL XY;[ }  6 l 8 vxw&lt;X M and not X q vxnX for all asr_sb Q
"To 6 exempt 8?  6 the l  holes l of 8  .the 6 If  segment l 8 is,awesingletondefine   l sequence then we write l 8 for the unique hole of l , i.e. the unique node with 6 l 8 Y #6 l 8 ."
"For instance, l ? X  X !"
M X is ! a segment in
"Fig. 5; its root is X , its 6 holes _8 L?"
X are M XX $ M and X !
"M X X Q ,.and it contains the nodes  l M"
Two 6 8 tree x segments 6 8 ?z . l The overlapsyntactic  properly iff x l equivalent MPOPOPO 3 of a segment is a segment M ¡¢M¤£2
M [ termfor them and extend o .
"We 6 use 8  the letters 8  6 8  , , and correspondingly."
"A correspondence function is intuitively an iso-morphism between segments, mapping holes to holes and roots to roots and respecting the struc-tures of the trees:"
"A correspondence M function be-tween 6 the 8 segments  6 8 such l thatis a ¥ mapsbijectivethe r mapping-th hole ¥ m¦ l of l to the 6 8 r -th hole of for E each r , and for every XZYZx l and every label ,"
Xnm E 6 +X a MPOPOPORM  8n§ ¥ 6 X 8 m E 6 ¥ 6 X+a 8 MPOPOPO ¥ 6  8 8 O
There is at most one correspondence function between any two 6 given (£ M [ 8&lt; segments 6 3 8 ? expresses.
"The correspon-that adence literal co correspondence £ function ¥ between 3 the segments denoted by and [ exists, that and denote nodes within these segment, and that these nodes are related by ¥ ."
"Together, these constructs allow us to define parallelism, which was originally introduced for the analysis of ellipsis[REF_CITE]."
The par-allelism relation l¨ holds iff there is a corre-spondence function between l and that satis-fies some natural conditions on -binding which we cannot go into here.
"To model parallelism in the presence of global -binders relating multiple parallel segments,[REF_CITE]general-allelism l ize parallelism 6 MPOPOPO to M group l o 8 ¨ parallelism 6 MPOPOPO M o . 8 Groupis entailedpar- by the conjunction  o&gt;q ­ l q ¨ q of ordinary par-allelisms, but imposes slightly weaker restrictions on -binding."
"By way of example 6  X , M X consider != X &quot; M X  the 8 ¨ -structure 6    M  in  Fig &quot; M .  5 &quot; ,  where 8 holds."
"On the syntactic side, 6 CLLS MPOPOPO M provides 8 group 6 ¡"
MPOPOPO®M parallelism ¡ o 8 to talkliteralsabout (group) parallelism o . ¨
Correspondences are also used in the definition of -reduction constraints[REF_CITE].
A -reduction constraint describes a single -reduction step between two -terms; it enforces correct reduction even if the two terms are only partially known.
Standard -reduction has the form £ 6 6 5 Oª¡ 8 8  £ 6 ¡°¯ 5  ²± 8³5 free for O £
The reducing -term 6 5 Oª¡ consists 8 of context which contains a redex .
The redex itself is an occurrence 5 Oª¡ of an ¡ application of a -abstraction with body to argument . -reduction then 5 replaces all occurrences of the bound vari-able in the body by the argument while preserv-ing the context.
"We can partition both redex and reduct into ar-gument, body, and context segments."
Consider Fig. 5 E . 6 The 6 5 Oª©  -structure 8&lt;6 G 8 8 startingcontainsat X the.
"Thereducing M reduced- term term can , be M y foundfor theatbody X y ."
"Writingand l M l ´y for ´ y forthe thear-context, gument tree segments of the reducing and the re-duced term ? , respectively  ? , we !µ find &quot; l ?"
X ´ ? X X  y ?
X X  l y ?
"Because we have both the reducing term and the reduced term as parts of the same -structure, we can express the fact that the structure below X y can be obtained by -reducing the structure be-low X by requiring that l corresponds to l y , to y , and ´ to ´ y , again modulo binding."
"This is indeed true in the given -structure, as we have seen above."
"More generally, we define the -reduction re- 6 ´ M ,M l 8  « 6 ´ y M y M l y"
MPOPOPOM l yo 8 lation for a body with b holes (for the variables bound two conditions are met: ´ in the redex).
"The -reduction 6 M ,M l 8 relationmust formholdsa re-iff ducing term, and the structural equalities that we have noted above must hold between the tree seg-ments."
"The latter can be stated by the following group parallelism relation, which also represents the correct binding behaviour: 6 ´ M ,M l MPOPOPOnM l 8 ¨ 6 %´ y M y M l y MPOPOPO®M l yo 8"
Note that any -structure satisfying this relation must contain both the reducing and the reduced term as substructures.
"Incidentally, this allows us to accommodate for global variables in © -terms; Fig. 5 shows this for the global variable ."
We now extend CLLS with -reduction con-straints 6 (£ M ¡¶M 8  « 6 £ y M ¡ y M y
MPOPOPO®M yo 8M which are interpreted by the -reduction relation.
The reduction steps in Section 2 can all be represented correctly by -reduction constraints.
Consider e.g. the first step in Fig. 1.
This is repre- «.
The entire middle con-straint in Fig. 1 is entailed by the -reduction   lit-eral.
"If we learn in addition that ) vxw e.g ) . because vxw ,the -reduction literal will entail the segments must correspond."
"This correlation between parallel segments is the exact same ef-fect (quantifier parallelism) that is exploited in the CLLS analysis of “Hirschbühler sentences”, where ellipses and scope interact[REF_CITE]. -reduction constraints also represent the prob-lematic example in Fig. 3 correctly."
"The spuri-ous solution of the right-hand constraint does not satisfy the -reduction constraint, as the bodies would not correspond."
"Having introduced -reduction constraints, we now show how to process them."
"In this section, we present the procedure usb, which performs a sequence of underspecified -reduction steps on CLLS descriptions."
"This procedure is parameter-ized by another procedure solve for solving -reduction constraints, which we discuss in the fol-lowing section."
A syntactic redex in a constraint | is a subfor-mula of the following form: redex · 6 £(M ¡¶M _8 ? df  [Footnote_6]£ 8 m  M  6 8 8 m @6  6 ¡ 8 8   6 +8 ¹? #6 ¡ 8  £ A 6 £ context 8 of a redex must have a unique hole .
"6 @ M %! M &quot; 8  sented 6 ! +) by M +) the ,) constraint M )+ 8"
An b -ary redex has b occurrences # 6 ¡ 8 of the is b .
"We bound variable, i.e. the ? length a . ofcall a redex linear if b"
The algorithm  is shown in Figure 6.
"It starts with a constraint | and a variable 3 , which denotes the root of the current -term to be re-duced. (For example, for the redex in Fig. 2, this root would be .)"
The procedure then se-lects an unreduced syntactic redex and adds a de-scription of its reduct at a disjoint position.
"Then the solve procedure is applied to resolve the -reduction constraint, at least partially."
"If it has to disambiguate, it returns one constraint for each reading it finds."
"Finally, usb is called recursively with the new constraint and the root variable of the new -term."
"Intuitively, the solve procedure adds entailed literals to | , making the new -reduction literal more explicit."
"When presented with the left-hand @ constraint in Fig. 1 and the root variable , usb will  add a -reduction constraint for the redex at ; then solve will derive the middle constraint."
"Finally, usb will  call ! itself recursively with the new ,) root variable and try to resolve the redex at , etc."
The partial solving steps do essentially the same as the naı̈ve graph rewriting approach in this case; but the new algorithm will behave differently on problematic constraints as in Fig. 3.
In this section we present a procedure solve for solving -reduction constraints.
We go through several examples to illustrate how it works.
We have to omit some details for lack of space; they can be found[REF_CITE].
The aim of the procedure is to make explicit information that is implicit in -reduction con-straints: it introduces new corresponding vari-ables and copies constraints from the reducing term to the reduced term.
We build upon the solver for -reduction con-straints[REF_CITE].
"This solver is complete, i.e. it can enumerate all solutions of a constraint; but it disambiguates a lot, which we want to avoid in underspecified -reduction."
We obtain an alternative procedure solve by dis-abling all rules which disambiguate and adding some new non-disambiguating rules.
This al-lows us to perform a complete underspecified -reduction for many examples from underspecified semantics without disambiguating at all.
"In those cases where the new rules alone are not sufficient, we can still fall back on the complete solver."
Our constraint solver is based on saturation with a given set of saturation rules.
"Very briefly, this means that a constraint is seen as the set of its lit-erals, to which more and more literals are added of the form | according to saturation ½7oq&gt;­ | rules q says."
Athatsaturationwe can ruleadd one of the | q to any constraint that contains at least the literals in | .
We only apply rules where each possible choice adds new literals to the set; a constraint is saturated under a set ¾ of saturation rules if no rule in ¾ can add anything else. solve returns the set of all possible saturations of its in-put.
"If the rule system contains nondeterminis-tic distribution rules, with bÀ¿Áa , this set can be non-singleton; but the rules we are going to intro-duce ? a are). all deterministic propagation rules (with b"
The main problem in doing underspecified -reduction is that we may not know to which part of a redex a certain node belongs (as in Fig. 1).
We address this problem by introducing under-specified correspondence literals of the form  £ M [ 8 MPOPOPOM 6 £ o M o[ 8 Q 8&lt;6 3 8 ?
"Such a literal is £ satisfied if the tree segments denoted by the ’s and by the [ ’s do not overlap 6 £ M properly &lt;8 6 3 8e? , and there is an r for which co q q[ is satisfied."
In Fig. 7 we present the rules UB for under-specified -reduction; the first five rules are the core of the algorithm.
"To keep the rules short, we use the following abbreviations (with _sb ): ? £2M ¡¢M 8  « 6 £ M ¡ M 8MPOPOPO M  £(M¤£ y 8 M 6 ¡¶M y¡ y 8 yM 6¸y M yq 8 Q 8 ¸yo beta ? co q co"
The procedure solve consists of UB together with the propagation rules[REF_CITE].
The rest of this section shows how this procedure operates and what it can and cannot do.
"First, we discuss the five core rules."
"Rule (Beta) states that whenever the -reduction rela-tion holds, group parallelism holds, too. (This al-lows us to fall back on a complete solver for group parallelism.)"
"Rule (Var) introduces a new variable as a correspondent of a redex variable, and (Lab) and (Dom) copy labeling and dominance literals from the redex to the reduct."
"To understand the exceptions they make, consider e.g. Fig. 5."
"Every node below X has a correspondent in the reduct, except for X ."
"Every labeling relation in the redex also holds in the reduct  , except for the labelings  of the &quot; -node X , the -node X , and the -node X ."
"For the variables that possess a correspon-dent, all dominance relations in the redex hold in the reduct too."
"The rule ( .Inv) copies inverse -binding literals, i.e. the information that all vari-ables bound by a -binder are known."
"For now, it is restricted to linear redexes; for the nonlinear case, we have to take recourse to disambiguation."
It can be shown that the rules in UB are sound in the sense that they are valid implications when interpreted over -structures.
"To see what the rules do, we go through the first reduction step in Fig. 1."
The -reduction con-straint that belongs « 6 £ M ¡ to M this 8 reduction is 6 (£ M ¡¶M 8  £ ? @% M ¡ y ? y²y % with M ? #&quot; M £ y ? =! P)+
"Mö¡ y ? ),=P)+ M ²y ? ),"
"Now saturation can add more constraints, for"
"We get (1), (2), (5) by propagation rules[REF_CITE]: variables bearing differ-ent labels must be different."
"Now we can apply (Var) to get (3) and (4), then (Lab) to get (6)."
"Fi-nally, (7) shows one of the dominances added by (Dom)."
Copies of all other variables and literals can be computed in a completely analogous fash-ion.
"In particular ),* , copying gives us another redex starting at , and we can continue with the algo-rithm usb in Figure 6."
"Note what happens in case of a nonlinear redex, as in the left picture of Fig. 8: as the redex is þ -ary, the rules produce two copies of the ! labeling constraint, one via co and one via co ."
The result is shown on the right-hand side of the figure.
We will return to this example in a minute.
The last two rules in Fig. 7 enforce consistency between scoping in the redex and scoping in the reduct.
"The rules use literals that were 3 introduced Y 6 8 , in 3 (Bodirsky Yÿ 6¡ 3 8 , etcet al., .where, 2001), ,of ¡ theareformssegment terms."
"YZ 6 8 to mean that 3 must be inside 3 We take the 6¡ 8 tree segment denoted by , and 3 we Y take ¡ 6¡ 8 and Y3ÿ (i for ’interior’) to mean that denotes neither the root nor a hole of ."
"As an example, reconsider Fig. 3: by rule 3) cannot represent the term(Par.part), the reduct (right-hand E 6 6[REF_CITE]picture G 8 8 8 becauseof Fig. that would require the operator to be in ÿ 6¡ y 8 ."
"Similarly in Fig. 8, where we have introduced two copies of the label."
"If the in the redex on the left ends up as part of the context, there should be only one copy in the reduct."
"This is brought about by the rule (Par.all) and the fact that correspondence is a function (which is enforced by rules[REF_CITE]which are part of the solver in ([REF_CITE],) ))."
"Together, they can be used to infer that can have only one correspondent in the reduct context."
"In this paper, we have shown how to perform an underspecified -reduction operation in the CLLS framework."
This operation transforms underspec-ified descriptions of higher-order formulas into descriptions of their -reducts.
It can be used to essentially -reduce all readings of an ambiguous sentence at once.
It is interesting to observe how our under-specified -reduction interacts with parallelism constraints that were introduced to model el-lipses.
Consider the elliptical three-reading ex-ample “Peter sees a loophole.
Every lawyer does too.”
"Under the standard analysis of ellipsis in CLLS[REF_CITE], “Peter” must be rep-resented as a generalized quantifier to obtain all three readings."
"This leads to a spurious ambigu- ity in the source sentence, which one would like to get rid of by -reducing the source sentence."
"Our approach can achieve this goal: Adding -reduction constraints for the source sentence leaves the original copy intact, and the target sen-tence still contains the ambiguity."
"Under the simplifying assumption that all re-dexes 6 b are 8 tolinearperform, we canstepsshowof underspecifiedthat it takes time-reduction on a constraint with b variables U ."
"This is feasible for large as long as b , which should be sufficient for most reasonable sen-tences."
"If there are non-linear redexes, the present algorithm can take exponential time because sub-terms are duplicated."
The same problem is known in ordinary -calculus; an interesting question to pursue is whether the sharing techniques devel-oped there[REF_CITE]carry over to the un-derspecification setting.
"In Sec. 6, we only employ propagation rules; that is, we never disambiguate."
"This is concep-tually very nice, but on more complex examples (e.g. in many cases with nonlinear redexes) dis-ambiguation is still needed."
This raises both theoretical and practical issues.
"On the theoretical level, the questions of com-pleteness (elimination of all redexes) and conflu-ence still have to be resolved."
"To that end, we first have to find suitable notions of completeness and confluence in our setting."
Also we would like to handle larger classes of examples without dis-ambiguation.
"On the practical side, we intend to implement the procedure and disambiguate in a controlled fashion so we can reduce completely and still disambiguate as little as possible."
We address the issue of on-line detec-tion of communication problems in spoken dialogue systems.
The useful-ness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances.
"By applying both rule-induction and memory-based learning techniques to data obtained with a Dutch train time-table information system, the current paper demonstrates that the aforementioned features indeed lead to a method for problem detec-tion that performs significantly above baseline."
The results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead.
"The results are interesting from a machine learning perspective, since they show that the rule-based method performs signific-antly better than the memory-based method, because the former is better capable of representing interactions between features."
"Given the state of the art of current language and speech technology, communication problems are unavoidable in present-day spoken dialogue sys-tems."
"The main source of these problems lies in the imperfections of automatic speech recogni-tion, but also incorrect interpretations by the nat-ural language understanding module or wrong de-fault assumptions by the dialogue manager are likely to lead to confusion."
"If a spoken dialogue system had the ability to detect communication problems on-line and with high accuracy, it might be able to correct certain errors or it could in-teract with the user to solve them."
"For instance, in the case of communication problems, it would be beneficial to change from a relatively natural dialogue strategy to a more constrained one in order to resolve the problems (see e.g.,[REF_CITE])."
"Similarly, it has been shown that users switch to a ‘marked’, hyperarticulate speak-ing style after problems (e.g.,[REF_CITE]), which itself is an important source of re-cognition errors."
"This might be solved by using two recognizers in parallel, one trained on nor-mal speech and one on hyperarticulate speech."
"If there are communication problems, then the sys-tem could decide to focus on the recognition res-ults delivered by the engine trained on hyperartic-ulate speech."
"For such approaches to work, however, it is essential that the spoken dialogue system is able to automatically detect communication problems with a high accuracy."
"In this paper, we investigate the usefulness for problem detection of the word graph and the history of system question types."
"These features are present in many spoken dia-logue systems and do not require additional com-putation, which makes this a very cheap method to detect problems."
"We shall see that on the basis of the previous and the current word graph and the six most recent system question types, communic-ation problems can be detected with an accuracy of 91%, which is a significant improvement over the relevant baseline."
This shows that spoken dia-logue systems may use these features to better pre-dict whether the ongoing dialogue is problematic.
"In addition, the current work is interesting from a machine learning perspective."
We apply two machine learning techniques: the memory-based IB 1- IG algorithm ([REF_CITE]) and the RIPPER rule induction algorithm[REF_CITE].
"As we shall see, some interesting differences between the two approaches arise."
Recently there has been an increased interest in developing automatic methods to detect problem-atic dialogue situations using machine learning techniques.
"For instance,[REF_CITE]and[REF_CITE]use RIPPER[REF_CITE]to classify problematic and unproblematic dialogues."
"Following up on this,[REF_CITE]aim at detecting problems at the utter-ance level, based on data obtained with AT&amp;Ts"
How May I Help You ( HMIHY ) system[REF_CITE].
"Walker and co-workers apply[REF_CITE]features which are automatically generated by three modules of the HMIHY system, namely the speech recognizer (ASR), the natural language un-derstanding module (NLU) and the dialogue man-ager (DM)."
"The best result is obtained using all features: communication problems are detected with an accuracy of 86%, a precision of 83% and a recall of 75%."
It should be noted that the NLU features play first fiddle among the set of all fea-tures.
"In fact, using only the NLU features per-forms comparable to using all features."
"The results which Walker and co-workers de-scribe show that it is possible to automatically de-tect communication problems in the HMIHY sys-tem, using machine learning techniques."
"Their ap-proach also raises a number of interesting follow-up questions, some concerned with problem de-tection, others with the use of machine learning techniques. (1) Walker et al. train their classi-fier on a large set of features, and show that the set of features produced by the NLU module are the most important ones."
"However, this leaves an important general question unanswered, namely which particular features contribute to what ex-tent? (2) Moreover, the set of features which the NLU module produces appear to be rather spe-cific to the HMIHY system and indicate things like the percentage of the input covered by the relev-ant grammar fragment, the presence or absence of context shifts, and the semantic diversity of sub-sequent utterances."
"Many current day spoken dia-logue systems do not have such a sophisticated NLU module, and consequently it is unlikely that they have access to these kinds of features."
"In sum, it is uncertain whether other spoken dialogue systems can benefit from the findings described[REF_CITE], since it is unclear which fea-tures are important and to what extent these fea-tures are available in other spoken dialogue sys-tems."
"Finally, (3) we agree with Walker et al. (and the machine learning community at large) that it is important to compare different machine learning techniques to find out which techniques perform well for which kinds of tasks."
Walker et al. found that RIPPER does not perform significantly better or worse than a memory-based learning technique.
Is this incidental or does it reflect a general prop-erty of the problem detection task?
"The current paper uses a similar methodology for on-line problem detection[REF_CITE], but (1) we take a bottom-up approach, focussing on a small number of features and in-vestigating their usefulness on a per-feature basis and (2) the features which we study are automat-ically available in the majority of current spoken dialogue system: the sequence of system ques-tion types and the word graphs corresponding to the respective user utterances."
"A word graph is a lattice of word hypotheses, and we conjec-ture that various features which have been shown to cue communication problems (prosodic, lin-guistic and ASR features, see e.g.,[REF_CITE]and[REF_CITE]) have correlates in the word graph."
The se-quence of system question types is taken to model the dialogue history.
"Finally, (3) to gain further in-sight into the adequacy of various machine learn- ing techniques for problem detection we use both RIPPER and the memory-based IB 1- IG algorithm."
"3.1 Data and Labeling The corpus we used con-sisted of 3739 question-answer pairs, taken from 444 complete dialogues."
The dialogues consist of users interacting with a Dutch spoken dialogue system which provides information about train time tables.
"The system prompts the user for un-known slots, such as departure station, arrival sta-tion, date, etc., in a series of questions."
The sys-tem uses a combination of implicit and explicit verification strategies.
The data were annotated with a highly limited set of labels.
"In particular, the kind of system question and whether the reply of the user gave rise to communication problems or not."
The latter feature is the one to be predicted.
The following labels are used for the system questions.
O open questions (“From where to where do you want to travel?”)
I implicit verification (“When do you want to travel from Tilburg to Schiphol Airport?”)
E explicit verification (“So you want to travel from Tilburg to Schiphol Airport?”)
Y yes/ no question (“Do you want me to repeat the connection?”)
M Meta-questions (“Can you please correct me?”)
The difference between an explicit verification and a yes/no question is that the former but not the latter is aimed at checking whether what the system understood or assumed corresponds with what the user wants.
"If the current system ques-tion is a repetition of the previous question it asked, this is indicated by the suffix R. A ques-tion only counts as a repetition when it has the same contents as the previous system question."
"Of the user inputs, we only labeled whether they gave rise to a communication problem or not."
"A com-munication problem arises when the value which the system assigns to a particular slot (departure station, date, etc.) does not coincide with the value given for that particular slot by the user in his or her most recent contribution to the dialogue or when the system makes an incorrect default as-sumption (e.g., the dialogue manager assumes that the date slot should be filled with the current date, i.e., that the user wants to travel today)."
Commu-nication problems are generally easy to label since the spoken dialogue system under consideration here always provides direct feedback (via verific-ation questions) about what it believes the user in-tends.
Consider the following exchange.
U: I want to go to Amsterdam.
So you want to go to Rotterdam?
"As soon as the user hears the explicit verification question of the system, it will be clear that his or her last turn was misunderstood."
The problem-feature was labeled by two of the authors to avoid labeling errors.
Differences between the two annotators were infrequent and could always easily be resolved.
1564 gave rise to communication problems (an error rate of 41.8%).
"The majority class is thus formed by the unproblematic user utterances, which form 58.2% of all user utterances."
This suggests that the baseline for predicting com-munication problems is obtained by always predicting that there are no communication prob-lems.
"This strategy has an accuracy of 58.2%, and a recall of 0% (all problems are missed)."
"The precision is not defined, and consequently neither is the ."
"This baseline is misleading, however, when we are interested in predicting whether the previous user utterance gave rise to communication problems."
There are cases when the dialogue system is itself clearly aware of communication problems.
This is in particular the case when the system repeats the question (labeled with the suffix R) or when it asks a meta-question (M).
In the corpus under investigation here this happens 1024 times.
It would not be very illuminating to develop an automatic error detector which detects only those problems that the system was already aware of.
"Therefore we take the following as our base-line strategy for predicting whether the previous user utterance gave rise to problems, henceforth referred to as the system-knows-baseline: if the Q( ( ) is repetition or meta-question, then predict user utterance ( -1 caused problems, else predict user utterance ( -1 caused no problems."
"This ‘strategy’ predicts problems with an ac-curacy of 85.6% (1024 of the 1564 problems are detected, thus 540 of 3739 decisions are wrong), a precision of 100% (of 1024 predicted problems 1024 were indeed problematic), a recall of 65.5% (1024 of the 1564 problems are predicted to be problematic) and thus an )* of 79.1."
"This is a sharp baseline, but for predicting whether the previous user utterance caused problems or not the system-knows-baseline is much more informative and relevant than the majority-class-baseline."
Table 1 summarizes the baselines.
pairs were represented as feature vectors (or patterns) of the following form.
Six features were reserved for the history of system questions asked so far in the current dialogue (6Q).
"Of course, if the system only asked 3 questions so far, only 3 types of system questions are stored in memory and the remaining three features for system ques-tion are not assigned a value."
The representation of the user’s answer is derived from the word graph produced by the ASR module.
It should be kept in mind that in general the word graph is much more complex than the recognized string.
"The latter typically is the most plausible path (e.g., on the basis of acoustic confidence scores) in the word graph, which itself may contain many other paths."
Different systems determine the plausibility of paths in the word graph in different ways.
"Here, for the sake of generality, we abstract over such differences and simply represent a word graph as a Bag of Words (BoW), collecting all words that occur in one of the paths, irrespective of the associated acoustic confidence score."
A lexicon was derived of all the words and phrases that occurred in the corpus.
"Each word graph is represented as a sequence of bits, where the + -th bit is set to 1 if the + -th word in the pre-derived lexicon occurred at least once in the word graph corresponding to the current user utterance and 0 otherwise."
"Finally, for each user utterance, a feature is reserved for indicating whether it gave rise to communication problems or not."
This latter feature is the one to be predicted.
There are basically two approaches for detect-ing communication problems.
One is to try to decide on the basis of the current user utterance whether it will be recognized and interpreted correctly or not.
The other approach uses the current user utterance to determine whether the processing of the previous user utterance gave rise to communication problems.
This approach is based on the assumption that users give feedback on communication problems when they notice that the system misunderstood their previous input.
"In this study, eight prediction tasks have been defined: the first three are con-cerned with predicting whether the current user input will cause problems, and naturally, for these three tasks, the majority-class-baseline is the relevant one; the last five tasks are concerned with predicting whether the previous user utter-ance caused problems, and for these the sharp, system-knows-baseline is the appropriate one."
"The eight tasks are: (1) predict on the basis of the (representation of the) current word graph BoW ( whether the current user utterance (at time ( ) will cause a communication problem, (2) predict on the basis of the six most recent system question types up to ( (6Q ( ), whether the current user utterance will cause a communication problem, (3) predict on the basis of both BoW ( and 6Q ( , whether the current user utterance will cause a problem, (4) predict on the basis of the current word graph BoW ( , whether the previous user ut-terance, uttered at time ( -1, caused a problem, (5) predict on the basis of the six most recent system questions, whether the previous user utterance caused a problem, (6) predict on the basis of BoW ( and 6Q ( , whether the previous user utterance caused a problem, (7) predict on the basis of the two most recent word graphs, BoW ( -1 and BoW ( , whether the previous user utterance caused a problem, and finally (8) predict on the basis of the two most recent word graphs, BoW ( -1 and BoW ( , and the six most recent system question types 6Q ( , whether the previous user utterance caused a problem."
"used the rule-induction algorithm RIPPER[REF_CITE]and the memory-based IB 1- IG algorithm ([REF_CITE]). ,"
RIPPER is a fast rule induction algorithm.
It starts with splitting the training set in two.
"On the basis of one half, it induces rules in a straightfor-ward way (roughly, by trying to maximize cov-erage for each rule), with potential overfitting."
"When the induced rules classify instances in the other half below a certain threshold, they are not stored."
Rules are induced per class.
"By default the ordering is from low-frequency classes to high frequency ones, leaving the most frequent class as the default rule, which is generally beneficial for the size of the rule set."
The memory-based IB 1- IG algorithm is one of the primary memory-based learning algorithms.
"Memory-based learning techniques can be char-acterized by the fact that they store a representa-tion of a set of training data in memory, and clas-sify new instances by looking for the most sim-ilar instances in memory."
"The most basic distance function between two features is the overlap met-ric in (1), where /- .103254$6 is the distance between patterns 0 and 4 (both consisting of 7 features) and 8 is the distance between the features."
"If 0 is the test-case, the - measure determines which group 9 of cases 4 in memory is the most sim-ilar to 0 ."
The most frequent value for the relevant category in 9 is the predicted value for 0 .
"Usu-ally, 9 is set to 1."
"Since some features are more important than others, a weighting function ;=&lt; is used."
Here ;=&lt; is the gain ratio measure.
"In sum, the weighted distance between vectors 0 and 4 of length 7 is determined by the following equa-tion, where ?. & gt;@&lt; C6&lt; gives a point-wise distance between features which is 1 if @&gt; &lt;EFD &lt; and 0 oth-erwise. /- 10[REF_CITE]H6."
F J I 8K?.
L&gt; &lt;&lt;M6 (1) &lt;
"Both learning techniques were used for the same 8 prediction tasks, and received exactly the same feature vectors as input."
"All experiments were performed using ten-fold cross-validation, which yields errors margins in the predictions."
First we look at the results obtained with the IB 1- IG algorithm (see Table 2).
Consider the problem of predicting whether the current user utterance will cause problems.
"Either looking at the current word graph (BoW ( ), at the six most recent sys-tem questions (6Q ( ) or at both, leads to a signi-ficant improvement with respect to the majority-class-baseline."
The best results are obtained with only the system question types (although the dif-ference with the results for the other two tasks is not significant): a 63.7% accuracy and an of 58.3.
"However, even though this is a signific-ant improvement over the majority-class-baseline, the accuracy is improved with only 5.5%."
Next consider the problem of predicting whether the previous user utterance caused communication problems (these are the five remaining tasks).
The best result is obtained by taking the two most recent word graphs and the six most recent system question types as input.
"This yields an accuracy of 88.1%, which is a significant improvement with respect to the P"
All checks for significance were performed with a one-tailed R Q test.
"As an aside, we performed one experiment with the words in the actual, transcribed user utterance at time Q in-stead of BoW Q , where the task is to predict whether the cur- sharp, system-knows-baseline."
"In addition, the )* of 84.8 is nearly 6 points higher than that of the relevant, majority-class baseline."
The results obtained with RIPPER are shown in Table 3.
"On the problem of predicting whether the current user utterance will cause a problem, RIPPER obtains the best results by taking as input both the current word graph and the types of the six most recent system questions, predicting prob-lems with an accuracy of 66.0%."
"This is a signific-ant improvement over the majority-class-baseline, but the result is not significantly better than that obtained with either the word graph or the system questions in isolation."
"Interestingly, the result is significantly better than the results for IB 1- IG on the same task."
"On the problem of predicting whether the previ-ous user utterance caused a problem, RIPPER ob-tains the best results by taking all features into ac-count (that is: the two most recent bags of words and the six system questions). c"
"This results in a 91.1% accuracy, which is a significant improve-ment over the sharp system-knows-baseline."
"This implies that 38% of the communication problems which were not detected by the dialogue system the basis of the Bags of Words for ( and ( -1, and the six most recent system questions."
Based on the entire data set.
The question features are defined in section 2.
"The word “naar” is Dutch for to, “om” for at, “uur” for hour, “van” for from, “vanuit” is slightly archaic variant of “van” (from), “ik” is Dutch for I, “nee” for no, “niet” for not and “wil”, finally, for want."
The ( 7 / i ) numbers at the end of each line indicate how many correct ( 7 ) and incorrect ( i ) decisions were taken using this particular if ...then ...statement. under investigation could be classified correctly using features which were already present in the system (word graphs and system question types).
"Moreover, the is 89, which is 10 points higher than the * associated with the system-knows baseline strategy."
Notice also that this RIP - PER result is significantly better than the IB 1- IG results for the same task.
"To gain insight into the rules learned by RIP - PER for the last task, we applied RIPPER to the complete data set."
The rules induced are dis-played in Figure 1.
RIPPER ’s first rule is con-cerned with repeated questions (compare with the system-knows-baseline).
"One important property of many other rules is that they explicitly com-bine pieces of information from the three main sources of information (the system questions, the current word graph and the previous word graph)."
"Moreover, it is interesting to note that the words which crop up in the RIPPER rules are primarily function words."
Another noteworthy feature of the RIPPER rules is that they reflect certain prop-erties which have been claimed to cue commu-nication problems.
"For instance,[REF_CITE], in their descriptive analysis of dialogue problems, found that repeated material is often an indication of problems, as is the use of a marked vocabulary."
"The rules 2, 3 and 7 are examples of the former cue, while the occurrence of the somewhat archaic “vanuit” instead of the ordinary “van” is an example of the latter."
"In this study we have looked at automatic meth-ods for problem detection using simple features which are available in the vast majority of spoken dialogue systems, and require little or no com-putational overhead."
We have investigated two approaches to problem detection.
"The first ap-proach is aimed at testing whether a user utter-ance, captured in a noisy j word graph, and/or the recent history of system utterances, would be pre-dictive of whether the utterance itself would be misrecognised."
"The results, which basically rep-resents a signal quality test, show that problem-atic cases could be discerned with an accuracy of about 65%."
"Although this is somewhat above the baseline of 58% decision accuracy when no problems would be predicted, signalling recogni-tion problems with word graph features and previ-ous system question types as predictors is a hard task."
"As other studies suggest (e.g.,[REF_CITE]), confidence scores and acoustic/prosodic features could be of help."
The second approach tested whether the word graph for the current user utterance and/or the re-cent history of system question types could be employed to predict whether the previous user utterance caused communication problems.
The underlying assumption is that users will signal problems as soon as they become aware of them through the feedback provided by the system.
"Thus, in a sense, this second approach represents a noisy channel filtering task: the current utterance has to be decoded as signalling a problem or not."
"As the results show, this task can be performed at a surprisingly high level: about 91% decision ac-curacy (which is an error reduction of 38%), with an )* of the problem category of 89."
This res-ult can only be obtained using a combination of features; neither the word graph features in isola-tion nor the system question types in isolation of-fer enough predictive power to reach above the sharp baseline of 86% accuracy and an )* on the problem category of 79.
Keeping information sources isolated or combining them directly influences the relative performances of the memory-based IB 1- IG algorithm versus the RIPPER rule induction algorithm.
"When features are of the same type, accuracies of the memory-based and the rule-induction systems do not differ significantly (with one exception)."
"In contrast, when features from different sources (e.g., words in the word graph and question type features) are combined, RIPPER profits more than IB 1- IG does, causing RIPPER to perform significantly more accurately."
"The fea-ture independence assumption of memory-based learning appears to be the harming cause: by its definition, IB 1- IG does not give extra weight to apparently relevant interactions of feature values from different sources."
"In contrast, in nine out of the twelve rules that RIPPER produces, word graph features and system questions type features are explicitly integrated as joint left-hand side conditions."
"The current results show that for on-line detec-tion of communication problems at the utterance level it is already beneficial to pay attention only to the lexical information in the word graph and the sequence of system question types, features which are present in most spoken dialogue system and which can be obtained with little or no com-putational overhead."
"An approach to automatic problem detection is potentially very useful for spoken dialogue systems, since it gives a quantit-ative criterion for, for instance, changing the dia- logue strategy (initiative, verification) or speech recognition engine (from one trained on normal speech to one trained on hyperarticulate speech)."
"~     $E      d E   &amp;          =E    |  |  T       O$    3|   1        T           )       1    0d E  OE O   3    0 1 &amp;E              =  E)       E =      E   O       |   E      E    E          @       3T               @      ¡      1E  E      E                  1  E  E   EO1          31 O3 TO~    1 E   E1   E         ,  1             1           &amp;      E            | 1   11             E    O¢¤£   ?  E ¦  6EB  5E E             E        1          =1E                     "
Educators are interested in essay evaluation systems that include feedback about writing features that can facilitate the essay revision process.
"For instance, if the thesis statement of a student’s essay could be automatically identified, the student could then use this information to reflect on the thesis statement with regard to its quality, and its relationship to other discourse elements in the essay."
"Using a relatively small corpus of manually annotated data, we use Bayesian classification to identify thesis statements."
This method yields results that are much closer to human performance than the results produced by two baseline systems.
Automated essay scoring technology can achieve agreement with a single human judge that is comparable to agreement between two single human judges ([REF_CITE]; and[REF_CITE]).
"Unfortunately, providing students with just a score (grade) is insufficient for instruction."
"To help students improve their writing skills, writing evaluation systems need to provide feedback that is specific to each individual’s writing and that is applicable to essay revision."
"The factors that contribute to improvement of student writing include refined sentence structure, variety of appropriate word usage, and organizational structure."
The improvement of organizational structure is believed to be critical in the essay revision process toward overall improvement of essay quality.
"Therefore, it would be desirable to have a system that could indicate as feedback to students, the discourse elements in their essays."
Such a system could present to students a guided list of questions to consider about the quality of the discourse.
"For instance, it has been suggested by writing experts that if the thesis statement [Footnote_1] of a student’s essay could be automatically provided, the student could then use this information to reflect on the thesis statement and its quality."
1 A thesis statement is generally defined as the sentence that explicitly identifies the purpose of the paper or previews its main ideas. See the Literacy Education On-line (LEO) site[URL_CITE]
"In addition, such an instructional application could utilize the thesis statement to discuss other types of discourse elements in the essay, such as the relationship between the thesis statement and the conclusion, and the connection between the thesis statement and the main points in the essay."
"In the teaching of writing, in order to facilitate the revision process, students are often presented with ‘Revision Checklists.’"
A revision checklist is a list of questions posed to the student to help the student reflect on the quality of his or her writing.
Such a list might pose questions such as: a) Is the intention of my thesis statement clear? b) Does my thesis statement respond directly to the essay question? c) Are the main points in my essay clearly stated? d) Do the main points in my essay relate to my original thesis statement?
"If these questions are expressed in general terms, they are of little help; to be useful, they need to be grounded and need to refer explicitly to the essays students write[REF_CITE]."
The ability to automatically identify and present to students the discourse elements in their essays can help them focus and reflect on the critical discourse structure of the essays.
"In addition, the ability for the application to indicate to the student that a discourse element could not be located, perhaps due to the ‘lack of clarity’ of this element, could also be helpful."
"Assuming that such a capability was reliable, this would force the writer to think about the clarity of an intended discourse element, such as a thesis statement."
"Using a relatively small corpus of essay data where thesis statements have been manually annotated, we built a Bayesian classifier using the following features: sentence position; words commonly used in thesis statements; and discourse features, based on Rhetorical Structure Theory (RST) parses ([REF_CITE]and[REF_CITE])."
Our results indicate that this classification technique may be used toward automatic identification of thesis statements in essays.
"Furthermore, we show that this method generalizes across essay topics."
A thesis statement is defined as the sentence that explicitly identifies the purpose of the paper or previews its main ideas (see footnote 1).
"This definition seems straightforward enough, and would lead one to believe that even for people to identify the thesis statement in an essay would be clear-cut."
"However, the essay in Figure 1 is a common example of the kind of first-draft writing that our system has to handle."
Figure 1 shows a student response to the essay question:
Often in life we experience a conflict in choosing between something we &quot;want&quot; to do and something we feel we &quot;should&quot; do.
"In your opinion, are there any circumstances in which it is better for people to do what they &quot;want&quot; to do rather than what they feel they &quot;should&quot; do?"
Support your position with evidence from your own experience or your observations of other people.
"The writing in Figure 1 illustrates one kind of challenge in automatic identification of discourse elements, such as thesis statements."
"In this case, the two human annotators independently chose different text as the thesis statement (the two texts highlighted in bold and italics in Figure 1)."
"In this kind of first-draft writing, it is not uncommon for writers to repeat ideas, or express more than one general opinion about the topic, resulting in text that seems to contain multiple thesis statements."
"Before building a system that automatically identifies thesis statements in essays, we wanted to determine whether the task was well-defined."
"In collaboration with two writing experts, a simple discourse-based annotation protocol was developed to manually annotate discourse elements in essays for a single essay topic."
"This was the initial attempt to annotate essay data using discourse elements generally associated with essay structure, such as thesis statement, concluding statement, and topic sentences of the essay’s main ideas."
The writing experts defined the characteristics of the discourse labels.
"These experts then annotated 100 essay responses to one English Proficiency Test (EPT) question, called Topic B, using a PC-based interface implemented in Java."
"We computed the agreement between the two human annotators using the kappa coefficient[REF_CITE], a statistic used extensively in previous empirical studies of discourse."
"The kappa statistic measures pairwise agreement among a set of coders who make categorial judgments, correcting for chance expected agreement."
"The kappa agreement between the two annotators with respect to the thesis statement labels was 0.733 (N=2391, where 2391 represents the total number of sentences across all annotated essay responses)."
This shows high agreement based on research in content analysis[REF_CITE]that suggests that values of kappa higher than 0.8 reflect very high agreement and values higher than 0.6 reflect good agreement.
"The corresponding z statistic was 27.1, which reflects a confidence level that is much higher than 0.01, for which the corresponding z value is 2.32[REF_CITE]."
"In the early stages of our project, it was suggested to us that thesis statements reflect the most important sentences in essays."
"In terms of summarization, these sentences would represent indicative, generic summaries[REF_CITE]."
"To test this hypothesis (and estimate the adequacy of using summarization technology for identifying thesis statements), we carried out an additional experiment."
"The same annotation tool was used with two different human judges, who were asked this time to identify the most important sentence of each essay."
The agreement between human judges on the task of identifying summary sentences was significantly lower: the kappa was 0.603 (N=2391).
Tables 1a and 1b summarize the results of the annotation experiments.
Table 1a shows the degree of agreement between human judges on the task of identifying thesis statements and generic summary sentences.
"The agreement figures are given using the kappa statistic and the relative precision (P), recall (R), and F-values (F), which reflect the ability of one judge to identify the sentences labeled as thesis statements or summary sentences by the other judge."
The results in Table 1a show that the task of thesis statement identification is much better defined than the task of identifying important summary sentences.
"In addition, Table 1b indicates that there is very little overlap between thesis and generic summary sentences: just 6% of the summary sentences were labeled by human judges as thesis statement sentences."
"This strongly suggests that there are critical differences between thesis statements and summary sentences, at least in first-draft essay writing."
"It is possible that thesis statements reflect an intentional facet[REF_CITE]of language, while summary sentences reflect a semantic one[REF_CITE]."
More detailed experiments need to be carried out though before proper conclusions can be derived.
Thesis statements vs. Summary sentences Percent Overlap 0.06
The results in Table 1a provide an estimate for an upper bound of a thesis statement identification algorithm.
"If one can build an automatic classifier that identifies thesis statements at recall and precision levels as high as 70%, the performance of such a classifier will be indistinguishable from the performance of humans."
We initially built a Bayesian classifier for thesis statements using essay responses to one English Proficiency Test (EPT) test question:
"They describe the multinominal model as being the more traditional approach for statistical language modeling (especially in speech recognition applications), where a document is represented by a set of word occurrences, and where probability estimates reflect the number of word occurrences in a document."
"In using the alternative, multivariate Bernoulli model, a document is represented by both the absence and presence of features."
"On a text classification task,[REF_CITE]show that the multivariate Bernoulli model performs well with small vocabularies, as opposed to the multinominal model which performs better when larger vocabularies are involved."
"Furthermore, the vocabulary used across these documents tends to be restricted."
"Based on the success of Larkey’s experiments, and McCallum and Nigam’s findings that the multivariate Bernoulli model performs better on texts with small vocabularies, this approach would seem to be the likely choice when dealing with data sets of essay responses."
"Therefore, we have adopted this approach in order to build a thesis statement classifier that can select from an essay the sentence that is the most likely candidate to be labeled as thesis statement. [Footnote_2]"
"2 In our research, we trained classifiers using a classical Bayes approach too, where two classifiers were built: a thesis classifier and a non-thesis"
"In our experiments, we used three general feature types to build the classifier: sentence position; words commonly occurring in thesis statements; and RST labels from outputs generated by an existing rhetorical structure parser[REF_CITE]."
We trained the classifier to predict thesis statements in an essay.
"Using the multivariate Bernoulli formula, below, this gives us the log probability that a sentence (S) in an essay belongs to the class (T) of sentences that are thesis statements."
"We found that it helped performance to use a Laplace estimator to deal with cases where the probability estimates were equal to zero. log(P(T | S)) =  log(P(A | T) /P(A)), i i log(P(T)) + ∑  if S contains A i  log(P(A i i | T) /P(A i )),  if S does not contain A i"
"In this formula, P(T) is the prior probability that a sentence is in class T, P(A i |T) is the conditional probability of a sentence having feature A i , given that the sentence is in T, and P(A i ) is the prior probability that a sentence contains feature A i ,"
"P( A i |T) is the conditional probability that a sentence does not have feature A i , given that it is in T, and P( A i ) is the prior probability that a sentence does not contain feature A i."
We found that the likelihood of a thesis statement occurring at the beginning of essays was quite high in the human annotated data.
"To account for this, we used one feature that reflected the position of each sentence in an essay."
All words from human annotated thesis statements were used to build the Bayesian classifier.
We will refer to these words as the thesis word list.
"From the training data, a vocabulary list was created that included one occurrence of each word used in all resolved human annotations of thesis statements."
All words in this list were used as independent lexical features.
"We found that the use of various lists of stop words decreased the performance of our classifier, so we did not use them."
"According to RST[REF_CITE], one can associate a rhetorical structure tree to any text."
The leaves of the tree correspond to elementary discourse units and the internal nodes correspond to contiguous text spans.
"Each node in a tree is characterized by a status (nucleus or satellite) and a rhetorical relation, which is a relation that holds between two non-overlapping text spans."
"The distinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer’s intention than the satellite; and that the nucleus of a rhetorical relation is comprehensible independent of the satellite, but not vice versa."
"When spans are equally important, the relation is multinuclear."
"Rhetorical relations reflect semantic, intentional, and textual relations that hold between text spans as is illustrated in Figure 2."
"For example, one text span may elaborate on another text span; the information in two text spans may be in contrast; and the information in one text span may provide background for the information presented in another text span."
Figure 2 displays in the style[REF_CITE]the rhetorical structure tree of a text fragment.
"In Figure 2, nuclei are represented using straight lines; satellites using arcs."
Internal nodes are labeled with rhetorical relation names.
We built RST trees automatically for each essay using the cue-phrase-based discourse parser[REF_CITE].
"We then associated with each sentence in an essay a feature that reflected the status of its parent node (nucleus or satellite), and another feature that reflected its rhetorical relation."
"For example, for the last sentence in Figure 2 we associated the status satellite and the relation elaboration because that sentence is the satellite of an elaboration relation."
"For sentence 2, we associated the status nucleus and the relation elaboration because that sentence is the nucleus of an elaboration relation."
We found that some rhetorical relations occurred more frequently in sentences annotated as thesis statements.
"Therefore, the conditional probabilities for such relations were higher and provided evidence that certain sentences were thesis statements."
"The Contrast relation shown in Figure 2, for example, was a rhetorical relation that occurred more often in thesis statements."
"Arguably, there may be some overlap between words in thesis statements, and rhetorical relations used to build the classifier."
"The RST relations, however, capture long distance relations between text spans, which are not accounted by the words in our thesis word list."
We estimated the performance of our system using a six-fold cross validation procedure.
We partitioned the 93 essays that were labeled by both human annotators with a thesis statement into six groups. (The judges agreed that 7 of the 100 essays they annotated had no thesis statement.)
We trained six times on 5/6 of the labeled data and evaluated the performance on the other 1/6 of the data.
The evaluation results in Table 2 show the average performance of our classifier with respect to the resolved annotation (Alg. wrt.
"Resolved), using traditional recall (R), precision (P), and F-value (F) metrics."
"For purposes of comparison, Table 2 also shows the performance of two baselines: the random baseline classifies the thesis statements randomly; while the position baseline assumes that the thesis statement is given by the first sentence in each essay."
"In commercial settings, it is crucial that a classifier such as the one discussed in Section 3 generalizes across different test questions."
"New test questions are introduced on a regular basis; so it is important that a classifier that works well for a given data set works well for other data sets as well, without requiring additional annotations and training."
"For the thesis statement classifier it was important to determine whether the positional, lexical, and RST-specific features are topic independent, and thus generalizable to new test questions."
"If so, this would indicate that we could annotate thesis statements across a number of topics, and re-use the algorithm on additional topics, without further annotation."
"We asked a writing expert to manually annotate the thesis statement in approximately 45 essays for 4 additional test questions: Topics A, C, D and E. The annotator completed this task using the same interface that was used by the two annotators in Experiment 1."
To test generalizability for each of the five
"EPT questions, the thesis sentences selected by a writing expert were used for building the classifier."
"Five combinations of 4 prompts were used to build the classifier in each case, and the resulting classifier was then cross-validated on the fifth topic, which was treated as test data."
"To evaluate the performance of each of the classifiers, agreement was calculated for each ‘cross-validation’ sample (single topic) by comparing the algorithm selection to our writing expert’s thesis statement selections."
"For example, we trained on Topics A, C, D, and E, using the thesis statements selected manually."
"This classifier was then used to select, automatically, thesis statements for Topic B. In the evaluation, the algorithm’s selection was compared to the manually selected set of thesis statements for Topic B, and agreement was calculated."
"Table 3 illustrates that in all but one case, agreement exceeds both baselines from Table 2."
"In this set of manual annotations, the human judge almost always selected one sentence as the thesis statement."
"This is why Precision, Recall, and the F-value are often equal in Table 3."
Table 3: Cross-topic generalizability of the thesis statement classifier.
The results of our experimental work indicate that the task of identifying thesis statements in essays is well defined.
"The empirical evaluation of our algorithm indicates that with a relatively small corpus of manually annotated essay data, one can build a Bayes classifier that identifies thesis statements with good accuracy."
The evaluations also provide evidence that this method for automated thesis selection in essays is generalizable.
"That is, once trained on a few human annotated prompts, it can be applied to other prompts given a similar population of writers, in this case, writers at the college freshman level."
"The larger implication is that we begin to see that there are underlying discourse elements in essays that can be identified, independent of the topic of the test question."
For essay evaluation applications this is critical since new test questions are continuously being introduced into on-line essay evaluation applications.
"Our results compare favorably with results reported[REF_CITE]who also use Bayes classification techniques to identify rhetorical arguments such as aim and background in scientific texts, although the texts we are working with are extremely noisy."
"Because EPT essays are often produced for high-stake exams, under severe time constraints, they are often ungrammatical, repetitive, and poorly organized at the discourse level."
"Current investigations indicate that this technique can be used to reliably identify other essay-specific discourse elements, such as, concluding statements, main points of arguments, and supporting ideas."
"In addition, we are exploring how we can use estimated probabilities as confidence measures of the decisions made by the system."
"If the confidence level associated with the identification of a thesis statement is low, the system would instruct the student that no explicit thesis statement has been found in the essay."
"The RAGS proposals for generic speci-fication of NLG systems includes a de-tailed account of data representation, but only an outline view of processing aspects."
In this paper we introduce a modular processing architecture with a concrete implementation which aims to meet the RAGS goals of transparency and reusability.
We illustrate the model with the RICHES system – a generation system built from simple linguistically-motivated modules.
"As part of the RAGS (Reference Architecture for Generation Systems) project,[REF_CITE]introduces a framework for the representation of data in NLG systems, the RAGS ‘data model’."
"This model offers a formally well-defined declar-ative representation language, which supports the complex and dynamic data requirements of gen-eration systems, e.g. different levels of repre-sentation (conceptual to syntax), mixed represen-tations that cut across levels, partial and shared structures and ‘canned’ representations."
"RAGS , as described in that paper, says very little about the functional structure of an NLG system, or the issues arising from more complex process-ing regimes (see for example[REF_CITE],[REF_CITE]for further discussion)."
"NLG systems, especially end-to-end, applied NLG systems, have many functionalities in com-mon."
"More recently[REF_CITE]attempted to re-peat the analysis, but found that while most sys-tems did implement a pipeline, they did not im-plement the same pipeline – different functional-ities occurred in different ways and different or-ders in different systems."
But this survey did identify a number of core functionalities which seem to occur during the execution of most sys-tems.
"In order to accommodate this result, a ‘pro-cess model’ was sketched which aimed to support both pipelines and more complex control regimes in a flexible but structured way (see[REF_CITE],[REF_CITE])."
"In this paper, we describe our attempts to test these ideas in a simple NLG application that is based on a concrete realisation of such an architecture [Footnote_1] ."
"1 More details about the RAGS project, the RICHES implementation and the OASYS subsys-tem can be found at the RAGS project web site[URL_CITE]"
"The RAGS data model aims to promote com-parability and re-usability in the NLG research community, as well as insight into the organisa-tion and processing of linguistic data in NLG."
The present work has similar goals for the processing aspects: to propose a general approach to organis-ing whole NLG systems in a way which promotes the same ideals.
"In addition, we aim to test the claims that the RAGS data model approach sup-ports the flexible processing of information in an NLG setting."
The starting point for our work here is the RAGS data model as presented[REF_CITE].
This model distinguishes the following five levels of data representation that underpin the genera-tion process:
Rhetorical representations (RhetReps) define how propo-sitions within a text are related.
"For example, the sen-tence “Blow your nose, so that it is clear” can be con-sidered to consist of two propositions: BLOW YOUR NOSE and YOUR NOSE"
"IS CLEAR , connected by a re-lation like MOTIVATION ."
"Document representations (DocReps) encode information about the physical layout of a document, such as tex-tual level (paragraph, orthographic sentence, etc.), layout (indentation, bullet lists etc.) and their relative positions."
Semantic representations (SemReps) specify information about the meaning of individual propositions.
"For each proposition, this includes the predicate and its arguments, as well as links to underlying domain ob-jects and scoping information."
"Syntactic representations (SynReps) define “abstract” syntactic information such as lexical features ( FORM , ROOT etc.) and syntactic arguments and adjuncts ( SUBJECT , OBJECT etc.)."
"Quote representations These are used to represent literal unanalysed content used by a generator, such as canned text, pictures or tables."
"The representations aim to cover the core com-mon requirements of NLG systems, while avoid-ing over-commitment on less clearly agreed is-sues relating to conceptual representation on the one hand and concrete syntax and document ren-dering on the other."
"When one considers process-ing aspects, however, the picture tends to be a lot less tidy: typical modules in real NLG systems often manipulate data at several levels at once, building structures incrementally, and often work-ing with ‘mixed’ structures, which include infor-mation from more than one level."
Furthermore this characteristic remains even when one consid-ers more purely functionally-motivated ‘abstract’ NLG modules.
"For example, Referring Expres-sion Generation, commonly viewed as a single task, needs to have access to at least rhetorical and document information as well as referencing and adding to the syntactic information."
"To accommodate this, the RAGS data model in-cludes a more concrete representational proposal, called the ‘whiteboard’[REF_CITE], in which all the data levels can be represented in a common framework consisting of networks of typed ‘objects’ connected by typed ‘arrows’."
This lingua franca allows NLG modules to manipulate data flexibly and consistently.
"It also facilitates modular design of NLG systems, and reusability of modules and data sets."
"However, it does not in itself say anything about how modules in such a system might interact."
"This paper describes a concrete realisation of the RAGS object and arrows model, OASYS, as applied to a simple but flexible NLG system called RICHES."
This is not the first such re-alisation:[REF_CITE]describes a par-tial re-implementation of the ‘Caption Generation System’[REF_CITE]which includes an objects and arrows ‘whiteboard’.
"The OASYS system includes more specific proposals for pro-cessing and inter-module communication, and RICHES demonstrates how this can be used to support a modular architecture based on small scale functionally-motivated units."
"OASYS (Objects and Arrows SYStem) is a soft-ware library which provides: an implementation of the RAGS Object and Arrows (O/A) data representation, support for representing the five-layer RAGS data model in O/A terms, an event-driven active database server for O/A representations."
"Together these components provide a central core for RAGS -style NLG applications, allowing sepa-rate parts of NLG functionality to be specified in independent modules, which communicate exclu-sively via the OASYS server."
The O/A data representation is a simple typed network representation language.
"An O/A database consists of a collection of objects, each of which has a unique identifier and a type, and arrows, each of which has a unique identifier, a type, and source and target objects."
Such a database can be viewed as a (possibly discon-nected) directed network representation: the fig-ures in section 5 give examples of such networks.
OASYS pre-defines object and arrow types re-quired to support the RAGS data model.
"Two ar-row types, el (element) and el(&lt;integer&gt;), are used to build up basic network structures – el identifies its target as a member of the set rep-resented by its source, el(3), identifies its tar-get as the third element of the tuple represented by its source."
"Arrow type realised by re-lates structures at different levels of representa-tion. for example, indicating that this SemRep object is realised by this SynRep object."
"Arrow type revised to provides for support for non-destructive modification of a structure, mapping from an object to another of the same type that can be viewed as a revision of it."
Arrow type refers to allows an object at one level to indi-rectly refer to an object at a different level.
"Object types correspond to the types of the RAGS data model, and are either atomic, tuples, sets or se-quences."
"For example, document structures are built out of DocRep (a 2-tuple),"
"DocAttr (a set of DocFeatAtoms – feature-value pairs), DocRe-pSeq (a sequence of DocReps or DocLeafs) and DocLeafs."
The active database server supports multiple independent O/A databases.
"Individual modules of an application publish and retrieve objects and arrows on databases, incrementally building the ‘higher level’, data structures."
Modules com-municate by accessing a shared database.
"Flow of control in the application is event-based: the OASYS module has the central thread of execu-tion, calls to OASYS generate ‘events’, and mod-ules are implemented as event handlers."
"A mod-ule registers interest in particular kinds of events, and when those events occur, the module’s hander is called to deal with them, which typically will involve inspecting the database and adding more structure (which generates further events)."
"OASYS supports three kinds of events: pub-lish events occur whenever an object or arrow is published in a database, module lifecycle events occur whenever a new module starts up or termi-nates, and synthetic events – arbitrary messages passed between the modules, but not interpreted by OASYS itself – may be generated by mod-ules at any time."
An application starts up by ini-tialising all its modules.
"This generates initialise events, which at least one module must respond to, generating further events which other modules may respond to, and so on, until no new events are generated, at which point OASYS generates finalise events for all the modules and terminates them."
This framework supports a wide range of archi-tectural possibilities.
Publish events can be used to make a module wake up whenever data of a particular sort becomes available for processing.
"Lifecycle events provide, among other things, an easy way to do pipelining: the second module in a pipeline waits for the finalise event of the first and then starts processing, the third waits similarly for the second to finalise etc."
"Synthetic events allow modules to tell each other more explicitly that some data is ready for processing, in situa-tion where simple publication of an object is not enough."
"RICHES includes examples of all three regimes: the first three modules are pipelined us-ing lifecycle events; LC and RE, FLO and REND interact using synthetic events; while SF watches the database specifically for publication events."
The RICHES system is a simple generation sys-tem that takes as input rhetorical plans and pro-duces patient advice texts.
The texts are intended to resemble those found at the PharmWeb site[URL_CITE]
"These are simple instructional texts telling patients how to use certain types of medicines, such as nosedrops, eye drops, suppositories etc.."
"An example text from PharmWeb is shown in figure 1, alongside the corresponding text produced by RICHES."
The main aim of RICHES is to demonstrate the feasibility of a system based on both the RAGS data model and the OASYS server model.
"The modules collectively construct and access the data representations in a shared blackboard space and this allows the modules to be defined in terms of their functional role, rather than say, the kind of data they manipulate or their position in a pro-cessing pipeline."
Each of the modules in the sys- tem is in itself very simple – our primary interest here is in the way they interact.
Figure 2 shows the structure of the system [Footnote_2] .
"2 The dashed lines indicate flow of information, solid ar-rows indicate approximately flow of control between mod-ules, double boxes indicate a completely reused module (from another system), while a double box with a dashed outer indicates a module partially reused. Ellipses indicate information sources, as opposed to processing modules."
The functionality of the individual modules is briefly described below.
Rhetorical Oracle (RO) The input to the sys-tem is a RhetRep of the document to be gen-erated: a tree with internal nodes labelled with (RST-style) rhetorical relations and RhetLeaves referring to semantic proposition representations (SemReps).
RO simply accesses such a represen-tation from a data file and initialises the OASYS database.
Media Selection (MS) RICHES produces doc-uments that may include pictures as well as text.
"As soon as the RhetRep becomes available, this module examines it and decides what can be il-lustrated and what picture should illustrate it."
"Pic- tures, annotated with their SemReps, are part of the picture library, and Media Selection builds small pieces of DocRep referencing the pictures."
"Document Planner (DP) The Document Plan-ner, based on the ICONOCLAST text planner[REF_CITE]takes the input RhetRep and pro-duces a document structure (DocRep)."
"This specifies aspects such as the text-level (e.g., paragraph, sentence) and the relative or-dering of propositions in the DocRep."
Its leaves refer to SynReps corresponding to syntac-tic phrases.
"This module is pipelined after MS, to make sure that it takes account of any pictures that have been included in the document."
Lexical Choice (LC) Lexical choice happens in two stages.
"In the first stage, LC chooses the lex-ical items for the predicate of each SynRep."
"This fixes the basic syntactic structure of the proposi-tion, and the valency mapping between semantic and syntactic arguments."
"At this point the ba-sic document structure is complete, and the LC advises REND and SF that they can start pro-cessing."
"LC then goes into a second phase, in- terleaved with RE and FLO: for each sentence, RE determines the referring expressions for each noun phrase, LC then lexicalises them, and when the sentence is complete FLO invokes LinGO to realise them."
Referring Expressions (RE) The Referring Expression module adapts the SynReps to add in-formation about the form of a noun phrase.
"It de-cides whether it should be a pronoun, a definite noun phrase or an indefinite noun phrase."
Sentence Finaliser (SF)
The Sentence Fi-naliser carries out high level sentential organisa-tion.
"LC and RE together build individual syntac-tic phrases, but do not combine them into whole sentences."
"SF uses rhetorical and document struc-ture information to decide how to complete the syntactic representations, for example, combin-ing main and subordinate clauses."
"In addition, SF decides whether a sentence should be imperative, depending on who the reader of the document is (an input parameter to the system)."
Finalise Lexical Output (FLO) RICHES uses an external sentence realiser component with its own non- RAGS input specification.
"FLO provides the interface to this realiser, extracting (mostly syntactic) information from OASYS and convert-ing it to the appropriate form for the realiser."
"Cur-rently, FLO supports the LinGO realiser[REF_CITE], but we are also looking at FLO mod-ules for RealPro[REF_CITE]and FUF/SURGE[REF_CITE]."
The Renderer is the module that puts the concrete document together.
"Guided by the document structure, it produces HTML for-matting for the text and positions and references the pictures."
"Individual sentences are produced for it by LinGO, via the FLO interface."
"FLO actu-ally processes sentences independently of REND, so when REND makes a request, either the sen-tence is there already, or the request is queued, and serviced when it becomes available."
"LinGO The LinGO realiser uses a wide-coverage grammar of English in the LKB HPSG framework,[REF_CITE]."
The tactical generation component accepts in-put in the Minimal Recursion Semantics formal-ism and produces the target text using a chart-driven algorithm with an optimised treatment of modificati[REF_CITE].
"No domain-specific tuning of the grammar was required for the RICHES system, only a few additions to the lexicon were necessary."
"In this section we show how RICHES generates the first sentence of the example text, Blow your nose so that it is clear and the picture that accom-panies the text."
The system starts with a rhetorical represen-tation (RhetRep) provided by the RO (see Fig-ure 3) [Footnote_3] .
"3 In the figures, labels indicate object types and the sub-script numbers are identifiers provided by OASYS for each"
"The first active module to run is MS which traverses the RhetRep looking at the se-mantic propositions labelling the RhetRep leaves, to see if any can be illustrated by pictures in the picture library."
Each picture in the library is en-coded with a semantic representation.
Matching between propositions and pictures is based on the algorithm presented[REF_CITE]which selects the most informative picture whose repre-sentation contains nothing that is not contained in the proposition.
"For each picture that will be in-cluded, a leaf node of document representation is created and a realised by arrow is added to it from the semantic proposition object (see Figure 4)."
The DP is an adaptation of the ICONOCLAST constraint-based planner and takes the RhetRep as its input.
"The DP maps the rhetorical repre-sentation into a document representation, decid- ing how the content will be split into sentences, paragraphs, item lists, etc., and what order the el-ements will appear in."
It also inserts markers that will be translated to cue phrases to express some rhetorical relations explicitly.
"Initially the plan-ner creates a skeleton document representation that is a one-to-one mapping of the rhetorical rep-resentation, but taking account of any nodes al-ready introduced by the MS module, and assigns finite-domain constraint variables to the features labelling each node."
"It then applies constraint sat-isfaction techniques to identify a consistent set of assignments to these variables, and publishes the resulting document structure for other modules to process."
"In our example, the planner decided that the whole document will be expressed as a paragraph (that in this case consists of a single text sen-tence) and that the document leaves will represent text-phrases."
"It also decides that these two text-phrases will be linked by a ‘subordinator’ marker (which will eventually be realised as “so that”), that “patient blows patient’s nose” will be realised before “patient’s nose is clear”."
"At this stage, the representation looks like Figure 5."
The first stage of LC starts after DP has finished and chooses the lexical items for the main pred-icates (in this case “blow” and “clear”).
"These are created as SynReps, linked to the leaves of the DocRep tree."
"In addition the initial SynReps for the syntactic arguments are created, and linked to the corresponding arguments of the semantic proposition (for example, syntactic SUBJECT is linked to semantic ACTOR )."
The database at this stage (showing only the representation pertinent to the first sentence) looks like Figure 6.
Until this point the flow of control has been a straight pipeline.
Referring Expression Genera-tion (RE) and the second stage of Lexical Choice (LC) operate in an interleaved fashion.
"RE col-lects the propositions in the order specified in the document representation and, for each of them, it inspects the semantic entities it contains (e.g., for our first sentence, those entities are ‘patient’ and ‘nose’) to decide whether they will be realised as a definite description or a pronoun."
"For our exam-ple, the final structure for the first argument in the first sentence can be seen in Figure 7 (although note that it will not be realised explicitly because the sentence is an imperative one)."
"SF waits for the syntactic structure of indvidual clauses to be complete, and then inspects the syn-tactic, rhetorical and document structure to decide how to combine clauses."
"In the example, it de-cides to represent the rhetorical ‘motivation’ rela-tion within a single text sentence by using the sub-ordinator ‘so that’."
"It also makes the main clause an imperative, and the subordinate clause indica-tive."
"As soon as SF completes a whole syntactic sentence, FLO notices, and extracts the informa-tion required to interface to LinGO with an MRS structure."
"The string of words returned by LinGO, is stored internally by FLO until REND requests it."
"Finally, REND draws together all the informa-tion from the document and syntactic structures, and the realiser outputs provided by FLO, and produces HTML."
The entire resultant text can be seen on the right hand side of figure 1.  (&apos;  )
"In this paper, we have described a small NLG sys-tem implemented using an event-driven, object-and-arrow based processing architecture."
"The system makes use of the data representation ideas proposed in the RAGS project, but adds a con-crete proposal relating to application organisation and process control."
"Our main aims were to de-velop this ‘process model’ as a complement to the RAGS ‘data model,’ show that it could be im-plemented and used effectively, and test whether the RAGS ideas about data organisation and devel-opment can actually be deployed in such a sys-tem."
"Although the RICHES generator is quite simple, it demonstrates that it is possible to con-struct a RAGS -style generation system using these ideas, and that the OASYS processing model has the flexibility to support the kind of modularised NLG architecture that the RAGS initiative presup-poses."
Some of the complexity in the RICHES sys-tem is there to demonstrate the potential for dif-ferent types of control strategies.
"Specifically, we do not make use of the possibilities offered by the interleaving of the RE and LC, as the examples we cover are too simple."
"However, this setup en-ables RE, in principle, to make use of information about precisely how a previous reference to an en-tity has been realised."
"Thus, if the first mention of an entity is as “the man”, RE may decide that a pronoun, “he” is acceptable in a subsequent refer-ence."
"If, however, the first reference was realised as “the person”, it may decide to say “the man” next time around."
At the beginning of this paper we men-tioned systems that do not implement a standard pipeline.
"The RICHES system demonstrates that the RAGS model is sufficiently flexible to permit modules to work concurrently (as the REND and LC do in RICHES), alternately, passing control backwards and forwards (as the RE and LC mod-ules do in RICHES) or pipelined (as the Docu-ment Planner and LC do in RICHES)."
The different types of events allow for a wide range of possible control models.
"In the case of a simple pipeline, each module only needs to know that its predecessor has finished."
"Depending on the precise nature of the work each module is doing, this may be best achievable through pub-lish events (e.g. when a DocRep has been pub-lished, the DP may be deemed to have finished its work) or through lifecycle events (e.g. the DP effectively states that it has finished)."
"A revision based architecture might require synthetic events to “wake up” a module to do some more work, after it has finished its first pass."
This paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users.
"Previous research has noted the importance of hand gestures, eye gaze and head nods in conversations between embodied agents and humans."
"We present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues, and discourse and conversation state in dialogues."
"On the basis of these findings, we have implemented an embodied conversational agent that uses Collagen in such a way as to generate postural shifts."
"This paper provides empirical support for the relationship between posture shifts and discourse structure, and then derives an algorithm for generating posture shifts in an animated embodied conversational agent from discourse states produced by the middleware architecture known as Collagen [18]."
Other nonverbal behaviors have been shown to be correlated with the underlying conversational structure and information structure of discourse.
"For example, gaze shifts towards the listener correlate with a shift in conversational turn (from the conversational participants’ perspective, they can be seen as a signal that the floor is available)."
"Gestures correlate with rhematic content in accompanying language (from the conversational participants’ perspective, these behaviors can be seen as a signal that accompanying speech is of high interest)."
"A better understanding of the role of nonverbal behaviors in conveying discourse structures enables improvements in the naturalness of embodied dialogue systems, such as embodied conversational agents, as well as contributing to algorithms for recognizing discourse structure in speech-understanding systems."
"Previous work, however, has not addressed major body shifts during discourse, nor has it addressed the nonverbal correlates of topic shifts."
Only recently have computational linguists begun to examine the association of nonverbal behaviors and language.
In this section we review research by non-computational linguists and discuss how this research has been employed to formulate algorithms for natural language generation or understanding.
"About three-quarters of all clauses in descriptive discourse are accompanied by gestures [17], and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech [13]."
"It has been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on gestural cues [22] (and, the higher the noise-to-signal ratio, the more facilitation by gesture)."
"Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech."
"In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]."
"On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures."
"For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to."
Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects.
"André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating."
"Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity."
Head and eye movement has also been examined in the context of discourse and conversation.
Looking away from one’s interlocutor has been correlated with the beginning of turns.
"From the speaker’s point of view, this look away may prevent an overload of visual and linguistic information."
"On the other hand, during the execution phase of an utterance, speakers look more often at listeners."
Head nods and eyebrow raises are correlated with emphasized linguistic items – such as words accompanied by pitch accents [7].
"Some eye movements occur primarily at the ends of utterances and at grammatical boundaries, and appear to function as synchronization signals."
"That is, one may request a response from a listener by looking at the listener, and suppress the listener’s response by looking away."
"Likewise, in order to offer the floor, a speaker may gaze at the listener at the end of the utterance."
"When the listener wants the floor, s/he may look at and slightly up at the speaker [10]."
It should be noted that turn taking only partially accounts for eye gaze behavior in discourse.
A better explanation for gaze behavior integrates turn taking with the information structure of the propositional content of an utterance [5].
"Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer."
"When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior."
Results such as these have led researchers to generate eye gaze and head movements in animated embodied conversational agents.
"Takeuchi and Nagao, for example, [21] generate gaze and head nod behaviors in a “talking head.”"
"Cassell et al. [2] generate eye gaze and head nods as a function of turn taking behavior, head turns just before an utterance, and eyebrow raises as a function of emphasis."
"To our knowledge, research on posture shifts and other gross body movements, has not been used in the design or implementation of computational systems."
"In fact, although a number of conversational analysts and ethnomethodologists have described posture shifts in conversation, their studies have been qualitative in nature, and difficult to reformulate as the basis of algorithms for the generation of language and posture."
"Nevertheless, researchers in the non-computational fields have discussed posture shifts extensively."
"Kendon [13] reports a hierarchy in the organization of movement such that the smaller limbs such as the fingers and hands engage in more frequent movements, while the trunk and lower limbs change relatively rarely."
A number of researchers have noted that changes in physical distance during interaction seem to accompany changes in the topic or in the social relationship between speakers.
For example Condon and Osgton [9] have suggested that in a speaking individual the changes in these more slowly changing body parts occur at the boundaries of the larger units in the flow of speech.
Blom &amp;[REF_CITE]identify posture changes and changes in the spatial relationship between two speakers as indicators of what they term &quot;situational shifts&quot; -- momentary changes in the mutual rights and obligations between speakers accompanied by shifts in language style.
"In his analysis of college counseling interviews, they occurred more frequently than any other coded indicator of segment changes, and were therefore the best predictor of new segments in the data."
"Unfortunately, in none of these studies are statistics provided, and their analyses rely on intuitive definitions of discourse segment or “major shift”."
"For this reason, we carried out our own empirical study."
Videotaped “pseudo-monologues” and dialogues were used as the basis for the current study.
"In “pseudo-monologues,” subjects were asked to describe each of the rooms in their home, then give directions between four pairs of locations they knew well (e.g., home and the grocery store)."
"The experimenter acted as a listener, only providing backchannel feedback (head nods, smiles and paraverbals such as &quot;uh-huh&quot;)."
"For dialogues, two subjects were asked to generate an idea for a class project that they would both like to work on, including: 1) what they would work on; 2) where they would work on it (including facilities, etc.), and 3) when they would work on it."
Subjects stood in both conditions and were told to perform their tasks in 5-10 minutes.
"The pseudo-monologue condition (pseudo- because there was in fact an interlocutor, although he gave backchannel feedback only and never took the turn) allowed us to investigate the relationship between discourse structure and posture shift independent of turn structure."
The two tasks were constructed to allow us to identify exactly where discourse segment boundaries would be placed.
"The video data was transcribed and coded for three features: discourse segment boundaries, turn boundaries, and posture shifts."
"A discourse segment is taken to be an aggregation of utterances and sub-segments that convey the discourse segment purpose, which is an intention that leads to the segment initiation [12]."
In this study we chose initially to look at high-level discourse segmentation phenomena rather than those discourse segments embedded deeper in the discourse.
"Thus, the time points at which the assigned task topics were started served as segmentation points."
"Turn boundaries were coded (for dialogues only) as the point in time in which the start or end of an utterance co-occurred with a change in speaker, but excluding backchannel feedback."
Turn overlaps were coded as open-floor time.
"We defined a posture shift as a motion or a position shift for a part of the human body, excluding hands and eyes (which we have dealt with in other work)."
"Posture shifts were coded with start and end time of occurrence (duration), body part in play (for this paper we divided the body at the waistline and compared upper body vs. lower body shifts), and an estimated energy level of the posture shift."
Energy level was normalized for each subject by taking the largest posture shift observed for each subject as 100% and coding all other posture shift energies relative to the 100% case.
"Posture shifts that occurred as part of gesture or were clearly intentionally generated (e.g., turning one&apos;s body while giving directions) were not coded."
"Data from seven monologues and five dialogues were transcribed, and then coded and analyzed independently by two raters."
A total of 70.5 minutes of data was analyzed (42.5 minutes of dialogue and 29.2 minutes of monologue).
"A total of 67 discourse segments were identified (25 in the dialogues and 42 in the monologues), which constituted 407 turns in the dialogue data."
We used the instructions given to subjects concerning the topics to discuss as segmentation boundaries.
"In future research, we will address the smaller discourse segmentation."
"For posture shift coding, raters coded all posture shifts independently, and then calculated reliability on the transcripts of one monologue (5.2 minutes) and both speakers from one dialogue (8.5 minutes)."
Agreement on the presence of an upper body or lower body posture shift in a particular location (taking location to be a 1-second window that contains all of or a part of a posture shift) for these three speakers was 89% (kappa = .64).
"For interrater reliability of the coding of energy level, a Spearman’s rho revealed a correlation coefficient of .48 (p&lt;.01)."
Posture shifts occurred regularly throughout the data (an average of 15 per speaker in both pseudo-monologues and dialogues).
"This, together with the fact that the majority of time was spent within discourse segments and within turns (rather than between segments), led us to normalize our posture shift data for comparison purposes."
"For relatively brief intervals (inter-discourse-segment and inter-turn) normalization by number of inter-segment occurrences was sufficient (ps/int), however, for long intervals (intra-discourse segment and intra-turn) we needed to normalize by time to obtain meaningful comparisons."
For this normalization metric we looked at posture-shifts-per-second (ps/s).
"This gave us a mean average of .06 posture shifts/second (ps/s) in the monologues (SD=.07), and .07 posture shifts/second in the dialogues (SD=.08)."
Our initial analysis compared posture shifts made by the current speaker within discourse segments (intra-dseg) to those produced at the boundaries of discourse segments (inter-dseg).
It can be seen (in Table 4.1.1) that posture shifts occur an order of magnitude more frequently at discourse segment boundaries than within discourse segments in both monologues and dialogues.
"Posture shifts also tend to be more energetic at discourse segment boundaries (F(1,251)=10.4; p&lt;0.001)."
"Initially, we classified data as being inter- or intra-turn."
Table 4.1.2 shows that turn structure does have an influence on posture shifts; subjects were five times more likely to exhibit a shift at a boundary than within a turn. ps/s ps/int inter-dseg/start-turn 0.562 0.542 inter-dseg/mid-turn 0.000 0.000 inter-dseg/end-turn 0.130 0.125 intra-dseg/start-turn 0.067 0.135 intra-dseg/mid-turn 0.041 intra-dseg/end-turn 0.053 0.107
An interaction exists between turns and discourse segments such that discourse segment boundaries are ten times more likely to co-occur with turn changes than within turns.
"Both turn and discourse structure exhibit an influence on posture shifts, with discourse having the most predictive value."
Starting a turn while starting a new discourse segment is marked with a posture shift roughly 10 times more often than when starting a turn while staying within discourse segment.
"We noticed, however, that posture shifts appeared to congregate at the beginnings or ends of turn boundaries, and so our subsequent analyses examined start-turns, mid-turns and end-turns."
"It is clear from these results that posture is indeed correlated with discourse state, such that speakers generate a posture shift when initiating a new discourse segment, which is often at the boundary between turns."
In addition to looking at the occurrence and energy of posture shifts we also analyzed the distributions of upper vs. lower body shifts and the duration of posture shifts.
"Speaker upper body shifts were found to be used more frequently at the start of turns (48%) than at the middle of turns (36%) or end of turns (18%) (F(2,147)=5.39; p&lt;0.005), with no significant dependence on discourse structure."
"Finally, speaker posture shift duration was found to change significantly as a function of both turn and discourse structure (see Figure 4.1.3)."
"At the start of turns, posture shift duration is approximately the same whether a new topic is introduced or not (2.5 seconds)."
"However, when ending a turn, speakers move significantly longer (7.0 seconds) when finishing a topic than when the topic is continued by the other interlocutor (2.7 seconds) (F(1,148)=17.9; p&lt;0.001)."
"In the following sections we discuss how the results of the empirical study were integrated along with Collagen into our existent embodied conversational agent, Rea."
Rea is an embodied conversational agent that interacts with a user in the real estate agent domain [2].
The system architecture of Rea is shown in Figure 5.1.
Rea takes input from a microphone and two cameras in order to sense the user’s speech and gesture.
The UM interprets and integrates this multimodal input and outputs a unified semantic representation.
The Understanding Module then sends the output to Collagen as the Dialogue Manager.
"Collagen, as further discussed below, maintains the state of the dialogue as shared between Rea and a user."
The Reaction Module decides Rea’s next action based on the discourse state maintained by Collagen.
It also assigns information structure to output utterances so that gestures can be appropriately generated.
"The semantic representation of the action, including verbal and non-verbal behaviors, is sent to the Generation Module which generates surface linguistic expressions and gestures, including a set of instructions to achieve synchronization between animation and speech."
These instructions are executed by a 3D animation renderer and a text-to-speech system.
Table 5.1 shows the associations between discourse and conversational state that Rea is currently able to handle.
In other work we have discussed how Rea deals with the association between information structure and gesture [6].
"In the following sections, we focus on Rea’s generation of posture shifts. structure information beat_and other_hand_gsts"
Collagen TM is JAVA middleware for building COLLAborative interface AGENts to work with users on interface applications.
"Collagen is designed with the capability to participate in collaboration and conversation, based on [12], [16]."
Collagen updates the focus stack and recipe tree using a combination of the discourse interpretation algorithm of [16] and plan recognition algorithms of [14].
"It takes as input user and system utterances and interface actions, and accesses a library of recipes describing actions in the domain."
"After updating the discourse state, Collagen makes three resources available to the interface agent: focus of attention (using the focus stack), segmented interaction history (of completed segments) and an agenda of next possible actions created from the focus stack and recipe tree."
"The Reaction Module works as a content planner in the Rea architecture, and also plays the role of an interface agent in Collagen."
It has access to the discourse state and the agenda using APIs provided by Collagen.
"Based on the results reported above, we describe here how Rea plans her next nonverbal actions using the resources that Collagen maintains."
"The empirical study revealed that posture shifts are distributed with respect to discourse segment and turn boundaries, and that the form of a posture shift differs according to these co-determinants."
"Therefore, generation of posture shifts in Rea is determined according to these two factors, with Collagen contributing information about current discourse state."
Any posture shift that occurs between the end of one discourse segment and the beginning of the next is defined as an inter-discourse segment posture shift.
"In order to elaborate different generation rules for inter- vs. intra-discourse segments, Rea judges (D1) whether the next utterance starts a new topic, or contributes to the current discourse purpose, (D2) whether the next utterance is expected to finish a segment."
"First, (D1) is calculated by referring to the focus stack and agenda."
"In planning a next action, Rea accesses the goal agenda in Collagen and gets the content of her next utterance."
She also accesses the focus stack and gets the current discourse purpose that is shared between her and the user.
"By comparing the current purpose and the purpose of her next utterance, Rea can judge whether the her next utterance contributes to the current discourse purpose or not."
"For example, if the current discourse purpose is to find a house to show the user (FindHouse), and the next utterance that Rea plans to say is as follows, (1) (Ask."
What (agent Propose.
What (user FindHouse &lt;city ?&gt;))) then Rea uses Collagen APIs to compare the current discourse purpose (FindHouse) to the purpose of utterance (1).
The purpose of this utterance is to ask the value of the transportation parameter of FindHouse.
"Thus, Rea judges that this utterance contributes to the current discourse purpose, and continues the same discourse segment (D1 = continue)."
"On the other hand, if Rea’s next utterance is about showing a house, (2) (Propose."
Should (agent ShowHouse (joint 123ElmStreet))
"Rea says: &quot;Let&apos;s look at 123 Elm Street.&quot; then this utterance does not directly contribute to the current discourse purpose because it does not ask a parameter of FindHouse, and it introduces a new discourse purpose ShowHouse."
"In this case, Rea judges that there is a discourse segment boundary between the previous utterance and the next one (D1 = topic change)."
"In order to calculate (D2), Rea looks at the plan tree in Collagen, and judges whether the next utterance addresses the last goal in the current discourse purpose."
"If it is the case, Rea expects to finish the current discourse segment by the next utterance (D1 = finish topic)."
"As for conversational structure, Rea needs to know; (T1) whether Rea is taking a new turn with the next utterance, or keeping her current turn for the next utterance, (T2) whether Rea’s next utterance requires that the user respond."
"First, (T1) is judged by referring to the dialogue history [Footnote_1] ."
1 We currently maintain a dialogue history in Rea even though Collagen has one as well. This is in order to store and manipulate the information to generate hand gestures and assign intonational accents. This information will be integrated into Collagen in the near future.
The dialogue history stores both system utterances and user utterances that occurred in the dialogue.
"In the history, each utterance is stored as a logical form based on an artificial discourse language [20]."
"As shown above in utterance (1), the first argument of the action indicates the speaker of the utterance; in this example, it is “agent”."
The turn boundary can be estimated by comparing the speaker of the previous utterance with the speaker of the next utterance.
"If the speaker of the previous utterance is not Rea, there is a turn boundary before the next utterance (T1 = take turn)."
"If the speaker of the previous utterance is Rea, that means that Rea will keep the same turn for the next utterance (T1 = keep turn)."
"Second, (T2) is judged by looking at the type of Rea’s next utterance."
"For example, when Rea asks a question, as in utterance (1), Rea expects the user to answer the question."
"In this case, Rea must convey to the user that the system gives up the turn (T2 = give up turn)."
"Combining information about discourse structure (D1, D2) and conversation structure (T1, T2), the system decides on posture shifts for the beginning of the utterance and the end of the utterance."
Rea decides to do or not to do a posture shift by calling a probabilistic function that looks up the probabilities in Table 5.3.1.
A posture shift for the beginning of the utterance is decided based on the combination of (D1) and (T1).
"For example, if the combined factors match Case (a), the system decides to generate a posture shift with 54% probability for the beginning of the utterance."
"Note that in Case (d), that is, Rea keeps the turn without changing a topic, we cannot calculate a per interval posture shift rate."
"Instead, we use a posture shift rate normalized for time."
"This rate is used in the GenerationModule, which calculates the utterance duration and generates a posture shift during the utterance based on this posture shift rate."
"On the other hand, ending posture shifts are decided based on the combination of (D2) and (T2)."
"For example, if the combined factors match Case (e), the system decides to generate a posture shift with 0.04% probability for the ending of the utterance."
"When Rea does decide to activate a posture shift, she then needs to choose which posture shift to perform."
Our empirical data indicates that the energy level of the posture shift differs depending on whether there is a discourse segment boundary or not.
"Moreover the duration of a posture shift differs depending on the place in a turn: start-, mid-, or end-turn."
"Based on these results, we define posture shift selection rules for energy, duration, and body part."
The correspondence with discourse information is shown in Table 5.3.1.
"For example, in Case (a), the system selects a posture shift with high energy, using both upper and lower body."
"After deciding whether or not Rea should shift posture and (if so) choosing a kind of posture shift, Rea sends a command to the Generation Module to generate a specific kind of posture shift within a specific time duration."
D1 topic h continue 0.04/sec low
"Posture shifts for pseudo-monologues can be decided using the same mechanism as that for dialogue, but omitting conversation structure information."
The probabilities are given in table Table 5.3.2.
"For example, if Rea changes the topic with her next utterance, a posture shift is generated 84% of the time with high-energy motion."
"In other cases, the system randomly generates low-energy posture shifts 0.04 times per second."
"Figure 6.1 shows a dialogue between Rea and the user, and shows how Rea decides to generate posture shifts."
"This dialogue consists of two major segments: finding a house (dialogue), and showing a house (pseudo-monologue)."
"Based on this task structure, we defined plan recipes for Collagen."
The first shared discourse purpose [goal: HaveConversation] is introduced by the user before the example.
"Then, in utterance (1), the user introduces the main part of the conversation [goal: FindHouse]."
"The next goal in the agenda, [goal: IdentifyPreferredCity], should be accomplished to identify a parameter value for [goal: FindHouse]."
"This goal directly contributes to the current purpose, [goal: FindHouse]."
"This case is judged to be a turn boundary within a discourse segment (Case (c)), and Rea decides to generate a posture shift at the beginning of the utterance with 13% probability."
If Rea decides to shift posture she selects a low energy posture shift using either upper or lower body.
"In addition to a posture shift at the beginning of the utterance, Rea may also choose to generate a posture shift to end the turn."
"As utterance (2) expects the user to take the turn, and continue to work on the same discourse purpose, this is Case (f)."
"Thus, the system generates an end utterance posture shift 11% of the time."
"If generated, a low energy posture shift is chosen."
"If a beginning and/or ending posture shifts are generated, they are sent to the GM, which calculates the schedule of these multimodal events and generates them."
"Rea, using a default rule, decides to take the initiative on this goal."
"At this point, Rea accesses the discourse state and confirms that a new goal is about to start."
Rea judges this case as a discourse segment boundary and also a turn boundary (Case (a)).
"Based on this information, Rea selects a high energy posture shift."
An example of Rea’s high energy posture shift is shown on the right in Figure 5.2.
"As a subdialogue of showing a house, in a discourse purpose [goal : DiscussFeature], Rea keeps the turn and continues to describe the house."
We handle this type of interaction as a pseudo-monologue.
"Therefore, we can use table Table 5.3.2 for deciding on posture shifts here."
"This is judged as Case (g), and a high energy body motion is generated 84% of the time. [Finding a house] &lt; dialogue&gt; (1) U:"
I’m looking for a house. (2) R: (c) Where do you want to live? (f) (3) U: I like Boston. (4) R: (c) (d) What kind of transportation access do you need? (f) (5) U: I need T access. …. (23) R: (c) (d) How much storage space do you need? (f) (24) U: I need to have a storage place in the basement.
"We have demonstrated a clear relationship between nonverbal behavior and discourse state, and shown how this finding can be incorporated into the generation of language and nonverbal behaviors for an embodied conversational agent."
"Speakers produce posture shifts at 53% of discourse segment boundaries, more frequently than they produce those shifts discourse segment-internally, and with more motion energy."
"Furthermore, there is a relationship between discourse structure and conversational structure such that when speakers initiate a new segment at the same time as starting a turn (the most frequent case by far), they are more likely to produce a posture shift; while when they end a discourse segment and a turn at the same time, their posture shifts last longer than when these categories do not co-occur."
"Although this paper reports results from a limited number of monologues and dialogues, the findings are promising."
"In addition, they point the way to a number of future directions, both within the study of posture and discourse, and more generally within the study of non-verbal behaviors in computational linguistics."
"First, given the relationship between conversational and information structure in [5], a natural next step is to examine the three-way relationship between discourse state, conversational structure (turns), and information structure (theme/rheme)."
"For the moment, we have demonstrated that posture shifts may signal boundaries of units; do they also signal the information content of units?"
"Next, we need to look at finer segmentations of the discourse, to see whether larger and smaller discourse segments are distinguished through non-verbal means."
"Third, the question of listener posture is an important one."
We found that a number of posture shifts were produced by the participant who was not speaking.
"More than half of these shifts were produced at the same time as a speaker shift, suggesting a kind of mirroring."
"In order to interpret these data, however, a more sensitive notion of turn structure is required, as one must be ready to define when exactly speakers and listeners shift roles."
"Also, of course, evaluation of the importance of such nonverbal behaviors to user interaction is essential."
"In a user study of our earlier Gandalf system [4], users rated the agent&apos;s language skills significantly higher under test conditions in which Gandalf deployed conversational behaviors (gaze, head movement and limited gesture) than when these behaviors were disabled."
Such an evaluation is also necessary for the Rea-posture system.
"But, more generally, we need to test whether generating posture shifts of this sort actually serves as a signal to listeners, for example to initiative structure in task and dialogue [8]."
These evaluations form part of our future research plans.
"This research was supported by MERL, France Telecom, AT&amp;T, and the other generous sponsors of the MIT Media Lab."
"Thanks to the other members of the Gesture and Narrative Language Group, in particular Ian Gouldstone and Hannes Vilhjálmsson."
"We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology."
The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model.
For the better of our two models these improvements are 24% and 14% respectively.
We also suggest that improvement of the un-derlying parser should significantly im-prove the model’s perplexity and that even in the near term there is a lot of po-tential for improvement in immediate-head language models.
"All of the most accurate statistical parsers [1,3, 6,7,12,14] are lexicalized in that they condition probabilities on the lexical content of the sen-tences being parsed."
"Furthermore, all of these parsers are what we will call immediate-head parsers in that all of the properties of the imme-diate descendants of a constituent c are assigned probabilities that are conditioned on the lexical head of c. For example, in Figure 1 the probability that the vp expands into v np pp is conditioned on the head of the vp, “put”, as are the choices of the sub-heads under the vp, i.e., “ball” (the head of the np) and “in” (the head of the pp)."
It is the ex-perience of the statistical parsing community that immediate-head parsers are the most accurate we can design.
"It is also worthy of note that many of these parsers [1,3,6,7] are generative — that is, for a sentence s they try to find the parse defined by Equation 1: arg max p( j s) = argmax p( ,s) (1)"
"This is interesting because insofar as they com-pute p( , s) these parsers define a language-model in that they can (in principle) assign a probability to all possible sentences in the language by com- X puting the sum in Equation 2: p(s) = p( ,s) (2) where p( ,s) is zero if the yield of 6 = s. Lan-guage models, of course, are of interest because speech-recognition systems require them."
These systems determine the words that were spoken by solving Equation 3: arg max s p(s j A) = arg max s p(s)p(A j s) (3) where A denotes the acoustic signal.
"The first term on the right, p(s), is the language model, and is what we compute via parsing in Equation 2."
Virtually all current speech recognition sys-tems use the so-called trigram language model in which the probability of a string is broken down into conditional probabilities on each word given
Y the two previous words.
"E.g., p(w 0,n ) = p(w i j w i 1 ,w i 2 ) (4) i=0,n 1"
"On the other hand, in the last few years there has been interest in designing language models based upon parsing and Equation 2."
We now turn to this previous research.
"There is, of course, a very large body of litera-ture on language modeling (for an overview, see [10]) and even the literature on grammatical lan-guage models is becoming moderately large [4, 9,15,16,17]."
"The research presented in this pa-per is most closely related to two previous efforts, that by Chelba and Jelinek [4] (C&amp;J) and that by Roark [15], and this review concentrates on these two papers."
"While these two works differ in many particulars, we stress here the ways in which they are similar, and similar in ways that differ from the approach taken in this paper."
In both cases the grammar based language model computes the probability of the next word based upon the previous words of the sentence.
"More specifically, these grammar-based models compute a subset of all possible grammatical re-lations for the prior words, and then compute the probability of the next grammatical situ-ation, and the probability of seeing the next word given each of these grammatical situations."
"Also, when computing the probability of the next word, both models condition on the two prior heads of constituents."
"Thus, like a trigram model, they use information about triples of words."
Neither of these models uses an immediate-head parser.
Rather they are both what we will call strict left-to-right parsers.
At each sentence position in strict left-to-right parsing one com-putes the probability of the next word given the previous words (and does not go back to mod-ify such probabilities).
This is not possible in immediate-head parsing.
"Sometimes the imme-diate head of a constituent occurs after it (e.g, in noun-phrases, where the head is typically the rightmost noun) and thus is not available for con-ditioning by a strict left-to-right parser."
"There are two reasons why one might prefer strict left-to-right parsing for a language model (Roark [15] and Chelba, personal communica-tion)."
"First, the search procedures for guessing the words that correspond to the acoustic signal works left to right in the string."
If the language model is to offer guidance to the search procedure it must do so as well.
The second benefit of strict left-to-right parsing is that it is easily combined with the standard tri-gram model.
In both cases at every point in the sentence we compute the probability of the next word given the prior words.
Thus one can inter-polate the trigram and grammar probability esti-mates for each word to get a more robust estimate.
"It turns out that this is a good thing to do, as is clear from Table 1, which gives perplexity results for a trigram model of the data in column one, re-sults for the grammar-model in column two, and results for a model in which the two are interpo- lated in column three."
"Both the were trained and tested on the same training and testing corpora, to be described in Section 4.1."
"As indicated in the table, the trigram model achieved a perplexity of 167 for the test-ing corpus."
"The grammar models did slightly bet-ter (e.g., 158.28 for the Chelba and Jelinek (C&amp;J) parser), but it is the interpolation of the two that is clearly the winner (e.g., 137.26 for the Roark parser/trigram combination)."
In both papers the interpolation constants were 0.36 for the trigram estimate and 0.64 for the grammar estimate.
"While both of these reasons for strict-left-to-right parsing (search and trigram interpolation) are valid, they are not necessarily compelling."
The ability to combine easily with trigram models is important only as long as trigram models can improve grammar models.
A sufficiently good grammar model would obviate the need for tri-grams.
"As for the search problem, we briefly re-turn to this point at the end of the paper."
"Here we simply note that while search requires that a language model provide probabilities in a left to right fashion, one can easily imagine proce-dures where these probabilities are revised after new information is found (i.e., the head of the constituent)."
Note that already our search pro-cedure needs to revise previous most-likely-word hypotheses when the original guess makes the subsequent words very unlikely.
"Revising the associated language-model probabilities compli-cates the search procedure, but not unimaginably so."
Thus it seems to us that it is worth finding out whether the superior parsing performance of immediate-head parsers translates into improved language models.
We have taken the immediate-head parser de-scribed in [3] as our starting point.
"This parsing model assigns a probability to a parse by a top-down process of considering each constituent c in and, for each c, first guessing the pre-terminal of c, t(c) (t for “tag”), then the lexical head of c, h(c), and then the expansion of c into further con-stituents e(c)."
Thus the probability of a parse is
"Y given by the equation p( ) = p(t(c) j l(c),H(c)) c 2 p(h(c) j t(c), l(c), H(c)) p(e(c) j l(c), t(c), h(c), H(c)) where l(c) is the label of c (e.g., whether it is a noun phrase (np), verb phrase, etc.) and H(c) is the relevant history of c — information outside c that our probability model deems important in de-termining the probability in question."
"In [3] H(c) approximately consists of the label, head, and head-part-of-speech for the parent of c: m(c), i(c), and u(c) respectively."
"One exception is the distri-bution p(e(c) j l(c), t(c), h(c), H(c)), where H only includes m and u. [Footnote_1]"
1 We simplify slightly in this section. See [3] for all the details on the equations as well as the smoothing used.
"Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c)."
In this notation the above equation takes the following
"Y form: p( ) = p(t j l, m, u, i) p(h j t, l, m, u, i) c 2 p(e j l, t, h, m, u). (5)"
"Because this is a point of contrast with the parsers described in the previous section, note that all of the conditional distributions are conditioned on one lexical item (either i or h)."
"Thus only p(h j t, l, m, u, i), the distribution for the head of c, looks at two lexical items (i and h itself), and none of the distributions look at three lexical items as do the trigram distribution of Equation 4 and the previously discussed parsing language models [4, 15]."
Next we describe how we assign a probabil-ity to the expansion e of a constituent.
"We break up a traditional probabilistic context-free gram-mar (PCFG) rule into a left-hand side with a label l(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols."
For each expansion we distinguish one of the right-hand side labels as the “middle” or “head” symbol M(c).
M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.
"To the left of M is a sequence of one or more left labels L i (c) including the special ter-mination symbol 4 , which indicates that there are no more symbols to the left, and similarly for the labels to the right, R i (c)."
Thus an expansion e(c) looks like: l ! 4 L m . . .
L 1 MR 1 . . .
R n 4 . (6)
"The expansion is generated by guessing first M, then in order L 1 through L m+1 (= 4 ), and similarly for R 1 through R n+1 ."
"In anticipation of our discussion in Section 4.2, note that when we are expanding an L i we do not know the lexical items to its left, but if we prop-erly dovetail our “guesses” we can be sure of what word, if any, appears to its right and before M, and similarly for the word to the left of R j ."
This makes such words available to be conditioned upon.
"Finally, the parser of [3] deviates in two places from the strict dictates of a language model."
"First, as explicitly noted in [3], the parser does not com-pute the partition function (normalization con-stant) for its distributions so the numbers it re-turns are not true probabilities."
"We noted there that if we replaced the “max-ent inspired” fea-ture with standard deleted interpolation smooth-ing, we took a significant hit in performance."
"We have now found several ways to overcome this problem, including some very efficient ways to compute partition functions for this class of mod-els."
"In the end, however, this was not neces-sary, as we found that we could obtain equally good performance by “hand-crafting” our inter-polation smoothing rather than using the “obvi-ous” method (which performs poorly)."
"Secondly, as noted in [2], the parser encourages right branching with a “bonus” multiplicative fac-tor of 1.2 for constituents that end at the right boundary of the sentence, and a penalty of 0.8 for those that do not."
This is replaced by explic-itly conditioning the events in the expansion of Equation 6 on whether or not the constituent is at the right boundary (barring sentence-final punctu- ation).
"Again, with proper attention to details, this can be known at the time the expansion is taking place."
"This modification is much more complex than the multiplicative “hack,” and it is not quite as good (we lose about 0.1% in precision/recall figures), but it does allow us to compute true prob-abilities."
The resulting parser strictly speaking defines a PCFG in that all of the extra conditioning in-formation could be included in the non-terminal-node labels (as we did with the head information in Figure 1).
"When a PCFG probability distribu-tion is estimated from training data (in our case the Penn tree-bank) PCFGs define a tight (sum-ming to one) probability distribution over strings [5], thus making them appropriate for language models."
"We also empirically checked that our in-dividual distributions (p(t j l,m,u,i), and p(h j t, l, m, u, i) from Equation 5 and p(L j l, t, h, m, u), p(M j l, t, h, m, u), and p(R j l, t, h, m, u) from Equation 5) sum to one for a large, random, se-lection of conditioning events [Footnote_2]"
2 They should sum to one. We are just checking that there are no bugs in the code.
"As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established thresh-old) have their probabilities recomputed accord-ing to the complete probability model of Equation 5."
Both searches are conducted using dynamic programming.
"The parser as described in the previous section was trained and tested on the data used in the pre-viously described grammar-based language mod-eling research [4,15]."
"This data is from the Penn Wall Street Journal tree-bank [13], but modified to make the text more “speech-like”."
"In particu-lar: 1. all punctuation is removed, 2. no capitalization is used, 3. all symbols and digits are replaced by the symbol N, and 4. all words except for the 10,000 most com-mon are replaced by the symbol UNK."
"As in previous work, files F0 to F20 are used for training,[REF_CITE]for development, and[REF_CITE]for testing."
The results are given in Table 2.
We refer to the current model as the bihead model. “Bihead” here emphasizes the already noted fact that in this model probabilities involve at most two lexical heads.
"As seen in Table 2, the immediate-bihead model with a perplexity of 144.98 outperforms both previous models, even though they use tri-grams of words in their probability estimates."
"We also interpolated our parsing model with the trigram model (interpolation constant .36, as with the other models) and this model outper-forms the other interpolation models."
"Note, how-ever, that because our parser does not define prob-abilities for each word based upon previous words (as with trigram) it is not possible to do the inte-gration at the word level."
Rather we interpolate the probabilities of the entire sentences.
"This is a much less powerful technique than the word-level interpolation used by both C&amp;J and Roark, but we still observe a significant gain in performance."
"While the performance of the grammatical model is good, a look at sentences for which the tri-gram model outperforms it makes its limitations apparent."
The sentences in question have noun phrases like “monday night football” that trigram models eats up but on which our bihead parsing model performs less well.
"For example, consider the sentence “he watched monday night football”."
"The trigram model assigns this a probability of 1.9 10 5 , while the grammar model gives it a probability of 2.77 10 7 ."
"To a first approxima-tion, this is entirely due to the difference in prob- ability of the noun-phrase."
"For example, the tri-gram probability p(football j monday,night) = 0.366, and would have been 1.0 except that smoothing saved some of the probability for other things it might have seen but did not."
"Because the grammar model conditions in a different order, the closest equivalent probability would be that for “monday”, but in our model this is only con-ditioned on “football” so the probability is much less biased, only 0.0306. (Penn tree-bank base noun-phrases are flat, thus the head above “mon-day” is “football”.)"
This immediately suggests creating a second model that captures some of the trigram-like probabilities that the immediate-bihead model misses.
"The most obvious extension would be to condition upon not just one’s parent’s head, but one’s grandparent’s as well."
"This does capture some of the information we would like, partic-ularly the case heads of noun-phrases inside of prepositional phrases."
"For example, in “united states of america”, the probability of “america” is now conditioned not just on “of” (the head of its parent) but also on “states”."
"Unfortunately, for most of the cases where tri-gram really cleans up this revision would do lit-tle."
"Thus, in “he watched monday night football” “monday” would now be conditioned upon “foot-ball” and “watched.”"
"The addition of “watched” is unlikely to make much difference, certainly compared to the boost trigram models get by, in effect, recognizing the complete name."
"It is interesting to note, however, that virtu-ally all linguists believe that a noun-phrase like “monday night football” has significant substruc-ture — e.g., it would look something like Figure 2."
"If we assume this tree-structure the two heads above “monday” are “night” and “football” re-spectively, thus giving our trihead model the same power as the trigram for this case."
"Ignoring some of the conditioning events, we now get a proba-bility p(h = monday j i = night,j = football), which is much higher than the corresponding bi-head version p(h = monday j i = football)."
"The reader may remember that h is the head of the cur-rent constituent, while i is the head of its parent."
We now define j to be the grandparent head.
"We decided to adopt this structure, but to keep things simple we only changed the definition of “head” for the distribution p(h j t, l, m, u, i, j)."
Thus we adopted the following revised definition of head for constituents of base noun-phrases:
"For a pre-terminal (e.g., noun) con-stituent c of a base noun-phrase in which it is not the standard head (h) and which has as its right-sister another pre-terminal constituent d which is not it-self h, the head of c is the head of d."
The sole exceptions to this rule are phrase-initial determiners and numbers which retain h as their heads.
"In effect this definition assumes that the sub-structure of all base noun-phrases is left branch-ing, as in Figure 2."
"This is not true, but Lauer [11] shows that about two-thirds of all branching in base-noun-phrases is leftward."
We believe we would get even better results if the parser could determine the true branching structure.
"We then adopt the following definition of a grandparent-head feature j. 1. if c is a noun phrase under a prepositional phrase, or is a pre-terminal which takes a revised head as defined above, then j is the grandparent head of c, else 2. if c is a pre-terminal and is not next (in the production generating c) to the head of its parent (i) then j(c) is the head of the con-stituent next to c in the production in the di-rection of the head of that production, else 3. j is a “none-of-the-above” symbol."
Case 1 now covers both “united states of amer-ica” and “monday night football” examples.
"Case 2 handles other flat constituents in Penn tree-bank style (e.g., quantifier-phrases) for which we do not have a good analysis."
Case three says that this feature is a no-op in all other situations.
"The results for this model, again trained on F0- F20 and tested on F23-24, are given in Figure 3 under the heading ”Immediate-trihead model”."
"We see that the grammar perplexity is reduced to 130.20, a reduction of 10% over our first model, 14% over the previous best grammar model (152.26%), and 22% over the best of the above trigram models for the task (167.02)."
"When we run the trigram and new grammar model in tandem we get a perplexity of 126.07, a reduction of 8% over the best previous tandem model and 24% over the best trigram model."
"One interesting fact about the immediate-trihead model is that of the 3761 sentences in the test cor-pus, on 2934, or about 75%, the grammar model assigns a higher probability to the sentence than does the trigram model."
One might well ask what went “wrong” with the remaining 25%?
Why should the grammar model ever get beaten?
Three possible reasons come to mind: 1.
"The grammar model is better but only by a small amount, and due to sparse data prob-lems occasionally the worse model will luck out and beat the better one. 2."
"The grammar model and the trigram model capture different facts about the distribution of words in the language, and for some set of sentences one distribution will perform bet-ter than the other. 3."
"The grammar model is, in some sense, al-ways better than the trigram model, but if the parser bungles the parse, then the grammar model is impacted very badly."
Obviously the trigram model has no such Achilles’ heel.
"We ask this question because what we should do to improve performance of our grammar-based language models depends critically on which of these explanations is correct: if (1) we should col-lect more data, if (2) we should just live with the tandem grammar-trigram models, and if (3) we should create better parsers."
"Based upon a few observations on sentences from the development corpus for which the tri-gram model gave higher probabilities we hypoth-esized that reason (3), bungled parses, is primary."
To test this we performed the following experi-ment.
"We divide the sentences from the test cor-pus into two groups, ones for which the trigram model performs better, and the ones for which the grammar model does better."
We then collect labeled precision and recall statistics (the stan-dard parsing performance measures) separately for each group.
"If our hypothesis is correct we ex-pect the “grammar higher” group to have more ac-curate parses than the trigram-higher group as the poor parse would cause poor grammar perplexity for the sentence, which would then be worse than the trigram perplexity."
If either of the other two explanations were correct one would not expect much difference between the two groups.
The re-sults are shown in Table 4.
"We see there that, for example, sentences for which the grammar model has the superior perplexity have average recall 5.9 (= 84. 9 79. 0) percentage points higher than the sentences for which the trigram model performed better."
The gap for precision is 5.6.
This seems to support our hypothesis.
"We have presented two grammar-based language models, both of which significantly improve upon both the trigram model baseline for the task (by 24% for the better of the two) and the best pre-vious grammar-based language model (by 14%)."
Furthermore we have suggested that improve-ment of the underlying parser should improve the model’s perplexity still further.
"We should note, however, that if we were deal-ing with standard Penn Tree-bank Wall-Street-Journal text, asking for better parsers would be easier said than done."
"While there is still some progress, it is our opinion that substantial im-provement in the state-of-the-art precision/recall figures (around 90%) is unlikely in the near fu-ture. [Footnote_3]"
"3 Furthermore, some of the newest wrinkles [8] use dis-criminative methods and thus do not define language models at all, seemingly making them ineligible for the competition on a priori grounds."
"However, we are not dealing with stan-dard tree-bank text."
"As pointed out above, the text in question has been “speechified” by re-moving punctuation and capitalization, and “sim-plified” by allowing only a fixed vocabulary of 10,000 words (replacing all the rest by the sym-bol “UNK”), and replacing all digits and symbols by the symbol “N”."
We believe that the resulting text grossly under-represents the useful grammatical information available to speech-recognition systems.
"First, we believe that information about rare or even truly unknown words would be useful."
"For example, when run on standard text, the parser uses ending information to guess parts of speech [3]."
"Even if we had never encountered the word “show-boating”, the “ing” ending tells us that this is almost certainly a progressive verb."
It is much harder to determine this about UNK. [Footnote_4]
"4 To give the reader some taste for the difficulties pre-sented by UNKs, we encourage you to try parsing the fol-lowing real example: “its supposedly unk unk unk a unk that makes one unk the unk of unk unk the unk radical unk of unk and unk and what in unk even seems like unk in unk”."
"Secondly, while punctuation is not to be found in speech, prosody should give us something like equiva-lent information, perhaps even better."
"Thus sig-nificantly better parser performance on speech-derived data seems possible, suggesting that high-performance trigram-less language models may be within reach."
We believe that the adaptation of prosodic information to parsing use is a worthy topic for future research.
"Finally, we have noted two objections to immediate-head language models: first, they complicate left-to-right search (since heads are often to the right of their children) and second, they cannot be tightly integrated with trigram models."
The possibility of trigram-less language mod-els makes the second of these objections without force.
Nor do we believe the first to be a per-manent disability.
"If one is willing to provide sub-optimal probability estimates as one proceeds left-to-right and then amend them upon seeing the true head, left-to-right processing and immediate-head parsing might be joined."
"Note that one of the cases where this might be worrisome, early words in a base noun-phrase could be conditioned upon a head which comes several words later, has been made significantly less problematic by our revised definition of heads inside noun-phrases."
"We be-lieve that other such situations can be brought into line as well, thus again taming the search prob-lem."
"However, this too is a topic for future re-search."
We consider the question “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” and propose some theoret-ical and practical constraints on this problem.
"We then introduce a formal-ism which, under these constraints, maximally squeezes strong generative power out of context-free grammar."
"Finally, we generalize this result to formalisms beyond CFG."
“How much strong generative power can be squeezed out of a formal system without increas-ing its weak generative power?”
"This question, posed[REF_CITE], is important for both lin-guistic description and natural language process-ing."
"The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG[REF_CITE], or the extension of context free gram-mar (CFG) to tree insertion grammar[REF_CITE]or regular form TAG[REF_CITE]can be seen as steps toward answering this question."
But this question is difficult to answer with much finality unless we pin its terms down more precisely.
"First, what is meant by strong generative power?"
In the standard definiti[REF_CITE]a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions Σ(G); the strong genera-tive capacity of a formalism F is then {Σ(G) | F provides G}.
"There is some vagueness in the literature, however, over what structural descrip-tions are and how they can reasonably be com-pared across theories ([REF_CITE]gives a good synopsis)."
"The approach th[REF_CITE]and[REF_CITE]take, elaborated on[REF_CITE], is to identify a very general class of formalisms, which they call linear context-free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared."
"Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of some larger space of structural descriptions."
"Second, why is preservation of weak generative power important?"
"If we interpret this constraint to the letter, it is almost vacuous."
"For example, the class of all tree adjoining grammars which gen-erate context-free languages includes the gram-mar shown in Figure 1a (which generates the lan-guage {a, b} ∗ )."
"We can also add the tree shown in Figure 1b without increasing the grammar’s weak generative capacity; indeed, we can add any trees we please, provided they yield only as and bs."
"In-tuitively, the constraint of weak context-freeness has little force."
This intuition is verified if we consider that weak context-freeness is desirable for computa-tional efficiency.
"Though a weakly context-free TAG might be recognizable in cubic time (if we know the equivalent CFG), it need not be parsable in cubic time—that is, given a string, to compute all its possible structural descriptions will take O(n 6 ) time in general."
"If we are interested in com-puting structural descriptions from strings, then we need a tighter constraint than preservation of weak generative power."
"In Section 3 below we examine some restric-tions on tree adjoining grammar which are weakly context-free, and observe that their parsers all work in the same way: though given a TAG G, they implicitly parse using a CFG G 0 which de-rives the same strings as G, but also their corre-sponding structural descriptions under G, in such a way that preserves the dynamic-programming structure of the parsing algorithm."
"Based on this observation, we replace the con-straint of preservation of weak generative power with a constraint of simulability: essentially, a grammar G 0 simulates another grammar G if it generates the same strings that G does, as well as their corresponding structural descriptions under G (see Figure 2)."
"So then, within the class of context-free rewrit-ing systems, how does this constraint of simu-lability limit strong generative power?"
"In Sec-tion 4.1 we define a formalism called multicom-ponent multifoot TAG (MMTAG) which, when restricted to a regular form, characterizes pre-cisely those CFRSs which are simulable by a CFG."
"Thus, in the sense we have set forth, this formalism can be said to squeeze as much strong generative power out of CFG as is possible."
"Fi-nally, we generalize this result to formalisms be-yond CFG."
First we define context-free rewriting systems.
"What these formalisms have in common is that their derivation sets are all local sets (that is, gen-erable by a CFG)."
These derivations are taken as structural descriptions.
The following definitions are adapted[REF_CITE].
"Definition1 A generalized context-free gram-mar G is a tuple hV, S, F, Pi, where [Footnote_1]. V is a finite set of variables, 2. S ∈ V is a distinguished start symbol, 3. F is a finite set of function symbols, and 4."
"1 This is similar in spirit, but not the same as, the notion of derivational generative capacity[REF_CITE]."
"P is a finite set of productions of the form A → f (A 1 , . . . , A n ) where n ≥ 0, f ∈ F, and A, A i ∈ V."
"A generalized CFG G generates a set T(G) of terms, which are interpreted as derivations under some formalism."
"In this paper we require that G be free of spurious ambiguity, that is, that each term be uniquely generated."
"Definition 2 We say that a formalism F is a context-free rewriting system (CFRS) if its deriva-tion sets can be characterized by generalized CFGs, and its derived structures are produced by a function ~· F from terms to strings such that for each function symbol f, there is a yield function f F such that ~ f (t 1 , . . . , t n ) F = f F (~t 1  F , . . . , ~t n  F ) (A linear CFRS is subject to further restrictions, which we do not make use of.)"
"As an example, Figure 3 shows a simple TAG with a corresponding GCFG and interpretation."
"A nice property of CFRS is that any formal-ism which can be defined as a CFRS immedi-ately lends itself to several extensions, which arise when we give additional interpretations to the function symbols."
"For example, we can interpret the functions as ranging over probabilities, cre-ating a stochastic grammar; or we can interpret them as yield functions of another grammar, cre-ating a synchronous grammar."
Now we define strong generative capacity as the relationship between strings and structural de-scriptions. 1
"Definition 3 The strong generative capacity of a grammar G a CFRS F is the relation {h~t F , ti | t ∈ T (G)}."
"For example, the strong generative capacity of the grammar of Figure 3 is {ha m b n c n d m , α(β m1 ( ()), β n2 ( ())) i} whereas any equivalent CFG must have a strong generative capacity of the form {ha m b n c n d m , f m (g n (e()))i}"
"That is, in a CFG the n bs and cs must appear later in the derivation than the m as and ds, whereas in our example they appear in parallel."
We now take a closer look at some examples of “squeezed” context-free formalisms to illustrate how a CFG can be used to simulate formalisms with greater strong generative power than CFG.
"Tree substitution grammar (TSG), tree insertion grammar (TIG), and regular-form TAG (RF-TAG) are all weakly context free formalisms which can additionally be parsed in cubic time (with a caveat for RF-TAG below)."
"For each of these formalisms a CKY-style parser can be written whose items are of the form [X, i, j] and are combined in various ways, but always according to the schema [X, i, j] [Y, j, k] [Z, i, k] just as in the CKY parser for CFG."
"In effect the parser dynamically converts the TSG, TIG, or RF-TAG into an equivalent CFG—each parser rule of the above form corresponds to the rule schema Z → XY."
"More importantly, given a grammar G and a string w, a parser can reconstruct all possible derivations of w under G by storing inside each chart item how that item was inferred."
"If we think of the parser as dynamically converting G into a CFG G 0 , then this CFG is likewise able to com-positionally reconstruct TSG, TIG, or RF-TAG derivations—we say that G 0 simulates G."
"Note that the parser specifies how to convert G into G 0 , but G 0 is not itself a parser."
"Thus these three formalisms have a special relationship to CFG that is independent of any particular pars-ing algorithm: for any TSG, TIG, or RF-TAG G, there is a CFG that simulates G. We make this no-tion more precise below."
"Strictly speaking, the recognition algorithm Rogers gives cannot be extended to parsing; that is, it generates all possible derived trees for a given string, but not all possible derivations."
"It is correct, however, as a parser for a further re-stricted subclass of TAGs: Definition 4 We say that a TAG is in strict reg-ular form if there exists some partial ordering over the nonterminal alphabet such that for ev-ery auxiliary tree β, if the root and foot of β are labeled X, then for every node η along β’s spine where adjunction is allowed, X label(η), and X = label(η) only if η is a foot node. (In this vari-ant adjunction at foot nodes is permitted.)"
Thus the only kinds of adjunction which can oc-cur to unbounded depth are off-spine adjunction and adjunction at foot nodes.
This stricter definition still has greater strong generative capacity than CFG.
"For example, the TAG in Figure 3 is in strict regular form, because the only nodes along spines where adjunction is allowed are foot nodes."
So far we have not placed any restrictions on how these structural descriptions are computed.
"Even though we might imagine attaching arbi-trary functions to the rules of a parser, an algo-rithm like CKY is only really capable of com-puting values of bounded size, or else structure-sharing in the chart will be lost, increasing the complexity of the algorithm possibly to exponen-tial complexity."
"For a parser to compute arbitrary-sized objects, such as the derivations themselves, it must use back-pointers, references to the values of sub-computations but not the values themselves."
The only functions on a back-pointer the parser can compute online are the identity function (by copy-ing the back-pointer) and constant functions (by replacing the back-pointer); any other function would have to dereference the back-pointer and destroy the structure of the algorithm.
Therefore such functions must be computed offline.
Definition 5 A simulating interpretation ~· is a bijection between two recognizable sets of terms such that 1.
"For each function symbol φ, there is a func-tion φ̄ such that ~φ(t 1 , . . . , t n ) = φ̄(~t 1 , . . . , ~t n ) 2."
"Each φ̄ is definable as: φ̄(hx 11 , . .. , x 1m 1 ) i), . .. , hx n1 , . . . , x mn m i) = hw 1 , . .. , w m i where each w i can take one of the following forms: (a) a variable x ij , or (b) a function application f (x i 1 j 1 , . . . x i n j n ), n ≥ 0 [Footnote_3]."
"3 Without this requirement, there are certain pathological cases that cause the construction of Section 4.2 to produce infinite MM-TAGs."
"Furthermore, we require that for any recog-nizable set T, ~T is also a recognizable set."
"We say that ~· is trivial if every φ̄ is definable as φ̄(x 1 , . . . x n ) = f (x π(1) , . . . x π(n) ) where π is a permutation of {1, . .. , n}. 2"
"The rationale for requirement (3) is that it should not be possible, simply by imposing local constraints on the simulating grammar, to produce a simulated grammar which does not even come from a CFRS. 3"
Definition 6 We say that a grammar G from a CFRS F is (trivially) simulable by a grammar G’ from another CFRS F if there is a (trivial) simu-lating interpretation ~· s : T (G 0 ) → T (G) which satisfies ~t F 0 = ~~t s  F for all t ∈ T (G 0 ).
"As an example, a CFG which simulates the TAG of Figure 3 is shown in Figure 4."
"Note that if we give additional interpretations to the simu-lated yield functions α, β 1 , and β 2 , this CFG can compute any probabilities, translations, etc., that the original TAG can."
"Note that if G 0 trivially simulates G, they are very nearly strongly equivalent, except that the yield functions of G 0 might take their arguments in a different order than G, and there might be sev-eral yield functions of G 0 which correspond to a single yield function of G used in several different contexts."
"In fact, for technical reasons we will use this notion instead of strong equivalence for test-ing the strong generative power of a formal sys-tem."
"Thus the original problem, which was, given a formalism F, to find a formalism that has as much strong generative power as possible but re-mains weakly equivalent to F , is now recast as the following problem: find a formalism that triv-ially simulates as many grammars as possible but remains simulable by F ."
The following is easy to show:
Proposition 1 Simulability is reflexive and tran-sitive.
"Because of transitivity, it is impossible that a for-malism which is simulable by F could simulate a grammar that is not simulable by F ."
So we are looking for a formalism that can trivially simulate exactly those grammars that F can.
"In Section 4.1 we define a formalism called multicomponent multifoot TAG (MMTAG), and then in Section 4.[Footnote_2] we prove the following result: Proposition 2 A grammar G from a CFRS is simulable by a CFG if and only if it is trivially simulable by an MMTAG in regular form."
"2 Simulating interpretations and trivial simulating inter-pretations are similar to the generalized and “ungeneralized” syntax-directed translations, respectively, of Aho and Ull-man (1969; 1971)."
"The “if” direction (⇐) implies (because simu-lability is reflexive) that RF-MMTAG is simula-ble by a CFG, and therefore cubic-time parsable. (The proof below does give an effective proce-dure for constructing a simulating CFG for any RF-MMTAG.)"
"The “only if” direction (⇒) shows that, in the sense we have defined, RF-MMTAG is the most powerful such formalism."
We can generalize this result using the notion of a meta-level grammar[REF_CITE].
"Definition 7 If F 1 and F 2 are two CFRSs, F 2 ◦ F 1 is the CFRS characterized by the interpretation function ~· F 2 ◦F 1 = ~· F 2 ◦ ~· F 1 ."
"F 1 is the meta-level formalism, which generates derivations for F 2 ."
Obviously F 1 must be a tree-rewriting system.
"Proposition 3 For any CFRS F 0 , a grammar G from a (possibly different) CFRS is simulable by a grammar in F 0 if and only if it is trivially simu-lable by a grammar in F 0 ◦"
The “only if” direction (⇒) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG.
The “if” direction (⇐) is a little trickier because the constructed CFG inserts and relabels nodes.
"MMTAG resembles a cross between set-local multicomponent TAG[REF_CITE]and ranked node rewriting grammar[REF_CITE], a variant of TAG in which auxiliary trees may have multiple foot nodes."
It also has much in common with d-tree substitution grammar[REF_CITE].
"Definition 8 An elementary tree set α~ is a finite set of trees (called the components of α~) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic ∗; 2. Zero or more (non-foot) nodes are desig-nated adjunction nodes, which are parti-tioned into one or more disjoint sets called adjunction sites."
We notate this by assigning an index i to each adjunction site and mark-ing each node of site i with the diacritic i . 3.
Each component is associated with a sym-bol called its type.
"This is analogous to the left-hand side of a CFG rule (again, follow-ing Abe). 4."
The components of α~ are connected by d-edges from foot nodes to root nodes (notated by dotted lines) to form a single tree struc-ture.
"A single foot node may have multiple d-children, and their order is significant. (See Figure 5 for an example.)"
"A multicomponent multifoot tree adjoining gram-mar is a tuple hΣ, P, S i, where: 1."
Σ is a finite alphabet; 2.
P is a finite set of tree sets; and 3. S ∈ Σ is a distinguished start symbol.
Definition 9 A component α is adjoinable at a node η if η is an adjunction node and the type of α equals the label of η.
"The result of adjoining a component α at a node η is the tree set formed by separating η from its children, replacing η with the root of α, and re-placing the ith foot node of α with the ith child of η. (Thus adjunction of a one-foot component is analogous to TAG adjunction, and adjunction of a zero-foot component is analogous to substi-tution.)"
A tree set ~α is adjoinable at an adjunction site ~η if there is a way to adjoin each component of ~α at a different node of ~η (with no nodes left over) such that the dominance and precedence relations within ~α are preserved. (See Figure 5 for an ex-ample.)
We now define a regular form for MMTAG that is analogous to strict regular form for TAG.
A spine is the path from the root to a foot of a sin-gle component.
"Whenever adjunction takes place, several spines are inserted inside or concatenated with other spines."
"To ensure that unbounded in-sertion does not take place, we impose an order-ing on spines, by means of functions ρ i that map the type of a component to the rank of that com-ponent’s ith spine."
"We say that an MMTAG G is in regular form if there are functions ρ i from Σ into the domain of some partial ordering such that for each com-ponent α of type X, for each adjunction node η ∈ α, if the jth child of η dominates the ith foot node of α (that is, another component’s jth spine would adjoin into the ith spine), then ρ i (X) ρ j (label(η)), and ρ i (X) = ρ j (label(η)) only if η is safe in the ith spine."
Thus the only kinds of adjunction which can oc-cur to unbounded depth are off-spine adjunction and safe adjunction.
The adjunction shown in Fig-ure 5 is an example of safe adjunction.
First we describe how to construct a simu-lating CFG for any RF-MMTAG; then this direc-tion of the proof follows from the transitivity of simulability.
"When a CFG simulates a regular form TAG, each nonterminal must encapsulate a stack (of bounded depth) to keep track of adjunctions."
"In the multicomponent case, these stacks must be generalized to trees (again, of bounded size)."
"So the nonterminals of G 0 are of the form [η, t], where t is a derivation fragment of G with a dot (·) at exactly one node α~, and η is a node of α~. Let η̄ be the node in the derived tree where η ends up."
A fragment t can be put into a normal form as follows: 1.
"For every ~α above the dot, if η̄ does not lie along a spine of ~α, delete everything above α~. 2."
"For every α~ not above or at the dot, if η̄ does not lie along a d-edge of ~α, delete ~α and everything below and replace it with &gt; if η̄ dominates ~α; otherwise replace it with ⊥. 3."
"If there are two nodes α~ 1 and ~α 2 along a path which name the same tree set and η̄ lies along the same spine or same d-edge in both of them, collapse α~ 1 and ~α 2 , deleting every-thing in between."
"Basically this process removes all unboundedly long paths, so that the set of normal forms is finite."
"In the rule schemata below, the terms in the left-hand sides range over normalized terms, and their corresponding right-hand sides are renormalized."
Let up(t) denote the tree that results from moving the dot in t up one step.
"The value of a subderivation t 0 of G 0 under ~· s is a tuple of partial derivations of G, one for each & gt; symbol in the root label of t 0 , in order."
"Where we do not define a yield function for a production below, the identity function is understood."
"For every set ~α with a single, S-type compo-nent rooted by η, add the rule"
"S → [η, α~· (&gt;, . . . , &gt;)] α~(x 1 , . . . , x n ) ← hx 1 , . . . , x n i"
"For every non-adjunction, non-foot node η with children η 1 , . . . , η n (n ≥ 0), [η, t] → [η 1 , t] · · · [η n , t]"
"For every component with root η 0 that is adjoin-able at η, [η, up(t)] → [η 0 , t]"
"If η 0 is the root of the whole set α~ 0 , this rule rewrites a &gt; to several &gt; symbols; the corre-sponding yield function is then h. . . , α~ 0 (x 1 , . .. , x n ), .. .i ← h. . . , x 1 , . .. , x n , . . .i"
"For every component with ith foot η 0i that is ad-joinable at a node with ith child η i , [η 0i , t] → [η i , up(t)]"
"This last rule skips over deleted parts of the derivation tree, but this is harmless in a regular form MMTAG, because all the skipped adjunc-tions are safe. (⇒)"
First we describe how to decompose any given derivation t 0 of G 0 into a set of elementary tree sets.
"Let t = ~t 0  s . (Note the convention that primed variables always pertain to the simulating gram-mar, unprimed variables to the simulated gram-mar.)"
"If, during the computation of t, a node η 0 creates the node η, we say that η 0 is productive and produces η."
"Without loss of generality, let us assume that there is a one-to-one correspondence between productive nodes and nodes of t. [Footnote_4]"
"4 If G 0 does not have this property, it can be modified so that it does. This may change the derived trees slightly, which makes the proof of Proposition 3 trickier."
"To start, let η be the root of t, and η 1 , . . . , η n its children."
"Define the domain of η i as follows: any node in t 0 that produces η i or any of its descendants is in the domain of η i , and any non-productive node whose parent is in the domain of η i is also in the domain of η i ."
"For each η i , excise each connected component of the domain of η i ."
"This operation is the reverse of adjunction (see Figure 6): each component gets mar of Figure 4, and first step of decomposition."
"Non-adjunction nodes are shown with the place-holder • (because the yield functions in the origi-nal grammar were anonymous), the Greek letters indicating what is produced by each node."
Ad-junction nodes are shown with labels Q i in place of the (very long) true labels.
"Labeling adjunction nodes For any node η 0 , and any list of nodes hη 01 ,...,η 0n i, let the sig-nature of η 0 with respect to hη 01 ,...,η 0n i be hA, a 1 , . . . , a m i, where A is the left-hand side of the GCFG production that generated η 0 , and a i = hj,ki if η 0 gets its ith field from the kth field of η 0j , or ∗ if η 0 produces a function symbol in its ith field."
"So when we excise the domain of η i , the la-bel of the node left behind by a component α is hs, s 1 , . . . , s n i, where s is the signature of the root of α with respect to the foot nodes and s 1 , . . . , s n are the signatures of the foot nodes with respect to their d-children."
"Note that the number of possible adjunction labels is finite, though large."
"Ĝ trivially simulates G. Since each tree of Ĝ corresponds to a function symbol (though not necessarily one-to-one), it is easy to write a triv-ial simulating interpretation ~· : T (Ĝ) → T (G)."
"To see that Ĝ does not overgenerate, observe that the nonterminal labels inside the signatures en-sure that every derivation of Ĝ corresponds to a valid derivation of G 0 , and therefore G. To see that ~· is one-to-one, observe that the adjunction la-bels keep track of how G 0 constructed its simu-lated derivations, ensuring that for any derivation tˆ of Ĝ, the decomposition of the derived tree of tˆ is tˆ itself."
"Therefore two derivations of Ĝ cannot correspond to the same derivation of G 0 , nor of G. foot nodes to replace its lost children, and the components are connected by d-edges according to their original configuration."
Meanwhile an adjunction node is created in place of each component.
This node is given a la-bel (which also becomes the type of the excised component) whose job is to make sure the final grammar does not overgenerate; we describe how the label is chosen below.
The adjunction nodes are partitioned such that the ith site contains all the adjunction nodes created when removing η i .
"The tree set that is left behind is the elementary tree set corresponding to η (rather, the function symbol that labels η); this process is repeated re-cursively on the children of η, if any."
Thus any derivation of G 0 can be decomposed into elementary tree sets.
Let Ĝ be the union of the decompositions of all possible derivations of G 0 (see Figure 7 for an example).
Ĝ is finite.
"Briefly, suppose that the number of components per tree set is unbounded."
"Then it is possible, by intersecting G 0 with a recognizable set, to obtain a grammar whose simulated deriva-tion set is non-recognizable."
"The idea is that mul-ticomponent tree sets give rise to dependent paths in the derivation set, so if there is no bound on the number of components in a tree set, neither is there a bound on the length of dependent paths."
This contradicts the requirement that a simulating interpretation map recognizable sets to recogniz-able sets.
Suppose that the number of nodes per compo-nent is unbounded.
"If the number of components per tree set is bounded, so must the number of ad-junction nodes per component; then it is possible, again by intersecting G 0 with a recognizable set, to obtain a grammar which is infinitely ambigu-ous with respect to simulated derivations, which contradicts the requirement that simulating inter-pretations be bijective."
Ĝ is in regular form.
"A component of Ĝ corre-sponds to a derivation fragment of G 0 which takes fields from several subderivations and processes them, combining some into a larger structure and copying some straight through to the root."
Let ρ i (X) be the number of fields that a component of type X copies from its ith foot up to its root.
"This information is encoded in X, in the signa-ture of the root."
"Then Ĝ satisfies the regular form constraint, because when adjunction inserts one spine into another spine, the the inserted spine must copy at least as many fields as the outer one."
"Furthermore, if the adjunction site is not safe, then the inserted spine must additionally copy the value produced by some lower node."
"We have proposed a more constrained version of Joshi’s question, “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power,” and shown that within these constraints, a vari-ant of TAG called MMTAG characterizes the limit of how much strong generative power can be squeezed out of CFG."
"Moreover, using the notion of a meta-level grammar, this result is extended to formalisms beyond CFG."
"It remains to be seen whether RF-MMTAG, whether used directly or for specifying meta-level grammars, provides further practical benefits on top of existing “squeezed” grammar formalisms like tree-local MCTAG, tree insertion grammar, or regular form TAG."
"This way of approaching Joshi’s question is by no means the only way, but we hope that this work will contribute to a better understanding of the strong generative capacity of constrained gram-mar formalisms as well as reveal more powerful formalisms for linguistic analysis and natural lan-guage processing."
"We develop a framework for formaliz-ing semantic construction within gram-mars expressed in typed feature struc-ture logics, including HPSG ."
"The ap-proach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification-based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability."
Some constraint-based grammar formalisms in-corporate both syntactic and semantic representa-tions within the same structure.
"For instance, Fig-ure 1 shows representations of typed feature struc-tures ( TFS s) for Kim, sleeps and the phrase Kim sleeps, in an HPSG -like representation, loosely based[REF_CITE]."
"The semantic representation expressed is intended to be equiv-alent to r name(x, Kim) ∧ sleep(e, x). 1 Note: [Footnote_1]. Variable equivalence is represented by coin-dexation within a TFS . 2."
"1 The variables are free, we will discuss scopal relation-ships and quantifiers below."
The coindexation in Kim sleeps is achieved as an effect of instantiating the SUBJ slot in the sign for sleeps. 3.
"Structures representing individual predicate applications (henceforth, elementary predi-cations, or EP s) are accumulated by an ap-pend operation."
Conjunction of EP s is im-plicit. 4.
All signs have an index functioning some-what like a λ-variable.
A similar approach has been used in a large number of implemented grammars (see[REF_CITE]for a fairly early example).
It is in many ways easier to work with than λ-calculus based approaches (which we discuss further below) and has the great advantage of allowing generaliza-tions about the syntax-semantics interface to be easily expressed.
But there are problems.
"The operations are only specified in terms of the TFS logic: the interpretation relies on an intuitive cor-respondence with a conventional logical represen-tation, but this is not spelled out."
Furthermore the operations on the semantics are not tightly specified or constrained.
"For instance, although HPSG has the Semantics Principle[REF_CITE]this does not stop the composition pro-cess accessing arbitrary pieces of structure, so it is often not easy to conceptually disentangle the syntax and semantics in an HPSG ."
"Nothing guar-antees that the grammar is monotonic, by which we mean that in each rule application the seman-tic content of each daughter subsumes some por-tion of the semantic content of the mother (i.e., no semantic information is dropped during com-position): this makes it impossible to guarantee that certain generation algorithms will work ef-fectively."
"Finally, from a theoretical perspective, it seems clear that substantive generalizations are being missed."
"Minimal Recursion Semantics ( MRS :[REF_CITE], see also[REF_CITE]) tight-ens up the specification of composition a little."
It enforces monotonic accumulation of EP s by making all rules append the EP s of their daugh-ters (an approach which was followed[REF_CITE]) but it does not fully spec- semantics in TFS s ify compositional principles and does not for-malize composition.
"We attempt to rectify these problems, by developing an algebra which gives a general way of expressing composition."
"The semantic algebra lets us specify the allowable operations in a less cumbersome notation than TFS s and abstracts away from the specific fea-ture architecture used in individual grammars, but the essential features of the algebra can be en-coded in the hierarchy of lexical and construc-tional type constraints."
Our work actually started as an attempt at rational reconstruction of se-mantic composition in the large grammar imple-mented by the LinGO project at CSLI (available via[URL_CITE]
"Se-mantics and the syntax/semantics interface have accounted for approximately nine-tenths of the development time of the English Resource Gram-mar ( ERG ), largely because the account of seman-tics within HPSG is so underdetermined."
"In this paper, we begin by giving a formal ac-count of a very simplified form of the algebra and in §3, we consider its interpretation."
"In §4 to §6, we generalize to the full algebra needed to capture the use of MRS in the LinGO English Resource Grammar ( ERG )."
Finally we conclude with some comparisons to the λ-calculus and to other work on unification based grammar.
The following shows the equivalents of the struc-tures in Figure 1 in our algebra:
"Kim: [x 2 ]{[] subj , [] comp }[r name(x 2 , Kim)]{} sleeps: [e 1 ]{[x 1 ] subj , [] comp }[sleep(e 1 , x 1 )]{} Kim sleeps: [e 1 ]{[] subj , [] comp }[sleep(e 1 , x 1 ), r name(x 2 , Kim)]{x 1 = x 2 }"
"The last structure is semantically equivalent to: [sleep(e 1 , x 1 ), r name(x 1 , Kim)]."
"In the structure for sleeps, the first part, [e 1 ], is a hook and the second part ([x 1 ] subj and [] comp ) is the holes."
"The third element (the lzt) is a bag of elementary predications ( EP s). [Footnote_2] Intuitively, the hook is a record of the value in the semantic en-tity that can be used to fill a hole in another entity during composition."
"2 As usual in MRS , this is a bag rather than a set because we do not want to have to check for/disallow repeated EP s; e.g., big big car."
The holes record gaps in the semantic form which occur because it represents a syntactically unsaturated structure.
"Some struc-tures have no holes, such as that for Kim."
"When structures are composed, a hole in one structure (the semantic head) is filled with the hook of the other (by equating the variables) and their lzts are appended."
"It should be intuitively obvious that there is a straightforward relationship between this algebra and the TFS s shown in Figure 1, al-though there are other TFS architectures which would share the same encoding."
We now give a formal description of the alge-bra.
"In this section, we simplify by assuming that each entity has only one hole, which is unlabelled, and only consider two sorts of variables: events and individuals."
The set of semantic entities is built from the following vocabulary: 1.
"The absurdity symbol ⊥. 2. indices i 1 , i 2 , . . . , consisting of two subtypes of indices: events e 1 , e 2 , . . . and individuals x 1 , x 2 , . . .. 3. n-place predicates, which take indices as ar-guments 4. =."
"Equality can only be used to identify variables of compatible sorts: e.g., x 1 = x 2 is well formed, but e = x is not."
Sort compatibility corresponds to unifiability in the TFS logic.
Definition 1 Simple Elementary Predications ( SEP )
An SEP contains two components: 1.
A relation symbol 2.
"A list of zero or more ordinary variable ar-guments of the relation (i.e., indices)"
"This is written relation(arg 1 , . . . ,arg n )."
"For in-stance, like(e, x, y) is a well-formed SEP ."
"Equality Conditions: Where i 1 and i 2 are in-dices, i 1 = i 2 is an equality condition."
"Definition 2 The Set Σ of Simple semantic Enti-ties ( SSEMENT ) s ∈ Σ if and only if s = ⊥ or s = hs 1 , s 2 , s 3 , s 4 i such that: • s 1 = {[i]} is a hook; • s 2 = ∅ or {[i 0 ]} is a hole; • s 3 is a bag of SEP s(the lzt) • s 4 is a set of equalities between variables (the eqs)."
We write a SSEMENT as: [i 1 ] [i 2 ][ SEP s]{ EQ s}.
Note for convenience we omit the set markers {} from the hook and hole when there is no possible confusion.
"The SEP s, and EQ s are (partial) de-scriptions of the fully specified formulae of first order logic."
"The SEP s and EQ s can be interpreted with respect to a first order model hE, A, F i where: 1. E is a set of events 2."
"A is a set of individuals 3. F is an interpretation function, which as-signs tuples of appropriate kinds to the pred-icates of the language."
"The truth definition of the SEP s and EQ s (which we group together under the term SMRS , for simple MRS ) is as follows: 1."
"For all events and individuals v, [v] hM,gi = g(v). 2."
"For all n-predicates P n , [P n ] hM,gi = {ht 1 , . . . , t n i : ht 1 , . . . , t n i ∈ F (P n )}. 3. [P n (v 1 , . . . , v n )] hM,gi = 1 iff h[v 1 ] hM,gi , . . . , [v n ] hM,gi i ∈ [P n ] hM,gi . 4. [φ ∧ ψ] hM,gi = 1 iff [φ] hM,gi = 1 and [ψ] hM,gi = 1."
"Thus, with respect to a model M, an SMRS can be viewed as denoting an element of P(G), where G is the set of variable assignment functions (i.e., elements of G assign the variables e, . . . and x, . . . their denotations): [smrs] M = {g : g is a variable assignment function and M |= g smrs}"
We now consider the semantics of the algebra.
This must define the semantics of the operation op in terms of a function f which is defined entirely in terms of the denotations of op’s arguments.
"In other words, [op(a 1 ,a 2 )] = f([a 1 ],[a 2 ]) for some function f. Intuitively, where the SMRS of the SEMENT a 1 denotes G 1 and the SMRS of the SEMENT a 2 denotes G 2 , we want the seman-tic value of the SMRS of op(a 1 , a 2 ) to denote the following:"
G 1 ∩ G 2 ∩ [hook(a 1 ) = hole(a 2 )]
But this cannot be constructed purely as a func-tion of G 1 and G 2 .
The solution is to add hooks and holes to the denotations of SEMENTS ( cf.[REF_CITE]).
"We define the denotation of a SEMENT to be an ele-ment of I × I × P(G), where I = E ∪ A, as follows:"
Definition 4 Denotations of SEMENT s
"If a 6= ⊥ is a SEMENT , [[a]] M = h[i],[i 0 ],Gi where: 1. [i] = hook(a) 2. [i 0 ] = hole(a) 3."
"G = {g : M |= g smrs(a)} [[⊥]] M = h∅, ∅, ∅i"
"So, the meanings of SEMENT s are ordered three-tuples, consisting of the hook and hole elements (from I) and a set of variable assignment func-tions that satisfy the SMRS ."
We can now define the following operation f over these denotations to create an algebra:
"Definition5 Semantics of the Semantic Con-struction Algebra hI × I × P(G), fi is an algebra, where: f(h∅, ∅, ∅i, h[i 2 ], [i 20 ], G 2 i) = h∅, ∅, ∅i f(h[i 1 ], [i 01 ], G 1 i, h∅, ∅, ∅i) = h∅, ∅, ∅i f(h[i 1 ], [i 01 ], G 1 i, h[i 2 ], ∅, G 2 i = h∅, ∅, ∅i f(h[i 1 ], [i 01 ], G 1 i, h[i 2 ], [i 02 ], G 2 i) = h[i 2 ], [i 01 ], G 1 ∩ G 2 ∩"
G 0 i where G 0 = {g : g(i 1 ) = g(i 02 )}
And this operation demonstrates that semantic construction is compositional:
Theorem 2 Semantics of Semantic Construction is Compositional
"The mapping [[]] : hΣ, opi −→ hhI, I, Gi, fi is a homomorphism (so [[op(a 1 , a 2 )]] = f([[a 1 ]], [[a 2 ]]))."
"This follows from the definitions of [], op and f."
We now start considering the elaborations neces-sary for real grammars.
"As we suggested earlier, it is necessary to have multiple labelled holes."
"There will be a fixed inventory of labels for any grammar framework, although there may be some differences between variants. [Footnote_3]"
"3 For instance,[REF_CITE]omit the distinction between SPR and SUBJ that is often made in other HPSG s."
"In HPSG , comple-ments are represented using a list, but in general there will be a fixed upper limit for the number of complements so we can label holes COMP 1, COMP 2, etc."
"The full inventory of labels for the ERG is: SUBJ , SPR , SPEC , COMP 1, COMP 2, COMP [Footnote_3] and MOD (see[REF_CITE])."
"3 For instance,[REF_CITE]omit the distinction between SPR and SUBJ that is often made in other HPSG s."
"To illustrate the way the formalization goes with multiple slots, consider op subj :"
"Definition 6 The definition of op subj op subj (a 1 , a 2 ) is the following: If a 1 = ⊥ or a 2 = ⊥ or hole subj (a 2 ) = ∅, then op subj (a 1 , a 2 ) = ⊥. And if ∃l =6 subj such that: |hole l (a 1 ) ∪ hole l (a 2 )| &gt; 1 then op subj (a 1 , a 2 ) = ⊥. Otherwise: 1. hook(op subj (a 1 , a 2 )) = hook(a 2 ) 2."
"For all labels l 6= subj: hole l (op subj (a 1 , a 2 )) hole l (a 1 ) ∪= hole l (a 2 ) 3. lzt(op subj (a 1 , a 2 )) = lzt(a 1 ) ⊕ lzt(a 2 ) 4. eq(op subj (a 1 , a 2 )) ="
T r(eq(a 1 ) ∪ eq(a 2 )∪ {hook(a 1 ) = hole subj (a 2 )}) where T r stands for transitive closure.
"There will be similar operations op comp1 , op comp2 etc for each labelled hole."
"These operations can be proved to form an algebra hΣ, op subj , op comp1 , . . .i in a similar way to the unlabelled case shown in Theorem 1."
A lit-tle more work is needed to prove that op l is closed on Σ.
"In particular, with respect to clause 2 of the above definition, it is necessary to prove that op l (a 1 , a 2 ) = ⊥ or for all labels l 0 , |hole l 0 (op l (a 1 , a 2 ))| ≤ 1, but it is straightforward to see this is the case."
"These operations can be extended in a straight-forward way to handle simple constituent coor-dination of the kind that is currently dealt with in the ERG (e.g., Kim sleeps and talks and Kim and Sandy sleep); such cases involve daughters with non-empty holes of the same label, and the semantic operation equates these holes in the mother SEMENT ."
"The algebra with labelled holes is sufficient to deal with simple grammars, such as that[REF_CITE], but to deal with scope, more is needed."
It is now usual in constraint based gram-mars to allow for underspecification of quantifier scope by giving labels to pieces of semantic in-formation and stating constraints between the la- bels.
"In MRS , labels called handles are associ-ated with each EP ."
Scopal relationships are rep-resented by EP s with handle-taking arguments.
"If all handle arguments are filled by handles la-belling EP s, the structure is fully scoped, but in general the relationship is not directly specified in a logical form but is constrained by the gram-mar via additional conditions (handle constraints or hcons). [Footnote_4] A variety of different types of condi-tion are possible, and the algebra developed here is neutral between them, so we will simply use rel h to stand for such a constraint, intending it to be neutral between, for instance, = q (qeq: equal-ity modulo quantifiers) relationships used in MRS and the more usual ≤ relationships from UDRT[REF_CITE]."
"4 The underspecified scoped forms which correspond to sentences can be related to first order models of the fully scoped forms (i.e., to models of WFF s without labels) via supervaluation (e.g.,[REF_CITE]). This corresponds to stip-ulating that an underspecified logical form u entails a base, fully specified form φ only if all possible ways of resolving the underspecification in u entails φ. For reasons of space, we do not give details here, but note that this is entirely con-sistent with treating semantics in terms of a description of a logical formula. The relationship between the SEMENTS of non-sentential constituents and a more ‘standard’ formal language such as λ-calculus will be explored in future work."
The conditions in hcons are accu-mulated by append.
"To accommodate scoping in the algebra, we will make hooks and holes pairs of indices and handles."
The handle in the hook corresponds to the LTOP feature in MRS .
The new vocabulary is: 1.
"The absurdity symbol ⊥. 2. handles h 1 , h 2 , . . . 3. indices i 1 , i 2 , . . . , as before 4. n-predicates which take handles and indices as arguments 5. rel h and =."
The revised definition of an EP is as in MRS :
Definition 7 Elementary Predications ( EP s)
"An EP contains exactly four components: 1. a handle, which is the label of the EP 2. a relation 3. a list of zero or more ordinary variable ar-guments of the relation (i.e., indices) 4. a list of zero or more handles corresponding to scopal arguments of the relation."
"This is written h:r(a 1 , . . . ,a n ,sa 1 , . .. ,sa m )."
"For instance, h:every(x, h 1 , h 2 ) is an EP . [Footnote_5]"
"5 Note every is a predicate rather than a quantifier in this language, since MRS s are partial descriptions of logical forms in a base language."
We revise the definition of semantic entities to add the hcons conditions and to make hooks and holes pairs of handles and indices.
"H-Cons Conditions: Where h 1 and h 2 are handles, h 1 rel h h 2 is an H-Cons condition."
"Definition 8 The Set Σ of Semantic Entities s ∈ Σ if and only if s = ⊥ or s = hs 1 , s 2 , s 3 , s 4 , s 5 i such that: • s 1 = {[h, i]} is a hook; • s 2 = ∅ or {[h 0 , i 0 ]} is a hole; • s 3 is a bag of EP conditions • s 4 is a bag of HCONS conditions • s 5 is a set of equalities between variables."
"SEMENT s are: [h 1 , i 1 ]{holes}[eps][hcons]{eqs}."
"We will not repeat the full composition def-inition, since it is unchanged from that in §2 apart from the addition of the append operation on hcons and a slight complication of eq to deal with the handle/index pairs: eq(op(a 1 , a 2 )) ="
"T r(eq(a 1 ) ∪ eq(a 2 )∪ {hdle(hook(a 1 )) = hdle(hole(a 2 )), ind(hook(a 1 )) = ind(hole(a 2 ))}) where Tr stands for transitive closure as before and hdle and ind access the handle and index of a pair."
"We can extend this to include (several) la-belled holes and operations, as before."
And these revised operations still form an algebra.
The truth definition for SEMENTS is analogous to before.
We add to the model a set of la-bels L (handles denote these via g) and a well-founded partial order ≤ on L (this helps interpret the hcons; cf.
A SEMENT then denotes an element of H × . . .
"H × P(G), where the Hs (= L × I) are the new hook and holes."
"Note that the language Σ is first order, and we do not use λ-abstraction over higher or-der elements. [Footnote_6]"
"6 Even though we do not use λ-calculus for composition, we could make use of λ-abstraction as a representation de-vice, for instance for dealing with adjectives such as former, cf.,[REF_CITE]."
"For example, in the standard Montagovian view, a quantifier such as every is represented by the higher-order expression λP λQ∀x(P (x), Q(x))."
"In our framework, how-ever, every is the following (using qeq conditions, as in the LinGO ERG ): [h f , x]{[] subj , [] comp1 , [h 0 , x] spec , . . .} [h e : every(x, h r , h s )] [h r = q h 0 ] {} and dog is: [h d , y]{[] subj , [] comp1 , [] spec , . . .}[h d : dog(y)] []{} So these composes via op spec to yield every dog: [h f , x]{[] subj , [] comp1 , [] spec , . . .} [h e : every(x, h r , h s ), h d : dog(y)] [h r = q h 0 ]{h 0 = h d , x = y}"
"This SEMENT is semantically equivalent to: [h f , x]{[] subj , [] comp1 , [] spec , . . .} [h e : every(x, h r , h s ), h d : dog(x)][h r = q h d ]{}"
A slight complication is that the determiner is also syntactically selected by the N 0 via the SPR slot (following[REF_CITE]).
"How-ever, from the standpoint of the compositional semantics, the determiner is the semantic head, and it is only its SPEC hole which is involved: the N 0 must be treated as having an empty SPR hole."
"In the ERG , the distinction between intersective and scopal modification arises because of distinc-tions in representation at the lexical level."
The repetition of variables in the SEMENT of a lexical sign (corresponding to TFS coindexation) and the choice of type on those variables determines the type of modification.
"Intersective modification: white dog: dog: [h d , y]{[] subj , [] comp1 , . . . , [] mod } [h d : dog(y)] []{} white: [h w , x]{[] subj , [] comp1 , .., [h w , x] mod } [h w : white(x)] []{} white dog: [h w , x]{[] subj , [] comp1 , . . . , [] mod } (op mod ) [h d : dog(y), h w : white(x)] [] {h w = h d , x = y}"
"Scopal Modification: probably walks: walks: [h w , e 0 ] {[h 0 , x] subj , [] comp1 , . . . , [] mod } [h w : walks(e 0 , x)] []{} probably: [h p , e]{[] subj , [] comp1 , . . . , [h, e] mod } [h p : probably(h s )] [h s = q h]{} probably [h p , e]{[h 0 , x] subj , [] comp1 , . . . , [] mod } walks: [h p :probably(h s ), h w :walks(e 0 , x)] (op mod ) [h s = q h]{h w = h, e = e 0 }"
"We need to make one further extension to allow for control, which we do by adding an extra slot to the hooks and holes corresponding to the external argument (e.g., the external argument of a verb always corresponds to its subject position)."
We illustrate this by showing two uses of expect; note the third slot in the hooks and holes for the exter-nal argument of each entity.
"In both cases, x 0e is both the external argument of expect and its sub-ject’s index, but in the first structure x 0e is also the external argument of the complement, thus giving the control effect. expect 1 (as in Kim expected to sleep) [h e , e e , x 0e ]{[h s , x 0e , x 0s ] subj , [h c , e c , x 0e ] comp1 , . . .} [h e : expect(e e , x 0e , h 0e )] [h 0e = q h c ]{} expect 2 (Kim expected that Sandy would sleep) [h e , e e , x e0 ]{[h s , x e0 , x 0s ] subj , [h c , e c , x 0c ] comp1 , . . .} [h : expect(e e , x 0e , h e0 )] [h 0e = q h c ]{} Although these uses require different lexical en-tries, the semantic predicate expect used in the two examples is the same, in contrast to Montago-vian approaches, which either relate two distinct predicates via meaning postulates, or require an additional semantic combinator."
"The HPSG ac-count does not involve such additional machinery, but its formal underpinnings have been unclear: in this algebra, it can be seen that the desired re-sult arises as a consequence of the restrictions on variable assignments imposed by the equalities."
This completes our sketch of the algebra neces-sary to encode semantic composition in the ERG .
We have constrained accessibility by enumerating the possible labels for holes and by stipulating the contents of the hooks.
"We believe that the han-dle, index, external argument triple constitutes all the semantic information that a sign should make accessible to a functor."
"The fact that only these pieces of information are visible means, for in-stance, that it is impossible to define a verb that controls the object of its complement. [Footnote_7] Although obviously changes to the syntactic valence fea-tures would necessitate modification of the hole labels, we think it unlikely that we will need to in-crease the inventory further."
"7 Readers familiar with MRS will notice that the KEY fea-ture used for semantic selection violates these accessibility conditions, but in the current framework, KEY can be re-placed by KEYPRED which points to the predicate alone."
"In combination with the principles defined[REF_CITE]for qeq conditions, the algebra presented here re-sults in a much more tightly specified approach to semantic composition than that[REF_CITE]."
"Compared with λ-calculus, the approach to com-position adopted in constraint-based grammars and formalized here has considerable advantages in terms of simplicity."
"The standard Montague grammar approach requires that arguments be presented in a fixed order, and that they be strictly typed, which leads to unnecessary multiplication of predicates which then have to be interrelated by meaning postulates (e.g., the two uses of ex-pect mentioned earlier)."
Type raising also adds to the complexity.
"As standardly presented, λ-calculus does not constrain grammars to be mono-tonic, and does not control accessibility, since the variable of the functor that is λ-abstracted over may be arbitrarily deeply embedded inside a λ-expression."
None of the previous work on unification-based approaches to semantics has considered constraints on composition in the way we have presented.
"In fact,[REF_CITE]explicitly advocates nonmonotonicity."
"He has to divorce the interpretation of the expres-sions from the notion of truth with respect to the model, which is much like treating the semantics as a description of a logic formula."
"Our strategy for formalization is closest to that adopted in Uni-fication Categorial Grammar[REF_CITE], but rather than composing actual logical forms we compose partial descriptions to handle semantic underspecification."
We have developed a framework for formally specifying semantics within constraint-based rep-resentations which allows semantic operations in a grammar to be tightly specified and which al-lows a representation of semantic content which is largely independent of the feature structure ar-chitecture of the syntactic representation.
"HPSG s can be written which encode much of the algebra described here as constraints on types in the gram-mar, thus ensuring that the grammar is consistent with the rules on composition."
"There are some as-pects which cannot be encoded within currently implemented TFS formalisms because they in-volve negative conditions: for instance, we could not write TFS constraints that absolutely prevent a grammar writer sneaking in a disallowed coin-dexation by specifying a path into the lzt."
There is the option of moving to a more general TFS logic but this would require very considerable research to develop reasonable tractability.
"Since the con-straints need not be checked at runtime, it seems better to regard them as metalevel conditions on the description of the grammar, which can any-way easily be checked by code which converts the TFS into the algebraic representation."
"Because the ERG is large and complex, we have not yet fully completed the exercise of retrospec-tively implementing the constraints throughout."
"However, much of the work has been done and the process revealed many bugs in the grammar, which demonstrates the potential for enhanced maintainability."
"We have modified the grammar to be monotonic, which is important for the chart generator described[REF_CITE]."
A chart generator must determine lexical entries di-rectly from an input logical form: hence it will only work if all instances of nonmonotonicity can be identified in a grammar-specific preparatory step.
We have increased the generator’s reliability by making the ERG monotonic and we expect fur-ther improvements in practical performance once we take full advantage of the restrictions in the grammar to cut down the search space.
"We present a machine learning approach to evaluating the well-formedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations."
"This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings."
"The method presented is fully automated and independent of source language, target language and domain."
"Human evaluation of machine translation (MT) output is an expensive process, often prohibitively so when evaluations must be performed quickly and frequently in order to measure progress."
This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement.
It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer.
"Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output."
"To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. ([REF_CITE]for useful discussion of this issue.)"
"The better the MT system, the more its output will resemble human-generated text."
"Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation."
We have observed that in general humans can easily and reliably categorize a sentence as either machine- or human-generated.
"Moreover, they can usually justify their decision."
"This observation suggests that evaluation of the wellformedness of output sentences can be treated as a classification problem: given a sentence, how accurately can we predict whether it has been translated by machine?"
In this paper we cast the problem of MT evaluation as a machine learning classification task that targets both linguistic features and more abstract features such as n-gram perplexity.
"Our corpus consists of 350,000 aligned Spanish-English sentence pairs taken from published computer software manuals and online help documents."
"From the remainder of the corpus, we extracted 100,000 aligned sentence pairs."
"The Spanish sentences in this latter sample were then translated by the Microsoft machine translation system, which was trained on documents from this doma[REF_CITE]."
"This yielded a set of 200,000 English sentences, one half of which were English reference sentences, and the other half of which were MT output. (The Spanish sentences were not used in building or evaluating the classifiers)."
"We split the 200,000[REF_CITE]/10, to yield 180,000 sentences for training classifiers and 20,000 sentences that we used as held-out test data."
Training and test data were evenly divided between reference English sentences and Spanish-to-English translations.
The selection of features used in our classification task was motivated by failure analysis of system output.
"We were particularly interested in those linguistic features that could aid in qualitative analysis, as we discuss in section 5."
For each sentence we automatically extracted 46 features by performing a syntactic parse using the Microsoft NLPWin natural language processing system[REF_CITE]and language modeling tools.
The features extracted fall into two broad categories: (i) Perplexity measures were extracted using the CMU-Cambridge Statistical Language Modeling Toolkit[REF_CITE].
"We calculated two sets of values: lexicalized trigram perplexity, with values discretized into deciles and part of speech (POS) trigram perplexity."
"For the latter we used the following sixteen POS tags: adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (ii) Linguistic features fell into several subcategories: branching properties of the parse; function word density, constituent length, and other miscellaneous features"
We employed a selection of features to provide a detailed assessment of the branching properties of the parse tree.
The linguistic motivation behind this was twofold.
"First, it had become apparent from failure analysis that MT system output tended to favor right-branching structures over noun compounding."
"Second, we hypothesized that translation from languages whose branching properties are radically different from English (e.g. Japanese, or a verb-second language like German) might pollute the English output with non-English characteristics."
"For this reason, assessment of branching properties is a good candidate for a language-pair independent measure."
The branching features we employed are given below.
"Indices are scalar counts; other measures are normalized for sentence length. ¾ number of right-branching nodes across all constituent types ¾ number of right-branching nodes for NPs only ¾ number of left-branching nodes across all constituent types ¾ number of left-branching nodes for NPs only ¾ number of premodifiers across all constituent types ¾ number of premodifiers within NPs only ¾ number of postmodifiers across all constituent types ¾ number of postmodifiers within NPs only ¾ branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes ¾ branching index for NPs only ¾ branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories ¾ branching weight index for NPs only ¾ modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories ¾ modification index for NPs only ¾ modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories ¾ modification weight index for NPs only ¾ coordination balance, i.e. the maximal length difference in coordinated constituents"
"We considered the density of function words, i.e. the ratio of function words to content words, because of observed problems in WinMT output."
Pronouns received special attention because of frequent problems detected in failure analysis.
"The density features are: ¾ overall function word density ¾ density of determiners/quantifiers ¾ density of pronouns ¾ density of prepositions ¾ density of punctuation marks, specifically commas and semicolons ¾ density of auxiliary verbs ¾ density of conjunctions ¾ density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns"
We also measured the following constituent sizes: ¾ maximal and average NP length ¾ maximal and average AJP length ¾ maximal and average PP length ¾ maximal and average AVP length ¾ sentence length
"On a lexical level, the presence of out of vocabulary (OOV) words is frequently caused by the direct transfer of source language words for which no translation could be found."
"The top-level syntactic template, i.e. the labels of the immediate children of the root node of a sentence, was also used, as was subject-verb disagreement."
"The final five features are: ¾ number of OOV words ¾ the presence of a word containing a non- English letter, i.e. an extended ASCII character."
"This is a special case of the OOV problem. ¾ label of the root node of the sentence (declarative, imperative, question, NP, or &quot;FITTED&quot; for non-spanning parses) ¾ sentence template, i.e. the labels of the immediate children of the root node. ¾ subject-verb disagreement"
We used a set of automated tools to construct decision trees[REF_CITE]based on the features extracted from the reference and MT sentences.
"To avoid overfitting, we specified that nodes in the decision tree should not be split if they accounted for fewer than fifty cases."
In the discussion below we distinguish the perplexity features from the linguistic features.
"Table 1 gives the accuracy of the decision trees, when trained on all 180,000 training sentences and evaluated against the 20,000 held-out test sentences."
"Since the training data and test data contain an even split between reference human translations and machine translations, the baseline for comparison is 50.00%."
"As Table 1 shows, the decision trees dramatically outperform this baseline."
Using only perplexity features or only linguistic features yields accuracy substantially above this baseline.
"Combining the two sets of features yields the highest accuracy, 82.89%."
"Notably, most of the annotated features were selected by the decision tree tools."
Two features were found not to be predictive.
"The first non-selected feature is the presence of a word containing an extended ASCII character, suggesting that general OOV features were sufficient and subsume the effect of this narrower feature."
"Secondly, subject-verb disagreement was also not predictive, validating the consistent enforcement of agreement constraints in the natural language generation component of the MT system."
"In addition, only eight of approximately 5,200 observed sentence templates turned out to be discriminatory."
"For a different use of perplexity in classification, see[REF_CITE]who compare the perplexity of a sentence using a language model built solely from reference translations to the perplexity using a language model built solely from machine translations."
The output of such a classifier could be used as an input feature in building decision trees.
"For our experiments, we had access to several hundred thousand sentences from the target domain."
"To measure the effect of reducing the size of the training data set on the accuracy of the classifier, we built classifiers using samples of the training data and evaluating against the same held-out sample of 20,000 sentences."
"We randomly extracted ten samples containing the following numbers of sentences: {1,000, 2,000, 3,000, 4,000, 5,000, 6,000, 12,000, 25,000, 50,000, 100,000, 150,000}."
Figure 1 shows the effect of varying the size of the training data.
"The data point graphed is the average accuracy over the ten samples at a given sample size, with error bars showing the range from the least accurate decision tree at that sample size to the most accurate."
"As Figure 1 shows, the models built using only perplexity features do not benefit from additional training data."
"The models built using linguistic features, however, benefit substantially, with accuracy leveling off after 150,000 training cases."
"With only 2,000 training cases, the classifiers built using all features range in accuracy from 75.06% to 78.84%, substantially above the baseline accuracy of 50%."
"As the results in section 4 show, it is possible to build classifiers that can distinguish human reference translations from the output of a machine translation system with high accuracy."
"We thus have an automatic mechanism that can perform the task that humans appear to do with ease, as noted in section 1."
"The best result, a classifier with 82.89% accuracy, is achieved by combining perplexity calculations with a set of finer-grained linguistic features."
"Even with as few as 2,000 training cases, accuracy exceeded 75%."
In the discussion below we consider the advantages and possible uses of this automatic evaluation methodology.
"Once an appropriate set of features has been selected and tools to automatically extract those features are in place, classifiers can be built and evaluated quickly."
This overcomes the two problems associated with traditional manual evaluation of MT systems: manual evaluation is both costly and time-consuming.
"Indeed, an automated approach is essential when dealing with an MT system that is under constant development in a collaborative research environment."
"The output of such a system may change from day to day, requiring frequent feedback to monitor progress."
The methodology does not crucially rely on any particular set of features.
"As an MT system matures, more and more subtle cues might be necessary to distinguish between human and machine translations."
Any linguistic feature that can be reliably extracted can be proposed as a candidate feature to the decision tree tools.
The methodology is also not sensitive to the domain of the training texts.
All that is needed to build classifiers for a new domain is a sufficient quantity of aligned translations.
"The classifiers can be used for evaluating a system overall, providing feedback to aid in system development, and in evaluating individual sentences."
Evaluating the accuracy of the classifier against held-out data is equivalent to evaluating the fluency of the MT system.
"As the MT system improves, its output will become more like the human reference translations."
"To measure improvement over time, we would hold the set of features constant and build and evaluate new classifiers using the human reference translations and the output of the MT system at a given point in time."
"Using the same set of features, we expect the accuracy of the classifiers to go down over time as the MT output becomes more like human translations."
Our primary interest in evaluating an MT system is to identify areas that require improvement.
This has been the motivation for using linguistic features in addition to perplexity measures.
"From the point of view of system development, perplexity is a rather opaque measure."
This can be viewed as both a strength and a weakness.
"On the one hand, it is difficult to tune a system with the express goal of causing perplexity to improve, rendering perplexity a particularly good objective measurement."
"On the other hand, given a poor perplexity score, it is not clear how to improve a system without additional failure analysis."
"We used the DNETVIEWER tool[REF_CITE], a visualization tool for viewing decision trees and Bayesian networks, to explore the decision trees and identify problem areas in our MT system."
"In one visualization, shown in Figure 2, DNETVIEWER allows the user to adjust a slider to see the order in which the features were selected during the heuristic search that guides the construction of decision trees."
"The most discriminatory features are those which cause the MT translations to look most awful, or are characteristics of the reference translations that ought to be emulated by the MT system."
"For the coarse model shown in Figure 2, the distance between pronouns (nPronDist) is the highest predictor, followed by the number of second person pronouns (n2ndPersPron), the number of function words (nFunctionWords), and the distance between prepositions (nPrepDist)."
"Using DNETVIEWER we are able to explore the decision tree, as shown in Figure 3."
"Viewing the leaf nodes in the decision tree, we see a probability distribution over the possible states of the target variable."
"In the case of the binary classifier here, this is the probability that a sentence will be a reference translation."
"In Figure 3, the topmost leaf node shows that p(Human translation) is low."
We modified DNETVIEWER so that double-clicking on the leaf node would display reference translations and MT sentences from the training data.
"We display a window showing the path through the decision tree, the probability that the sentence is a reference translation given that path, and the sentences from the training data identified by the features on the path."
This visualization allows the researcher to view manageable groups of similar problem sentences with a view to identifying classes of problems within the groups.
A goal for future research is to select additional linguistic features that will allow us to pinpoint problem areas in the MT system and thereby further automate failure analysis.
Decision trees are merely one form of classifier that could be used for the automated evaluation of an MT system.
"In preliminary experiments, the accuracy of classifiers using support vector machines (SVMs)[REF_CITE]exceeded the accuracy of the decision tree classifiers by a little less than one percentage point using a linear kernel function, and by a slightly greater margin using a polynomial kernel function of degree three."
We prefer the decision tree classifiers because they allow a researcher to explore the classification system and focus on problem areas and sentences.
We find this method for exploring the data more intuitive than attempting to visualize the location of sentences in the high-dimensional space of the corresponding SVM.
"In addition to system evaluation and failure analysis, classifiers could be used on a per-sentence basis to guide the output of an MT system by selecting among multiple candidate strings."
"If no candidate is judged sufficiently similar to a human reference translation, the sentence could be flagged for human post-editing."
"We have presented a method for evaluating the fluency of MT, using classifiers based on linguistic features to emulate the human ability to distinguish MT from human translation."
The techniques we have described are system- and language-independent.
"Possible applications of our approach include system evaluation, failure analysis to guide system development, and selection among alternative possible outputs."
We have focused on structural aspects of a text that can be used to evaluate fluency.
A full evaluation of MT quality would of course need to include measurements of idiomaticity and techniques to verify that the semantic and pragmatic content of the source language had been successfully transferred to the target language.
Polarized dependency (PD-) grammars are proposed as a means of efficient treatment of discontinuous construc-tions.
"PD-grammars describe two kinds of dependencies : local, explicitly de-rived by the rules, and long, implicitly specified by negative and positive va-lencies of words."
"If in a PD-grammar the number of non-saturated valencies in derived structures is bounded by a constant, then it is weakly equivalent  -to a cf-grammar and has a time parsing algorithm."
"It happens that such bounded PD-grammars are strong enough to express such phenomena as unbounded raising, extraction and ex-traposition."
Syntactic theories based on the concept of depen-dency have a long tradition.
"Tesnière[REF_CITE]was the first who systematically described the sentence structure in terms of binary relations between words (dependencies), which form a de-pendency tree (D-tree for short)."
D-tree itself does not presume a linear order on words.
"How-ever, any its surface realization projects some lin-ear order relation (called also precedence)."
Some properties of surface syntactic structure can be ex-pressed only in terms of both dependency (or its transitive closure called dominance) and prece-dence.
"One of such properties, projectivity, re-quires that any word occurring between a word and a word dependent on be dominated by In first dependency grammars[REF_CITE]and in some more recent proposals: link gram-mars[REF_CITE], projective dependency grammars[REF_CITE]the projectivity is implied by definition."
"In some other theories, e.g. in word grammar[REF_CITE], it is used as one of the axioms defin-ing acceptable surface structures."
"In presence of this property, D-trees are in a sense equiva-lent to phrase structures with head selection [Footnote_1] ."
1 See[REF_CITE]for more details.
"It is for this reason that D-trees determined by grammars of Robins[REF_CITE], cate-gorial grammars[REF_CITE], classi-cal Lambek calculus[REF_CITE], and some other formalisms are projective."
"Projectivity af-fects the complexity of parsing : as a rule, it al-lows dynamic programming technics which  lead-timeto polynomial time algorithms (cf. algorithm for link grammars[REF_CITE])."
"Meanwhile, the projectivity is not the norm in natural languages."
"For example, in most European languages there are such regu-lar non-projective constructions as WH- or rel-ative clause extraction, topicalization, compara-tive constructions, and some constructions spe-cific to a language, e.g. French pronominal cli-tics or left dislocation."
"In terms of phrase struc-ture, non-projectivity corresponds to discontinu-ity."
In this form it is in the center of dis-cussions till 70-ies.
There are various depen-dency based approaches to this problem.
"In the framework of Meaning-Text Theory (Mel’čuk and[REF_CITE]), dependencies between (some- times non adjacent) words are determined in terms of their local neighborhood, which leads to non-tractable parsing (the NP-hardness argu-ment[REF_CITE]applies to them)."
"More recent versions of dependency gram-mars (see e.g.[REF_CITE]) impose on non-projective D-trees some constraints weaker than projectivity (cf. meta-projectivity[REF_CITE]or pseudo-projectivity[REF_CITE]), suffi-cient for existence of a polynomial time parsing algorithm."
"Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from deriva-tions (cf. e.g. a method[REF_CITE]for Lambek calculus)."
"In this context, non-projective D-trees are determined with the use of hypotheti-cal reasoning and of structural rules such as com-mutativity and associativity (see e.g.[REF_CITE])."
"In this paper, we put forward a novel ap-proach to handling discontinuity in terms of de-pendency structures."
"We propose a notion of a polarized dependency (PD-) grammar combining several ideas from cf-tree grammars, dependency grammars and resource-dependent logics."
"As most dependency grammars, the PD-grammars are analyzing."
They reduce continuous groups to their types using local (context-free) reduction rules and simultaneously assign partial depen-dency structures to reduced groups.
The valencies (positive for governors and negative for subordi-nates) are used to specify discontinuous (long) de-pendencies lacking in partial dependency struc-tures.
The mechanism of establishing long de-pendencies is orthogonal to reduction and is im-plemented by a universal and simple rule of va-lencies saturation.
A simplified version of PD-grammars adapted for the theoretical analysis is introduced and explored[REF_CITE].
"In this paper, we describe a notion of PD-grammar more adapted for practical tasks."
"We fix finite alphabets of terminals (words), of nonterminals (syntactic types or classes), and of dependency names."
Let  set  ! #%&quot; $ of trees  be a string.
"A (called components ) which cover exactly &amp; have no nodes in of common, and whose arcs are labeled by names in is a dependency (D-) structure on if one component (&apos; of is selected as its head 2 ."
We use the notation )+* is a terminal D-structure if is a string of terminals.
"When has only one component, it is a dependency (D-) tree on"
"For example, the .-0 D-structure / in Fig. 1 has two components. , is the root of the non .%/ (- &amp; projective head component, the other component is a unit tree. []_\ ^"
D5` NPO5Q ; ?&lt; A&gt; @ CB&gt; D5E!:W BNPO5QRF ;9 : @ &gt;CB&gt; V@ @
WGIH7J9 :=WX O5Y.Z
"In distinction[REF_CITE], the non-terminals (and even dependency names) can be structured."
"We follow (Mel’čuk  and[REF_CITE]) and distinguish syntactical a and mor-phological aedgfih features of a nonterminal a The alphabets being finite, the features unification is a means of compacting a grammar."
The D-structures we will use will be polarized in the sense that some words will have valencies specifying long dependencies which must enter or go from them.
"A valency 2 is an 2 expression of one of the forms 2 3kmlj , 2 jonpl (a positive va-lency 2 ), or rq  , qrnpl (a negative valency), being a dependency name."
"For example, 2 the intuitive sense of a positive valency 2 onplj of a node is that a long dependency might go from somewhere on the right."
All nonterminals will be signed: we presume that  is decomposed into two u classes : of positive (  ) and negative sxw ) nonterminals respectively.
"D-structures ( with valencies, DV-structures, are defined so that valencies saturation would imply connectivity."
"2 We visualize Py z underlining it or its root, when there are some other components. may repeat in 3 In the original L B 0FR&gt; definition {[REF_CITE], valenciesbut this seems to be a natural constraint."
"A terminal is polarized if  a &amp; finite list of pairwise different valencies 3 , (its valency &amp; list ) is assigned to it. is positive , if , does not contain negative valencies, A D-tree with polarized nodes is positive if its root is positive , otherwise it is negative . symbols is a DV-structure on &amp; if theof polarizedfollowing"
"A D-structure on a string conditions are satisfied : , &amp; (v1) if a terminal node of is negative, then  contains exactly one negative valency, (v2) if a dependency of enters a node then is positive, (v3) the non-head components of (if any) are all negative."
The polarity of a DV-structure is that of its head.
"In Fig. 2 [Footnote_4] , both words in  have no valencies, all nonterminals in  and  are positive (we label only negative nonterminals),  is positive because its head component is a positive unit D-tree, =® and  are negative because their roots are negative."
"4 For the reasons of space, in our examples we are not accurate with morphological features. in the place of G GrV J (gov:upon) we should rather have E.g., GrV(gov:upon) inf ."
Valencies are saturated by long dependencies. be a 2 terminal  Definition 3.
"A triplet  2   where  ® are nodes of and is a long dependency 2 to ® (nota- with the name directed from ), if there are valencies · tion  : µ, h ® ®, such ® 2 that : , and ·?d  precedes ® ), · jonpl (v4 2 ) either ("
"We will say that  saturates , ·® andby ?nel 2 , or 2 (v5) ®¦d . long depen-dency ² ."
"The set of valencies in is totally ordered by the order of nodes and the orders in their valency lists:  if   (o1)  either +,  +,  ® and (o2) or ,C» &amp; in ,C» &amp; Let  be the structureandresulting  from by adding C»,  the long  dependency C»,   ²$ and replacing  ® by ,  and ,C» by ,C»   ® ,C»  ®  $ is a saturation of by ² and denoteWe willit bysay  that  Among all possible saturations of we will select the following particular  one :"
"Let , positive valency in  and  be the first non saturated  ®C», be the closest corresponding 5 non saturated neg-ative valency in Then the long dependency  qºq¶µ h =® Ä saturating ·® by  is first available ² is first(FAavailable) in Theor resulting FA-saturation saturation(notationof : by  )."
We transform the relations  ÅvÆ into partial orders closing them by transitivity.
Y W YW W W É N N [ [ y WN : Y W YW W[ W  N [ y : W : Y W W YW W[ W   N N [ y
"Suppose that in Fig. 3, both occurrences - of - in , .±-ËÊ # 2 $# in  have and the first occurrence of and both occurrences of Í in , Í Ê  2 $ and the second occurrence of Í in  ® have Then Êo¿ ÅvÆ  ÅvÆ 5 Corresponding &gt;  GÎ&gt;  ª|Ï means: Ç : (c1) &gt;  GÎ&gt;  and ª  Ï ¢Î : if ªªP% ÏÏ ¡KÇ¡K¢Î _::_{Ð and (c2) and if"
"Ñ InIf (Dikovskyis a terminal, 2001)DV-structure, we prove thatand  then either  has a cycle, or it is a DV-structure (Lemma 1)."
"As it follows from Definition 3, each satura-tion of a terminal DV-structure has the same set of nodes and a strictly narrower set of valencies."
"Therefore, any terminal DV-structure has maxi-mal saturations with respect to the order relations  ÅvÆ"
"Very importantly, there is a single max-imal FA-saturation of denoted ÔpÕ"
"O E.g., in Fig. 3, ÖÔ Õ Ê ) is a D-tree."
In order to keep track of those valencies which are not yet saturated we use the following notion of integral valency.
Let be a terminal DV-structure. valency ×
"O is the list The Ø integral , ÛÝÜ  s » u &amp; of ordered ÅvÆ by the order of va- ¼Ù©IÚ&quot; s » u &quot; If ÔÖÕ lencies in is a d-tree, we say that this D-tree saturates saturable .and call"
"By this definition, × ÔÖÕ"
Saturability is easily vÅ
Æ expressed in Åv× Æ terms of integral valency (Lemma 2[REF_CITE]) :
Let Ñ ÔÖÕ be a terminalis a DV-structureD-tree iff it is.
Thencycle-free: and ÅvÆ× Ñ  has at most one saturating D-tree.
The semantics of PD-grammars will be defined in terms of composition of DV-structures which generalizes strings substitution.
"Let be a DV-node of one of its components, and # DV-structure of the same polarity as á and bewitha the head component #ã&apos; Then the result of the of  into  in á is the DV-structure composition ½ ®= á in which  is substituted for  the root of (ã&apos; inherits all dependencies of á in  and the head component is that of  (changed respectively if touched on by composition) 6 ."
It is easy to see that DV-structure in Fig. 4 can be derived by the following series of compo- TAGs[REF_CITE]( needs not be a leaf) and is not 6 This composition generalizes  the substitution used in like the adjunction.
The DV-structures composition has natural properties Ñ
"The :result of a composition into a DV-structure is a DV-structure of the same polarity 2001)). ½ as Ñ If(Lemma × 3 in × (Dikovsky ® then, ÅvÆ× Ê å á ÔÖÕ æ (Lemma ÅvÆ× ÅvÆ å á ½ ÔÖÕ ÅvÆ ® æ for any terminal ! ® 4[REF_CITE])."
"Polarized dependency grammars determine DV-structures in the bottom-up manner in the course of reduction of phrases to their types, just as the categorial grammars do."
Each reduction step is accompanied by DV-structures composition and by subsequent FA-saturation.
The yield of a suc-cessful reduction is a D-tree.
"In this paper, we describe a superclass of grammars[REF_CITE]which are more realistic from the point of view of real applications and have the same pars- 1 ing complexity."
Definition  ! 6. !
"A n PD-grammar where  öT¦ is a system are as de- scribed above, ÷gù) are positive nonterminals),  is a set of axioms  (whichis a ternary relation of lexical interpretation , k being the set of lists of pairwise different valencies, and n is a set of reduction rules ."
"For simplicity, we start with the strict reduction rules (the only rules[REF_CITE]) of the form where  and is a DV-structure over of the same polarity as A (below we will extend the strict rules by side effects)."
"In the special case, where the DV-structures in the rules are D-trees, the PD-grammar is local [Footnote_7] ."
7 Local PD-grammars are strongly equivalent to depen-dency tree grammars[REF_CITE]which are generating and not analyzing as here.
"Intuitively, we can think of ø as of the com-bined information available after the phase of morphological analysis (i.e. dictionary informa-tion and tags)."
So *!  means that a type á and a valency list ·0² can be a priori as-signed to the word * 2 be the unit DV-structure * *!² and Ê
"Semantics. 1. Let with , * 2 is a reduction of the structure  to its typeThen á (notation Ê µá ) and ·0² is the integral Ê valency by  × of this reduction 2 denoted  á be a reduction µ rule 2."
Let with nonterminals’  occurrences  !? á à in  Çh   and   áÈ 0à( à 2#½  3á¦à be some reductions.
Then is a reduction ½ of the structure  to its type á (notation Ê   Aá !á¦à 0à ). itself are subreductions × of  The as well as integral ½ valency of ½ Ê × via is determined ÅvÆ×    ! á¦à if there is ÅvÆ a reduction Ê 1 A D-tree is by ;Õ  1 The ç1 DT-language determined by 1È is the * of all D-trees  $ set it determines. k 1  .4 1ç is the language determined by denotes the class of languages determined by PD-grammars.
"By way of 1 illustration, let us consider the PD-grammar Ê with the lexical interpretation ø containing 1o2  triplets %é ïU·7lè 2( /é%ï  2 jokgl©éf (- ìb 21o2èé±ï, b f¶ïU æ : * ì 1o2.432, / èP²é±ï * îC-0í 25ì á¦1o2Rð#* qrn æ l©é èé±ï 0è #æ=ì  and the following reduction 2 left 132 parts &amp; are shown in Fig. 2: rules whose  ü 2   &amp;î / / é±ï &amp;&amp; w 2  132 ® ü = 2 ñ  %î é±ï Then î the D-tree in Fig. 4 is reducible in 1 Ê to  and its reduction is depicted in Fig. 5."
"As we show[REF_CITE], the weak generative capacity of PD-grammars is stronger than that of 1 cf-grammars."
"For  example, the PD- in Fig. 3 is deter-mined by Its reduction combined with the diagram of local and long dependencies is presented in Fig. 6."
"The local PD-grammars are weakly equivalent to cf-grammars, so they are weaker than general PD-grammars."
"Meanwhile, what is really im-portant concerning the dependency grammars, is their strong generative capacity, i.e. the D-trees mars likethey derive 1 ."
"From this point of view, the gram-above are too strong."
"Let us remark that in the reduction in Fig. 6, the first saturation becomes possible only after all positive valencies emerge."
"This means that the integral valency of subreductions increases with This seems to be never the case in natural languages, where next valencies arise only after the preceding ones are saturated."
This is why we restrict ourself to the class of PD-grammars which have such a prop- 1 erty.
Let be a PD-grammar.
"For a reduction of a terminal -  ×  structure, its defect is duction of $ 1 Óf 0ã is a subre- defined as fect if there is some (there is no) constant whichhas bounded ( unbounded 8 ) de-bounds the defect 8 of all its reductions."
The mini-mal constant 1 having this  property (if any) is the). defect of (denoted
There is a certain technical problem concerning PD-grammars.
"Even if in a reduction to an axiom all valencies are saturated, this does not guaran-tee that a D-tree is derived: the graph may have cycles."
In[REF_CITE]we give a sufficient condition for a PD-grammar of never producing cycles while FA-saturation.
We call the grammars satisfying this condition lc- (locally cycle-) free.
"For the space reasons, we don’t cite its defini-tion, the more so that the linguistic PD-grammars should certainly be lc-free."
In[REF_CITE]we prove the following theorem.
For any lc-free PD-grammar 1 of bounded defect there is an equivalent cf-grammar.
Together with this we show an example of a DT-language which cannot be determined by lo-cal PD-grammars.
This means that not all struc-tures determined in terms of long dependencies can be determined without them.
An important consequence of Theorem 1 is that  lc-free bounded defect PD-grammars have aparsing algorithm.
"In fact, it is the clas-sical Earley algorithm in charter form (the char-ters being DV-structures)."
"To apply this algo-rithm in practice, we should analyze the asymp-totic factor which depends on the size of the grammar."
"The idea of theorem 1 is that the in-tegral valency being bounded, it can be com-piled into ü types á . shouldThis ½ means that a reduction rule ½ be substituted by rules    , !á¦à  , æ ü á´å , =Ê æ with types keeping all possible integral valencies not causing à cycles u times. theTheoreticallysize of a ,grammarthis mightwithblowde-up · 8 s &quot;! · fect valencies and the maximal length stant factor in theof left parts of rules  ."
"So theoretically, the con-time bound is great."
"In practice, it shouldn’t 8 be as awful, because in lin-will certainly equal  # one guistic grammars rule will mostly treat one valency (i.e.  $# ) and the majority of rules will be local."
"Practi-cally, the effect may be that some local rules will have variants ½ propagating á´å(· æ upwards a certain va-lency:   % å % (· æ æ­ü"
The actual prob-lem lies elsewhere 1 .
Let us analyze the illustration grammar Ê and the reduction in Fig. 5.
This reduction is successful 2 due b to the fact that the negative valency / rnq l©é   is assigned to the preposition é±ï 2 and b the corresponding pos-itive valency 2 ì jokgl©é èé±ï  is assigned to the verb
What might serve the formal 2 basis ì for these assignments?
Let us start with èP²
"This verb ï / %é ï has the strong government over prepositions In the clause in Fig. 4, the group of the preposition is moved, which is of course a sufficient condition for assigning the positive va- lency to the verb."
"But this condition is not avail-able in the dictionary 2 ì , nor even through morpho-logical analysis (  may occur at a certain dis-tance from the end of the clause)."
"So it can only be derived in the course of reduction, but strict PD-grammars have no rules assigning valencies."
"Theoretically, there is no problem: we should just introduce into the dictionary both variants of the 2 verb b description – with the local dependency é  qÈ 2 to the b right and with the positive va-lency j3kgl ©é %ï  to the left."
"Practically, this “solution” is inacceptable because such a lex-ical ambiguity will lead to a brute force search."
"The same argument shows that we 2 shouldn’t b as-sign the negative valency q?n l©é è é±ï qÈïÍ]ð to / é±ï in the dictionary, but rather “calculate” it in the reduction."
"If we compare the clause in Fig. 4 with the clauses what theories we may rely upon; what kind of theories we may rely upon; the de-pendency theories of what kind we may rely upon etc., we see that we can assign a qrn valency to wh-words in the dictionary and then raise nega-tive valencies till the FA-saturation."
The problem is that in the strict PD-grammars there are no rules of valency raising.
For these reasons we extend the reduction rules by side effects sufficient for the calculations of both kinds.
We introduce two &apos; ] &amp; kinds ( µ · ® of side effects: valency raising  &apos; ] &amp; *) å· æ· ·  beingand valency assignment &amp; valency names and an integer.
"A rule of the å · æ &apos;&amp; ] ( µ · ® ü á form with &amp; + nonterminalsis valency raising  !? á à in and #,+ if : (r1)  are of the same 2 polarity, (r2) a local dependency enters ?áá"
"Ù in , (r3) for positive  is a strict (r4) if ·  ® are negative, then á Ù !   u reduction rule, and replacing á?Ù by any positive nonterminal we obtain a DV-structure  &apos; ] &amp; .) ."
"A  á [Footnote_8] rule of the form with &amp; + nonterminalsis valency assigning  !? á à in and # +, reduction rule, (a2) if · is vu negative and áru Ù is the root of  and  sw (a3) if · s  then á?;Ù and á?Ù is not the root of K then á is negative u  Ù   head component of 9 and replacing ? á is Ù bya anynon negative nonterminal we obtain a DV-structure."
8 So this occurrence of  0/ in  contradicts to the point (v2) of definition 2.
"We change the reduction semantics å  &apos;  &amp; ( µ ·® ü as Ñ followsFor a. raising rule  result of 21  the !0 3 å áÈ! %árÙ reduction ½ is the 43 ½ DV-structure  ! the á¦à ½   Õ is the DV-structure  ï5ï |í   where resulting  1 from  by deleting · from , and · ! is the  DV-structure ï5ï |í   resulting from  by Ñ adding ) · · to ,  &apos; ] &amp;"
For a valency assignment rule ü  the result of  ½  the reduc-tion ½ 51  isthe · !
Ù DV-structure #á?à ½  )ÔÖÕá?Ù
A PD-grammar with side effect rules is a PDSE-grammar.
This definition is correct in the sense that the result of a reduction with side effects is always a 1
We can prove
For any lc-free PDSE-grammar of bounded defect there is an equivalent cf-grammar. the bounded defect PDSE-
"Moreover, grammars are also parsed in time In fact, we can drop negative  in raising rules í| (it is unique) and indicate the type of 2 ï5ï in both side effect rules, because the composition we use makes this information 1 Ê local."
"Now, we can revise the grammar above / ±é ï , e . . 432 g.   qrn l©é èé±ï #æ excluding the 2 dictionary b assignmentand using in its place several valency D5TED: |\ X raising rules such as:"
B @ TEcD: #F B yy Z=W{ }&amp;: ^F ;9 :T&lt;¦W &gt; 76 B F 98 9 :|  B O|D \ FRÐ0F&gt;  ; where  B :&lt; xB ; &gt;  = ;T: &lt;¦0FcF&gt; @?BADC&gt;G ?
BEDF H AICJ? -R:prepos-obj
The main ideas underlying our approach to dis-continuity are the following  K/  : 9 So this occurrence of in contradicts to the point (v3) of definition 2.
"Ñ Continuous (local, even if non projective) dependencies are treated in terms of trees com-position (which reminds TAGs)."
"E.g., the French pronominal Ñ Discontinuousclitics can(longbe )treateddependenciesin this wayare. cap-tured in terms of FA-saturation of valencies in the course of bottom-up reduction of dependency groups to their types."
"As compared with the SLASH of GPSG or the regular expression lifting control in non projective dependency grammars, these means turn out to be more efficient under the conjecture of bounded defect."
This conjec-ture seems to be true for natural languages (the contrary would mean the possibility of unlimited extraction Ñ
The valencyfrom extractedraising andgroupsassignment). rules of-fer a way of deriving a proper valency saturation without unwarranted increase of lexical ambigu-ity.
A theoretical analysis and experiments in En-glish syntax description show that the proposed grammars may serve for practical tasks and can be implemented by an efficient parser.
I would like to express my heartfelt gratitude to N. Pertsov for fruitful discussions of this paper.
The idea of valency raising has emerged from our joint work over a project of a PD-grammar for a fragment of English.
"Current alternatives for language mod-eling are statistical techniques based on large amounts of training data, and hand-crafted context-free or finite-state grammars that are difficult to build and maintain."
One way to address the problems of the grammar-based ap-proach is to compile recognition gram-mars from grammars written in a more expressive formalism.
"While theoreti-cally straight-forward, the compilation process can exceed memory and time bounds, and might not always result in accurate and efficient speech recogni-tion."
We will describe and evaluate two approaches to this compilation prob-lem.
We will also describe and evalu-ate additional techniques to reduce the structural ambiguity of the language model.
Language models to constrain speech recogni-tion are a crucial component of interactive spo-ken language systems.
"The more varied the lan-guage that must be recognized, the more critical good language modeling becomes."
Research in language modeling has heavily favored statisti-cal approaches ([REF_CITE]) while hand-coded finite-state or context-free language models dominate the commercial sector ([REF_CITE]
The difference re-volves around the availability of data.
"Research systems can achieve impressive performance us-ing statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available."
"Data may be unavailable because the domain has not been explored before, the relevant data may be con-fidential, or the system may be designed to do new functions for which there is no human-human analog interaction."
The statistical approach is un-workable in such cases for both the commercial developers and for some research systems ([REF_CITE]).
"Even in cases for which there is no impediment to col-lecting data, the expense and time required to col-lect a corpus can be prohibitive."
The existence of the ATIS database[REF_CITE]is no doubt a factor in the popularity of the travel do-main among the research community for exactly this reason.
"A major problem with grammar-based finite-state or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases."
One way to address this problem is to write the gram-mar in a more expressive formalism and gener-ate an approximation of this grammar in the for-mat needed by the recognizer.
"This approach has been used in several systems, CommandTalk[REF_CITE], RIALIST PSA simula-tor[REF_CITE], WITAS[REF_CITE], and SETHIVoice[REF_CITE]."
"While theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the po-tential for a combinatorial explosion that will ex-ceed memory and time bounds."
There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition.
We will be interested in this paper in sound ap-proximations[REF_CITE]in which the language accepted by the approximation is a superset of language accepted by the original grammar.
"While we conceed that alternative tech-niques that are not sound ([REF_CITE]([REF_CITE]) may still be useful for many purposes, we prefer sound approxima-tions because there is no chance that the correct hypothesis will be eliminated."
"Thus, further pro-cessing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution."
We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar.
We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model.
"Typed Unification Grammars (TUG), like HPSG[REF_CITE]and Gemini[REF_CITE]are a more expressive formalism in which to write formal grammars [Footnote_1] ."
"1 This paper specifically concerns grammars written in the Gemini formalism. However, the basic issues involved in compiling typed unification grammars to context-free gram-mars remain the same across formalisms."
"As opposed to atomic nonterminal symbols in a CFG, each non-terminal in a TUG is a complex feature structure[REF_CITE]where features with values can be attached."
"For example, the rule: s[] np:[num=N] vp:[num=N] can be considered a shorthand for 2 context free rules (assuming just two values for number): s np singular vp singular s np plural vp plural"
This expressiveness allows us to write grammars with a small number of rules (from dozens to a few hundred) that correspond to grammars with large numbers of CF rules.
Note that the approx-imation need not incorporate all of the features from the original grammar in order to provide a sound approximation.
"In particular, in order to de-rive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features."
We can use the technique of restricti[REF_CITE]to remove these features from our feature structures.
"Removing these features may give us a more permissive language model, but it will still be a sound approximation."
"The experimental results reported in this pa-per are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant (PSA)."
"We consider this grammar to be medium-sized, with 61 grammar rules and 424 lexical entries."
"While this may sound small, if the grammar were expanded by instantiating vari-ables in all legal permutations, it would contain over  context-free rules."
We will be studying the compilation process to convert typed unification grammars expressed in Gemini notation into language models for use with the Nuance speech recognizer[REF_CITE].
"We are using Nuance in part because it supports context-free language models, which is not yet industry standard. [Footnote_2] Figure 1 illustrates the stages of processing: a typed unification grammar is first compiled to a context-free grammar."
"2 The standard is moving in the direction of context-free language models, as can be seen in the draft standard for Speech Recognition Grammars being developed by the World Wide Web Consortium[REF_CITE]."
"This is in turn converted into a grammar in Nuance’s Grammar Specification Language (GSL), which is a form of context-free grammar in a BNF-like notation, with one rule defining each nonterminal, and allowing alternation and Kleene closure on the right-hand-side."
"Critically, the GSL must not contain any left-recursion, which must be elimi-nated before the GSL representation is produced."
The GSL representation is then compiled into a Nuance package with the nuance compiler.
This package is the input to the speech recognizer.
"In our experience, each of the compilation stages, as well as speech recognition itself, has the po-tential to lead to a combinatorial explosion that exceeds practical memory or time bounds."
"We will now describe implementations of the first stage, generating a context-free grammar from a typed unification grammar, by two differ-ent algorithms, one defined[REF_CITE]and one by Moore and Gawron, described[REF_CITE]"
The critical difficulty for both of these approaches is how to select the set of derived nonterminals that will appear in the final CFG.
"The algorithm of Kiefer&amp;Krieger (K&amp;K) divides this compilation step into two phases: first, the set of context-free nonterminals is determined by iterating a bottom-up search until a least fixed-point is reached; second, this least fixed-point is used to instantiate the set of context-free produc- tions."
"The computation of the fixed-point , de-scribed in Table 1, proceeds as follows."
"First, is constructed by finding the most-general set of feature structures that occur in the lexicon (lines 1-4)."
Each feature structure has the lexical restric- (line 3) tor L applied &quot; $ to it before being added to with a the operator.
This operator maintains the set of most-general feature structures.
"A new feature structure is added to the set only when it is not subsumed by any current members of the set, and any current members that are subsumed by the new member are removed as the new el-ement is added."
"The computation of proceeds with the call to Iterate (line 6), which adds new feature structures that can be derived bottom-up GFIHKJ ."
"Each call GF to Iterate generates a new set , in-cluding as its base aFIHKJ (line 8)."
"It then adds new feature structures to 6 by instantiating every grammar rule r in , the set of grammar rules."
"The first step in the instantiation is to unify every combination of daughters aF with all possible feature structures from (FillDaughters, line 10)."
"The rule restrictor is applied to each resulting aFIHKJ feature structure &quot; $ (line 11) before it is added to using the operator (line 12), similar to the lexical case."
"If after checking all rule applications bottom up FIHKJ , no new feature structures have been added to (line 13), then the least fixed-point had been found, and the process terminates."
"Otherwise, It- erate is called recursively."
"Having computed the least fixed-point , the next step is to compute the set 6 of corresponding CF J productions."
"For each r in , of J the form f f @@:fhg , instantiate the daughters f @@\g using all combinations of unifiable feature structures J from ."
"Context-free productions fj f @@ g will be added, where f%j and j k`lam/ . [Footnote_3]"
"3 There is a minor bug in K&amp;K where they state that the result t will always be in u and ?t vwt\x[yAyAyzt|{ will be a CF production in the approximation, but this may not be true if t was removed from u by G} ~ . Instead, the subsuming nonterminal t should be the new mother."
"While K&amp;K uses subsumption to generate the set of most-general nonterminals, the algorithm of Moore and Gawron (M&amp;G), described[REF_CITE]attempts to propagate features values both bottom-up and top-down through the grammar to generate a set of nonterminals that contains no variables."
"Also unlike K&amp;K, the production of the CF rules and associated nonterminals is inter-leaved."
"The process consists of a preprocessing stage to eliminate singleton variables, a bottom-up propagation stage, and a top-down propagation stage."
The preprocessing stage rewrites the grammar to eliminate singleton variables.
This step ef-fective replaces singleton variables with a new unique atomic symbol ’ANY’.
"The feature struc-ture for each lexical item and grammar rule is rewritten such that singleton variables are uni-fied with a special value ’ANY’, and every non-singleton variable expression is embedded in a val() term."
"After this transformation, singleton variables will not unify with non-singleton vari-able expressions, only with other singletons."
Ad-ditional rules are then introduced to deal with the singleton variable cases.
"For each daughter in a grammar rule in which a singleton variable ap-pears, new lexical items and grammar rules are introduced which unify with that daughter in the original grammar."
"As an example, consider the grammar fragment: vp:[num=N] v:[num=N] np: [] np:[num=N] det:[num=N] n:[num=N] np:[num=pl] n:[num=pl]"
"Here, the np object of vp is underspecified for num (as English does not generally require num-ber agreement between the verb and its object), so it will be a singleton variable."
"So, the following rules will be generated: vp:[num=val(N)] v:[num=val(N)] np:[num=’ANY’] np:[num=val(N)] det:[num=val(N)] n:[num=val(N)] np:[num=val(pl)] n:[num=val(pl)] np:[num=’ANY’] det:[num=val(N)] n:[num=val(N)] np:[num=’ANY’] n:[num=val(pl)]"
"After preprocessing, any variables remaining in the bodies of grammar rules will be shared variables."
"Singleton variable elimination by it-self is very effective at shrinking the size of the CF grammar space, reducing the size of @M the rule space for @M J1 the  PSA grammar from  rules to  rules."
"The bottom-up stage starts from this grammar, and derives a new grammar by propagating fea-ture values up from the lexicon."
"The process acts like a chart parser, except that indicies are not kept."
"When a rule transitions from an active edge to an inactive edge, a new rule with those feature instantiations is recorded."
"As a side-effect of this compilation,  -productions are eliminated."
"Top-down processing fires last, and performs a recursive-descent walk of the grammar starting atthe start symbol  , generating a new grammar that propagates features downward through the grammar."
A side-effect of this computation is that useless-productions (rules not reachable from  ) are removed.
It might still be possible that after top-down propagation there would still be vari-ables present in the grammar.
"For example, if the grammar allows sentences like “the deer walked”, which are ambiguous for number, then there will be a rule in the grammar that contains a shared variable for the number feature."
"To address this, as top-down propagation is progressing, all re-maining variables are identified and unified with a special value ’ALL’."
"Since each nonterminal is now ground, it is trivial to assign each nontermi-nal a unique atomic symbol, and rewrite the gram-mar as a CFG."
Table 2 contains a summary of some key statistics generated using both techniques.
The recognition results were obtained on a test set of 250 utter-ances.
"Recognition accuracy is measured in word error rate, and recognition speed is measured in multiples of real time (RT), the length of the ut-terance compared with the length of the CPU time required for the recognition result [Footnote_4] ."
"4 All timing results presented in this paper were executed on a[REF_CITE]workstation, running at 330MHz with 1.5 GB physical memory and an additional 1GB swap."
"The size of the resulting language model is measured in terms of the number of nonterminals in the grammar, and the size of the Nuance node array, a binary representation of the recursive transition network it uses to search the grammar."
Ambiguity counts the average number of parses per sentence that were allowed by the CF grammar.
"As can be read-ily seen, the compilation time for the K&amp;K algo-rithm is dramatically lower than the M&amp;G algo-rithm, while producing a similarly lower recog-nition performance, measured in both word error rate and recognition speed."
"Given that the two techniques generate gram-mars of roughly similar sizes, the difference in performance &quot; $ is striking."
We believe that the use of the in K&amp;K is partially responsible.
"Consider a grammar that contains a lexical item like “deer” that is underspecified for number, and will contain a singleton variable."
"This will lead to a nontermi-nal feature structure for noun phrase that is also underspecified for number, which will be more general than any noun phrase feature &quot; $ structures that are marked for number."
"The operator will remove those noun phrases as being less general, effectively removing the number agreement con-straint between subject and verb %&quot; from $ the context-free approximation."
The use of allows a single grammar rule or lexical item to have non-local ef-fects on the approximation.
"As seen in Table 2, the grammar derived from the K&amp;K algorithm is much more ambiguous than the grammar derived the M&amp;G algorithm, and, as is further elaborated in Section 4, we believe that the amount of am-biguity can be a significant factor in recognition performance."
"On the other hand, attention must be paid to the amount of time and memory required by the Moore algorithm."
"On a medium-sized grammar, this compilation step took over 3 hours, and was close to exceeding the memory capacity of our computer, with a process size of over 1GB."
The approximation is only valuable if we can succeed in computing it.
"Finally, it should also be noted that M&amp;G’s algorithm removes  -productions and useless-productions, while we had to add a sepa-rate postprocessing stage to K&amp;K’s algorithm to get comparable results."
For future work we plan to explore possible in-tegrations of these two algorithms.
One possi-bility is to include the singleton-elimination pro-cess as an early stage in the K&amp;K algorithm.
"This is a relatively fast step, but may lead to a significant increase in the size of the grammar."
"Another possibility is to embed a variant of the K&amp;K algorithm, and its clean separation of gen-erating nonterminals from generating CF produc-tions, in place of the bottom-up processing stage in M&amp;G’s algorithm."
It has been observed[REF_CITE]that a potential difficulty with using linguistically-motivated grammars as language models is that ambiguity in the grammar will lead to multiple paths in the language model for the same recog-nition hypothesis.
"In a standard beam-search ar-chitecture, depending on the level of ambiguity, this may tend to fill the beam with multiple hy-potheses for the same word sequence, and force other good hypotheses out of the beam, poten- tially increasing word error rate."
This observation appears to be supported in practice.
"The original form of the PSA grammar allows an average of 1.4 parses per sentence, and while both the K&amp;K and M&amp;G algorigthm increase the level of ambi-guity, the K&amp;K algorithm increases much more dramatically."
We are investigating techniques to transform a CFG into one weakly equivalent but with less am-biguity.
While it is not possible in general to re-move all ambiguity[REF_CITE]we hope that reducing the amount of ambiguity in the resulting grammar will result in improved recognition performance.
The first technique is actually a combination of three related transformations:
"Duplicate Nonterminal Elimination – If two nonterminals A and B have exactly the same set of productions   J @@  g   J @@  g then remove the productions for B, and rewrite B as A everywhere it occurs in the grammar."
"Unit Rule Elimination – If there is only one production for a nonterminal A, and it has a single daughter on its right-hand side   r8    [ then remove the production for A, and rewrite A as  everywhere it occurs in the grammar."
Duplicate Production Elimination – If a non-terminal A has two productions that are iden-tical   8     J  @@  g 8  F = then remove the production for  F .
These transformations are applied repeatedly un-til they can no longer be applied.
"Each of these transformations may introduce opportunities for the others to apply, so the process needs to be order insensitive."
"This technique can be applied after the traditional reduction techniques of  -elimination, cycle-elimination, and left-recursion elimination, since they don’t introduce any new  -productions or any new left-recursion."
"Although these transformations seem rather specialized, they were surprisingly effective at re-ducing the size of the grammar."
"For the K&amp;K algorithm, the number of grammar rules was re-duced from 3,246 to 2,893, a reduction of 9.2%, and for the M&amp;G algorithm, the number of rules was reduced from 4,758 to 1,837, a reduction of 61%."
"While these transforms do reduce the size of the grammar, and modestly reduce the level of ambiguity from 1.96 to 1.92, they did not ini-tially appear to improve recognition performance."
"However, that was with the nuance parameter -node array optimization level set to the default value FULL."
"When set to the value MIN, the compacted grammar was approxi-mately 60% faster, and about 9% reduction in the word error rate, suggesting that the nuance compiler is performing a similar form of compaction during node array optimiza-tion."
Another technique to reduce ambiguity was moti-vated by a desire to reduce the amount of preposi-tional phrase attachment ambiguity in our gram-mar.
"This technique detects when a Kleene clo-sure will be introduced into the final form of the grammar, and takes advantage of this to remove ambiguity."
Consider this grammar fragment:
NP NP PP VP V NP PP
"The first rule tells us that an NP can be followed by an arbitrary number of PPs, and that the PP following the NP in the second rule will be am-biguous."
"In addition, any nonterminal that has an NP as its rightmost daughter can also be followed by an arbitrary number of PPs, so we can detect ambiguity following those nonterminals as well."
We define a predicate follows as:
A follows B iff
B B A or
B  C and A follows C
"Now, the follows relation can be used to reduce ambiguity by modifying other productions where a  B is  followed J @@  F by  FIHKJ an @ A @ :  g where  FIHKJ follows  F and  J   can  be J rewritten @@  F  FcH as @@  g"
There is an exactly analogous transformation involving immediate right-recursion and a simi-lar predicate preceeds.
"These transformation pro-duce almost the same language, but can modify it by possibly allowing constructions that were not allowed in the original grammar."
"In our case, the initial grammar fragment above would require that at least one PP be generated within the scope of the VP, but after the transformation that is no longer required."
"So, while these transformations are not exact, they are still sound aproximations, as the resulting language is a superset of the orig-inal language."
"Unfortunately, we have had mixed results with applying these transformations."
"In earlier ver-sions of our implementation, applying these trans-formations succeeded in improving the recogni-tion speed up to 20%, while having some modest improvements in word error rate."
"But, as we im-proved other aspects of the compilation process, notably the grammar compaction techniques and the left-recursion elimination technique, those improvements disappeared, and the transforma-tions actually made things worse."
"The problem appears to be that both transformations can in-troduce cycles, and the right-recursive case can introduce left-recursion even in cases where cy-cles are not introduced."
"When the introduced cy-cles and left-recursions are later removed, the size of the grammar is increased, which can lead to poorer recognition performance."
"In the earlier im-plementations, cycles were fortuitously avoided, probably due to the fact that there were more unique nonterminals overall."
"We expect that these transformations may be effective for some gram-mars, but not others."
We plan to continue to ex-plore refinements to these techiques to prevent them from applying in cases where cycles or left-recursion may be introduced.
"We have used two left-recursion elimination tech-niques, the traditional one based on Paull’s al-gorithm, as reported[REF_CITE], and one described[REF_CITE][Footnote_5] , based on a technique described[REF_CITE]."
"5 There is a minor bug in the description of Moore’s algo-rithm that occurs in his paper, that the set of “retained non-terminals” needs to be extended to include any nonterminals that occur either in the non-initial daughter of a left-recursive nonterminal, or in any daughter of a non-left-recursive non-terminal. Thanks to Robert Moore for providing the solution to this bug. This bug applies only to the description of his algorithm, not to the implementation on which the empirical results reported is based. Please see[REF_CITE]for more details."
Our experience concurs with Moore that the left-corner transform he describes produces a more compact left-recursion free grammar than that of Paull’s algorithm.
"For the K&amp;K approx-imation, we were unable to get any grammar to compile through to a working language model using Paull’s algorithm (the models built with Paull’s algorithm caused the recognizer to ex-ceed memory bounds), and only succeeded with Moore’s left-recursion elimination technique."
"We have presented descriptions of two algorithms for approximating typed unification grammars with context-free grammars, and evaluated their performance during speech recognition."
"Initial re-sults show that high levels of ambiguity coorelate with poor recognition performance, and that size of the resuling language model does not appear to directly coorelate with recognition performance."
"We have developed new techniques for further re-ducing the size and amount of ambiguity in these context-free grammars, but have so far met with mixed results."
"In a language generation system, a content planner embodies one or more “plans” that are usually hand–crafted, sometimes through manual analysis of target text."
"In this paper, we present a system that we developed to automati-cally learn elements of a plan and the ordering constraints among them."
"As training data, we use semantically an-notated transcripts of domain experts performing the task our system is de-signed to mimic."
"Given the large degree of variation in the spoken language of the transcripts, we developed a novel al-gorithm to find parallels between tran-scripts based on techniques used in computational genomics."
Our proposed methodology was evaluated two–fold: the learning and generalization capabil-ities were quantitatively evaluated us-ing cross validation obtaining a level of accuracy of 89%.
A qualitative evalua-tion is also provided.
"In a language generation system, a content plan-ner typically uses one or more “plans” to rep-resent the content to be included in the out-put and the ordering between content elements."
"Some researchers rely on generic planners (e.g.,[REF_CITE]) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g.,[REF_CITE]) or schemas (e.g.,[REF_CITE])."
"In all cases, constraints on application of rules (e.g., plan op-erators), which determine content and order, are usually hand-crafted, sometimes through manual analysis of target text."
"In this paper, we present a method for learn-ing the basic patterns contained within a plan and the ordering among them."
"As training data, we use semantically tagged transcripts of domain ex-perts performing the task our system is designed to mimic, an oral briefing of patient status af-ter undergoing coronary bypass surgery."
"Given that our target output is spoken language, there is some level of variability between individual tran-scripts."
It is difficult for a human to see patterns in the data and thus supervised learning based on hand-tagged training sets can not be applied.
We need a learning algorithm that can discover order-ing patterns in apparently unordered input.
"We based our unsupervised learning algorithm on techniques used in computational genomics[REF_CITE], where from large amounts of seemingly unorganized genetic sequences, pat-terns representing meaningful biological features are discovered."
"In our application, a transcript is the equivalent of a sequence and we are searching for patterns that occur repeatedly across multiple sequences."
"We can think of these patterns as the basic elements of a plan, representing small clus-ters of semantic units that are similar in size, for example, to the nucleus-satellite pairs of RST. [Footnote_1]"
"1 Note, however, that we do not learn or represent inten-tion."
"By learning ordering constraints over these ele- ments, we produce a plan that can be expressed as a constraint-satisfaction problem."
"In this pa-per, we focus on learning the plan elements and the ordering constraints between them."
Our sys-tem uses combinatorial pattern matching[REF_CITE]combined with clustering to learn plan elements.
"Subsequently, it applies counting procedures to learn ordering constraints among these elements."
"Our system produced a set of 24 schemata units, that we call “plan elements” [Footnote_2] , and 29 order-ing constraints between these basic plan elements, which we compared to the elements contained in the orginal hand-crafted plan that was constructed based on hand-analysis of transcripts, input from domain experts, and experimental evaluation of the system[REF_CITE]."
2 These units can be loosely related to the concept of mes-sages[REF_CITE].
The remainder of this article is organized as follows: first the data used in our experiments is presented and its overall structure and acqui-sition methodology are analyzed.
"In Section 3 our techniques are described, together with their grounding in computational genomics."
The quan-titative and qualitative evaluation are discussed in Section 4.
Related work is presented in Sec-tion 5.
Conclusions and future work are discussed in Section 6.
"Our research is part of MAGIC[REF_CITE], a system that is designed to produce a briefing of patient status after un-dergoing a coronary bypass operation."
"Currently, when a patient is brought to the intensive care unit (ICU) after surgery, one of the residents who was present in the operating room gives a brief-ing to the ICU nurses and residents."
Several of these briefings were collected and annotated for the aforementioned evaluation.
"The resident was equipped with a wearable tape recorder to tape the briefings, which were transcribed to provide the base of our empirical data."
The text was sub-sequently annotated with semantic tags as shown in Figure 1.
The figure shows that each sentence is split into several semantically tagged chunks.
"The tag-set was developed with the assistance of a domain expert in order to capture the different information types that are important for commu-nication and the tagging process was done by two non-experts, after measuring acceptable agree-ment levels with the domain expert (see[REF_CITE])."
The tag-set totalled over 200 tags.
These categories are the ones used for our current research.
"From these transcripts, we derive the sequences of semantic tags for each transcript."
"These se-quences constitute the input and working material of our analysis, they are an average length of 33 tags per transcript (min = 13, max = 66, σ = 11.6)."
A tag-set distribution analysis showed that some of the categories dominate the tag counts.
"Furthermore, some tags occur fairly regularly to-wards either the beginning (e.g., date-of-birth ) or the end (e.g., urine-output ) of the transcript, while others (e.g., intraop-problems ) are spread more or less evenly throughout."
Getting these transcripts is a highly expensive task involving the cooperation and time of nurses and physicians in the busy ICU.
Our corpus con-tains a total number of 24 transcripts.
"Therefore, it is important that we develop techniques that can detect patterns without requiring large amounts of data."
"During the preliminary analysis for this research, we looked for techniques to deal with analysis of regularities in sequences of finite items (semantic tags, in this case)."
We were interested in devel-oping techniques that could scale as well as work with small amounts of highly varied sequences.
Computational biology is another branch of computer science that has this problem as one topic of study.
We focused on motif detection techniques as a way to reduce the complexity of the overall setting of the problem.
"In biological terms, a motif is a small subsequence, highly con-served through evolution."
"From the computer sci-ence standpoint, a motif is a fixed-order pattern, simply because it is a subsequence."
The problem of detecting such motifs in large databases has attracted considerable interest in the last decade (see[REF_CITE]for a recent sur-vey).
"Combinatorial pattern discovery, one tech-nique developed for this problem, promised to be a good fit for our task because it can be pa-rameterized to operate successfully without large amounts of data and it will be able to iden-tify domain swapped motifs: for example, given a–b–c in one sequence and c–b–a in another."
"This difference is central to our current research, given that order constraints are our main focus."
TEIRESIAS[REF_CITE]and SPLASH[REF_CITE]are good representa-tives of this kind of algorithm.
We used an adap-tation of TEIRESIAS.
The algorithm can be sketched as follows: we apply combinatorial pattern discovery (see Sec-tion 3.1) to the semantic sequences.
The obtained patterns are refined through clustering (Section 3.2).
Counting procedures are then used to es-timate order constraints between those clusters (Section 3.3).
"In this section, we provide a brief explanation of our pattern discovery methodology."
"The explana-tion builds on the definitions below: hL, Wi pattern."
"Given that Σ represents the se-mantic tags alphabet, a pattern is a string of the form Σ(Σ|?) ∗ Σ, where ? represents a don’t care (wildcard) position."
"The hL, Wi parameters are used to further control the amount and placement of the don’t cares: every subsequence of length W, at least L positions must be filled (i.e., they are non-wildcards characters)."
"This definition entails that L ≤ W and also that a hL, Wi pattern is also a hL, W + 1i pattern, etc."
The support of pattern p given a set of sequences S is the number of sequences that contain at least one match of p.
It indicates how useful a pattern is in a certain environ-ment.
The offset list records the matching locations of a pattern p in a list of sequences.
"They are sets of ordered pairs, where the first position records the sequence number and the second position records the offset in that sequence where p matches (see Figure 3)."
"We define a partial order relation on the pattern space as follows: a pattern p is said to be more specific than a pattern q if: (1) p is equal to q in the defined posi-tions of q but has fewer undefined (i.e., wild-cards) positions; or (2) q is a substring of p."
Specificity provides a notion of complexity of a pattern (more specific patterns are more complex).
See Figure 4 for an example.
"Using the previous definitions, the algorithm re-duces to the problem of, given a set of sequences, L, W, a minimum windowsize, and a support threshold, finding maximal hL, W i-patterns with at least a support of support threshold."
Our im-plementation can be sketched as follows:
"For a given window size n, all the pos-sible subsequences (i.e., n-grams) occurring in the training set are identified."
This process is repeated for different window sizes.
"For each of the identified subse-quences, patterns are created by replacing valid positions (i.e., any place but the first and last positions) with wildcards."
"Only hL,Wi patterns with support greater than support threshold are kept."
Figure 5 shows an example.
The above process is repeated increas-ing the window size until no patterns with enough support are found.
"The list of iden-tified patterns is then filtered according to specificity: given two patterns in the list, one of them more specific than the other, if both have offset lists of equal size, the less spe-cific one is pruned [Footnote_3] ."
"3 Since they match in exactly the same positions, we prune the less specific one, as it adds no new information."
This gives us the list of maximal motifs (i.e. patterns) which are supported by the training data.
"After the detection of patterns is finished, the number of patterns is relatively large."
"Moreover, as they have fixed length, they tend to be pretty similar."
"In fact, many tend to have their support from the same subsequences in the corpus."
We are interested in syntactic similarity as well as simi-larity in context.
"A convenient solution was to further cluster the patterns, according to an approximate matching distance measure between patterns, defined in an appendix at the end of the paper."
We use agglomerative clustering with the dis-tance between clusters defined as the maximum pairwise distance between elements of the two clusters.
Clustering stops when no inter-cluster distance falls below a user-defined threshold.
Each of the resulting clusters has a single pat-tern represented by the centroid of the cluster.
This concept is useful for visualization of the cluster in qualitative evaluation.
"The last step of our algorithm measures the fre-quencies of all possible order constraints among pairs of clusters, retaining those that occur of-ten enough to be considered important, accord-ing to some relevancy measure."
We also discard any constraint that it is violated in any training sequence.
We do this in order to obtain clear-cut constraints.
Using the number of times a given constraint is violated as a quality measure is a straight-forward extension of our framework.
The algorithm proceeds as follows: we build a table of counts that is updated every time a pair of pat-terns belonging to particular clusters are matched.
"To obtain clear-cut constraints, we do not count overlapping occurrences of patterns."
"From the table of counts we need some rele- vancy measure, as the distribution of the tags is skewed."
We use a simple heuristic to estimate a relevancy measure over the constraints that are never contradicted.
We are trying to obtain an es-timate of
P r (A ≺ precedes B) from the counts of c = A ≺˜ preceded B
We normalize with these counts (where x ranges over all the patterns that match before/after A or B): c 1 = A ≺˜ preceded x and c 2 = x ≺˜ preceded B
"The obtained estimates, e 1 = c/c 1 and e 2 = c/c 2 , will in general yield different numbers."
"We use the arithmetic mean between both, e = (e 1 +2e 2 ) , as the final estimate for each constraint."
"It turns out to be a good estimate, that predicts accuracy of the generated constraints (see Section 4)."
We use cross validation to quantitatively evaluate our results and a comparison against the plan of our existing system for qualitative evaluation.
We evaluated two items: how effective the pat-terns and constraints learned were in an unseen test set and how accurate the predicted constraints were.
This figure measures the percentage of identified patterns that were able to match a sequence in the test set.
An ordering constraint between two clusters can only be checkable on a given sequence if at least one pattern from each cluster is present.
We measure the percentage of the learned constraints that are indeed checkable over the set of test se-quences.
"This is, from our perspec-tive, the most important judgement."
It mea-sures the percentage of checkable ordering
"Using 3-fold cross-validation for computing these metrics, we obtained the results shown in Ta-ble 1 (averaged over 100 executions of the exper-iment)."
"The different parameter settings were de-fined as follows: for the motif detection algorithm hL, W i = h2, 3i and support threshold of 3."
The algorithm will normally find around 100 maximal motifs.
The clustering algorithm used a relative distance threshold of 3.5 that translates to an ac-tual treshold of 120 for an average inter-cluster distance of 174.
The number of produced clusters was in the order of the 25 clusters or so.
"Finally, a threshold in relevancy of 0.1 was used in the con-straint learning procedure."
Given the amount of data available for these experiments all these pa-rameters were hand-tunned.
"The system was executed using all the available information, with the same parametric settings used in the quantitative evaluation, yielding a set of 29 constraints, out of 23 generated clusters."
These constraints were analyzed by hand and compared to the existing content-planner.
We found that most rules that were learned were val-idated by our existing plan.
"Moreover, we gained placement constraints for two pieces of semantic information that are currently not represented in the system’s plan."
"In addition, we found minor order variation in relative placement of two differ-ent pairs of semantic tags."
This leads us to believe that the fixed order on these particular tags can be relaxed to attain greater degrees of variability in the generated plans.
"The process of creation of the existing content-planner was thorough, in-formed by multiple domain experts over a three year period."
The fact that the obtained constraints mostly occur in the existing plan is very encour-aging.
"As explained[REF_CITE], mo-tif detection is usually targeted with alignment techniques (as[REF_CITE]) or with combinatorial pattern discovery techniques such as the ones we used here."
"Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permu-tations, for representation of wild cards and for use on smaller data sets."
Similar techniques are used in NLP.
"Align-ments are widely used in MT, for example[REF_CITE], but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us."
"Pattern discovery techniques are often used for information extraction (e.g.,[REF_CITE]), but most work uses data that con-tains patterns labelled with the semantic slot the pattern fills."
"Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those de-veloped in computational genomics."
"Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g.,[REF_CITE])."
Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together[REF_CITE].
This results in a bottom-up approach for ordering that opportunistically groups sentences together based on content fea-tures.
"In contrast, our work attempts to automati-cally learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content."
In this paper we presented a technique for extract-ing order constraints among plan elements that performs satisfactorily without the need of large corpora.
"Using a conservative set of parameters, we were able to reconstruct a good portion of a carefully hand-crafted planner."
"Moreover, as dis-cussed in the evaluation, there are several pieces of information in the transcripts which are not present in the current system."
"From our learned results, we have inferred placement constraints of the new information in relation to the previous plan elements without further interviews with ex-perts."
"Furthermore, it seems we have captured order-sensitive information in the patterns and free-order information is kept in the don’t care model."
"The patterns, and ordering constraints among them, provide a backbone of relatively fixed struc-ture, while don’t cares are interspersed among them."
"This model, being probabilistic in nature, means a great deal of variation, but our gener-ated plans should have variability in the right po-sitions."
"This is similar to findings of floating posi-tioning of information, together with oportunistic rendering of the data as used in STREAK[REF_CITE]."
We are planning to use these techniques to revise our current content-planner and incorporate infor-mation that is learned from the transcripts to in-crease the possible variation in system output.
The final step in producing a full-fledged content-planner is to add semantic constraints on the selection of possible orderings.
This can be generated through clustering of semantic input to the generator.
We also are interested in further evaluating the technique in an unrestricted domain such as the Wall Street Journal (WSJ) with shallow seman-tics such as the WordNet top-category for each NP-head.
This kind of experiment may show strengths and limitations of the algorithm in large corpora.
This research is supported in part by NLM Con-tract[REF_CITE]-01 and the Columbia Uni-versity Center for Advanced Technology in In-formation Management (funded by the New York State Science and Technology Foundation).
"The authors would like to thank Regina Barzilay,"
Noemie Elhadad and Smaranda Muresan for help-ful suggestions and comments.
The aid of two anonymous reviewers was also highly appreci-ated.
"We describe a new framework for de-pendency grammar, with a modular de-composition of immediate dependency and linear precedence."
Our approach distinguishes two orthogonal yet mutu-ally constraining structures: a syntactic dependency tree and a topological de-pendency tree.
"The syntax tree is non-projective and even non-ordered, while the topological tree is projective and partially ordered."
Linear precedence in so-called free word order languages remains challenging for modern gram-mar formalisms.
"To address this issue, we pro-pose a new framework for dependency gram-mar which supports the modular decomposition of immediate dependency and linear precedence."
"In this article, we develop a complementary approach dedicated to the treat-ment of linear precedence."
"Our framework distinguishes two orthogonal, yet mutually constraining structures: a syntactic dependency tree ( ID tree) and a topological de-pendency tree ( LP tree)."
"While edges of the ID tree are labeled by syntactic roles, those of the LP tree are labeled by topological fields[REF_CITE]."
The shape of the LP tree is a flattening of the ID tree’s obtained by allowing nodes to ‘climb up’ to land in an appropriate field at a host node where that field is available.
Our theory of ID / LP trees is formulated in terms of (a) lexicalized con-straints and (b) principles governing e.g. climbing conditions.
"In Section 2 we discuss the difficulties pre-sented by discontinuous constructions in free word order languages, and briefly touch on the limitations of Reape’s (1994) popular theory of ‘word order domains’."
In Section 3 we introduce the concept of topological dependency tree.
In Section 4 we outline the formal framework for our theory of ID / LP trees.
"Finally, in Section 5 we illustrate our approach with an account of the word-order phenomena in the verbal complex of German verb final sentences."
"In free word order languages, discontinuous con-structions occur frequently."
"German, for example, is subject to scrambling and partial extraposition."
"In typical phrase structure based analyses, such phenomena lead to e.g. discontinuous VP s: (1) (dass) einen Mann Maria zu lieben versucht (that) a man acc Maria nom to love tries whose natural syntax tree exhibits crossing edges:"
"Since this is classically disallowed, discontinu-ous constituents must often be handled indirectly through grammar extensions such as traces."
"Reape distin-guished two orthogonal tree structures: (a) the un-ordered syntax tree, (b) the totally ordered tree of word order domains."
The latter is obtained from the syntax tree by flattening using the operation of domain union to produce arbitrary interleav-ings.
The boolean feature [∪±] of each node con-trols whether it must be flattened out or not.
In-finitives in canonical position are assigned [∪+] :
"Thus, the above licenses the following tree of word order domains:"
Extraposed infinitives are assigned [∪−] :
"As a consequence, Reape’s theory correctly pre-dicts scrambling (2,3) and full extraposition (4), but cannot handle the partial extraposition in (5): (2) (dass) Maria einen Mann zu lieben versucht (3) (dass) einen Mann Maria zu lieben versucht (4) (dass) Maria versucht, einen Mann zu lieben (5) (dass) Maria einen Mann versucht, zu lieben"
Our approach is based on dependency grammar.
"We also propose to distinguish two structures: (a) a tree of syntactic dependencies, (b) a tree of topo-logical dependencies."
The syntax tree ( ID tree) is unordered and non-projective (i.e. it admits cross-ing edges).
"For display purposes, we pick an ar-bitrary linear arrangement: subject zuvinf object det (dass) Maria einen Mann zu lieben versucht"
The topological tree ( LP tree) is partially ordered and projective: mf mf vc df v n n v d (dass) Maria einen Mann zu lieben versucht
Its edge labels are called (external) fields and are totally ordered: df ≺ mf ≺ vc.
This induces a linear precedence among the daughters of a node in the LP tree.
This precedence is partial because daughters with the same label may be freely per-muted.
"In order to obtain a linearization of a LP tree, it is also necessary to position each node with respect to its daughters."
"For this reason, each node is also assigned an internal field (d, n, or v) shown above on the vertical pseudo-edges."
The set of internal and external fields is totally or-dered: d ≺ df ≺ n ≺ mf ≺ vc ≺ v
"Like Reape, our LP tree is a flattened version of the ID tree[REF_CITE], but the flattening doesn’t happen by ‘unioning up’; rather, we allow each individual daughter to climb up to find an appropriate landing place."
"This idea is reminiscent of GB , but, as we shall see, pro-ceeds rather differently."
The framework underlying both ID and LP trees is the configuration of labeled trees under valency (and other) constraints.
"Consider a finite set L of edge labels, a finite set V of nodes, and E ⊆ V × V × L a finite set of directed labeled edges, such that (V, E) forms a tree."
We write w−− ` →w 0 for an edge labeled ` from w to w 0 .
We define the `-daughters `(w) of w ∈ V as follows: `(w) = {w 0 ∈ V | w−− ` →w 0 ∈ E}
We write Lb for the set of valency specifications b̀ defined by the following abstract syntax: b̀ ::= ` | `? | `∗ (` ∈ L)
A valency is a subset of Lb.
"The tree (V, E) satis-fies the valency assignment valency : V → 2 Lb if for all w ∈ V and all ` ∈ L: ` ∈ valency(w) ⇒ |`(w)| = 1 `? ∈ valency(w) ⇒ |`(w)| ≤ 1 `∗ ∈ valency(w) ⇒ |`(w)| ≥ 0 otherwise ⇒ |`(w)| = 0"
"An ID tree (V, E ID , lex, cat, valency ID ) consists of a tree (V, E ID ) with E ID ⊆ V × V × R, where the set R of edge labels (Figure 1) represents syn-tactic roles such as subject or vinf (bare infinitive argument). lex : V → Lexicon assigns a lexi-cal entry to each node."
An illustrative Lexicon is displayed in Figure 1 where the 2 features cats and valency ID of concern to ID trees are grouped under table heading “Syntax”.
"Finally, cat and valency ID assign a category and an Rb valency to each node w ∈ V and must satisfy: cat(w) ∈ lex(w).cats valency ID (w) = lex(w).valency ID (V, E ID ) must satisfy the valency ID assignment as described earlier."
"For example the lexical entry for versucht specifies (Figure 1): valency ID (versucht) = {subject, zuvinf}"
"Furthermore, (V, E ID ) must also satisfy the edge constraints stipulated by the grammar (see Figure 1)."
"For example, for an edge w−− det −−→w 0 to be licensed, w 0 must be assigned category det and both w and w 0 must be assigned the same agreement. [Footnote_1]"
1 Issues of agreement will not be further considered in this paper.
"An LP tree (V, E LP , lex, valency LP , field ext , field int ) consists of a tree (V, E LP ) with E LP ⊆ V × V × F ext , where the set F ext of edge labels represents topological fields[REF_CITE]: df the determiner field, mf the ‘Mittelfeld’, vc the verbal complement field, xf the extraposition field."
"Features of lexical entries relevant to LP trees are grouped under table heading “Topology” in Figure 1. valency LP assigns a Fd ext valency to each node and is subject to the lexicalized constraint: valency LP (w) = lex(w).valency LP (V,E LP ) must satisfy the valency LP assignment as described earlier."
"For example, the lexical en-try for zu lieben 2 specifies: valency LP (zu lieben 2 ) = {mf∗, xf?} which permits 0 or more mf edges and at most one xf edge; we say that it offers fields mf and xf."
"Unlike the ID tree, the LP tree must be projective."
"The grammar stipulates a total order on F ext , thus inducing a partial linear precedence on each node’s daughters."
This order is partial because all daughters in the same field may be freely per-muted: our account of scrambling rests on free permutations within the mf field.
"In order to ob-tain a linearization of the LP tree, it is necessary to specify the position of a node with respect to its daughters."
For this reason each node is assigned an internal field in F int .
The set F ext ∪ F int is to-tally ordered: d ≺ df ≺ n ≺ mf ≺ vc ≺ v ≺ xf
In what (external) field a node may land and what internal field it may be assigned is deter-mined by assignments field ext :
V → F ext and field int : V → F int which are subject to the lexi-calized constraints: field ext (w) ∈ lex(w).field ext field int (w) ∈ lex(w).field int
"For example, zu lieben [Footnote_1] may only land in field vc (canonical position), and zu lieben 2 only in xf (ex-traposed position)."
1 Issues of agreement will not be further considered in this paper.
The LP tree must satisfy: w−− ` →w 0 ∈ E LP ⇒ ` = field ext (w 0 )
"Thus, whether an edge w−− ` →w 0 is licensed de-pends both on valency LP (w) and on field ext (w 0 )."
In other words: w must offer field ` and w 0 must accept it.
"For an edge w−− ` →w 0 in the ID tree, we say that w is the head of w 0 ."
"For a similar edge in the LP tree, we say that w is the host of w 0 or that w 0 lands on w. The shape of the LP tree is a flat-tened version of the ID tree which is obtained by allowing nodes to climb up subject to the follow-ing principles:"
Principle1 a node must land on a transitive head [Footnote_2]
2 This is Bröcker’s terminology and means a node in the transitive closure of the head relation.
Principle 2 it may not climb through a barrier
"We will not elaborate the notion of barrier which is beyond the scope of this article, but, for exam-ple, a noun will prevent a determiner from climb-ing through it, and finite verbs are typically gen-eral barriers."
"Principle 3 a node must land on, or climb higher than, its head Subject to these principles, a node w 0 may climb up to any host w which offers a field licensed by field ext (w 0 )."
We now illustrate our theory by applying it to the treatment of word order phenomena in the verbal complex of German verb final sentences.
We as-sume the grammar and lexicon shown in Figure 1.
These are intended purely for didactic purposes and we extend for them no claim of linguistic ad-equacy.
Control verbs like versuchen or versprechen al-low their zu-infinitival complement to be option-ally extraposed.
"This phenomenon is also known as optional coherence. (6) (dass) Maria einen Mann zu lieben versucht (7) (dass) Maria versucht, einen Mann zu lieben"
Both examples share the following ID tree: subject zuvinf object det (dass) Maria einen Mann zu lieben versucht
Optional extraposition is handled by having two lexical entries for zu lieben.
One requires it to land in canonical position: field ext (zu lieben 1 ) = {vc} the other requires it to be extraposed: field ext (zu lieben 2 ) = {xf}
"In the canonical case, zu lieben 1 does not offer field mf and einen Mann must climb to the finite verb: mf mf vc df v n n v d (dass) Maria einen Mann zu lieben versucht"
"In the extraposed case, zu lieben 2 itself offers field mf: mf xf mf v n df v n d (dass) Maria versucht einen Mann zu lieben"
"In example (8), the zu-infinitive zu lieben is extra-posed to the right of its governing verb versucht, but its nominal complement einen Mann remains in the Mittelfeld: (8) (dass) Maria einen Mann versucht, zu lieben"
"In our account, Mann is restricted to land in an mf field which both extraposed zu lieben 2 and finite verb versucht offer."
In example (8) the nominal complement simply climbed up to the finite verb: mf mf xf df v n n v d (dass) Maria einen Mann versucht zu lieben
Verb clusters are typically head-final in German: non-finite verbs precede their verbal heads. (9) (dass) Maria einen Mann lieben wird (that) Maria nom a man acc love will
The ID tree for (9) is: subject vinf object det (dass) Maria einen Mann lieben wird
"The lexical entry for the bare infinitive lieben re-quires it to land in a vc field: field ext (lieben) = {vc} therefore only the following LP tree is licensed: 3 mf mf vc df v n n v d (dass) Maria einen Mann lieben wird where mf ≺ vc ≺ v, and subject and ob-ject, both in field mf, remain mutually unordered."
Thus we correctly license (9) and reject (10).
"In an auxiliary flip constructi[REF_CITE], the verbal complement of an auxiliary verb, such as haben or werden, follows rather than precedes its head."
Only a certain class of bare infinitive verbs can land in extraposed po-sition.
"As we illustrated above, main verbs do not belong to this class; however, modals such as können do, and may land in either canonical (11) or in extraposed (12) position."
This behavior is called ‘optional auxiliary flip’. (11) (dass) Maria einen Mann lieben können wird (that)
Maria a man love can will (that) Maria will be able to love a man (12) (dass) Maria einen Mann wird lieben können
Both examples share the following ID tree: subject vinf vinf object det (dass) Maria einen Mann wird lieben können
"Our grammar fragment describes optional auxil-iary flip constructions in two steps: • wird offers both vc and xf fields: valency ID (wird) = {mf∗, vc?, xf?} • können has two lexical entries, one canonical and one extraposed: field ext (können 1 ) = {vc} field ext (können 2 ) = {xf}"
Thus we correctly account for examples (11) and (12) with the following LP trees: mf mf vc df vc v n n v d v (dass) Maria einen Mann lieben können wird mf mf xf df vcv n vn d v (dass) Maria einen Mann wird lieben können
The astute reader will have noticed that other LP trees are licensed for the earlier ID tree: they are considered in the section below.
This phenomenon related to auxiliary flip de-scribes the case where non-verbal material is in-terspersed in the verb cluster: (13) (dass) Maria wird einen Mann lieben können (14)*(dass)
Maria lieben einen Mann können wird (15)*(dass)
Maria lieben können einen Mann wird
The ID tree remains as before.
"The NP einen Mann must land in a mf field. lieben is in canon-ical position and thus does not offer mf, but both extraposed können 2 and finite verb wird do."
"Therefore, einen Mann must precede both lieben and können."
The Zwischenstellung construction describes cases where the auxiliary has been flipped but its verbal argument remains in the Mittelfeld.
These are the remaining linearizations predicted by our theory for the running example started above: (16) (dass) Maria einen Mann lieben wird können (17) (dass) einen Mann Maria lieben wird können where lieben has climbed up to the finite verb.
Substitute infinitives (Ersatzinfinitiv) are further examples of extraposed verbal forms.
"A sub-stitute infinitive exhibits bare infinitival inflec-tion, yet acts as a complement of the perfectizer haben, which syntactically requires a past partici-ple."
"Only modals, AcI-verbs such as sehen and lassen, and the verb helfen can appear in substi-tute infinitival inflection."
A substitute infinitive cannot land in canonical position; it must be extraposed: an auxiliary flip involving a substitute infinitive is called an ‘oblig-atory auxiliary flip’. (18) (dass) Maria einen Mann hat lieben können (that)
Maria a man has love can (that) Maria was able to love a man (19) (dass) Maria hat einen Mann lieben können (20)*(dass) Maria einen Mann lieben können hat
"These examples share the ID tree: subject xvinf vinf object det (dass) Maria einen Mann hat lieben können hat subcategorizes for a verb in past participle in-flection because: valency ID (hat) = {subject, vpast} and the edge constraint for w−− vpast −−−→w 0 requires: cat(w 0 ) = vpast"
"This is satisfied by können 2 which insists on being extraposed, thus ruling (20) out: field ext (können 2 ) = {xf}"
Double auxiliary flip constructions occur when an auxiliary is an argument of another auxiliary.
Each extraposed verb form offers both vc and mf: thus there are more opportunities for verbal and nominal arguments to climb to. (21) (dass) Maria wird haben einen Mann lieben können (that) Maria will have been able to love a man (22) (dass) Maria einen Mann wird haben lieben können (23) (dass) Maria wird einen Mann lieben haben können (24) (dass) Maria einen Mann wird lieben haben können (25) (dass) Maria einen Mann lieben wird haben können
These examples have ID tree: subject vinf vpast vinf object det
Maria einen Mann wird haben lieben können and (22) obtains LP tree: mf mf xf df xf v n n vcv d v v Maria einen Mann wird haben lieben können
Certain verbs like scheint require their argument to appear in canonical (or coherent) position.
"Obligatory coherence may be enforced with the following constraint principle: if w is an obliga-tory coherence verb and w 0 is its verbal argument, then w 0 must land in w’s vc field."
"Like barri-ers, the expression of this principle in our gram-matical formalism falls outside the scope of the present article and remains the subject of active research. [Footnote_4]"
4 we also thank an anonymous reviewer for pointing out that our grammar fragment does not permit intraposition
"In this article, we described a treatment of lin-ear precedence that extends the constraint-based framework for dependency grammar proposed[REF_CITE]."
"We distinguished two orthogo-nal, yet mutually constraining tree structures: un-ordered, non-projective ID trees which capture purely syntactic dependencies, and ordered, pro-jective LP trees which capture topological depen-dencies."
Our theory is formulated in terms of (a) lexicalized constraints and (b) principles which govern ‘climbing’ conditions.
"We illustrated this theory with an application to the treatment of word order phenomena in the ver-bal complex of German verb final sentences, and demonstrated that these traditionally challenging phenomena emerge naturally from our simple and elegant account."
"Although we provided here an account spe-cific to German, our framework intentionally per-mits the definition of arbitrary language-specific topologies."
Whether this proves linguistically ad-equate in practice needs to be substantiated in fu-ture research.
Characteristic of our approach is that the for-mal presentation defines valid analyses as the so-lutions of a constraint satisfaction problem which is amenable to efficient processing through con-straint propagation.
A prototype was imple-mented in Mozart/Oz and supports a parsing mode as well as a mode generating all licensed linearizations for a given input.
It was used to prepare all examples in this article.
"While the preliminary results presented here are encouraging and demonstrate the potential of our approach to linear precedence, much work re-mains to be done to extend its coverage and to arrive at a cohesive and comprehensive grammar formalism."
"This paper presents methods for a qual-itative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples ex-tracted from German corpora."
"In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified “true positives”."
We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples.
"In computational linguistics, a variety of (statis-tical) measures have been proposed for identify-ing lexical associations between words in lexi-cal tuples extracted from text corpora."
Methods used range from pure frequency counts to infor-mation theoretic measures and statistical signifi-cance tests.
"While the mathematical properties of those measures have been extensively discussed, [Footnote_1] the strategies employed for evaluating the iden-tification results are far from adequate."
"1 See for instance ([REF_CITE]chap-ter 5),[REF_CITE], and[REF_CITE]."
Another crucial but still unsolved issue in statistical col-location identification is the treatment of low-frequency data.
"In this paper, we first specify requirements for a qualitative evaluation of lexical association mea- sures (AMs)."
"Based on these requirements, we introduce an experimentation procedure, and dis-cuss the evaluation results for a number of widely used AMs."
"Finally, methods and strategies for handling low-frequency data are suggested.  "
"The measures [Footnote_2] – Mutual Information ( )[REF_CITE], the log-likelihood t-test and  -test, and co-occurrence frequency – ratio test[REF_CITE], two statistical tests: are applied to two sets of data: adjective-noun (AdjN) pairs and preposition-noun-verb (PNV) triples, where the AMs are applied to (PN,V) pairs."
"2 For a more detailed description of these measures and relevant literature, see ([REF_CITE]chapter 5) or[URL_CITE]where several other AMs are discussed as well."
See section 3 for a description of the base data.
"For evaluation of the association measures, -best strategies (section 4.1) are supplemented with precision and recall graphs (section 4.2) over the complete data sets."
Samples comprising par-ticular frequency strata (high versus low frequen-cies) are examined (section 4.3).
"In section 5, methods for the treatment of low-frequency data, single (hapaxlegomena) and double occurrences are discussed."
The significance of differences be-tween the AMs is addressed in section 6.
A standard procedure for the evaluation of AMs is manual judgment of the -best candidates identi-fied in a particular corpus by the measure in ques-tion.
"Typically, the number of true positives (TPs) among the 50 or 100 (or slightly more) highest ranked word combinations is manually identified by a human evaluator, in most cases the author of the paper in which the evaluation is presented."
This method leads to a very superficial judgment of AMs for the following reasons: (1) The identification results are based on small subsets of the candidates extracted from the cor-pus.
"Consequently, results achieved by individ-ual measures may very well be due to chance (cf. sections 4.1 and 4.2), and evaluation with respect to frequency strata is not possible (cf. section 4.3). (2) For the same reason, it is impossible to determine recall values, which are important for many practical applications. (3) The introduc-tion of new measures or changes to the calculation methods require additional manual evaluation, as new -best lists are generated."
"To improve the reliability of the evaluation re-sults, a number of properties need to be con-trolled."
"We distinguish between two classes: (1) Characteristics of the set of candidate data employed for collocation identification: (i) the syntactic homogeneity of the base data, i.e., whether the set of candidate data consists only of adjective-noun, noun-verb, etc. pairs or whether different types of word combinations are mixed; (ii) the grammatical status of the individual word combinations in the base set, i.e., whether they are part of or constitute a phrase or simply co-occur within a given text window; (iii) the per-centage of TPs in the base set, which is typically higher among high-frequency data than among low-frequency data. (2) The evaluation strategies applied: Instead of examining only a small sample of -best can-didates for each measure as it is common practice, we make use of recall and precision values for -best samples of arbitrary size, which allows us to plot recall and precision curves for the whole set of candidate data."
"In addition, we compare preci-sion curves for different frequency strata."
The base data for our experiments are extracted from two corpora which differ with respect to size and text type.
The base sets also differ with re- spect to syntactic homogeneity and grammatical correctness.
Both candidate sets have been man-ually inspected for TPs.
"The first set comprises bigrams of adjacent, lemmatized  AdjN pairs extracted from a small ( word) corpus of freely available Ger-man law texts. [Footnote_3] Due to the extraction strategy, the data are homogeneous and grammatically correct, i.e., there is (almost) always a grammatical de-pendency between adjacent adjectives and nouns in running text."
3 See[REF_CITE]for a description of the part-of-speech tagger used to identify adjectives and nouns in the corpus.
"Two human annotators indepen-dently marked candidate pairs perceived as “typ-ical” combinations, including idioms ((die) hohe See, ‘the high seas’), legal terms (üble Nachrede, ‘slander’), and proper names (Rotes Kreuz, ‘Red Cross’)."
Candidates accepted by either one of the annotators were considered TPs.
"The second set consists of PNV triples ex-tracted from an 8 million word portion of the Frankfurter Rundschau Corpus [Footnote_4] , in which part-of-speech tags and minimal PPs were identified. [Footnote_5] The PNV triples were selected automatically such that the preposition and the noun are constituents of the same PP, and the PP and the verb co-occur within a sentence."
4 The Frankfurter Rundschau Corpus is part of the Euro-pean Corpus Initiative Multilingual Corpus I.
5 See[REF_CITE]for a description of the tag-ger and chunker.
"Only main verbs were con-sidered and full forms were reduced to bases. [Footnote_6] The PNV data are partially inhomogeneous and not fully grammatically correct, because they in-clude combinations with no grammatical relation between PN and V. PNV collocations were man-ually annotated."
"6 Mmorph – the MULTEXT morphology tool provided by ISSCO/SUISSETRA, Geneva, Switzerland – has been em-ployed for determining verb infinitives."
"The criteria used for the dis-tinction between collocations and arbitrary word combinations are: There is a grammatical rela-tion between the verb and the PP, and the triple can be interpreted as support verb construction and/or a metaphoric or idiomatic reading is avail-able, e.g.: zur Verfügung stellen (at_the availabil-ity put, ‘make available’), am Herzen liegen (at the heart lie, ‘have at heart’). [Footnote_7]"
"7 For definitions of and literature on idioms, metaphors and support verb constructions (Funktionsverbgefüge) see for instance[REF_CITE]."
General statistics for the AdjN and PNV base sets are given in Table 1.
Manual annotation  was and PNV triples withperformed for AdjN pairs  with frequencyonly (see section 5 for a discussion of the excluded low-frequency candidates).
"After extraction of the base data and manual iden-tification of TPs, the AMs are applied, resulting in an ordered candidate list for each measure (hence-forth significance list, SL)."
The order indicates the degree of collocativity.
Multiple candidates with identical scores are listed in random order.
"This is necessary, in particular, when co-occurrence fre-quency is used as an association measure."
"In this approach, the set of the highest ranked word combinations is evaluated for each measure, and the proportion of TPs among this -best list (the precision) is computed."
Another measure of goodness is the proportion of TPs in the base data that are also contained in the -best list (the re-call).
"While precision measures the quality of the -best lists produced, recall measures their cov-erage, i.e., how many of all true collocations in the corpus were identified."
The most problematic aspect here is that conclusions drawn from -best lists for a single (and often small) value of are only snapshots and likely to be misleading.
"For instance, considering the set of AdjN base data with we might arrive at the following results (Table 2 gives the precision values of the highest   ranked word combinations with studies (e.g.[REF_CITE]), the precision of): As expected from the results of other  is significantly lower than that of log-likelihood, [Footnote_8] whereas the t-test competes with log-likelihood, and  , and, for especially for larger values of ."
"8 This is to a large part due to the fact that systemati-cally overestimates the collocativity of low-frequency pairs, cf. section 4.3."
"Frequency leads to clearly  better results than , comes close to the accuracy of t-test and log-likelihood."
"For a clearer picture, however, larger portions of the SLs need to be examined."
A well suited means for comparing the goodness of different AMs are the precision and recall graphs obtained by step-wise processing of the complete SLs (Figures 1 to 10 below). [Footnote_9]
9 Colour versions of all plots in this paper will be avail-able[URL_CITE]
"The -axis represents the percentage of data processed in the respective SL, while the -axis represents the precision (or recall) values achieved."
"For instance  , the precision values for and for the AdjN data can be read from   the -axis in Figure  1 at positions whereand (marked by verti-cal lines)."
The dotted horizontal line represents the percentage of true collocations in the base set.
"This value corresponds to the expected precision value for random selection, and provides a base-line for the interpretation of the precision curves."
General findings from the precision graphs are: (i)
"It is only useful to consider the first halves wards. (ii) Precision of log-likelihood,  , t-test of the SLs, as the measures approximate after-and frequency strongly decreases  in the first partof the SLs, whereas precision of remains al-most constant (cf."
Figure 1) or even increases slightly (cf.
"Figure 2). (iii) The identification re-sults are instable for the first few percent of the data, with log-likelihood  , t-test and frequency sta-and  , and the PNV data bilizing earlier than stabilizing earlier than the AdjN data."
"This in-stability is caused by “random fluctuations”, i.e., whether a particular TP ends up on rank (and thus increases the precision of the -best list) or on rank ,  ) contain a particularly ."
The -best lists for AMs with low precision values ( small number of TPs.
"Therefore, they are more susceptible to random variation, which illustrates that evaluation based on a small number of -best candidate pairs cannot be reliable."
"With respect to the recall curves (Figures 3 and 4), we find: (i)[REF_CITE]% of the data in the SLs leads to identification of between 75% (AdjN) and 80% (PNV) of the TPs. (ii)"
"For the results, with  outperformingand  first 40% of the SLs, lead to the worst ."
"Examining the precision and recall graphs in more detail, we find that for the AdjN data (Fig-ure 1), log-likelihood and t-test lead to the best re-sults, with log-likelihood giving an overall better result than the t-test."
The picture differs slightly for the PNV data (Figure 2).
"Here t-test outper-forms log-likelihood, and even precision gained by frequency is better than or at least comparable to log-likelihood."
"These pairings – log-likelihood and t-test for AdjN, and t-test and frequency for PNV – are also visible in the recall curves (Fig-ures 3 and 4)."
"Moreover, for the PNV data the t-test leads to a recall of over 60% when approx. 20% of the SL has been considered."
"In the Figures above, there are a number of po-sitions on the -axis where the precision and re-call values of different measures are almost iden-tical."
This shows that a simple -best approach will often produce misleading results.  
"For in-stance, if we just look at the first of the SLs for the PNV data, we might conclude that the t-test and frequency measures are equally well suited for the extraction of PNV collocations."
"However, the full curves in Figures 2 and 4 show that t-test is consistently better than frequency."
"While we have previously considered data from a broad frequency range (i.e., frequencies for AdjN and for PNV), we will now split up the candidate sets into high-frequency and low-frequency occurrences."
This procedure al-lows us to assess the performance of AMs within different frequency strata  .
"For instance, there is and  are inferior a widely held belief that to other measures because they overestimate the remarks on the  measure  collocativity of low-frequency candidates (cf. the and  to yield much[REF_CITE])."
"One might thus expect better results for higher frequencies. ples withWe have  divided the AdjN data into  two  sam-(high frequencies) and (low frequencies), because the number of data in the base sample is quite small."
"As there are enough PNV data, we used a higher threshold and selected samples  with (high frequencies) and (low frequencies)."
"Considering our high-frequency AdjN data (Fig-ure 5), we find that all precision curves decline as more of the data in the SLs is examined."
"Espe-cially for , this is markedly different from the results obtained before."
"As the full curves show, log-likelihood is obviously the best measure  ."
"It ,  frequency and is followed by t-test, in this order."
"Frequency and approximate when remaining part of the lists,50% of the data in the SLs are  examined."
In theyields better re-sults than frequency and is practically identical to the best-performing measures.
"Surprisingly, the precision curves of  and in particular increase over the first 60% of the SLs for high-frequency PNV data, whereas the curves for t-test, log-likelihood, and frequency have the usual downward slope (see Figure 6)."
"Log-likelihood achieves precision values above 50% for the first 10% of the list, but is outper-formed by the t-test afterwards."
"Looking at the first 40% of the data, there is a big gap between the good measures (t-test, log-likelihood, and  fre-quency) and the weak measures (  and )."
"In the second half of the data in the SLs, how-   , and the other measures, with the exception of, ever, there is virtually no difference between mere co-occurrence frequency."
"Summing up, t-test – with a few exceptions around the first 5% of the data in the SLs – leads to the overall best precision results for high-frequency PNV data."
Log-likelihood is sec-ond best but achieves the best results for high-frequency AdjN data.
"Figures 7 and 8 show that there is little differ-ence between the AMs for low-frequency data, except for co-occurrence frequency, which leads to worse results than all other measures."
"For AdjN data, the AMs at best lead to an im-provement of factor 3 compared to random selec-tion (when up to of the SL is examined, log-likelihood achieves precision values above 30%)."
"Log-likelihood is the overall best measure for identifying AdjN collocations, except for -coordinates between 15% and 20% where t-test outperforms log-likelihood."
"For PNV data, the curves of all measures (ex-cept for frequency) are nearly identical."
Their precision values are not significantly [Footnote_10] different from the baseline obtained by random selection.
10 According to the -test as described in section 6.
"In contrast to our expectation stated at the  be-and  relative to the other AMs is not better for ginning of this section, the performance of high-frequency data than for low-frequency data."
"Instead, the poor performance observed in section 4.2 is explained by the considerably higher base-line precision of the high-frequency data (cf."
"Fig-ures 5 to 8): unlike the -best lists for “frequency-sensitive  ” measures such as log-likelihood, those and  contain a large proportion of low- of frequency candidates."
"As the frequency distribution of word combina-tions in texts is characterised by a large number of rare events, low-frequency data are a serious challenge for AMs."
One way to deal with low-frequency candidates is the introduction of cut-off thresholds.
"This is a widely used strategy, and it is motivated by the fact that it is in gen-eral highly problematic to draw conclusions from low-frequency data with statistical methods (cf."
"A practical reason for cutting off low-frequency data is the need to reduce the amount of manual work when the complete data set has to be evaluated, which is a precondition for the exact calculation of recall and for plotting precision curves."
The major drawback of an approach where all low-frequency candidates are excluded is that a large part of the data is lost for collocation extrac-tion.
"In our data, for instance, 80% of the full set of PNV data and 58% of the AdjN data are ha-paxes."
Thus it is important to know how many (and which) true collocations there are among the excluded low-frequency candidates.
"In this section, we estimate the number of col-locations in the data excluded from our experi-ments (i.e., AdjN pairs with and PNV triples with )."
"Because of the large num-ber of candidates in those sets (6 435[REF_CITE]880 for PNV), manual inspection of the en-tire data is impractical."
"Therefore, we use ran-dom samples from the candidate sets to obtain es-timates for the proportion of true collocations among the low-frequency data."
"We randomly se-lected 965 items (15%) from the AdjN hapaxes, and 983 items ( 0.35%) from the low-frequency PNV triples."
Manual examination of the samples yielded 31 TPs for AdjN (a proportion of 3.2%) and 6 TPs for PNV (0.6%).
"Considering the low proportion of collocations in the samples, we must expect highly skewed frequency distributions (where is very small), which are problematic for standard statistical tests."
"In order to obtain reliable estimates, we have used an exact test based on the following model: Assuming a proportion of TPs in the full low-frequency data (AdjN or PNV), the number of TPs in a random sample of size is described by a binomially distributed random variable with parameter . [Footnote_11] Consequently, the proba-bility of finding  or less TPs  in  the sample."
"11 To be precise, the binomial distribution is itself an ap-proximation of the exact hypergeometric probabilities (cf.[REF_CITE]). This approximation is sufficiently accu-rate as long as the sample size is small compared to the size of the base set (i.e., the number of low-frequency candi-dates)."
"We ap-is ply a one-tailed  statistical test based on the proba-bilities to our samples in order to ob-tain an upper estimate for the actual proportion of collocations among  the low-frequency data: theestimate  is accepted at a given signifi-cance level if .  In the case of the  AdjN data ( , ), we find that at a confidence level of 99% ( )."
"Thus, there should be at most 320 TPs among the AdjN candidates with . data withCompared  to the 737 TPs identified in the AdjN, our decision to exclude the ha-paxlegomena was well justified."
"The proportion   of TPs in the PNV sample ( , ) was much lower and we find that at the same confidence level of 99%."
"However, due to the very large number of low-frequency candi-dates, there may be as many as 4200 collocations in the PNV data with , more than 4 times the number identified in our experiment."
"It is imaginable, then, that one of the AMs might succeed in extracting a substantial num-ber of collocations from the low-frequency PNV data."
Figure 9 shows precision curves for the 10 000 highest ranked word combinations from each SL for PNV combinations with (the vertical  lines   correspond to -best lists for).
"In order to reduce the amount of manual work, the precision values for each AM are based on a 10% random sample from the 10 000 highest ranked candidates."
"We have applied the statisti-cal test described above to obtain confidence in-tervals for the true precision values of the best-performing AM (frequency), given our 10% sam-ple."
The upper and lower bounds of the 95% con-fidence intervals are shown as thin lines.
Even the highest precision estimates fall well below the 6.41% precision baseline of the PNV data with .
"Again, we conclude that the exclusion of low-frequency candidates was well justified."
between AMs using the well-known  test as de-
We have assessed the significance of differences scribed[REF_CITE]. [Footnote_12]
12 See[REF_CITE]for a short discussion of the applicability of this test.
The thin lines[REF_CITE]delimit 95% confidence intervals around withthe best-performing  measure for the AdjN data(log-likelihood).
There is no significant difference between log-likelihood and t-test.
"And only for -best lists with , frequency performs marginally significantly worse than log-likelihood."
"For the PNV data (not shown), the t-test is signifi-cantly better than log-likelihood, but the differ-ence between frequency and the t-test is at best marginally significant."
"We have shown that simple -best approaches are not suitable for a qualitative evaluation of lexi-cal association measures, mainly for the follow-ing reasons: the instability of precision values ob-tained from the first few percent of the data in the SLs; the lack of significant differences between the AMs after approx. 50% of the data in the SLs have been examined; and the lack of significant differences between the measures except for cer-tain specific values of ."
"We have also shown that the evaluation results and the ranking of AMs dif-fer depending on the kind of collocations to be identified, and the proportion of hapaxes in the candidate sets."
"Finally, our results question the widely accepted argument that the strength of log-likelihood lies in handling low-frequency data."
"In our experiments, none of the AMs was able to ex-tract a substantial number of collocations from the set of hapaxlegomena."
"We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web."
We first search the Web for pages contain-ing a term in question.
Then we use lin-guistic patterns and HTML structures to ex-tract text fragments describing the term.
"Fi-nally, we organize extracted term descrip-tions based on word senses and domains."
"In addition, we apply an automatically gener-ated encyclopedia to a question answering system targeting the Japanese Information-Technology Engineers Examination."
"Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural lan-guage processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities."
"A sam-ple of these includes methods to extract linguistic resources[REF_CITE], retrieve useful information in re-sponse to user queries[REF_CITE]and mine/discover knowledge latent in the Web[REF_CITE]."
"In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources."
"Specifically, we enhance the method proposed[REF_CITE], which extracts encyclopedic knowledge (i.e., term descriptions) from the Web."
"In brief, their method searches the Web for pages containing a term in question, and uses linguistic ex-pressions and HTML layouts to extract fragments de-scribing the term."
They also use a language model to discard non-linguistic fragments.
"In addition, a clus-tering method is used to divide descriptions into a spe-cific number of groups."
"On the one hand, their method is expected to en-hance existing encyclopedias, where vocabulary size is relatively limited, and therefore the quantity prob-lems has been resolved."
"On the other hand, encyclopedias extracted from the Web are not comparable with existing ones in terms of quality."
"In hand-crafted encyclopedias, term descrip-tions are carefully organized based on domains and word senses, which are especially effective for human usage."
"However, the output of Fujii’s method is simply a set of unorganized term descriptions."
"Although clus-tering is optionally performed, resultant clusters are not necessarily related to explicit criteria, such as word senses and domains."
"To sum up, our belief is that by combining extrac-tion and organization methods, we can enhance both quantity and quality of Web-based encyclopedias."
"Motivated by this background, we introduce an or-ganization model to Fujii’s method and reformalize the whole framework."
"In other words, our proposed method is not only extraction but generation of ency-clopedic knowledge."
"Section 2 explains the overall design of our ency-clopedia generation system, and Section 3 elaborates on our organization model."
"Section 4 then explores a method for applying our resultant encyclopedia to NLP research, specifically, question answering."
Sec-tion 5 performs a number of experiments to evaluate our methods.
"Figure 1 depicts the overall design of our system, which generates an encyclopedia for input terms."
"Our system, which is currently implemented for Japanese, consists of three modules: “retrieval,” “ex-traction” and “organization,” among which the orga-nization module is newly introduced in this paper."
"In principle, the remaining two modules (“retrieval” and “extraction”) are the same as proposed[REF_CITE]."
"In Figure 1, terms can be submitted either on-line or off-line."
"A reasonable method is that while the system periodically updates the encyclopedia off-line, terms unindexed in the encyclopedia are dynamically pro-cessed in real-time usage."
"In either case, our system processes input terms one by one."
"We briefly explain each module in the following three sections, respectively."
"The retrieval module searches the Web for pages con-taining an input term, for which existing Web search engines can be used, and those with broad coverage are desirable."
"However, search engines performing query expan-sion are not always desirable, because they usually re-trieve a number of pages which do not contain an in-put keyword."
"Since the extraction module (see Sec-tion 2.3) analyzes the usage of the input term in re-trieved pages, pages not containing the term are of no use for our purpose."
"Thus, we use as the retrieval module “Google,” which is one of the major search engines and does not conduct query expansion 1 ."
"In the extraction module, given Web pages containing an input term, newline codes, redundant white spaces and HTML tags that are not used in the following pro-cesses are discarded to standardize the page format."
"Second, we approximately identify a region describ-ing the term in the page, for which two rules are used."
"The first rule is based on Japanese linguistic patterns typically used for term descriptions, such as “X toha Y dearu (X is Y).”"
"Following the method proposed[REF_CITE], we semi-automatically produced 20 patterns based on the Japanese CD-ROM World Encyclopedia[REF_CITE], which in-cludes approximately 80,000 entries related to various fields."
It is expected that a region including the sen-tence that matched with one of those patterns can be a term description.
The second rule is based on HTML layout.
"In a typ-ical case, a term in question is highlighted as a heading with tags such as &lt;DT&gt;, &lt;B&gt; and &lt;Hx&gt; (“x” denotes a digit), followed by its description."
"In some cases, terms are marked with the anchor &lt;A&gt; tag, providing hyperlinks to pages where they are described."
"Finally, based on the region briefly identified by the above method, we extract a page fragment as a term description."
"Since term descriptions usually consist of a logical segment (such as a paragraph) rather than a single sentence, we extract a fragment that matched with one of the following patterns, which are sorted according to preference in descending order: [URL_CITE]. description tagged with &lt;DD&gt; in the case where the term is tagged with &lt;DT&gt; 2 , [Footnote_2]. paragraph tagged with &lt;P&gt;, 3. itemization tagged with &lt;UL&gt;, 4. N sentences, where we empirically set N = 3."
2 &lt;DT&gt; and &lt;DD&gt; are inherently provided to describe terms in HTML.
"As discussed in Section 1, organizing information ex-tracted from the Web is crucial in our framework."
"For this purpose, we classify extracted term descriptions based on word senses and domains."
"Although a number of methods have been proposed to generate word senses (for example, one based on the vector space model[REF_CITE]), it is still difficult to accurately identify word senses without explicit dic-tionaries that define sense candidates."
"In addition, since word senses are often associated with domains[REF_CITE], word senses can be consequently distinguished by way of determining the domain of each description."
"For example, different senses for “pipeline (processing method/transportation pipe)” are associated with the computer and construc-tion domains (fields), respectively."
"To sum up, the organization module classifies term descriptions based on domains, for which we use do-main and description models."
"In Section 3, we elabo-rate on our organization model."
"Given one or more (in most cases more than one) descriptions for a single input term, the organization module selects appropriate description(s) for each do-main related to the term."
"We do not need all the extracted descriptions as fi-nal outputs, because they are usually similar to one another, and thus are redundant."
"For the moment, we assume that we know a priori which domains are related to the input term."
"From the viewpoint of probability theory, our task here is to select descriptions with greater probability for given domains."
"The probability for description d given domain c, P(d|c), is commonly transformed as in Equation (1), through use of the Bayesian theorem."
P(d|c) = P(c|d) ·
P(d) (1) P(c)
"In practice, P(c) can be omitted because this factor is a constant, and thus does not affect the relative proba-bility for different descriptions."
"In Equation (1), P(c|d) models a probability that d corresponds to domain c. P(d) models a probability that d can be a description for the term in question, disregarding the domain."
"We shall call them domain and description models, respectively."
"To sum up, in principle we select d’s that are strongly associated with a specific domain, and are likely to be descriptions themselves."
Extracted descriptions are not linguistically under-standable in the case where the extraction process is unsuccessful and retrieved pages inherently contain non-linguistic information (such as special characters and e-mail addresses).
"To resolve this problem,[REF_CITE]used a language model to filter out descriptions with low perplexity."
"However, in this paper we integrated a description model, which is practically the same as a language model, with an organization model."
The new framework is more understandable with respect to probability theory.
"In practice, we first use Equation (1) to compute P(d|c) for all the c’s predefined in the domain model."
Then we discard such c’s whose P(d|c) is below a spe-cific threshold.
"As a result, for the input term, related domains and descriptions are simultaneously selected."
"Thus, we do not have to know a priori which domains are related to each term."
"In the following two sections, we explain methods to realize the domain and description models, respec-tively."
"The domain model quantifies the extent to which de-scription d is associated with domain c, which is fun-damentally a categorization task."
"Among a number of existing categorization methods, we experimentally used one proposed[REF_CITE], which formulates P(c|d) as in Equation (2)."
P(t|c) · P(t|d) P(c|d) =
P(c) · (2) t P(t)
"Here, P(t|d), P(t|c) and P(t) denote probabilities that word t appears in d, c and all the domains, respec-tively."
We regard P(c) as a constant.
"While P(t|d) is simply a relative frequency of t in d, we need prede-fined domains to compute P(t|c) and P(t)."
"For this purpose, the use of large-scale corpora annotated with domains is desirable."
"However, since those resources are prohibitively expensive, we used the “Nova” dictionary for Japanese/English machine translation systems [Footnote_3] , which includes approximately one million entries related to 19 technical fields as listed below: aeronautics, biotechnology, business, chem-istry, computers, construction, defense, ecology, electricity, energy, finance, law, mathematics, mechanics, medicine, metals, oceanography, plants, trade."
"3 Produced by NOVA, Inc."
"We extracted words from dictionary entries to esti-mate P(t|c) and P(t), which are relative frequencies of t in c and all the domains, respectively."
We used the ChaSen morphological analyzer[REF_CITE]to extract words from Japanese entries.
We also used English entries because Japanese descriptions of-ten contain English words.
"It may be argued that statistics extracted from dic-tionaries are unreliable, because word frequencies in real word usage are missing."
"However, words that are representative for a domain tend to be frequently used in compound word entries associated with the domain, and thus our method is a practical approximation."
The description model quantifies the extent to which a given page fragment is feasible as a description for the input term.
"In principle, we decompose the description model into language and quality properties, as shown in Equation (3)."
"P(d) = P L (d) · P Q (d) (3) Here, P L (d) and P Q (d) denote language and quality models, respectively."
It is expected that the quality model discards in-correct or misleading information contained in Web pages.
"For this purpose, a number of quality rating methods for Web pages[REF_CITE]can be used."
"However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality[REF_CITE], we tenta-tively regarded P Q (d) as a constant."
"Thus, in practice the description model is approximated solely with the language model as in Equation (4)."
P(d) ≈ P L (d) ([Footnote_4])
4 Japan Information-Technology Engineers Examination Center.[URL_CITE]
"Statistical approaches to language modeling have been used in much NLP research, such as machine translati[REF_CITE]and speech recogni-ti[REF_CITE]."
"Our model is almost the same as existing models, but is different in two respects."
"First, while general language models quantify the extent to which a given word sequence is linguisti-cally acceptable, our model also quantifies the extent to which the input is acceptable as a term description."
"Thus, we trained the model based on an existing ma-chine readable encyclopedia."
"We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclope-dia[REF_CITE]into words (we replaced head-words with a common symbol), and then used the CMU-Cambridge toolkit[REF_CITE]to model a word-based trigram."
"Consequently, descriptions in which word se-quences are more similar to those in the World En-cyclopedia are assigned greater probability scores through our language model."
"Second, P(d), which is a product of probabilities for N-grams in d, is quite sensitive to the length of d. In the cases of machine translation and speech recog-nition, this problem is less crucial because multiple candidates compared based on the language model are almost equivalent in terms of length."
"However, since in our case length of descriptions are significantly different, shorter descriptions are more likely to be selected, regardless of the quality."
"To avoid this problem, we normalize P(d) by the number of words contained in d."
"Encyclopedias generated through our Web-based method can be used in a number of applications, in-cluding human usage, thesaurus producti[REF_CITE]and natural lan-guage understanding in general."
"Among the above applications, natural language un-derstanding (NLU) is the most challenging from a sci-entific point of view."
"Current practical NLU research includes dialogue, information extraction and question answering, among which we focus solely on question answering (QA) in this paper."
A straightforward application is to answer inter-rogative questions like “What is X?” in which a QA system searches the encyclopedia database for one or more descriptions related to X (this application is also effective for dialog systems).
"In general, the performance of QA systems are eval-uated based on coverage and accuracy."
Coverage is the ratio between the number of questions answered (disregarding their correctness) and the total number of questions.
Accuracy is the ratio between the num-ber of correct answers and the total number of answers made by the system.
"While coverage can be estimated objectively and systematically, estimating accuracy relies on human subjects (because there is no absolute description for term X), and thus is expensive."
"In view of this problem, we targeted Information Technology Engineers Examinations 4 , which are bian-nual (spring and autumn) examinations necessary for candidates to qualify to be IT engineers in Japan."
"Among a number of classes, we focused on the “Class II” examination, which requires fundamental and general knowledge related to information technol-ogy."
Approximately half of questions are associated with IT technical terms.
"Since past examinations and answers are open to the public, we can evaluate the performance of our QA system with minimal cost."
"The Class II examination consists of quadruple-choice questions, among which technical term questions can be subdivided into two types."
"In the first type of question, examinees choose the most appropriate description for a given technical term, such as “memory interleave” and “router.”"
"In the second type of question, examinees choose the most appropriate term for a given question, for which we show examples collected from the exami-nation in the autumn of 1999 (translated into English by one of the authors) as follows: 1. Which data structure is most appropriate for FIFO (First-In First-Out)? a) binary trees, b) queues, c) stacks, d) heaps 2. Choose the LAN access method in which mul-tiple terminals transmit data simultaneously and thus they potentially collide. a) ATM, b) CSM/CD, c) FDDI, d) token ring"
"In the autumn of 1999, out of 80 questions, the num-ber of the first and second types were 22 and 18, re-spectively."
"For the first type of question, human examinees would search their knowledge base (i.e., memory) for the de-scription of a given term, and compare that description with four candidates."
Then they would choose the can-didate that is most similar to the description.
"For the second type of question, human examinees would search their knowledge base for the description of each of four candidate terms."
Then they would choose the candidate term whose description is most similar to the question description.
The mechanism of our QA system is analogous to the above human methods.
"However, unlike human examinees, our system uses an encyclopedia generated from the Web as a knowledge base."
"In addition, our system selectively uses term de-scriptions categorized into domains related to infor-mation technology."
"In other words, the description of “pipeline (transportation pipe)” is irrelevant or mis-leading to answer questions associated with “pipeline (processing method).”"
"To compute the similarity between two descriptions, we used techniques developed in IR research, in which the similarity between a user query and each document in a collection is usually quantified based on word fre-quencies."
"In our case, a question and four possible answers correspond to query and document collection, respectively."
"We used a probabilistic method[REF_CITE], which is one of the major IR methods."
"To sum up, given a question, its type and four choices, our QA system chooses one of four candi-dates as the answer, in which the resolution algorithm varies depending on the question type."
"Motivated partially by the TREC-8 QA collec-ti[REF_CITE], question answering has of late become one of the major topics within the NLP/IR communities."
"In fact, a number of QA systems targeting the TREC QA collection have recently been pro-posed[REF_CITE]."
"Those sys-tems are commonly termed “open-domain” systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why."
"However,[REF_CITE]found that each of the TREC questions can be recast as ei-ther a single axis or a combination of axes."
"They also found that out of the 200[REF_CITE]ques-tions (approximately one third) were associated with the what axis, for which the Web-based encyclopedia is expected to improve the quality of answers."
"The use of encyclopedic knowledge for QA systems, as we demonstrated, needs to be further explored."
We conducted a number of experiments to investigate the effectiveness of our methods.
"First, we generated an encyclopedia by way of our Web-based method (see Sections 2 and 3), and evalu-ated the quality of the encyclopedia itself."
"Second, we applied the generated encyclopedia to our QA system (see Section 4), and evaluated its per-formance."
The second experiment can be seen as a task-oriented evaluation for our encyclopedia genera-tion method.
"In the first experiment, we collected 96 terms from technical term questions in the Class II examination (the autumn of 1999)."
"We used as test inputs those 96 terms and generated an encyclopedia, which was used in the second experiment."
"For all the 96 test terms, Google (see Section 2.2) retrieved a positive number of pages, and the average number of pages for one term was 196,503."
"Since Google practically outputs contents of the top 1,000 pages, the remaining pages were not used in our ex-periments."
"In the following two sections, we explain the first and second experiments, respectively."
"For each test term, our method first computed P(d|c) using Equation (1) and discarded domains whose P(d|c) was below 0.05."
"Then, for each remaining do-main, descriptions with higher P(d|c) were selected as the final outputs."
"We selected the top three (not one) descriptions for each domain, because reading a couple of descriptions, which are short paragraphs, is not laborious for human users in real-world usage."
"As a result, at least one de-scription was generated for 85 test terms, disregarding the correctness."
The number of resultant descriptions was 326 (3.8 per term).
We analyzed those descrip-tions from different perspectives.
"First, we analyzed the distribution of the Google ranks for the Web pages from which the top three de- scriptions were eventually retained."
"Figure 2 shows the result, where we have combined the pages in groups of 50, so that the leftmost bar, for example, de-notes the number of used pages whose original Google ranks ranged from 1 to 50."
"Although the first group includes the largest number of pages, other groups are also related to a relatively large number of pages."
"In other words, our method exploited a number of low ranking pages, which are not browsed or utilized by most Web users. of correct descriptions, disregarding the domain cor-rectness, was 58.0% (189/326), and the ratio of cor-rect descriptions categorized into the correct domain was 47.9% (156/326)."
"However, since all the test terms are inherently re-lated to the IT field, we focused solely on descriptions categorized into the computer domain."
"In this case, the ratio of correct descriptions, disregarding the do-main correctness, was 62.0% (124/200), and the ratio of correct descriptions categorized into the correct do-main was 61.5% (123/200)."
"Second, we analyzed the distribution of domains assigned to the 326 resultant descriptions."
"Figure 3 shows the result, in which, as expected, most descrip-tions were associated with the computer domain."
"However, the law domain was unexpectedly asso-ciated with a relatively great number of descriptions."
"We manually analyzed the resultant descriptions and found that descriptions for which appropriate domains are not defined in our domain model, such as sports, tended to be categorized into the law domain."
"Third, we evaluated the accuracy of our method, that is, the quality of an encyclopedia our method gen-erated."
"For this purpose, each of the resultant descrip-tions was judged as to whether or not it is a correct de-scription for a term in question."
Each domain assigned to descriptions was also judged correct or incorrect.
"We analyzed the result on a description-by-description basis, that is, all the generated descriptions were considered independent of one another."
"In addition, we analyzed the result on a term-by-basis, because reading only a couple of descrip-is not crucial."
"In other words, we evaluated term (not description), and in the case where at one correct description categorized into the cor-domain was generated for a term in question, we it correct."
"The ratio of correct terms was 89.4% 76/85), and in the case where we focused solely on the domain, the ratio was 84.8% (67/79)."
"In other words, by reading a couple of descriptions 3.8 descriptions per term), human users can obtain of approximately 90% of input terms."
"Finally, we compared the resultant descriptions with an existing dictionary."
"For this purpose, we used the “Nichigai” computer dictionary[REF_CITE], which lists approximately 30,000 Japanese technical terms related to the computer field, and con-tains descriptions for 13,588 terms."
In the[REF_CITE]out of the 96 test terms were described.
"Our method, which generated correct descriptions as-sociated with the computer domain for 67 input terms, enhanced the Nichigai dictionary in terms of quantity."
These results indicate that our method for generat-ing encyclopedias is of operational quality.
"We used as test inputs 40 questions, which are related to technical terms collected from the Class II exami-nation in the autumn of 1999."
"The objective here is not only to evaluate the perfor-mance of our QA system itself, but also to evaluate the quality of the encyclopedia generated by our method."
"Thus, as performed in the first experiment (Sec-tion 5.2), we used the Nichigai computer dictionary as a baseline encyclopedia."
"We compared the following three different resources as a knowledge base: • the Nichigai dictionary (“Nichigai”), • the descriptions generated in the first experiment (“Web”), • combination of both resources (“Nichigai + Web”)."
"Table 1 shows the result of our comparative exper-iment, in which “C” and “A” denote coverage and ac-curacy, respectively, for variations of our QA system."
"Since all the questions we used are quadruple-choice, in case the system cannot answer the question, random choice can be performed to improve the cov-erage to 100%."
"Thus, for each knowledge resource we compared cases without/with random choice, which are denoted “w/o Random” and “w/ Random” in Ta-ble 1, respectively."
"In the case where random choice was not per-formed, the Web-based encyclopedia noticeably im-proved the coverage for the Nichigai dictionary, but decreased the accuracy."
"However, by combining both resources, the accuracy was noticeably improved, and the coverage was comparable with that for the Nichi-gai dictionary."
"On the other hand, in the case where random choice was performed, the Nichigai dictionary and the Web-based encyclopedia were comparable in terms of both the coverage and accuracy."
"Additionally, by combin-ing both resources, the accuracy was further improved."
We also investigated the performance of our QA system where descriptions related to the computer do-main are solely used.
"However, coverage/accuracy did not significantly change, because as shown in Figure 3, most of the descriptions were inherently related to the computer domain."
"The World Wide Web has been an unprecedentedly enormous information source, from which a number of language processing methods have been explored to extract, retrieve and discover various types of infor-mation."
"In this paper, we aimed at generating encyclopedic knowledge, which is valuable for many applications including human usage and natural language under-standing."
"For this purpose, we reformalized an exist-ing Web-based extraction method, and proposed a new statistical organization model to improve the quality of extracted data."
"Given a term for which encyclopedic knowledge (i.e., descriptions) is to be generated, our method se-quentially performs a) retrieval of Web pages contain- ing the term, b) extraction of page fragments describ-ing the term, and c) organizing extracted descriptions based on domains (and consequently word senses)."
"In addition, we proposed a question answering sys-tem, which answers interrogative questions associated with what, by using a Web-based encyclopedia as a knowledge base."
"For the purpose of evaluation, we used as test inputs technical terms collected from the Class II IT engineers examination, and found that the encyclopedia generated through our method was of operational quality and quantity."
"We also used test questions from the Class II exam-ination, and evaluated the Web-based encyclopedia in terms of question answering."
We found that our Web-based encyclopedia improved the system coverage ob-tained solely with an existing dictionary.
"In addition, when we used both resources, the performance was further improved."
"Future work would include generating information associated with more complex interrogations, such as ones related to how and why, so as to enhance Web-based natural language understanding."
"Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a cor-rect word sense disambiguation."
One way to deal with this problem within the statistical framework is to use max-imum entropy methods.
"In this paper, we present how to use this type of in-formation within a statistical machine translation system."
We show that it is possible to significantly decrease train-ing and test corpus perplexity of the translation models.
"In addition, we per-form a rescoring of -Best lists us-ing our maximum entropy model and thereby yield an improvement in trans-lation quality."
Experimental results are presented on the so-called “Verbmobil Task”.
"Typically, the lexicon models used in statistical machine translation systems are only single-word based, that is one word in the source language cor-responds to only one word in the target language."
Those lexicon models lack from context infor-mation that can be extracted from the same paral-lel corpus.
This additional information could be:
Simple context information: information of the words surrounding the word pair;
"Syntactic information: part-of-speech in-formation, syntactic constituent, sentence mood;"
"Semantic information: disambiguation in-formation (e.g. from WordNet), cur-rent/previous speech or dialog act."
To include this additional information within the statistical framework we use the maximum en-tropy approach.
"This approach has been applied in natural language processing to a variety of tasks.[REF_CITE]applies this approach to the so-called IBM Candide system to build con-text dependent models, compute automatic sen-tence splitting and to improve word reordering in translation."
Similar techniques are used[REF_CITE]for so-called direct translation models instead of those proposed[REF_CITE].[REF_CITE]describes two methods for incorporating informa-tion about the relative position of bilingual word pairs into a maximum entropy translation model.
Other authors have applied this approach to lan-guage modeling[REF_CITE].
A short review of the maximum entropy approach is outlined in Section 3.
The goal of the translation process in statisti-cal machine translation can be formulated as fol-  lows: A source language string  is to be translated into a target language string  .
"In the experiments reported in this paper, the source language is German and the target language is English."
Every target string is considered as a possible translation for the input.
"If we assign a probability    to each pair of strings  , then according to Bayes’ de-cision rule, we have to choose the target string that maximizes the product of the target language model  and the string translation model  ."
Many existing systems for statistical machine translati[REF_CITE]make use of a special way of structuring the string translation model like proposed[REF_CITE]: The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position.
The lexicon prob-ability  of a certain target word to occur in the target string is assumed to depend basically only on the source word aligned to it.
These alignment models are similar to the con-cept of Hidden Markov models (HMM) in speech recognition.
The alignment mapping is  &quot;! # from source position to target position $%! # .
The alignment !  may contain align-ments ! %#&amp;(&apos; with the ‘empty’ word ) to ac-count for source words that are not aligned to any target word.
"In (statistical) alignment models   !  , the alignment !  is introduced as a hidden variable."
"Typically, the search is performed using the so-called maximum approximation: 7 * + ., -01/ 3546+ !   &lt;: &gt;; 6= % ? 7   /@; 6=+ 2  !  + ., -01/ 3 +46 % ?"
The search space consists of the set of all possible target language strings and all possible align-ments !  .
The overall architecture of the statistical trans-lation approach is depicted in Figure 1.
The translation probability  !  can be rewritten as follows:
C ED  # ! # # H  !   ! # H  # C ED   ! # # H ! # H  #  # # ! #
"Typically, the probability  # # H ! # M; L is approximated by a lexicon model # H # # by dropping the dependencies on , ! , and ."
"Obviously, this simplification is not true for a lot of natural language phenomena."
The straightfor-ward approach to include more dependencies in the lexicon model would 5L; be ;ML. to N 6 add additional de-pendencies(e.g.  # ).
This approach would yield a significant data sparseness problem.
"Here, the role of maximum entropy (ME) is to build a stochastic model that efficiently takes a larger context into account."
"In the following, we will use   to denote the probability that the ME model assigns to in the context O in order to distinguish this model from the basic lexicon model  ."
In the maximum entropy approach we describe all properties that we feel are useful by so-called feature functions  O .
"For example, if we want to model the existence or absence of a spe-cific word R in the context of an English word which has the translation we can express this dependency using the following feature function: 7XW R and RZY O Q 3TSBUV3MU O if &apos; (1) otherwise"
The ME principle suggests that the optimal parametric form of a model   taking into account W ._ onlyis giventhebyfeature: functions \Q [ ^]
W   ` OZ 2cb Ie:[ dD  gQ[ \[ O
Here `  is a normalization factor.
The re-sulting model has an exponential W ._ .
Theformparameterwith free values which maximize the likelihood for a givenparameters f [ ^] training corpus can be computed with the so-called GIS algorithm (general iterative scaling) or its improved version IIS[REF_CITE].
It is important to notice that we will have to ob-tain one ME model for each target word observed in the training data.
"In order to train the ME model 3   associated to a target word , we need to construct a corre-sponding training sample from the whole bilin-gual corpus depending on the contextual informa-tion that we want to use."
"To construct this sample, we need to know the word-to-word alignment be-tween each sentence pair within the corpus."
That is obtained using the Viterbi alignment provided by a translation model as described[REF_CITE].
"Specifically, we use the Viterbi align-ment that was produced by Model 5."
"We use the program GIZA++[REF_CITE], which is an extension of the training program available in EGYPT[REF_CITE]."
The authors propose as context the 3 words to the left and the 3 words to the right of the target word.
In this work we use the following contextual information:
Target context: As[REF_CITE]we consider a window of 3 words to the left and to the right of the target word considered.
"Source context: In addition, we consider a window of 3 words to the left of the source word which is connected to according to the Viterbi alignment."
Word classes: Instead of using a dependency on the word identity we include also a de-pendency on word classes.
"By doing this, we improve the generalization of the models and include some semantic and syntactic infor-mation with."
The word classes are computed automatically using another statistical train-ing procedure[REF_CITE]which often pro-duces word classes including words with the same semantic meaning in the same class.
"A training event, for a specific target word , is composed by three items:"
The source word aligned to .
The context in which the aligned pair appears.
The number of occurrences of the event in the training corpus.
Table 1 shows some examples of training events for the target word “which”.
Once we have a set of training events for each tar-get word we need to describe our feature func-tions.
We do this by first specifying a large pool of possible features and then by selecting a subset of “good” features from this pool.
All the features we consider form a triple (  label-1 label-2) where: pos: is the position that label-2 has in a spe-cific context. label-1: is the source word of the aligned word pair or the word class of the source word (  ). label-2: is one word of the aligned word pair or the word class to which these words belong (   ).
Using this notation and given a context O :
"O n H\o pp  pp   o # \o pp # for the word pair n # , we use the following categories of features: 1. ( &apos; # ) 2. ( } W # R ) and R nr~ 3. ( } # R ) and RY n H\o pp rn q o% 4. ( } W  #  R ) and R nr~ 5. ( }  #  R ) and R Y  n \H o pp  og 6. (  W R ) and R# # 7. (  # R ) and R Y  # \o pp # H  8. (  W  #  R ) and R # H 9. (   #  R ) and RY # \o p # H "
Category 1 features depend only on the source word # and the target word  .
"A ME model that uses only those, predicts each source translation # with the probability  3  # determined by the empirical data."
This is exactly the standard lex-icon probability  employed in the transla-tion model described[REF_CITE]and in Section 2.
Category 2 describes features which depend in addition on the word R one position to the left or to the right of  .
The same explanation is valid for category 3 but in this case R could appears in any position of the context O .
Categories 4 and 5 are the analogous categories to 2 and 3 using word classes instead of words.
"In the categories 6, 7, 8 and 9 the source context is used instead of the target context."
Table 2 gives an overview of the different feature categories.
Examples of specific features and their respec-tive category are shown in Table 3.
Table 3:[REF_CITE]most important features and their word “which”.respective category and f values for the English
The number of possible features that can be used according to the German and English vocabular-ies and word classes is huge.
"In order to re-duce the number of features we perform a thresh-old based feature selection, that is every feature which occurs less than  times is not used."
The aim of the feature selection is two-fold.
"Firstly, we obtain smaller models by using less features, and secondly, we hope to avoid overfitting on the training data."
In order to obtain the threshold  we compare the test corpus perplexity for various thresholds.
The different threshold used in the experiments range from 0 to 512.
The threshold is used as a cut-off for the number of occurrences that a spe-cific feature must appear.
So a cut-off of 0 means that all features observed in the training data are used.
A cut-off of 32 means those features that appear 32 times or more are considered to train the maximum entropy models.
We select the English words that appear at least 150 times in the training sample which are in total 348 of the 4673 words contained in the English vocabulary.
Table 4 shows the different number of features considered for the 348 English words selected using different thresholds.
In choosing a reasonable threshold we have to balance the number of features and observed per-plexity.
Table 4: Number of features used according to different cut-off threshold.
In the second column of the table are shown the number of features used when only the English context is considered.
"The third column correspond to English, German and Word-Classes contexts."
"The “Verbmobil Task” is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation."
The task is dif-ficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable.
For the rescor-ing experiments we use the corpus described in Table 5.
To train the maximum entropy models we used the “Ristad ME Toolkit” described[REF_CITE].
"In order to compute the training and test perplex-ities, we split the whole aligned training corpus in two parts as shown in Table 6."
The training and test perplexities are shown in Table 7.
"As expected, the perplexity reduction in the test cor-pus is lower than in the training corpus, but in both cases better perplexities are obtained using the ME models."
The best value is obtained when a threshold of 4 is used.
We expected to observe strong overfitting ef-fects when a too small cut-off for features gets used.
"Yet, for most words the best test corpus perplexity is observed when we use all features including those that occur only once."
Table 7: Training and Test perplexities us-ing different contextual information and different thresholds  .
The reference perplexities obtained with the basic translation model 5 are TrainPP = 10.38 and TestPP = 13.22.
In order to make use of the ME models in a statis-tical translation system we implemented a rescor-ing algorithm.
This algorithm take as input the standard lexicon model (not using maximum en-tropy) and the 348 models obtained with the ME training.
For an hypothesis sentence and a cor-responding alignment !   the algorithm modifies the score   !   according to the refined maximum entropy lexicon model.
We carried out some preliminary experiments with the -best lists of hypotheses provided by the translation system in order to make a rescor-ing of each i-th hypothesis and reorder the list ac-cording to the new score computed with the re-fined lexicon model.
"Unfortunately, our -best extraction algorithm is sub-optimal, i.e. not the translations are extracted W ."
"In addition, true best so far we had to use a limit of only &apos; translations per sentence."
"Therefore, the results of the transla-tion experiments are only preliminary."
For the evaluation of the translation quality we use the automatically computable Word Er-ror Rate (WER).
The WER corresponds to the edit distance between the produced translation and one predefined reference translation.
A short-coming of the WER is the fact that it requires a perfect word order.
"This is particularly a prob-lem for the Verbmobil task, where the word or-der of the German-English sentence pair can be quite different."
"As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone can be misleading."
"In order to overcome this problem, we introduce as additional measure the position-independent word error rate (PER)."
This measure compares the words in the two sen-tences without taking the word order into account.
"Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or dele-tion errors in addition to substitution errors."
The PER is guaranteed to be less than or equal to the WER.
We use the top-10 list of hypothesis provided by the translation system described[REF_CITE]for rescoring the hypothesis us-ing the ME models and sort them according to the new maximum entropy score.
The translation re-sults in terms of error rates are shown in Table 8.
We use Model 4 in order to perform the transla-tion experiments because Model 4 typically gives better translation results than Model 5.
We see that the translation quality improves slightly with respect to the WER and PER.
The translation quality improvements so far are quite small compared to the perplexity measure im-provements.
We attribute this to the fact that the algorithm for computing the -best lists is sub-optimal.
Table 8: Preliminary translation results for the Verbmobil[REF_CITE]for different contextual infor-mation and different thresholds using the top-10 translations.
The baseline translation results for model 4 are WER=54.80 and PER=43.07.
Table 9 shows some examples where the trans-lation obtained with the rescoring procedure is better than the best hypothesis provided by the translation system.
We have developed refined lexicon models for statistical machine translation by using maximum entropy models.
We have been able to obtain a significant better test corpus perplexity and also a slight improvement in translation quality.
We be-lieve that by performing a rescoring on translation word graphs we will obtain a more significant im-provement in translation quality.
For the future we plan to investigate more re-fined feature selection methods in order to make the maximum entropy models smaller and better generalizing.
"In addition, we want to investigate more syntactic, semantic features and to include features that go beyond sentence boundaries."
"While the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps, the axiomatic view eliminates in-valid linguistic structures out of a set of possible structures by means of well-formedness principles."
"We present a generator based on the axiomatic view and argue that when combined with a TAG-like grammar and a flat seman-tics, this axiomatic view permits avoid-ing drawbacks known to hold either of top-down or of bottom-up generators."
We take the axiomatic view of language and show that it yields an interestingly new perspective on the tactical generation task i.e. the task of produc-ing from a given semantics a string with seman-tics .
"As (Cornell and Rogers, To appear) clearly shows, there has recently been a surge of interest in logic based grammars for natural language."
"In this branch of research sometimes referred to as “Model Theoretic Syntax”, a grammar is viewed as a set of axioms defining the well-formed struc-tures of natural language."
The motivation for model theoretic grammars is initially theoretical: the use of logic should sup-port both a more precise formulation of grammars and a different perspective on the mathematical and computational properties of natural language.
But eventually the question must also be ad-dressed of how such grammars could be put to work.
One obvious answer is to use a model gen-erator.
"Given a logical formula , a model genera- tor is a program which builds some of the models satisfying this formula."
"Thus for parsing, a model generator can be used to enumerate the (minimal) model(s), that is, the parse trees, satisfying the conjunction of the lexical categories selected on the basis of the input string plus any additional constraints which might be encoded in the gram-mar."
"And similarly for generation, a model gener-ator can be used to enumerate the models satisfy-ing the bag of lexical items selected by the lexical look up phase on the basis of the input semantics."
How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars?[REF_CITE]shows that constraint programming can be used to im-plement a model generator for tree logic[REF_CITE].
"Further,[REF_CITE]shows that this model generator can be used to parse with descriptions based grammars[REF_CITE]that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic."
"In this paper, we build[REF_CITE]and show that modulo some minor modi-fications, the same model generator can be used to generate with description based grammars."
We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms.
"In specific, we argue that the change of perspective offered by the constraint-based, axiomatic approach to pro-cessing presents some interesting differences with the more traditional generative approach usually pursued in tactical generation and further, that the combination of this static view with a TAG-like grammar and a flat semantics results in a system which combines the positive aspects of both top- down and bottom-up generators."
The paper is structured as follows.
"Sec-tion 2 presents the grammars we are working with namely, Description Grammars (DG), Sec-tion 3 summarises the parsing model presented[REF_CITE]and Section 4 shows that this model can be extended to generate with DGs."
"In Section 5, we compare our generator with top-down and bottom-up generators, Section 6 reports on a proof-of-concept implementation and Section 7 concludes with pointers for further research."
There is a range of grammar formalisms which depart from Tree Adjoining Grammar (TAG) by taking as basic building blocks tree descriptions rather than trees.
D-Tree Grammar (DTG) is pro-posed[REF_CITE]to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced[REF_CITE]to support syntactic and se-mantic underspecification and Interaction Gram-mar is presented[REF_CITE]as an alterna-tive way of formulating linear logic grammars.
"Like all these frameworks, DG uses tree de-scriptions and thereby benefits first, from the ex-tended domain of locality which makes TAG par-ticularly suitable for generation (cf.[REF_CITE]) and second, from the monotonicity which differ-entiates descriptions from trees with respect to ad-junction (cf.[REF_CITE])."
"DG differs from DTG and TDG however in that it adopts an axiomatic rather than a genera-tive view of grammar: whereas in DTG and TDG, derived trees are constructed through a sequence of rewriting steps, in DG derived trees are mod-els satisfying a conjunction of elementary tree de-scriptions."
"Moreover, DG differs from Interaction Grammars in that it uses a flat rather than a Mon-tague style recursive semantics thereby permitting a simple syntax/semantics interface (see below)."
A Description Grammar is a set of lexical en-tries of the form  where is a tree descrip-tion and is the semantic representation associ-ated with .
A tree description is a con-junction of literals that specify either the label of a node or the position of a node relative to other nodes.
"As a logical notation quickly be-comes unwieldy, we use graphics instead."
Fig-ure 1 gives a graphic representation of a small DG fragment.
The following conventions are used.
"Nodes represent node variables, plain edges strict dominance and dotted edges dominance."
"The la-bels of the nodes abbreviate a feature structure, e.g. the label NP: Z represents the feature struc-ture []\ `^ _! ahgji , while the anchor represents the clkWm!b value in the feature structure of the im-mediately dominating node variable."
"Node variables can have positive, negative or neutral polarity which are represented by black, white and gray nodes respectively."
"Intuitively, a negative node variable can be thought of as an open valency which must be filled exactly once by a positive node variable while a neutral node variable is a variable that may not be identified with any other node variable."
"Formally, polari-ties are used to define the class of saturated mod-els."
"A saturated model n for a tree description (written n op S ) is a model in which each nega-tive node variable is identified with exactly one positive node variable, each positive node vari-able with exactly one negative node variable and neutral node variables are not identified with any other node variable."
"Intuitively, a saturated model for a given tree description is the smallest tree sat-isfying this description and such that all syntactic valencies are filled."
"In contrast, a free model n for (written, n p F ) is a model such that ev- o ery node in that model interprets exactly one node variable in ."
"In DG, lexical tree descriptions must obey the following conventions."
"First, the polarities are used in a systematic way as follows."
"Roots of fragments (fully specified subtrees) are always positive; except for the anchor, all leaves of frag-ments are negative, and internal node variables are neutral."
"This guarantees that in a saturated model, tree fragments that belong to the denota-tion of distinct tree descriptions do not overlap."
"Second, we require that every lexical tree descrip-tion has a single minimal free model, which es-sentially means that the lexical descriptions must be tree shaped."
"Following[REF_CITE], we represent meaning using a flat semantic representation, i.e. as multisets, or con-junctions, of non-recursive propositions."
This treatment offers a simple syntax-semantics inter-face in that the meaning of a tree is just the con-junction of meanings of the lexical tree descrip-tions used to derive it once the free variables oc-curring in the propositions are instantiated.
A free variable is instantiated as follows: each free vari-able labels a syntactic node variable  and is uni-fied with the label of any node variable identified with  .
"For the purpose of this paper, a simple se-mantic representation language is adopted which in particular, does not include “handles” i.e. la-bels on propositions."
"For a wider empirical cov-erage including e.g. quantifiers, a more sophisti-cated version of flat semantics can be used such as Minimal Recursion Semantics[REF_CITE]."
"Parsing with DG can be formulated as a model generation problem, the task of finding models satisfying a give logical formula."
"If we restrict our attention to grammars where every lexical tree description has exactly one anchor and (unreal-istically) assuming that each word is associated with exactly one lexical entry, then parsing a sen-tence   consists in finding the saturated model(s) n with yield   such that n sat-isfies the conjunction ¡ of lexical tree descriptions   with the tree description associ-ated with the word  by the grammar."
Figure 2 illustrates this idea for the sentence “John loves Mary”.
"The tree on the right hand side represents the saturated model satisfying the conjunction of the descriptions given on the left and obtained from parsing the sentence “John sees Mary” (the isolated negative node variable, the “R OOT description”, is postulated during parsing to cancel out the negative polarity of the top-most S-node in the parse tree)."
The dashed lines between the left and the right part of the fig-ure schematise the interpretation function: it indi-cates which node variables gets mapped to which node in the model.
"As[REF_CITE]shows however, lexical ambiguity means that the parsing problem is in fact more complex as it in effect requires that models be searched for that satisfy a conjunction of disjunctions (rather than simply a conjunction) of lexical tree descriptions."
The constraint based encoding of this problem presented[REF_CITE]can be sketched as follows [Footnote_1] .
"1 For a detailed presentation of this constraint based en-coding, see the paper itself."
"To start with, the conjunc-tion of disjunctions of descriptions obtained on the basis of the lexical lookup is represented as a matrix, where each row corresponds to a word from the input (except for the first row which is filled with the above mentioned R OOT descrip-tion) and columns give the lexical entries asso-ciated by the grammar with these words."
Any matrix entry which is empty is filled with the for-mula _ #£¡¤ which is true in all models.
Figure 3 shows an example parsing matrix for the string “John saw Mary” given the grammar in Figure 1. 2
"Given such a matrix, the task of parsing con- sists in: 1. selecting exactly one entry per row thereby producing a conjunction of selected lexical entries, [Footnote_2]. building a saturated model for this conjunc-tion of selected entries such that the yield of that model is equal to the input string and 3. building a free model for each of the remain-ing (non selected) entries."
"2 For lack of space in the remainder of the paper, we omit the R OOT description in the matrices."
The important point about this way of formu-lating the problem is that it requires all constraints imposed by the lexical tree descriptions occurring in the matrix to be satisfied (though not neces-sarily in the same model).
This ensures strong constraint propagation and thereby reduces non-determinism.
"In particular, it avoids the combina-torial explosion that would result from first gener-ating the possible conjunctions of lexical descrip-tions out of the CNF obtained by lexical lookup and second, testing their satisfiability."
"We now show how the parsing model just de-scribed can be adapted to generate from some se-mantic representation , one or more sentence(s) with semantics ."
The parsing model outlined in the previous sec-tion can directly be adapted for generation as fol-lows.
"First, the lexical lookup is modified such that propositions instead of words are used to de-termine the relevant lexical tree descriptions: a lexical tree description is selected if its seman-tics subsumes part of the input semantics."
"Sec-ond, the constraint that the yield of the saturated model matches the input string is replaced by a constraint that the sum of the cardinalities of the multisets of propositions associated with the lex-ical tree descriptions composing the solution tree equals the cardinality of the input semantics."
"To-gether with the above requirement that only lexi-cal entries be selected whose semantics subsumes part of the goal semantics, this ensures that the se-mantics of the solution trees is identical with the input semantics."
The following simple example illustrates this idea.
Suppose the input semantics is `bl^ !k/b¨ª#^!¢^ #¯¡ª##[ and the grammar is as given in Figure 1.
The generating matrix then is: ! #&quot;%&apos;$ &amp; .  35!$ UV$ $ &amp; W? ( *²+. 7 :  ! #&quot;%&lt; $&apos;( &amp;W?
YG( . ²¸­¹²º »  53 ! $ G(#G
"Given this generating matrix, two matrix mod-els will be generated, one with a saturated model n½¼ satisfying #ÁÂ# and a free model satisfying  and the other with the sat-urated model nÍÌ satisfying #Á   ÇdÈ²ÉYÊ and a free model satisfying   ."
"The first solution yields the sentence “John sees Mary” whereas the second yields the topicalised sen-tence “Mary, John sees.”"
The problem with the simple method outlined above is that it severely restricts the class of gram-mars that can be used by the generator.
"Recall that[REF_CITE]’s parsing model, the assumption is made that each lexical entry has exactly one anchor."
In practice this means that the parser can deal neither with a grammar assign-ing trees with multiple anchors to idioms (as is argued for in e.g.[REF_CITE]) nor with a grammar allowing for trace anchored lexical entries.
The mirror restriction for genera-tion is that each lexical entry must be associated with exactly one semantic proposition.
The re-sulting shortcomings are that the generator can deal neither with a lexical entry having an empty semantics nor with a lexical entry having a multi-propositional semantics.
We first show that these restrictions are too strong.
We then show how to adapt the generator so as to lift them.
Arguably there are words such as “that” or infinitival “to” whose semantic contribution is void.
"As[REF_CITE]showed, the problem with such words is that they cannot be selected on the basis of the input semantics."
"To circumvent this problem, we take advantage of the TAG extended domain of locality to avoid having such entries in the grammar."
"For instance, complementizer “that” does not anchor a tree de-scription by itself but occurs in all lexical tree de-scriptions providing an appropriate syntactic con-text for it, e.g. in the tree description for “say”."
Lexical entries with a multi-propositional semantics are also very com-mon.
"For instance, a neo-Davidsonian seman-tics would associate e.g. ¢#)ª#^XÏW¤&apos;b¡_) with the verb “run” or #!^ °Ñ)_ §Y±ª with the past tensed “ran”."
"Similarly, agentless passive “be” might be represented by an overt quantifi-cation over the missing agent position (such as Ò Ò ± Z ÔÓ¦§Y±)ªdÕ^XÏW¤&apos;b¡_ with Ó a variable over the complement verb semantics)."
And a gram-mar with a rich lexical semantics might for in-stance associate the semantics
"Ö×!^ b¡_ ! , kW!^ Ù¤]§Y±6Ø! with “want” (cf.[REF_CITE]which argues for such a semantics to account for examples such as “Reuters wants the report to-morrow” where “tomorrow” modifies the “hav-ing” not the “wanting”)."
"Because it assumes that each lexical entry is associated with exactly one semantic proposi-tion, such cases cannot be dealt with the gener-ator sketched in the previous section."
A simple method for fixing this problem would be to first partition the input semantics in as many ways as are possible and to then use the resulting parti-tions as the basis for lexical lookup.
The problems with this method are both theo-retical and computational.
"On the theoretical side, the problem is that the partitioning is made in-dependent of grammatical knowledge."
"It would be better for the decomposition of the input se-mantics to be specified by the lexical lookup phase, rather than by means of a language in-dependent partitioning procedure."
"Computation-ally, this method is unsatisfactory in that it im-plements a generate-and-test procedure (first, a partition is created and second, model genera-tion is applied to the resulting matrices) which could rapidly lead to combinatorial explosion and is contrary in spirit[REF_CITE]constraint-based approach."
We therefore propose the following alternative procedure.
"We start by marking in each lexi-cal entry, one proposition in the associated se-mantics as being the head of this semantic rep-resentation."
The marking is arbitrary: it does not matter which proposition is the head as long as each semantic representation has exactly one head.
We then use this head for lexical lookup.
"Instead of selecting lexical entries on the basis of their whole semantics, we select them on the basis of their index."
"That is, a lexical entry is selected iff its head unifies with a proposition in the input semantics."
"To preserve coherence, we further maintain the additional constraint that the total semantics of each selected entries sub-sumes (part of) the input semantics."
"For instance, given the grammar in Figure 4 (where seman-tic heads are underlined) and the input semantics #!^ #°Ñ^ _! §Y±)ª , the generat-ing matrix will be: #2&quot; $ &amp; /.  3$ &amp; ]? .(9² * +   ã6#U  &amp;&apos;&lt; &lt; (Y. ?  Û  3 $"
"Given this matrix, two solutions will be found: the saturated tree for “John ran” satisfying the conjunction  ¿,#À Áø Á and that for “John did run” satisfying  ,¿ #À Áù Á !ý²þý ."
"No other so-lution is found as for any other conjunction of de-scriptions made available by the matrix, no satu-rated model exists."
Our generator presents three main characteristics: (i)
"It is based on an axiomatic rather than a gen-erative view of grammar, (ii) it uses a TAG-like grammar in which the basic linguistic units are trees rather than categories and (iii) it assumes a flat semantics."
In what follows we show that this combina-tion of features results in a generator which in-tegrates the positive aspects of both top-down and bottom-up generators.
"In this sense, it is not un-[REF_CITE]’s semantic-head-driven generation."
"As will become clear in the follow-ing section however, it differs from it in that it integrates stronger lexicalist (i.e. bottom-up) in-formation."
"Bottom-up or “lexically-driven” generators (e.g.,[REF_CITE]) start from a bag of lexical items with instantiated semantics and generates a syn-tactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input."
There are two known disadvantages to bottom-up generators.
"On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics."
"On the other hand, they are often overly non-deterministic (though see[REF_CITE]for an exception)."
We now show how these problems are dealt with in the present algorithm.
Two main sources of non-determinism affect the performance of bottom-up generators: the lack of an indexing scheme and the presence of intersective modifiers.
"In[REF_CITE], a chart-based bottom-up generator is presented which is devoid of an in-dexing scheme: all word edges leave and enter the same vertex and as a result, interactions must be considered explicitly between new edges and all edges currently in the chart."
"The standard solution to this problem (cf.[REF_CITE]) is to index edges with semantic indices (for instance, the edge with category N/x:dog(x) will be indexed with x) and to restrict edge combination to these edges which have compatible indices."
"Specifically, an active edge with category A(...)/C(c ...) (with c the se-mantics index of the missing component) is re-stricted to combine with inactive edges with cate-gory C(c ...), and vice versa."
"Although our generator does not make use of a chart, the constraint-based processing model de-scribed[REF_CITE]imposes a similar restriction on possible combinations as it in essence requires that only these nodes pairs be tried for identification which (i) have opposite po-larity and (ii) are labeled with the same semantic index."
"Let us now turn to the second known source of non-determinism for bottom-up generators namely, intersective modifiers."
"Within a construc-tive approach to lexicalist generation, the number of structures (edges or phrases) built when gener-ating a phrase with  intersective modifiers is ÿ  in the case where the grammar imposes a single linear ordering of these modifiers."
"For instance, when generating “The fierce little black cat”, a naive constructive approach will also build the subphrases (1) only to find that these cannot be part of the output as they do not exhaust the input semantics. (1) The fierce black cat, The fierce little cat, The little black cat, The black cat, The fierce cat, The little cat, The cat."
"To remedy this shortcoming, various heuristics and parsing strategies have been proposed.[REF_CITE]combines a constraint-propagation mech-anism with a shift-reduce generator, propagating constraints after every reduction step.[REF_CITE]advocate a two-step generation algo-rithm in which first, the basic structure of the sen-tence is generated and second, intersective mod-ifiers are adjoined in."
And[REF_CITE]make use of a tree reconstruction method which incrementally improves the syntactic tree until it is accepted by the grammar.
"In effect, the constraint-based encoding of the axiomatic view of generation proposed here takes advantage of Brew’s observation that constraint propagation can be very effective in pruning the search space involved in the generation process."
"In constraint programming, the solutions to a constraint satisfaction problem (CSP) are found by alternating propagation with distribution steps."
Propagation is a process of deterministic infer-ence which fills out the consequences of a given choice by removing all the variable values which can be inferred to be inconsistent with the prob-lem constraint while distribution is a search pro-cess which enumerates possible values for the problem variables.
"By specifying global proper-ties of the output and letting constraint propaga-tion fill out the consequences of a choice, situa-tions in which no suitable trees can be built can be detected early."
"Specifically, the global constraint stating that the semantics of a solution tree must be identical with the goal semantics rules out the generation of the phrases in (1b)."
"In practice, we observe that constraint propagation is indeed very efficient at pruning the search space."
"As table 5 shows, the number of choice points (for these specific examples) augments very slowly with the size of the input."
Lexical lookup only re-turns these categories in the grammar whose se-mantics subsumes some portion of the input se-mantics.
"Therefore if some grammar rule involves a daughter category whose semantics is not part of the mother semantics i.e. if the grammar is se-mantically non-monotonic, this rule will never be applied even though it might need to be."
Here is an example.
"Suppose the grammar contains the following rule (where X/Y abbreviates a category with part-of-speech X and semantics Y): vp/call up(X,Y) v/call up(X,Y), np/Y, pp/up"
And suppose the input semantics is  \ ^ )/!^ ¢Ñ¯¡ª .
"On the basis of this £ input, lexical lookup will return the categories V/call up(john,mary), NP/john and NP/mary (because their semantics subsumes some portion of the input semantics) but not the category PP/up."
Hence the sentence “John called Mary up” will fail to be generated.
"In short, the semantic monotonicity constraint makes the generation of collocations and idioms problematic."
Here again the extended domain of locality provided by TAG is useful as it means that the basic units are trees rather than categories.
"Furthermore, as argued[REF_CITE], these trees can have multiple lexical an-chors."
"As in the case of vestigial semantics dis-cussed in Section 4 above, this means that phono-logical material can be generated without its se-mantics necessarily being part of the input."
"As shown in detail[REF_CITE], top-down generators can fail to terminate on certain grammars because they lack the lexical informa-tion necessary for their well-foundedness."
"A sim-ple example involves the following grammar frag-ment: r1. s/S np/NP, vp(NP)/S r2. np/NP det(N)/NP, n/N r3. det(N)/NP np/NP0, poss(NP0,NP)/NP r4. np/john john r5. poss(NP0,NP)/mod(N,NP0) s r6. n/father father r7. vp(NP)/left(NP) left"
"Given a top-down regime proceeding depth-first, left-to-right through the search space defined by the grammar rules, termination may fail to occur as the intermediate goal semantics NP (in the sec-ond rule) is uninstantiated and permits an infinite loop by iterative applications of rules r2 and r3."
Such non-termination problems do not arise for the present algorithm as it is lexically driven.
"So for instance given the corresponding DG frag-ment for the above grammar and the input seman-tics [  ¤ &apos;_#  ^ _  ª#!^  , the generator will simply select the tree de-scriptions for “left”, “John”, “s” and “father” and generate the saturated model satisfying the conjunction of these descriptions."
The ideas presented here have been implemented using the concurrent constraint programming lan-guage Oz[REF_CITE].
"The implementation includes a model generator for the tree logic pre-sented in section 2, two lexical lookup modules (one for parsing, one for generation) and a small DG fragment for English which has been tested in parsing and generation mode on a small set of English sentences."
This implementation can be seen as a proof of concept for the ideas presented in this paper: it shows how a constraint-based encoding of the type of global constraints suggested by an ax-iomatic view of grammar can help reduce non-determinism (few choice points cf. table 5) but performance decreases rapidly with the length of the input and it remains a matter for further re-search how efficiency can be improved to scale up to bigger sentences and larger grammars.
"We have shown that modulo some minor changes, the constraint-based approach to parsing pre-sented[REF_CITE]could also be used for generation."
"Furthermore, we have ar-gued that the resulting generator, when combined with a TAG-like grammar and a flat semantics, had some interesting features: it exhibits the lex-icalist aspects of bottom-up approaches thereby avoiding the non-termination problems connected with top-down approaches; it includes enough top-down guidance from the TAG trees to avoid typical bottom-up shortcomings such as the re-quirement for grammar semantic monotonicity and by implementing an axiomatic view of gram-mar, it supports a near-deterministic treatment of intersective modifiers."
It would be interesting to see whether other axiomatic constraint-based treatments of gram-mar could be use to support both parsing and generation.
"In particular, we intend to investi-gate whether the dependency grammar presented[REF_CITE], once equipped with a se-mantics, could be used not only for parsing but also for generating."
"And similarly, whether the description based treatment of discourse parsing sketched[REF_CITE]could be used to generate discourse."
"This paper proposes a description of German word order including phe-nomena considered as complex, such as scrambling, (partial) VP fronting and verbal pied piping."
Our description relates a syntactic de-pendency structure directly to a topological hierarchy without re-sorting to movement or similar mechanisms. [Footnote_1]
"1 We would like to thank Werner Abraham, Tilman Becker, Ralph Debusmann, Denys Duchier, and Stefan Müller for fruitful discussions. Particular thanks to Igor Mel&apos;cuk for the inspiration of the particular status we give to the phrase structure."
The aim of this article is to describe the word order of German verbs and their comple-ments.
"German word order is not free, but based on fairly simple rules, forming what is usually called topological model, which sub-divides the sentence into a hierarchy of topo-logical domains that are themselves composed of fields (Vorfeld, Mittelfeld, right bracket…)[REF_CITE]."
"We start from a syntactic dependency tree, i.e. an unordered tree whose nodes are labeled with the words of the sentence, and whose branches are labeled with syntactic relations among the words (subject, direct object…)."
"The syntactic dependency structure only en-codes subcategorization and modification and must be completed by the communicative structure (partition into theme/rheme, focus…), which plays a fundamental role in word order."
It permits us to choose among all the different possible orders corresponding to a given de-pendency structure.
"In this paper we do not pursue this problem any further, but have limited our description to the link between dependency and topology."
Note that it is fun-damental to our approach that syntactic structure does not include word order.
"To get the words in order, we group them in a hierarchy of phrases."
The nature and the posi-tion of these phrases are constrained by our topological model.
"For instance, a non-finite verb can open two kinds of topological phrases, either a phrase, which we call domain, with positions for all of its dependents, or a restricted phrase, which forms the verb cluster, with no positions for dependents other than predicative elements."
These two kinds of phrases must be placed in very different topological positions.
The fact that we pass through a (topological) phrase structure in order to relate dependency and word order distinguishes our approach from usual dependency grammars (Mel&apos;cuk &amp;[REF_CITE]; Duchier &amp;[REF_CITE]).
"The description of German word order closest to our analysis is the HPSG grammar of Kathol (1995; see also[REF_CITE]), who proposes linearization rules exclusively based on a for-malization of the topological structure."
"How-ever, as required by the formalism he uses, a regular phrase structure, which we do not need in our analysis, still underlies the structures obtained."
Our work constitutes a syntactic module which links (unordered) syntactic structures with topological phrase structures.
"Syntactic struc-tures are related to semantic structures, whereas topological phrase structures are re-lated to phonological structures."
"In other words, our work lies within the scope of the general framework of Meaning-Text-Theory (Mel&apos;cuk 1988), which considers the modeling of a language as a modular (bi-directional) correspondence between meaning and text."
"It must be clear that, in contrast to X-bar syntax, our topological phrase structure does not rep-resent the syntactic structure of the sentence."
"Although the dependency information is es-sential in its construction, the phrase structure only represents topology, i.e. the surface grouping of the words."
"Topological phrases can be directly related to prosodic groups, and topology represents an intermediate level be-tween dependency and phonology."
"In Section 2, the results of our findings are presented, without recourse to any mathemati-cal formalism, in the usual terminology of traditional German grammars."
"In Section 3, a mathematical formalism is proposed to state the rules and the grammar fragment described in Section 2."
Word order in German is much freer than in English.
"The dependency tree of Fig. 1, which will be our reference example, has a few dozen linearizations: (1) a. Niemand hat diesem Mann das Buch zu lesen versprochen b. Diesem Mann hat das Buch niemand zu lesen versprochen c. Das Buch zu lesen hat diesem Mann niemand versprochen d. Diesem Mann hat niemand verspro-chen, das Buch zu lesen e. Diesem Mann hat, das Buch zu lesen, niemand versprochen f. Zu lesen hat diesem Mann das Buch niemand versprochen g. Das Buch hat niemand diesem Mann versprochen zu lesen ‘Nobody promised this man to read the book.’"
"In this paper, we do not attempt to characterize well-formed German dependency trees al-though we recognize that such a characteriza-tion is essential if we attempt to describe the acceptable sentences of German."
The internal structure of a domain is a se-quence of fields.
"For example, the main do-main is the underlying pattern of a declarative sentence, and it consists of the following se-quence of five fields: [Vorfeld, left bracket, Mittelfeld, right bracket, Nachfeld]."
"A domain resembles a box whose ordered compartments, called fields, can themselves accommodate new boxes."
"In addition to the rules listing the fields of each type of box, we propose two further types of rules: • rules that indicate into which field a word can go–depending on the position of its governor; • rules that indicate which type of box a word can create when it is placed into a given field."
The hierarchy of boxes forms the phrase structure we construct.
"We have established the following rules for the linear order of verbs and their dependents: • The finite verb takes the second position of the main domain, the left bracket."
"This verb is also called V2. • A non-finite verb depending on V[Footnote_2] can go into the right bracket. 2 As a result, it opens a reduced phrase with only one po-sition for a verbal dependent (see Section 2.8 for another possibility)."
"2 We consider that in a compound verb form such as hat gelesen ‘has read’ the past participle depends syntactically on the auxiliary, which is the finite verb form (cf.[REF_CITE]Mel&apos;cuk 1988). The V2 is thus always the root of the syntactic dependency tree."
"If a subse-quent third verb joins the verb already in the right bracket, it will again open a phrase with a position to its left, and so on."
"The verbal constituent occupying the right bracket is called the verb cluster. • Some non-verbal dependents, such as separable verbal prefixes (for example the an of anfangen ‘begin’), predicative ad-jectives, and nouns governed by a copular verb or a support verb, can go into the right bracket (the prefix even forms one word with its following governor)."
"In con-trast to verbs, these elements do not usu-ally open up a new position for their de-pendents, which consequently have to be placed somewhere else. [Footnote_3] • One dependent (verbal or non-verbal) of any of the verbs of the main domain (V2, any verb in the right bracket or even an embedded verb) has to occupy the first position, called the Vorfeld (VF, pre-field). • All the other non-verbal dependents of the verbs in the domain (V2 or part of the verbal cluster) can go in the Mittelfeld (MF, middle-field). • Some phrases, in particular sentential complements (complementizer and rela-tive clauses), prepositional phrases, and even some sufficiently heavy noun phrases, can be positioned in a field right of the right bracket, the Nachfeld (NF, af-ter-field)."
"3 In examples such as (i), the separable verbal prefix an behaves like a subordinated verb intervening be-tween the ‘main’ verb and its dependent:"
"Like the Mittelfeld, the Nachfeld can accommodate several dependents. • When a verb is placed in any of the Major Fields (Vor-, Mittel-, Nachfeld), it opens a new embedded domain."
In the following section we illustrate our rules with the dependency tree of Fig. 1 and show how we describe phenomena such as scram-bling and (partial) VP fronting.
"Let us start with cases without embedding, i.e. where the subordinated verbs versprochen ‘promised’ and zu lesen ‘to read’ will go into the right bracket of the main domain (Fig. 2)."
The constituents which occupy the left and right brackets are represented by shadowed ovals.
"The other three phrases, niemand ‘no-body’, diesem Mann ‘to this man’, and das Buch ‘the book’, are on the same domain level; one of them has to take the Vorfeld, the other two will go into the Mittelfeld."
"We obtain thus 6 possible orders, among them (1a) and (1b)."
There are nevertheless some general restrictions on the relative constituent order in the Mittelfeld.
"We do not consider these rules here (see for instance[REF_CITE]), but we want to insist on the fact that the order of the constituents depends very little on their hierarchical position in the syntactic structure. [Footnote_4] Even if the order is not free, there are restrictions that weigh more heavily than the hierarchical position: pronominalization, focus, new information, weight, etc."
"4 Dutch has the same basic topological structure, but has lost morphological case except on pronouns. For a simplified description of the order in the Dutch Mittelfeld, we have to attach to each complement placed in the Mittelfeld its height in the syntactic"
"The fact that a verbal projection (i.e. the verb and all of its direct and indirect dependents) does not in general form a continuous phrase, unlike in English and French, is called scram-bling[REF_CITE]."
This terminology is based on an erroneous conception of syntax that supposes that word order is always an immedi-ate reflection of the syntactic hierarchy (i.e. every projection of a given element forms a phrase) and that any deviation from this con-stitutes a problem.
"In fact, it makes little sense to form a phrase for each verb and its depend-ents."
"On the contrary, all verbs placed in the same domain put their dependents in a com-mon pot."
"In other words, there is no scram-bling in German, or more precisely, there is no advantage in assuming an operation that de-rives ‘scrambled’ sentences from ‘non-scrambled’ ones."
iobj ‘noboby’ zu lesen diesem Mann ‘to read’ ‘to this man’ dobj das Buch ‘the book’
"As we have said, when a verb is placed in one of the major fields, it opens an embedded domain."
We represent domains by ovals with a bold outline.
"In the situation of Fig. 3, where zu lesen ‘to read’ opens an embedded do-main, hat ‘has’ and versprochen ‘promised’ occupy the left and right bracket of the main domain and we find three phrases on the same level: niemand ‘nobody’, diesem Mann ‘to this man’, and das Buch zu lesen ‘to read the book’."
"The embedded domain can go into the Vorfeld (1c), the Nachfeld (1d), or the Mit-telfeld (1a,e)."
"Note that we obtain the word order (1a) a sec-ond time, giving us two phrase structures: (2) a. [Niemand] [hat] [diesem Mann] [das Buch zu lesen] [versprochen] b. [Niemand] [hat] [diesem Mann] [das Buch] [zu lesen versprochen]"
"This structural ambiguity corresponds, we believe, to a semantic ambiguity of communi-cative type:"
"In (2a), the fact of reading the book is marked (as in Reading the book, no-body promised him that), whereas (2b) is neu-tral in this respect (Nobody promised him to read the book)."
"Moreover, the structures (2a) and (2b) corre-spond to different prosodies (the left border of the right bracket is clearly marked with an accent on the first syllable of the radical)."
"Finally, the existence of this ambiguity is also confirmed by the contrast between full infini-tives (with zu) and bare infinitives (without zu): Bare infinitives cannot form an embedded domain outside of the Vorfeld."
"Consequently, there are two different prosodies for (3a) (with or without detachment of das Buch ‘the book’ from zu lesen ‘to read’), whereas only one prosody without detachment is permitted for (3b), although (3a) and (3b) have isomor-phic dependency trees."
"Evidence comes also from the written form recommending a comma for (3a) (i.e. preference for the em-bedded structure), whereas the comma is not allowed for (3b). (3) a. Niemand versucht(,) das Buch zu lesen ‘Nobody tries to read the book.’ b. Niemand will das Buch lesen ‘Nobody wants to read the book.’"
The dependents of a verb do not have to be in their governor’s domain: They can be ‘eman-cipated’ and end up in a superior domain.
"For example, in Fig. 4, the verb zu lesen ‘to read’ has created an embedded domain from which its dependent das Buch ‘the book’ has been emancipated."
"We have thus four complements to place in the superior domain, allowing more than thirty word orders, among them (1f) and (1g)."
"Among these orders, only those that have das Buch or zu lesen in the Vorfeld are truly acceptable, i.e. those where embedding and emancipation are communicatively moti-vated by focus on das Buch or zu lesen."
German permits different orders inside the verb cluster.
The tense auxiliaries haben ‘have’ (past) and werden ‘become/will’ (fu-ture) also allow their dependents to take a place on their right in the right bracket (Oberfeldumstellung or auxiliary flip;[REF_CITE]) (4a).
"The dependents of this verb go again on the left side of their governor, just as in standard order (we thus obtain V 1 V 2 , V 1 V 3 V 2 , V 1 V 4 V 3 V 2 ) but it can also join the place to the left of the auxiliary (we thus ob-tain the marginal Zwischenstellung V 3 V 1 V 2 (4c), V 4 V 3 V 1 V 2 )."
"The governed verbs V 2 accepting this inverse order form a closed class including the modal and perception verbs and some others (helfen, ‘help’, the causative/permissive lassen ‘make/let’ … – haben ‘have’ itself also allows this right-placement, which suffices to explain the cases of ‘double flip’ as in (4b) giving V 1 V 2 V 4 V 3 )."
Note that the dependent of haben ‘have’ is the bare infinitive.
"This form, called the Ersatzinfinitiv, is also possible or even preferable for certain verbs when the auxiliary is in V2 position. (4) a."
Er wird das Buch haben lesen können.
"He will the book have read can. ‘He will have been able to read the book.’ b. Ich glaube, dass er das Buch wird ha-ben lesen können."
"I believe that he the book will have read can. ‘I believe that he will have been able to read the book.’ c. Ich glaube, dass er das Buch lesen wird können."
I believe that he the book read will can. ‘I believe that he will be able to read the book.’
"In related languages like Dutch or Swiss-German, which have the same topological structure, the standard order in the right bracket is somewhat similar to the German Oberfeldumstellung."
The resulting order gives rise to cross serial dependencies ([REF_CITE])
Such constructions have often been studied for their supposed com-plexity.
"With our subsequent description of the Oberfeldumstellung, we obtain a formal structure that applies equally to Dutch."
"Indeed, the two structures have identical descriptions with the exception of the relative order of dependent verbal elements in the right bracket (keeping in mind that we do not describe the order of the Mittelfeld)."
Relative clauses open an embedded domain with the main verb going into the right bracket.
"The relative pronoun takes the first position of the domain, but it can take other elements along (pied-piping) (5)."
German differs from English and Romance languages in that even verbs can be brought along by the relative pronoun (5b). (5) a. Der Mann [[von dem] [Maria] [geküsst wird]] liebt sie.
"The man [[by whom] [Maria] [kissed is]] loves her. b. Das war eine wichtige Einnahmequel-le, [[die zu erhalten] [sich] [die EU] [verpflichtet hat]]."
"This was an important source_of_income, [[that to conserve] [itself] [the EU] [com-mited has]]. ‘This was an important source_of_income, that the EU obliged itself to conserve.’"
"Before we discuss the topological structure of relative clauses, we will discuss their syntactic representation."
"For this reason, we attribute to the relative pronoun a double position: as a complemen-tizer, it is the head of the relative clause and it therefore depends directly on the antecedent noun and it governs the main verb of the rela-tive clause."
"As a pronoun, it takes its usual position in the relative clause."
It is now possible to give the word order rules for relative clauses.
"The complementizing part of the relative pronoun opens an embedded domain consisting of the complementizer field[REF_CITE], Mittelfeld, right bracket, and Nachfeld."
The main verb that depends on it joins the right bracket.
"The other rules are identical to those for other domains, with the group containing the pronominal part of the relative pronoun having to join the other part of the pronoun in the complementizer field."
"In a sense, the complementizer field acts like the fusion of the Vorfeld and the left bracket of the main domain: The complementizing part of the pronoun, being the root of the dependency tree of the relative clause, takes the left bracket (just like the top node of the whole sentence in the main domain), while the pronominal part of the relative pronoun takes the Vorfeld."
The fact that the pronoun is one word requires the fusion of the two parts and hence of the two fields into one.
Note that verbal pied-piping is very easy to explain in this analysis: It is just an embedding of a verb in the complementizer field.
"Just like the Vor-feld, the complementizer field can be occu-pied by a non-verbal phrase or by a verb cre-ating an embedded domain."
A grammar in the formalism we introduce in the following will be called a Topological Dependency Grammar.
"For a grammar, the parameters to instantiate are the vocabulary V, the set of (lexical) cate-gories C, the set of syntactic relations R, the set of box names B, the set of field names F, the initial field i, the order of permeability of the boxes, which is a partial ordering on B (used for emancipation) and four sets of rules: [Footnote_5] 1."
"5 We will not present lexical rules indicating each lexical entry’s characteristics, in particular its cate-gory."
"Box description rules: The rule b f1 f2 … fn indicates that the box b consists of the list of fields f1, f2, …, fn. b f1 f2 … fn 2."
"Field description rules: The pair (f,ε) in F×{!,?,+,∗} indicates that the field f has to contain exactly one element (!), at most one element (?), at least one element (+) or any number of elements (∗). 3."
Correspondence rules (between the de-pendency and the topological structure):
"The rule (r,c1,c2,f2,b) indicates that a word w2 of category c2, that exhibits a dependency of type r on a word w1 of category c1, can go into field f2 of a box containing w1, if this box is separated from w1 by borders of type ≤ b (in other words, the parameter b controls the emancipation). b r &gt; c1 c2 f2 (In all our figures, boxes are represented by ovals, fields by rectangles or sections of an oval.) 4."
"Box creation rules: The rule (c,f,b,f’) indicates that a word of category c, placed into a field f, can create a box b and go into the field f’ of this box."
"Box creation rules are applied recursively until a lexical rule of type (c,f,b,-) is encountered where b is a lexical box with a unique lexical field, into which the word has to be placed."
Phrase structure derivation starting from a dependency tree The word labeling the root node of the tree is placed into the initial field i. Box creation rules are then activated until the word is placed in a lexical field (-).
"A correspondence rule is activated for one of the dependents of the root node, placing it in an accessible field."
"Just as for the root node, box creation rules are acti-vated until the word is assigned to a lexical field."
This procedure continues until the whole tree is used up.
"Each time a box creation rule is triggered, a box is created and a description rule for this box has to be activated."
"Finally, the constraints of the field description rules have to be respected (e.g. a field requiring at least one element can not remain empty)."
We will now instantiate our formalism for the German grammar fragment described in sec-tion 2 (leaving aside non-verbal elements in the right bracket) and we will put forward the derivation of (1f) with this grammar (Fig.5).
We have shown how to obtain all acceptable linear orders for German sentences starting from a syntactic dependency tree.
To do that we have introduced a new formalism which constructs phrase structures.
"These structures differ from X-bar phrase structures in at least two respects: First, we do not use the phrase structure to represent the syntactic structure of the sentence, but only for linearization, i.e. as an intermediate step between the syntactic and the phonological levels."
"Secondly, the nature of the phrase opened by a lexical element depends not only on the syntactic position of this element, but also on its position in the topological structure (e.g. the different be-haviors of a verb in the right bracket vs. in a major field)."
We have to investigate further in various di-rections:
"From a linguistic point of view, the natural continuation of our study is to find out how the communicative structure (which completes the dependency tree) restricts us to certain word orders and prosodies and how to incorporate this into our linearization rules."
"It would also be interesting to attempt to de-scribe other languages in this formalism, con-figurational languages such as English or French, as well as languages such as Russian where the surface order is mainly determined by the communicative structure."
"However, German is an especially interesting case be-cause surface order depends strongly on both the syntactic position (e.g. finite verb in V2 or Vfinal position) and the communicative structure (e.g. content of the Vorfeld)."
"From a computational point of view, we are interested in the complexity of our formalism."
It is possible to obtain a polynomial parser provided that we limit the number of nodes simultaneously involved in non-projective configurations (see[REF_CITE]for similar techniques).
Such limitations seem reasonable for Germanic languages (e.g. verb clusters with more than four verbs are un-usual).
A good decoding algorithm is critical to the success of any statistical machine translation system.
The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combin-ing them).
"Since the space of possi-ble translations is extremely large, typ-ical decoding algorithms are only able to examine a portion of it, thus risk-ing to miss good solutions."
"In this pa-per, we compare the speed and out-put quality of a traditional stack-based decoding algorithm with two new de-coders: a fast greedy decoder and a slow but optimal decoder that treats de-coding as an integer-programming opti-mization problem."
"A statistical MT system that translates (say) French sentences into English, is divided into three parts: (1) a language model (LM) that as-signs a probability P(e) to any English string, (2) a translation model (TM) that assigns a probability P(f e) to any pair of English and French strings, and (3) a decoder."
"The decoder takes a previ-ously unseen sentence and tries to find the that maximizes P(e f), or equivalently maximizes P(e) P(f e)."
"If the source and target languages are con-strained to have the same word order (by choice or through suitable pre-processing), then the lin-ear Viterbi algorithm can be applied[REF_CITE]."
"If re-ordering is limited to rotations around nodes in a binary tree, then optimal decod-ing can be carried out by a high-polynomial algo-rithm[REF_CITE]."
"For arbitrary word-reordering, the decoding problem is NP-complete[REF_CITE]."
A sensible strategy[REF_CITE]is to examine a large subset of likely decodings and choose just from that.
"Of course, it is possible to miss a good translation P(e  f), this is called this way."
If the decoder returns e but there exists some e for which P(e f) a search error.
"Thus, while decoding is a clear-cut optimiza-tion task in which every problem instance has a right answer, it is hard to come up with good answers quickly."
"This paper reports on mea-surements of speed, search errors, and translation quality in the context of a traditional stack de-coder[REF_CITE]and two new decoders."
"The first is a fast greedy decoder, and the second is a slow optimal decoder based on generic mathematical programming techniques."
"In this paper, we work with IBM Model 4, which revolves around the notion of a word alignment over a pair of sentences (see Figure 1)."
A word alignment assigns a single home (English string position) to each French word.
"If two French words align to the same English word, then that"
English word is said to have a fertility of two.
"Likewise, if an English word remains unaligned-to, then it has fertility zero."
The word align-ment in Figure 1 is shorthand for a hypothetical stochastic process by which an English string gets converted into a French string.
There are several sets of decisions to be made.
"First, every English word is assigned a fertil-ity."
These assignments are made stochastically according to a table n( e ).
"We delete from the string any word with fertility zero, we dupli-cate any word with fertility two, etc."
"If a word has fertility greater than zero, we call it fertile."
"If its fertility is greater than one, we call it very fertile."
"After each English word in the new string, we may increment the fertility of an invisible En-glish NULL element with probability p (typi-cally about 0.02)."
The NULL element ultimately produces “spurious” French words.
"Next, we perform a word-for-word replace- French words, according to the table t(f  e ). ment of English words (including NULL) by"
"Finally, we permute the French words."
"In per-muting, Model 4 distinguishes between French words that are heads (the leftmost French word generated from a particular English word), non-heads (non-leftmost, generated only by very fer-tile English words), and NULL-generated."
The head of one English word is as-signed a French string position based on the po-an English word e  translates into something sition assigned to the previous English word.
"If at French position j, then the French head word k with distortion probability d (k–j class(e  ), of e is stochastically placed in French position class(f )), where “class” refers to automatically determined word classes for French and English vocabulary items."
This relative offset k–j encour-jacent French words.
"If e  is infertile, then j is ages adjacent English words to translate into ad- taken from e  , etc."
"If e  is very fertile, then j is the average of the positions of its French trans-lations."
Non - heads.
"If the head of English word e is placed in French position j, then its first non-ing to another table d  (k–j class(f ))."
"The next head is placed in French position k ( j) accord-d  (q–k class(f )), and so forth. non-head is placed at position q with probability"
"After heads and non-heads are placed, NULL-generated words are permuted are  NULL-generated words, then any place-into the remaining vacant slots randomly."
If there ment scheme is chosen with probability 1/  .
"These stochastic decisions, starting with e, re-sult in different choices of f and an alignment of f with e. We map an e onto a particular a,f pair with probability: ) *+ where the factors separated by &apos; symbols denote fertility, translation, head permutation, non-head permutation, null-fertility, and null-translation probabilities. 1"
"If we observe a new sentence f, then an optimal decoder will search for an e that maximizes P(e f) i P(e) P(f e)."
"Here, P(f e) is the sum of P(a,f e) over all possible alignments a."
"Because this sum involves significant computation, we typi-pair that maximizes P(e,a f) P(e) P(a,f e)."
"Wecally avoid it by instead searching i for an e,a take the language model P(e) to be a smoothed n-gram model of English."
The stack (also called A*) decoding algorithm is a kind of best-first search which was first intro-duced in the domain of speech recogniti[REF_CITE].
"By building solutions incremen-tally and storing partial solutions, or hypotheses, in a “stack” (in modern terminology, a priority queue), the decoder conducts an ordered search of the solution space."
"In the ideal case (unlimited stack size and exhaustive search time), a stack de-coder is guaranteed to find an optimal solution; our hope is to do almost as well under real-world constraints of limited space and time."
"The generic stack j Initializedecodingthealgorithmstack withfollowsan :empty hy-j pothesis. j PopIf h ish,athecompletebest hypothesissentence,,offoutputthe stackh and. j terminateFor each possible. next word w, extend h by adding w and push the resulting hy-j pothesisReturn toontothe secondthe stackstep. (pop)."
One crucial difference between the decoding process in speech recognition (SR) and machine translation (MT) is that speech is always pro-duced in the same order as its transcription.
"Con-sequently, in SR decoding there is always a sim-ple left-to-right correspondence between input and output sequences."
"By contrast, in MT the left-to-right relation rarely holds even for language pairs as similar as French and English."
"We ad-dress this problem by building the solution from left to right, but allowing the decoder to consume its input in any order."
"This change makes decod-ing significantly more complex in MT; instead of must consider all  permutations of an k -word knowing the order of the input in advance, we input sentence."
Another important difference between SR and MT decoding is the lack of reliable heuristics in MT.
A heuristic is used in A* search to es-timate the cost of completing a partial hypothe-sis.
"A good heuristic makes it possible to accu-rately compare the value of different partial hy-potheses, and thus to focus the search in the most promising direction."
The left-to-right restriction in SR makes it possible to use a simple yet reli-able class of heuristics which estimate cost based on the amount of input left to decode.
"Partly be-cause of the absence of left-to-right correspon-dence, MT heuristics are significantly more dif-ficult to develop[REF_CITE]."
"With-out a heuristic, a classic stack decoder is inef-fective because shorter hypotheses will almost al-ways look more attractive than longer ones, since as we add words to a hypothesis, we end up mul-tiplying more and more terms to find the proba-bility."
"Because of this, longer hypotheses will be pushed off the end of the stack by shorter ones even if they are in reality better decodings."
"For-tunately, by using more than one stack, we can eliminate this effect."
"In a multistack decoder, we employ more than one stack to force hypotheses to compete fairly."
"More specifically, we have one stack for each sub-set of input words."
"This way, a hypothesis can only be pruned if there are other, better, hypothe-ses that represent the same portion of the input."
"With more than one stack, however, how does a multistack decoder choose which hypothesis to extend during each iteration?"
"We address this is-sue by simply taking one hypothesis from each stack, but a better solution would be to somehow compare hypotheses from different stacks and ex-tend only the best ones."
The multistack decoder we describe is closely patterned on the Model 3 decoder described in the[REF_CITE]patent.
We build solutions incrementally by applying operations to hypothe-ses j .
ThereAdd areaddsfoura operationsnew English: word and j AddZfertaligns a singleaddsFrenchtwo newwordEnglishto it.words.
"The first has fertility zero, while the second is aligned to a single French j Extendword. aligns an additional French word to the most recent English word, increasing its fertility. j AddNull aligns a French word to the English NULL element."
"AddZfert is by far the most expensive opera-tion, as we must consider inserting a zero-fertility English word before each translation of each un-aligned French word."
"With an English vocabulary size of 40,000,[REF_CITE]000 times more expensive than AddNull!"
We can reduce the cost of AddZfert in two ways.
"First, we can consider only certain English words as candidates for zero-fertility, namely words which both occur frequently and have a high probability of being assigned frequency zero."
"Second, we can only insert a zero-fertility word if it will increase the probability of a hypoth-esis."
"According to the definition of the decoding problem, a zero-fertility English word can only make a decoding more likely by increasing P(e) more than it decreases P(a,f e). [Footnote_2]"
"2 We p p * q know that adding a zero-fertility word will decrease P(a,f e) because it adds a term n(0 e ) 1 to the calculation."
"By only con-sidering helpful zero-fertility insertions, we save ourselves significant overhead in the AddZfert operation, in many cases eliminating all possi-bilities and reducing its cost to less than that of AddNull."
"Over the last decade, many instances of NP-complete problems have been shown to be solv-able in reasonable/polynomial time using greedy methods[REF_CITE]."
"Instead of deeply probing the search space, such greedy methods typically start out with a random, approximate solution and then try to improve it incrementally until a satisfactory so-lution is reached."
"In many cases, greedy methods quickly yield surprisingly good solutions."
We conjectured that such greedy methods may prove to be helpful in the context of MT decod-ing.
The greedy decoder that we describe starts the translation process from an English gloss of the French sentence given as input.
The gloss is constructed by aligning each French word f with its most likely English translation ef l (ef lnm argmax o t(e f )).
"For example, in translating the French sentence “Bien entendu , il parle de une belle victoire .”, the greedy decoder initially as- sumes that a good translation of it is “Well heard , it talking a beautiful victory” because the best translation of “bien” is “well”, the best translation of “entendu” is “heard”, and so on."
The alignment corresponding to this translation is shown at the top of Figure 2.
"Once the initial alignment is created, the greedy decoder tries to improve it, i.e., tries to find an alignment (and implicitly translation) of higher probability, by applying one of the follow-ing j operationstranslateOneOrTwoWords: (  ,e , +r ,e ) words, those located at positions rK and +r , changes the translation of one or two French from e s lJt and e s l#u into e and e ."
If e sl is e sl is deleted from the translation.
"If e sl is a word of fertility 1 and e is NULL, then the NULL word, the word e is inserted into alignment of highest probability."
"If e s lJt m the translation at the position that yields the e or e s l#u m e , this operation amounts to j changingtranslateAndInsertthe translation( r of a single word. ,e ,e ) changes the sition r from e sl into  and simulataneously translation of the French word located at po-alignment of highest probability."
Word  inserts word e at the position that yields the is selected from an automatically derived list ing fertility 0.
"When e sl m of 1024 words with high probability of hav-e , this operation amounts to inserting a word of fertility 0 into j removeWordOfFertility0the alignment. ( w word of fertility 0 at position w in the current ) deletes the j swapSegmentsalignment. ( w3 ) creates a new alignment from the old one by swap-ments x w3&lt;y and xrK&lt;y ."
"During the swap ping non-overlapping English word seg-operation, all existing links between English and French words are preserved."
"The seg-ments !{7 can T wordsbe as,smallwhereas  a | wordis theor aslengthlong asof j thejoinWordsEnglish(sentence w B3w . ment the English word at position w3 (or  ) ) eliminates from the align-and links the French words generated by t (or  u ) to } u (or } t )."
"In a stepwise fashion, starting from the initial gloss, the greedy decoder iterates exhaustively over all alignments that are one operation away from the alignment under consideration."
"At every step, the decoder chooses the alignment of high-est probability, until the probability of the current alignment can no longer be improved."
"When it starts from the gloss of the French sentence “Bien entendu, il parle de une belle victoire.”, for ex-ample, the greedy decoder alters the initial align-ment incrementally as shown in Figure 2, eventu-ally producing the translation “Quite naturally, he talks about a great victory.”."
"In the process, the decoder explores a total of 77421 distinct align-ments/translations, of which “Quite naturally, he talks about a great victory.” has the highest prob-ability."
We chose the operation types enumerated above for two reasons: (i) they are general enough to enable the decoder escape local maxima and modify in a non-trivial manner a given align-ment in order to produce good translations; (ii) they are relatively inexpensive (timewise).
"The most time consuming operations in the decoder are swapSegments, translateOneOrTwoWords, and translateAndInsert."
SwapSegments iter-that can be built on a sequence of length  . ates over all possible non-overlapping span pairs iterates over  TranslateOneOrTwoWords &apos;  .
"French X sentencealignmentsand, where  is the size of the is the number of trans-lations we associate with each word (in our im-plementation, we limit this number to the top 10 translations !  )."
"TranslateAndInsertalignments, whereiterates  isoverthe size of the X list of words with high probability of having fertility 0 (1024 words in our implementa-tion)."
"Because any TSP problem instance can be transformed into a de-coding problem instance, Model 4 decoding is provably NP-complete in the length of f."
It is interesting to consider the reverse direction—is it possible to transform a decoding problem in-stance into a TSP instance?
"If so, we may take great advantage of previous research into efficient TSP algorithms."
"We may also take advantage of existing software packages, obtaining a sophisti-cated decoder with little programming effort."
"It is difficult to convert decoding into straight TSP, but a wide range of combinatorial optimiza-tion problems (including TSP) can be expressed in the more general framework of linear integer programming."
A sample integer program (IP) looks like this: minimize objective function: 3.2 * x1 + 4.7 * x2 - 2.1 * x3 subject to constraints: x1 - 2.6 * x3 &gt; 5 7.3 * x2 &gt; 7
A solution to an IP is an assignment of inte-ger values to variables.
Solutions are constrained by inequalities involving linear combinations of variables.
"An optimal solution is one that re-spects the constraints and minimizes the value of the objective function, which is also a linear com-bination of variables."
We can solve IP instances with generic problem-solving software such as lp solve or CPLEX. [Footnote_3]
3 Available[URL_CITE]solve and[URL_CITE]
In this section we explain tence f = “CE NE EST PAS CLAIR .”
"There is one city for each word in f. City boundaries are marked with bold lines, and hotels are illustrated with rectangles."
A tour of cities is a sequence of hotels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start. how to express MT decoding (Model 4 plus En-glish bigrams) in IP format.
We first create a salesman graph like the one in Figure 3.
"To do this, we set up a city for each word in the observed sentence f. City boundaries are shown with bold lines."
We populate each city with ten hotels corresponding to ten likely En-glish word translations.
Hotels are shown as small rectangles.
The owner of a hotel is the English word inside the rectangle.
"If two cities have hotels with the same owner x, then we build a third x-generally, if k cities all have hotels owned by x, owned hotel on the border of the two cities."
"More we build ®0¯z7Nk¨7°T new hotels (one for each non-empty, non-singleton subset of the cities) on various city borders and intersections."
"Finally, we add an extra city representing the sentence bound-ary."
We define a tour of cities as a sequence and ho-tels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start.
"If a hotel sits on the border between two cities, then staying at that hotel counts as visit-ing both cities."
"We can view each tour of cities as corresponding to a potential decoding e,a ."
"The owners of the hotels on the tour give us e, while the hotel locations yield a."
"The next task is to establish real-valued (asym-metric) distances between pairs of hotels, such that the length of any tour is exactly the negative of log(P(e) P(a,f e))."
"Because log is monotonic, the shortest tour will correspond to the likeliest decoding."
The distance we assign to each pair of hotels consists of some small piece of the Model 4 for-mula.
The usual case is typified by the large black arrow in Figure 3.
"Because the destination ho-tel “not” sits on the border between cities NE and PAS, it corresponds to a partial alignment in which the word “not” has fertility two: ... what not ... / __/\ _ / / \ CE NE EST PAS CLAIR ."
"If we assume that we have already paid the price for visiting the “what” hotel, then our inter-hotel distance need only account for the partial alignment concerning “not”: distance = – log(bigram(not what)) – log(n(2 not)) – log(t(NE not)) – log(t(PAS not)) – log(d  (+2 class(PAS))) – log(d (+1 class(what), class(NE)))"
NULL-owned hotels are treated specially.
"We require that all non-NULL hotels be visited be-fore any NULL hotels, and we further require that at most one NULL hotel visited on a tour."
"More-over, the NULL fertility sub-formula is easy to compute if we allow only one NULL hotel to be is simply M the number of cities that ho-visited: tel straddles, and is the number of cities minus one."
This case is typified by the large gray arrow shown in Figure 3.
"Between hotels that are located (even partially) in the same city, we assign an infinite distance in both directions, as travel from one to the other can never be part of a tour."
"For 6-word French sen-tences, we normally come up with a graph that has about 80 hotels and 3500 finite-cost travel seg-ments."
The next step is to cast tour selection as an inte-ger program.
Here we adapt a subtour elimination (0/1) integer variable  for each pair of hotels w strategy used in standard TSP.
We create a binary and r . $± m T hotel r is on theifitineraryand only. ifThetravelobjectivefrom hotelfunction w to is straightforward: minimize: Ha²  I ± distance &quot;%BhrA&amp;
This minimization is subject to three classes of constraints.
"First, every city must be visited ex-actly once."
That means exactly one tour segment must exit each city:
"Second, the segments must be linked to one another, i.e., every hotel has either (a) one tour segment coming in and one going out, or (b) no segments in and none out."
"To put it another way, every hotel must have an equal number of tour segments going in and out: ³ ¹ ² $ m ² ± ±"
"Third, it is necessary to prevent multiple inde-pendent sub-tours."
"To do this, we require that ev-ery proper subset of cities have at least one tour segment leaving it:"
There are an exponential number of constraints in this third class.
"Finally, we invoke our IP solver."
"If we assign mnemonic names to the variables, we can easily extract e,a from the list of variables and their binary values."
The shortest tour for the graph in Figure 3 corresponds to this optimal decoding: it is not clear .
We can obtain the second-best decoding by adding a new constraint to the IP to stop it from choosing the same solution again. [Footnote_4]
"4 If we simply replace “minimize” with “maximize,” we can obtain the longest tour, which corresponds to the worst decoding!"
"In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20."
"We evaluated all decoders with respect to (1) speed, (2) search op-timality, and (3) translation accuracy."
"The last two factors may not always coincide, as Model 4 is an imperfect model of the translation process—i.e., there is no guarantee that a numerically optimal"
"Suppose a decoder outputs  , while the opti-decoding is actually a good translation. mal decoding turns out to be ."
"Then we consider six j possibleno erroroutcomes(NE): } m : , and } is a perfect j translationpure model. error (PME):  m , but } j isdeadlynot asearchperfecterrortranslation(DSE).: m¼ while is a perfect translation, while } , and j isfortuitousnot. and  is a perfectsearch errortranslation(FSE,)while:  m¼ , is j harmlessnot. but  andsearchareerrorboth(HSEperfectly): } m¼ , good j translationscompound error. (CE): m¼ , and nei-ther is a perfect translation."
"Here, “perfect” refers to a human-judged transla-tion that transmits all of the meaning of the source sentence using flawless target-language syntax."
We have found it very useful to have several de-coders on hand.
"It is only through IP decoder out-put, for example, that we can know the stack de-coder is returning optimal solutions for so many sentences (see Table 1)."
"The IP and stack de-coders enabled us to quickly locate bugs in the greedy decoder, and to implement extensions to the basic greedy search that can find better solu-tions. (We came up with the greedy operations discussed in Section 5 by carefully analyzing er-ror logs of the kind shown in Table 1)."
The results in Table 1 also enable us to prioritize the items on our research agenda.
"Since the majority of the translation errors can be attributed to the language and translation models we use (see column PME in Table 1), it is clear that significant improve-ment in translation quality will come from better trigram language model."
Greedy ¿ and greedy are Table 2: Comparison between decoders using a greedy decoders optimized for speed. models.
"The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm."
"Even when the greedy decoder uses an optimized-for-speed set of operations in which at most one word is translated, moved, or inserted at a time and at is labeled “greedy ¿ ” in Table 2—the translation most 3-word-long segments are swapped—which accuracy is affected only slightly."
"In contrast, the translation speed increases with at least one or-der of magnitude."
"Depending on the application of interest, one may choose to use a slow decoder that provides optimal results or a fast, greedy de-coder that provides non-optimal, but acceptable results."
"One may also run the greedy decoder us-ing a time threshold, as any instance of anytime algorithm."
"When the threshold is set to one sec-ond per sentence (the greedy label in Table 1), the performance is affected only slightly."
This work was supported by DARPA-ITO grant[REF_CITE]-00-1-9814.
We offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification.
"We show that this goes beyond standard techniques used in anaphora and ellipsis resolu-tion and requires operations on highly structured, linguistically heterogeneous representations."
We characterize these operations and the representations on which they operate.
We offer an analy-sis couched in a version of Head-Driven Phrase Structure Grammar combined with a theory of information states (IS) in dialogue.
We sketch an algorithm for the process of utterance integration in ISs which leads to grounding or clarifi-cation.
"Clarification ellipsis (CE), nonsentential ellipti-cal queries such as (1a(i),(ii)) are commonplace in human conversation."
"Two common read-ings/understandings of CE are exemplified in (1b,c): the clausal reading is commonly used sim-ply to confirm the content of a particular subutter-ance."
The main function of the constituent read-ing is to elicit an alternative description or osten-sion to the content (referent or predicate etc) in-tended by the original speaker of the reprised sub-utterance. (1) a. A: Did Bo finagle a raise?
B: (i) Bo?/ (ii) finagle? b. Clausal reading: Are you asking if BO (of all people) finagled a raise/Bo FI-NAGLED a raise (of all actions) c. Constituent reading: Who is Bo?/What does it mean to finagle?
"The issue of whether CE involves an ambi-guity or is simply vague is an important one. [Footnote_1] [Footnote_2] Clearly, pragmatic reasoning plays an important role in understanding CEs."
"1 An anonymous ACL reviewer proposed to us that all CE could be analyzed in terms of a single reading along the lines of “I thought I heard you say Bo, and I don’t know why you would do so?”."
"2 Closely related to this issue is the issue of what other readings/understandings CE exhibits. We defer discussion of the latter issue[REF_CITE], which provides a detailed analysis of the frequency of CEs and their under-standings among clarification utterances in the British Na-tional Corpus (BNC)."
"Some considerations do, nonetheless, favour the existence of an ambi-guity."
"First, the BNC provides numerous exam-ples of misunderstandings concerning CE inter-pretation, [Footnote_3] where a speaker intends one reading, is misunderstood, and clarifies his original inter-pretation: (2) a. A: ... you always had er er say every foot he had with a piece of spunyarn in the wire/B: Spunyarn?/A:"
"3 This confirms our (non-instrumentally tested) impres-sion that these understandings are not on the whole disam-biguated intonationally. All our CE data from the BNC was found using SCoRE, Matt Purver’s dialogue oriented BNC search engine[REF_CITE]."
"Spunyarn, yes/B: What’s spunyarn? b. A: Have a laugh and joke with Dick./ B: Dick?/A: Have a laugh and joke with Dick./B: Who’s Dick?"
"More crucially, the clausal and constituent readings involve distinct syntactic and phonolog-ical parallelism conditions."
The constituent read-ing seems to actually require phonological iden-tity.
"With the resolution associated with clausal readings, there is no such requirement."
"How-ever, partial syntactic parallelism does obtain: an XP used to clarify an antecedent sub-utterance  must match  categorially, though there is no re-quirement of phonological identity: (3) a. A:"
I phoned him.
B: him? / #he? b. A: Did he adore the book.
B: adore? / #adored? c. A: We’re leaving?
We are used to systems that will confirm the user’s utterances by repeating part of them.
These presuppose no sophisticated linguistic analysis.
"However, it is not usual for a system to be able to process CEs produced by the user."
"It would be a great advantage in negotiative dialogues, where, for example, the system and the user might be discussing several options and the system may make alternative suggestions, for a system to be able to recognize and interpret a CE."
Consider the following (constructed) dialogue in the route-planning domain: (4) Sys: Would you like to make that trip via Malvern?
At this point the system has to consider a num-ber of possible intepretations for the user’s utter-ance all of which involve recognizing that this is a clarification request concerning the system’s last utterance.
"Appropriate responses might be (5a-c); the sys-tem should definitely not say (5d), as it might if it does not recognize that the user is trying to clarify its previous utterance. (5) a. Yes, Malvern b. Malvern – M-A-L-V-E-R-N c. Going via Malvern is the quickest route d."
"So, you would like to make that trip via Malvern instead of Malvern?"
In this paper we examine the interpretation of CEs.
CE is a singularly complex ellip-sis/anaphoric phenomenon which cannot be han-dled by standard techniques such as first order unification (as anaphora often is) or by higher or-der unification (HOU) on logical forms (see e.g.[REF_CITE]).
"For a start, in order to cap-ture the syntactic and phonological parallelism exemplified in (3), logical forms are simply in-sufficient."
"Moreover, although an HOU account could, given a theory of dialogue that structures context appropriately, generate the clausal read-ing, the constituent reading cannot be so gener-ated."
Clark (e.g.[REF_CITE]) initiated work on the grounding of an utterance (for computa-tional and formal work see e.g.[REF_CITE]).
"However, existing work, while spelling out in great detail what up-dates arise in an IS as a result of grounding, do not offer a characterization of the clarification possi-bilities spawned by a given utterance."
A sketch of such a characterization is provided in this pa-per.
"On the basis of this we offer an analysis of CE, integrated into a large existing grammar framework, Head-Driven Phrase Structure Gram-mar (HPSG) (specifically the version developed[REF_CITE])."
We start by infor-mally describing the grounding/clarification pro-cesses and the representations on which they op-erate.
"We then provide the requisite background on HPSG and on the KOS framework[REF_CITE], in which our analy-sis of ISs is couched."
We sketch an algorithm for the process of utterance integration which leads to grounding or clarification.
"Finally, we formalize the operations which underpin clarification and sketch a grammatical analysis of CE."
We start by offering an informal description of how an utterance such as (6) can get grounded or spawn a clarification by an addressee B: (6) A: Did Bo leave?
A is attempting to convey to B her question whether the property she has referred to with her utterance of leave holds of the person she has referred to with the name Bo. B is required to try and find values for these references.
"Finding values is, with an important caveat, a necessary condition for B to ground A’s utterance, thereby signalling that its content has been integrated in"
B’s IS. [Footnote_4] Modelling this condition for success-ful grounding provides one obvious constraint on the representation of utterance types: such a rep-resentation must involve a function from or -abstract over a set of certain parameters (the con-textual parameters) to contents.
"4 The caveat is, of course, that the necessity is goal driven. Relative to certain goals, one might decide simply to existen-tially quantify the problematic referent. For this operation on meanings see[REF_CITE]. We cannot enter here into a discussion of how to integrate the view developed here in a plan based view of understanding, but see (Ginzburg, (forth-coming)) for this."
This much is fa-miliar already from early work on context depen-dence[REF_CITE]et seq.
What hap-pens when B cannot or is at least uncertain as to how he should instantiate in his IS a contextual parameter ?
In such a case B needs to do at least the following: (1) perform a partial update of the existing context with the successfully processed components of the utterance (2) pose a clarifica-tion question that involves reference to the sub-utterance u from which emanates.
"Since the original speaker, A, can coherently integrate a clarification question once she hears it, it follows that, for a given utterance, there is a predictable range of partial updates + consequent clarifica-tion questions ."
These we take to be specified by a set of coercion operations on utterance repre-sentations. [Footnote_5]
5 The term coercion operation is inspired by work on ut-terance representation within a type theoretic framework re-ported[REF_CITE].
Indeed we assume that a component of dialogue competence is knowledge of these co-ercion operations.
CE gives us some indication concerning both the input and required output of these operations.
"One such operation, which we will refer to as parameter identification, essentially involves as output a question paraphrasable as what is the in-tended reference of sub-utterance u ?."
The par-tially updated context in which such a clarifica-tion takes place is such that simply repeating the segmental phonology of u using rising intona-tion enables that question to be expressed.
An-other existent coercion operation is one which we will refer to as parameter focussing.
This in-volves a (partially updated) context in which the issue under discussion is a question that arises by instantiating all contextual parameters except for and abstracting over .
"In such a context, one can confirm that gets the value B suspects it has by uttering with rising intonation any apparently co-referential phrase whose syntactic category is identical to  ’s."
"From this discussion, it becomes clear that co-ercion operations (and by extension the ground-ing process) cannot be defined simply on mean-ings."
"Rather, given the syntactic and phonologi-cal parallelism encoded in clarification contexts, these operations need to be defined on repre-sentations that encode in parallel for each sub-utterance down to the word level phonological, syntactic, semantic, and contextual information."
"With some minor modifications, signs as con-ceived in HPSG are exactly such a representa-tional format and, hence, we will use them to de-fine coercion operations. 6 More precisely, given that an addressee might not be able to come up with a unique or a complete parse, due to lexi-cal ignorance or a noisy environment, we need to utilize some ‘underspecified’ entity (see e.g.[REF_CITE])."
For simplicity we will use descrip-tions of signs.
An example of the format for signs we employ is given in (7): [Footnote_7] 6 We make two minor modifications to the version of HPSG described[REF_CITE]).
"7 Within the phrasal type system[REF_CITE]root-cl constitutes the ‘start’ symbol of the grammar. In particular, phrases of this type have as their content an illocutionary operator embedding the appropriate semantic"
"First, we re-vamp the existing treatment of the feature C - INDICES ."
"This will now encode the entire inventory of contextual parame-ters of an utterance (proper names, deictic pronouns, indexi-cals) not merely information about speaker/hearer/utterance-time, as standardly."
"Indeed, in principle, relation names should also be included, since they vary with context and are subject to clarification as well."
Such a step involves a signif-icant change to how argument roles are handled in existing HPSG.
"Hence, we do not make such a move here."
"This mod-ification of C - INDICES will allow signs to play a role akin to the role associated with ‘meanings’, i.e. to function as ab-stracts with roles that need to be instantiated."
The second modification we make concerns the encoding of phrasal con-
"Before we can explain how these representa-tions can feature in dialogue reasoning and the resolution of CE, we need to sketch briefly the approach to dialogue ellipsis that we assume."
"We adopt the situation semantics based theory of dialogue context developed in the KOS frame-work ([REF_CITE]; Ginzburg, (forthcoming);[REF_CITE])."
The common ground com-ponent of ISs is assumed to be structured as fol-lows: [Footnote_8]  (8) FACTS set of facts LATEST-MOVE (illocutionary) fact QUD p.o. set of questions
"8 Here FACTS corresponds to the set of commonly ac-cepted assumptions; QUD(‘questions under discussion’) is a set consisting of the currently discussable questions, par-tially ordered by ! (‘takes conversational precedence’); LATEST-MOVE represents information about the content and structure of the most recent accepted illocutionary move."
In[REF_CITE]this framework is integrated into HPSG[REF_CITE];[REF_CITE]define two new at-tributes within the CONTEXT ( CTXT ) feature structure:
"Maximal Question Under Discussion ( MAX - QUD ), whose value is of sort question; [Footnote_9]  and Salient Utterance ( SAL - UTT ), whose value  is a set (singleton or empty) of elements of type  sign."
9 Questions are represented as semantic objects compris-ing a set of parameters—empty for a polar question—and a
"In information structure terms, SAL - UTT  can be thought of as a means of underspecifying  the subsequent focal (sub)utterance or as a poten-  tial parallel element."
MAX - QUD corresponds to  the ground of the dialogue at a given point.
"Since  SAL - UTT is a sign, it enables one to encode syn-  tactic categorial parallelism and, as we will see  below, also phonological parallelism."
"SAL - UTT  is computed as the (sub)utterance associated with  the role bearing widest scope within MAX - QUD . [Footnote_10]   Belowof parallelism, we willtoshowclarificationhow to queriesextend .this account"
"10 For Wh-questions, SAL - UTT is the wh-phrase associated with the PARAMS set of the question; otherwise, its possible values are either the empty set or the utterance associated with the widest scoping quantifier in MAX - QUD ."
"To account for elliptical constructions such as short answers and sluicing, Ginzburg and Sag posit a phrasal type headed-fragment-phrase (hd-frag-ph)—a subtype of hd-only-ph—governed by the constraint in (9)."
The various fragments ana-lyzed here will be subtypes of hd-frag-ph or else will contain such a phrase as a head daughter. [Footnote_11]  (9) HEAD v #  CTXT SAL - UTT &quot; CATCONT INDEX
"11 In the[REF_CITE]version of HPSG infor-mation about phrases is encoded by cross-classifying them in a multi-dimensional type hierarchy. Phrases are classi-fied not only in terms of their phrase structure schema or X-bar type, but also with respect to a further informational dimension of CLAUSALITY . Clauses are divided into inter alia declarative clauses (decl-cl), which denote propositions, and interrogative clauses (inter-cl) denoting questions. Each maximal phrasal type inherits from both these dimensions. This classification allows specification of systematic corre-lations between clausal construction types and types of se-mantic content."
HD - DTR $ CAT HEAD nominal % CONT INDEX
This constraint coindexes the head daughter with the SAL - UTT .
This will have the effect of ‘unifying in’ the content of the former into a con-textually provided content.
"A subtype of hd-frag-ph relevant to the current paper is (decl-frag-cl)— also a subtype of decl-cl—used to analyze short answers: proposition &amp; (&amp; . *&apos; This ),+-.+ /+ is &apos;0+. the +.+ 1 feature structure counterpart of the-abstract ."
"The content of this phrasal type is a proposition: whereas in most headed clauses the content is en-tirely (or primarily) derived from the head daugh-ter, here it is constructed for the most part from the contextually salient question."
"This provides the concerned situation and the nucleus, whereas if the fragment is (or contains) a quantifier, that quantifier must outscope any quantifiers already present in the contextually salient question."
"Before we turn to formalizing the coercion opera-tions and describing CE, we need to explain how on our view utterances get integrated in an agent’s IS."
The basic protocol we assume is given in (11) below. [Footnote_12] (11) Utterance processing protocol
"12 In this protocol, PENDING is a stack whose elements are (unintegrated) utterances."
"For an agent B with IS 9 : if an utterance : is Maximal in PENDING: (a) Try to: (1) find an assignment ; in 9 for &lt; , where &lt; is the (maximal description available for) the sign associated with : (2) update LATEST-MOVE with : : 1."
"If LATEST-MOVE is grounded, then FACTS:= FACTS + LATEST-MOVE; 2. LATEST-MOVE := = ?&lt; &gt;@;4A (3) React to content(u) according to querying/assertion pro-tocols. (4) If successful, : is removed from PENDING (b) Else: Repeat from stage (a) with MAX - QUD and ) CD SAL 1 EGFI - HKJMLNPODQNR: .FIS*JT:(UVU , where"
"UTT obtaining the various C values of coe B is the sign associated with LATEST-MOVE and coe B is one of the   (c) Else: make an utterance appropriate for a context such  that MAX - QUD and ) SAL - 1 UTT get values according to the specification in coe B 4: @&gt; &lt; , where coe B is one of the avail-   able coercion operations."
The protocol involves the assumption that an  agent always initially tries to integrate an utter-  ance by assuming it constitutes an adjacency pair  with the existing LATEST-MOVE.
"If this route   is blocked somehow, because the current utter-ance cannot be grounded or the putative resolu-tion leads to incoherence, only then does she try to repair by assuming the previous utterance is a clarification generated in accordance with the ex-isting coercion operations."
"If that too fails, then, she herself generates a clarification."
"Thus, the prediction made by this protocol is that A will tend to initially interpret (12(2)) as a response to her question, not as a clarification: (12) A(1): Who do you think is the only per-son that admires Mary?"
We now turn to formalizing the coercion op-erations we specified informally in section 2.
The first operation we define is parameter fo-cussing: parameter focussing B : (13)  root-cl
"This is to be understood as follows: given an ut-terance (whose associated sign is one) which sat-isfies the specification in the LHS of the rule, a CP may respond with any utterance which satisfies the specification in the RHS of the rule. [Footnote_13] More specifically, the input of the rules singles out a contextual parameter , which is the content of an element of the daughter set of the utterance 2 ."
13 The fact that both the LHS and the RHS of the rule are of type root-cl ensures that the rule applies only to signs as-sociated with complete utterances.
"Intuitively, is a parameter whose value is prob-lematic or lacking."
The sub-utterance 2 is speci-fied to constitute the value of the feature SAL - UTT associated with the context of the clarification ut-terance \ ^] .
"The descriptive content of \ ^] is a question, any question whose open proposition 3 (given in terms of the feature PROP ) is identi-cal to the (uninstantiated) content of the clarified utterance."
MAX - QUD associated with the clarifi-cation is fully specified as a question whose open proposition is 3 and whose PARAMS set consists of the ‘problematic’ parameter .
We can exemplify the effect of parameter fo-cussing with respect to clarifying an utterance of (7).
"The output this yields, when applied to Bo’s index 1 , is the partial specification in (14)."
"Such an utterance will have as its MAX - QUD a ques-tion cq ] paraphrasable as who _ , named Bo, are you asking if t left, whereas its SAL - UTT is the sub-utterance of Bo."
The content is underspeci-fied:  CONT MSG - ARG a question b c `  (14) PROP SAL - UTT d     question
This (partial) specification allows for clarifica-tion questions such as the following: (15) a. Did WHO leave? b. WHO? c. BO? (= Are you asking if BO left?)
"Given space constraints, we restrict ourselves to explaining how the clausal[REF_CITE], gets ana-lyzed."
This involves direct application of the type decl-frag-cl discussed above for short answers.
"The QUD-maximality of cq ] allows us to ana-lyze the fragment as a ‘short answer’ to cq ] , using the type bare-decl-cl."
And out of the proposition which emerges courtesy of bare-decl-cl a (polar) question is constructed using the type dir-is-int-cl. [Footnote_14] (16) S
14 The phrasal type dir-is-int-cl which constitutes the type of the mother node in (16) is a type that inter alia enables a polar question to be built from a head daughter whose con-tent is propositional. See[REF_CITE]for de-tails.
The second coercion operation we discussed previously is parameter identification: for a given problematic contextual parameter its out-put is a partial specification for a sign whose con-tent and MAX - QUD involve a question querying the content of that utterance parameter:
"To exemplify: when this operation is applied to (7), it will yield as output the partial specification in (18):"
This specification will allows for clarification questions such as the following: (19) a.
Who do you mean
BO? b. WHO? (= who is
Bo) c. Bo? (= who is Bo)
"We restrict attention to (19c), which is the most interesting but also tricky example."
"The tricky part arises from the fact that in a case such as this, in contrast to all previous examples, the fragment does not contribute its conventional content to the clausal content."
"Rather, as we suggested earlier, the semantic function of the fragment is merely to serve as an anaphoric element to the phono-logically identical to–be–clarified sub-utterance."
The content derives entirely from MAX - QUD .
"Such utterances can still be analyzed as subtypes of head-frag-ph, though not as decl-frag-cl, the short-answer/reprise sluice phrasal type we have been appealing to extensively."
"Thus, we posit constit(uent)-clar(ification)-int-cl, a new phrasal subtype of head-frag-ph and of inter-cl which en-capsulates the two idiosyncratic facets of such utterances, namely the phonological parallelism and the max-qud/content identity:"
"In this paper we offered an analysis of the types of representations needed to analyze CE, the requi-site operations thereon, and how these update ISs during grounding and clarification."
Systems which respond appropriately to CEs in general will need a great deal of background knowledge.
Even choosing among the responses in (5) might be a pretty knowledge intensive busi-ness.
"However, there are some clear strategies that might be pursued."
"For example, if Malvern has been discussed previously in the dialogue and understood then (5a,b) would not be appropriate responses."
In order to be able to build dialogue systems that can handle even some restricted as-pects of CEs we need to understand more about what the possible interpretations are and this is what we have attempted to do in this paper.
"We are currently working on a system which inte-grates SHARDS (see[REF_CITE], a system which processes dialogue ellipses) with GoDiS (see[REF_CITE], a dialogue sys-tem developed using TRINDIKIT, which makes use of ISs modelled on those suggested in the KOS framework."
Our aim in the near future is to in-corporate simple aspects of negotiative dialogue including CEs in a GoDiS-like system employing SHARDS.
This paper describes automatic tech-niques for mapping 9611 entries in a database of English verbs to Word-Net senses.
The verbs were initially grouped into 491 classes based on syntactic features.
Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval.
"Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes."
"The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment."
"Our goal is to map entries in a lexical database of 4076 English verbs automatically to Word-Net senses[REF_CITE],[REF_CITE]to support such applications as ma- chine translation and cross-language information retrieval."
"For example, the verb drop is multi-ply ambiguous, with many potential translations in Spanish: bajar, caerse, dejar caer, derribar, disminuir,echar, hundir, soltar, etc."
"The database specifies a set of interpretations for drop, depend-ing on its context in the source-language (SL)."
In-clusion of WordNet senses in the database enables the selection of an appropriate verb in the target language (TL).
"Final selection is based on a fre-quency count of WordNet senses across all classes to which the verb belongs—e.g., disminuir is se-lected when the WordNet sense corresponds to the meaning of drop in Prices dropped."
Our task differs from standard word sense dis-ambiguation (WSD) in several ways.
"First, the words to be disambiguated are entries in a lexical database, not tokens in a text corpus."
"Second, we take an “all-words” rather than a “lexical-sample” approach[REF_CITE]: All words in the lexical database “text” are disam-biguated, not just a small number for which de-tailed knowledge is available."
"Third, we replace the contextual data typically used for WSD with information about verb senses encoded in terms of thematic grids and lexical-semantic representa-tions[REF_CITE]."
"Fourth, whereas a single word sense for each token in a text corpus is often assumed, the absence of sentential context leads to a situation where several WordNet senses may be equally appropriate for a database entry."
"Indeed, as distinctions between WordNet senses can be fine-grained[REF_CITE], it may be un-clear, even in context, which sense is meant."
"The verb database contains mostly syntactic in- formation about its entries, much of which ap-plies at the class level within the database."
"Word-Net, on the other hand, is a significant source for information about semantic relationships, much of which applies at the “synset” level (“synsets” are WordNet’s groupings of synonymous word senses)."
Mapping entries in the database to their corresponding WordNet senses greatly extends the semantic potential of the database.
"We use an existing classification of 4076 English verbs, based initially on English Verbs Classes and Alternations[REF_CITE]and extended through the splitting of some classes into sub-classes and the addition of new classes."
"The re-sulting 491 classes (e.g., “Roll Verbs, Group I”, which includes drift, drop, glide, roll, swing) are referred to here as Levin+ classes."
"As verbs may be assigned to multiple Levin+ classes, the actual number of entries in the database is larger, 9611."
"Following the model[REF_CITE], each Levin+ class is associated with a thematic grid (henceforth abbreviated -grid), which sum-marizes a verb’s syntactic behavior by specify-ing its predicate argument structure."
"For exam-ple, the Levin+ class “Roll Verbs, Group I” is as-sociated with the -grid [th goal], in which a theme and a goal are used (e.g., The ball dropped to the ground). [Footnote_1] Each -grid specification corre-sponds to a Grid class."
"1 There is also a Levin+ class “Roll Verbs, Group II” which is associated with the -grid [th particle(down)], in which a theme and a particle ‘down’ are used (e.g., The ball dropped down)."
"WordNet, the lexical resource to which we are mapping entries from the lexical database, groups synonymous word senses into “synsets” and struc-tures the synsets into part-of-speech hierarchies."
"Our mapping operation uses several other data el-ements pertaining to WordNet: semantic relation-ships between synsets, frequency data, and syn-tactic information."
"Seven semantic relationship types exist be-tween synsets, including, for example, antonymy, hyperonymy, and entailment."
Synsets are often related to a half dozen or more other synsets; they may be related to multiple synsets through a single relationship or may be related to a single synset through multiple relationship types.
Our frequency data for WordNet senses is de-rived from SEMCOR —a semantic concordance in-corporating tagging of the Brown corpus with WordNet senses. [Footnote_2]
"2 For further information see the WordNet manuals, sec-tion 7, SEMCOR[URL_CITE]"
"Syntactic patterns (“frames”) are associated with each synset, e.g., Somebody s something; Something s; Somebody s somebody into V-ing something."
Our mapping of verbs in Levin+ classes to WordNet senses relies in part on the relation be-tween thematic roles in Levin+ and verb frames in WordNet.
Both reflect how many and what kinds of arguments a verb may take.
"However, con-structing a direct mapping between -grids and WordNet frames is not possible, as the underly-ing classifications differ in significant ways."
The correlations between the two sets of data are better viewed probabilistically.
Table 1 illustrates the relation between Levin+ classes and WordNet for the verb drop.
"In our multilingual applications (e.g., lexical selection in machine translation), the Grid information pro-vides a context-based means of associating a verb with a Levin+ class according to its usage in the SL sentence."
"The WordNet sense possibilities are thus pared down during SL analysis, but not suffi-ciently for the final selection of a TL verb."
"For ex-ample, Levin+ class 9.4 has three possible Word-Net senses for drop."
"However, the WordNet sense 8 is not associated with any of the other classes; thus, it is considered to have a higher “information content” than the others."
The upshot is that the lexical-selection routine prefers dejar caer over other translations such as derribar and bajar. [Footnote_3]
"3 This lexical-selection approach is an adaptation of the notion of reduction in entropy, measured by information ga[REF_CITE]. Using information content to quan-tify the “value” of a node in the WordNet hierarchy has also been used for measuring semantic similarity in a tax-onomy[REF_CITE]. More recently, context-based mod-els of disambiguation have been shown to represent signif-icant improvements over the baseline[REF_CITE],[REF_CITE]."
"The other classes are similarly associated with ap- propriate TL verbs during lexical selection: dis-minuir (class 45.6), hundir (class 47.7), and bajar (class 51.3.1). 4"
"We began with the lexical database[REF_CITE], which contains a significant number of WordNet-tagged verb entries."
"Some of the as-signments were in doubt, since class splitting had occurred subsequent to those assignments, with all old WordNet senses carried over to new sub-classes."
New classes had also been added since the manual tagging.
"It was determined that the tagging for only 1791 entries—including 1442 verbs in 167 classes—could be considered stable; for these entries, 2756 assignments of WordNet senses had been made."
"Data for these entries, taken from both WordNet and the verb lexicon, constitute the training data for this study."
"The following probabilities were generated from the training data:   ! &quot; # $&amp;%(!&apos; )+#$[Footnote_4]*%5&apos;613,./- # /* 0213# , where 7 is a relation (of relationship type 8 , e.g., synonymy) between two synsets, : and 96; , where 9 : is mapped to by a verb in Grid class G : and 96; is mapped to by a verb in Grid class G ; ."
4 The full set of Spanish translations is selected from WordNet associations developed in the EuroWordNet effort[REF_CITE].
"This is the probability that if one synset is related to another through a particular relationship type, then a verb mapped to the first synset will belong to the same Grid class as a verb mapped to the second synset."
Computed values generally range between .3 and .35. & gt;&lt; @= ?
"BA /!G&quot;H#&amp;$ %(&apos;IKJML) #&amp;$ %5&apos;313, -NJML# 0 13# , where 7 is as above, except that s : is mapped to by a verb in Levin+ class L+ : and s ; is mapped to by a verb in Levin+ class L+ ; ."
"This is the probability that if one synset is related to another through a particular relationship type, then a verb mapped to the first synset will belong to the same Levin+ class as a verb mapped to the second synset."
Computed values generally range between .25 and .3. ./ST= !
"YZ&quot; # $2[5\^] _ )a`cbVd ( ] _# , where  e is the occurrence of the entire # $25[ ^\ ] _ -grid # f for verb entry g and cf &amp;Y X e is the occurrence of the entire frame sequence h for a WordNet sense to which verb entry g is mapped."
This is the prob-ability that a verb in a Levin+ class is mapped to a WordNet verb sense with some specific combi-nation of frames.
"Values average only .11, but in some cases the probability is 1.0. ?."
"R /ST= / Y &quot; # $25[ ^\ ] _ a`cbVd) (] _# , where  e is the occurrence of the single # 2$ 5[ \^] _ -grid # component f for verb entry g and cf &amp;Y X e is the occur- rence of the single frame h for a WordNet sense to which verb entry g is mapped."
This is the proba-bility that a verb in a Levin+ class with a partic-ular -grid component (possibly among others) is mapped to a WordNet verb sense assigned a spe-cific frame (possibly among others).
"Values aver-age .20, but in some cases the probability is 1.0. mln /&quot; # &amp;$ vcwx1y# , where .{ is an occurrence of tag 9 (for a particular # $&amp;v _ 13# synset) z in SEMCOR and ze is an occurrence of any of a set of tags for verb g in SEMCOR , with 9 being one of the senses possible for verb g ."
This probability is the prior probability of specific WordNet verb senses.
"Values average .11, but in some cases the probability is 1.0."
"In addition to the foregoing data elements, based on the training set, we also made use of a semantic similarity measure, which reflects the confidence with which a verb, given the total set of verbs assigned to its Levin+ class, is mapped to a specific WordNet sense."
"This represents an implementation of a class disambiguation algo-rithm[REF_CITE], modified to run against the WordNet verb hierarchy. [Footnote_5]"
"5 The assumption underlying this measure is that the ap-propriate word senses for a group of semantically related words should themselves be semantically related. Given WordNet’s hierarchical structure, the semantic similarity be-tween two WordNet senses corresponds to the degree of in-formativeness of the most specific concept that subsumes them both."
"We also made a powerful “same-synset as-sumption”: If (1) two verbs are assigned to the same Levin+ class, (2) one of the verbs g : has been mapped to a specific WordNet sense : , and (3) the other verb ; has a[REF_CITE]; syn-onymous with 9 : , then g ; should be mapped to 9 ; ."
"Since WordNet groups synonymous word senses into “synsets,” : and 96; would correspond to the same synset."
"Since Levin+ verbs are mapped to WordNet senses via their corresponding synset identifiers, when the set of conditions enumer-ated above are met, the two verb entries would be mapped to the same WordNet synset."
"As an example, the two verbs tag and mark have been assigned to the same Levin+ class."
"In WordNet, each occurs in five synsets, only one in which they both occur."
"If tag has a WordNet synset assigned to it for the Levin+ class it shares with mark, and it is the synset that covers senses of both tag and mark, we can safely assume that that synset is also appropriate for mark, since in that context, the two verb senses are synonymous."
"Subsequent to the culling of the training set, sev-eral processes were undertaken that resulted in full mapping of entries in the lexical database to WordNet senses."
"Much, but not all, of this map-ping was accomplished manually."
"Each entry whose WordNet senses were as-signed manually was considered by at least two coders, one coder who was involved in the entire manual assignment process and the other drawn from a handful of coders working independently on different subsets of the verb lexicon."
"In the manual tagging, if a WordNet sense was consid-ered appropriate for a lexical entry by any one of the coders, it was assigned."
The kappa coefficient ( | ) of intercoder agreement was .47 for a first round of manual tagging and (only) .24 for a second round of more problematic cases. [Footnote_6]
"6 The kappa statistic measures the degree to which pair-wise agreement of coders on a classification task surpasses what would be expected by chance; the standard definition of this coefficient is: !!cc3c!!c , where QV is the actual percentage of agreement and ! is the expected percentage of agreement, averaged over all pairs ofassignments. Severaladjustmentsin the computation of the kappa coefficient were made necessary by the possible assignmentof multiplesenses for each verbin a Levin+ class, since without prior knowledge of how many senses are to be assigned, there is no basis on which to compute ! ."
"While the full tagging of the lexical database may make the automatic tagging task appear su-perfluous, the low rate of agreement between coders and the automatic nature of some of the tagging suggest there is still room for adjust-ment of WordNet sense assignments in the verb database."
"On the one hand, even the higher of the kappa coefficients mentioned above is signifi-cantly lower than the standard suggested for good reliability ( ~| } ) or even the level where ten-tative conclusions may be drawn (  H|   )[REF_CITE],[REF_CITE]."
"On the other hand, if the automatic assignments agree with human coding at levels comparable to the de-gree of agreement among humans, it may be used to identify current assignments that need review and to suggest new assignments for consideration."
"In addition, consistency checking is done more easily by machine than by hand."
"For example, the same-synset assumption is more easily enforced automatically than manually."
"When this assump-tion is implemented for the 2756 senses in the training set, another 967 sense assignments are generated, only 131 of which were actually as-signed manually."
"Similarly, when this premise is enforced on the entirety of the lexical database of 13452 assignments, another 5059 sense assign-ments are generated."
"If the same-synset assump-tion is valid and if the senses assigned in the database are accurate, then the human tagging has a recall of no more than 73%."
"Because a word sense was assigned even if only one coder judged it to apply, human coding has been treated as having a precision of 100%."
"How-ever, some of the solo judgments are likely to have been in error."
"To determine what proportion of such judgments were in reality precision failures, a random sample of 50 WordNet senses selected by only one of the two original coders was in-vestigated further by a team of three judges."
"In this round, judges rated WordNet senses assigned to verb entries as falling into one of three cate-gories: definitely correct, definitely incorrect, and arguable whether correct."
"As it turned out, if any one of the judges rated a sense definitely correct, another judge independently judged it definitely correct; this accounts for 31 instances."
No con-sensus was reached on the remaining 6 instances.
Extrapolating from this sample to the full set of solo judgments in the database leads to an estimate that approximately 1725 (26% of 6636 solo judg-ments) of those senses are incorrect.
This suggests that the precision of the human coding is approx-imately 87%.
"The upper bound for this task, as set by human performance, is thus 73% recall and 87% preci-sion."
"The lower bound, based on assigning the WordNet sense with the greatest prior probability, is 38% recall and 62% precision."
Recent work[REF_CITE]has demonstrated improvement in part-of-speech tag- ging when the outputs of multiple taggers are combined.
"When the errors of multiple classi-fiers are not significantly correlated, the result of combining votes from a set of individual classi-fiers often outperforms the best result from any single classifier."
"Using a voting strategy seems es-pecially appropriate here: The measures outlined in Section 3 average only 41% recall on the train-ing set, but the senses picked out by their highest values vary significantly."
"The investigations undertaken used both sim-ple and aggregate voters, combined using var-ious voting strategies."
The simple voters were the [Footnote_7] measures previously introduced. [Footnote_7]
7 Only 6 measures (including the semantic similarity mea-sure) were set out in the earlier section; the measures total 7 because Indv frame probability is used in two different ways.
7 Only 6 measures (including the semantic similarity mea-sure) were set out in the earlier section; the measures total 7 because Indv frame probability is used in two different ways.
"In addi-tion, three aggregate voters were generated: (1) the product of the simple measures (smoothed so that zero values wouldn’t offset all other mea-sures); (2) the weighted sum of the simple mea-sures, with weights representing the percentage of the training set assignments correctly identified by the highest score of the simple probabilities; and (3) the maximum score of the simple measures."
"Using these data, two different types of vot-ing schemes were investigated."
"The schemes dif-fer most significantly on the circumstances un-der which a voter casts its vote for a WordNet sense, the size of the vote cast by each voter, and the circumstances under which a WordNet sense was selected."
We will refer to these two schemes as Majority Voting Scheme and Threshold Voting Scheme.
"Although we do not know in advance how many WordNet senses should be assigned to an entry in the lexical database, we assume that, in general, there is at least one."
"In line with this intuition, one strategy we investigated was to have both simple and aggregate measures cast a vote for whichever sense(s) of a verb in a Levin+ class received the highest (non-zero) value for that measure."
Ten variations are given here:
PriorProb: Prior Probability of WordNet senses
"SemSim: Semantic Similarity all variations of this voting scheme, both with and without enforcement of the same-synset as-sumption."
"If we use the harmonic mean of recall and precision as a criterion for comparing results, the best voting scheme is MajAggr, with 58% re-call and 72% precision without enforcement of the same-synset assumption."
"Note that if the same-synset assumption is correct, the drop in precision that accompanies its enforcement mostly reflects inconsistencies in human judgments in the train-ing set; the true precision value for MajAggr after enforcing the same-synset assumption is probably close to 67%."
"Of the simple voters, only PriorProb and Sem-Sim are individually strong enough to warrant dis-cussion."
"Although PriorProb was used to estab-lish our lower bound, SemSim proves to be the stronger voter, bested only by MajAggr (the ma-jority vote of SimpleProd and SimpleWtdSum) in voting that enforces the same-synset assumption."
Both PriorProb and SemSim provide better results than the majority vote of all 7 simple voters (Ma-jSimpleSgl) and the majority vote of all 21 pairs of simple voters (MajSimplePair).
"Moreover, the inclusion of MajSimpleSgl and MajSimplePair in a majority vote with MajAggr (in MajSgl+Aggr and MapPair+Aggr, respectively) turn in poorer results than MajAggr alone."
"The poor performance of MajSimpleSgl and MajSimplePair do not point, however, to a gen-eral failure of the principle that multiple voters are better than individual voters."
"SimpleProd, the product of all simple measures, and SimpleWtd-Sum, the weighted sum of all simple measures, provide reasonably strong results, and a majority vote of the both of them (MajAggr) gives the best results of all."
"When they are joined by SemSim in Maj3Best, they continue to provide good results."
"The bottom line is that SemSim makes the most significant contribution of any single simple voter, while the product and weighted sums of all simple voters, in concert with each other, provide the best results of all with this voting scheme."
"The second voting strategy first identified, for each simple and aggregate measure, the threshold value at which the product of recall and precision scores in the training set has the highest value if that threshold is used to select WordNet senses."
"During the voting, if a WordNet sense has a higher score for a measure than its threshold, the measure votes for the sense; otherwise, it votes against it."
The weight of the measure’s vote is the precision-recall product at the threshold.
This voting strat-egy has the advantage of taking into account each individual attribute’s strength of prediction.
Five variations on this basic voting scheme were investigated.
"In each, senses were selected if their vote total exceeded a variation-specific threshold."
Table 3 summarizes recall and pre-cision for these variations at their optimal vote thresholds.
"In the AutoMap+ variation, Grid and Levin+ probabilities abstain from voting when their val-ues are zero (a common occurrence, because of data sparsity in the training set); the same-synset assumption is automatically implemented."
AutoMap- differs in that it disregards the Grid and Levin+ probabilities completely.
"The Triples variation places the simple and composite mea-sures into three groups, the three with the high-est weights, the three with the lowest weights, and the middle or remaining three."
"Voting first occurs within the group, and the group’s vote is brought forward with a weight equaling the sum of the group members’ weights."
This variation also adds to the vote total if the sense was as-signed in the training data.
"The Combo variation is like Triples, but rather than using the weights and thresholds calculated for the single measures from the training data, this variation calculates weights and thresholds for combinations of two, three, four, five, six, and, seven measures."
"Finally, the Combo&amp;Auto variation adds the same-synset assumption to the previous variation."
"Although not evident in Table 3 because of rounding, AutoMap- has slightly higher values for both recall and precision than does AutoMap+, giving it the highest recall-precision product of the threshold voting schemes."
This suggests that the Grid and Levin+ probabilities could profitably be dropped from further use.
"Of the more exotic voting variations, Triples voting achieved results nearly as good as the Au-toMap voting schemes, but the Combo schemes fell short, indicating that weights and thresholds are better based on single measures than combi-nations of measures."
"The voting schemes still leave room for improve-ment, as the best results (58% recall and 72% pre-cision, or, optimistically, 63% recall and 67% pre-cision) fall shy of the upper bound of 73% re-call and 87% precision for human coding. [Footnote_9] At the same time, these results are far better than the lower bound of 38% recall and 62% precision for the most frequent WordNet sense."
9 The criteria for the majority voting schemes preclude their assigning more than 2 senses to any single database en-try. Controlledrelaxation of thesecriteriamay achievesome-what better results.
"As has been true in many other evaluation stud-ies, the best results come from combining classi-fiers (MajAggr): not only does this variation use a majority voting scheme, but more importantly, the two voters take into account all of the sim-ple voters, in different ways."
"The next-best re-sults come from Maj3Best, in which the three best single measures vote."
"We should note, however, that the single best measure, the semantic similar-ity measure from SemSim, lags only slightly be-hind the two best voting schemes."
This research demonstrates that credible word sense disambiguation results can be achieved without recourse to contextual data.
"Lexical re-sources enriched with, for example, syntactic in-formation, in which some portion of the resource is hand-mapped to another lexical resource may be rich enough to support such a task."
"The de-gree of success achieved here also owes much to the confluence of WordNet’s hierarchical struc-ture and SEMCOR tagging, as used in the compu-tation of the semantic similarity measure, on the one hand, and the classified structure of the verb lexicon, which provided the underlying groupings used in that measure, on the other hand."
We introduce a new categorial formal-ism based on intuitionistic linear logic.
"This formalism, which derives from current type-logical grammars, is ab-stract in the sense that both syntax and semantics are handled by the same set of primitives."
"As a consequence, the formalism is reversible and provides different computational paradigms that may be freely composed together."
Type-logical grammars offer a clear cut between syntax and semantics.
"On the one hand, lexical items are assigned syntactic categories that com-bine via a categorial logic akin to the Lambek cal-culus[REF_CITE]."
"On the other hand, we have so-called semantic recipes, which are ex-pressed as typed λ-terms."
"The syntax-semantics interface takes advantage of the Curry-Howard correspondence, which allows semantic readings to be extracted from categorial deductions (van[REF_CITE])."
These readings rely upon a homomorphism between the syntactic categories and the semantic types.
The distinction between syntax and semantics is of course relevant from a linguistic point of view.
"This does not mean, however, that it must be wired into the computational model."
"On the contrary, a computational model based on a small set of primitives that combine via simple compo-sition rules will be more flexible in practice and easier to implement."
"In the type-logical approach, the syntactic con-tents of a lexical entry is outlined by the following patern: & lt;atom&gt; : &lt;syntactic category&gt; On the other hand, the semantic contents obeys the following scheme: & lt;λ-term&gt; : &lt;semantic type&gt;"
"This asymmetry may be broken by: 1. allowing λ-terms on the syntactic side (atomic expressions being, after all, partic-ular cases of λ-terms), 2. using the same type theory for expressing both the syntactic categories and the seman-tic types."
The first point is a powerfull generalization of the usual scheme.
"It allows λ-terms to be used at a syntactic level, which is an approach that has been advocated[REF_CITE]."
The sec-ond point may be satisfied by dropping the non-commutative (and non-associative) aspects of cat-egorial logics.
"This implies that, contrarily to the usual categorial approaches, word order con-straints cannot be expressed at the logical level."
As we will see this apparent loss in expressive power is compensated by the first point.
"In this section, we define an elementary gram-matical formalism based on the ideas presented in the introduction."
This elementary formalism is founded on the multiplicative fragment of linear logic[REF_CITE].
"For this reason, we call it a multiplicative kernel."
Possible extensions based on other fragments of linear logic are discussed in Section 5.
We first introduce the mathematical apparatus that is needed in order to define our notion of an ab-stract categorial grammar.
Let A be a set of atomic types.
"The set T (A) of linear implicative types built upon A is induc-tively defined as follows: 1. if a ∈ A, then a ∈ T (A); 2. if α, β ∈ T (A), then (α −◦ β) ∈ T (A)."
We now introduce the notion of a higher-order linear signature.
"It consists of a triple Σ = hA, C, τi, where: 1."
A is a finite set of atomic types; 2. C is a finite set of constants; 3. τ : C → T (A) is a function that assigns to each constant in C a linear implicative type in T (A).
Let X be a infinite countable set of λ-variables.
"The set Λ(Σ) of linear λ-terms built upon a higher-order linear signature Σ = hA, C, τi is in-ductively defined as follows: 1. if c ∈ C, then c ∈ Λ(Σ); 2. if x ∈ X, then x ∈ Λ(Σ); 3. if x ∈ X, t ∈ Λ(Σ), and x occurs free in t exactly once, then (λx. t) ∈ Λ(Σ); 4. if t, u ∈ Λ(Σ), and the sets of free variables of t and u are disjoint, then (t u) ∈ Λ(Σ)."
"Λ(Σ) is provided with the usual notion of cap-ture avoiding substitution, α-conversion, and β-reducti[REF_CITE]."
"Given a higher-order linear signature Σ = hA, C, τi, each linear λ-term in Λ(Σ) may be as-signed a linear implicative type in T (A)."
This type assignment obeys an inference system whose judgements are sequents of the following form:
Γ − Σ t : α where: 1.
"Γ is a finite set of λ-variable typing declara-tions of the form ‘x : β’ (with x ∈ X and β ∈ T (A)), such that any λ-variable is de-clared at most once; 2. t ∈ Λ(Σ); 3. α ∈ T (A)."
"The axioms and inference rules are the following: − Σ c : τ(c) (cons) x : α − Σ x : α (var) Γ, x : α − Σ t : β (abs)"
"Γ − Σ (λx. t) : (α −◦ β) Γ − Σ t : (α −◦ β) ∆ − Σ u : α (app) Γ, ∆ − Σ (t u) : β"
"We now introduce the abstract notions of a vocab-ulary and a lexicon, on which the central notion of an abstract categorial grammar is based."
A vocabulary is simply defined to be a higher-order linear signature.
"Given two vocabularies Σ 1 = hA 1 , C 1 , τ 1 i and Σ 2 = hA 2 ,C 2 ,τ 2 i, a lexicon L from Σ 1 to Σ 2 (in notation, L : Σ 1 → Σ 2 ) is defined to be a pair L = hF, Gi such that: 1. F : A 1 → T (A 2 ) is a function that inter-prets the atomic types of Σ 1 as linear im-plicative types built upon A 2 ; 2."
"G : C 1 → Λ(Σ 2 ) is a function that interprets the constants of Σ 1 as linear λ-terms built upon Σ 2 ; 3. the interpretation functions are compatible with the typing relation, i.e., for any c ∈ C 1 , the following typing judgement is derivable: − Σ 2 G(c) :"
"F̂(τ 1 (c)), where F̂ is the unique homomorphic exten-sion of F ."
"As stated in Clause 3 of the above defini-tion, there exists a unique type homomorphism F̂ :"
"T (A 1 ) → T (A 2 ) that extends F. Simi-larly, there exists a unique λ-term homomorphism Ĝ : Λ(Σ 1 ) → Λ(Σ 2 ) that extends G. In the se-quel, when ‘L ’ will denote a lexicon, it will also denote the homorphisms F̂ and Ĝ induced by this lexicon."
"In any case, the intended meaning will be clear from the context."
"Condition 3, in the above definition of a lexi-con, is necessary and sufficient to ensure that the homomorphisms induced by a lexicon commute with the typing relations."
"In other terms, for any lexicon L : Σ 1 → Σ 2 and any derivable judge-ment x 0 : α 0 , . . . , x n : α n − Σ 1 t : α the following judgement x 0 : L (α 0 ), . . . , x n : L (α n ) − Σ 2 L (t): L (α) is derivable."
"This property, which is reminis-cent of Montague’s homomorphism requirement[REF_CITE], may be seen as an abstract realization of the compositionality principle."
We are now in a position of giving the defini-tion of an abstract categorial grammar.
"An abstract categorial grammar (ACG) is a quadruple G = hΣ 1 , Σ 2 , L , si where: 1."
"Σ 1 = hA 1 ,C 1 ,τ 1 i and Σ 2 = hA 2 ,C 2 ,τ 2 i are two higher-order linear signatures; Σ 1 is called the abstract vovabulary and Σ 2 is called the object vovabulary; 2."
L : Σ 1 → Σ 2 is a lexicon from the abstract vovabulary to the object vovabulary; 3. s ∈ T (A 1 ) is a type of the abstract vocabu-lary; it is called the distinguished type of the grammar.
"Any ACG generates two languages, an abstract language and an object language."
The abstract language generated by G (A(G)) is defined as follows:
A(G ) = {t ∈ Λ(Σ 1 ) | − Σ 1 t: s is derivable}
"In words, the abstract language generated by G is the set of closed linear λ-terms, built upon the abstract vocabulary Σ 1 , whose type is the distin-guished type s. On the other hand, the object lan-guage generated by G (O(G )) is defined to be the image of the abstract language by the term homo-morphism induced by the lexicon L :"
O(G ) = {t ∈ Λ(Σ 2 ) | ∃u ∈ A(G ). t = L (u)}
"It may be useful of thinking of the abstract lan-guage as a set of abstract grammatical structures, and of the object language as the set of concrete forms generated from these abstract structures."
Section 4 provides examples of ACGs that illus-trate this interpretation.
"In order to exemplify the concepts introduced so far, we demonstrate how to accomodate the PTQ fragment[REF_CITE]."
We concentrate on Montague’s famous sentence:
John seeks a unicorn (1)
"For the purpose of the example, we make the two following assumptions: 1. the formalism provides an atomic type ‘string’ together with a binary associative operator ‘+’ (that we write as an infix op-erator for the sake of readability); 2. we have the usual logical connectives and quantifiers at our disposal."
"We will see in Section 4 and 5 that these two as-sumptions, in fact, are not needed."
"In order to handle the syntactic part of the ex-ample, we define an ACG (G 12 )."
The first step consists in defining the two following vocabular-ies:
"Σ 1 = h {n, np, s}, {J, S re , S dicto , A, U}, {J 7→ np, S re 7→ (np −◦ (np −◦ s)), S dicto 7→ (np −◦ (np −◦ s)), A 7→ (n −◦ np), U 7→ n} i Σ 2 = h {string}, {John, seeks, a, unicorn}, {John 7→ string, seeks 7→ string, a 7→ string, unicorn 7→ string} i"
"Then, we define a lexicon L 12 from the abstract vocabulary Σ 1 to the object vocabulary Σ 2 :"
"L 12 = h {n 7→ string, np 7→ string, s 7→ string}, {J 7→ John, S re 7→ λx. λy. x + seeks + y, S dicto 7→ λx. λy. x + seeks + y, A 7→ λx. a + x, U 7→ unicorn} i"
"Finally we have G 12 = hΣ 1 , Σ 2 , L 12 , si."
"The semantic part of the example is handled by another ACG (G 13 ), which shares with G 12 the same abstract language."
The object language of this second ACG is defined as follows:
"Σ 3 = h {e, t}, { JOHN , TRY - TO , FIND , UNICORN }, { JOHN 7→ e, TRY - TO 7→ (e −◦ ((e −◦ t) −◦ t)),"
"FIND 7→ (e −◦ (e −◦ t)), UNICORN 7→ (e −◦ t)} i"
"Then, a lexicon from Σ 1 to Σ 3 is defined:"
"L 13 = h {n 7→ (e −◦ t), np 7→ ((e −◦ t) −◦ t), s 7→ t}, {J 7→ λP. P JOHN , S re 7→ λP. λQ. Q (λx."
"P (λy. TRY - TO y (λz. FIND z x))), S dicto 7→ λP. λQ. P (λx."
"TRY - TO x (λy. Q (λz. FIND y z))), A 7→ λP. λQ. ∃x. P x ∧ Q x, U 7→ λx."
UNICORN x} i
"This allows the[REF_CITE]to be defined as hΣ 1 , Σ 3 , L 13 , si."
The abstract language shared by G 12 and G 13 contains the two following terms:
S re J (A U) (2) S dicto J (A U) (3)
The syntactic lexicon L 12 applied to each of these terms yields the same image.
It β-reduces to the following object term:
John + seeks + a + unicorn
"On the other hand, the semantic lexicon L 13 yields the de re reading when applied to (2): ∃x. UNICORN x ∧ TRY - TO JOHN (λz. FIND z x) and it yields the de dicto reading when applied to (3):"
TRY - TO JOHN (λy. ∃x.
UNICORN x ∧ FIND y x)
Our handling of the two possible readings of (1) differs from the type-logical account[REF_CITE]and[REF_CITE].
The main difference is that our abstract vocabulary con-tains two constants corresponding to seek.
"Con-sequently, we have two distinct entries in the se-mantic lexicon, one for each possible reading."
This is only a matter of choice.
We could have adopt Morrill’s solution (which is closer to Mon-tague original analysis) by having only one ab-stract constant S together with the following type assignment:
S 7→ (np −◦ (((np −◦ s) −◦ s) −◦ s))
"Then the types of J and A, and the two lexicons should be changed accordingly."
The semantic lex-icon of this alternative solution would be simpler.
"The syntactic lexicon, however, would be more involved, with entries such as:"
S 7→ λx. λy. x + seeks + y (λz. z)
A 7→ λx. λy. y (a + x)
"Compositional semantics associates meanings to utterances by assigning meanings to atomic items, and by giving rules that allows to compute the meaning of a compound unit from the meanings of its parts."
"In the type logical approach, follow-ing the Montagovian tradition, meanings are ex-pressed as typed λ-terms and combine via func-tional application."
They present a deduc-tive approach in which linear logic is used as a glue language for assembling meanings.
Their approach is more in the tradition of logic pro-gramming.
The grammatical framework introduced in the previous section realizes the compositionality principle in a abstract way.
"Indeed, it provides compositional means to associate the terms of a given language to the terms of some other language."
Both the applicative and deductive paradigms are available.
"In our framework, the applicative paradigm con-sists simply in computing, according to the lex-icon of a given grammar, the object image of an abstract term."
From a computational point of view it amounts to performing substitution and β-reduction.
"The deductive paradigm, in our setting, answers the following problem: does a given term, built upon the object vocabulary of an ACG, belong to the object language of this ACG."
It amounts to a kind of proof-search that has been de-scribed[REF_CITE]and[REF_CITE].
"This proof-search relies on lin-ear higher-order matching, which is a decidable problem (de[REF_CITE])."
"The example developped in Section 2.3 suggests a third paradigm, which is obtained as the com-position of the applicative paradigm with the de-ductive paradigm."
We call it the transductive paradigm because it is reminiscent of the math-ematical notion of transduction (see Section 4.2).
"This paradigm amounts to the transfer from one object language to another object language, using a common abstract language as a pivot."
"In this section, we illustrate the expressive power of ACGs by showing how some other families of formal grammars may be subsumed."
"It must be stressed that we are not only interested in a weak form of correspondence, where only the gener-ated languages are equivalent, but in a strong form of correspondence, where the grammatical struc-tures are preserved."
"First of all, we must explain how ACGs may manipulate strings of symbols."
"In other words, we must show how to encode strings as linear λ-terms."
The solution is well known: it suffices to represent strings of symbols as compositions of functions.
"Consider an arbitrary atomic type ∗, and define the type ‘string’ to be (∗ −◦ ∗)."
"Then, a string such as ‘abbac’ may be repre-sented by the linear λ-term λx. a (b (b (a (c x)))), where the atomic strings ‘a’, ‘b’, and ‘c’ are declared to be constants of type (∗ −◦ ∗)."
"In this setting, the empty word ( ) is represented by the identity function (λx.x) and concatena-tion (+) is defined to be functional composition (λf. λg. λx. f (g x)), which is indeed an associa-tive operator that admits the identity function as a unit."
"Let G = hT, N, P, Si be a context-free grammar, where T is the set of terminal symbols, N is the set of non-terminal symbol, P is the set of rules, and S is the start symbol."
We write L(G) for the language generated by G.
"We show how to con-struct an ACG G G = hΣ 1 , Σ 2 , L , Si correspond-ing to G."
"The abstract vocabulary Σ 1 = hA 1 , C 1 , τ 1 i is defined as follows: 1."
The set of atomic types A 1 is defined to be the set of non-terminal symbols N. 2.
"The set of constants C 1 is a set of symbols in 1-1-correspondence with the set of rules P . 3. Let c ∈ C 1 and let ‘X → ω’ be the rule cor-responding to c. τ 1 is defined to be the func-tion that assigns the type [[ω]] X to c, where [[·]] X obeys the following inductive defini-tion: (a) [[ ]] X = X; (b) [[Y ω]] X = (Y −◦ [[ω]] X ), for Y ∈ N; (c) [[aω]] X = [[ω]] X , for a ∈ T."
"The definition of the object vocabulary Σ 2 = hA 2 , C 2 , τ 2 i is as follows: 1."
A 2 is defined to be {∗}. 2.
The set of constants C 2 is defined to be the set of terminal symbols T . 3. τ 2 is defined to be the function that assigns the type ‘string’ to each c ∈ C 2 .
"It remains to define the lexicon L = hF, Gi: 1. F is defined to be the function that interprets each atomic type a ∈ A 1 as the type ‘string’. 2. Let c ∈ C 1 and let ‘X → ω’ be the rule corresponding to c. G is de-fined to be the function that interprets c as λx 1 . . . . λx n . |ω|, where x 1 . . . x n is the se-quence of λ-variables occurring in |ω|, and | · | is inductively defined as follows: (a) | | = λx. x; (b) |Y ω| = y + |ω|, for Y ∈ N, and where y is a fresh λ-variable; (c) |aω| = a + |ω|, for a ∈ T ."
"It is then easy to prove that G G is such that: 1. the abstract language A(G G ) is isomorphic to the set of parse-trees of G. 2. the language generated by G coincides with the object language of G G , i.e., O(G G ) = L(G)."
For instance consider the CFG whose produc-tion rules are the following:
"S → , S → aSb, which generates the language a n b n ."
"The cor-responding ACG has the following abstract lan-guage, object language, and lexicon:"
"Σ 1 = h {S}, {A, B}, {A 7→ S, B 7→ ((S −◦ S)} i Σ 2 = h {∗}, {a, b}, {a 7→ string, b 7→ string} i L = h {S 7→ string}, {A 7→ λx. x, B 7→ λx. a + x + b} i"
"Regular grammars being particular cases of context-free grammars, they may be handled by the same construction."
The resulting ACGs (which we will call “regular ACGs” for the pur-pose of the discussion) may be seen as finite state automata.
"The abstract language of a regular ACG correspond then to the set of accepting se-quences of transitions of the corresponding au-tomaton, and its object language to the accepted language."
"More interestingly, rational transducers may also be accomodated."
"Indeed, two regular ACGs that shares the same abstract language correspond to a regular language homomorphism composed with a regular language inverse homomorphism."
"Now, after Nivat’s theorem[REF_CITE], any ra-tional transducer may be represented as such a bi-morphism."
The construction that allows to handle the tree adjoining grammars of Joshi[REF_CITE]may be seen as a generalization of the con-struction that we have described for the context-free grammars.
"Nevertheless, it is a little bit more involved."
"For instance, it is necessary to triplicate the non-terminal symbols in order to distinguish the initial trees from the auxiliary trees."
We do not have enough room in this paper for giving the details of the construction.
We will rather give an example.
Consider the TAG with the following initial tree and auxiliary tree: {{{{{{ S NA CCCCCC S a ||||||| S BBBBBBB d b S ∗ NA c
It generates the non context-free language a n b n c n d n .
"This TAG may be represented by the ACG, G = hΣ 1 , Σ 2 , L , Si, where:"
"Σ 1 = h {S, S 0 , S 00 }, {A, B, C}, {A 7→ ((S 00 −◦ S 0 ) −◦ S), B 7→ (S 00 −◦ ((S 00 −◦ S 0 ) −◦ S 0 )), C 7→ (S 00 −◦ S 0 )} i Σ 2 = h {∗}, {a, b, c, d}, {a 7→ string, b 7→ string, c 7→ string, d 7→ string} i"
"L = h {S 7→ string, S 0 7→ string,"
"S 00 7→ string}, {A 7→ λf. f (λx. x), B 7→ λx. λg. a + g (b + x + c) + d, C 7→ λx. x} i"
One of the keystones in the above translation is to represent an adjunction node A as a functional parameter of type A 00 −◦ A 0 .
The linear λ-calculus on which we have based our definition of an ACG may be seen as a rudi-mentary functional programming language.
"The results in Section 4 indicate that, in theory, this rudimentary language is powerful enough."
"Never-theless, in practice, it would be useful to increase the expressive power of the multiplicative kernel defined in Section 2 by providing features such as records, enumerated types, conditional expres-sions, etc."
"From a methodological point of view, there is a systematic way of considering such extensions."
It consists of enriching the type system of the formalism with new logical connectives.
"Indeed, each new logical connective may be interpreted, through the Curry-Howard isomorphism, as a new type constructor."
"Nonetheless, the possible addi-tional connectives must satisfy the following re-quirements: 1. they must be provided with introduction and elimination rules that satisfy Prawitz’s inver-sion principle[REF_CITE]and the result-ing system must be strongly normalizable; 2. the resulting term language (or at least an in-teresting fragment of it) must have a decid-able matching problem."
"The first requirement ensures that the new types come with appropriate data constructors and dis-criminators, and that the associated evaluation rule terminates."
This is mandatory for the applica-tive paradigm of Section 3.
The second require-ment ensures that the deductive paradigm (and consequently the transductive paradigm) may be fully automated.
The other connectives of linear logic are natural candidates for extending the formalism.
"In partic-ular, they all satisfy the first requirement."
"On the other hand, the satisfaction of the second require-ment is, in most of the cases, an open problem."
The additive connectives of linear logic ‘&amp;’ and ‘⊕’ corresponds respectively to the cartesian product and the disjoint union.
The cartesian product allows records to be defined.
"The dis-joint union, together with the unit type ‘1’, al-lows enumerated types and case analysis to be defined."
"Consequently, the additive connectives offer a good theoretical ground to provide ACG with feature structures."
The exponentials of linear logic are modal oper-ators that may be used to go beyond linearity.
"In particular, the exponential ‘!’ allows the intuition-istic implication ‘→’ to be defined, which cor-responds to the possibility of dealing with non-linear λ-terms."
A need for such non-linear λ-terms is already present in the example of Sec-tion 2.3.
"Indeed, the way of getting rid of the second assumption we made at the beginning of section 2.3 is to declare the logical symbols (i.e., the existential quantifier and the conjunction that occurs in the interpretation of A[REF_CITE]) as constants of the object vocabulary Σ 3 ."
"Then, the interpretation of A would be something like: λP. λQ. EXISTS (λx."
AND (P x) (Q x)).
"Now, this expression must be typable, which is not possible in a purely linear framework."
"Indeed, the λ-term to which EXISTS is applied is not linear (there are two occurrences of the bound variable x)."
"Consequently, EXISTS must be given ((e → t) −◦ t) as a type."
Quantifiers may also play a part.
"Uses of first-order quantification, in a type logical setting, are exemplified[REF_CITE],[REF_CITE], and[REF_CITE]."
"As for second-order quantifi-cation, it allows for polymorphism."
The difference we make between an abstract vo-cabulary and an object vocabulary is purely con-ceptual.
"In fact, it only makes sense relatively to a given lexicon."
"Indeed, from a technical point of view, any vocabulary is simply a higher-order linear signature."
"Consequently, one may think of a lexicon L 12 : Σ 1 → Σ 2 whose object lan-guage serves as abstract language of another lex-icon L 23 : Σ 2 → Σ 3 ."
This allows lexicons to be sequentially composed.
"Moreover, one may eas-ily construct a third lexicon L 13 : Σ 1 → Σ 3 that corresponds to the sequential composition of L 23 with L 12 ."
"From a practical point of view, this means that the sequential composition of two lex-icons may be compiled."
"From a theoretical point of view, it means that the ACGs form a category whose objects are vocabularies and whose arrows are lexicons."
This opens the door to a theory where operations for constructing new grammars from other grammars could be defined.
This paper presents the first steps towards the de-sign of a powerful grammatical framework based on a small set of computational primitives.
The fact that these primitives are well known from programming theory renders the framework suit-able for an implementation.
A first prototype is currently under development.
"We describe the use of XML tokenisa-tion, tagging and mark-up tools to pre-pare a corpus for parsing."
Our tech-niques are generally applicable but here we focus on parsing Medline abstracts with the ANLT wide-coverage grammar.
Hand-crafted grammars inevitably lack coverage but many coverage failures are due to inadequacies of their lexi-cons.
We describe a method of gain-ing a degree of robustness by interfac-ing POS tag information with the exist-ing lexicon.
"We also show that XML tools provide a sophisticated approach to pre-processing, helping to ameliorate the ‘messiness’ in real language data and improve parse performance."
The field of parsing technology currently has two distinct strands of research with few points of contact between them.
"On the one hand, there is thriving research on shallow parsing, chunk-ing and induction of statistical syntactic analysers from treebanks; and on the other hand, there are systems which use hand-crafted grammars which provide both syntactic and semantic coverage. ‘Shallow’ approaches have good coverage on cor-pus data, but extensions to semantic analysis are still in a relative infancy."
"The ‘deep’ strand of research has two main problems: inadequate cov-erage, and a lack of reliable techniques to select the correct parse."
In this paper we describe on-going research which uses hybrid technologies to address the problem of inadequate coverage of a ‘deep’ parsing system.
"In Section 2 we describe how we have modified an existing hand-crafted grammar’s look-up procedure to utilise part-of-speech ( POS ) tag information, thereby ameliorat-ing the lexical information shortfall."
In Section 3 we describe how we combine a variety of existing NLP tools to pre-process real data up to the point where a hand-crafted grammar can start to be use-ful.
The work described in both sections is en-abled by the use of an XML processing paradigm whereby the corpus is converted to XML with analysis results encoded as XML annotations.
In Section 4 we report on an experiment with a ran-dom sample of 200 sentences which gives an ap-proximate measure of the increase in performance we have gained.
"The work we describe here is part of a project which aims to combine statistical and symbolic processing techniques to compute lexical seman-tic relationships, e.g. the semantic relations be-tween nouns in complex nominals."
We have cho-sen the medical domain because the field of med-ical informatics provides a relative abundance of pre-existing knowledge bases and ontologies.
Our efforts so far have focused on the OHSUMED corpus[REF_CITE]which is a collection of Medline abstracts of medical journal papers. [Footnote_1]
1[REF_CITE]describe the Linguistic String Project’s approach to parsing medical texts.
"While the focus of the project is on seman-tic issues, a prerequisite is a large, reliably an-notated corpus and a level of syntactic process- ing that supports the computation of semantics."
"The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage ([REF_CITE]) and there are few other robust semantic pro-cessors, and none in the medical domain."
"We have therefore chosen to re-use an existing hand-crafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Lan-guage Tools ( ANLT ) system ([REF_CITE])."
"Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) proba-bilistic extension of the ANLT software."
"We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools[REF_CITE]."
"In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc."
The research re-ported here builds on that mark-up in a further stage of pre-processing prior to parsing.
The XML paradigm has proved invaluable throughout.
"The ANLT grammar is a unification grammar based on the GPSG formalism[REF_CITE], which is a precursor of more recent ‘lex-icalist’ grammar formalisms such as HPSG[REF_CITE]."
In these frameworks lexical entries carry a significant amount of information including subcategorisation information.
Thus the practical parse success of a grammar is sig-nificantly dependent on the quality of the lexicon.
The ANLT grammar is distributed with a large lexicon which was derived semi-automatically from a machine-readable dictionary[REF_CITE].
"This lexicon is of varying quality: function words such as complementizers, prepo-sitions, determiners and quantifiers are all reli-ably hand-coded but content words are less reli-able."
Verbs are generally coded to a high stan-dard but the noun and adjective lexicons are full of redundancies and duplications.
"Since these du-plications can lead to huge increases in the num-ber of spurious parses, an obvious first step was to remove all duplications from the existing lex-icons and to collapse certain ambiguities such as the count/mass distinction into single underspeci-fied entries."
"A second critical step was to increase the character set that the spelling rules in the mor-phological analyser handle, so as to accept capi-talised and non-alphabetic characters in the input."
"Once these ANLT -internal problems are over-come, the main problem of inadequate lexi-cal coverage still remains: if we try to parse OHSUMED sentences using the ANLT lexicon and no other resources, we achieve very poor results because most of the medical domain words are simply not in the lexicon and there is no ‘robust-ness’ strategy built into ANLT ."
One solution to this problem would be to find domain specific lex-ical resources from elsewhere and to merge the new resources with the existing lexicon.
"How-ever, the resulting merged lexicon may still not have sufficient coverage and a means of achieving robustness in the face of unknown words would still be required."
"Furthermore, every move to a new domain would depend on domain-specific lexical resources being available."
"Because of these disadvantages, we have pursued an alter-native solution which allows parsing to proceed without the need for extra lexical resources and with robustness built into the strategy."
This alter-native strategy does not preclude the use of do-main specific lexical resources but it does pro-vide a basic level of performance which further resources can be used to improve upon.
The strategy we have adopted relies first on sophisticated XML -based tokenisation (see Sec-tion 3) and second on the combination of POS tag information with the existing ANLT lexical re-sources.
"Our view is that POS tag information for content words (nouns, verbs, adjectives, adverbs) is usually reliable and informative, while tag-ging of function words (complementizers, deter-miners, particles, conjunctions, auxiliaries, pro-nouns, etc.) can be erratic and provides less in-formation than the hand-written entries for func-tion words that are typically developed side-by-side with wide coverage grammars."
"Furthermore, unknown words are far more likely to be con- tent words than function words, so knowledge of the POS tag will most often be needed for con-tent words."
"Our idea, then, is to tag the input but to retain only the content word POS tags and use them during lexical look-up in one of two ways."
If the word exists in the lexicon then the POS tag is used to access only those entries of the same basic category.
"If, on the other hand, the word is not in the lexicon then a basic underspecified en-try for the POS tag is used as the lexical entry for the word."
"In the first case, the POS tag is used as a filter, accessing only entries of the appropriate category and cutting down on the parser’s search space."
"In the second case, the basic category of the unknown word is supplied and this enables parsing to proceed."
"For example, if the following partially tagged sentence is input to the parser, it is successfully parsed. [Footnote_2]"
"2 The LT TTT tagger uses the Penn Treebank tagset[REF_CITE]: JJ labels adjectives, NN labels nouns and VB labels verbs."
"We have developed VBN a variable JJ suction NN system NN for irrigation NN , aspiration NN and vitrectomy NN"
Without the tags there would be no parse since the words irrigation and vitrectomy are not in the ANLT lexicon.
"Furthermore, tagging variable as an adjective ensures that the noun entry for vari-able is not accessed, thus cutting down on parse numbers (3 versus 6 in this case)."
The two cases interact where a lexical entry is present in the ANLT lexicon but not with the rele-vant category.
"For example, monitoring is present in the ANLT lexicon as a verb but not as a noun:"
We studied VBD the value NN of transcutaneous JJ carbon NN dioxide NN monitoring NN during transport NN
Look up of the word tag pair monitoring NN fails and the basic entry for the tag NN is used in-stead.
"Without the tag, the verb entry for monitor-ing would be accessed and the parse would fail."
"In the following example the adjectives dimin-ished and stabilized exist only as verb entries: with the JJ tag the parse succeeds but without it, the verb entries are accessed and the parse fails."
"Note that cases such as these would be problem-atic for a strategy where tagging was used only when lexical look-up failed, since here lexical look-up doesn’t fail, it just provides an incom-plete set of entries."
It is of course possible to aug-ment the grammar and/or lexicon with rules to in-fer noun entries from verb+ing entries and adjec-tive entries from verb+ed entries.
"However, this will increase lexical ambiguity quite considerably and lead to higher numbers of spurious parses."
We expect the technique outlined above to be ap-plicable across a range of parsing systems.
In this section we describe how we have implemented it within ANLT .
"The version of the ANLT system described[REF_CITE]and[REF_CITE]does not allow tagged input but work[REF_CITE]on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags."
"We use this version of the system, running in a mode where ‘words’ are looked up according to three distinct cases: word look-up: the word has no tag and must be looked up in the lexicon (and if look-up fails, the parse fails) tag look-up: the word has a tag, look-up of the word tag pair fails, but the tag has a spe-cial hand-written entry which is used instead word tag look-up: the word has a tag and look-up of the word tag pair succeeds."
The resources provided by the system already ad-equately deal with the first two cases but the third case had to be implemented.
The existing mor-phological analysis software was relatively easily adapted to give the performance we required.
The ANLT morphological analyser performs regular inflectional morphology using a unification gram-mar for combining morphemes and rules govern-ing spelling changes when morphemes are con-catenated.
Thus a plural noun such as patients is composed of the morphemes patient and +s with the features on the top node being inherited par-tially from the noun and partially from the inflec-tional affix:
"N , V , PLU  N , V , PLU  PLU , STEM PLU  patient +s"
"In dealing with word tag pairs, we have used the word grammar to treat the tag as a novel kind of affix which constrains the category of the lex-ical entry it attaches to."
We have defined mor-pheme entries for content word tags so they can be used by special word grammar rules and at-tached to words of the appropriate category.
Thus patient NN is analysed using the noun entry for patient but not the adjective entry.
"Tag mor-phemes can be attached to inflected as well as to base forms, so the string patients NNS has the following internal structure:"
"In defining the rules for word tag pairs, we were careful to ensure that the resulting category would have exactly the same feature specification as the word itself."
Thus the tag morpheme is spec-ified only for basic category features which the word grammar requires to be shared by word and tag.
"All other feature specifications on the cov-ering node are inherited from the word, not the tag."
"This method of combining POS tag infor-mation with lexical entries preserves all informa-tion in the lexical entries, including inflectional and subcategorisation information."
The preserva-tion of subcategorisation information is particu-larly necessary since the ANLT lexicon makes so-phisticated distinctions between different subcat-egorisation frames which are critical for obtaining the correct parse and associated logical form.
"The techniques described in this section, and those in the previous section, are made possi-ble by our use of an XML processing paradigm throughout."
"We use the LT TTT and LT XML tools in pipelines where they add, modify or remove pieces of XML mark-up."
Different combinations of the tools can be used for different processing tasks.
Some of the XML programs are rule-based while others use maximum entropy modelling.
We have developed a pipeline which converts OHSUMED data into XML format and adds lin-guistic annotations.
The early stages of the pipeline segment character strings first into words and then into sentences while subsequent stages perform POS tagging and lemmatisation.
A sam-ple part of the output of this basic pipeline is shown in Figure 1.
"The initial conversion to XML and the identification of words is achieved us-ing the core LT TTT program fsgmatch, a gen-eral purpose transducer which processes an in-put stream and rewrites it using rules provided in a grammar file."
"The identification of sentence boundaries, mark-up of sentence elements and POS tagging is done by the statistical program lt-pos[REF_CITE]."
Words are marked up as W elements with further information encoded as values of attributes on the W elements.
"In the ex-ample, the P attribute’s value is a POS tag and the LM attribute’s is a lemma (only on nouns and verbs)."
The lemmatisation is performed by Min-nen et al.’s (2000) morpha program which is not an XML processor.
In such cases we pass data out of the pipeline in the format required by the tool and merge its output back into the XML mark-up.
"Typically we use McKelvie’s (1999) xmlperl pro-gram to convert out of and back into XML : for ANLT this involves putting each sentence on one line, converting some W elements into word tag pairs and stripping out all other XML mark-up to provide input to the parser in the form it requires."
We are currently experimenting with bringing the labelled bracketing of the parse result back into the XML as ‘stand-off’ mark up.
In Section 2 we showed how POS tag mark-up could be used to add to existing lexical re-sources.
In this section we demonstrate how the
XML approach allows for flexibility in the way data is converted from marked-up corpus mate-rial to parser input.
"This method enables ‘messy’ linguistic data to be rendered innocuous prior to parsing, thereby avoiding the need to make hand-written low-level additions to the grammar itself."
One of the failings of the ANLT lexicon is in the subcategorisation of nouns: each noun has a zero subcategorisation entry but many nouns which optionally subcategorise a complement lack the appropriate entry.
"For example, the nouns use and management do not have entries with an of- PP subcategorisation frame so that in contexts where an of- PP is present, the correct parse will not be found."
The case of of- PP s is a special one since we can assume that whenever of follows a noun it marks that noun’s complement.
We can encode this assumption in the layer of processing that converts the XML mark-up to the format required by the parser: an fsgmatch rule changes the value of the P attribute of a noun from NN to NNOF or from NNS to NNSOF whenever it is followed by of.
By not adding morpheme entries for NNOF and NNSOF we ensure that word tag look-up will fail and the system will fall back on tag look-up using special entries for NNOF and NNSOF which have only an of- PP subcategorisation frame.
In this way the parser will be forced to attach of-
PP s following nouns as their complements.
"Although we have stated that we only retain content word tags, in practice we also retain cer-tain other tags for which we provide no mor-pheme entry in the morphological system so as to achieve tag rather than word tag look-up."
"For example, we retain the CD tag assigned to numer-als and provide a general purpose entry for it so that sentences containing numerals can be parsed without needing lexical entries for them."
We also use a pre-existing tokenisation component which recognises spelled out numbers to which the CD tag is also assigned:
W P=’CD’ thirty-five /W thirty-five CD W P=’CD’
Twenty one CD W P=’CD’ 176 /W 176 CD
The program fsgmatch can be used to group words together into larger units using handwritten rules and small lexicons of ‘multi-word’ words.
"For the purposes of parsing, these larger units can be treated as words, so the grammar does not need to contain special rules for ‘multi-word’ words:"
WP=’IN’ Inorderto /W
In order to IN WP=’IN’ inrelation to /W in relation to IN W P=’JJ’ invitro /W in vitro JJ
The same technique can be used to pack-age up a wide variety of formulaic expressions which would cause severe problems to most hand-crafted grammars.
Thus all of the following ‘words’ have been identified using fsgmatch rules and can be passed to the parser as unanalysable chunks. [Footnote_3] The classification of the examples be-low as nouns reflects a working hypothesis that they can slot into the correct parse as noun phrases but there is room for experimentation since the conversion to parser input format can rewrite the tag in any way.
3[REF_CITE]discuss tokenisation issues in bio-logical texts.
It may turn out that they should be given a more general tag which corresponds to several major category types.
It is important to note that our method of divid-ing the labour between pre-processing and pars-ing allows for experimentation to get the best pos-sible balance.
We are still developing our for-mula recognition subcomponent which has so far been entirely hand-coded using fsgmatch rules.
We believe that it is more appropriate to do this hand-coding at the pre-processing stage rather than with the relatively unwieldy formalism of the ANLT grammar.
"Moreover, use of the XML paradigm might allow us to build a component that can induce rules for regular formulaic expres-sions thus reducing the need for hand-coding."
"The tagger we use, ltpos, has a reported per-formance comparable to other state-of-the-art tag-gers."
"However, all taggers make errors, especially when used on data different from their training data."
"With the strategy outlined in this paper, where we only retain a subset of tags, many tag-ging errors will be harmless."
"However, con-tent word tagging errors will be detrimental since the basic noun/verb/adjective/adverb distinction drives lexical look-up and only entries of the same category as the tag will be accessed."
"If we find that the tagger consistently makes the same er-ror in a particular context, for example mistag-ging +ing nominalisations as verbs (VBG), then we can use fsgmatch rules to replace the tag in just those contexts."
"The new tag can be given a defi-nition which is ambiguous between NN and VBG, thereby ensuring that a parse can be achieved."
A second strategy that we are exploring in-volves using more than one tagger.
Our cur-rent pipeline includes a call to Elworthy’s (1994)
CLAWS 2 tagger.
We encode the tags from this tagger as values of the attribute C2 on words:
W P=’NNS’ C2=’NN2’ LM=’case’ cases /W
W P=’VBN’ C2=’VVN’ LM=’find’ found /W
Many mistaggings can be found by searching for words where the two taggers disagree and they can be corrected in the mapping from XML for-mat to parser input by assigning a new tag which is ambiguous between the two possibilities.
"For example, ltpos incorrectly tags the word bound in the following example as a noun but the CLAWS 2 tagger correctly categorises it as a verb. a large JJ body NNOF of hemoglobin NN bound NNVVN to the ghost NN membrane NN"
"We use xmlperl rules to map from XML to ANLT input and reassign these cases to the ‘compos-ite’ tag NNVVN, which is given both a noun and a verb entry."
This allows the correct parse to be found whichever tagger is correct.
An alternative approach to the mistagging problem would be to use just one tagger which returns multiple tags and to use the relative probabil-ity of the tags to determine cases where a com-posite tag could be created in the mapping to parser input.
"Charniak et al. (forthcoming) reject a multiple tag approach when using a probabilis-tic context-free-grammar parser, but it is unclear whether their result is relevant to a hand-crafted grammar."
There are numerous advantages to working with XML tools.
"One general advantage is that we can add linguistic annotations in an entirely automatic and incremental fashion, so as to produce a heav-ily annotated corpus which may well prove useful to a number of researchers for a number of lin-guistic activities."
In the work described here we have not used any domain specific information.
"However, it would clearly be possible to add do-main specific information as further annotations using such resources as UMLS[REF_CITE]."
"In-deed, we have begun to utilise UMLS and hope to improve the accuracy of the existing mark-up by incorporating lexical and semantic information."
"Since the annotations we describe are computed entirely automatically, it would be a simple mat-ter to use our system to mark up new Medline data to increase the size of our corpus considerably."
A heavily annoted corpus quickly becomes un-readable but if it is an XML annotated corpus then there are several tools to help visualise the data.
"For example, we use xmlperl to convert from XML to HTML to view the corpus in a browser."
"With a corpus such as OHSUMED where there is no gold-standard tagged or hand-parsed sub-part, it is hard to reliably evaluate our system."
"However, we did an experiment on 200 sentences taken at random from the corpus (average sen-tence length: 21 words)."
We ran three versions of our pre-processor over the 200 sentences to pro-duce three different input files for the parser and for each input we counted the sentences which were assigned at least one parse.
"All three ver-sions started from the same basic XML annotated data, where words were tagged by both taggers and parenthesised material was removed."
Ver-sion 1 converted from this format to ANLT input simply by discarding the mark-up and separating off punctuation.
Version 2 was the same except that content word POS tags were retained.
"Ver-sion 3 was put through our full pipeline which recognises formulae, numbers etc. and which cor-rects some tagging errors."
The following table shows numbers of sentences successfully parsed with each of the three different inputs:
Version 1 Version 2 Version 3 Parses 4 (2%) 32 (16%) 79 (39.5%)
The extremely low success rate of Version 1 is a reflection of the fact that the ANLT lexicon does not contain any specialist lexical items.
"In fact, of the 200 sentences, 188 contained words that were not in the lexicon, and of the 12 that remained, 4 were successfully parsed."
The figure for Version 2 gives a crude measure of the contribution of our use of tags in lexical look-up and the figure for Version 3 shows further gains when further pre- processing techniques are used.
"Although we have achieved an encouraging overall improvement in performance, the total of 39.5% for Version 3 is not a precise reflection of accuracy of the parser."
"In order to determine ac-curacy, we hand-examined the parser output for the 79 sentences that were parsed and recorded whether or not the correct parse was among the parses found."
"While this figure is rather low for a practi-cal application, it is worth reiterating that this still means that nearly one in three sentences are not only correctly parsed but they are also assigned a logical form."
We are confident that the further work outlined below will achieve an improvement in performance which will lead to a useful seman-tic analysis of a significant proportion of the cor-pus.
"Furthermore, in the case of the 18 sentences which were parsed incorrectly, it is important to note that the ‘wrong’ parses may sometimes be capable of yielding useful semantic information."
"For example, the grammar’s compounding rules do not yet include the possibility of coordinations within compounds so that the NP the MS and di-rect blood pressure methods can only be wrongly parsed as a coordination of two NPs."
"However, the rest of the sentence in which the NP occurs is correctly parsed."
An analysis of the 18 sentences which were parsed incorrectly reveals that the reasons for fail-ure are distributed evenly across three causes: a word was mistagged and not corrected during pre-processing (6); the segmentation into tokens was inadequate (5); and the grammar lacked coverage (7).
A casual inspection of a random sample of 10 of the sentences which failed to parse at all re-veals a similar pattern although for several there were multiple reasons for failure.
"Lack of gram-matical coverage was more in evidence, perhaps not surprisingly since work on tuning the gram-mar to the domain has not yet been done."
"Although we are only able to parse between 30 and 40 percent of the corpus, we will be able to improve on that figure quite considerably in the future through continued development of the pre-processing component."
"Moreover, we have not yet incorporated any domain specific lexical knowledge from, e.g., UMLS but we would expect this to contribute to improved performance."
"Fur-thermore, our current level of success has been achieved without significant changes to the origi-nal grammar and, once we start to tailor the gram-mar to the domain, we will gain further significant increases in performance."
"As a final stage, we may find it useful to follow[REF_CITE]and have a ‘fallback’ strategy for failed parses where the best partial analyses are assembled in a robust processing phase."
"A hybrid system is described which combines the strength of manual rule-writing and statistical learning, obtain-ing results superior to both methods if applied separately."
"The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambigua-tion with recall close to 100% is applied first, and a trigram HMM tagger runs on its results."
An experiment in Czech tag-ging has been performed with encour-aging results.
"Inflective languages pose a specific problem in tagging due to two phenomena: highly inflec-tive nature (causing sparse data problem in any statistically-based system), and free word order (causing fixed-context systems, such as n-gram Hidden Markov Models (HMMs), to be even less adequate than for English)."
"The average tagset contains about [Footnote_1],000 - 2,000 distinct tags; the size of the set of possible and plausible tags can reach several thousands."
"1 Mainly because of the ease with which it is trained even on large data, and also because no other publicly available tagger was able to cope with the amount and ambiguity of the data in reasonable time."
"Apart from agglutinative languages such as Turkish, Finnish and Hungarian (see e.g.[REF_CITE]), and Basque[REF_CITE], which pose quite different and in the end less severe problems, there have been at-tempts at solving this problem for some of the highly inflectional European languages, such[REF_CITE],[REF_CITE](Slovenian),[REF_CITE],[REF_CITE](Czech) and[REF_CITE](five Central and Eastern European languages), but so far no system has reached - in the absolute terms - a performance comparable to English tag-ging (such[REF_CITE]), which stands around or above 97%."
"For example,[REF_CITE]report results on Czech slightly above 93% only."
"One has to realize that even though such a performance might be adequate for some tasks (such as word sense disambiguation), for many other (such as parsing or translation) the implied sentence error rate at 50% or more is sim-ply too much to deal with."
"Statistical tagging of inflective languages has been based on many techniques, rang-ing from plain-old HMM taggers[REF_CITE], memory-based[REF_CITE]to maximum-entropy and feature-based[REF_CITE],[REF_CITE]."
"For Czech, the best result achieved so far on approximately 300 thousand word training data set has been described[REF_CITE]."
We are using 1.8M manually annotated tokens from the Prague Dependency Treebank (PDT) project[REF_CITE].
"We have decided to work with an HMM tagger 1 in the usual source-channel setting, with proper smoothing."
The HMM tag-ger uses the Czech morphological processor from PDT to disambiguate only among those tags which are morphologically plausible for a given input word form.
The idea of tagging by means of hand-written disambiguation rules has been put forward and implemented for the first time in the form of Constraint-Based Grammars[REF_CITE].
"From languages we are acquainted with, the method has been applied on a larger scale only to English[REF_CITE],[REF_CITE], and French[REF_CITE]."
"Also[REF_CITE]and[REF_CITE]use manually written rules for Brazilian Portuguese, and there are several publications by Oflazer for Turkish."
"Authors of such systems claim that hand-written systems can perform better than sys-tems based on machine learning[REF_CITE]; however, except for the work cited, comparison is difficult to impossible due to the fact that they do not use the standard evalua-tion techniques (and not even the same data)."
But the substantial disadvantage is that the develop-ment of manual rule-based systems is demanding and requires a good deal of very subtle linguistic expertise and skills if full disambiguation also of “difficult” texts is to be performed.
Combination of (manual) rule-writing and statis-tical learning has been studied before.
"E.g.,[REF_CITE]and[REF_CITE]provide a thorough description of many experiments in-volving rule-based systems and statistical learn-ers for NP bracketing."
"For tagging, combination of purely statistical classifiers has been described[REF_CITE], with about 3% relative improve-ment (error reduction from 18.6% to 18%, trained on small data) over the best original system."
"We regard such systems as working in parallel, since all the original classifiers run independently of each other."
"In the present study, we have chosen a differ-ent strategy (similar to the one described for other types of languages[REF_CITE],[REF_CITE]and[REF_CITE])."
"At the same time, the rule-based component is known to perform well in eliminating the incorrect alternatives [Footnote_2] , rather than picking the correct one under all circumstances."
2 Such a “negative” learning is thought to be difficult for any statistical system.
"Moreover, the rule-based system used can exam-ine the whole sentential context, again a difficult thing for a statistical system [Footnote_3] ."
3 Causing an immediate data sparseness problem.
"That way, the ambi-guity of the input text [Footnote_4] decreases."
4 As prepared by the morphological analyzer.
"This is exactly what our statistical HMM tagger needs as its in-put, since it is already capable of using the lexical information from a dictionary."
"However, also in the rule-based approach, there is the usual tradeoff between precision and recall."
"We have decided to go for the “perfect” solution: to keep 100% recall, or very close to it, and grad-ually improve precision by writing rules which eliminate more and more incorrect tags."
"This way, we can be sure (or almost sure) that the perfor-mance of the HMM tagger performance will not be hurt by (recall) errors made by the rule compo-nent."
"Taken strictly formally, the rule-based component has the form of a restarting automaton with dele-ti[REF_CITE], that is, each rule can be thought of as a finite-state automaton starting from the beginning of the sentence and passing to the right until it finds an input configuration on which it can operate by deletion of some parts of the input."
"Having performed this, the whole sys-tem is restarted, which means that the next rule is applied on the changed input (and this input is again read from the left end)."
"This means that a single rule has the power of a finite state automa-ton, but the system as a whole has (even more than) a context-free power."
"The system of hand-written rules for Czech has a twofold objective: practical: an error-free and at the same time the most accurate tagging of Czech texts theoretical: the description of the syntactic system of Czech, its langue, rather than pa-role."
The rules are to reduce the input ambiguity of the input text.
"During disambiguation the whole rule system combines two methods: the oblique one consisting in the elimination of syntactically wrong tag(s), i.e. in the re-duction of the input ambiguity by deleting those tags which are excluded by the context the direct choice of the correct tag(s)."
The overall strategy of the rule system is to keep the highest recall possible (i.e. 100%) and gradually improve precision.
"Thus, the rules are (manually) assigned reliabilities which divide the rules into reliability classes, with the most reli-able (“bullet-proof”) group of rules applied first and less reliable groups of rules (threatening to decrease the 100% recall) being applied in subse-quent steps."
"The bullet-proof rules reflect general syntactic regularities of Czech; for instance, no word form in the nominative case can follow an unambiguous preposition."
The less reliable rules can be exemplified by those accounting for some special intricate relations of grammatical agree-ment in Czech.
"Within each reliability group the rules are applied independently, i.e. in any or-der in a cyclic way until no ambiguity can be re-solved."
"Besides reliability, the rules can be generally divided according to the locality/nonlocality of their scope."
"Some phenomena (not many) in the structure of Czech sentence are local in nature: for instance, for the word “se” which is two-way ambiguous between a preposition (with) and a re-flexive particle/pronoun (himself, as a particle) a prepositional reading can be available only in lo-cal contexts requiring the vocalisation of the basic form of the preposition “s” (with) resulting in the form “se”."
"However, in the majority of phenom-ena the correct disambiguation requires a much wider context."
"Thus, the rules use as wide con-text as possible with no context limitations be-ing imposed in advance."
"During rules develop-ment performed so far, sentential context has been used, but nothing in principle limits the context to a single sentence."
"If it is generally appropri-ate for the disambiguation of the languages of the world to use unlimited context, it is especially fit for languages with free word order combined with rich inflection."
There are many syntactic phenom-ena in Czech displaying the following property: a word form wf1 can be part-of-speech determined by means of another word form wf2 whose word-order distance cannot be determined by a fixed number of positions between the two word forms.
This is exactly a general phenomenon which is grasped by the hand-written rules.
"Formally, each rule consists of the description of the context (descriptive component), and the action to be performed given the context (executive component): i.e. which tags are to be discarded or which tag(s) are to be pro-claimed correct (the rest being discarded as wrong)."
"Context: unambiguous finite verb, fol-lowed/preceded by a sequence of tokens containing neither comma nor coordinating conjunction, at either side of a word x am-biguous between a finite verb and another reading"
Action: delete the finite verb reading(s) at the word x.
"There are two ways of rule development: the rules developed by syntactic introspec-tion: such rules are subsequently verified on the corpus material, then implemented and the implemented rules are tested on a testing corpus the rules are derived from the corpus by in-trospection and subsequently implemented"
The rules are formulated as generally as pos-sible and at the same time as error-free (recall-wise) as possible.
This approach of combining the requirements of maximum recall and maximum precision demands sophisticated syntactic knowl-edge of Czech.
This knowledge is primarily based on the study of types of morphological ambiguity occurring in Czech.
There are two main types of such ambiguity: regular (paradigm-internal) casual (lexical)
"The regular (paradigm-internal) ambiguities occur within a paradigm, i.e. they are common to all lexemes belonging to a particular inflection class."
"For example, in Czech (as in many other in-flective languages), the nominative, the accusative and the vocative case have the same form (in sin-gular on the one hand, and in plural on the other)."
"The casual (lexical, paradigm-external) morpho-logical ambiguity is lexically specific and hence cannot be investigated via paradigmatics."
"In addition to the general rules, the rule ap-proach includes a module which accounts for col-locations and idioms."
The problem is that the majority of collocations can – besides their most probable interpretation just as collocations – have also their literal meaning.
"Currently, the system (as evaluated in Sect. 2.3) consists of 80 rules."
"The rules had been implemented procedurally in the initial phase; a special feature-oriented, in-terpreted “programming language” is now under development."
The results are presented in Table 1.
"We use the usual  equal-weight  formula ),#.! &quot;%!#$ %&amp;$ /0&amp; for ())( ## F-measure #* *+&quot;2&quot;++ + 1 : 3 where ,75#8&apos;9:&lt;; ?= &gt; A$ @; ?= &gt; B&amp; $P! @ &amp;B&quot;FQN!.&amp; #$,*,##R6OFLFM; * NBO ; and S 4  ; =?&gt; A$ @; ?= B&amp; &gt; !"
"P$ @ .&amp;B&quot;F !.&amp;#$ * ,#6O* FQFW; ANXO* ;"
"We have used an HMM tagger in the usual source-channel setting, fine-tuned to perfection using"
"Z \ a [ ]A^ _ ] ^ 3-gram 1 ] ^ , tag language model a tag-to-word lexical (translation) model us-ing bigram histories Z\ [ instead ^ _]"
"A^ 1 ]A^ of just same-word conditioning [Footnote_5] , a bucketed linear interpolation smoothing for both models."
"5 First used[REF_CITE], as far as we know."
Thus the HMM tagger outputs a sequence of tags f according to the usual equation f  jEk:gml)nVo qpr[ _ f c o [ f c 1 o [ f ctsvu ^xwQy7z{z| ZL-} ~IPAx[ ] ^ _
A] ^ 1 A] ^ 1 where and o qpr[ _ f cesu MwQy7z^ {z | ZL}q~P x) [ ^ _ ]
A^ 1 ]A^
"The tagger has been trained in the usual way, using part of the training data as heldout data for smoothing of the two models employed."
There is no threshold being applied for low counts.
"Smoothing has been done first without using buckets, and then with them to show the differ-ence."
Table 2 shows the resulting interpolation coefficients for the tag language model using the usual linear interpolation smoothing formula
ZL}- IPAx~  )Z\ [[ ]A] ^^ _
A] ^^1 ]A^ aKZ [ ] ^qc0yAZ\[ ] 6^ _
A] ^ _{1 _
"A] ^ where p(...) is the “raw” Maximum Likelihood estimate of the probability distributions, i.e. the relative frequency in the training data."
"The bucketing scheme for smoothing (a neces-sity when keeping all tag trigrams and tag-to-word bigrams) uses “buckets bounds” computed according to the following formula (for more on bucketing, see[REF_CITE]): Q[Kc v K[ Lc  _&quot;6d  [K 1 _."
"It should be noted that when using this bucket-ing scheme, the weights of the detailed distribu-tions (with longest history) grow quickly as the history reliability increases."
"However, it is not monotonic; at several of the most reliable histo-"
"We have found that a sudden drop inries, the weight coefficients “jump” up y andhappensdown,. e.g., for the bucket containing a history consisting of two consecutive punctuation symbols, which is not so much surprising after all."
"A similar formula has been used for the lex-ical model (Table 3), and the strenghtening of the weights of the most detailed distributions has been observed, too."
The HMM tagger described in the previous para-graph has achieved results shown in Table 4.
"It produces only the best tag sequence for every sen-tence, therefore only accuracy is reported."
"Five-fold cross-validation has been performed (Exp 1- 5) on a total data size of 1489983 tokens (exclud-ing heldout data), divided up to five datasets of roughly the same size."
"When the two systems are coupled together, the manual rules are run first, and then the HMM tag-ger runs as usual, except it selects from only those tags retained at individual tokens by the manual rule component, instead of from all tags as pro-duced by the morphological analyzer:"
The morphological analyzer is run on the test data set.
Every input token receives a list of possible tags based on an extensive Czech morphological dictionary.
The manual rule component is run on the output of the morphology.
The rules elimi-nate some tags which cannot form grammat-ical sentences in Czech.
"The HMM tagger is run on the output of the rule component, using only the remain-ing tags at every input token."
"The output is best-only; i.e., the tagger outputs exactly one tag per input token."
"If there is no tag left at a given input token after the manual rules run, we reinsert all the tags from morphology and let the statistical tagger decide as if no rules had been used."
Table 5 contains the final evaluation of the main contribution of this paper.
"Since the rule-based component does not attempt at full disambigua-tion, we can only use the F-measure for compari-son and improvement evaluation [Footnote_6] ."
"6 For the HMM tagger, which works in best-only mode, accuracy = precision = recall = F-measure, of course."
"The not-so-perfect recall of the rule component has been caused either by some deficiency in the rules, or by an error in the input morphology (due to a deficiency in the morphological dictionary), or by an error in the ’truth’ (caused by an imper-fect manual annotation)."
"As Czech syntax is extremely complex, some of the rules are either not yet absolutely perfect, or they are too strict [Footnote_7] ."
"7 “Too strict” is in fact good, given the overall scheme with the statistical tagger coming next, except in cases when it severely limits the possibility of increasing the precision. Nothing unexpected is happening here."
An example of the rule which decreases 100% recall for the test data is the following one:
"In Czech, if an unambiguous preposition is de-tected in a clause, it “must” be followed - not necessarily immediately - by a nominal element (noun, adjective, pronoun or numeral) or, in very special cases, such a nominal element may be missing as it is elided."
This fact about the syn-tax of prepositions in Czech is accounted for by a rule associating an unambiguous preposition with such a nominal element which is headed by the preposition.
"The rule, however, erroneously ignores the fact that some prepositions function as heads of plain adverbs only (e.g., adverbs of time)."
"As an example occurring in the test data we can take a simple structure “do kdy” (lit. till when), where “do” is a preposition (lit. till), when is an adverb of time and no nominal element fol-lows."
This results in the deletion of the preposi-tional interpretation of the preposition “do” thus causing an error.
"However, in cases like this, it is more appropriate to add another condition to the context (gaining back the lost recall) of such a rule rather than discard the rule as a whole (which would harm the precision too much)."
"As examples of erroneous tagging results which have been eliminated for good due to the architecture described we might put forward: preposition requiring case  not followed by any form in case  : any preposition has to be followed by at least one form (of noun, ad-jective, pronoun or numeral) in the case re-quired."
"Turning this around, if a word which is ambiguous between a preposition and an-other part of speech is not followed by the respective form till the end of the sentence, it is safe to discard the prepositional reading in almost all non-idiomatic, non-coordinated cases. two finite verbs within a clause: Similarly to most languages, a Czech clause must not contain more than one finite verb."
"This means that if two words, one genuine finite verb and the other one ambiguous between a finite verb and another reading, stand in such a configuration that the material between them contains no clause separator (comma, conjunction), it is safe to discard the finite verb reading with the ambiguous word. two nominative cases within a clause: The subject in Czech is usually case-marked by nominative, and simultaneously, even when the position of subject is free (it can stand both to the left or to the right of the main verb) in Czech, no clause can have two non-coordinated subjects."
"The improvements obtained (4.58% relative er-ror reduction) beat the pure statistical classifier combinati[REF_CITE], which obtained only 3% relative improvement."
"The most important task for the manual-rule component is to keep re-call very close to 100%, with the task of improv-ing precision as much as possible."
"Even though the rule-based component is still under develop-ment, the 19% relative improvement in F-measure over the baseline (i.e., 16% reduction in the F-complement while keeping recall just 0.34% un-der the absolute one) is encouraging."
"In any case, we consider the clear “division of labor” between the two parts of the system a strong advantage."
It allows now and in the future to use different taggers and different rule-based systems within the same framework but in a com-pletely independent fashion.
"The performance of the pure HMM tagger alone is an interesting result by itself, beating the best Czech tagger published[REF_CITE]by almost 2% (30% relative improvement) and a previous HMM tagger on Czech[REF_CITE]by almost 4% (44% relative improvement)."
"We believe that the key to this success is both the increased data size (we have used three times more training data then reported in the previ-ous papers) and the meticulous implementation of smoothing with bucketing together with using all possible tag trigrams, which has never been done before."
"One might question whether it is worthwhile to work on a manual rule component if the im-provement over the pure statistical system is not so huge, and there is the obvious disadvantage in its language-specificity."
"However, we see at least two situations in which this is the case: first, the need for high quality tagging for local language projects, such as human-oriented lexicography, where every 1/10th of a percent of reduction in error rate counts, and second, a situation where not enough training data is available for a high-quality statistical tagger for a given language, but a language expertise does exist ; the improvement over an imperfect statistical tagger should then be more visible [Footnote_8] ."
"8 However, a feature-based log-linear tagger might per-form better for small training data, as argued[REF_CITE]."
Another interesting issue is the evaluation method used for taggers.
"From the linguistic point of view, not all errors are created equal; it is clear that the manual rule component does not commit linguistically trivial errors (see Sect. 4.2)."
"However, the relative weighting (if any) of errors should be application-based, which is already out-side of the scope of this paper."
"It has been also observed that the improved tag-ger can serve as an additional means for discov-ering annotator’s errors (however infrequent they are, they are there)."
See Fig. 1 for an example of wrong annotation of “se”.
"In the near future, we plan to add more rules, as well as continue to work on the statistical tagging."
"The lexical component of the tagger might still have some room for improvement, such as the use of o [ _ f ces u xwQy7z^ {z | ZL-} IPAx~ ) [ ^ _ ] ^ 1 d ^ 1 which can be feasible with the powerful smoothing we now employ."
"The work described herein has been supported by the following grants:[REF_CITE](“Cen-trum komputačnı́ lingvistiky”),[REF_CITE](Kontakt), and[REF_CITE]/96/[REF_CITE]."
"Most of the current work on corpus annotation is concentrated on morphemics, lexical semantics and sentence structure."
"However, it becomes more and more obvious that attention should and can be also paid to phenomena that reflect the links between a sentence and its context, i.e. the discourse anchoring of utterances."
"If conceived in this way, an annotated corpus can be used as a resource for linguistic research not only within the limits of the sentence, but also with regard to discourse patterns."
"Thus, the applications of the research to issues of information retrieval and extraction may be made more effective; also applications in new domains become feasible, be it to serve for inner linguistic (and literary) aims, such as text segmentation, specification of topics of parts of a discourse, or for other disciplines."
"These considerations have been a motivation for the tectogrammatical (i.e. underlying, see below) tagging done within the Prague Dependency Treebank (PDT) to contain also attributes concerning certain contextual features, i.e. the contextual anchoring of word tokens and their relationships to their coreferential antecedents."
"Along with this enrichment in the intersentential aspect, we do not neglect to pay attention to intrasentential issues, i.e. to sentence structure, which displays its own features oriented towards the contextual potential of the sentence, namely its topic-focus articulation (TFA)."
"In the present paper, we give first an outline of the annotation scenario of the PDT (Section 2), concentrating then on the use of one of the PDT attributes for the specification of the Topic and the Focus (the &apos;information structure&apos;) of the sentence (Section 3)."
In Section 4. we present certain heuristics that partly are based on TFA and that allow for the specification of the degrees of salience in a discourse.
The application of these heuristics is illustrated in Section 5.
"The Prague Dependency Treebank (PDT) is being built on the basis of the Czech National Corpus (CNC), which grows rapidly in the range of hundreds of millions of word occurrences in journalistic and fiction texts."
"The PDT scenario comprises three layers of annotation: (i) the morphemic (POS) layer with about 2000 tags for the highly inflectional Czech language; the whole CNC has been tagged by a stochastic tagger ([REF_CITE];1998,[REF_CITE]) with a success rate of 95%; the tagger is based on a fully automatic morphemic analysis of Czech (Hajič in press); (ii) a layer of &apos;analytic&apos; (&quot;surface&quot;) syntax (see[REF_CITE]): cca 100 000 Czech sentences, i.e. samples of texts (each randomly chosen sample consisting of 50 sentences of a coherent text), taken from CNC, have been assigned dependency tree structures; every word (as well as every punctuation mark) has a node of its own, the label of which specifies its analytic function, i.e. Subj, Pred, Obj, Adv, different kinds of function words, etc. (total of 40 values); no nodes are added that are not in the surface shape of the sentence (except for the root of the tree, carrying the identification number of the sentence); the sentences from CNC are preprocessed by a dependency-based modification of Collins et al.&apos;s (1999) automatic parser (with a success rate of about 80%), followed by a manual tagging procedure that is supported by a special user-friendly software tool that enables the annotators to work with (i.e. modify) the automatically derived graphic representations of the trees; (iii) the tectogrammatical (underlying) syntactic layer: tectogrammatical tree structures (TGTSs) are being assigned to a subset of the set tagged according to (ii); by now, the experimental phase has resulted in 20 samples of 50 sentences each; the TGTSs, based on dependency syntax, are much simpler than structural trees based on constituency (minimalist or other), displaying a much lower number of nodes and a more perspicuous patterning; their basic characteristics are as follows (a more detailed characterization of tectogrammatics and motivating discussion, which cannot be reproduced here, can be found[REF_CITE]): (a) only autosemantic (lexical) words have nodes of their own; function words, as far as semantically relevant, are reflected by parts of complex node labels (with the exception of coordinating conjunctions); (b) nodes are added in case of deletions on the surface level; (c) the condition of projectivity is met (i.e. no crossing of edges is allowed); (d) tectogrammatical functions (&apos;functors&apos;) such as Actor/Bearer, Patient, Addressee, Origin, Effect, different kinds of Circumstantials are assigned; (e) basic features of TFA are introduced; (f) elementary coreference links (both grammatical and textual) are indicated."
"Thus, a TGTS node label consists of the lexical value of the word, of its &apos;(morphological) grammatemes&apos; (i.e. the values of morphological categories), its &apos;functors&apos; (with a more subtle differentiation of syntactic relations by means of &apos;syntactic grammatemes&apos; (e.g. &apos;in&apos;, &apos;at&apos;, &apos;on&apos;, &apos;under&apos;), of the attribute of Contextual Boundness (see below), and of values concerning intersentential links (see below)."
"The dependency based TGTSs in PDT allow for a highly perspicuous notation of sentence structure, including an economical representation of TFA, understood as one of the main aspects of (underlying) sentence structure along with all other kinds of semantically relevant information expressed by grammatical means."
"TFA is accounted for by one of the following three values of a specific TFA attribute assigned to every lexical (autosemantic) occurrence: t for &apos;contextually bound&apos; (prototypically in Topic), c for &apos;contrastive (part of) Topic&apos;, or f (‘non-bound’, typically in Focus)."
"The opposition of contextual boundness is understood as the linguistically structured counterpart of the distinction between &quot;given&quot; and &quot;new&quot; information, rather than in a straightforward etymological way (see Sgall,[REF_CITE]Ch. 3)."
"Our approach to TFA, which uses such operational criteria of empirical adequateness as the question test (with the item corresponding to a question word prototypically constituting the focus of the answer), represents an elaboration of older ideas, discussed especially in Czech linguistics since V. Mathesius and J. Firbas, in the sense of an explicit treatment meeting the methodological requirements of formal syntax."
"The following rules determine the appurtenance of a lexical occurrence to the Topic (T) or to the Focus (F) of the sentence: (a) the main verb (V) and any of its direct dependents belong to F iff they carry index f; (b) every item i that does not depend directly on V and is subordinated to an element of F different from V, belongs to F (where &quot;subordinated to&quot; is defined as the irreflexive transitive closure of &quot;depend on&quot;); (c) iff V and all items k j directly depending on it carry index t, then those items k j to which some items l m carrying f are subordinated are called &apos;proxy foci&apos; and the items l m together with all items subordinated to one of them belong to F, where 1 ≤ j,m; (d) every item not belonging to F according to (a) - (c) belongs to T."
"To illustrate how this approach makes it possible to analyze also complex sentences as for their TFA patterns, with neither T nor F corresponding to a single constitutent, let us present the following example, in which (1&apos;) is a highly simplified linearized TGTS of (1); every dependent item is enclosed in a pair of parentheses; for the sake of transparency, syntactic subscripts of the parentheses are left out here, as well as subscripts indicating morphological values, with the exception of the two which correspond to function words, i.e. Temp and Necess(ity); Fig. 1. presents the respective tree structure, in which three parts of each node label are specified, namely the lexical value, the syntactic function (with ACT for Actor/Bearer, RSTR for Restrictive, MANN for Manner, and OBJ for Objective), and the TFA value: (1) České radiokomunikace musí v tomto roce rychle splatit dluh televizním divákům."
"This year, Czech Radiocommunications have quickly to pay their debt to the TV viewers. (1&apos;) ((České.f) radiokomunikace.t) ((tomto.t) Czech Radiocommunications this roce."
Necess.f (rychle.f) in-year must-pay quickly (dluh.f ((televizním.f) divákům.f)) debt TV viewers
"During the development of a discourse, in the prototypical case, a new discourse referent emerges as corresponding to a lexical occurrence that carries the index f; its further occurrences in the discourse carry t and are primarily guided by the scale of their degrees of salience."
"This scale, which was discussed[REF_CITE], has to be reflected in a description of the semantico-pragmatic layer of the discourse."
In this sense our approach can be viewed as pointing to a useful enrichment of the existing theories of discourse representation (cf. also[REF_CITE]).
"In the annotation system of PDT, not only values of attributes concerning sentence structure are assigned, but also values of attributes for coreferential links in the discourse, which capture certain features typical for the linking of sentences to each other and to the context of situation and allow for a tentative characterization of the discourse pattern in what concerns the development of salience degrees during the discourse."
"The following attributes of this kind are applied within a selected part of PDT, called &apos;model collection&apos; (for the time being, essentially only pronouns such as &apos;on&apos; (he), including its zero form, or &apos;ten&apos; (this) are handled in this way):"
"The development of salience degrees during a discourse, as far as determined by these rules, may be illustrated on the basis of five sentence tokens (utterances) from PDT, starting from (1), which constitute a segment of a newspaper text (we indicate the numerical values of salience reduction for every noun token that is a referring expression)."
"We present here - similarly as with (1&apos;) in Section 3 above - highly simplified representations of these sentences, with parentheses for every dependent member and the symbols t, c, and f for contextual boundness; numbers of the degrees of salience (more precisely, of salince reduction) for every referring expression are inserted in the sentences themselves."
"This example should enable the reader to check (at least in certain aspects) the general function of the procedure we use, as well as the degree of its empirical adequacy in the points it covers, and also our consistence in assigning the indices."
"We are aware of the preliminary character of our analysis, which may and should be enriched in several respects (not to cover only noun groups, to account for possible episodic text segments, for oral speech with the sentence prosody, for cases of deictically, rather anaphoricallythan conditioned salience, etc.)."
"We do not reflect several peripheral points, such as the differences between surface word order and the scale of CD (underlying WO), mainly caused by the fact that a dependent often precedes its head word on the surface (in morphemics), although if the dependent has f (as e.g. rychle (quickly) has in (1)), then it follows its head under CD (with the exceptions of focus sensitive particles, cf."
"Hajičová,[REF_CITE]); our translations are literal. (1) České radiokomunikace.[Footnote_1] musí v tomto roce.[Footnote_1] rychle splatit dluh.0 televizním divákům.0"
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"In this year, Czech Radiocommunications have quickly to pay their debt to the TV viewers. (1&apos;) ((České.f) radiokomunikace.t) ((tomto.t) Czech Radiocommunications this roce."
Necess.f (rychle.f) in-year must-pay quickly (dluh.f ((televizním.f) divákům.f)) debt TV viewers (2) Jejich.[Footnote_1] vysílače.[Footnote_1] dosud pokrývají signálem.0 programu.0 ČT.1 2.0 méně než-polovinu.0 území.0 republiky.0.
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"Their transmitters hitherto cover by-signal of-the-program ČT2 less than a-half of-the-territory of-the-Republic. ([Footnote_2]&apos;) ((jejich.t) vysílače.t) (dosud.t) pokrývají.f (signálem.f (programu.f (ČT.t (2.f)))) ((méně.f (než-polovinu.f)) území.f (republiky.t)) (3) Na moravsko-slovenském pomezí.1 je řada míst.0, kde nezachytí ani první program.0 České televize.1."
"2 These tentative rules, which have been presented at several occasions (starting[REF_CITE]) for the aims of a further discussion, still wait for a systematic testing and evaluation, as well as for enrichments and more precise formulations. These issues may find new opportunities now, when e.g. a comparison with the centering theory gets possible and when a large set of annotated examples from continuous texts in PDT is available. An automatic derivation of such features can only be looked for after the lexical units included get a very complex and subtle semantic classification."
On the-Moravian-Slovakian borderline there-is a-number of-places where (they) do-not-get even the-first program of-Czech Television. (3&apos;) ((na-moravsko-slovenském.t) pomezí.t) je.f (řada.f (míst.f ((kde.t) (oni.t) (ne.f) zachytí.f ((ani.f) (první.f) program.t ((České.t) televize.t))))) (4) Do rozdělení.[Footnote_1] federace.[Footnote_1] totiž signál.[Footnote_1] zajišťovaly vysílače.0 v SR.0.
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
"1 Only immediate associative links are taken into account for the time being, such as those between (Czech) crown and money, or between TV or (its) signal and (its) viewer."
Until the-division of-the-federation as-a-matter-of-fact the-signal.
Accusative provided transmitters.
Nominative in S(lovac)R(epublic). (4&apos;) (do-rozdělení.t (federace.t)) (totiž.t) (signál.t) zajišťovaly.t (vysílače.f (v-SR.f)). (5) Česká televize žádá urychlenou výstavbu nových vysílačů.
Czech Television requires quick construction of-new transmitters. (5&apos;) ((Česká.t) televize.t) žádá.f ((urychlenou.f) výstavbu.f ((nových.f) vysílačů.t))
"The development of salience reduction of the referents most frequently mentioned in (1) - (5) is characterized in Tab. 1, which includes numbers of salience reduction degrees and of those rules from Section 3 that are the main sources of the degrees."
"Two further remarks may be added, concerning details of our analysis that have not been discussed above and may not be directly found in the previous publications we refer to: (a) a noun group consisting of a head with t or c and of one or more adjuncts with f constitutes a referring expression as a whole, in the prototypical case, and gets degree 0, if it occurs in F; this concerns e.g. the group vysílače v SR (‘transmitters in the Slovac Republic’) in sentence (4), or ČT 2 (CTV 2) in (2); here 2 is treated as an adjunct of CT; (b) the difference between the degrees 0 and 1 is not sufficient for a safe choice of reference, so that, e.g., the reference of the pronoun jejich (their) after (1) by itself is indistinct, and only inferencing helps to establish that České radiokomunikace (Czech Radiocommunications) are referred to (viewers normally do not have transmitters at their diposal)."
"Even with this short piece of discourse, its segmentation is reflected, if its first subsegment, discussed up to now (sentences (1) - (5)), is compared with its continuation, i.e. sentences (6) - (9), given below."
"While the first segment deals primarily with CTV and its signal (cf. the relatively high salience of CTV, CTV1, CTV2, RC, signal and viewer in most parts of the segment), sentences (6) – (9) are devoted to financial issues, as can be seen from the following facts: (a) money gets degree 0 after (6), in which it functions as its focus proper (the most dynamic item), (b) Czech crown gets degree 1 after (7), in which it is an embedded part of the focus, and (c) the group financial coverage gets degree 1 in sentence (8)."
"The continuation is presented here without the TGTSs: (6) Naše společnost může úkol splnit, ale chybějí nám peníze."
Our company can the-task.
"Accusative fulfil, but is-lacking us."
"Nominative. (7) Letos by výstavba technického zařízení v sedmi lokalitách stála 120 miliónů korun, ale můžeme uvolnit jen 80 miliónů."
"This-year, would the-construction of-technical equipment in seven localities cost 120 million crowns, but we-can spend only 80 million. (8) Proto o finančním zabezpečení jednáme s Českou televizí, uvádí ekonomický ředitel Českých radiotelekomunikací Miroslav Cuřín."
"Therefore about (its) financial coverage we-discuss with Czech Television, states the-economic director of-Czech Radiotelcommunications M. C. (9)[REF_CITE]miliónů korun si vyžádá výstavba vysílačů a převaděčů signálu v pohraničí."
Accusative Refl. will-require the-construction.
Nominative of-transmitters and transferrers of-the-signal in the-border-area.
"We are aware that, along with the rules characterized above, there are other factors that have to be investigated, which are important for different kinds of discourses."
"This concerns various aspects of the discourse situation, of domain knowledge, of specific textual patterns (with episodes, poetic effects, and so on)."
"Factors of these and further kinds can be studied on the basis of the salience degrees, which are typical for basic discourse situations."
"In any case, we may conclude that it is useful for a theory of discourse semantics to reflect the degrees of salience."
This makes it possible to distinguish the reference potential of referring expressions and thus the connectedness of the discourse.
"Discourse analysis of this kind may also be useful for application domains such as text segmentation (in accordance with topics of individual segments), or data mining (specifying texts in which a given topic is actually treated, rather than being just occasionally mentioned)."
This paper presents an open-domain textual Question-Answering system that uses several feedback loops to en-hance its performance.
"These feedback loops combine in a new way statistical results with syntactic, semantic or pragmatic information derived from texts and lexical databases."
The paper presents the contribution of each feed-back loop to the overall performance of 76% human-assessed precise answers.
"Open-domain textual Question-Answering (Q&amp;A), as defined by the TREC competitions [Footnote_1] , is the task of identifying in large collections of documents a text snippet where the answer to a natural language question lies."
"1 The Text REtrieval Conference (TREC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-of-the-art in information retrieval (IR)"
The answer is constrained to be found either in a short (50 bytes) or a long (250 bytes) text span.
"Frequently, keywords extracted from the natural language question are either within the text span or in its immediate vicinity, forming a text para-graph."
"Since such paragraphs must be identified throughout voluminous collections, automatic and autonomous Q&amp;A systems incorporate an index of the collection as well as a paragraph retrieval mechanism."
Recent results from the TREC evaluations ([REF_CITE]) show that Information Retrieval (IR) techniques alone are not sufficient for finding an-swers with high precision.
"In fact, more and more systems adopt architectures in which the seman-tics of the questions are captured prior to para-graph retrieval (e.g.[REF_CITE]) and used later in extracting the answer (cf.[REF_CITE])."
When processing a natural language question two goals must be achieved.
"First we need to know what is the expected answer type; in other words, we need to know what we are looking for."
"Sec-ond, we need to know where to look for the an-swer, e.g. we must identify the question keywords to be used in the paragraph retrieval."
"The expected answer type is determined based on the question stem, e.g. who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example what), as described[REF_CITE]."
However finding question keywords that retrieve all candidate an-swers cannot be achieved only by deriving some of the words used in the question.
"Frequently, question reformulations use different words, but imply the same answer."
"Moreover, many equiv-alent answers are phrased differently."
In this pa-per we argue that the answer to complex natural language questions cannot be extracted with sig-nificant precision from large collections of texts unless several lexico-semantic feedback loops are allowed.
In Section 2 we survey the related work whereas in Section 3 we describe the feedback loops that refine the search for correct answers.
Section 4 presents the approach of devising key-word alternations whereas Section 5 details the recognition of question reformulations.
Section 6 evaluates the results of the Q&amp;A system and Sec-tion 7 summarizes the conclusions.
Mechanisms for open-domain textual Q&amp;A were not discovered in the vacuum.
"In parallel, In-formation Extraction (IE) techniques were devel-oped under the TIPSTER Message Understand-ing Conference (MUC) competitions."
"Typically, IE systems identify information of interest in a text and map it to a predefined, target represen-tation, known as template."
"Although simple com-binations of IR and IE techniques are not practical solutions for open-domain textual Q&amp;A because IE systems are based on domain-specific knowl-edge, their contribution to current open-domain Q&amp;A methods is significant."
"For example, state-of-the-art Named Entity (NE) recognizers devel-oped for IE systems were readily available to be incorporated in Q&amp;A systems and helped recog-nize names of people, organizations, locations or dates."
"Assuming that it is very likely that the answer is a named entity,[REF_CITE]describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer."
"Un-fortunately this system is not fully autonomous, as it depends on IR results provided by exter-nal search engines."
Answer extractions based on NE recognizers were also developed in the Q&amp;A presented[REF_CITE].
"As noted[REF_CITE], Q&amp;A sys-tems that did not include NE recognizers per-formed poorly in the TREC evaluations, espe-cially in the short answer category."
"Some Q&amp;A systems,[REF_CITE]relied both on NE recognizers and some empirical indicators."
"However, the answer does not always belong to a category covered by the NE recognizer."
For such cases several approaches have been devel-oped.
"The first one, presented[REF_CITE], the answer type is derived from a large answer taxonomy."
"A different approach, based on statistical techniques was proposed[REF_CITE].[REF_CITE]presents a method of extracting answers as noun phrases in a novel way."
Answer extraction based on grammatical information is also promoted by the system de-scribed[REF_CITE].
"One of the few Q&amp;A systems that takes into account morphological, lexical and semantic al-ternations of terms is described[REF_CITE]."
"To our knowledge, none of the cur-rent open-domain Q&amp;A systems use any feed-back loops to generate lexico-semantic alterna-tions."
This paper shows that such feedback loops enhance significantly the performance of open-domain textual Q&amp;A systems.
"Before initiating the search for the answer to a natural language question we take into account the fact that it is very likely that the same ques-tion or a very similar one has been posed to the system before, and thus those results can be used again."
"To find such cached questions, we measure the similarity to the previously processed ques-tions and when a reformulation is identified, the system returns the corresponding cached correct answer, as illustrated in Figure 1."
"When no reformulations are detected, the search for answers is based on the conjecture that the eventual answer is likely to be found in a text paragraph that (a) contains the most repre-sentative question concepts and (b) includes a tex-tual concept of the same category as the expected answer."
"Since the current retrieval technology does not model semantic knowledge, we break down this search into a boolean retrieval, based on some question keywords and a filtering mech-anism, that retains only those passages containing the expected answer type."
Both the question key-words and the expected answer type are identified by using the dependencies derived from the ques-tion parse.
"By implementing our own version of the pub-licly available Collins parser[REF_CITE], we also learned a dependency model that enables the mapping of parse trees into sets of binary rela-tions between the head-word of each constituent and its sibling-words."
"For example, the parse tree of TREC-9 questi[REF_CITE]: “How many dogs pull a sled in the Iditarod ?” is:"
"For each possible constituent in a parse tree, rules first described[REF_CITE]and[REF_CITE]identify the head-child and propagate the head-word to its parent."
For the parse of questi[REF_CITE]the propagation is:
"When the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form[REF_CITE]."
COUNT dogs pull sled Iditarod
"In the structure above, C OUNT represents the expected answer type, replacing the question stem “how many”."
"Few question stems are unambigu-ous (e.g. who, when)."
"If the question stem is am-biguous, the expected answer type is determined by the concept from the question semantic form that modifies the stem."
This concept is searched in an A NSWER T AXONOMY comprising several tops linked to a significant number of WordNet noun and verb hierarchies.
"Each top represents one of the possible expected answer types imple-mented in our system (e.g. P ERSON , P RODUCT , N UMERICAL V ALUE , C OUNT , L OCATION )."
We encoded a total of 38 possible answer types.
"In addition, the question keywords used for paragraph retrieval are also derived from the ques-tion semantic form."
"The question keywords are organized in an ordered list which first enumer- ates the named entities and the question quota-tions, then the concepts that triggered the recogni-tion of the expected answer type followed by all adjuncts, in a left-to-right order, and finally the question head."
The conjunction of the keywords represents the boolean query applied to the doc-ument index.[REF_CITE]details the empirical methods used in our system for trans-forming a natural language question into an IR query.
It is well known that one of the disadvantages of boolean retrieval is that it returns either too many or too few documents.
"However, for ques-tion answering, this is an advantage, exploited by the first feedback loop represented in Figure 1."
Feedback loop 1 is triggered when the number of retrieved paragraphs is either smaller than a min-imal value or larger than a maximal value deter-mined beforehand for each answer type.
"Alterna-tively, when the number of paragraphs is within limits, those paragraphs that do not contain at least one concept of the same semantic category as the expected answer type are filtered out."
"The remaining paragraphs are parsed and their depen-dency structures, called answer semantic forms, are derived."
Feedback loop 2 illustrated in Figure 1 is acti-vated when the question semantic form and the answer semantic form cannot by unified.
The uni-fication involves three steps:
Step 1: The recognition of the expected answer type.
The first step marks all possible concepts that are answer candidates.
"For example, in the case of TREC -9 questi[REF_CITE]: “Where did the ukulele originate ?”, the expected answer type is L OCATION ."
In the paragraph “the ukulele intro-duced from Portugal into the Hawaiian islands” contains two named entities of the category
L O - CATION and both are marked accordingly.
Step [Footnote_2]: The identification of the question con-cepts.
2 Some modifiers might be missing from the answer.
"The second step identifies the question words, their synonyms, morphological deriva-tions or WordNet hypernyms in the answer se-mantic form."
Step 3: The assessment of the similarities of dependencies.
"In the third step, two classes of similar dependencies are considered, generating unifications of the question and answer semantic forms:"
Class L2-1: there is a one-to-one mapping be-tween the binary dependencies of the question and binary dependencies from the answer seman-tic form.
"Moreover, these dependencies largely cover the question semantic form 2 ."
An example is:
"We find an entailment between producing, or making and selling goods, derived from Word-Net, since synset make, produce, create has the genus manufacture, defined in the gloss of its ho-momorphic nominalization as “for sale”."
There-fore the semantic form of questi[REF_CITE]and its illustrated answer are similar.
"Class L2-2: Either the question semantic form or the answer semantic form contain new con- cepts, that impose a bridging inference."
The knowledge used for inference is of lexical nature and is later employed for abductions that justify the correctness of the answer.
"Nouns head and government are constituents of a possible paraphrase of president, i.e. “head of government”."
"However, only world knowledge can justify the answer, since there are countries where the prime minister is the head of govern-ment."
"Presupposing this inference, the semantic form of the question and answer are similar."
Feedback loop 3 from Figure 1 brings forward additional semantic information.
"Two classes of similar dependencies are considered for the ab-duction of answers, performed in a manner simi-lar to the justifications described[REF_CITE]."
"Class L3-1: is characterized by the need for contextual information, brought forward by ref-erence resolution."
"In the following example, a chain of coreference links Bill Gates and Mi-crosoft founder in the candidate answer:"
Class L3-2: Paraphrases and additional infor-mation produce significant differences between the question semantic form and the answer se-mantic form.
"However, semantic information contributes to the normalization of the answer dependencies until they can be unified with the question dependencies."
"For example, if (a) a vol-cano I S -A mountain; (b) lava I S -P ART of vol-cano, and moreover it is a part coming from the inside; and (c) fragments of lava have all the prop-erties of lava, the following question semantic form and answer semantic form can be unified:"
The semantic information and the world knowledge needed for the above unifications are available from WordNet[REF_CITE].
"More-over, this knowledge can be translated in ax-iomatic form and used for abductive proofs."
Each of the feedback loops provide the retrieval en-gine with new alternations of the question key-words.
Feedback loop 2 considers morphological and lexical alternations whereas Feedback loop 3 uses semantic alternations.
The method of gener-ating the alternations is detailed in Section 4.
"To enhance the chance of finding the answer to a question, each feedback loop provides with a different set of keyword alternations."
Such alternations can be classified according to the linguistic knowledge they are based upon: 1.Morphological Alternations.
"When lexical alternations are necessary because no answer was found yet, the first keyword that is altered is determined by the question word that either prompted the expected answer type or is in the same semantic class with the expected answer type."
"For example, in the case of questi[REF_CITE]: “Who invented the paper clip ?”, the expected answer type is P ERSON and so is the subject of the verb invented , lexicalized as the nominalization inventor."
"Moreover, since our retrieval mechanism does not stem keywords, all the inflections of the verb are also considered."
"Therefore, the initial query is expanded into: 2."
WordNet encodes a wealth of semantic information that is easily mined.
"Seven types of semantic relations span concepts, enabling the retrieval of synonyms and other semantically related terms."
Such alternations improve the recall of the answer paragraphs.
"For example, in the case of questi[REF_CITE]: “Who killed Martin Luther King ?”, by considering the synonym of killer, the noun assassin, the Q&amp;A system retrieved paragraphs with the correct answer."
"Similarly, for the questi[REF_CITE]: “How far is the moon ?”, since the adverb far is encoded in WordNet as being an attribute of distance, by adding this noun to the retrieval keywords, a correct answer is found. 3."
Semantic Alternations and Paraphrases.
We define as semantic alternations of a keyword those words or collocations from WordNet that (a) are not members of any WordNet synsets containing the original keyword; and (b) have a chain of WordNet relations or bigram relations that connect it to the original keyword.
These relations can be translated in axiomatic form and thus participate to the abductive backchaining from the answer to the question - to justify the answer.
For example semantic alternations involving only WordNet relations were used in the case of questi[REF_CITE]: “Where do lobsters like to live ?”.
"Since in WordNet the verb prefer has verb like as a hypernym, and moreover, its glossed definition is liking better, the query becomes:"
QUERY[REF_CITE]: lobsters AND (like OR prefer) AND live
Sometimes multiple keywords are replaced by a semantic alternation.
"Sometimes these alterna-tions are similar to the relations between multi-term paraphrases and single terms, other time they simply are semantically related terms."
"In the case of questi[REF_CITE]: “How many dogs pull a sled in the Iditarod ?”, since the definition of Word-Net sense 2 of noun harness contains the bigram “pull cart” and both sled and cart are forms of vehicles, the alternation of the pair of keywords pull, slide is rendered by harness."
"Only when this feedback is received, the paragraph contain-ing the correct answer is retrieved."
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described[REF_CITE]):
"Heuristic 1: Whenever the first feedback loop re-quires the addition of the main verb of the ques-tion as a query keyword, generate all verb conju-gations as well as its nominalizations."
"Heuristic 2: Whenever the second feedback loop requires lexical alternations, collect from Word-Net all the synset elements of the direct hyper-nyms and direct hyponyms of verbs and nomi-nalizations that are used in the query."
"If multiple verbs are used, expand them in a left-to-right or-der."
"Heuristic 3: Whenever the third feedback loop imposes semantic alternations expressed as para-phrases, if a verb and its direct object from the question are selected as query keywords, search for other verb-object pairs semantically related to the query pair."
"When new pairs are located in the glosses of a synset ! , expand the query verb-object pair with all the elements from ! ."
"Another set of possible alternations, defined by the existence of lexical relations between pairs of words from different question are used to de-tect question reformulations."
The advantage of these different forms of alternations is that they enable the resolution of similar questions through answer caching instead of normal Q&amp;A process-ing.
"In[REF_CITE]questions were reformulations of 54 inquiries, thus asking for the same answer."
"The reformulation classes contained variable number of questions, ranging from two to eight questions."
Two examples of reformulation classes are listed in Table 1.
"To classify questions in reformulation groups, we used the algorithm:"
"Reformulation Classes(new question, old questions) 1."
"For each question from old questions 2. Compute similarity(question,new question) 3. Build a new similarity matrix &quot; such that it is generated by adding to the matrix for the old questions a new row and a new column representing the similarities computed at step 2. 4. Find # the transitive closures # for the set old questions &amp;$ % new question $ 5."
Result: reformulation classes as transitive closures.
In Figure 2 we represent the similarity matrix for six questions that were successively posed to the answer engine.
"Since question reformulations are transitive relations, if at a step &apos; questions *( ) and ,( + are found similar and ( ) already belongs to - , a reformulation class previously discovered (i.e. a group of at least two similar questions), then question ( + is also included in - ."
Figure 2 illustrates the transitive closures for reformula-tions at each of the five steps from the succession of six questions.
"To be noted that at step 4 no new similarities were found , thus (/. is not found sim-ilar to *( 0 at this step."
"However, at step 5, since /1 is found similar to both 20( and *( . , (*0 results ( similar to all the other questions but *( 3 ."
The algorithm that measures the similarity be-tween two questions is:
"Algorithm Similarity(Q, Q’)"
Input: a pair of question represented as two word strings:
Q: :9:9;&lt; and Q’: 48=5 48=7 9&gt;9:=&lt; 9:&gt;9 9;? 1. Apply a part-of-speech tagger on both questions:
Tag(Q): 465A@;;@ :9:K&lt; LBMCFEN@ &lt;
Tag(Q’): 4 5= ;@ BDCFE 5= 4 7= @;BDCIE 7= 9:9:6? @LBMCFE =? 2. Set nr matches=0 3. Identify quadruples  T= RSBMCFE ;T= U such that if 4 Q and =T are content words with BMCFE QWV BMCFEIT= and Lexical relation OP4 Q RS4 =T U holds then increase nr matches 4. Relax the Lexical relation and goto step 3; 5.
If (nr matches @ number of content words XYB U then Q and Q’ are similar
The Lexical relation between a pair of con-tent words is initially considered to be a string identity.
"In later loops starting at step 3 one of the following three possible relaxations of Lex-ical relation are allowed: (a) common morpho-logical root (e.g. owner and owns, from questi[REF_CITE]: “Who is the owner of CNN ?” and ques-ti[REF_CITE]: “Who owns CNN ?” respectively); (b) WordNet synonyms (e.g. gestation and preg-nancy from questi[REF_CITE]: “How long is hu-man gestation ?” and questi[REF_CITE]: “A nor-mal human pregnancy lasts how many months ?”, respectively) or (c) WordNet hypernyms (e.g. the verbs erect and build from questi[REF_CITE]: “When was Berlin’s Brandenburg gate erected ?” and questi[REF_CITE]: “When was the Brandenburg Gate in Berlin built ?” respectively)."
To evaluate the role of lexico-semantic feedback loops in an open-domain textual Q&amp;A system we have relied on the 890 questions employed in the TREC-8 and TREC-9 Q&amp;A evaluations.
"In TREC, for each question the performance was computed by the reciprocal value of the rank (RAR) of the highest-ranked correct answer given by the system."
"Given that only the first five an-swers were considered in the TREC evaluations, i f the RAR is defined as *Z \[ ^Z ] `La;_bdc Q its value is 1 if the first answer is correct; 0.5 if the second an-swer was correct, but not the first one; 0.33 when the correct answer was on the third position; 0.25 if the fourth answer was correct; 0.2 when the fifth answer was correct and 0 if none of the first five answers were correct."
"The Mean Reciprocal An-swer Rank (MRAR) is used to compute the over-all performance of the systems participating b in the TREC evaluation efZ*[gZ^] bi_ hkj ) `La;bd_ c Qml In ad-dition, TREC-9 imposed the constraint that an an-swer is considered correct only when the textual context from the document that contains it can account for it."
"When the human assessors were convinced this constraint was satisfied, they con-sidered the RAR to be strict, otherwise, the RAR was considered lenient."
NIST for the system on which we evaluated the role of lexico-semantic feedbacks.
Table 3 lists the quantitative analysis of the feedback loops.
Loop 1 was generated more often than any other loop.
"However, the small overall average number of feedback loops that have been carried out in-dicate that the fact they port little overhead to the Q&amp;A system."
More interesting is the qualitative analysis of the effect of the feedback loops on the Q&amp;A eval-uation.
"Overall, the precision increases substan-tially when all loops were enabled, as illustrated in Table 4."
"Individually, the effect of Loop 1 has an ac-curacy increase of over 40%, the effect of Loop 2 had an enhancement of more than 52% while Loop 3 produced an enhancement of only 8%."
"Ta-ble 4 lists also the combined effect of the feed- backs, showing that when all feedbacks are en-abled, for short answers we obtained an MRAR of 0.568, i.e. 76% increase over Q&amp;A without feed-backs."
The MRAR for long answers had a sim-ilar increase of 91%.
"Because we also used the answer caching technique, we gained more than 1% for short answers and almost 3% for long an-swers, obtaining the result listed in Table 2."
"In our experiments, from the total of 890 TREC ques-tions, lexical alternations were used for 129 ques-tions and the semantic alternations were needed only for 175 questions."
This paper has presented a Q&amp;/A system that em-ploys several feedback mechanisms that provide lexical and semantic alternations to the question keywords.
"By relying on large, open-domain lin-guistic resources such as WordNet we enabled a more precise approach of searching and mining answers from large collections of texts."
"Evalua-tions indicate that when all three feedback loops are enabled we reached an enhancement of al-most 76% for short answers and 91% for long an-swers, respectively, over the case when there are no feedback loops."
"In addition, a small increase is produced by relying on cached answers of sim-ilar questions."
Our results so far indicate that the usage of feedback loops that produce alter-nations is significantly more efficient than multi-word indexing or annotations of large corpora with predicate-argument information.
We present conditions under which verb phrases are elided based on a cor-pus of positive and negative examples.
"Factor that affect verb phrase ellipsis in-clude: the distance between antecedent and ellipsis site, the syntactic relation between antecedent and ellipsis site, and the presence or absence of adjuncts."
"Building on these results, we exam-ine where in the generation architec-ture a trainable algorithm for VP ellip-sis should be located."
We show that the best performance is achieved when the trainable module is located after the realizer and has access to surface-oriented features (error rate of 7.5%).
"While there is a vast theoretical and computa-tional literature on the interpretation of elliptical forms, there has been little study of the generation of ellipsis. [Footnote_1]"
"1 We would like to thank Marilyn Walker, three review-ers for a previous submission, and three reviewers for this submission for helpful comments."
"In this paper, we focus on Verb Phase Ellipsis (VPE), in which a verb phrase is elided, with an auxiliary verb left in its place."
Here is an example: (1)
"Here, the verb phase concluded at trial is omit-ted, and the auxiliary did appears in its place."
The basic condition on VPE is clear from the litera-ture: [Footnote_2] there must be an antecedent VP that is iden-tical in meaning to the elided VP.
"2 The classic study is[REF_CITE]; for more recent work, see, eg,[REF_CITE]."
"Furthermore, it seems clear that the antecedent must be suffi-ciently close to the ellipsis site (in a sense to be made precise)."
This basic condition provides a beginning of an account of the generation of VPE.
"However, there is more to be said, as is shown by the following examples: (2) Ernst &amp;"
Young said Eastern’s plan would miss projections by $100 million.
Goldman said Eastern would miss the same mark by at least $120 million.
"In this example, the italicized VP could be elided, since it has a nearby antecedent (in bold) with the same meaning."
Indeed the antecedents in this example is closer than in the following exam-ple in which ellipsis does occur: (3) In particular Mr Coxon says businesses are paying out a smaller percentage of their profits and cash flow in the form of dividends than they have VPE historically.
"In this paper, we identify factors which govern the decision to elide VPs."
"We examine a corpus of positive and negative examples; i.e., examples in which VPs were or were not elided."
"We find that, indeed, the distance between ellipsis site and an-tecedent is correlated with the decision to elide, as are the syntactic relation between antecedent and ellipsis site, and the presence or absence of adjuncts."
"Building on these results, we use ma-chine learning techniques to examine where in the generation architecture a trainable algorithm for VP ellipsis should be located."
We show that the best performance (error rate of 7.5%) is achieved when the trainable module is located after the re-alizer and has access to surface-oriented features.
"In what follows, we first describe our corpus of negative and positive examples."
"Next, we de-scribe the factors we coded for."
"Then we give the results of the statistical analysis of those factors, and finally we describe several algorithms for the generation of VPE which we automatically ac-quired from the corpus."
All our examples are taken from the Wall Street Journal corpus of the Penn Treebank (PTB).
We collected both negative and positive examples from Sections 5 and 6 of the PTB.
The negative examples were collected using a mixture of man-ual and automatic techniques.
"First, candidate ex-amples were identified automatically if there were two occurrences of the same verb, separated by fewer than 10 intervening verbs."
"Then, the col-lected examples were manually examined to de-termine whether the two verb phrases had identi-cal meanings or not. [Footnote_3] If not, the examples were eliminated."
"3 The proper characterization of the identity condition li-censing VPE remains an open area of research, but it is known to permit various complications, such as “sloppy identity” and “vehicle change” (see[REF_CITE]and references therein)."
The positive examples were taken from the cor-pus collected in previous work[REF_CITE].
"This is a corpus of several hundred examples of VPE from the Treebank, based on their syntac-tic analysis."
VPE is not annotated uniformly in the PTB.
"We found several different bracketing patterns and searched for these patterns, but one cannot be certain that no other bracketing patterns were used in the PTB."
"The negative and positive examples from Sections 5 and 6 – 126 in total – form our basic corpus, which we will refer to as S ECTIONS 5+6."
"While not pathologically peripheral, VPE is a fairly rare phenomenon, and 15 positive exam-ples is a fairly small number."
We created a second corpus by extending S ECTIONS 5+6 with positive examples from other sections of the PTB so that the number of positive examples equals that of the negative examples.
"Specifically, we included all positive examples from Section 8 through 13."
The result is a corpus with 111 negative examples – those from S ECTIONS 5+6 – and 121 positive ex-amples (including the 15 positive examples from S ECTIONS 5+6).
"We call this corpus B ALANCED ; clearly B ALANCED does not reflect the distribu-tion of VPE in naturally occurring text, as does S ECTIONS 5+6; we therefore use it only in exam-ining factors affecting VPE in Section 4, and we do not use it in algorithm evaluation in Section 5."
"Each example was coded for several features, each of which has figured implicitly or explicitly in the research on VPE."
The following surface-oriented features were added automatically.
"Sentential Distance (sed): Measures dis-tance between possible antecedent and can-didate, in sentences."
A value of 0 means that the VPs are in the same sentence.
"Word Distance (vpd): Measures distance between possible antecedent and candidate, in words."
Antecedent VP Length(anl):
"Measures size of the antecedent VP, in words."
All subsequent features were coded by hand by two of the authors.
The following morphological features were used:
"Auxiliaries (in1 and in2): Two features, for antecedent and candidate VP."
The value is the list of full forms of the auxiliaries (and verbal particle to) on the antecedent and can-didate verbs.
This information  can be an-notated  reliably (  and  ). [Footnote_4]
"4 Following[REF_CITE], we use the statistic to esti-mate reliability of annotation. We assume that values show reliability, and values &quot;#! $!% show suffi-cient reliability for drawing conclusions, given that the other variable we are comparing these variables to (VPE) is coded 100% correctly."
The following syntactic features were coded:
Voice (vox): Grammatical voice (ac-tive/passive) of antecedent and candidate.
This information  can be annotated reliably ( &amp; ).
Syntactic Structure (syn):
"This feature de-scribes the syntactic relation between the head verbs of the two VPs, i.e., conjunction (which includes “conjunction” by juxtaposi-tion of root sentences), subordination, com-parative constructions, and as-appositive (for example, the index maintains a level be-low 50%, as it has for the past couple of months)."
This information *) can +&apos; be annotated reasonably reliably ( ( ).
Subcategorization frame for each verb.
Standard distinctions between intransitive and transitive verbs with special categories for other subcategorization frames (total of six possible values).
These two features  can be annotated highly reliably ( &amp; ).
We now turn to semantic and discourse fea-tures.
"Adjuncts (adj): that the arguments have the same meaning is a precondition for VPE, and it is also a precondition for us to include a negative example in the corpus."
"Therefore, semantic similarity of arguments need not be coded."
"However, we do need to code for the semantic similarity of adjuncts, as they may differ in the case of VPE: in (3) above, the second (elided) VP has the additional ad-verb historically."
"We distinguish the follow-ing cases: adjuncts being identical in mean-ing, similar in meaning (of the same seman-tic category, such as temporal adjuncts), only the antecedent or candidate VP having an ad-junct, the adjuncts being different, there be-ing no adjuncts at all."
"This information can be annotated , reliably at a satisfactory level ( &amp; )."
"In-Quotes (qut): Is the antecedent and/or the candidate within a quoted passage, and if yes, is it semantically the same quote."
This information can be annotated highly reliably ( &amp;.- ).
Discourse Structure (dst): Are the dis-course segments containing the antecedent and candidate directly related in the dis-course structure?
"Possible values are Y and N. Here, “directly related” means that the two VPs are in the same segment, the seg-ments are directly related to each other, or the segments are both directly related to the same third discourse segment."
"For this fea-ture, inter-annotator agreement could not / be achieved to a satisfactory degree ( &amp; ), but the feature was not identified as use-ful during machine learning anyway."
"In fu-ture research, we hope to use independently coded discourse structure in order to investi-gate its interaction with ellipsis decisions."
Polarity (pol): Does the antecedent or can-didate sentence contain the negation marker not or one of its contractions.
This informa-tion can be annotated highly reliably ( - ).
"In this section, we analyze the data to find which factors correlate with the presence of absence of VPE."
We use the ANOVA test (or a linear model in the case of continuous-valued indepen-dent variables) and report the probability of the 2 value.
"We present results for both of our corpora: the S ECTIONS 5+6 corpus consisting only of exam-ples from Sections 5 and 6 of the Penn Tree Bank, and the B ALANCED corpus, containing a bal-anced number of negative and positive examples."
"Recall that B ALANCED is derived from S EC - TIONS 5+6 by adding positive examples, but no negative examples."
"Therefore, when summariz-ing the data, we report three figures: for the nega-tive cases (No VPE), all from S ECTIONS 5+6; for the positive cases in S ECTIONS 5+6 (SEC VPE); and for the positive cases in B ALANCED (BAL VPE)."
The two distance measures (based on words and based on sentences) both are significantly corre-lated with the presence of VPE while the length of the antecedent VP is not.
The results are sum-marized in Figure 1.
"For the two auxiliaries features, we do not get significant correlation for the auxiliaries on the antecedent VP, with either corpus."
"The situa-tion does not change if we distinguish only two classes, namely the presence or absence of auxil-iaries"
"When VPE occurs, the voice of the two VPs is the same, an effect [Footnote_5] that  is significant only in B AL - ANCED  ( ) ) but not in S ECTIONS 5+6 (  ), presumably because of the small number of data points."
"5 The interface between sentence planner and realizer dif-fers among approaches and can be more or less semantic; we will assume that it is an abstract syntactic interface, with structures marked for grammatical function, but which does not represent word order."
The counts are shown in Figure 2.
"The syntactic structure also correlates with VPE, with the different forms of subordination tion disfavoring VPE ( 394 favoring VPE, and the absence 5 of - fora directboth Srela- EC - TIONS 5+6 and B ALANCED )."
The frequency dis-tributions are shown in Figure 2.
Features related to argument structure are not significantly correlated with VPE.
"However, whether the two argument structures are identi-cal is a factor approaching significance: in the two cases 56/ - ).whereMore datatheymaydiffermake, nothisVPEresulthappensmore( &amp;3 robust."
"If the adjuncts of the antecedent and candidate VPs (matched pairwise) are the same, then VPE is more likely to happen."
"If only one VP or the other has adjuncts, or if the VPs have different tion is significant for both corpora ( 394 adjuncts, VPE is unlikely to happen."
The [Footnote_5] correla- - ).
"5 The interface between sentence planner and realizer dif-fers among approaches and can be more or less semantic; we will assume that it is an abstract syntactic interface, with structures marked for grammatical function, but which does not represent word order."
The distribution is shown in Figure 2.
Feature In-Quotes correlates [Footnote_5] significantly ;) with VPE 5 in 6 both corpora ( 3: for SEC and &lt;3 for BAL).
"5 The interface between sentence planner and realizer dif-fers among approaches and can be more or less semantic; we will assume that it is an abstract syntactic interface, with structures marked for grammatical function, but which does not represent word order."
"We see that VPE does not often occur across quotes, and that it occurs un-usually frequently within quotes, suggesting that it is more common in spoken language than in written language (or, at any rate, in the WSJ)."
The binary discourse structure [Footnote_5] feature  corre-lates significantly with VPE (  for S EC -
"5 The interface between sentence planner and realizer dif-fers among approaches and can be more or less semantic; we will assume that it is an abstract syntactic interface, with structures marked for grammatical function, but which does not represent word order."
"TIONS 5+6 and 3=4 5 - for BAL), with pres-ence of a close relation correlating with VPE."
"Since inter-annotator agreement was not achieved at a satisfactory level, the value of this feature re-mains to be confirmed."
The previous section has presented a corpus-based static analysis of factors affecting VPE.
"In this section, we take a computational approach."
We would like to use a trainable module that learns rules to decide whether or not to perform VPE.
Trainable components have the advantage of easily being ported to new domains.
For this reason we use the machine learning system Rip-per[REF_CITE].
"However, before we can use Ripper, we must discuss the issue of how our new trainable VPE module fits into the architecture of generation."
"Tasks in the generation process have been di-vided into three stages[REF_CITE]: the text planner has access only to in-formation about communicative goals, the dis-course context, and semantics, and generates a non-linguistic representation of text structure and content."
"The sentence planner chooses abstract linguistic resources (meaning-bearing lexemes, syntactic constructions) and determines sentence boundaries."
"It passes an abstract lexico-syntactic specification 5 to the Realizer, which inflects, adds function words, and linearizes, thus produc-ing the surface string."
The question arises where in this architecture the decision about VPE should be made.
"We will investigate this question in this section by distinguishing three places for making the VPE decision: in or just after the text planner; in or just after the sentence planner; and in or just after the realizer (i..e, at the end of the whole gen-eration process if there are no modules after real-ization, such as prosody)."
"We will refer to these three architecture options as TP, SP, and Real."
"From the point of view of this study, the three options are distinguished by the subset of the fea- tures as identified in Section 3 that the algorithm has access to: TP only has access to discourse and semantic features; SP can also use syntactic features, but not morphological features or those that relate to surface ordering."
Real can access all features.
We summarize the relation between architecture option and features in Figure 3.
We use Ripper to automatically learn rule sets from the data.
"Ripper is a rule learning program, which unlike some other machine learning pro-grams supports bag-valued features. 6 Using a set of attributes, Ripper greedily learns rule sets that choose one of several classes for each data set."
"We use two classes, vpe and novpe."
"By using different parameter settings for Ripper, we obtain different rule sets."
"These parameter settings are of two types: first, parameters internal to Ripper, such as the number of optimization passes; and second, the specification of which attributes are used."
"To determine the optimal number of opti-mization passes, we randomly divided our S EC - TIONS 5+6 corpus into a training and test part, with the test corpus representing 20% of the data."
We then ran Ripper with different settings for the optimization pass parameter.
We determined that best results are obtained with six passes.
We then used this setting in all subsequent work with Rip-per.
The test/training partition used to determine this setting was not used for any other purpose.
"In the next subsection (Section 5.3), we present and discuss several rule sets, as they bring out dif-ferent properties of ellipsis."
"We discuss rule sets trained on and evaluated against the entire set of data from S ECTIONS 5+6: since our data set is relatively small, we decided not to divide it into distinct training and test sets (except for deter-mining the internal parameter; see above)."
"The fact that these rule sets are obtained by a ma-chine learning algorithm is in some sense inci-dental here, and while we give the coverage fig-ures for the training corpus, we consider them of mainly qualitative interest."
"We present three rule sets, one each for each of three architecture options, each one with its own set of attributes."
"We start out with a full set of attributes, and suc- cessively eliminate the more surface-oriented and syntactic ones."
"As we will see, the earlier the VPE decision is made, the less reliable it is."
"In the subsection after next (Section 5.4), we present results using ten-fold cross-validation, for which the quantitative results are meaningful."
"However, since each run produces ten different rule sets, the qualitative results, in some sense, are not meaningful."
We therefore do not give any rule sets; the cross-validation demonstrates that effec-tive rule sets can be learned even from relatively small data sets.
We will present three different rule sets for the three architecture options.
"All rule sets must be used in conjunction with a basic screening al-gorithm, which is the same one that we used in order to identify negative examples: there must be two identical verbs with at most ten interven-ing verbs, and the arguments of the verbs must have the same meaning."
Then the following rule sets can be applied to determine whether a VPE should be generated or not.
"We start out with the Real set of features, which is available after realization has completed, and thus all surface-oriented and morphological features are available."
"Of course, we also assume that all other features are still available at that time, not just the surface features."
We obtain the following rule set:
Choose VPE if sed&lt;=0 and syn=com ([Footnote_6]/0).
"6 Our only bag-valued set of features is the set of auxil-iaries, which is not used in the rules we present here."
"Choose VPE if vpd&lt;=14, sed&lt;=0, and anl&gt;=3 (7/1)."
Otherwise default to no VPE (110/2).
Each rule (except the first) only applies if the preceding ones do not.
"The first rule says that if the distance in sentences between the antecedent VP and candidate VP (sed) is less than or equal to 0, i.e., the candidate and the antecedent are in the same sentence, and the syntactic construc-tion is a comparative, then choose VPE."
This rule accounts for 6 cases correctly and misclassified none.
"The second rule says that if the distance in words between antecedent VP and candidate VP is less than or equal to 14, and the VPs are in the same sentence, and the antecedent VP con-tains 3 or more words, then the candidate VP is elided."
This rule accounts for 7 cases correctly but misclassified one.
"Finally, all other cases are not treated as VPE, which misses 2 examples but classifies 110 correctly."
This yields an overall training error rate of 2.4% (3 misclassified exam-ples). (Recall that we are here comparing the per-formance against the training set.)
"We now consider the examples from the intro-duction, which are repeated here for convenience. (4)"
Young said Eastern’s plan would miss projections by $100 million.
Goldman said Eastern would miss the same mark by at least $120 million. (6) In particular Mr Coxon says businesses are paying out a smaller percentage of their profits and cash flow in the form of dividends than they have VPE historically.
Consider example (4).
"The first rule does not apply (this is not a comparative), but the second does, since both VPs are in the same sentence, and the antecedent has three words, and the dis-tance between them is fewer than 14 words."
Thus (4) would be generated as a VPE.
"The first rule does apply to example (6), so it would also be generated as a VPE."
"Example (5), however, is not caught by either of the first two rules, so it would not yield a VPE."
We thus replicate the data in the corpus for these three examples.
We now turn to SP.
"We assume that we are making the VPE decision before realization, and therefore have access only to syntactic and se-mantic features, but not to surface features."
"As a result, distance in words is no longer available as a feature."
"Here, we first choose VPE if the antecedent and candidate are in the same sentence and the an-tecedent VP length is greater than three, or if the two VPs are in the same sentence and they have the same adjuncts."
"In all other cases, we choose not to elide."
The training error rate goes up to 3.97%.
"With this rule set, we can correctly predict a VPE for examples (4) and (6), using the first rule."
"We do not generate a VPE for (5), since it does not match either of the two first rules."
"Finally, we consider architecture option TP, in which the VPE decision is made right after text planning, and only semantic and discourse fea-tures are available."
The rule set is simplified:
Choose VPE if adj=sam (6/3).
Otherwise default to no VPE (108/9).
"VPE is only chosen if the adjuncts are the same; in all other cases, VPE is avoided."
The training error rate climbs to 9.52%.
"For our examples, only example (4) generates a VPE since the adjuncts are the same on the two VPS [Footnote_7] (6) fails to meet the requirements of the first rule since the second VP has an adjunct of its own, historically."
"7 The adjunct is elided on the second VP, of course, but present in the input representation, not shown here."
In the previous subsection we presented different rule sets.
We now show that rule sets can be de-rived in a consistent manner and tested on a held-out test set with satisfactory results.
"We take these results to be indicative of performance on unseen data (which is in the WSJ domain and genre, of course)."
"We use ten-fold cross-validation for this purpose, with the same three sets of possible at-tributes used above."
The results for the three attribute sets are shown in Figure 4 (average error rates for the tenfold can be made after sentence planning.
"However, it is also possible that decisions about VPE (and related pronominal constraints) cannot be made before the text is linearized, presumably because of the processing limitations of the hearer/reader (and of the speaker/writer)."
"The baseline is obtained by never choosing VPE (which, recall, is relatively rare in the S ECTIONS 5+6 corpus)."
"We see that the TP architecture does not do better than the baseline, while SP results in an error reduction of 23% and the Real architecture in an error reduc-tion of 35%, for an average error rate of 7.5%."
"We have found that the decision to elide VPs is statistically correlated with several factors, in-cluding distance between antecedent and candi-date VPs by word or sentence, and the pres-ence or absence of syntactic and discourse rela-tions."
These findings provide a strong founda-tion on which to build algorithms for the gener-ation of VPE.
"We have explored several possible algorithms with the help of a machine learning system, and we have found that these automati-cally derived algorithms perform well on cross-validation tests."
"We have also seen that the decision whether or not to elide can be better made later in the gen-eration process: the more features are available, the better."
"It is perhaps not surprising that the de-cision cannot be made very well just after after text planning: it is well known that VPE is subject to syntactic constraints, and the relevant informa-tion is not yet available."
"It is perhaps more sur-prising that the surface-oriented features appear to contribute to the quality of the decision, push-ing the decision past the realization phase."
"One possible explanation is that there are in fact other features, which we have not yet identified, and for which the surface-oriented features are stand-ins."
"If this is the case, further work will allow us to define algorithms so that the decision on VPE"
"In this paper we address the problem of extracting key pieces of information from voicemail messages, such as the identity and phone number of the caller."
"This task differs from the named entity task in that the information we are inter-ested in is a subset of the named entities in the message, and consequently, the need to pick the correct subset makes the problem more difficult."
"Also, the caller’s identity may include informa-tion that is not typically associated with a named entity."
"In this work, we present three information extraction methods, one based on hand-crafted rules, one based on maximum entropy tagging, and one based on probabilistic trans-ducer induction."
We evaluate their per-formance on both manually transcribed messages and on the output of a speech recognition system.
"In recent years, the task of automatically extract-ing information from data has grown in impor-tance, as a result of an increase in the number of publicly available archives and a realization of the commercial value of the available data."
One as-pect of information extraction (IE) is the retrieval of documents.
"Another aspect is that of identify-ing words from a stream of text that belong in pre-defined categories, for instance, “named entities” such as proper names, organizations, or numerics."
"Though most of the earlier IE work was done in the context of text sources, recently a great deal of work has also focused on extracting information from speech sources."
"Examples of this are the Spoken Document Retrieval (SDR) task[REF_CITE], named entity (NE) extracti[REF_CITE]."
The SDR task focused on Broadcast News and the NE task focused on both Broadcast News and telephone conversations.
"In this paper, we focus on a source of con-versational speech data, voicemail, that is found in relatively large volumes in the real-world, and that could benefit greatly from the use of IE tech-niques."
"The goal here is to query one’s personal voicemail for items of information, without hav-ing to listen to the entire message."
"For instance, “who called today?”, or “what is X’s phone num-ber?”."
"Because of the importance of these key pieces of information, in this paper, we focus pre-cisely on extracting the identity and the phone number of the caller."
"Other attempts at sum-marizing voicemail have been made in the past[REF_CITE], however the goal there was to compress a voicemail message by summarizing it, and not to extract the answers to specific questions."
"An interesting aspect of this research is that be-cause a transcription of the voicemail is not avail-able, speech recognition algorithms have to be used to convert the speech to text and the sub-sequent IE algorithms must operate on the tran-scription."
One of the complications that we have to deal with is the fact that the state-of-the-art ac-curacy of speech recognition algorithms on this type of data 1 is only in the neighborhood of 60- 70%[REF_CITE].
The task that is most similar to our work is named entity extraction from speech data[REF_CITE].
"Although the goal of the named entity task is similar - to identify the names of per-sons, locations, organizations, and temporal and numeric expressions - our task is different, and in some ways more difficult."
"There are two main reasons for this: first, caller and number informa-tion constitute a small fraction of all named enti-ties."
"Not all person-names belong to callers, and not all digit strings specify phone-numbers."
"In this sense, the algorithms we use must be more precise than those for named entity detection."
"Second, the caller’s identity may include infor-mation that is not typically found in a named en-tity, for example, “Joe on the third floor”, rather than simply “Joe”."
We discuss our definitions of “caller” and “number” in Section 2.
"To extract caller information from transcribed speech text, we implemented three different sys-tems, spanning both statistical and non-statistical approaches."
We evaluate these systems on man-ual voicemail transcriptions as well as the out-put of a speech recognizer.
The first system is a simple rule-based system that uses trigger phrases to identify the information-bearing words.
"The second system is a maximum entropy model that tags the words in the transcription as belong-ing to one of the categories, “caller’s identity”, “phone number” or “other”."
The third system is a novel technique based on automatic stochastic-transducer induction.
It aims to learn rules auto-matically from training data instead of requiring hand-crafted rules from experts.
"Although the re-sults with this system are not yet as good as the other two, we consider it highly interesting be-cause the technology is new and still open to sig-nificant advances."
The rest of the paper is organized as follows: Section 2 describes the database we are using; Section 3 contains a description of the baseline system; Section 4 describes the maximum en-tropy model and the associated features; Section 5 discusses the transducer induction technique; Section 6 contains our experimental results and Section 7 concludes our discussions.
"Our work focuses on a database of voicemail mes-sages gathered at IBM, and made publicly avail-able through the LDC."
"This database and related speech recognition work is described fully by (Huang et  al., 2000)."
"We worked with approx-imately   messages, which  we divided into messages for  training, for develop-ment test set, and for evaluation test set."
"The messages were manually transcribed [Footnote_2] , and then a human tagger identified the portions of each message that specified the caller and any return numbers that were left."
2 The manual transcription has a word error rate
"In this work, we take a broad view of what constitutes a caller or num-ber."
The caller was defined to be the consecutive sequence of words that best answered the ques-tion “who called?”.
The definition of a number we used is a sequence of consecutive words that enables a return call to be placed.
"Thus, for ex-ample, a caller might be “Angela from P.C. Labs,” or “Peggy Cole Reed Balla’s secretary”."
"Simi-larly, a number may not be a digit string, for ex-ample: “tieline eight oh five six,” or “pager one three five”."
"No more than one caller was identi-fied for a single message, though there could be multiple numbers."
The training of the maximum entropy model and statistical transducer are done on these annotated scripts.
"In voicemail messages, people often identify themselves and give their phone numbers in highly stereotyped ways."
"So for example, some-one might say, “Hi Joe it’s Harry...” or “Give me a call back at extension one one eight four.”"
Our baseline system takes advantage of this fact by enumerating a set of transduction rules - in the form of a flex program - that transduce out the key information in a call.
The baseline system is built around the notion of “trigger phrases”.
These hand-crafted phases are patterns that are used in the flex program to recognize caller’s identity and phone numbers.
"Examples of trigger phrases are “Hi this is”, and “Give me a call back at”."
"In order to identify names and phone numbers as generally as pos-sible, our baseline system has defined classes for person-names and numbers."
"In addition to trigger phrases, “trigger suf-fixes” proved to be useful for identifying phone numbers."
"For example, the phrase “thanks bye” frequently occurs immediately after the caller’s phone number."
"In general, a random sequence of digits cannot be labeled as a phone number; but, a sequence of digits followed by “thanks bye” is almost certainly the caller’s phone number."
"So when the flex program matches a sequence of dig-its, it stores it; then it tries to match a trigger suf-fix."
"If this is successful, the digit string is recog-nized a phone number string."
Otherwise the digit string is ignored.
Our baseline system has about 200 rules.
"Its creation was aided by an automatically generated list of short, commonly occurring phrases that were then manually scanned, generalized, and added to the flex program."
"It is the simplest of the systems presented, and achieves a good per-formance level, but suffers from the fact that a skilled person is required to identify the rules."
Maximum entropy modeling is a powerful frame-work for constructing statistical models from data.
"It has been used in a variety of difficult classification tasks such as part-of-speech tagging[REF_CITE], prepositional phrase attach-ment[REF_CITE]and named en-tity tagging[REF_CITE], and achieves state of the art performance."
"In the following, we briefly describe the application of these models to extracting caller’s information from voicemail messages."
"The problem of extracting the information per-taining to the callers identity and phone number can be thought of as a tagging problem, where the tags are “caller’s identity,” “caller’s phone num-ber” and “other.”"
The objective is to tag each word in a message into one of these categories.
The information that can be used to predict a word’s tag is the identity of the surrounding words and their associated tags.
"Let denote the set of possible word and tag contexts, called “histo- ries”, and denote the set of tags."
"The maxent model is then defined over  ,and predicts the conditional probability   for a tag given the history ."
The computation of this probabil-ity depends on a set of binary-valued “features” &quot;!# .
"Given some training data and a set of features the maximum entropy estimation procedure com-  putes a weight parameter $ for every feature and parameterizes % &amp; as follows:  (&apos; ) $ 5 ,* +.-0/ 1 243 5 where is a normalization constant."
"The role of the features is to identify charac-teristics in the histories that are strong predictors of specific tags. (for example, the tag “caller” is very often preceded by the word sequence “this is”)."
"If a feature is a very strong predictor of a particular tag, then the corresponding $ would be high."
"It is also possible that a particular fea-ture may be a strong predictor of the absence of a particular tag, in which case the associated $ would be near zero."
Training a maximum entropy model involves the selection of the features and the subsequent estimation of weight parameters $ .
The testing procedure involves a search to enumerate the can-didate tag sequences for a message and choos-ing the one with highest probability.
We use the “beam search” technique[REF_CITE]to search the space of all hypotheses.
Designing effective features is crucial to the max-ent model.
"In the following sections, we de-scribe the various feature functions that we ex-perimented with."
We first preprocess the text in the following ways: (1) map rare words (with counts less than 6 ) to the symbol “UNKNOWN”; (2) map words in a name dictionary to the sym-bol “NAME.”
The first step is a way to handle out of vocabulary words in test data; the second step takes advantage of known names.
This mapping makes the model focus on learning features which help to predict the location of the caller identity and leave the actual specific names later for ex-traction.
"To compute unigram lexical features, we used the neighboring two words, and the tags associ-ated with the previous two words to define the history 798 as"
The features are generated by scanning each pair K 798 ? #I .8
L in the training data with feature tem-plate in Table 1.
"Note that although the window is two words on either side, the features are defined in terms of the value of a single word."
"The trigger phrases used in the rule-based ap-proach generally consist of several words, and turn out to be good predictors of the tags."
"In order to incorporate this information in the maximum entropy framework, we decided to use ngrams that occur in the surrounding word context to gen-erate features."
"Due to data sparsity and computa-tional cost, we restricted ourselves to using only bigrams."
The bigram feature template is shown in Table 2.
"First, a number dictionary is used to scan the training data and generate a code for each word which represents “number” or “other”."
"Sec-ond, a multi-word dictionary is used to match known pre-caller trigger prefixes and after-phone-number trigger suffixes."
The same code is as-signed to each word in the matched string as ei-ther “pre-caller” or “after-phone-number”.
The combined stream of codes is added to the history 798 and used to generate features the same way the word sequence are used to generate lexical fea-tures.
"In general, the feature templates define a very large number of features, and some method is needed to select only the most important ones."
A simple way of doing this is to discard the fea-tures that are rarely seen in the data.
Discard-ing all features with fewer than %[ \ occurrences resulted in about %[ \ ? \ \ \ features.
We also ex-perimented with a more sophisticated incremen-tal scheme.
This procedure starts with no features and a uniform distribution ]
"I%^ 7&amp;L , and sequen-tially adds the features that most increase the data likelihood."
The procedure stops when the gain in likelihood on a cross-validation set becomes small.
"Our baseline system is essentially a hand speci-fied transducer, and in this section, we describe how such an item can be automatically induced from labeled training data."
"The overall goal is to take a set of labeled training examples in which the caller and number information has been tagged, and to learn a transducer such that when voicemail messages are used as input, the trans-ducer emits only the information-bearing words."
"First we will present a brief description of how an automaton structure for voicemail messages can be learned from examples, and then we describe how to convert this to an appropriate transducer structure."
"Finally, we extend this process so that the training procedure acts hierarchically on dif-ferent portions of the messages at different times."
"In contrast to the baseline flex system, the trans-ducers that we induce are nondeterministic and stochastic – a given word sequence may align to multiple paths through the transducer."
"In the case that multiple alignments are possible, the lowest cost transduction is preferred, with the costs being determined by the transition probabilities encoun-tered along the paths."
"Many techniques have evolved for inducing finite state automata from word sequences, e.g.[REF_CITE], and we chose to adapt the tech-nique[REF_CITE]."
"This is a simple method for inducing acyclic automata, and is at-tractive because of its simplicity and theoretical guarantees."
"Here we present only an abbreviated description of our implementation, and refer the reader[REF_CITE]for a full description of the original algorithm."
"In[REF_CITE], finite state transducers were also used for named entity extraction, but they were hand spec-ified."
"The basic idea of the structure induction algo-rithm is to start with a prefix tree, where arcs are labeled with words, that exactly represents all the word sequences in the training data, and then to gradually transform it, by merging internal states, into a directed acyclic graph that represents a gen-eralization of the training data."
An example of a merge operation is shown in Figure 1.
"The decision to merge two nodes is based on the fact that a set of strings is rooted in each node of the tree, specified by the paths to all the reach-able leaf nodes."
A merge of two nodes is permis-sible when the corresponding sets of strings are statistically indistinguishable from one another.
"The precise definition of statistical similarity can be found[REF_CITE], and amounts to deeming two nodes indistinguishable unless one of them has a frequently occurring suffix that is rarely seen in the other."
The exact ordering in which we merged nodes is a variant of the process described[REF_CITE][Footnote_3] .
"3 A frontier of nodes is maintained, and is initialized to the children of the root. The weight of a node is defined as the number of strings rooted in it. At each step, the heaviest node is removed, and an attempt is made to merge it with an-other fronteir node, in order of decreasing weight. If a merge is possible, the result is placed on the frontier; otherwise, the heaviest node’s children are added."
"The transition probabilities are determined by aligning the train-ing data to the induced automaton, and counting the number of times each arc is used."
"Once a structure is induced for the training data, it can be converted into an information extract-ing transducer in a straightforward manner."
"When the automaton is learned, we keep track of which words were found in information-bearing por-tions of the call, and which were not."
"The struc-ture of the transducer is identical to that of the au-tomaton, but each arc makes a transduction."
"If the arc is labeled with a word that was information-bearing in the training data, then the word itself is transduced out; otherwise, an _ epsilon ` is trans-duced."
"Conceptually, it is possible to induce a structure for voicemail messages in one step, using the al-gorithm described in the previous sections."
"In practice, we have found that this is a very diffi-cult problem, and that it is expedient to break it into a number of simpler sub-problems."
This has led us to develop a three-step induction process in which only short segments of text are processed at once.
"First, all the examples of phone numbers are gathered together, and a structure is induced."
"Similarly, all the examples of caller’s identities are collected, and a structure is induced for them To further simplify the task, we replaced number strings by the single symbol “NUMBER+”, and person-names by the symbol “PERSON-NAME”."
"The transition costs for these structures are esti-mated by aligning the training data, and counting the number of times the different transitions out of each state are taken."
A phone number structure induced in this way from a subset of the data is shown at the top of Figure 2.
"In the second step, occurrences of names and numbers are replaced by single symbols, and the segments of text immediately surrounding them are extracted."
This results in a database of ex-amples like “Hi PERSON-NAME it’s CALLER-STRUCTURE
"I wanted to ask you”, or “call me at NUMBER-STRUCTURE thanks bye”."
"In this example, the three words immediately preced-ing and following the number or caller are used."
"Using this database, a structure is induced for these segments of text, and the result is essen-tially an induced automaton that represents the trigger phrases that were manually identified in the baseline system."
A small second level struc-ture is shown at the bottom of Figure 2.
"In the third step, the structure of a background language model is induced."
"The structures dis-covered in these three steps are then combined into a single large automaton that allows any se-quence of caller, number, and background seg- ments."
"For the system we used in our experi-ments, we used a unigram language model as the background."
"In the case that information-bearing patterns exist in the input, it is desirable for paths through the non-background portions of the final automaton to have a lower cost, and this is most likely with a high perplexity background model."
"To evaluate the performance of different systems, we use the conventional precision, recall and their F-measure."
"Significantly, we insist on exact matches for an answer to be counted as correct."
"The reason for this is that any error is liable to ren-der the information useless, or detrimental."
"For example, an incorrect phone number can result in unwanted phone charges, and unpleasant conver-sations."
"This is different from typical named en-tity evaluation, where partial matches are given partial credit."
"Therefore, it should be understood that the precision and recall rates computed with this strict criterion cannot be compared to those from named entity detection tasks."
A summary of our results is presented in Tables 3 and 4.
Table 3 presents precision and recall rates when manual word transcriptions are used; Table 4 presents these numbers when speech recogni-tion transcripts are used.
"On the heading line, P refers to precision, R to recall, F to F-measure, C to caller-identity, and N to phone number."
Thus P/C denotes “precision on caller identity”.
"In these tables, the maximum entropy model is referred to as ME."
ME1-U uses unigram lex-ical features only; ME1-B uses bigram lexical features only.
"ME1-B performs somewhat better than ME1-U, but uses more than double number of features."
ME2-U-f1 uses unigram lexical features and number dictionary features.
"It improves the recall of phone number by aXbdcfe upon ME1-U. ME2- U-f12 adds the trigger phrase dictionary features to ME2-U-f1, and it improves the recall of caller and phone numbers but degrades on the preci-sion of both."
Overall it improves a little on the F-meansures.
"ME2-B-f12 uses bigram lexical features, number dictionary features and trigger phrase dictionary features."
"It has the best recall of caller, again with over two times number of fea-tures of ME2-U-f12."
The above variants of ME features are chosen using simple count cutoff method.
"When the in-cremental feature selection is used, ME2-U-f12-I reduces the number of features from gfhFaih to gkj%l with minor performance loss; ME2-B-f12-I re- duces the number of features from   to  with minor performance loss."
This shows that the main power of the maxent model comes from a a very small subset of the possible features.
"Thus, if memory and speed are concerned, the incremen-tal feature selection is highly recommended."
There are several observations that can be made from these results.
"First, the maximum en-tropy approach systematically beats the baseline in terms of precision, and secondly it is better on recall of the caller’s identity."
We believe this is because the baseline has an imperfect set of rules for determining the end of a “caller identity” de-scription.
"On the other hand, the baseline system has higher recall for phone numbers."
"The results of structure induction are worse than the other two methods, however as this is a novel approach in a developmental stage, we expect the performance will improve in the future."
Another important point is that there is a signif-icant difference in performance between manual and decoded transcriptions.
"As expected, the pre-cision and recall numbers are worse in the pres-ence of transcription errors (the recognizer had a word error rate of about 35%)."
The degradation due to transcription errors could be caused by ei-ther: (i) corruption of words in the context sur-rounding the names and numbers; or (ii) corrup-tion of the information itself.
"To investigate this, we did the following experiment: we replaced the regions of decoded text that correspond to the cor-rect caller identity and phone number with the correct manual transcription, and redid the test."
The results are shown in Table 5.
"Compared to the results on the manual transcription, the recall numbers for the maximum-entropy tagger are just slightly ( o&amp;prqfs ) worse, and precision is still high."
This indicates that the corruption of the informa-tion content due to transcription errors is much more important than the corruption of the context.
"If measured by the string error rate, none of our systems can be used to extract exact caller and phone number information directly from de-coded voicemail."
"However, they can be used to locate the information in the message and high-light those positions."
"To evaluate the effective-ness of this approach, we computed precision and recall numbers in terms of the temporal overlap of the identified and true information bearing seg-ments."
"Table 6 shows that the temporal loca-tion of phone numbers can be reliably determined, with an F-measure of 80%."
"In this paper, we have developed several tech-niques for extracting key pieces of information from voicemail messages."
"In contrast to tradi-tional named entity tasks, we are interested in identifying just a selected subset of the named entities that occur."
We implemented and tested three methods on manual transcriptions and tran-scriptions generated by a speech recognition sys-tem.
"For a baseline, we used a flex program with a set of hand-specified information extraction rules."
"Two statistical systems are compared to the base-line, one based on maximum entropy modeling, and the other on transducer induction."
"Both the baseline and the maximum entropy model per-formed well on manually transcribed messages, while the structure induction still needs improve-ment."
"Although performance degrades signifi-cantly in the presence of speech racognition er-rors, it is still possible to reliably determine the sound segments corresponding to phone num-bers."
"It is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources, and that standards for linguistic annotation are becoming increasingly mandatory."
"To answer this need, we have developed a representation framework comprised of an abstract model for a variety of different annotation types (e.g., morpho-syntactic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotators approach and goals."
In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation.
We show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes.
"It is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources, and that standards for linguistic annotation are becoming increasingly mandatory."
"In particular, there is a need for a general framework for linguistic annotation that is flexible and extensible enough to accommodate different annotation types and different theoretical and practical approaches, while at the same time enabling their representation in a pivot format that can serve as the basis for comparative evaluation of parser output, such as PARSEVAL[REF_CITE], as well as the development of reusable editing and processing tools."
"To answer this need, we have developed a representation framework comprised of an abstract model for a variety of different annotation types (e.g., morpho-syntactic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotators approach and goals."
"We have implemented both the abstract model and various instantiations using XML schem[REF_CITE], the Resource Definition Framework (RDF)[REF_CITE]and RDF schem[REF_CITE], which enable description and definition of abstract data models together with means to interpret, via the model, information encoded according to different conventions."
"The results have been incorporated into XCES[REF_CITE], part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES) [Footnote_1] ."
"The XCES provides a ready-made, standard encoding format together with a data architecture designed specifically for linguistically annotated corpora."
In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation.
"The framework has been applied to the representation of terminology (Terminological Markup Framework 2 , ISO project n.16642) and computational lexicons[REF_CITE], thus demonstrating its general applicability for a variety of linguistic annotation types."
We also show how the framework can contribute to comparison and merging of diverse syntactic annotation schemes.
"At the highest level of abstraction, syntactic annotation schemes represent the following kinds of information: • Category information: labeling of components based on syntactic category (e.g., noun phrase, prepositional phrase), syntactic role (subject, object), etc.; • Dependency information: relations among components, including constituency relations, grammatical role relations, etc."
"For example, the annotation in Figure 1, drawn from the Penn Treebank II [URL_CITE] (hereafter, PTB ), uses LISP-like list structures to specify constituency relations and provide syntactic category labels for constituents."
"Some grammatical roles (subject, object, etc.) are implicit in the structure of the encoding: for instance, the nesting of the NP the front room implies that the NP is the object of the prepositional phrase, whereas the position of the NP him following and at the same level as the VP node implies that this NP is the grammatical object."
Additional processing (or human intervention) is required to render these relations explicit.
"Note that the PTB encoding provides some explicit information about grammatical role, in that subject is explicitly labeled (although its relation to the verb remains implicit in the structure), but most relations (e.g., object) are left implicit."
"Relations among non-contiguous elements demand a special numbering mechanism to enable cross-reference, as in the specification of the NP-SBJ of the embedded sentence by reference to the earlier NP-SBJ-1 node."
"Although they differ in the labels and in some cases the function of various nodes in the tree, most annotation schemes provide a similar constituency-based representation of relations among syntactic components (see Abeille, forthcoming, for a comprehensive survey of syntactic annotation schemes)."
"In contrast, dependency schemes (e.g.,[REF_CITE]; Carroll, et al., forthcoming) do not provide a constituency analysis [Footnote_4] but rather specify grammatical relations among elements explicitly; for example, the sentence Paul intends to leave IBM could be represented as shown in Figure 2, where the predicate is the relation type, the first argument is the head, the second the dependent, and additional arguments may provide category-specific information (e.g., introducer for prepositional phrases, etc.)."
"4 So-called hybrid systems (e.g.,[REF_CITE]) combine constituency analysis and functional dependencies, usually producing a shallow constituent parse that brackets major phrase types and identifying the dependencies between heads of constituents."
The goal in the XCES is to provide a framework for annotation that is theory and tagset independent.
We accomplish this by treating the description of any specific syntactic annotation scheme as a process involving several knowledge sources that interact at various levels.
"The process allows one to specify, on the one hand, the informational properties of the scheme (i.e., its capacity to represent a given piece of information), and, on the other, the way the scheme can be instantiated (e.g., as an XML document)."
Figure 3 shows the overall architecture of the XCES framework for syntactic annotation.
Two knowledge sources are used define the abstract model:
Data Category Registry:
"Within the framework of the XCES we are establishing an inventory of data categories for syntactic annotation, initially based on the EAGLES Recommendations for Syntactic Annotation of Corpora[REF_CITE]."
Data categories are defined using RDF descriptions that formalize the properties associated with each.
"The categories are organized in a hierarchy, from general to specific."
"For example, a general dependent relation may be defined, which may have one of the possible values argument or modifier; argument in turn may have the possible values subject, object, or complement; etc. [Footnote_5] Note that RDF descriptions function much like class definitions in an object-oriented programming language: they provide, effectively, templates that describe how objects may be instantiated, but do not constitute the objects themselves."
"5 Cf. the hierarchy in Figure 1.1, Caroll, Minnen, and Briscoe (forthcoming)."
"Thus, in a document containing an actual annotation, several objects with the type argument may be instantiated, each with a different value."
The RDF schema ensures that each instantiation of argument is recognized as a sub-class of dependent and inherits the appropriate properties.
"Structural Skeleton: a domain-dependent abstract structural framework for syntactic annotations, capable of fully capturing all the information in a specific annotation scheme."
The structural skeleton for syntactic annotations is described below in section 12.1.
"Two other knowledge sources are used to define a project-specific format for the annotation scheme, in terms of its expressive power and its instantiation in XML: Data Category Specification (DCS): describes the set of data categories that can be used within a given annotation scheme, again using RDF schema."
"The DCS defines constraints on each category, including restrictions on the values they can take (e.g., &quot;text with markup&quot;; a &quot;picklist&quot; for grammatical gender, or any of the data types defined for XML), restrictions on where a particular data category can appear (level in the structural hierarchy)."
The DCS may include a subset of categories from the DCR together with application-specific categories additionally defined in the DCS.
The DCS also indicates a level of granularity based on the DCR hierarchy.
"Dialect specification: defines, using XML schemas, XSLT scripts, and XSL style sheets, the project-specific XML format for syntactic annotations."
The specifications may include: • Data category instantiation styles: Data categories may be realized in a project-specific scheme in any of a variety of formats.
"For example, if there exists a data category NounPhrase, this may be realized as an &lt;NounPhrase&gt; element (possibly containing additional elements), a typed element (e.g. &lt;cat type=NounPhrase&gt;), tag content (e.g., &lt;cat&gt;NounPhrase&lt;/cat&gt;), etc. • Data category vocabulary styles: Project-specific formats can utilize names different from those in the Data Category Registry; for instance, a DCR specification for NounPhrase can be expressed as NP or SN ( syntagme nominal) in the project-specific format, if desired. • Expansion structures: A project-specific format may alter the structure of the annotation as expressed using the structural skeleton."
"For example, it may be desirable for processing or other reasons to create two sub-nodes under a given &lt;struct&gt; node, one to group features and one to group relations."
The combination of the structural skeleton and the DCS defines a virtual annotation markup language (AML).
"Any information structure that corresponds to a virtual AML has a canonical expression as an XML document; therefore, the inter-operability of different AMLs is dependent only on their compatibility at the virtual level."
"As such, virtual AML is the hub of the annotation framework: it defines a lingua franca for syntactic annotations that can be used to compare and merge annotations, as well as enable design of generic tools for visualization, editing, extraction, etc."
"The combination of a virtual AML with the Dialect Specification provides the information necessary to automatically generate a concrete AML representation of the annotation scheme, which conforms to the project-specific format provided in the Dialect Specification."
"XSLT filters translate between the representations of the annotation in concrete and virtual AML, as well as between non-XML formats (such as the LISP-like PTB notation) and concrete AML. [Footnote_6]"
"6 Strictly speaking, an application-specific format could be translated directly into the virtual AML, eliminating the need for the intermediary concrete AML format. However, especially for existing formats, it is typically more straightforward to perform the two-step process."
"For syntactic annotation, we can identify a general, underlying model that informs current practice: specification of constituency relations (with some set of application-specific names and properties) among syntactic or grammatical components (also with a set of application-specific names and properties), whether this is modeled with a tree structure or the relations are given explicitly."
"Because of the common use of trees in syntactic annotation, together with the natural tree-structure of markup in XML documents, we provide a structural skeleton for syntactic markup following this model."
"The most important element in the skeleton is the &lt;struct&gt; element, which represents a node (level) in the syntax tree. & lt;struct&gt; elements may be recursively nested at any level to reflect the structure of the corresponding tree."
"The &lt;struct&gt; element has the following attributes: • type : specifies the node label (e.g., S, NP, etc.) or points to an object in another document that provides the value."
This allows specifying complex data items as annotations.
It also enables generating a single instantiation of an annotation value in a separate document that can be referenced as needed. • xlink : points to the data to which the annotation applies.
"In the XCES, we recommend the use of s t a n d - o f f a n n o t a t i o n i.e., annotation that is maintained in a document separate from the primary (annotated) data. [Footnote_7] The xlink attribute uses the XML Path Language (XPath) (Clark &amp;[REF_CITE]) to specify the location of the relevant data in the primary document. • ref : refers to a node defined elsewhere, used instead of xlink. • rel˚: specifies a type of relation (e.g., subj) • head : specifies the node corresponding to the head of the relation • dependent : specifies the node corresponding to the dependent of the relation • introducer : specifies the node corresponding to an introducing word or phrase • initial : gives a thematic or semantic role of a component, e.g., subj for the object of a by-phrase in a passive sentence."
"7 The stand-off scheme also provides means to represent ambiguities, since there can be multiple links between data and alternative annotations."
The hierarchy of &lt;struct&gt; elements corresponds to the nodes in a phrase structure analysis; each &lt;struct&gt; element is typed accordingly.
"The grammar underlying the annotation therefore specifies constraints on embedding that can be instantiated in an XML schema, which can then be used to prevent or detect tree structures that do not conform to the grammar."
"Conversely, the grammar rules implicit in annotated treebanks, which are typically not annotated according to a formal grammar, can be easily extracted from the abstract structural encoding."
"The skeleton also includes a &lt;feat&gt; (feature) element, which can be used to provide additional information (e.g., gender, number) that is attached to the node in the tree represented by the enclosing &lt;struct&gt; element."
"Like &lt;struct&gt;, this element can be recursively nested or can point to a description in another document, thereby providing means to associate information at any level of detail or complexity to the annotated structure."
Figure 4 shows the annotation from the PTB (Figure 1) rendered in the abstract XML format.
"Note that in this example, relations are encoded only when they appear explicitly in the original annotation (therefore, heads of relations default to unknown.)"
"An XSLT script could be used to create a second XML document that includes the relations implicit in the embedding (e.g., the first embedded &lt;struct&gt; with category NP has relation subject, the first VP is the head, etc.)."
"A strict dependency annotation encoded in the abstract format uses a flat hierarchy and specifies all relations explicitly with the rel attribute, as shown in Figure 5. [Footnote_8]"
"8 For the sake of readability, this encoding assumes that the sentence Paul intends to leave IBM is marked up as &lt;s1&gt;&lt;w1&gt;Paul&lt;/w1&gt;&lt;w2&gt;intends&lt;/w2&gt;&lt;w3&gt;to&lt;/w3&gt;&lt;w 4&gt;leave&lt;/w4&gt;&lt;w5&gt;IBM&lt;/w5&gt;&lt;/s1&gt;."
"The Virtual AML provides a pivot format that enables comparison of annotations in different formats including not only different constituency-based annotations, but also constituency-based and dependency annotations."
"For example, the PTB annotation corresponding to the dependency annotation in Figure 2 is shown in Figure 6."
Figure 7 gives the corresponding encoding in the XCES abstract scheme.
It is relatively trivial with an XSLT script to extract the information in the dependency annotation (Figure 5) from the PTB encoding (Figure 7) to produce a nearly identical dependency encoding.
"The script would use rules to make relations that are implicit in the structure of the PTB encoding explicit (for example, the xcomp relation that is implicit in the embedding of the S phrase)."
The ability to generate a common representation for different annotations overcomes several obstacles that have hindered evaluation exercises in the past.
"For instance, the evaluation technique used in the PARSEVAL exercise is applicable to phrase structure analyses only, and cannot be applied to dependency-style analyses or lexical parsing frameworks such as finite-state constraint parsers."
"As the example above shows, this problem can be addressed using the XCES framework."
"It has also been noted that that the PARSEVAL bracket-precision measure penalizes parsers that return more structure than exists in the relatively flat treebank structures, even if they are correct[REF_CITE]."
"XSLT scripts can extract the appropriate information for comparison purposes while retaining links to additional parts of the annotation in the original document, thus eliminating the need to dumb down parser output in order to participate in the evaluation exercise."
"Similarly, information lost in the transduction from phrase structure to a dependency-based analysis (as in the example above), which,[REF_CITE]points out, may eliminate grammatical information potentially required for later processing, can also be retained."
"Despite its seeming complexity, the XCES framework is designed to reduce overhead for annotators and users."
"Part of the work of the XCES is to provide XML support (e.g., development of XSLT scripts, XML schemas, etc.) for use by the research community, thus eliminating the need for XML expertise at each development site."
"Because XML-encoded annotated corpora are increasingly used for interchange between processing and analytic tools, we are developing XSLT scripts for mapping, and extraction of annotated data, import/export of (partially) annotated material, and integration of results of external tools into existing annotated data in XML."
"Tools for editing annotations in the abstract format, which automatically generate virtual AML from Data Category and Dialect Specifications, are already under development in the context of work on the Terminological Markup Language, and a tool for automatically generating RDF specifications for user-specified data categories has already been developed in the SALT project. [URL_CITE] Several freely distributed interpreters for XSLT have also been developed (e.g., xt [Footnote_10] ,[REF_CITE])."
"10 Clark, J., 1999.[REF_CITE]105.[URL_CITE]"
"In practice, annotators and users of annotated corpora will rarely see XML and RDF instantiations of annotated data; rather, they will access the data via interfaces that automatically generate, interpret, and display the data in easy-to-read formats."
The abstract model that captures the fundamental properties of syntactic annotation schemes provides a conceptual tool for assessing the coherence and consistency of existing schemes and those being developed.
"The model enforces clear distinctions between implicit and explicit information (e.g., functional relations implied by structural relations in constituent analyses), and phrasal and functional relations."
"It is alarmingly common for annotation schemes to represent these different kinds of information in the same way, rendering their distinction computationally intractable (even if they are perfectly understandable by the informed human reader)."
"Hand-developed annotation schemes used in treebanks are often described informally in guidebooks for annotators, leaving considerable room for variation; for example,[REF_CITE]notes that the PTB implicitly contains more than 10,000 context-free rules, most of which are used only once."
Comparison and transduction of schemes becomes virtually impossible under such circumstances.
"While requiring that annotators make relations explicit and consider the mapping to the XCES abstract format increases overhead, we feel that the exercise will help avoid such problems and can only lead to greater coherence, consistency, and inter-operability among annotation schemes."
The most important contribution to inter-operability of annotation schemes is the Data Category Registry.
"By mapping site-specific categories onto definitions in the Registry, equivalences (and non-equivalences) are made explicit."
"Again, the provision of a standard set of categories, together with the requirement that scheme-specific categories are mapped to them where possible, will contribute to greater consistency and commonality among annotation schemes."
The XCES framework for linguistic annotation is built around some relatively straightforward ideas: separation of information conveyed by means of structure and information conveyed directly by specification of content categories; development of an abstract format that puts a layer of abstraction between site-specific annotation schemes and standard specifications; and creation of a Data Category Registry to provide a reference set of annotation categories.
The emergence of XML and related standards such as RDF provides the enabling technology.
"We are, therefore, at a point where the creation and use of annotated data and concerns about the way it is represented can be treated separatelythat is, researchers can focus on the question of what to encode, independent of the question of how to encode it."
"The end result should be greater coherence, consistency, and ease of use and access for annotated data."
"Named entity (NE) recognition is a task in which proper nouns and nu-merical information in a document are detected and classified into categories such as person, organization, location, and date."
NE recognition plays an es-sential role in information extraction systems and question answering sys-tems.
"It is well known that hand-crafted systems with a large set of heuris-tic rules are difficult to maintain, and corpus-based statistical approaches are expected to be more robust and require less human intervention."
Several statis-tical approaches have been reported in the literature.
"In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree sys-tems and most hand-crafted systems."
"Here, we propose an alternative method based on a simple rule generator and decision tree learning."
Our exper-iments show that its performance is comparable to the ME approach.
We also found that it can be trained more efficiently with a large set of training data and that it improves readability.
"Named entity (NE) recognition is a task in which proper nouns and numerical informa-tion in a document are detected and classi- fied into categories such as person, organiza-tion, location, and date."
"NE recognition plays an essential role in information extraction sys-tems (see[REF_CITE]) and ques-tion answering systems (see TREC-QA docu-ments,[URL_CITE]"
"When you want to know the location of the Taj Ma-hal, traditional IR techniques direct you to rele-vant documents but do not directly answer your question."
NE recognition is essential for finding possible answers from documents.
"Although it is easy to build an NE recognition system with mediocre performance, it is difficult to make it re-liable because of the large number of ambiguous cases."
"For instance, we cannot determine whether “Washington” is a person’s name or a location’s name without the necessary context."
There are two major approaches to building NE recognition systems.
The first approach employs hand-crafted rules.
It is well known that hand-crafted systems are difficult to maintain because it is not easy to predict the effect of a small change in a rule.
"The second approach employs a statis-tical method, which is expected to be more robust and to require less human intervention."
Several statistical methods have been reported in the liter-ature[REF_CITE].
"IREX (Information Retrieval and Extraction Exercise,[REF_CITE]) was held in 1999, and fifteen systems par-ticipated in the formal run of the Japanese NE ex-cercise."
"In the formal run, participants were re-quested to tag two data sets (GENERAL and AR-REST), and their scores were compared in terms of F-measure, i.e., the harmonic mean of ‘recall’ and ‘precision’ defined as follows. recall = x/(the number of correct NEs) precision = x/(the number of NEs extracted by the system) where x is the number of NEs correctly ex-tracted and classified by the system."
"GENERAL was the larger test set, and its best system was a hand-crafted one that at-tained F=83.86%."
The second best system (F=80.05%) was also hand-crafted but enhanced with transformation-based error-driven learning.
The third best system (F=77.37%) was Borth-wick’s ME system enhanced with hand-crafted rules and dictionaries (1999).
"Thus, the best three systems used quite different approaches."
"In this paper, we propose an alternative ap-proach based on a simple rule generator and de-cision tree learning (RG+DT)."
"Our experiments show that its performance is comparable to the ME method, and we found that it can be trained more efficiently with a large set of training data."
"By adding in-house data, the proposed system’s performance was improved by several points, while a standard ME toolkit crashed."
"When we try to extract NEs in Japanese, we encounter several problems that are not serious in English."
It is relatively easy to detect En-glish NEs because of capitalization.
"In Japanese, there is no such useful hint."
Proper nouns and common nouns look very similar.
"In English, it is also easy to tokenize a sentence because of inter-word spacing."
"In Japanese, inter-word spac-ing is rarely used."
"We can use an off-the-shelf morphological analyzer for tokenization, but its word boundaries may differ from the correspond-ing NE boundaries in the training data."
"For in-stance, a morphological analyzer may divide a four-character expression OO-SAKA-SHI-NAI into two words OO-SAKA (= Osaka) and SHI-NAI (= in the city), but the training data would be tagged as &lt;LOCATION&gt;OO-SAKA-SHI&lt;/LO-CATION&gt;NAI (= in &lt;LOCATION&gt;Osaka City &lt;/LOCATION&gt;)."
"Moreover, unknown words are often divided excessively or incorrectly because an analyzer tries to interpret a sentence as a se-quence of known words."
"Throughout this paper, the typewriter-style font is used for Japanese, and hyphens indicate char-acter boundaries."
"Different types of charac-ters are used in Japanese: hiragana, katakana, kanji, symbols, numbers, and letters of the Ro-man alphabet."
Our RG+DT system (Fig. 1) generates a recogni-tion rule from each NE in the training data.
"Then, the rule is refined by decision tree learning."
"By applying the refined recognition rules to a new document, we get NE candidates."
"Then, non-overlapping candidates are selected by a kind of longest match method."
"In our method, each tokenized NE is converted to a recognition rule that is essentially a sequence of part-of-speech (POS) tags in the NE."
"For in-stance, OO-SAKA-GIN-KOU (= Osaka Bank) is tokenized into two words: OO-SAKA:all-kanji:location-name (= Osaka) and GIN-KOU:all-kanji:common-noun (= Bank), where location-name and common-noun are POS tags."
"In this case, we get the following recognition rule."
"Here, ‘*’ matches anything. *:*:location-name, *:*:common-noun -&gt; ORGANIZATION"
"However, this rule is not very good."
"For in-stance, OO-SAKA-WAN (= Osaka Bay) follows this pattern, but it is a location’s name."
"GIN-KOU and WAN strongly imply ORGANIZATION and LOCATION, respectively."
"Thus, the last word of an NE is often a head that is more useful than other words for the classification."
"Therefore, we register the last word into a suffix dictionary for each non-numerical NE class (i.e., ORGANIZA-TION, PERSON, LOCATION, and ARTIFACT) in order to accept only reliable candidates."
"If the last word appears in two or more different NE, we call it a reliable NE suffix."
We register only reli-able ones.
"In the above examples, the last words were common nouns."
"However, the last word can also be a proper noun."
"For instance, we will get the following rule from &lt;ORGANIZATION&gt;OO-SAKA-TO-YO-TA&lt;/ORGANIZATION&gt; (= Os-aka Toyota) because Japanese POS taggers know that TO-YO-TA is an organization name (a kind of proper noun). *:*:location-name, *:*:org-name -&gt; ORGANIZATION,0,0"
"Since Yokohama Honda and Kyoto Sony also follow this pattern, the second element *:*:org-name should not be restricted to the words in the training data."
"Therefore, we do not restrict proper nouns by a suffix dictionary, and we do not restrict numbers either."
"In addition, the first or last word of an NE may contain an NE boundary as we described before (SHI&lt;/LOCATION&gt;NAI)."
"In this case, we can get OO-SAKA-SHI by removing no character of the first word OO-SAKA and one character of the last word SHI-NAI."
"Accordingly, this modifica-tion can be represented by two integers: 0,1."
"Furthermore, one-word NEs are different from other NEs in the following respects."
"The word is usually a proper noun, an un-known word, or a number; otherwise, it is an exceptional case."
The character type of a one-word NE gives a useful hint for its classification.
"For instance, all-uppercase words (e.g., IOC) are of-ten classified as ORGANIZATION."
"Since unknown words are often proper nouns, we assume they are tagged as misc-proper-noun."
"If the training data contains &lt;ORGANIZATION&gt;I-O-C&lt;/ORGANIZATION&gt; and I-O-C (= IOC) is an unknown word, we will get I-O-C:all-uppercase:misc-proper-noun."
"By considering these facts, we modify the above rule generation."
"That is, we replace every word in an NE and its character type by ‘*’ to get the left-hand side of the corresponding recogni-tion rule except the following cases."
"A word that contains an NE boundary If the first or last word of the NE contains an NE boundary (e.g, SHI&lt;/LOCATION&gt;NAI), the word is not replaced by ‘*’."
The number of characters to be deleted is also recorded in the right-hand side of the recognition rule.
The following exceptions are ap-plied to one-word NEs.
"If the word is a proper noun or a number, its character type is not replaced by ‘*’."
"Otherwise, the word is not replaced by ‘*’."
The last word of a longer NE The following exceptions are applied to the last word of a non-numerical NE that is composed of two or more words when the word is neither a proper noun nor a number.
"If the last word is a reliable NE suffix (i.e., it appears in two or more different NEs in the class), its information (i.e., the last word, its character type, and its POS tag) is registered into a suffix dictionary for the NE class."
The last word of the recognition rule must be an ele-ment of the suffix dictionary.
Unreliable NE suffixes are not replaced by ‘*’.
"Suffixes of numerical NEs (i.e., DATE, TIME, MONEY, PERCENT) are not replaced, either."
"Now, we obtain the following recognition rules from the above examples. *:*:location-name, *:*:common-noun -&gt; ORGANIZATION,0,0."
The first rule extracts CNN as an organization.
The second rule extracts YOKO-HAMA-SHI (= Yokohama City) from YOKO-HAMA-SHI-NAI (= in Yokohama City).
The third rule extracts YOKO-HAMA-GIN-KOU (= Yokohama Bank) as an organization.
"Note that, in this rule, the second element (*:*:common-noun) is constrained by the suffix dictionary for ORGANIZATION be-cause it is neither a proper noun nor a number."
"Hence, the rule does not match YOKO-HAMA-WAN (= Yokohama Bay)."
"If the suffix dictionary also happens to have KOU-KOU:all-kanji: commmon-noun (= senior high school), the rule also matches YOKO-HAMA-KOU-KOU (= Yoko-hama Senior High School)."
"IREX introduced &lt;ARTIFACT&gt; for product names, prizes, pacts, books, and fine arts, among other nouns."
Titles of books and fine arts are often long and have atypical word patterns.
"However, they are often delimited by a pair of symbols that correspond to quotation marks in English."
Some atypical organization names are also delimited by these symbols.
"In order to extract such a long NE, we concatenate all words within a pair of such symbols into one word."
We employ the first and last word of the quoted words as extra features.
"In addition, we do not regard the quotation symbols as adjacent words because they are constant and lack semantic meaning."
"When a large amount of training data is given, thousands of recognition rules are generated."
"For efficiency, we compile these recognition rules by using a hash table that converts a hash key into a list of relevant rules that have to be examined."
We make this hash table as follows.
"If the left-hand side of a rule contains only one element, the element is used as a hash key and its rule identi-fier is appended to the corresponding rule list."
"If the left-hand side contains two or more elements, the first two elements are concatenated and used as a hash key and its rule identifier is appended to the corresponding rule list."
"After this compila-tion, we can efficiently apply all of the rules to a new document."
"By taking the first two elements into consideration, we can reduce the number of rules that need to be examined."
Some recognition rules are not reliable.
"For in-stance, we get the following rule when a person’s name is incorrectly tagged as a location’s name by a POS tagger. *:all-kanji:location-name -&gt; PERSON,0,0"
"Therefore, we have to consider a way to refine the recognition rules."
"By applying each recognition rule to the un-tagged training data, we can obtain NE candidates for the rule."
"By comparing the candidates with the given answer for the training data, we can classify them into positive examples and negative exam-ples for the recognition rule."
"Consequently, we can apply decision tree learning to classify these examples correctly."
"We represent each example by a list of features: words in the NEs, pre-ceding words, succeeding words, their character types, and their POS tags."
"If we consider one pre-ceding word and two succeeding words, the fea-ture list for a two-word named entity (  ) will ,  , , , ,  ,  , be , , , , ,  ,  ,  ,  , where is the preceding and  are the succeeding words. word and  is  ’s character type and  is ’s POS tag.   is a boolean value that indicates whether it is a positive example."
"If a feature value appears less than three times in the examples, it is replaced by a dummy constant."
"We also replace numbers by dummy constants because most numerical NEs follow typical patterns, and their specific values are often useless for NE recognition."
"Here, we discuss handling short NEs."
"For example, NO-O-BE-RU-SHOU-SEN-KOU-I-IN-KAI (= the Nobel Prize Selection Com-mittee) is an organization’s name that contains a person’s name NO-O-BE-RU (= Nobel) and an artifact name NO-O-BE-RU-SHOU (= Nobel Prize), but &lt;PERSON&gt;NO-O-BE-RU&lt;/PER-SON&gt; and &lt;ARTIFACT&gt;NO-O-BE-RU-SHOU &lt;/ARTIFACT&gt; are incorrect in this case."
"If the training data contain NO-O-BE-RU as both pos-itive and negative examples of a person’s name, the decision tree learner will be confused."
They are rejected because there is a longer named entity and overlapping tags are not allowed.
We do not have to change our knowledge that Nobel is a per-son’s name.
"Therefore, we remove such negative examples caused by longer NEs."
"Consequently, the decision tree may fail to reject &lt;PERSON&gt; NO-O-BE-RU&lt;/PERSON&gt;, but it will disappear in the final output because we use a longest match method for arbitration."
"For readability, we translate each decision tree into a set of production rules by c4.5rules[REF_CITE]."
"Throughout this paper, we call them dt-rules (Fig. 1) in order to distinguish them from recognition rules."
"Thus, each recognition rule is enhanced by a set of dt-rules."
The dt-rules removes unlikely candidates.
"Once the refined rules are generated, we can ap-ply them to a new document."
This obtains a large number of NE candidates (Fig. 1).
"Since overlap-ping tags are not allowed, we use a kind of left-to-right longest match method."
"First, we compare their starting points and select the earliest ones."
"If two or more candidates start at the same point, their ending points are compared and the longest candidate is selected."
"Therefore, the candidates overlapping the selected candidate are removed from the candidate set."
This procedure is repeated until the candidate set becomes empty.
The rank of a candidate starting at the -th word boundary and ending at the -th word boundary can be represented by a pair  !&quot;$# .
"The beginning of a sentence is the zeroth word boundary, and the first word ends at the first word boundary, etc."
"Then, the selected candi-date should have the minimum rank according to the lexicographical ordering of %&amp;&quot;! $# ."
"When a candidate starts or ends within a word (e.g., SHI-NAI), we assume that the entire word is a member of the candidate for the definition of  !&quot;$# ."
"According to this ordering, two candidates can have the same rank."
One of them might assert that a certain word is an organization’s name and an-other candidate might assert that it is a person’s name.
"In order to apply the most frequently used rule, we extend this ordering by  &quot;! $&amp;!(&apos;*) # , where &apos;+) is the number of positive examples for the rule , ."
"In order to compare our method with the ME approach, we also implement an ME system based on Ristad’s toolkit (1997)."
Borthwick’s (1999) and Uchimoto’s (2000) ME systems are quite similar but differ in details.
They re-garded Japanese NE recognition as a classifica-tion problem of a word.
The first word of a per-son name is classified as PERSON - BEGIN .
The last word is classified as PERSON - END .
Other words in the person’s name (if any) are classi-fied as PERSON - MIDDLE .
"If the person’s name is composed of only one word, it is classified as PERSON - SINGLE ."
Similar labels are given to all other classes such as LOCATION.
Non-NE words are classified as OTHER .
"Thus, every word is classified into 33 classes, i.e., - ORGANIZATION , PERSON , LOCATION , ARTIFACT , DATE , TIME , MONEY , PERCENT 0. /1- BEGIN , MIDDLE , END , SINGLE .321- OTHER . ."
"For instance, the words in “President &lt;PERSON&gt; George Herbert Walker Bush &lt;/PERSON&gt;” are classified as follows: President = OTHER , George = PERSON - BEGIN , Herbert = PERSON - MIDDLE , Walker ="
"PERSON - MIDDLE , Bush = PERSON - END ."
"We use the following features for each word in the training data: the word itself, preceding words, succeeding words, their character types, and their POS tags."
"By following Uchimoto, we disregard words that appear fewer than five times and other features that appear fewer than three times."
"Then, the ME-based classifier gives a probabil-ity for each class to each word in a new sentence."
"Finally, the Viterbi algorithm (see textbooks, e.g.,[REF_CITE]) enhanced with consistency check-ing (e.g., PERSON - END should follow PERSON - BEGIN or PERSON - MIDDLE ) determines the best combination for the entire sentence."
We generate the word boundary rewriting rules as follows.
"First, the NE boundaries inside a word are assumed to be at the nearest word boundary outside the named entity."
"Hence, SHI&lt;/LOCATION&gt;NAI is rewritten as SHI-NAI&lt;/LOCATION&gt;."
"Accordingly, SHI-NAI is classified as LOCATION - END ."
"The original NE boundary is recorded for the pair SHI-NAI/ LOCATION - END , If SHI-NAI/ LOCATION - END is found in the output of the Viterbi algorithm, it is rewritten as SHI&lt;/LOCATION&gt;NAI."
"Since rewriting rules from rare cases can be harmful, we employ a rewriting rule only when the rule cor-rectly works for more than 50% of the word/class pairs in the training data."
"Now, we compare our method with the ME system."
We used the standard IREX training data (CRL NE 1.4[REF_CITE]KB) and the formal run test data (GENERAL and AR-REST).
"When human annotators were not sure, they used &lt;OPTIONAL POSSIBILITY=... &gt; where POSSIBILITY is a list of possible NE classes."
We also used 7.4 MB of in-house NE data that did not contain optional tags.
All of the training data (all = CRL NE+NERT+in-house) were based on the Mainichi Newspaper’s 1994 and 1995 CD-ROMs.
Table 1 shows the details.
"We removed an optional tag when its possibility list contains NONE, which means this part is ac-cepted without a tag."
"Otherwise, we selected the majority class in the list."
"As a result, 56 NEs were added to CRL NE."
"For tokenization, we used chasen 2.2.1[URL_CITE]chasen. aist-nara. ac. jp/)."
"It has about 90 POS tags and large proper noun dictionaries (persons = 32,167, organizations = 16,610, locations = 67,296, miscellaneous proper nouns = 26,106). (Large dictionaries sometimes make the extraction of NEs difficult."
"If OO-SAKA-GIN-KOU is registered as a single word, GIN-KOU is not extracted as an organization suffix from this example.)"
We tuned chasen’s parameters for NE recognition.
"In order to avoid the excessive division of unknown words (see Introduction), we reduced the cost for unknown words (30000 4 7000)."
We also changed its setting so that an unknown word are classified as a misc-proper-noun.
"Then, we compared the above methods in terms of the averaged F-measures by 5-fold cross-validation of CRL NE data."
The ME system at- # :;9 &amp;;9 # and 82.67% for tained 82.77% for &lt; &lt;#=# 6&gt; &lt; # .
"The RG+DT system attained 84.10% for , 84.02% for  # , and 84.03% for :&lt;;&amp;&lt;;# . (Even if we do not use C4.5, RG+[REF_CITE].18% for  &lt; # by removing bad tem-plates with fewer positive examples than negative ones.)"
"Thus, the two methods returned similar re-sults."
"However, we cannot expect good perfor-mance for other documents because CRL NE is limited[REF_CITE]."
Figure 2 compares these systems by using the formal run data.
We cannot show the ME re-sults for the large training data because Ristad’s toolkit crashes even on a 2 GB memory machine.
"According to this graph, the RG+DT system’s scores are comparable to those of the ME system."
"When all the training data was used, RG+DT’s F-measure[REF_CITE].43%."
We also examined RG+DT’s variants.
"When we replaced character types of one-word NEs by ‘*’, the score dropped to 86.79%."
"When we did not replace any character type by ‘*’ at all, the score was 86.63%."
RG+DT/n in the figure is a variant that also ap-plies suffix dictionary to numerical NE classes.
"When we used tokenized CRL NE for training, the RG+DT system’s training time was about 3 minutes on a[REF_CITE]MB mem-ory Linux machine."
"This performance is much faster than that of the ME system, which takes a few hours; this difference cannot be explained by the fact that the ME system is implemented on a slower machine."
"When we used all of the training data, the training time was less than one hour and the processing time of tokenized GENERAL (79 KB before tokenization) was about 14 seconds."
"Before the experiments, we did not expect that the RG+DT system would perform very well because the number of possible combinations of POS tags increases exponentially with respect to the num- ber of words in an NE."
"However, the above results are encouraging."
Its performance is comparable to the ME system.
Why did it work so well?
"First, the percentage of long NEs is negligible. 91% of the NEs in the training data have at most three words."
"Second, the POS tags frequently used in NEs are limited."
"When we compare the RG+DT method with other statistical methods, its advantage is its readability and independence of generated rules."
"When using cascaded rules, a small change in a rule can damage another rule’s functionality."
"On the other hand, the recognition rules of our sys-tem are not cascaded (Fig. 1)."
"Therefore, rewrit-ing a recognition rule does not influence the per-formance of other rules at all."
"Moreover, dt-rules are usually very simple."
"When all of the training data were used, most of the RG+DT’s recognition rules had a simple additional constraint that al-ways accepts (65%) or rejects (16%) candidates."
This result also implies the usefulness of our rule generator.
Only 2% of the recognition rules have 10 or more dt-rules.
"For instance, the following recognition rule has dozens of dt-rules. *:all-katakana:misc-proper-noun -&gt; PERSON,0,0."
"However, they are easy to understand as follows."
"If the next word is SHI (honorific), accept it."
"If the next word is SAN (honorific), accept it."
"If the next word is DAI-TOU-RYOU (=president), accept it."
"If the next word is KAN-TOKU (=director), accept it. :"
"Otherwise, reject it."
We can explain this tendency as follows.
"Short NEs like ‘Washington’ are often ambiguous, but longer NEs like ‘Washington State University’ are less ambiguous."
"Thus, short recognition rules of-ten have dozens of dt-rules, whereas long rules have simple constraints."
Some NE systems use decision tree learning to classify a word.
"Sekine’s system (1998) is simi-lar to the above ME systems, but C4.5[REF_CITE]is used instead."
"A similar system partic-ipated in IREX, but failed to show good perfor-mance."
"When he added lexical ques-tions (e.g., whether the current word is or not) to Sekine’s system, C4.5 crashed with CRL NE."
"Accordingly, the decision tree systems did not di-rectly use words as features."
"Instead, they used a word’s memberships in their word lists."
Baluja’s system (2000) simply determines whether a word is in an NE or not and does not classify it.
"On the other hand,[REF_CITE]uses decision tree learning for classification of a noun phrase by assuming that named entities are noun phrases."
"Brill’s rule generation method[REF_CITE]is not used for NE tasks, but it might be useful."
"Recently, unsupervised or minimally super-vised models have been proposed[REF_CITE]."
"Collins’ system is not a full NE system and Ut-suro’s score is not very good yet, but they repre-sent interesting directions."
"As far as we can tell, Japanese NE recognition technology has not yet matured."
Conventional de-cision tree systems have not shown good perfor-mance.
"The maximum entropy method is compet-itive, but adding more training data causes prob-lems."
"In this paper, we presented an alterna-tive method based on decision tree learning and longest match."
"According to our experiments, this method’s performance is comparable to that of the maximum entropy system, and it can be trained more efficiently."
We hope our method can be ap-plicable to other languages.
This paper compares two different ways of estimating statistical language mod-els.
Many statistical NLP tagging and parsing models are estimated by max-imizing the (joint) likelihood of the fully-observed training data.
"How-ever, since these applications only re-quire the conditional probability distri-butions, these distributions can in prin-ciple be learnt by maximizing the con-ditional likelihood of the training data."
"Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by max-imizing the conditional, even though some of the latter models intuitively had access to “more information”."
"Many statistical NLP applications, such as tag-ging and parsing, involve finding the value of some hidden variable Y (e.g., a tag or a parse tree) which maximizes a conditional prob-ability distribution P θ (Y|X), where X is a given word string."
"The model parameters θ are typically estimated by maximum likelihood: i.e., maximizing the likelihood of the training data."
"Given a (fully observed) training cor-pus D = ((y 1 , x 1 ), . . . , (y n , x n )), the maximum (joint) likelihood estimate (MLE) of θ is: n θ̂ = argmax Y P θ (y i , x i ). (1) θ i=1"
"However, it turns out there is another maximum likelihood estimation method which maximizes the conditional likelihood or “pseudo-likelihood” of the training data[REF_CITE]."
Maximum conditional likelihood is consistent for the con-ditional distribution.
"Given a training corpus D, the maximum conditional likelihood estimate (MCLE) of the model parameters θ is:"
Figure 1 graphically depicts the difference be-tween the MLE and MCLE.
"Let Ω be the universe of all possible pairs (y,x) of hidden and visible values."
"Informally, the MLE selects the model parameter θ which make the training data pairs (y i ,x i ) as likely as possible relative to all other pairs (y 0 , x 0 ) in Ω. The MCLE, on the other hand, selects the model parameter θ in order to make the training data pair (y i ,x i ) more likely than other pairs (y 0 , x i ) in Ω, i.e., pairs with the same visible value x i as the training datum."
"In statistical computational linguistics, max-imum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually com-putationally intractable[REF_CITE]."
Well-known computational linguistic models such as
"Maximum-Entropy Markov Models[REF_CITE]and Stochastic Unification-based Grammars[REF_CITE]are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model."
"It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort."
"Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs from the MCLE in actual appli-cations, and if so, how."
"Because the MLE is consistent for the joint distribution P(Y,X) (e.g., in a tagging applica-tion, the distribution of word-tag sequences), it is also consistent for the conditional distribution P(Y |X) (e.g., the distribution of tag sequences given word sequences) and the marginal distribu-tion P(X) (e.g., the distribution of word strings)."
"On the other hand, the MCLE is consistent for the conditional distribution P(Y |X) alone, and pro-vides no information about either the joint or the marginal distributions."
"Applications such as lan-guage modelling for speech recognition and EM procedures for estimating from hidden data ei-ther explicitly or implicitly require marginal dis-tributions over the visible data (i.e., word strings), so it is not statistically sound to use MCLEs for such applications."
"On the other hand, applications which involve predicting the value of the hidden variable from the visible variable (such as tagging or parsing) usually only involve the conditional distribution, which the MCLE estimates directly."
"Since both the MLE and MCLE are consistent for the conditional distribution, both converge in the limit to the “true” distribution if the true dis-tribution is in the model class."
"However, given that we often have insufficient data in computa-tional linguistics, and there are good reasons to believe that the true distribution of sentences or parses cannot be described by our models, there is no reason to expect these asymptotic results to hold in practice, and in the experiments reported below the MLE and MCLE behave differently ex-perimentally."
"A priori, one can advance plausible arguments in favour of both the MLE and the MCLE."
"Infor-mally, the MLE and the MCLE differ in the fol-lowing way."
Since the MLE is obtained by maxi-mizing Q i P θ (y i |x i )
"P θ (x i ), the MLE exploits in-formation about the distribution of word strings x i in the training data that the MCLE does not."
"Thus one might expect the MLE to converge faster than the MCLE in situations where training data is not over-abundant, which is often the case in compu-tational linguistics."
"On the other hand, since the intended applica-tion requires a conditional distribution, it seems reasonable to directly estimate this conditional distribution from the training data as the MCLE does."
"Furthermore, suppose that the model class is wrong (as is surely true of all our current lan-guage models), i.e., the “true” model P(Y, X) =6 P θ (Y, X) for all θ, and that our best models are particularly poor approximations to the true dis-tribution of word strings P(X)."
Then ignoring the distribution of word strings in the training data as the MCLE does might indeed be a reasonable thing to do.
The rest of this paper is structured as fol-lows.
The next section formulates the MCLEs for HMMs and PCFGs as constrained optimiza-tion problems and describes an iterative dynamic-programming method for solving them.
"Because of the computational complexity of these prob-lems, the method is only applied to a simple PCFG based on the ATIS corpus."
"For this ex-ample, the MCLE PCFG does perhaps produce slightly better parsing results than the standard MLE (relative-frequency) PCFG, although the re-sult does not reach statistical significance."
It seems to be difficult to find model classes for which the MLE and MCLE are both easy to com-pute.
"However, often it is possible to find two closely related model classes, one of which has an easily computed MLE and the other which has an easily computed MCLE."
"Typically, the model classes which have an easily computed MLE de-fine joint probability distributions over both the hidden and the visible data (e.g., over word-tag pair sequences for tagging), while the model classes which have an easily computed MCLE de-fine conditional probability distributions over the hidden data given the visible data (e.g., over tag sequences given word sequences)."
"Section 3 investigates closely related joint and conditional tagging models (the lat-ter can be regarded as a simplification of the Maximum Entropy Markov Models[REF_CITE]), and shows that MLEs outperform the MCLEs in this application."
"The final empirical section investigates two different kinds of stochastic shift-reduce parsers, and shows that the model estimated by the MLE outperforms the model estimated by the MCLE."
"In this application, the pairs (y,x) consist of a parse tree y and its terminal string or yield x (it may be simpler to think of y containing all of the parse tree except for the string x)."
"Recall that in a PCFG with production set R, each produc-tion (A → α) ∈ R is associated with a parameter θ"
A → α .
These parameters satisfy a normalization constraint for each nonterminal A:
A → α = 1 (3) α:(A → α)∈R
"For each production r ∈ R, let f r (y) be the num-ber of times r is used in the derivation of the tree y. Then the PCFG defines a probability distribu-tion over trees:"
Y →α (Y ) P θ (Y ) = θ
A → αf A (A → α)∈R
The MLE for θ is the well-known “relative-frequency” estimator: n → α (y i ) θ̂ A → α = P i=1 f A . n P i=1
P α :(A α ) 0 → 0 ∈R f
A → α 0 (y i )
Unfortunately the MCLE for a PCFG is more complicated.
"If x is a word string, then let τ(x) be the set of parse trees with terminal string or yield x generated by the PCFG."
"Then given a training corpus D = ((y 1 , x 1 ), . . . , (y n , x n )), where y i is a parse tree for the string x i , the log conditional likelihood of the training data log P(~y|x~) and its derivative are given by: log P(~y|x~) ="
X log P θ (y i ) − log X P θ (y)  n  i=1  y∈τ(x i ) n 1 ∂ log P(~y|~x) = θ
A → α X (f A α (y i ) − E θ (f A α |x i )) → → ∂θ A → α i=1
Here E θ (f|x) denotes the expectation of f with respect to P θ conditioned on Y ∈ τ(x).
"There does not seem to be a closed-form solution for the θ that maximizes P(~y|~x) subject to the con-straints (3), so we used an iterative numerical gra-dient ascent method, with the constraints (3) im-posed at each iteration using Lagrange multipli- n ers."
Note that P i=1 E θ (f A → α |x i ) is a quantity calculated in the Inside-Outside algorithm[REF_CITE]and P(~y|x~) is easily computed as a by-product of the same dynamic program-ming calculation.
"Since the expected production counts E θ (f|x) depend on the production weights θ, the entire training corpus must be reparsed on each itera-tion (as is true of the Inside-Outside algorithm)."
This is computationally expensive with a large grammar and training corpus; for this reason the MCLE PCFG experiments described here were performed with the relatively small ATIS tree-bank corpus of air travel reservations distributed by LDC.
"In this experiment, the PCFGs were always trained on the 1088 sentences of the ATIS1 corpus and evaluated on the 294 sentences of the ATIS2 corpus."
Lexical items were ignored; the PCFGs generate preterminal strings.
"The iterative algo-rithm for the MCLE was initialized with the MLE parameters, i.e., the “standard” PCFG estimated from a treebank."
Table 1 compares the MLE and MCLE PCFGs.
"The data in table 1 shows that compared to the MLE PCFG, the MCLE PCFG assigns a higher conditional probability of the parses in the train-ing data given their yields, at the expense of as-signing a lower marginal probability to the yields themselves."
The labelled precision and recall parsing results for the MCLE PCFG were slightly higher than those of the MLE PCFG.
"Because both the test data set and the differences are so small, the significance of these results was esti-mated using a bootstrap method with the differ-ence in F-score in precision and recall as the test statistic[REF_CITE]."
This test showed that the difference was not significant (p ≈ 0.1).
Thus the MCLE PCFG did not perform significantly bet-ter than the MLE PCFG in terms of precision and recall.
"As noted in the previous section, maximizing the conditional likelihood of a PCFG or a HMM can be computationally intensive."
"This section and the next pursues an alternative strategy for com-paring MLEs and MCLEs: we compare similiar (but not identical) model classes, one of which has an easily computed MLE, and the other of which has an easily computed MCLE."
"The appli-cation considered in this section is bitag POS tag-ging, but the techniques extend straight-forwardly to n-tag tagging."
"In this application, the data pairs (y,x) consist of a tag sequence y = t 1 ...t m and a word sequence x = w 1 ... w m , where t j is the tag for word w j (to simplify the formu-lae, w 0 , t 0 , w m+1 and t m+1 are always taken to be end-markers)."
Standard HMM tagging models define a joint distribution over word-tag sequence pairs; these are most straight-forwardly estimated by maximizing the likelihood of the joint train-ing distribution.
"However, it is straight-forward to devise closely related HMM tagging models which define a conditional distribution over tag sequences given word sequences, and which are most straight-forwardly estimated by maximizing the conditional likelihood of the distribution of tag sequences given word sequences in the train-ing data."
"All of the HMM models investigated in this section are instances of a certain kind of graph-ical model th[REF_CITE]calls “Bayes nets”; Figure 2 sketches the networks that correspond to all of the models discussed here. (In such a graph, the set of incoming arcs to a node depicting a vari-able indicate the set of variables on which this variable is conditioned)."
"Recall the standard bitag HMM model, which defines a joint distribution over word and tag se-quences: m+1 P(Y, X) ="
Y P̂(T j |T j−1 )P̂(W j |T j ) (4) j=1
"As is well-known, the MLE for (4) sets P̂ to the empirical distributions on the training data."
Now consider the following conditional model of the conditional distribution of tags given words (this is a simplified form of the model described[REF_CITE]): m+1 P(Y |X) =
"Y P 0 (T j |W j , T j−1 ) (5) j=1"
The MCLE of (5) is easily calculated: P 0 should be set the empirical distribution of the training data.
"However, to minimize sparse data prob-lems we estimated P 0 (T j |W j , T j−1 ) as a mixture of P̂(T j |W j ), P̂(T j |T j−1 ) and P̂(T j |W j ,T j−1 ), where the P̂ are empirical probabilities and the (bucketted) mixing parameters are determined us-ing deleted interpolation from heldout data[REF_CITE]."
These models were trained on sections 2-21 of the Penn tree-bank corpus.
"The tagging accuracy of the models was evaluated on section 23 of the tree-bank corpus (in both cases, the tag t j assigned to word w j is the one which maximizes the marginal P(t j |w 1 ...w m ), since this minimizes the ex-pected loss on a tag-by-tag basis)."
The conditional model (5) has the worst perfor-mance of any of the tagging models investigated in this section: its tagging accuracy is 94.4%.
The joint model (4) has a considerably lower error rate: its tagging accuracy is 95.5%.
"One possible explanation for this result is that the way in which the interpolated estimate of P 0 is calculated, rather than conditional likelihood estimation per se, is lowering tagger accuracy somehow."
"To investigate this possibility, two ad-ditional joint models were estimated and tested, based on the formulae below. m+1 P(Y,X) ="
"Y P̂(W j |T j )P 1 (T j |W j−1 ,T j−1 ) (6) j=1 m+1"
"Y P 0 (T j |W j ,T j−1 )"
P̂(W j |T j−1 ) (7) j=1
"The MLEs for both (6) and (7) are easy to cal-culate. (6) contains a conditional distribution P 1 which would seem to be of roughly equal com-plexity to P 0 , and it was estimated using deleted interpolation in exactly the same way as P 0 , so if the poor performance of the conditional model was due to some artifact of the interpolation pro-cedure, we would expect the model based on (6) to perform poorly."
"Yet the tagger based on (6) performs the best of all the taggers investigated in this section: its tagging accuracy is 96.2%. (7) is admitted a rather strange model, since the right hand term in effect predicts the follow-ing word from the current word’s tag."
"However, note that (7) differs from (5) only via the pres-ence of this rather unusual term, which effectively converts (5) from a conditional model to a joint model."
"Yet adding this term improves tagging ac-curacy considerably, to 95.3%."
"Thus for bitag tag-ging at least, the conditional model has a consid-erably higher error rate than any of the joint mod-els examined here. (While a test of significance was not conducted here, previous experience with this test set shows that performance differences of this magnitude are extremely significant statis-tically)."
The previous section compared similiar joint and conditional tagging models.
This section com-pares a pair of joint and conditional parsing mod-els.
The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated.
These parsers are direct simplifications of the Structured Language Model[REF_CITE].
"Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers[REF_CITE]."
The distribution over trees generated by the joint model is a probabilistic context-free language[REF_CITE].
"As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ig-nored, and the POS tags are used as the terminals."
"These two parsers only produce trees with unary or binary nodes, so we binarized the train-ing data before training the parser, and debina-rize the trees the parsers produce before evaluat-ing them with respect to the test data[REF_CITE]."
We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children.
"We binarized by first joining the head to all of the constituents to its right, and then joining the re-sulting structure with constituents to the left."
The label of a new node is the label of the head fol-lowed by the suffix “-1” if the head is (contained in) the right child or “-2” if the head is (contained in) the left child.
Figure 3 depicts an example of this transformation.
"The Structured Language Model is described in detail[REF_CITE], so it is only reviewed here."
Each parser’s stack is a sequence of node labels (possibly including labels introduced by bi-narization).
"In what follows, s 1 refers to the top element of the stack, or ‘?’ if the stack is empty; similarly s 2 refers to the next-to-top element of the stack or ‘?’ if the stack contains less than two elements."
"We also append a ‘?’ to end of the ac-tual terminal string being parsed (just as with the HMMs above), as this simplifies the formulation of the parsers, i.e., if the string to be parsed is w 1 . . . w m , then we take w m+1 = ?."
A shift-reduce parse is defined in terms of moves.
"A move is either shift(w), reduce 1 (c) or reduce 2 (c), where c is a nonterminal label and w is either a terminal label or ‘?’."
"Moves are par-tial functions from stacks to stacks: a shift(w) move pushes a w onto the top of stack, while a reduce i (c) move pops the top i terminal or non-terminal labels off the stack and pushes a c onto the stack."
A shift-reduce parse is a sequence of moves which (when composed) map the empty stack to the two-element stack whose top element is ‘?’ and whose next-to-top element is the start symbol. (Note that the last move in a shift-reduce parse must always be a shift(?) move; this cor-responds to the final “accept” move in an LR parser).
"The isomorphism between shift-reduce parses and standard parse trees is well-known[REF_CITE], and so is not de-scribed here."
"A (joint) shift-reduce parser is defined by a distribution P(m|s 1 ,s 2 ) over next moves m given the top and next-to-top stack labels s 1 and s 2 ."
"To ensure that the next move is in fact a possible move given the current stack, we require that P(reduce 1 (c)|?,?) = 0 and P(reduce 2 (c)|c 0 ,?) = 0 for all c,c 0 , and that P(shift(?)|s 1 , s 2 ) = 0 unless s 1 is the start sym-bol and s 2 = ?."
Note that this extends to a probability distribution over shift-reduce parses (and hence parse trees) in a particularly simple way: the probability of a parse is the product of the probabilities of the moves it consists of.
"As-suming that P meets certain tightness conditions, this distribution over parses is properly normal-ized because there are no “dead” stack configura-tions: we require that the distribution over moves be defined for all possible stacks."
"A conditional shift-reduce parser differs only minimally from the shift-reduce parser just described: it is defined by a distribution P(m|s 1 , s 2 , t) over next moves m given the top and next-to-top stack labels s 1 , s 2 and the next input symbol w (w is called the look-ahead sym-bol)."
"In addition to the requirements on P above, we also require that if w 0 6= w then P(shift(w 0 ) |s 1 ,s 2 ,w) = 0 for all s 1 ,s 2 ; i.e., shift moves can only shift the current look-ahead symbol."
"This restriction implies that all non-zero probability derivations are derivations of the parse string, since the parse string forces a single se-quence of symbols to be shifted in all derivations."
"As before, since there are no “dead” stack con-figurations, so long as P obeys certain tightness conditions, this defines a properly normalized dis-tribution over parses."
"Since all the parses are re-quired to be parses of of the input string, this de-fines a conditional distribution over parses given the input string."
"It is easy to show that the MLE for the joint model, and the MCLE for the conditional model, are just the empirical distributions from the train-ing data."
"We ran into sparse data problems using the empirical training distribution as an estimate for P(m|s 1 ,s 2 ,w) in the conditional model, so in fact we used deleted interpolation to interpo-late P̂(m|s 1 , s 2 , w), and P̂(m|s 1 , s 2 ) to estimate P(m|s 1 , s 2 , w)."
"The models were estimated from sections 2–21 of the Penn treebank, and tested on the 2245 sentences of length 40 or less in section 23."
The deleted interpolation parameters were es-timated using heldout training data from section 22.
We calculated the most probable parses using a dynamic programming algorithm based on the one described[REF_CITE].
"Jelinek notes that this algorithm’s running time is n 6 (where n is the length of sentence being parsed), and we found exhaustive parsing to be computationally imprac-tical."
"We used a beam search procedure which thresholded the best analyses of each prefix of the string being parsed, and only considered analyses whose top two stack symbols had been observed in the training data."
"In order to help guard against the possibility that this stochastic pruning influ-enced the results, we ran the parsers twice, once with a beam threshold of 10 −6 (i.e., edges whose probability was less than 10 −6 of the best edge spanning the same prefix were pruned) and again with a beam threshold of 10 −9 ."
"The results of the latter runs are reported in table 2; the labelled precision and recall results from the run with the more restrictive beam threshold differ by less than 0.001, i.e., at the level of precision reported here, are identical with the results presented in table 2 except for the Precision of the Joint SR parser, which was 0.665."
"For comparision, table 2 also reports results from the non-lexicalized treebank PCFG estimated from the transformed trees in sections 2-21 of the treebank; here exhaustive CKY parsing was used to find the most probable parses."
"All of the precision and recall results, including those for the PCFG, presented in table 2 are much lower than those from a standard treebank PCFG; presumably this is because the binarization trans-formation depicted in Figure 3 loses informa-tion about pairs of non-head constituents in the same local tree ([REF_CITE]reports similiar performance degradation for other binarization transformations)."
Both the joint and the condi-tional shift-reduce parsers performed much worse than the PCFG.
"This may be due to the pruning effect of the beam search, although this seems unlikely given that varying the beam threshold did not affect the results."
The performance dif-ference between the joint and conditional shift-reduce parsers bears directly on the issue ad-dressed by this paper: the joint shift-reduce parser performed much better than the conditional shift-reduce parser.
"The differences are around a per-centage point, which is quite large in parsing re-search (and certainly highly significant)."
The fact that the joint shift-reduce parser out-performs the conditional shift-reduce parser is somewhat surprising.
"Because the conditional parser predicts its next move on the basis of the lookahead symbol as well as the two top stack categories, one might expect it to predict this next move more accurately than the joint shift-reduce parser."
"The results presented here show that this is not the case, at least for non-lexicalized pars-ing."
The label bias of conditional models may be responsible for this[REF_CITE].
"This paper has investigated the difference be-tween maximum likelihood estimation and max-imum conditional likelihood estimation for three different kinds of models: PCFG parsers, HMM taggers and shift-reduce parsers."
"The results for the PCFG parsers suggested that conditional es-timation might provide a slight performance im-provement, although the results were not statis-tically significant since computational difficulty of conditional estimation of a PCFG made it necessary to perform the experiment on a tiny training and test corpus."
"In order to avoid the computational difficulty of conditional estima-tion, we compared closely related (but not identi-cal) HMM tagging and shift-reduce parsing mod-els, for some of which the maximum likelihood estimates were easy to compute and for others of which the maximum conditional likelihood esti-mates could be easily computed."
"In both cases, the joint models outperformed the conditional models by quite large amounts."
"This suggests that it may be worthwhile investigating meth-ods for maximum (joint) likelihood estimation for model classes for which only maximum con-ditional likelihood estimators are currently used, such as Maximum Entropy models and MEMMs, since if the results of the experiments presented in this paper extend to these models, one might expect a modest performance improvement."
"As explained in the introduction, because max-imum likelihood estimation exploits not just the conditional distribution of hidden variable (e.g., the tags or the parse) conditioned on the visible variable (the terminal string) but also the marginal distribution of the visible variable, it is reason-able to expect that it should outperform maxi-mum conditional likelihood estimation."
"Yet it is counter-intuitive that joint tagging and shift-reduce parsing models, which predict the next tag or parsing move on the basis of what seems to be less information than the corresponding con-ditional model, should nevertheless outperform that conditional model, as the experimental re-sults presented here show."
"The recent theoreti-cal and simulation results[REF_CITE]suggest that conditional models may suffer from label bias (the discovery of which Lafferty et. al. attribute[REF_CITE]), which may provide an insightful explanation of these results."
"None of the models investigated here are state-of-the-art; the goal here is to compare two dif-ferent estimation procedures, and for that rea-son this paper concentrated on simple, easily im-plemented models."
"However, it would also be interesting to compare the performance of joint and conditional estimators on more sophisticated models."
"We present a rule−based shallow− parser compiler, which allows to generate a robust shallow−parser for any language, even in the absence of training data, by resorting to a very limited number of rules which aim at identifying constituent boundaries."
We contrast our approach to other approaches used for shallow−parsing (i.e. finite−state and probabilistic methods).
We present an evaluation of our tool for English (Penn Treebank) and for French (newspaper corpus &quot;LeMonde&quot;) for several tasks (NP−chunking &amp; &quot;deeper&quot; parsing) .
"Full syntactic parsers of unrestricted text are costly to develop, costly to run and often yield errors, because of lack of robustness of wide− coverage grammars and problems of attachment."
"This has led, as early as 1958 (Joshi &amp;[REF_CITE]), to the development of shallow−parsers, which aim at identifying as quickly and accurately as possible, main constituents (and possibly syntactic functions) in an input, without dealing with the most difficult problems encountered with &quot;full−parsing&quot;."
"Hence, shallow−parsers are very practical tools."
There are two main techniques used to develop shallow−parsers: [Footnote_1]− Probabilistic techniques (e.g.[REF_CITE]Daelmans &amp; al. 99) [Footnote_2]− Finite−state techniques (e.g.[REF_CITE])
"1 We are leaving aside unsupervised learning techniques here, since to our knowledge they have not proved a successful for developing practical shallow−parsers."
2 See[REF_CITE]for the definition of a chunk.
"Probabilistic techniques require large amounts of syntactically−annotated training data 1 , which makes them very unsuitable for languages for which no such data is available (i.e. most languages except English) and also, they are not domain−independent nor &quot;style−independent&quot; (e.g. they do not allow to successfully shallow− parse speech, if no annotated data is available for that “style”)."
"Finally, a shallow−parser developed using these techniques will have to mirror the information contained in the training data."
"For instance, if one trains such a tool on data were only non recursive NP chunks are marked 2 , then one will not be able to obtain richer information such as chunks of other categories, embeddings, syntactic functions..."
"On the other hand, finite−state techniques rely on the development of a large set of rules (often based on regular expressions) to capture all the ways a constituent can expend."
"So for example for detecting English NPs, one could write the following rules :"
NP → Det adj* noun adj*
NP → Det adj (for noun ellipsis)
NP → ProperNoun etc ....
"But this is time consuming and difficult since one needs to foresee all possible rewriting cases, and if some rule is forgotten, or if too many POS errors are left, robustness and/or accuracy will suffer."
"Then these regular expressions have to be manipulated i.e. transformed into automata, which will be determinized and minimized (both being costly operations)."
"And even though determinization and minimization must be done only once (in theory) for a given set of rules, it is still costly to port such tools to a new set of rules (e.g. for a new language, a new domain) or to change some existing rules."
"In this paper, we argue that in order to accomplish the same task, it is unnecessary to develop full sets of regular expression : instead of specifying all the ways a constituent can be rewritten, it is sufficient to express how it begins and/or ends."
"This allows to achieve similar results but with far fewer rules, and without a need for determinization or minimization because rules which are written that way are de−facto deterministic."
"So in a sense, our approach bears some similarities with the constraint−based formalism because we resort to “local rules” (Karlsson &amp; al. 95), but we focus on identifying constituent boundaries (and not syntactic functions), and allow any level of embedding thanks to the use of a stack."
"In the first part of this paper, we present our tool: a shallow−parser compiler."
"In a second part, we present output samples as well as several evaluations for French and for English, where the tool has been used to develop both an NP−chunker and a richer shallow−parser."
We also explain why our approach is more tolerant to POS−tagging errors.
"Finally, we discuss some other practical uses which are made of this shallow−parser compiler."
"Our tool has been developed using JavaCC (a compiler compiler similar to Lex &amp; Yacc, but for java)."
The program takes as input a file containing rules.
"These rules aim at identifying constituent boundaries for a given language. (For example for English, one such rule could say &quot;When encountering a preposition, start a PP&quot;), either by relying on function words, or on morphological information (e.g. gender) if it is appropriate for the language which is being considered."
"These rule files specify : • A mapping between the &quot;abstract&quot; morpho− syntactic tags, used in the rules, and &quot;real&quot; morpho−syntactic tags as they will appear in the input. • A declaration of the syntactic constituents which will be detected (e.g. NP, VP, PP ...) • A set of unordered rules"
"From this rule file, the compiler generates a java program, which is a shallow−parser based on the rule file."
One can then run this shallow− parser on an input to obtain a shallow−parsed text 3 .
"The compiler itself is quite simple, but we have decided to compile the rules rather than interpret them essentially for efficiency reasons."
"Also, it is language independent since a rule file may be written for any given language, and compiled into a shallow−parser for that language."
Each rule is of the form: {Preamble} disjunction of patterns then actions 2.1 A concrete example : compiling a simple NP−chunker for English
In this section we present a very simple &quot;toy&quot; example which aims at identifying some NPs in the Penn Treebank [Footnote_4] (Marcus &amp; al 93).
4 This example is kept very simple for sake of clarity. It does not aim at yielding a very accurate result.
"In order to do so, we write a rule file, shown on figure 1."
"The top of the file declares a mapping between the abstract tagset we use in our rules, and the tagset of the PennTreebank."
"For example commonN corresponds to the [Footnote_3] tags NN, NNS, NNPS in the PennTreebank."
"3 The input is generally POS−tagged, although this is not an intrinsic requirement of the compiler."
It then declares the labels of the constituents which will be detected (here there is only one: NP).
"Finally, it declares [Footnote_3] rules. %% A small NP−chunker for the Penn−treebank tagmap &lt;QuantityAdv:any,some,many&gt;; tagmap&lt;ProperN:NNP&gt;; tagmap&lt;det:DT,PDT&gt;; tagmap&lt;commonN:NN,NNS,NNPS&gt;; tagmap&lt;DemPro:D*&gt;; tagmap&lt;Adj:JJ*&gt;; tagmap&lt;OtherTags:V*,P,C*,RB*.,:,,&gt;; label NP; %% rule 1 {} (:$det) | ($QuantityAdv:) | (:$DemPro) then close(),open(NP); %% rule 2 {!NP} (:$commonN) | (:$Adj) | (:$ProperN) then close(),open(NP); %% rule 3 {} (:$OtherTags) then close(); FIGURE 1 :"
"3 The input is generally POS−tagged, although this is not an intrinsic requirement of the compiler."
An example of a rule−file
"Rule 1 says that when a determiner, a quantity adverb or a demonstrative pronoun is encountered, the current constituent must be closed, and an NP must be opened."
"Rule 2 says that, when not inside an NP, if a common noun, an adjective or a proper noun is encountered, then the current constituent should be closed and an NP should be opened."
"Finally, Rule [Footnote_3] says that when some other tag is encountered (i.e. a verb, a preposition, a punctuation, a conjunction or an adverb) then the current constituent should be closed."
"3 The input is generally POS−tagged, although this is not an intrinsic requirement of the compiler."
This rule file is then compiled into an NP− chunker.
"If one inputs (a) to the NP−chunker, it will then output (b) (a) The/DT cat/NNS eats/VBZ the/DT mouse/NNS ./. (b) &lt;NP&gt; The/DT cat/NNS &lt;/NP&gt; eats/VBZ &lt;NP&gt; the/DT mouse/NNS &lt;/NP&gt; ./."
"In our compiler, rules access a limited context : • The constituent(s) being built • The previous form and POS • The current form and POS • The next form and POS"
"So contrary to standard finite−state techniques, only constituent boundaries are explicited, and it is not necessary (or even possible) to specify all the possible ways a constituent may be realized ."
"As shown in section 3, this reduces greatly the number of rules in the system (from several dozens to less than 60 for a wide−coverage shallow−parser)."
"Also, focussing only on constituent boundaries ensures determinism : there is no need for determinizing nor minimizing the automata we obtain from our rules."
Our tool is robust : it never fails to provide an output and can be used to create a parser for any text from any domain in any language.
"It is also important to note that the parsing is done incrementally : the input is scanned strictly from left to right, in one single pass."
"And for each pattern matched, the associated actions are taken (i.e. constituent boundaries are added)."
"Since there is no backtracking, this allows an output in linear time."
"If several patterns match, the longest one is applied."
Hence our rules are declarative and unordered.
"Although in theory conflicts could appear between 2 patterns of same length (as shown in (c1) and (c2)), this has never happened in practice."
"Of course the case is nonetheless dealt with in the implementation, and a warning is then issued to the user. (c1) {} (:a) (:b) then close(); (c2) {} (:a) (:b) then open(X);"
"As is seen on figure 1, one can write disjunctions of patterns for a given rule."
"In this very simple example, only non recursive NP−chunks are marked, by choice."
"But it is not an intrinsic limitation of the tool, since any amount of embedding can be obtained (as shown in section 3 below), through the use of a Stack."
"From a formal point of view, our tool has the power of a deterministic push−down automaton."
"When there is a match between the input and the pattern in a rule, the following actions may be taken : • close(): closes the constituent last opened by inserting &lt;/X&gt; in the output, were X is the syntactic label at the top of the Stack. • open(X): opens a new constituent by inserting label &lt;X&gt; in the output • closeWhenOpen(X,Y): delays the closing of constituent labeled X, until constituent labeled Y is opened. • closeWhenClose(X,Y): delays the closing of constituent labeled X, until constituent labeled Y is closed. • doNothing(): used to &quot;neutralize&quot; a shorter match."
Examples for the actions open() and close() were provided on figure 1.
"The actions closeWhenOpen(X,Y) and closeWhenClose(X,Y) allow to perform some attachments."
"For example a rule for English could say : {NP} (:$conjCoord) then close(), open(NPcoord), closeWhenClose(NPcoord,NP);"
"This rule says that when inside an NP, a coordinating conjunction is encountered, a NPcoord should be opened, and should be closed only when the next NP to the right will be closed."
"This allows, for example, to obtain output (d) for a coordination of NPs [Footnote_5] . (d) John likes &lt;NP&gt;Apples&lt;/NP&gt; &lt;NPcoord&gt; and &lt;NP&gt; green beans &lt;/NP&gt; &lt;/NPcoord&gt;"
"5 This is shown as an example as to how this action can be used, it does not aim at imposing this structure to coordinations, which could be dealt with differently using other rules."
An example of the action doNothing() for English could be: {} (:$prep) then open(PP); {} P(:$prep) (:$prep) then doNothing() ;
"The first rule says that when a preposition is encountered, a PP should be opened."
"The second rule says that when a preposition is encountered, if the previous tag was also a preposition, nothing should be done."
"Since the pattern for rule 2 is longer than the pattern for rule 1, it will apply when the second preposition in a row is encountered, hence &quot;neutralizing&quot; rule 1."
"This allows to obtain &quot;flatter&quot; structures for PPs, such as the one in (e1)."
"Without this rule, one would obtain the structure in (e2) for the same input."
"In this section, we present some uses which have been made of this Shallow−Parser compiler."
"First we explain how the tool has been used to develop a 1 million word Treebank for French, along with an evaluation."
Then we present an evaluation for English.
"It is well known that evaluating a Parser is a difficult task, and this is even more true for Shallow−Parsers, because there is no real standard task (some Shallow−parsers have embedded constituents, some encode syntactic functions, some encode constituent information, some others dependencies or even a mixture of the 2) There also isn’t standard evaluation measures for such tools."
"To perform evaluation, one can compare the output of the parser to a well−established Treebank developed independently (assuming one is available for the language considered), but the result is unfair to the parser because generally in Treebanks all constituents are attached."
One can also compare the output of the parser to a piece of text which has been manually annotated just for the purpose of the evaluation.
But then it is difficult to ensure an objective measure (esp. if the person developing the parser and the person doing the annotation are the same).
"Finally, one can automatically extract, from a well− established Treebank, information that is relevant to a given , widely agreed on, non ambiguous task such as identifying bare non− recursive NP−chunks, and compare the output of the parser for that task to the extracted information."
But this yields an evaluation that is valid only for this particular task and may not well reflect the overall performance of the parser.
"In what follows, in order to be as objective as possible, we use these 3 types of evaluation, both for French and for English [Footnote_6] , and use standard measures of recall and precision."
"6 Of course, manual annotation was done by a different person than the one who developed the rules."
"Please bear in mind though that these metric measures, although very fashionable, have their limits [Footnote_7] ."
"7 For instance in a rule−based system, performance may often be increased by adding more rules."
"Our goal is not to show that our tool is the one which provides the best results when compared to other shallow−parsers, but rather to show that it obtains similar results, although in a much simpler way, with a limited number of rules compared to finite−state techniques and more tolerance to POS errors, and even in the absence of available training data (i.e. cases were probabilistic techniques could not be used)."
"To achieve this goal, we also present samples of parsed outputs we obtain, so that the reader may judge for himself/herself."
We used our compiler to create a shallow−parser for French.
"Contrary to English, very few shallow−parsers exist for French, and no Treebank actually exist to train a probabilistic parser (although one is currently being built using our tool c.f. (Abeillé &amp; al. 00))."
"Concerning shallow−parsers, one can menti[REF_CITE]who aims at isolating NPs representing technical terms, whereas we wish to have information on other constituents as well, and (Ait−Moktar &amp;[REF_CITE]) whose tool is not publicly available."
"One can also menti[REF_CITE], who developed a parser for French which also successfully relies on function words to identify constituent boundaries."
"But contrary to us, his tool does not embed constituents [Footnote_8] ."
"8 Instead, it identifies chunks and then assigns some syntactic functions to these chunks."
And it is also not publicly available.
"In order to develop a set of rules for French, we had to examine the linguistic characteristics of this language."
"It turns out that although French has a richer morphology than English (e.g. gender for nouns, marked tense for verbs), most constituents are nonetheless triggered by the occurrence of a function word."
"Following the linguistic tradition, we consider as function words all words associated to a POS which labels a closed−class i.e. : determiners, prepositions, clitics, auxiliaries, pronouns (relative, demonstrative), conjunctions (subordinating, coordinating), auxiliaries, punctutation marks and adverbs belonging to a closed class (e.g. negation adverbs &quot;ne&quot; &quot;pas&quot;) 9 ."
The presence of function words makes the detection of the beginning of a constituent rather easy.
"For instance, contrary to English, subordinating conjunctions (que/that) are never omitted when a subordinating clause starts."
"Similarly, determiners are rarely omitted at the beginning of an NP."
"Our aim was to develop a shallow−parser which dealt with some embedding, but did not commit to attach potentially ambiguous phrases such as PPs and verb complements."
"We wanted to identify the following constituents : NP, PP, VN (verbal nucleus), VNinf (infinitivals introduced by a preposition), COORD (for coordination), SUB (sentential complements), REL (relative clauses), SENT (sentence boundaries), INC (for constituents of unknown category), AdvP (adverbial phrases)."
"We wanted NPs to include all adjectives but not other postnominal modifiers (i.e. postposed relative clauses and PPs), in order to obtain a structure similar to (f). (f) &lt;NP&gt; Le beau livre bleu &lt;/NP&gt; &lt;PP&gt; de &lt;NP&gt;ma cousine&lt;/NP&gt; &lt;/PP&gt; … (my cousin’s beautiful blue book)"
Relative clauses also proved easy to identify since they begin when a relative pronoun is encountered.
"The ending of clauses occurs essentially when a punctuation mark or a conjunction of coordination is encountered or when another clause begins, or when a sentence ends (g1) ."
"These rules for closing clauses work fairly well in practice (see evaluation below) but could be further refined, since they will yield a wrong closing boundary for the relative in a sentence like (g2)"
"Concerning clitics, we have decided to group them with the verb (h1) even when dealing with subject clitics (h2)."
One motivation is the possible inversion of the subject clitic (h3). (h1) &lt;SENT&gt;&lt;NP&gt; JEAN &lt;/NP&gt; &lt;VN&gt; le lui donne&lt;/VN&gt; . &lt;/SENT&gt; (J. gives it to him). (h2) &lt;SENT&gt; &lt;VN&gt; Il le voit &lt;/VN&gt; . &lt;/SENT&gt; (He sees it) (h3) &lt;SENT&gt;&lt;VN&gt; L’as tu vu &lt;/VN&gt; ? &lt;/SENT&gt; (Him did you see ?).
"Sentences are given a flat structure, that is complements are not included in a verbal phrase [Footnote_10] (i)."
10 Hence the use of VN(for verbal nucleus) instead of VP.
From a practical point of view this eases our task.
"From a theoretical point of view, the traditional VP (with complements) is subject to much linguistic debate and is often discontinuous in French as is shown in (j1) and (j2): In (j1) the NP subject (IBM) is postverbal and precedes the locative complement (sur le marché)."
"In (j2), the adverb certainement is also postverbal and precedes the NP object (une augmentation de capital). (i) &lt;SENT&gt;&lt;NP&gt; JEAN &lt;/NP&gt; & lt;VN&gt; donne&lt;/VN&gt; &lt;NP&gt;une pomme&lt;/NP&gt; &lt;PP&gt; à &lt;NP&gt; Marie &lt;/NP&gt; &lt;/PP&gt; . &lt;/SENT&gt; (John gives an apple to Mary) (j1) les actions qu’a mises IBM sur le marché (the shares that IBM put on the market) (j2) Les actionnaires décideront certainement une augmentation de capital (the stock holders will certainly decide on a raise of capital) 3.1.1 Evaluation for French & lt;/PP&gt; &lt;/COORD&gt; &lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt; &lt;VN&gt; il:CL faut:VP&lt;/VN&gt; &lt;NP&gt; un:Dind enseignement_supérieur:NC fort:Aqual&lt;/NP&gt; &lt;PONCT&gt; .:PONCT&lt;/PONCT&gt; &lt;/SENT&gt; &lt;SENT&gt; &lt;COORD&gt; Mais:CC &lt;PP&gt; en_dehors_de:P &lt;NP&gt; ces:Ddem raisons:NC économiques:Aqual ou:CC philosophiques:Aqual &lt;/NP&gt; &lt;/PP&gt; &lt;/COORD&gt; &lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt; &lt;VN&gt; il:CL y:CL a:VP &lt;/VN&gt; &lt;NP&gt; la:Ddef réalité:NC &lt;/NP&gt; &lt;NP&gt; les:Ddef étudiants:NC&lt;/NP&gt; &lt;VN&gt; sont:VP&lt;/VN&gt; &lt;PP&gt; à:P &lt;NP&gt; notre:Dposs porte:NC&lt;/NP &gt; &lt;/PP&gt; &lt;PONCT&gt; .:PONCT&lt;/PONCT&gt; &lt;/SENT&gt;"
FIGURE 2 : Sample output for French
"When we began our task, we had at our disposal a 1 million word POS tagged and hand− corrected corpus (Abeillé &amp;[REF_CITE])."
The corpus was meant to be syntactically annotated for constituency.
"To achieve this, precise annotation guidelines for constituency had been written and a portion of the corpus had been hand−annotated (independently of the development of the shallow−parser) to test the guidelines (approx. 25 000 words) ."
"To evaluate the shallow parser, we performed as described at the beginning of section 3 : We parsed the 1 million words."
We set aside 500 sentences (approx. 15 000 words) for quickly tuning our rules.
We also set aside the 25 000 words that had been independently annotated in order to compare the output of the parser to a portion of the final Treebank.
"In addition, an annotator hand−corrected the output of the shallow−parser on 1000 new randomly chosen sentences (approx. 30 000 words)."
"Contrary to the 25 000 words which constituted the beginning of the Treebank, for these 30 000 words verb arguments, PPs and modifiers were not attached."
"Finally, we extracted bare non− recursive NPs from the 25 000 words, in order to evaluate how the parser did on this particular task."
"When compared to the hand−corrected parser’s output, for opening brackets we obtain a recall of 94.3 % and a precision of 95.2%."
"For closing brackets, we obtain a precision of 92.2 % and a recall of 91.4 %."
"These unknown constituents, rather then errors, constitute a mechanism of underspecification (the idea being to assign as little wrong information as possible) [Footnote_11] ."
"11 These underspecified label can be removed at a deeper parsing stage, or one can add a guesser ."
"When compared to the 25 000 words of the Treebank, For opening brackets, the recall is 92.9% and the precision is 94%."
"For closing brackets, the recall is 62,8% and the precision is 65%."
"These lower results are normal, since the Treebank contains attachments that the parser is not supposed to make."
"Finally, on the specific task of identifying non− recursive NP−chunks, we obtain a recall of 96.6 % and a precision of 95.8 %. for opening brackets, and a recall and precision of resp. 94.3% and 92.9 % for closing brackets."
"To give an idea about the coverage of the parser, sentences are on average 30 words long and comprise 20.6 opening brackets (and thus as many closing brackets)."
"Errors difficult to correct with access to a limited context involve mainly &quot;missing&quot; brackets (e.g. &quot;comptez vous * ne pas le traiter&quot; (do you expect not to treat him) appears as single constituent, while there should be 2) , while &quot;spurious&quot; brackets can often be eliminated by adding more rules (e.g. for multiple prepositions : &quot;de chez&quot;)."
Most errors for closing brackets are due to clause boundaries(i.e.
"SUB, COORD and REL)."
"To obtain these results, we had to write only 48 rules."
"Concerning speed, as argued in (Tapanainen &amp;[REF_CITE]), we found that rule−based systems are not necessarily slow, since the 1 million words are parsed in 3mn 8 seconds."
"One can compare this to (Ait−Moktar &amp;[REF_CITE]), who, in order to shallow−parse French resort to 14 networks and parse 150words /sec (Which amounts to approx. 111 minutes for one million words) [Footnote_12] ."
"12 They report a recall ranging from 82.6% and 92.6% depending on the type of texts, and a precision of 98% for subject recognition, but their results are not directly comparable to ours, since the task is different."
"It is difficult to compare our result to other results, since most Shallow−parsers pursue different tasks, and use different evaluation metrics."
"However to give an idea, standard techniques typically produce an output for one million words in 20 mn and report a precision and a recall ranging from 70% to 95% depending on the language, kind of text and task."
"Again, we are not saying that our technique obtains best results, but simply that it is fast and easy to use for unrestricted text for any language."
"To give a better idea to the reader, we provide an output of the Shallow−parser for French on figure 2."
"In order to improve our tool and our rules, a demo is available online on the author’s homepage."
"We wanted to evaluate our compiler on more than one language, to make sure that our results were easily replicable."
"So we wrote a new set of rules for English using the PennTreebank tagset, both for POS and for constituent labels."
"We sat aside sections 00 and 01 of the WSJ for evaluation (i.e. approx. 3900 sentences), and used other sections of the WSJ for tuning our rules."
"Contrary to the French Treebank, the Penn Treebank contains non−surfastic constructions such as empty nodes, and constituents that are not triggered by a lexical items."
"Therefore, before evaluating our new shallow− parser, we automatically removed from the test sentences all opening brackets that were not immediately followed by a lexical item, with their corresponding closing brackets, as well as all the constituents which contained an empty element."
We also removed all information on pseudo−attachment.
We then evaluated the output of the shallow−parser to the test sentences.
"For bare NPs, we compared our output to the POS tagged version of the test sentences (since bare−NPs are marked there)."
"For the shallow−parsing task, we obtain a precision of 90.8% and a recall of 91% for opening brackets, a precision of 65.7% and recall of 66.1% for closing brackets."
"For the NP−chunking task, we obtain a precision of 91% and recall of 93.2%, using an “exact match” measure (i.e. both the opening and closing boundaries of an NP must match to be counted as correct)."
"The results, were as satisfactory as for French."
"Concerning linguistic choices when writing the rules, we didn’t really make any, and simply followed closely those of the Penn Treebank syntactic annotation guidelines (modulo the embeddings, the empty categories and pseudo− attachments mentioned above)."
"Concerning the number of rules, we used 54 of them in order to detect all constituents, and 27 rules for NP−chunks identification. ."
"Even though these 1200 patterns corresponded to a lower number of regular expressions, a standard finite−state approach would have to resort to more than 27 rules."
"One can also compare this result to the one reported in (Ramshaw &amp;[REF_CITE]) who, obtain up to 93.5% recall and 93.1% precision on the same task, but using between 500 and 2000 rules."
"To test the tolerance to POS tagging errors, we have extracted the raw text from the English corpus from section 3.2., and retagged it using the publicly available tagger TreeTagger[REF_CITE]. without retraining it."
The authors of the tagger advertise an error−rate between 3 and 4%.
"We then ran the NP−chunker on the output of the tagger, and still obtain a precision of 90.2% and a recall of 92% on the “exact match” NP identification task: the fact that our tool does not rely on regular expressions describing &quot;full constituent patterns&quot; allows to ignore some POS errors since mistagged words which do not appear at constituent boundaries (i.e. essentially lexical words) have no influence on the output."
This improves accuracy and robustness.
"For example, if &quot;first&quot; has been mistagged noun instead of adjective in [NP the first man ] on the moon ..., it won’t prevent detecting the NP, as long as the determiner has been tagged correctly."
This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaus-tively parsing the Penn Treebank with the Treebank’s own CFG grammar.
"We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strate-gies."
"We discuss grammatical saturation, in-cluding analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations."
This paper originated from examining the empirical performance of an exhaustive active chart parser us-ing an untransformed treebank grammar over the Penn Treebank.
Our initial experiments yielded the sur-prising result that for many configurations empirical parsing speed was super-cubic in the sentence length.
This led us to look more closely at the structure of the treebank grammar.
"The resulting analysis builds on the presentation[REF_CITE], but extends it by elucidating the structure of non-terminal inter-relationships in the Penn Treebank grammar."
"On the basis of these studies, we build simple theoretical models which closely predict observed parser perfor-mance, and, in particular, explain the originally ob-served super-cubic behavior."
We used treebank grammars induced directly from the local trees of the entire WSJ section of the Penn Treebank[REF_CITE](release 3).
"For each length and parameter setting, 25 sentences evenly dis-tributed through the treebank were parsed."
"Since we were parsing sentences from among those from which our grammar was derived, coverage was never an is- sue."
Every sentence parsed had at least one parse – the parse with which it was originally observed. [Footnote_1]
"1 Effectively “testing on the training set” would be invalid if we wished to present performance results such as precision and recall, but it is not a problem for the present experiments, which focus solely on the parser load and grammar structure."
The sentences were parsed using an implementa-tion of the probabilistic chart-parsing algorithm pre-sented[REF_CITE].
"In that paper, we present a theoretical analysis showing an  worst-case time bound for exhaustively parsing arbi-trary context-free grammars."
"In what follows, we do not make use of the probabilistic aspects of the gram-mar or parser."
The parameters we varied were:
"Tree Transforms: N O T RANSFORM , N O E MPTIES , N O U NARIES H IGH , and N O U NARIES L OW ."
"Grammar Rule Encodings: L IST , T RIE , or M IN Rule Introduction: T OP D OWN or B OTTOM U P"
"In all cases, the grammar was directly induced from (transformed) Penn treebank trees."
The transforms used are shown in figure 1.
"For all settings, func-tional tags and crossreferencing annotations were stripped."
"For N O T RANSFORM , no other modification was made."
"In particular, empty nodes (represented as - NONE - in the treebank) were turned into rules that generated the empty string ( ), and there was no col-lapsing of categories (such as PRT and ADVP ) as is of-ten done in parsing work ([REF_CITE]etc.)."
"N O E MPTIES , empties were removed by pruning non-terminals which covered no overt words."
"For N O U NA - RIES H IGH , and N O U NARIES L OW , unary nodes were removed as well, by keeping only the tops and the bot-toms of unary chains, respectively. [Footnote_2]"
2 In no case were the nonterminal-to-word or TOP -to-nonterminal unaries altered.
The parser operates on Finite State Automata (FSA) grammar representations.
"We compiled grammar rules into FSAs in three ways: L IST s, T RIE s, and M IN imized FSAs."
An example of each representa-tion is given in figure 2.
"For L IST encodings, each local tree type was encoded in its own, linearly struc-tured FSA, corresponding[REF_CITE]-style dot-ted rules."
"For T RIE , there was one FSA per cate-gory, encoding together all rule types producing that category."
"For M IN , state-minimized FSAs were con-structed from the trie FSAs."
"Note that while the rule encoding may dramatically affect the efficiency of a parser, it does not change the actual set of parses for a given sentence in any way. [Footnote_3]"
"3 FSAs are not the only method of representing and com-pacting grammars. For example, the prefix compacted tries we use are the same as the common practice of ignoring items before the dot in a dotted rule[REF_CITE]. Another"
"In this section, we outline the observed performance of the parser for various settings."
"We frequently speak in terms of the following: span: a range of words in the chart, e.g., [1,3] [Footnote_4] edge: a category over a span, e.g., NP :[1,3] traversal: a way of making an edge from an active and a passive edge, e.g., NP :[1,3] ( NP DT ."
4 Note that the number of words (or size) of a span is equal to the difference between the endpoints.
"NN : [1,2] +"
"The parser has an  theoretical time bound, where is the number of words in the sentence to be parsed, is the number of nonterminal categories in the grammar and is the number of (active) states in the FSA encoding of the grammar."
"The time bound is derived from counting the number of traversals pro-cessed by the parser, each taking  time."
"In figure 3, we see the average time [Footnote_5] taken per sen-tence length for several settings, with the empirical ex-ponent (and correlation -value) from the best-fit sim-ple power law model to the right."
"5 The hardware was a 700 MHz Intel Pentium III, and we used up to 2GB of RAM for very long sentences or very poor parameters. With good parameter settings, the system can parse 100+ word treebank sentences."
Notice that most settings show time growth greater than  .
"Although,  is simply an asymptotic bound, there are good explanations for the observed behav-ior."
There are two primary causes for the super-cubic time values.
The first is theoretically uninteresting.
"The parser is implemented in Java, which uses garbage collection for memory management."
"Even when there is plenty of memory for a parse’s primary data struc-tures, “garbage collection thrashing” can occur when parsing longer sentences as temporary objects cause increasingly frequent reclamation."
"To see past this ef-fect, which inflates the empirical exponents, we turn to the actual traversal counts, which better illuminate the issues at hand."
Figures 4 (a) and (b) show the traversal curves corresponding to the times in figure 3.
The interesting cause of the varying exponents comes from the “constant” terms in the theoretical bound.
The second half of this paper shows how modeling growth in these terms can accurately predict parsing performance (see figures 9 to 13).
The memory bound for the parser is  .
"Since the parser is running in a garbage-collected environ-ment, it is hard to distinguish required memory from utilized memory."
"However, unlike time and traversals which in practice can diverge, memory requirements match the number of edges in the chart almost exactly, since the large data structures are all proportional in size to the number of edges  . [Footnote_6]"
"6 A standard chart parser might conceivably require stor-ing more than +) * ., - traversals on its agenda, but ours prov-ably never does."
"Almost all edges stored are active edges ( &quot;$! &amp;# % for sentences longer than 30 words), of which there can be  : one for every grammar state and span."
"Pas-sive edges, of which there can be ( , one for ev-ery category and span, are a shrinking minority."
"This is because, while is bounded above by 27 in the tree-bank [Footnote_7] (for spans 2), numbers in the thousands (see figure 12)."
7 This count is the number of phrasal categories with the introduction of a TOP label for the unlabeled top treebank nodes.
"Thus, required memory will be implicitly modeled when we model active edges in section 4.3."
Figure 4 (a) shows the effect of the tree transforms on traversal counts.
"The N O U NARIES settings are much more efficient than the others, however this efficiency comes at a price in terms of the utility of the final parse."
"For example, regardless of which N O U NARIES transform is chosen, there will be NP nodes missing from the parses, making the parses less useful for any task requiring NP identification."
"For the remainder of the paper, we will focus on the settings N O T RANS - FORM and N O E MPTIES ."
Figure 4 (b) shows the effect of each tree transform on traversal counts.
"The more compacted the grammar representation, the more time-efficient the parser is."
Figure 4 (c) shows the effect on total edges and traversals of using top-down and bottom-up strategies.
"There are some extremely minimal savings in traver-sals due to top-down filtering effects, but there is a cor-responding penalty in edges as rules whose left-corner cannot be built are introduced."
"Given the highly unre-strictive nature of the treebank grammar, it is not very surprising that top-down filtering provides such little benefit."
"However, this is a useful observation about real world parsing performance."
"The advantages of top-down chart parsing in providing grammar-driven prediction are often advanced (e.g.,[REF_CITE]:66), but in practice we find almost no value in this for broad coverage CFGs."
"While some part of this is perhaps due to errors in the treebank, a large part just reflects the true nature of broad coverage grammars: e.g., once you allow adverbial phrases almost anywhere and al-low PPs, (participial) VPs, and (temporal) NPs to be adverbial phrases, along with phrases headed by ad-verbs, then there is very little useful top-down control left."
"With such a permissive grammar, the only real constraints are in the POS tags which anchor the local trees (see section 4.3)."
"Therefore, for the remainder of the paper, we consider only bottom-up settings."
In the remainder of the paper we provide simple mod-els that nevertheless accurately capture the varying magnitudes and exponents seen for different grammar encodings and tree transformations.
"Since the  term of  comes directly from the number of start, split, and end points for traversals, it is certainly not responsible for the varying growth rates."
"An initially plausible possibility is that the quantity bounded by the term is non-constant in in practice, because longer spans are more ambiguous in terms of the num-ber of categories they can form."
"This turns out to be generally false, as discussed in section 4.2."
"Alter-nately, the effective term could be growing with , which turns out to be true, as discussed in section 4.3."
The number of (possibly zero-size) spans for a sen-tence of length is fixed: / /5468764 .
"Thus, to be able to evaluate and model the total edge counts, we look to the number of edges over a given span."
Definition 1 The passive (or active) saturation of a given span is the number of passive (or active) edges over that span.
"In the total time and traversal bound  , the effective value of is determined by the active satu-ration, while the effective value of is determined by the passive saturation."
"An interesting fact is that the saturation of a span is, for the treebank grammar and sentences, essentially independent of what size sen-tence the span is from and where in the sentence the span begins."
"Thus, for a given span size, we report the average over all spans of that size occurring anywhere in any sentence parsed."
The reason that effective growth is not found in the component is that passive saturation stays almost constant as span size increases.
"However, the more in-teresting result is not that saturation is relatively con-stant (for spans beyond a small, grammar-dependent size), but that the saturation values are extremely large compared to (see section 4.2)."
"For the N O T RANS - FORM and N O E MPTIES grammars, most categories are reachable from most other categories using rules which can be applied over a single span."
"Once you get one of these categories over a span, you will get the rest as well."
We now formalize this.
Definition 2 A category [Footnote_9] is empty-reachable in a grammar : if 9 can be built using only empty ter-minals.
9 Implied arcs have been removed for clarity. The relation is in fact the transitive closure of this graph.
The empty-reachable set for the N O T RANSFORM grammar is shown in figure 5. [Footnote_8]
"8 The set of phrasal categories used in the Penn Tree-bank is documented in Manning and Schütze (1999, 413); Marcus et al. (1993, 281) has an early version."
"Definition 3 A category ; is same-span-reachable from a category [Footnote_9] in a grammar : if ; can be built from 9 using a parse tree in which, aside from at most one instance of 9 , every node not dominating that in-stance is an instance of an empty-reachable category."
9 Implied arcs have been removed for clarity. The relation is in fact the transitive closure of this graph.
The same-span-reachability relation induces a graph over the 27 non-terminal categories.
The strongly-connected component (SCC) reduction of that graph is shown in figures 6 and 7. 9
"Unsurprisingly, the largest SCC, which contains most “common” categories (S, NP, VP, PP, etc.) is slightly larger for the N O T RANS - FORM grammar, since the empty-reachable set is non-empty."
"However, note that even for N O T RANSFORM , the largest SCC is smaller than the empty-reachable set, since empties provide direct entry into some of the lower SCCs, in particular because of WH-gaps."
"Interestingly, this same high-reachability effect oc-curs even for the N O U NARIES grammars, as shown in the next section."
The total growth and saturation of passive edges is rel-atively easy to describe.
"Figure 8 shows the total num- ber of passive edges by sentence length, and figure 9 shows the saturation as a function of span size. 10 The grammar representation does not affect which passive edges will occur for a given span."
The large SCCs cause the relative independence of passive saturation from span size for the N O T RANS - FORM and N O E MPTIES settings.
"Once any category in the SCC is found, all will be found, as well as all cate-gories reachable from that SCC."
"For these settings, the passive saturation can be summarized by three satura-tion numbers: zero-spans (empties) &lt;&gt;= $? @BA , one-spans (words) &gt;&lt; = ?$@8C , and all larger spans (categories) &lt;&gt;= $? @ ."
"Taking averages directly from the data, we have our first model, shown on the right in figure 9."
"For the N O U NARIES settings, there will be no same-span reachability and hence no SCCs."
To reach a new category always requires the use of at least one overt word.
"However, for spans of size 6 or so, enough words exist that the same high saturation effect will still be observed."
"This can be modeled quite simply by assuming each terminal unlocks a fixed fraction of the nonterminals, as seen in the right graph of figure 9, but we omit the details here."
"Using these passive saturation models, we can di-rectly estimate the total passive edge counts by sum-mation:"
D&lt; @FE @ A /S&gt;&lt; = ? $@ J
The predictions are shown in figure 8.
"For the N O - T RANSFORM or N O E MPTIES settings, this reduces to: T&lt; @FE @ IXW  I &lt;&gt;=Z&amp;? @ / &lt;[= ?&amp;@8C\/ ]/^"
S[&lt; = ?&amp;@BA
"We correctly predict that the passive edge total ex-ponents will be slightly less than 2.0 when unaries are present, and greater than 2.0 when they are not."
"With unaries, the linear terms in the reduced equation are significant over these sentence lengths and drag down the exponent."
"The linear terms are larger for N O - T RANSFORM and therefore drag the exponent down more. 11 Without unaries, the more gradual satura-tion growth increases the total exponent, more so for N O U NARIES L OW than N O U NARIES H IGH ."
"However, note that for spans around 8 and onward, the saturation curves are essentially constant for all settings."
Active edges are the vast majority of edges and essen-tially determine (non-transient) memory requirements.
"While passive counts depend only on the grammar transform, active counts depend primarily on the en-coding for general magnitude but also on the transform for the details (and exponent effects)."
"Total active growth is sub-quadratic for L IST , but has an exponent of up to about 2.4 for the T RIE settings. 11 Note that, over these values of _ , even a basic quadratic function like the simple sum H _^*S_ [_ fZg has a best-fit simple power curve exponent of only hicZj k l for the same reason."
"Moreover, note that _ has a higher best-fit expo-nent, yet will never actually outgrow it."
"To model the active totals, we again begin by mod-eling the active saturation curves, shown in figure 11."
"The active saturation for any span is bounded above by , the number of active grammar states (states in the grammar FSAs which correspond to active edges)."
"For list grammars, this number is the sum of the lengths of all rules in the grammar."
"For trie grammars, it is the number of unique rule prefixes (including the LHS) in the grammar."
"For minimized grammars, it is the number of states with outgoing transitions (non-black states in figure 2)."
The value of is shown for each setting in figure 12.
Note that the maximum number of active states is dramatically larger for lists since com-mon rule prefixes are duplicated many times.
"For min-imized FSAs, the state reduction is even greater."
"Since states which are earlier in a rule are much more likely to match a span, the fact that tries (and min FSAs) compress early states is particularly advantageous."
"Unlike passive saturation, which was relatively close to its bound , active saturation is much farther below ."
"Furthermore, while passive saturation was relatively constant in span size, at least after a point, active saturation quite clearly grows with span size, even for spans well beyond those shown in figure 11."
We now model these active saturation curves.
What does it take for a given active state to match a given span?
"For T RIE and L IST , an active state cor- responds to a prefix of a rule and is a mix of POS tags and phrasal categories, each of which must be matched, in order, over that span for that state to be reached."
"Given the large SCCs seen in section 4.1, phrasal categories, to a first approximation, might as well be wildcards, able to match any span, especially if empties are present."
"However, the tags are, in com-parison, very restricted."
Tags must actually match a word in the span.
"More precisely, consider an active state ? in the grammar and a span = ."
"In the T RIE and L IST encod-ings, there is some, possibly empty, list n of labels that must be matched over = before an active edge with this state can be constructed over that span. [Footnote_12] Assume that the phrasal categories in n can match any span (or any non-zero span in N O E MPTIES ). [Footnote_13]"
"12 The essence of the M IN model, which is omitted here, is that states are represented by the “easiest” label sequence which leads to that state."
"13 The model for the N O U NARIES cases is slightly more complex, but similar."
"Therefore, phrasal categories in n do not constrain whether ? can match = ."
The real issue is whether the tags in n will match words in = .
"Assume that a random tag matches a random word with a fixed probability &lt; , independently of where the tag is in the rule and where the word is in the sentence. [Footnote_14] Assume further that, although tags oc-cur more often than categories in rules (63.9% of rule items are tags in the N O T[REF_CITE]), given a fixed number of tags and categories , all permutations are equally likely to appear as rules. 16"
"14 This is of course false; in particular, tags at the end of rules disproportionately tend to be punctuation tags."
"Under these assumptions, the probability that an ac-tive state ? is in the treebank grammar will depend only on the number @ of tags and o of categories in n ."
Call this pair p\q? @et8oZ the signature of ? .
"For a given signature p , let o2E [u @ be the number of ac-tive states in the grammar which have that signature."
"Now, take a state ? of signature @etwox and a span = ."
"If we align the tags in ? with words in = and align the categories in ? with spans of words in = , then pro-vided the categories align with a non-empty span (for N O E MPTIES ) or any span at all (for N O T RANSFORM ), then the question of whether this alignment of ? with = matches is determined entirely by the @ tags."
"However, with our assumptions, the probability that a randomly chosen set of @ tags matches a randomly chosen set of @ words is simply &lt;Dy ."
We then have an expression for the chance of match-ing a specific alignment of an active state to a specific span.
"Clearly, there can be many alignments which differ only in the spans of the categories, but line up the same tags with the same words."
"However, there will be a certain number of unique ways in which the words and tags can be lined up between ? and = ."
"If we know this number, we can calculate the total probability that there is some alignment which matches."
"For example, consider the state NP NP CC NP ."
"PP (which has signature (1,2) – the PP has no effect) over a span of length , with empties available."
"The NP s can match any span, so there are alignments which are distinct from the standpoint of the CC tag – it can be in any position."
"The chance that some alignment will match is therefore [&lt; I , which, for small &lt; is roughly linear in ."
"It should be clear that for an active state like this, the longer the span, the more likely it is that this state will be found over that span."
It is unfortunately not the case that all states with the same signature will match a span length with the same probability.
"For example, the state NP NP NP CC ."
"NP has the same signature, but must align the CC with the final element of the span."
A state like this will not become more likely (in our model) as span size increases.
"However, with some straightfor-ward but space-consuming recurrences, we can calcu-late the expected chance that a random rule of a given signature will match a given span length."
"Since we know how many states have a given signature, we can calculate the total active saturation q? =Z?&amp;@ as"
This model has two parameters.
"First, there is &lt; which we estimated directly by looking at the expected match between the distribution of tags in rules and the distri-bution of tags in the Treebank text (which is around 1/17.7)."
"No factor for POS tag ambiguity was used, another simplification. [Footnote_17] Second, there is the map o2E u[@ from signatures to a number of active states, which was read directly from the compiled grammars."
"17 In general, the  we used was lower for not having mod-eled tagging ambiguity, but higher for not having modeled the fact that the SCCs are not of size 27."
This model predicts the active saturation curves shown to the right in figure 11.
"Note that the model, though not perfect, exhibits the qualitative differences between the settings, both in magnitudes and expo-nents. [Footnote_18]"
18 And does so without any “tweakable” parameters.
"The transform primarily changes the saturation over short spans, while the encoding determines the over-all magnitudes."
"For example, in T RIE -N O E MPTIES the low-span saturation is lower than in T RIE - N O T RANSFORM since short spans in the former case can match only signatures which have both @ and o small, while in the latter only @ needs to be small."
"Therefore, the several hundred states which are reachable only via categories all match every span starting from size 0 for N O T RANSFORM , but are accessed only gradually for N O E MPTIES ."
"How-ever, for larger spans, the behavior converges to counts characteristic for T RIE encodings."
"For L IST encodings, the early saturations are huge, due to the fact that most of the states which are available early for trie grammars are precisely the ones duplicated up to thousands of times in the list grammars."
"However, the additive gain over the ini-tial states is roughly the same for both, as after a few items are specified, the tries become sparse."
"The actual magnitudes and exponents [Footnote_19] of the sat-urations are surprisingly well predicted, suggesting that this model captures the essential behavior."
19 Note that the list curves do not compellingly suggest a power law model.
"These active saturation curves produce the active to-tal curves in figure 10, which are also qualitatively cor-rect in both magnitudes and exponents."
"Now that we have models for active and passive edges, we can combine them to model traversal counts as well."
We assume that the chance for a passive edge and an active edge to combine into a traversal is a sin-gle probability representing how likely an arbitrary ac-tive state is to have a continuation with a label match-ing an arbitrary passive state.
"List rule states have only one continuation, while trie rule states in the branch- ing portion of the trie average about 3.7 (min FSAs 4.2). 20 Making another uniformity assumption, we as-sume that this combination probability is the contin-uation degree divided by the total number of passive labels, categorical or tag (73)."
"Our model correctly predicts the approx-imate values and qualitative facts, including:"
"For L IST , the observed exponent is lower than for T RIE s, though the total number of traversals is dra-matically higher."
This is because the active satura-tion is growing much faster for T RIE s; note that in cases like this the lower-exponent curve will never actually outgrow the higher-exponent curve.
"Of the settings shown, only T RIE -N O E MPTIES exhibits super-cubic traversal totals."
"Despite their similar active and passive exponents, T RIE - N O E MPTIES and T RIE -N"
O T RANSFORM vary in traversal growth due to the “early burst” of active edges which gives T RIE -N
O T RANSFORM signifi-cantly more edges over short spans than its power law would predict.
"This excess leads to a sizeable quadratic addend in the number of transitions, caus-ing the average best-fit exponent to drop without greatly affecting the overall magnitudes."
"Overall, growth of saturation values in span size in-creases best-fit traversal exponents, while early spikes in saturation reduce them."
The traversal exponents therefore range from L IST -N
O T RANSFORM at 2.6 to T RIE -N
O U NARIES L OW at over 3.8.
"However, the fi-nal performance is more dependent on the magnitudes, which range from L IST -N"
"O T RANSFORM as the worst, despite its exponent, to M IN -N"
O U NARIES H IGH as the best.
"The single biggest factor in the time and traver-sal performance turned out to be the encoding, which is fortunate because the choice of grammar transform will depend greatly on the application."
We built simple but accurate models on the basis of two observations.
"First, passive saturation is relatively constant in span size, but large due to high reachability among phrasal categories in the grammar."
"Second, ac-tive saturation grows with span size because, as spans increase, the tags in a given active edge are more likely to find a matching arrangement over a span."
"Combin-ing these models, we demonstrated that a wide range of empirical qualitative and quantitative behaviors of an exhaustive parser could be derived, including the potential super-cubic traversal growth over sentence lengths of interest."
Chunk parsing has focused on the recognition of partial constituent struc-tures at the level of individual chunks.
Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances.
Such larger structures are not only desirable for a deeper syntactic analysis.
They also constitute a necessary prerequisite for assigning function-argument structure.
"The present paper offers a similarity-based algorithm for assigning func-tional labels such as subject, object, head, complement, etc. to complete syntactic structures on the basis of pre-chunked input."
The evaluation of the algorithm has concentrated on measuring the quality of functional labels.
It was performed on a German and an English treebank using two different annotation schemes at the level of function-argument struc-ture.
The results of 89.73% cor-rect functional labels[REF_CITE].40 % for English validate the general approach.
"Current research on natural language parsing tends to gravitate toward one of two extremes: robust, partial parsing with the goal of broad data coverage versus more traditional parsers that aim at complete analysis for a narrowly defined set of data."
Chunk parsing[REF_CITE]offers a particularly promising and by now widely used example of the former kind.
"The main insight that underlies the chunk pars-ing strategy is to isolate the (finite-state) analysis of non-recursive syntactic structure, i.e. chunks, from larger, recursive structures."
This results in a highly-efficient parsing architecture that is realized as a cascade of finite-state transducers and that pursues a leftmost longest-match pattern-matching strategy at each level of analysis.
"Despite the popularity of the chunk parsing ap-proach, there seems to be a gap in current re-search:"
Chunk parsing research has focused on the recognition of partial constituent structures at the level of individual chunks.
"By comparison, lit-tle or no attention has been paid to the ques-tion of how such partial analyses can be com-bined into larger structures for complete utter-ances."
Such larger structures are not only de-sirable for a deeper syntactic analysis; they also constitute a necessary prerequisite for assigning function-argument structure.
Automatic assignment of function-argument structure has long been recognized as a desider-atum beyond pure syntactic labeling[REF_CITE][Footnote_1] .
"1 With the exception of dependency-grammar-based parsers[REF_CITE], where functional labels are treated as first-class citizens as relations between words, and recent work on a semi-automatic method for treebank con-structi[REF_CITE], little has been reported on"
"The present paper offers a similarity- based algorithm for assigning functional labels such as subject, object, head, complement, etc. to complete syntactic structures on the basis of pre-chunked input."
The evaluation of the algo-rithm has concentrated on measuring the quality of these functional labels.
"In order to ensure a robust and efficient archi-tecture, TüSBL, a similarity-based chunk parser, is organized in a three-level architecture, with the output of each level serving as input for the next higher level."
The first level is part-of-speech (POS) tagging of the input string with the help of the bigram tagger LIKELY[REF_CITE]. [Footnote_2]
2 The inventory of POS tags is based on the STTS[REF_CITE]for German and on the Penn Treebank tagset[REF_CITE]for English.
"The parts of speech serve as pre-terminal ele-ments for the next step, i.e. the chunk analysis."
"Chunk parsing is carried out by an adapted ver-sion of Abney’s (1996) CASS parser, which is realized as a cascade of finite-state transducers."
"The chunks, which extend if possible to the sim-plex clause level, are then remodeled into com-plete trees in the tree construction level."
The tree construction level is similar to the DOP approach[REF_CITE]in that it uses complete tree structures instead of rules.
"Contrary to Bod, we only use the complete trees and do not allow tree cuts."
Thus the number of possible combinations of partial trees is strictly controlled.
The resulting parser is highly efficient (3770 English sentences took 106.5 seconds to parse on an[REF_CITE]).
The division of labor between the chunking and tree construction modules can best be illustrated by an example.
"For sentences such as the input shown in Fig. 1, the chunker produces a structure in which some constituents remain unattached or partially anno-tated in keeping with the chunk-parsing strategy to factor out recursion and to resolve only unam-biguous attachments."
"Since chunks are by definition non-recursive structures, a chunk of a given category cannot fully automatic recognition of functional labels."
Input: alright and that should get us there about nine in the evening contain another chunk of the same type.
"In the case at hand, the two prepositional phrases (’prep p’) about nine and in the evening in the chunk output cannot be combined into a sin-gle chunk, even though semantically these words constitute a single constituent."
"At the level of tree construction, as shown in Fig. 2, the prohibition against recursive phrases is suspended."
"There-fore, the proper PP attachment becomes possible."
"Additionally, the phrase about nine was wrongly categorized as a ’prep p’."
Such miscategoriza-tions can arise if a given word can be assigned more than one POS tag.
In the case of about the tags ’in’ (for: preposition) or ’rb’ (for: ad-verb) would be appropriate.
"However, since the POS tagger cannot resolve this ambiguity from local context, the underspecified tag ’about’ is as-signed, instead."
"However, this can in turn lead to misclassification in the chunker."
The most obvious deficiency of the chunk out-put shown in Fig. 1 is that the structure does not contain any information about the function-argument structure of the chunked phrases.
"How-ever, once a (more) complete parse structure is created, the grammatical function of each ma-jor constituent needs to be identified."
"The la-bels SUBJ (for: subject), HD (for: head), ADJ (for: adjunct) COMP (for: complement), SPR (for: specifier), which appear as edge-labels be-tween tree nodes in Fig. 2, signify the grammati-cal functions of the constituents in question."
E.g. the label SUBJ encodes that the NP that is the subject of the whole sentence.
The label ADJ above the phrase about nine in the evening signi-fies that this phrase is an adjunct of the verb get.
"TüSBL currently uses as its instance base two semi-automatically constructed treebanks of Ger-man and English that consist of appr. 67,000 and 35,000 fully annotated sentences, respectively [Footnote_3] ."
3 See[REF_CITE]for further details.
Each treebank uses a different annotation scheme at the level of function-argument structure [Footnote_4] .
4 The annotation for German follows the topological-field-model standardly used in empirical studies of German syntax. The annotation for English is modeled after the theo-retical assumptions of Head-Driven Phrase Structure Gram-mar.
"As shown in Table 1, the English treebank uses a to-tal of 13 functional labels, while the German tree-bank has a richer set of 36 function labels."
"For German, therefore, the task of tree con-struction is slightly more complex because of the larger set of functional labels."
Fig. 3 gives an ex-ample for a German input sentence and its corre-sponding chunk parser output.
"In this case, the subconstituents of the extra-posed coordinated noun phrase are not attached to the simplex clause that ends with the non-finite verb that is typically in clause-final position in declarative main clauses of German."
"Moreover, each conjunct of the coordinated noun phrase forms a completely flat structure."
TüSBL’s tree construction module enriches the chunk output as shown in Fig. 4.
Here the internally recur-sive NP conjuncts have been coordinated and in-
Input: dann w”urde ich vielleicht noch vorschlagen
Donnerstag den elften und Freitag den zw”olften August (then I would suggest maybe Thursday eleventh tegrated correctly into the clause as a whole.
"In addition, function labels such as MOD (for: mod-ifier), HD (for head), ON (for: subject), OA (for: direct object), OV (for: verbal object), and APP (for: apposition) have been added that encode the function-argument structure of the sentence."
The tree construction algorithm is based on the machine learning paradigm of memory-based learning[REF_CITE]. 5 Memory-based learning assumes that the classification of a given input should be based on the similarity to previously seen instances of the same type that have been stored in memory.
"This paradigm is an instance of lazy learning in the sense that these previously encountered instances are stored “as is” and are crucially not abstracted over, as is typically the case in rule-based systems or other learning approaches."
Previous applications of memory-based learning to NLP tasks consisted of classification problems in which the set of classes to be learnt was simple in the sense that the class items did not have any internal structure and the number of distinct items was small.
"Since in the current application, the set of classes are parse trees, the classification task is much more com-plex."
"The classification is simple only in those cases where a direct hit is found, i.e. where a com-plete match of the input with a stored instance ex-ists."
"In all other cases, the most similar tree from the instance base needs to be modified to match the chunked input."
This means that the output tree will group together only those elements from the chunked input for which there is evidence in the instance base.
"If these strategies fail for com-plete chunks, TüSBL attempts to match smaller subchunks."
The algorithm used for tree construction is pre-sented in a slightly simplified form in Figs. 5-8.
"For readability, we assume here that chunks and complete trees share the same data structure so that subroutines like string yield can operate on both of them indiscriminately."
"The main routine construct tree in Fig. 5 sepa-rates the list of input chunks and passes each one to the subroutine process chunk in Fig. 6 where the chunk is then turned into one or more (partial) trees. process chunk first checks if a complete match with an instance from the instance base is possible. [Footnote_6] If this is not the case, a partial match on the lexical level is attempted."
"6 string yield returns the sequence of words included in the input structure, pos yield the sequence of POS tags."
"If a partial tree is found, attach next chunk in Fig. 7 and extend tree in Fig. 8 are used to extend the tree by either at-taching one more chunk or by resorting to a com-parison of the missing parts of the chunk with tree extensions on the POS level. attach next chunk is necessary to ensure that the best possible tree is found even in the rare case that the original seg-mentation into chunks contains mistakes."
"If no partial tree is found, the tree construction backs off to finding a complete match at the POS level or to starting the subroutine for processing a chunk recursively with all the subchunks of the present chunk."
The application of memory-based techniques is implemented in the two subroutines com-plete match and partial match.
The presentation of the two cases as two separate subroutines is for expository purposes only.
"In the actual implemen-tation, the search is carried out only once."
"The two subroutines exist because of the postprocess-ing of the chosen tree, which is necessary for par-tial matches and which also deviates from stan-dard memory-based applications."
Postprocessing mainly consists of shortening the tree from the in-stance base so that it covers only those parts of the chunk that could be matched.
"However, if the match is done on the lexical level, a correction of tagging errors is possible if there is enough evi-dence in the instance base."
"TüSBL currently uses an overlap metric, the most basic metric for in- stances with symbolic features, as its similarity metric."
This overlap metric is based on either lexical or POS features.
"Instead of applying a more sophisticated metric like the weighted over-lap metric, TüSBL uses a backing-off approach that heavily favors similarity of the input with pre-stored instances on the basis of substring identity."
Splitting up the classification and adaptation pro-cess into different stages allows TüSBL to prefer analyses with a higher likelihood of being correct.
This strategy enables corrections of tagging and segmentation errors that may occur in the chun-ked input.
"Quantitive evaluations of robust parsers typically focus on the three PARSEVAL measures: labeled precision, labeled recall and crossing accuracy."
"It has frequently been pointed out that these evalu-ation parameters provide little or no information as to whether a parser assigns the correct seman-tic structure to a given input, if the set of category labels comprises only syntactic categories in the narrow sense, i.e. includes only names of lexi-cal and phrasal categories."
This justified criticism observes that a measure of semantic accuracy can only be obtained if the gold standard includes an-notations of syntactic-semantic dependencies be-tween bracketed constituents.
It is to answer this criticism that the evaluation of the TüSBL system presented here focuses on the correct assignment of functional labels.
"For an in-depth evaluation that focuses on syntactic categories, we refer the interested reader[REF_CITE]."
The quantitative evaluation of TüSBL has been conducted on the treebanks of German and En-glish described in section 3.
Each treebank uses a different annotation scheme at the level of function-argument structure.
"As shown in Table 1, the English treebank uses a total of 13 func-tional labels, while the German treebank has a richer set of 36 function labels."
"The evaluation consisted of a ten-fold cross-validation test, where the training data provide an instance base of already seen cases for TüSBL’s tree construction module."
The evaluation was per-formed for both the German and English data.
"For each language, the following parameters were measured: 1. labeled precision for syntactic cat- egories alone, and 2. labeled precision for func-tional labels."
The results of the quantitative evaluation are shown in Tables 2 and 3.
The results for labeled recall underscore the difficulty of applying the classical PARSEVAL measures to a partial pars- ing approach like ours.
"We have, therefore di-vided the incorrectly matched nodes into three categories: the genuine false positives where a tree structure is found that matches the gold stan-dard, but is assigned the wrong label; nodes which, relative to the gold standard, remain unattached in the output tree; and nodes contained in the gold standard for which no match could be found in the parser output."
Our approach follows a strategy of positing and attaching nodes only if sufficient evidence can be found in the instance base.
Therefore the latter two categories can-not really be considered errors in the strict sense.
"Nevertheless, in future research we will attempt to significantly reduce the proportion of unattached and unmatched nodes by exploring matching al-gorithms that permit a higher level of generaliza-tion when matching the input against the instance base."
What is encouraging about the recall results reported in Table 2 is that the parser produces gen-uine false positives for an average of only 3.03 % for German and 3.25 % for English.
"For German, labeled precision for syntactic categories yielded 81.56% correctness."
"While these results do not reach the performance re-ported for other parsers (cf.[REF_CITE]), it is important to note that the two treebanks consist of transliterated spontaneous speech data."
The fragmentary and partially ill-formed nature of such spoken data makes them harder to analyze than written data such as the Penn treebank typically used as gold standard.
It should also be kept in mind that the basic PARSEVAL measures were developed for parsers that have as their main goal a complete analy-sis that spans the entire input.
"This runs counter to the basic philosophy underlying an amended chunk parser such as TüSBL, which has as its main goal robustness of partially analyzed struc-tures."
Labeled precision of functional labels for the German data resulted in a score of 89.73 % cor-rectness.
"For English, precision of functional la-bels was 90.40 %."
The slightly lower correctness rate for German is a reflection of the larger set of function labels used by the grammar.
This raises interesting more general issues about trade-offs in accuracy and granularity of functional annota-tions.
The results of 89.73 % (German) and 90.40 % (English) correctly assigned functional labels val-idate the general approach.
We anticipate fur-ther improvements by experimenting with more sophisticated similarity metrics 7 and by enrich-ing the linguistic information in the instance base.
"The latter can, for example, be achieved by pre-serving more structural information contained in the chunk parse."
Yet another dimension for ex-perimentation concerns the way in which the al-gorithm generalizes over the instance base.
"In the current version of the algorithm, generaliza-tion heavily relies on lexical and part-of-speech information."
"However, a richer set of backing-off strategies that rely on larger domains of structure are easy to envisage and are likely to significantly improve recall performance."
"While we intend to pursue all three dimensions of refining the basic algorithm reported here, we have to leave an experimentation of which modi-fications yield improved results to future research."
Previous research has shown that the plausibility of an adjective-noun com-bination is correlated with its corpus co-occurrence frequency.
"In this paper, we estimate the co-occurrence frequen-cies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and com-pare them to human plausibility rat-ings."
"Both class-based smoothing and distance-weighted averaging yield fre-quency estimates that are significant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques."
Certain combinations of adjectives and nouns are perceived as more plausible than others.
"A classi-cal example is strong tea , which is highly plausi-ble, as opposed to powerful tea , which is not."
"On the other hand, powerful car is highly plausible, whereas strong car is less plausible."
"It has been argued in the theoretical literature that the plausi-bility of an adjective-noun pair is largely a collo-cational (i.e., idiosyncratic) property, in contrast to verb-object or noun-noun plausibility, which is more predictable[REF_CITE]."
The collocational hypothesis has recently been investigated in a corpus study[REF_CITE].
"This study investigated potential statistical predictors of adjective-noun plausibility by using correlation analysis to com-pare judgements elicited from human subjects with five corpus-derived measures: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik’s (1993) selectional association measure."
All predictors but one were positively correlated with plausibility; the highest correlation was obtained with co-occurrence frequency.
Resnik’s selectional association measure surprisingly yielded a significant negative correlation with judged plausibility.
These results suggest that the best predictor of whether an adjective-noun combination is plausible or not is simply how often the adjective and the noun collocate in a record of language experience.
"As a predictor of plausibility, co-occurrence frequency has the obvious limitation that it can-not be applied to adjective-noun pairs that never occur in the corpus."
A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that the adjective-noun pair is in-herently implausible.
"In the present paper, we ad-dress this problem by using smoothing techniques (distance-weighted averaging and class-based smoothing) to recreate missing co-occurrence counts, which we then compare to plausibility judgements elicited from human subjects."
"By demonstrating a correlation between recreated frequencies and plausibility judgements, we show that these smoothing methods produce realistic frequency estimates for missing co-occurrence data."
This approach allows us to establish the va-lidity of smoothing methods independent from a specific natural language processing task.
"Smoothing techniques have been used in a variety of statistical natural language processing applica-tions as a means to address data sparseness, an in-herent problem for statistical methods which rely on the relative frequencies of word combinations."
The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated.
"The smoothing meth-ods proposed in the literature (overviews are pro-vided[REF_CITE]and[REF_CITE]) can be generally divided into three types: discount-ing[REF_CITE], class-based smoothing[REF_CITE], and distance-weighted averaging[REF_CITE]."
"Discounting methods decrease the probability of previously seen events so that the total prob-ability of observed word co-occurrences is less than one, leaving some probability mass to be re-distributed among unseen co-occurrences."
Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: inter-word dependencies are modelled by relying on the corpus evidence available for words that are similar to the words of interest.
The two ap-proaches differ in the way they measure word similarity.
"Distance-weighted averaging estimates word similarity from lexical co-occurrence infor-mation, viz., it finds similar words by taking into account the linguistic contexts in which they oc-cur: two words are similar if they occur in sim-ilar contexts."
"In class-based smoothing, classes are used as the basis according to which the co-occurrence probability of unseen word combina-tions is estimated."
Classes can be induced directly from the corpus[REF_CITE]or taken from a manually crafted taxonomy[REF_CITE].
In the latter case the taxonomy is used to provide a mapping from words to concep-tual classes.
"In language modelling, smoothing techniques are typically evaluated by showing that a lan-guage model which uses smoothed estimates in-curs a reduction in perplexity on test data over a model that does not employ smoothed estimates[REF_CITE]."
They also com-pare different distance-weighted averaging meth-ods on a pseudo-word disambiguation task where the language model decides which of two verbs v 1 and v 2 is more likely to take a noun n as its object.
"The method being tested must reconstruct which of the unseen (v 1 ,n) and (v 2 ,n) is a valid verb-object combination."
In our experiments we recreated co-occurrence frequencies for unseen adjective-noun pairs using two different approaches: taxonomic class-based smoothing and distance-weighted averaging. 1
We evaluated the recreated frequencies by comparing them with plausibility judgements elicited from human subjects.
"In contrast to previous work, this type of evaluation does not presuppose that the recreated frequencies are needed for a specific natural language processing task."
"Rather, our aim is to establish an independent criterion for the validity of smoothing techniques by comparing them to plausibility judgements, which are known to correlate with co-occurrence frequency[REF_CITE]."
In the remainder of this paper we present class- based smoothing and distance-weighted averag-ing as applied to unseen adjective-noun combina-tions (see Sections 2.[Footnote_1] and 2.2).
1 Discounting methods were not included[REF_CITE]demonstrated that distance-weighted averaging achieves better language modelling performance than back-off.
Section 3 details our judgement elicitation experiment and reports our results.
We recreated co-occurrence frequencies for un-seen adjective-noun pairs using a simplified ver-sion of Resnik’s (1993) selectional association measure.
"Selectional association is defined as the amount of information a given predicate carries about its argument, where the argument is rep-resented by its corresponding classes in a taxon-omy such as WordNet[REF_CITE]."
"This means that predicates which impose few restric-tions on their arguments have low selectional as-sociation values, whereas predicates selecting for a restricted number of arguments have high se-lectional association values."
"Consider the verbs see and polymerise : intuitively there is a great variety of things which can be seen, whereas there is a very specific set of things which can be polymerised (e.g., ethylene)."
Resnik demon-strated that his measure of selectional associa-tion successfully captures this intuition: selec-tional association values are correlated with verb-argument plausibility as judged by native speak-ers.
"However,[REF_CITE]found that the success of selectional association as a predictor of plausibility does not seem to carry over to adjective-noun plausibility."
"There are two poten-tial reasons for this: (1) the semantic restrictions that adjectives impose on the nouns with which they combine appear to be less strict than the ones imposed by verbs (consider the adjective su-perb which can combine with nearly any noun); and (2) given their lexicalist nature, adjective-noun combinations may defy selectional restric-tions yet be intuitively plausible (consider the pair sad day , where sadness is not an attribute of day )."
"To address these problems, we replaced Resnik’s information-theoretic measure with a simpler measure which makes no assumptions with respect to the contribution of a semantic class to the total quantity of information provided by the predicate about the semantic classes of its argument."
We simply substitute the noun oc-curring in the adjective-noun combination with the concept by which it is represented in the taxonomy and estimate the adjective-noun co-occurrence frequency by counting the number of times the concept corresponding to the noun is ob-served to co-occur with the adjective in the cor-pus.
"Because a given word is not always repre-sented by a single class in the taxonomy (i.e., the noun co-occurring with an adjective can gener-ally be the realisation of one of several conceptual classes), we constructed the frequency counts for an adjective-noun pair for each conceptual class by dividing the contribution from the adjective by the number of classes to which it belongs[REF_CITE]: (1) f(a,c) ≈ ∑ f(a,n 0 ) n 0 ∈c |classes(n 0 )| where f(a,n 0 ) is the number of times the ad-jective a was observed in the corpus with con-cept c ∈ classes(n 0 ) and |classes(n 0 )| is the num-ber of conceptual classes noun n 0 belongs to."
"Note that the estimation of the frequency f(a,c) relies on the simplifying assumption that the noun co-occurring with the adjective is distributed evenly across its conceptual classes."
This simplification is necessary unless we have a corpus of adjective-noun pairs labelled explicitly with taxonomic in-formation. 2
Consider the pair proud chief which is not attested in the British National Corpus (BNC)[REF_CITE].
"The word chief has two senses in WordNet and belongs to seven conceptual classes (hcausal agenti, hentityi, hleaderi, hlife formi, hpersoni, hsuperiori, and hsupervisori)"
"This means that the co-occurrence frequency of the adjective-noun pair will be constructed for each of the seven classes, as shown in Table 1."
Suppose for example that we see the pair proud leader in the corpus.
"The word leader has two senses in WordNet and belongs to eight conceptual classes (hpersoni, hlife fromi, hentityi, hcausal agenti, hfeaturei, hmerchandisei, hcommodityi, and hobjecti)."
"The words chief and leader have four conceptual classes in common, i.e., hpersoni and hlife formi, hentityi, and hcausal agenti."
"This means that we will increment the observed co-occurrence count of proud and hpersoni, proud and hlife formi, proud and hentityi, and proud and hcausal agenti by 18 ."
"Since we do not know the actual class of the noun chief in the corpus, we weight the contribution of each class by taking the average of the constructed frequencies for all seven classes: ∑ ∑ |classesf(a,n( 0 n) 0 ) | n)n 0 ∈c (2) f(a,n) = c∈classes |classes ( (n)|"
Based on (2) the recreated frequency for the pair proud chief in the BNC is 6.12 (see Table 1).
Distance-weighted averaging induces classes of similar words from word co-occurrences with-out making reference to a taxonomy.
A key fea-ture of this type of smoothing is the function which measures distributional similarity from co-occurrence frequencies.
Several measures of dis-tributional similarity have been proposed in the literature[REF_CITE].
"We used two measures, the Jensen-Shannon diver-gence and the confusion probability."
Those two measures have been previously shown to give promising performance for the task of estimat-ing the frequencies of unseen verb-argument pairs[REF_CITE].
In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs.
"The Jensen-Shannon divergence is an information-theoretic measure that recasts the concept of distributional similarity into a measure of the “distance” (i.e., dissimilarity) between two probability distributions."
Let w 1 and w 10 be an unseen sequence of two words whose distributional similarity is to be determined.
Let P(w 2 |w 1 ) denote the condi-tional probability of word w 2 given word w 1 and P(w 2 |w 01 ) denote the conditional probability of w 2 given w 01 .
For notational simplicity we write p(w 2 ) for P(w 2 |w 1 ) and q(w 2 ) for P(w 2 |w 01 ).
"The Jensen-Shannon divergence is defined as the av-erage Kullback-Leibler divergence of each of two distributions to their average distribution:   (3) J(p,q)= 1 D p p+q +D q p+q 2 2 2 where (p+ q)/[Footnote_2] denotes the average distribution: 1 (4)"
"2 There are several ways of addressing this problem, e.g., by discounting the contribution of very general classes by finding a suitable class to represent a given concept[REF_CITE]."
P(w 2 |w 1 )+P(w 2 |w 01 ) 2
"The Kullback-Leibler divergence is an information-theoretic measure of the dissim-ilarity of two probability distributions p and q, defined as follows: (5) D(p||q) = ∑ p i log pq iii"
In our case the distributions p and q are the conditional probability distributions P(w 2 |w 1 ) and
"P(w 2 |w 01 ), respectively."
Computation of the Jensen-Shannon divergence depends only on the linguistic contexts w 2 which the two words w 1 and w 01 have in common.
"The Jensen-Shannon di-vergence, a dissimilarity measure, is transformed to a similarity measure as follows: (6) W J (p,q) = 10 −βJ(p,q)"
"The parameter β controls the relative influence of the words most similar to w 1 : if β is high, only words extremely similar to w 1 contribute to the estimate, whereas if β is low, less similar words also contribute to the estimate."
"The confusion proba-bility is an estimate of the probability that word w 01 can be substituted by word w 1 , in the sense of being found in the same linguistic contexts. (7) P c (w 1 |w 01 ) = ∑"
"P(w 1 |w 2 )P(w 2 |w 01 ) w 2 where P c (w 01 |w 1 ) is the probability that word w 01 occurs in the same contexts w 2 as word w 1 , aver-aged over these contexts."
Let w 2 w 1 be two unseen co-occurring words.
"We can estimate the conditional probability P(w 2 |w 1 ) of the unseen word pair w 2 w 1 by com-bining estimates for co-occurrences involving similar words: (8) P SIM (w 2 |w 1 ) = ∑ W(w 1 ,w 0 ) 1 P(w 2 |w 01 ) w 01 ∈S(w 1 ) N(w 1 ) where S(w 1 ) is the set of words most similar to w 1 , W(w 1 ,w 01 ) is the similarity function between w 1 and w 01 , and N(w 1 ) is a normalising factor N(w 1 ) = ∑ w 0"
"W(w 1 ,w 01 )."
"The conditional proba- 1 bility P SIM (w 2 |w 1 ) can be trivially converted to co-occurrence frequency as follows: (9) f(w 1 ,w 2 ) ="
P SIM (w 2 |w 1 )f(w 1 )
"We experimented with two approaches to computing P(w 2 |w 01 ): (1) us-ing the probability distribution P(n|a), which dis-covers similar adjectives and treats the noun as the context; and (2) using P(a|n), which discovers similar nouns and treats the adjective as the con-text."
"These conditional probabilities can be easily estimated from their relative frequency in the cor-pus as follows: (10) P(n|a)= f(a,n) P(a|n) = f(a,n) f(a) f(n)"
"The performance of distance-weighted averaging depends on two parameters: (1) the number of items over which the similarity function is com-puted (i.e., the size of the set S(w 1 ) denoting the set of words most similar to w 1 ), and (2) the value of the parameter β (which is only relevant for the Jensen-Shannon divergence)."
"In this study we recreated adjective-noun frequencies using the 1,000 and 2,000 most frequent items (nouns and adjectives), for both the confusion probabil-ity and the Jensen-Shannon divergence. [Footnote_3] Further-more, we set β to .5, which experiments showed to be the best value for this parameter."
"3 These were shown to be the best parameter settings[REF_CITE]. Note that considerable latitude is available when setting these parameters; there are 151,478 distinct ad-jective types and 367,891 noun types in the BNC."
Once we know which words are most simi-lar to the either the adjective or the noun (irre-spective of the function used to measure similar-ity) we can exploit this information in order to recreate the co-occurrence frequency for unseen adjective-noun pairs.
"We use the weighted aver-age of the evidence provided by the similar words, where the weight given to a word w 01 depends on its similarity to w 1 (see (8) and (9))."
Table 2 shows the ten most similar adjectives to the word proud and then the ten most similar nouns to the word chief using the Jensen-Shannon divergence and the confusion probability.
"Here the similarity function was calculated over the 1,000 most fre-quent adjectives in the BNC."
"In order to evaluate the smoothing methods intro-duced above, we first needed to establish an inde-pendent measure of plausibility."
The standard ap-proach used in experimental psycholinguistics is to elicit judgements from human subjects; in this section we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli.
"We used a part-of-speech annotated, lemmatised version of the BNC."
"The BNC is a large, balanced corpus of British English, consist-ing of 90 million words of text and 10 million words of speech."
Frequency information obtained from the BNC can be expected to be a reason-able approximation of the language experience of a British English speaker.
The experiment used the same set of 30 adjec-tives discussed[REF_CITE].
"These ad-jectives were chosen to be minimally ambiguous: each adjective had exactly two senses according to WordNet and was unambiguously tagged as ‘adjective’ 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC."
For each adjective we obtained all the nouns (excluding proper nouns) with which it failed to co-occur in the BNC.
"We identified adjective-noun pairs by using Gsearch[REF_CITE], a chart parser which detects syntactic patterns in a tagged corpus by exploiting a user-specified context free grammar and a syntactic query."
From the syntactic anal-ysis provided by the parser we extracted a ta-ble containing the adjective and the head of the noun phrase following it.
"In the case of compound nouns, we only included sequences of two nouns, and considered the rightmost occurring noun as the head."
"From the adjective-noun pairs obtained this way, we removed all pairs where the noun had a BNC frequency of less than 10 per million, in order to reduce the risk of plausibility ratings being influenced by the presence of a noun un-familiar to the subjects."
Each adjective was then paired with three randomly-chosen nouns from its list of non-co-occurring nouns.
Example stimuli are shown in Table 3.
"The experimental paradigm was magnitude estimation (ME), a technique stan-dardly used in psychophysics to measure judge-ments of sensory stimuli[REF_CITE], which[REF_CITE]and"
The ME procedure requires subjects to estimate the magnitude of physical stimuli by assigning numerical values proportional to the stimulus magnitude they perceive.
"In contrast to the 5- or 7-point scale conventionally used to measure hu-man intuitions, ME employs an interval scale, and therefore produces data for which parametric in-ferential statistics are valid."
ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion.
"Subjects are first exposed to a modulus item, which they assign an arbitrary number."
All other stimuli are rated proportional to the modu-lus.
"In this way, each subject can establish their own rating scale, thus yielding maximally fine-graded data and avoiding the known problems with the conventional ordinal scales for linguis-tic data[REF_CITE]."
"In the present experiment, subjects were pre-sented with adjective-noun pairs and were asked to rate the degree of adjective-noun fit propor-tional to a modulus item."
"The experiment was car-ried out using WebExp, a set of Java-Classes for administering psycholinguistic studies over the World-Wide Web[REF_CITE]."
"Subjects first saw a set of instructions that explained the ME technique and included some examples, and had to fill in a short questionnaire including basic demographic information."
Each subject saw the entire set of 90 experimental items.
Forty-one native speakers of English volunteered to participate.
Subjects were re-cruited over the Internet by postings to relevant newsgroups and mailing lists.
"Correlation analysis was used to assess the degree of linear relationship between plausibility ratings (Plaus) and the three smoothed co-occurrence frequency estimates: distance-weighted averaging using Jensen-Shannon divergence (Jen), distance-weighted averaging using confusion probability (Conf), and class-based smoothing using Word-Net (WN)."
"For the two similarity-based measures, we smoothed either over the similarity of the ad-jective (subscript a) or over the similarity of the noun (subscript n)."
All frequency estimates were natural log-transformed.
Table 4 displays the results of the corre-lation analysis.
"Mean plausibility ratings were significantly correlated with co-occurrence fre-quency recreated using our class-based smooth-ing method based on WordNet (r = .356, p &lt; .01)."
"As detailed in Section 2.2, the Jensen-Shannon divergence and the confusion probability are pa- rameterised measures."
There are two ways to smooth the frequency of an adjective-noun com-bination: over the distribution of adjectives or over the distribution of nouns.
We tried both ap-proaches and found a moderate correlation be-tween plausibility and both the frequency recre-ated using distance-weighted averaging and con-fusion probability.
"The correlation was significant both for frequencies recreated by smoothing over adjectives (r = .214, p &lt; .05) and over nouns (r = .232, p &lt; .05)."
"However, co-occurrence fre-quency recreated using the Jensen-Shannon di-vergence was not reliably correlated with plausi-bility."
"Furthermore, there was a reliable correla-tion between the two Jensen-Shannon measures Jen a and Jen n (r = .781, p &lt; .01), and similarly between the two confusion measures Conf a and Conf n (r = .864, p &lt; .01)."
"We also found a high correlation between Jen a and Conf a (r = .941, p &lt; .01) and Jen n and Conf n (r = .956, p &lt; .01)."
This indicates that the two similarity measures yield comparable results for the given task.
We also examined the effect of varying one further parameter (see Section 2.2).
"The recre-ated frequencies were initially estimated using the n = 1,000 most similar items."
"We examined the effects of applying the two smoothing meth-ods using a set of similar items of twice the size (n = 2,000)."
"No improvement in terms of the cor-relations with rated plausibility was found when using this larger set, whether smoothing over the adjective or the noun: a moderate correlation with plausibility was found for Conf a (r = .239, p &lt; .05) and Conf n (r = .239, p &lt; .05), while the cor-relation with Jen a and Jen n was not significant."
An important question is how well people agree in their plausibility judgements.
Inter-subject agreement gives an upper bound for the task and allows us to interpret how well the smoothing techniques are doing in relation to the human judges.
We computed the inter-subject correlation on the elicited judgements using leave-one-out re-sampling[REF_CITE].
"Aver-age inter-subject agreement was .55 (Min = .01, Max = .76, SD = .16)."
This means that our ap-proach performs satisfactorily given that there is a fair amount of variability in human judgements of adjective-noun plausibility.
One remaining issue concerns the validity of our smoothing procedures.
We have shown that co-occurrence frequencies recreated using smoothing techniques are significantly correlated with rated plausibility.
But this finding consti-tutes only indirect evidence for the ability of this method to recreate corpus evidence; it depends on the assumption that plausibility and frequency are adequate indicators of each other’s values.
Does smoothing accurately recreate the co-occurrence frequency of combinations that actually do occur in the corpus?
"To address this question, we ap-plied the class-based smoothing procedure to a set of adjective-noun pairs that occur in the cor-pus with varying frequencies, using the materials[REF_CITE]."
"First, we removed all relevant adjective-noun combinations from the corpus."
"Effectively we assumed a linguistic environment with no evi-dence for the occurrence of the pair, and thus no evidence for any linguistic relationship be-tween the adjective and the noun."
"Then we recre-ated the co-occurrence frequencies using class-based smoothing and distance-weighted averag-ing, and log-transformed the resulting frequen-cies."
Both methods yielded reliable correlation between recreated frequency and actual BNC fre-quency (see Table 5 for details).
This result pro-vides additional evidence for the claim that these smoothing techniques produce reliable frequency estimates for unseen adjective-noun pairs.
"Note that the best correlations were achieved for Conf a and Conf n (r = .646, p &lt; .01 and r = .728, p &lt; .01, respectively)."
"Finally, we carried out a further test of the quality of the recreated frequencies by correlat-ing them with the plausibility judgements re-ported[REF_CITE]."
"Again, a signifi-cant correlation was found for all methods (see Table 5)."
"However, all correlations were lower than the correlation of the actual frequencies with plausibility (r = .570, p &lt; .01) reported[REF_CITE]."
"Note also that the con-fusion probability outperformed Jensen-Shannon divergence, in line with our results on unfamiliar adjective-noun pairs."
The present experiment extended this result to adjective-noun pairs that do not co-occur in the corpus.
We applied two smoothing techniques in order to recreate co-occurrence frequency and found that the class-based smoothing method was the best predictor of plausibility.
"This result is inter- esting because the class-based method does not use detailed knowledge about word-to-word rela-tionships in real language; instead, it relies on the notion of equivalence classes derived from Word-Net, a semantic taxonomy."
It appears that making predictions about plausibility is most effectively done by collapsing together the speaker’s experi-ence with other words in the semantic class occu-pied by the target word.
"The distance-weighted averaging smoothing methods yielded a lower correlation with plausi-bility (in the case of the confusion probability), or no correlation at all (in the case of the Jensen-Shannon divergence)."
"The worse performance of distance-weighted averaging is probably due to the fact that this method conflates two kinds of distributional similarity: on the one hand, it gen-erates words that are semantically similar to the target word."
"On the other hand, it also generates words whose syntactic behaviour is similar to that of the target word."
"Rated plausibility, however, seems to be more sensitive to semantic than to syntactic similarity."
"As an example refer to Table 6, which displays the ten most distributionally similar words to the adjectives guilty and dangerous and to the nouns stop and giant discovered by the Jensen-Shannon measure."
The set of similar words is far from se-mantically coherent.
As far as the adjective guilty is concerned the measure discovered antonyms such as innocent and honest .
"Semantically unre-lated adjectives such as injured , democratic , or in-terested are included; it seems that their syntactic behaviour is similar to that of guilty , e.g., they all co-occur with party ."
"The same pattern can be ob-served for the adjective dangerous , to which none of the discovered adjectives are intuitively seman-tically related, perhaps with the exception of bad ."
The set of words most similar to the noun stop also does not appear to be semantically coherent.
This problem with distance-weighted averag-ing is aggravated by the fact that the adjective or noun that we smooth over can be polysemous.
"Take the set of similar words for giant , for in-stance."
"The words company , manufacturer , indus-try and firm are similar to the ‘enterprise’ sense of giant , whereas artist , star , master are similar to the ‘important/influential person’ sense of gi-ant ."
"However, no similar word was found for ei-ther the ‘beast’ or ‘heavyweight person’ sense of giant ."
This illustrates that the distance-weighted averaging approach fails to take proper account of the polysemy of a word.
"The class-based ap-proach, on the other hand, relies on WordNet, a lexical taxonomy that can be expected to cover most senses of a given lexical item."
Recall that distance-weighted averaging dis-covers distributionally similar words by look-ing at simple lexical co-occurrence information.
In the case of adjective-noun pairs we concen-trated on combinations found in the corpus in a head-modifier relationship.
This limited form of surface-syntactic information does not seem to be sufficient to reproduce the detailed knowl-edge that people have about the semantic relation-ships between words.
"Our class-based smoothing method, on the other hand, relies on the semantic taxonomy of WordNet, where fine-grained con-ceptual knowledge about words and their rela-tions is encoded."
This knowledge can be used to create semantically coherent equivalence classes.
"Such classes will not contain antonyms or items whose behaviour is syntactically related, but not semantically similar, to the words of interest."
"To summarise, it appears that distance-weighted averaging smoothing is only partially successful in reproducing the linguistic depen-dencies that characterise and constrain the forma-tion of adjective-noun combinations."
"The class-based smoothing method, however, relies on a pre-defined taxonomy that allows these depen-dencies to be inferred, and thus reliably estimates the plausibility of adjective-noun combinations that fail to co-occur in the corpus."
This paper investigated the validity of smoothing techniques by using them to recreate the frequen-cies of adjective-noun pairs that fail to occur in a 100 million word corpus.
We showed that the recreated frequencies are significantly correlated with plausibility judgements.
These results were then extended by applying the same smoothing techniques to adjective-noun pairs that occur in the corpus.
"These recreated frequencies were sig-nificantly correlated with the actual frequencies, as well as with plausibility judgements."
Our results provide independent evidence for the validity of the smoothing techniques we em-ployed.
"In contrast to previous work, our evalu- ation does not presuppose that the recreated fre-quencies are used in a specific natural language processing task."
"Rather, we established an in-dependent criterion for the validity of smooth-ing techniques by comparing them to plausibil-ity judgements, which are known to correlate with co-occurrence frequency."
"We also carried out a comparison of different smoothing meth-ods, and found that class-based smoothing outper-forms distance-weighted averaging. [Footnote_4]"
"4 Two anonymous reviewers point out that this conclusion only holds for an approach that computes similarity based on adjective-noun co-occurrences. Such co-occurrences might not reflect semantic relatedness very well, due to the idiosyn-cratic nature of adjective-noun combinations. It is possible that distance-weighted averaging would yield better results if applied to other co-occurrence data (e.g., subject-verb, verb-object), which could be expected to produce more reliable information about semantic similarity."
"From a practical point of view, our findings provide a very simple account of adjective-noun plausibility."
"Extending the results[REF_CITE], we confirmed that co-occurrence frequency can be used to estimate the plausibility of an adjective-noun pair."
"If no co-occurrence counts are available from the corpus, then counts can be recreated using the corpus and a structured source of taxonomic knowledge (for the class-based approach)."
Distance-weighted averaging can be seen as a ‘cheap’ way to obtain this sort of taxonomic knowledge.
"However, this method does not draw upon semantic informa-tion only, but is also sensitive to the syntactic distribution of the target word."
This explains the fact that distance-weighted averaging yielded a lower correlation with perceived plausibility than class-based smoothing.
"A taxonomy like WordNet provides a cleaner source of conceptual information, which captures essential aspects of the type of knowledge needed for assessing the plausibility of an adjective-noun combination."
"We provide a logical definition of Min-imalist grammars, that are Stabler’s formalization of Chomsky’s minimal-ist program."
"Our logical definition leads to a neat relation to catego-rial grammar, (yielding a treatment of Montague semantics), a parsing-as-deduction in a resource sensitive logic, and a learning algorithm from struc-tured data (based on a typing-algorithm and type-unification)."
Here we empha-size the connection to Montague se-mantics which can be viewed as a for-mal computation of the logical form.
"The connection between categorial grammars (es-pecially in their logical setting) and minimalist grammars, which has already been observed and discussed[REF_CITE], deserve a further study: although they both are lexicalized, and resource consumption (or feature checking) is their common base, they differ in various re-spects."
"On the one hand, traditional categorial grammar has no move operation, and usually have a poor generative capacity unless the good prop-erties of a logical system are damaged, and on the other hand minimalist grammars even though they were provided with a precise formal defini-ti[REF_CITE], still lack some computational properties that are crucial both from a theoreti-cal and a practical viewpoint."
"Regarding appli-cations, one needs parsing, generation or learning algorithms, and, considering more conceptual as-pects, such algorithms are needed too to validate or invalidate linguistic claims regarding economy or efficiency."
Our claim is that a logical treat-ment of these grammars leads to a simpler de-scription and well defined computational proper-ties.
"Of course among these aspects the relation to semantics or logical form is quite important; it is claimed to be a central notion in minimal-ism, but logical forms are rather obscure, and no computational process from syntax to semantics is suggested."
"Our logical presentation of mini-malist grammar is a first step in this direction: to provide a description of minimalist grammar in a logical setting immediately set up the com-putational framework regarding parsing, genera-tion and even learning, but also yields some good hints on the computational connection with logi-cal forms."
"The logical system we use, a slight extension of (de[REF_CITE]), is quite similar to the fa-mous Lambek calculus[REF_CITE], which is known to be a neat logical system."
This logic has recently shown to have good logical properties like the subformula property which are relevant both to linguistics and computing theory (e.g. for modeling concurrent processes).
The logic under consideration is a super-imposition of the Lam-bek calculus (a non commutative logic) and of intuitionistic multiplicative logic (also known as Lambek calculus with permutation).
"The context, that is the set of current hypotheses, are endowed with an order, and this order is crucial for obtain-ing the expected order on pronounced and inter-preted features but it can also be relaxed when necessary: that is when its effects have already been recorded (in the labels) and the correspond-ing hypotheses can therefore be discharged."
"Having this logical description of syntactic analyses allows to reduce parsing (and produc-tion) to deduction, and to extract logical forms from the proof; we thus obtain a close connection between syntax and semantics as the one between Lambek-style analyses and Montague semantics."
The general picture of these logical grammars is as follows.
"A lexicon maps words (or, more generally, items) onto a logical formula, called the (syntactic) type of the word."
Types are de-fined from syntactic of formal features (which are propositional variables from the logical view-point): categorial features (categories   ) involved inmerge: BASE  functional   features  involved in move: FUN
The connectives in the logic for constructing formulae  togetherare withthe Lambekthe commutativeimplicationsproduct(or slashesof lin-) ear logic . [Footnote_1]
"1 The , . logical system also contains a commutative impli-cation, , and a non commutative product but they do not appear in the lexicon, and because of the subformula prop-erty, they are not needed for the proofs we use."
"Once an array of items has been selected, a sen-tence (or any phrase) is a deduction of IP (or of the phrasal category) under the assumptions provided by the syntactic types of the involved items."
"This first step works exactly as Lambek grammars, ex-cept that the logic and the formulae are richer."
"Now, in order to compute word order, we pro-ceed by labeling each formula in the proof."
"These labels, that are called phonological and seman-tic features in the transformational tradition, are computed from the proofs and consist of two parts denoted by &quot;$! #&amp;% ( , and a semantic label that can be superimposed: a phonological label, [Footnote_2] de-noted by )*$! #&amp;%(+ — the super-imposition of both label being denoted by $! &amp;# % ."
2 We prefer semantic label to logical form not to confuse logical forms with the logical formulae present at each node of the proof.
"The reason for hav-ing such a double labeling, is that, as usual in minimalism, semantic and phonological features can move separately."
"It should be observed that the labels are not some extraneous information; indeed the whole information is encoded in the proof, and the labeling is just a way to extract the phonological form and the logical form from the proof."
We rather use chains or copy theory than move-ments and traces: once a label or one aspect (se-mantic or phonological) has been met it should be ignored /103240 5% ) ;% &lt;: &gt;+ ?= #&amp;@ when it is met 0&quot;A %;: again.
For instance a label mantic label ) 103240/
C% 1/+D)726B0 % ;% E:&amp;=?F+ #&quot;@)??= 0H#&amp;A @ + corresponds 0 to a se-and ;% to : .the phonological form
"Because of the sub-formula property we need not present all the rules of the system, but only the ones that can be used according to the types that appear in the lexicon."
"Further more, up to now there is no need to use introduction rules (called hypothetical reasoning in the Lambek cal-culus): so our system looks more like Com-binatory Categorial Grammars or classical AB-grammars."
Nevertheless some hypotheses can be cancelled during the derivation by the product-elimination rule.
This is essential since this rule is the one representing chains or movements.
We also have to specify how the labels are car-ried out by the rules.
"At this point some non logical properties can be taken into account, for instance the strength of the features, if we wish to take them into account."
They are denoted by lower-case variables.
The rules of this system in a Natural  Deduction KYX T &amp;SLZN : T P9R : format L are: P S U &amp;VDW T L : P S [K L KM\NLOP9NQR P S R U VFW K U 7TO) T ] X T_3^ `+: W LaR  %;#: K U )7TO] _3^ `W+ LaR KMLaghPCR iS  
NQP9RU g  N : : P (P _m j LZklP(m U IVFW
This later rule encodes movement k U g  N :  and deserves special attention. g
"The label means N , : the substitution of to the unordered set g that N is the simultaneous substitution of N for both and : , no matter the order between and : is."
Here some non logical but linguistically mo-tivated distinction can be made.
"For instance ac-cording to the strength of a feature (e.g. weak case versus strong case ), it is possible g to de-cide that only N the semantic part that is ) + is sub-stituted with ."
"In the figure 1, the reader is provided with an example of a lexicon and 0 of a A derivation."
The re-sulting label is 0 ?) 8 A ;# #  &lt;+ % 8(&apos; 8 #&amp;  # cal form is &quot;%  &amp;8 #&amp;#  0 &lt; A phonologi-logical form is )?8 #;#  &lt;+ *) % (8 &apos; while + . the resulting
Notice that language variation from SVO to SOV does not change the analysis.
"To ob-tain the SOV word order, one should sim-ply use (strong case feature) instead of (weak case feature) in the lexicon, and use the same 8 ;# #  5 analysis % 0  "
A 8 #&amp;#.
The resulting label would be ical from &amp;8 ;#  # &lt;&quot;% which 0 (8 &apos; A yields the phonolog-remains the same ?) 8 #;#  &lt;+ )*% and 0 (8 &apos; + the A logical form.
"Observe that although entropy which sup-presses some order has been used, the labels con-sist in ordered sequences of phonological and log-ical forms."
"It is so because when using [/ E] and [ E], we necessarily order the labels, and this or-der is then recorded inside the label and is never suppressed, even when using the entropy rule: at this moment, it is only the order on hypotheses which is relaxed."
"In order to represent the minimalist grammars[REF_CITE], the above subsystem of par-tially commutative intuitionistic linear logic (de[REF_CITE]) is enough and the types appearing in the lexicon also are a strict subset of all possi-ble types:"
Definition 1 -proofs contain only three kinds of steps: implication steps (elimination rules for / and ) tensor steps (elimination rule for ) entropy steps (entropy rule)
Definition L !
P 2 A lexical entry consists in an axiom where is a type: ) )  
"F R + + + + YJ] +^ )  )  )   5J]  IH^ where: m and n can be any number greater than or equal to 0,"
"F ] , ..., F are attractors, G ] , ..., G are features, A is the resulting category type"
Derivations in this system can be seen as T-markers in the Chomskyan sense. [/E] and [ E] steps are merge steps. [ E] gives a co-indexation of two nodes that we can see as a move step.
"For instance in a tree presentation of natural deduc-tion, we shall only keep the coindexation R (corre-and S : this is sponding to the cancellation of harmless since the conclusion is not modified, and makes our natural deduction T-markers)."
"Such lexical entries, when processed with -rules encompass Stabler minimalist gram-mars; this system nevertheless overgenerates, be-cause some minimalist principles are not yet sat-isfied: they correspond to constraints on deriva-tions."
The restriction which is still lacking concerns the way the proofs are built.
"Observe that this is an algorithmic advantage, since it reduces the search space."
The simplest of these restriction is the follow-ing: the attractor F in the label L of the target locates the closest F’ in its domain.
This simply corresponds to the following restriction.
Definition 3 (Shortest Move) : A -proof is said to respect the shortest move condition if it is such that: the same formula never occurs twice as a hy-pothesis of any sequent every active hypothesis during the proof pro-cess is discharged as soon as possible
The consequences of this definition are the fol-lowing:
"In order to handle head-movement, we shall also use the product but between functor types. if there is a sequent ... ..."
KML C j 2.
We have seen above that we are able to account for SVO and SOV orders quite easily.
Neverthe-less we could not handle this way VSO language.
Indeed this order requires head-movement. ple example of: peter loves mary.
"Starting from the following lexicon in figure 3 we can build the tree given in the same figure; it represents a natural deduction in our system, hence a syntac-tic /103240 analysis %&amp;=*#&amp;@ . 0HA"
The %;: resulting phonological form is ical form is ) 1/ 03240
C% J+ ) %&amp;&lt;: J+ ?) =?#&amp;@ while the 0HA + resulting log- — the possi-bility to obtain SOV word order with a instead of a also applies here.
"In categorial grammar[REF_CITE], the pro-duction of logical forms is essentially AH2 % b 2 :&quot;d  on the association of pairs based 0 with lambda terms representing the logical form of the items, and on the application of the Curry-Howard homomorphism: each ( or ) -elimination rule translates into application and each introduction step into abstraction."
Compo-sitionality assumes that each step in a derivation is associated with a semantical operation.
"In generative grammar[REF_CITE], the production of logical forms is in last part of the derivation, performed after the so-called Spell Out point, and consists in movements of the semanti-cal features only."
"Once this is done, two forms can be extracted from the result of the derivation: a phonological form and a logical one."
"These two approaches are therefore very differ- ent, but we can try to make them closer by replac-ing semantic features by lambda-terms and using some canonical transformations on the derivation trees."
"Instead of converting directly the derivation tree obtained by composition of types, something which is not possible in our translation of mini-malist grammars, we extract a logical tree from the previous, and use the operations of Curry-Howard on this extracted tree."
"Actually, this ex-tracted tree is also a deduction tree: it represents the proof we could obtain in the semantic compo-nent, by combining the semantic types associated with the syntactic ones (by a homomorphism to specify)."
Such a proof is in fact a proof in im-plicational intuitionistic linear logic.
"Coindexed nodes refer to ancient hypotheses which have been discharged simultaneously, thus resulting in phonological features and semantical ones at their right place [Footnote_3] ."
"3 For the time being, we make abstraction of the repre-sentation of time, mode, aspect... that would be supported by the inflection category."
"By extracting the subtree the leaves of which are full of semantic content, we obtain a structure that can be easily seen as a composition: (peter)((mary)(to love))"
"If we replace these ”semantic features” by -terms, we have: )   )d 30 240 %9+ )   ) %&amp;:&lt;+  N &lt;: ?= &amp;# @ 0 ) *: N + + +"
"This shows that necessarily raised constituants in the structure are not only ”syntactically” raised but also 30 240 ”semantically 9% +   ) d ” lifted, in the sense that is the high order representation of the individual peter 4 ."
Let us look at now the example: mary seems to work From the lexicon in figure 4 we obtain the deduction tree given in the same figure.
"This time, it is not so easy to obtain the logical representation:"
A 0H0&amp; ) 2 # $! &amp;# % c) 89%;:&lt;+ +
"The best way to handle this situation consists in assuming that: plies to a variable which occupies the -the verbal infinitive N head (here to work) ap-position, the semantics of the main verb (here to A &quot;0 0 ) ) # $! &amp;# % c) + + seem &quot; applies 2 to N the result, in order to obtain, N variable N &quot;A &quot;0 0 is ) 2 # ! $#&amp;% c) + + abstracted N in order the to obtain just be-fore the semantic content of the specifier   ) %&amp;&lt;: + ) applies. (here the nominative position, occupied by"
This shows that the semantic tree we want to extract from the derivation tree in types logic is not simply the subtree the leaves of which are se-mantically full.
We need in fact some transforma-tion which is simply the stretching of some nodes.
These stretchings correspond to 2 -introduction steps in a Natural deduction tree.
"They are al-lowed each time a variable has been used before, which is not yet discharged and they necessarily occur just before a semantically full content of a specifier node (that means in fact a node labelled by a functional feature) applies."
"Actually, if we say that the tree so obtained repre-sents a deduction in a natural deduction format, we have to specify which formulae it uses and what is the conclusion formula."
We must there-fore define a homomorphism between syntactic and semantic types.
Let be this homomorphism.
"We shall assume: ( )=t, ( ) 3 t, ) /45276 &amp;+ , ( )=e, M)   + = M) 9 .8 + = ) H  ) (+ :2 H ) 9 + + , &lt;; = H ) = + &gt; [ 3 ) ) /?[Footnote_4] 2A a @ + @ +) /@@ + [Footnote_5]"
"4 It is important to notice that if we consider a typed lambda term, we must only assume it is of some type freely raised from , something we can represent by , where X is a type-variable, here X = &quot;!# because $ % &amp;(*&apos; ) +,./- %$ has type &quot;!"
"5 X is a variable of type. This may appear as non-determinism but the instantiation of X is always unique. Moreover, when D is of type , it is in fact en-dowed with the identity function, something which happens everytime D is linked by a chain to a higher node."
"With this homomorphism of labels, the transfor-mation of trees consisting in stretching ”interme-diary projection nodes” and erasing leaves with-out semantic content, we obtain from the deriva-tion tree of the second example, the following ”se-mantic” tree: where coindexed nodes are linked by the dis-charging relation."
Let us notice that the characteristic weak or strong of the features may often be encoded in the lexi-cal entries.
"For instance, Head-movement from V to I is expressed by the fact that tensed verbs are such that: the full phonology is associated with the in-flection component, the empty phonology and the semantics are associated with the second one, the empty semantics occupies the first one [Footnote_6]"
6 as long we don’t take a semantical representation of tense and aspect in consideration.
"Unfortunately, such rigid assignment does not always work."
"For instance, for phrasal movement (say of a to a ) that depends of course on the particular -node in the tree (for instance the sit-uation is not necessary the same for nominative and for accusative case)."
"In such cases, we may assume that multisets are associated with lexical entries instead of vectors."
"Let us try now to enrich this lexicon by consid-ering other phenomena, like reflexive pronouns."
The assignment for himself is given in fig-ure 5 — where the semantical 0 ) 0 type 2 + + of himself ) 0 assumed to be ) ) is 2 + +.
"We obtain for paul shaves himself as the syntactical tree something similar to the tree obtained for our first little example (peter loves mary), and the semantic tree is given in figure 5."
"In our setting, parsing is reduced to proof search, it is even optimized proof-search: indeed the re- striction on types, and on the structure of proof imposed by the shortest move principle and the absence of introduction rules considerably reduce the search space, and yields a polynomial algo-rithm."
Nevertheless this is so when traces are known: otherwise one has to explore the possible places of theses traces.
Here we did focus on the interface with se-mantics.
"Another excellent property of categorial grammars is that they allow — especially when there are no introduction rules — for learning al-gorithms, which are quite efficient when applied to structured data."
This kind of algorithm applies here as well when the input of the algorithm are derivations.
"In this paper, we have tried to bridge a gap be-tween minimalist program and the logical view of categorial grammar."
"We thus obtained a de-scription of minimalist grammars which is quite formal and allows for a better interface with se-mantics, and some usual algorithms for parsing and learning."
"This paper focuses on the analysis and prediction of so-called aware sites, defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error."
"We describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus, which re-veal significant prosodic differences between such turns, compared with turns that ‘correct’ speech recogni-tion errors as well as with ‘normal’ turns that are neither aware sites nor corrections."
"We then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn, a correction, and/or an aware site."
This paper describes new results in our continu-ing investigation of prosodic information as a po-tential resource for error recovery in interactions between a user and a spoken dialogue system.
"In human-human interaction, dialogue partners ap-ply sophisticated strategies to detect and correct communication failures so that errors of recog-nition and understanding rarely lead to a com-plete breakdown of the interacti[REF_CITE]."
"In particular, various stud-ies have shown that prosody is an important cue in avoiding such breakdown, e.g.[REF_CITE]."
"Human-machine interactions between a user and a spoken dialogue system (SDS) ex-hibit more frequent communication breakdowns, due mainly to errors in the Automatic Speech Re-cognition (ASR) component of these systems."
"In such interactions, however, there is also evidence showing prosodic information may be used as a resource for error recovery."
"In previous work, we identified new procedures to detect recogni-tion errors."
"In particular, we found that pros-odic features, in combination with other inform-ation already available to the recognizer, can dis-tinguish user turns that are misrecognized by the system far better than traditional methods used in ASR rejecti[REF_CITE]."
"We also found that user corrections of system misrecognitions exhibit certain typical prosodic features, which can be used to identify such turns[REF_CITE]."
"These findings are consistent with previ-ous research showing that corrections tend to be hyperarticulated — higher, louder, longer ...than other turns[REF_CITE]."
"In the current study, we focus on another turn category that is potentially useful in error hand-ling."
"In particular, we examine what we term aware sites — turns where a user, while interact-ing with a machine, first becomes aware that the system has misrecognized a previous user turn."
"Note that such aware sites may or may not also be corrections (another type of post-misrecognition turn), since a user may not immediately provide correcting information."
"We will refer to turns that are both aware sites and corrections as corr-awares, to turns that are only corrections as corrs, to turns that are only aware sites as awares, and to turns that are neither aware sites nor corrections as norm."
We believe that it would be useful for the dialogue manager in an SDS to be able to de-tect aware sites for several reasons.
"First, if aware sites are detectable, they can function as backward-looking error-signaling devices, mak-ing it clear to the system that something has gone wrong in the preceding context, so that, for ex-ample, the system can reprompt for information."
"In this way, they are similar to what others have termed ‘go-back’ signals[REF_CITE]."
"Second, aware sites can be used as forward-looking signals, indicating upcoming corrections or more drastic changes in user behavior, such as complete restarts of the task."
"Given that, in current systems, both corrections and restarts of-ten lead to recognition error[REF_CITE], aware sites may be useful in preparing systems to deal with such problems."
"In this paper, we investigate whether aware sites share acoustic properties that set them apart from normal turns, from corrections, and from turns which are both aware sites and corrections."
"We also want to test whether these different turn categories can be distinguished automatically, via their prosodic features or from other features known to or automatically detectible by a spoken dialogue system."
"Our domain is the TOOT spoken dialogue corpus, which we describe in Section 2."
"In Section 3, we present some descriptive findings on different turn categories in TOOT ."
Section 4 presents results of our machine learning experi-ments on distinguishing the different turn classes.
In Section 5 we summarize our conclusions.
The TOOT corpus was collected using an experi-mental SDS developed for the purpose of compar-ing differences in dialogue strategy.
"It provides access to train information over the phone and is implemented using an internal platform com-bining ASR, text-to-speech, a phone interface, and modules for specifying a finite-state dialogue manager, and application functions."
"Subjects per-formed four tasks with versions of TOOT , which varied confirmation type and locus of initiative (system initiative with explicit system confirma-tion, user initiative with no system confirmation until the end of the task, mixed initiative with im-plicit system confirmation), as well as whether the user could change versions at will using voice commands."
The exchanges were recorded and the system and user behavior logged automatic-ally.
Dialogues were manually transcribed and user turns automatically compared to the corres-ponding ASR (one-best) recognized string to pro-duce a word accuracy score (WA) for each turn.
"Each turn’s concept accuracy (CA) was labeled by the experimenters from the dialogue recordings and the system log; if the recognizer correctly cap-tured all the task-related information given in the user’s original input (e.g. date, time, departure or arrival cities), the turn was given a CA score of 1, indicating a semantically correct recognition."
"Otherwise, the CA score reflected the percentage of correctly recognized task concepts in the turn."
"For the study described below, we examined 2328 user turns from 152 dialogues generated during these experiments. 194 of the 2320 turns were re-jected by the system."
"To identify the different turn categories in the corpus, two authors independently labeled each turn as to whether or not it constituted a correction of a prior system failure (a CA error or a rejection) and what turn was being corrected, and whether or not it represented an aware site for a prior fail-ure, and, if so, the turn which the system had failed on."
Labeler disagreement was subsequently re-solved by consensus.
"The fragment in Figure 1, produced with a version of TOOT in which the user has the initiative with no confirmation until the end of the task, illustrates these labels."
"This example illustrates cases of corraware, in which both the user’s awareness and correction of a mis-recognition occur in the same turn (e.g. turns 1159 and 1160, after system prompts for informa-tion already given in turn 1158)."
It also illustrates cases in which aware sites and corrections occur in different turns.
"For example, after the immedi-ate explicit system confirmation of turn 1162, the user first becomes aware of the system errors (turn 1163), then separately corrects them (turn 1164); turn 1163 is thus an aware turn and turn 1164 a corr."
"When no immediate confirmation of an ut-terance occurs (as with turn 1158), it may take sev-eral turns before the user becomes aware of any misrecognition errors."
"For example, it is not un-til turn 1161 that the user first becomes aware of the error in date and time from 1158; the user then corrects the error in 1162."
"Note that corr turns represent 13% of the turns in our corpus, awares represent 14%, corrawares account for 16%, and norm turns represent 57% of the turns in the cor-pus."
"We examined prosodic features for each user turn which had previously been shown to be useful for predicting misrecognized turns and corrections: maximum and mean fundamental frequency val-ues (F0 Max, F0 Mean), maximum and mean en-ergy values (RMS Max, RMS Mean), total dur-ation (Dur), length of pause preceding the turn (Ppau), speaking rate (Tempo) and amount of si-lence within the turn (%Sil)."
"F0 and RMS val-ues, representing measures of pitch excursion and loudness, were calculated from the output of En-tropic Research Laboratory’s pitch tracker, get f0, with no post-correction."
Timing variation was represented by four features.
Duration within and length of pause between turns was computed from the temporal labels associated with each turn’s be- ginning and end.
"Speaking rate was approximated in terms of syllables in the recognized string per second, while %Sil was defined as the percentage of zero frames in the turn, i.e., roughly the per-centage of time within the turn that the speaker was silent."
"To see whether the different turn categories were prosodically distinct from one another, we applied the following procedure."
We first calcu-lated mean values for each prosodic feature for each of the four turn categories produced by each individual speaker.
"So, for speaker A, we divided all turns produced into four classes."
"For each class, we then calculated mean F0 Max, mean F0 Mean, and so on."
"After this step had been repeated for each speaker and for each feature, we then cre-ated four vectors of speaker means for each indi-vidual prosodic feature."
"Then, for each prosodic feature, we ran a one-factor within subjects anova on the means to learn whether there was an overall effect of turn category."
"Table 1 shows that, overall, the turn categor-ies do indeed differ significantly with respect to different prosodic features; there is a signific-ant, overall effect of category on F0 Max, RMS Max, RMS Mean, Duration, Tempo and %Sil."
"To identify which pairs of turns were significantly different where there was an overall significant ef-fect, we performed posthoc paired t-tests using the Bonferroni method to adjust the p-level to 0.008 (on the basis of the number of possible pairs that can be drawn from an array of 4 means)."
"Res-ults are summarized in Table 2, where ‘ + ’ or ‘ – ’ indicates that the feature value of the first cat-egory is either significantly higher or lower than the second."
"Note that, for each of the pairs, there is at least one prosodic feature that distinguishes the categories significantly, though it is clear that some pairs, like aware vs. corr and norm vs. corr appear to have more distinguishing features than others, like norm vs. aware."
"It is also interesting to see that the three types of post-error turns are in-deed prosodically different: awares are less prom-inent in terms of F0 and RMS maximum than cor-rawares, which, in turn, are less prominent than corrections, for example."
"In fact, awares, except for duration, are prosodically similar to normal turns."
"We next wanted to determine whether the pros-odic features described above could, alone or in combination with other automatically avail-able features, be used to predict our turn categor-ies automatically."
This section describes experi-ments using the machine learning program RIP - PER[REF_CITE]to automatically induce pre-diction models from our data.
"Like many learn-ing programs, RIPPER takes as input the classes to be learned, a set of feature names and possible values, and training data specifying the class and feature values for each training example."
"RIPPER outputs a classification model for predicting the class of future examples, expressed as an ordered set of if-then rules."
"The main advantages of RIP - PER for our experiments are that RIPPER supports “set-valued” features (which allows us to repres-ent the speech recognizer’s best hypothesis as a set of words), and that rule output is an intuitive way to gain insight into our data."
"In the current experiments, we used 10-fold cross-validation to estimate the accuracy of the rulesets learned."
Our predicted classes corres-pond to the turn categories described in Section 2 and variations described below.
"We repres-ent each user turn using the feature set shown in Figure 2, which we previously found useful for predicting corrections[REF_CITE]."
"A subset of the features includes the automatic-ally computable raw prosodic features shown in Table 1 (Raw), and normalized versions of these features, where normalization was done by first turn (Norm1) or by previous turn (Norm2) in a dialogue."
"The set labeled ‘ASR’ contains stand-ard input and output of the speech recognition pro-cess, which grammar was used for the dialogue state the system believed the user to be in (gram),"
"Raw: f0 max, f0 mean, rms max, rms mean, dur, ppau, tempo, %sil;"
"Norm1: f0 max1, f0 mean1, rms max1, rms mean1, dur1, ppau1, tempo1, %sil1;"
"Norm2: f0 max2, f0 mean2, rms max2, rms mean2, dur2, ppau2, tempo2, %sil2;"
"ASR: gram, str, conf, ynstr, nofeat, canc, help, wordsstr, syls, rejbool;"
"System Experimental: inittype, conftype, adapt, realstrat;"
Dialogue Position: diadist;
"PreTurn: features for preceding turn (e.g., pref0max);"
"PrepreTurn: features for preceding preceding turn (e.g., ppref0max);"
"Prior: for each boolean-valued feature (ynstr, nofeat, canc, help, rejbool), the number/percentage of prior turns exhibiting the feature (e.g., prioryn-strnum/priorynstrpct);"
"PMean: for eachcontinuous-valuedfeature, the mean of the feature’s value over all prior turns (e.g., pmnf0max); the system’s best hypothesis for the user input (str), and the acoustic confidence score produced by the recognizer for the turn (conf)."
"As subcases of the str feature, we also included whether or not the recognized string included the strings yes or no (ynstr), some variant of no such as nope (nofeat), cancel (canc), or help (help), as these lexical items were often used to signal problems in our sys-tem."
We also derived features to approximate the length of the user turn in words (wordsstr) and in syllables (syls) from the str features.
And we ad-ded a boolean feature identifying whether or not the turn had been rejected by the system (rejbool).
"Next, we include a set of features representing the system’s dialogue strategy when each turn was produced."
"These include the system’s current ini-tiative and confirmation strategies (inittype, conf-type), whether users could adapt the system’s dia-logue strategies (adapt), and the combined initiat-ive/confirmation strategy in effect at the time of the turn (realstrat)."
"Finally, given that our previ-ous studies showed that preceding dialogue con-text may affect correction behavior[REF_CITE], we included a fea- ture (diadist) reflecting the distance of the current turn from the beginning of the dialogue, and a set of features summarizing aspects of the prior dia-logue: for the latter features, we included both the number of times prior turns exhibited certain char-acteristics (e.g. priorcancnum) and the percent-age of the prior dialogue containing one of these features (e.g. priorcancpct)."
We also examined means for all raw and normalized prosodic fea-tures and some word-based features over the en-tire dialogue preceding the turn to be predicted (pmn ).
"Finally, we examined more local con-texts, including all features of the preceding turn (pre ) and for the turn preceding that (ppre )."
"We provided all of the above features to the learning algorithm first to predict the four-way classification of turns into normal, aware, corr and corraware."
"A baseline for this classification (al-ways predicting norm, the majority class) has a success rate of 57%."
"Compared to this, our fea-tures improve classification accuracy to 74.23% (+/– 0.96%)."
Figure 3 presents the rules learned for this classification.
"Of the features that appear in the ruleset, about half are features of current turn and half features of the prior context."
"Only once does a system feature appear, suggesting that the rules generalize beyond the experimental con-ditions of the data collection."
"Of the features spe-cific to the current turn, prosodic features domin-ate, and, overall, timing features (dur and tempo especially) appear most frequently in the rules."
"About half of the contextual features are prosodic ones and half are ASR features, with ASR confid-ence score appearing to be most useful."
ASR fea-tures of the current turn which appear most often are string-based features and the grammar state the system used for recognizing the turn.
There appear to be no differences in which type of fea-tures are chosen to predict the different classes.
"If we express the prediction results in terms of precision and recall, we see how our classification accuracy varies for the different turn categories (Table 3)."
"From Table 3, we see that the majority class (normal) is most accurately classified."
"Pre-dictions for the other three categories, which oc-cur about equally often in our corpus, vary consid-erably, with modest results for corr and corraware, and rather poor results for aware."
"Table 4 shows a confusion matrix for the four classes, produced by if (gram=universal) (dur2 7.31) then CORR if (dur2 2.19) (priornofeatpct 0.09) (tempo 1.50) ( if (dur2 1.53) (pmnwordsstr 2.06) (tempo1 1.07) pmntempo 2.39) then CORR (predur 0.80) (prenofeat=F) (presyls 4) then CORR if (predur1 0.26) (dur 0.79) (rmsmean2 1.51) (f0mean 173.49) then CORR if (dur2 1.41) (prenofeat=T) (str contains word ‘eight’) then CORR if (predur1 0.18) (dur2 4.21) (dur1 0.50) (f0mean if (predur1 0.19) (ppregram=cityname) (rmsmax1 1.10 applying our best ruleset to the whole corpus."
"This matrix clearly shows a tendency for the minority classes, aware, corr and corraware, to be falsely classified as normal."
It also shows that aware and corraware are more often confused than the other categories.
"These confusability results motivated us to col-lapse the aware and corraware into one class, which we will label isaware; this class thus rep-resents all turns in which users become aware of a problem."
"From a system perspective, such a 3-way classification would be useful in identify-ing the existence of a prior system failure and in further identifying those turns which simply rep-resent corrections; such information might be as useful, potentially, as the 4-way distinction, if we could achieve it with greater accuracy."
"Indeed, when we predict the three classes (isaware, corr, and norm) instead of four, we do improve in predictive power — from 74.23% to 81.14% (+/– 0.83%) classification success."
"Again, this compares to the baseline (predicting norm, which is still the majority class) of 57%."
"We also get a corresponding improvement in terms of precision and recall, as shown in Table 5, with the isaware category considerably better distin-guished than either aware or corraware in Table 3."
The ruleset for the 3-class predictions is given in
The distribution of features in this rule-set is quite similar to that in Figure 3.
"However, there appear to be clear differences in which fea-tures best predict which classes."
"First, the features used to predict corrections are balanced between those from the current turn and features from the preceding context, whereas isaware rules primar-ily make use of features of the preceding context."
"Second, the features appearing most often in the rules predicting corrections are durational features (dur2, predur1, dur), while duration is used only once in isaware rules."
"Instead, these rules make considerable use of the ASR confidence score of the preceding turn; in cases where aware turns im-mediately follow a rejection or recognition error, one would expect this to be true."
Isaware rules also appear distinct from correction rules in that they make frequent use of the tempo feature.
"It is also interesting to note that rules for predicting isaware turns make only limited use of the nofeat feature, i.e. whether or not a variant of the word no appears in the turn."
"We might expect this lex-ical item to be a more useful predictor, since in the explicit confirmation condition, users should become aware of errors while responding to a re-quest for confirmation."
"Note that corrections, now the minority class, are more poorly distinguished than other classes in our 3-way classification task (Table 5)."
"In a third set of experiments, we merged corrections with normal turns to form a 2-way distinction over all between aware turns and all others."
"Thus, we only distinguish turns in which a user first becomes aware of an ASR failure (our original isaware and corraware categories) from those that are not (our original corr and norm categories)."
"Such a dis- tinction could be useful in flagging a prior sys-tem problem, even though it fails to target the ma-terial intended to correct that problem."
"For this new 2-way distinction, we obtain a higher de-gree of classification accuracy than for the 3-way classification — 87.80% (+/– 0.61%) compared to 81.14%."
"Note, however, that the baseline (predict majority class of !isaware) for this new classifica-tion is 70%, considerably higher than the previous baseline."
"Table 6 shows the improvement in terms of accuracy, precision, and recall."
"The ruleset for the 2-way distinction is shown in in these rules are similar to those in the previous two rulesets in some ways, but quite different in others."
"Like the rules in Figures 3 and 4, they ap-pear independent of system characteristics."
"Also, of the contextual features appearing in the rules, about half are prosodic features and half ASR-related; and, of the current turn features, pros-odic features dominate."
And timing features again (especially tempo) dominate the prosodic features that appear in the rules.
"However, in contrast to previous classification rulesets, very few features of the current turn appear in the rules at all."
"So, it would seem that, for the broader classification task, contextual features are far more important than for the more fine-grained distinctions."
"Continuing our earlier research into the use of prosodic information to identify system misrecog-nitions and user corrections in a SDS, we have studied aware sites, turns in which a user first no-tices a system error."
"We find first that these sites have prosodic properties which distinguish them from other turns, such as corrections and normal turns."
"Subsequent machine learning experiments distinguishing aware sites from corrections and from normal turns show that aware sites can be classified as such automatically, with a consid-erable degree of accuracy."
"In particular, in a 2-way classification of aware sites vs. all other turns we achieve an estimated success rate of 87.8%."
"Such classification, we believe, will be especially useful in error-handling for SDS."
"We have pre-viously shown that misrecognitions can be clas-sified with considerable accuracy, using prosodic and other automatically available features."
"With our new success in identifying aware sites, we acquire another potentially powerful indicator of prior error."
"Using these two indicators together, we hope to target system errors considerably more accurately than current SDS can do and to hypo-thesize likely locations of user attempts to correct these errors."
"Our future research will focus upon combining these sources of information identify-ing system errors and user corrections, and invest-igating strategies to make use of this information, including changes in dialogue strategy (e.g. from user or mixed initiative to system initiative after errors) and the use of specially trained acoustic models to better recognize corrections."
This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting.
"It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes."
The semantic nodes are then used to infer the topic within an input document.
"Experiments[REF_CITE]data set demonstrate that SPN is able to capture the semantics of topics, and it performs well on topic spotting task."
Topic spotting is the problem of identifying the presence of a predefined topic in a text document.
"More formally, given a set of n topics together with a collection of documents, the task is to determine for each document the probability that one or more topics is present in the document."
"Topic spotting may be used to automatically assign subject codes to newswire stories, filter electronic emails and on-line news, and pre-screen document in information retrieval and information extraction applications."
"Topic spotting, and its related problem of text categorization, has been a hot area of research for over a decade."
"A large number of techniques have been proposed to tackle the problem, including: regression model, nearest neighbor classification, Bayesian probabilistic model, decision tree, inductive rule learning, neural network, on-line learning, and, support vector machine (Yang &amp;[REF_CITE]; Tzeras &amp;[REF_CITE])."
"Most of these methods are word-based and consider only the relationships between the features and topics, but not the relationships among features."
"It is well known that the performance of the word-based methods is greatly affected by the lack of linguistic understanding, and, in particular, the inability to handle synonymy and polysemy."
"A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp;[REF_CITE]), to word-sense disambiguation (Chen &amp;[REF_CITE]; Ide &amp;[REF_CITE]) and context (Cohen &amp;[REF_CITE]; Jing &amp;[REF_CITE])."
"The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding ([REF_CITE]; Lee &amp;[REF_CITE]; Sarkas &amp;[REF_CITE]; Wang &amp;[REF_CITE])."
"Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting."
"However, there have been few attempts in this direction."
This is mainly because of difficulties in automatically constructing the semantic networks for the topics.
"In this paper, we propose an approach to automatically build a semantic perceptron net (SPN) for topic spotting."
The SPN is a connectionist model with hierarchical structure.
"It uses a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words to form basic semantic nodes."
The semantic nodes are then used to identify the topic.
"This paper discusses the design, implementation and testing of an SPN for topic spotting."
The paper is organized as follows.
"Section 2 discusses the topic representation, which is the prototype structure for SPN."
"Sections 3 &amp; 4 respectively discuss our approach to extract the semantic correlations between words, and build semantic groups and topic tree."
"Section 5 describes the building and training of SPN, while Section 6 presents the experiment results."
"Finally, Section 7 concludes the paper."
The frame[REF_CITE]is a well-known knowledge representation technique.
"A frame represents a high-level concept as a collection of slots, where each slot describes one aspect of the concept."
The situation is similar in topic spotting.
"For example, the topic “water” may have many aspects (or sub-topics)."
"One sub-topic may be about “water supply”, while the other is about “water and environment protection”, and so on."
"These sub-topics may have some common attributes, such as the word “water”, and each sub-topic may be further sub-divided into finer sub-topics, etc."
"The above points to a hierarchical topic representation, which corresponds to the hierarchy of document classes (Figure 1)."
"In the model, the contents of the topics and sub-topics (shown as circles) are modeled by a set of attributes, which is simply a group of semantically related words (shown as solid elliptical shaped bags or rectangles)."
The context (shown as dotted ellipses) is used to identify the exact meaning of a word.
"However, the method is not suitable when the set of training examples is sparse."
"To avoid the problem of automatically constructing the hierarchical model,[REF_CITE]required the users to supply the model, which is used as queries in the system."
"Most automated methods, however, avoided this problem by modeling the topic as a feature vector, rule set, or instantiated example (Yang &amp;[REF_CITE])."
"These methods typically treat each word feature as independent, and seldom consider linguistic factors such as the context or lexical chain relations among the features."
"As a result, these methods are not good at discriminating a large number of documents that typically lie near the boundary of two or more topics."
"In order to facilitate the automatic extraction and modeling of the semantic aspects of topics, we adopt a compromise approach."
We model the topic as a tree of concepts as shown in Figure 1.
"However, we consider only one level of hierarchy built from groups of semantically related words."
These semantic groups may not correspond strictly to sub-topics within the domain.
Figure 2 shows an example of an automatically constructed topic tree on “water”.
"In Figure 2, node “a” contains the common feature set of the topic; while nodes “b”, “c” and “d” are related to sub-topics on “water supply”, “rainfall”, and “water and environment protection” respectively."
"Node “e” is the context of the word “plant”, and node “f” is the context of the word “bank”."
"Here we use training to automatically resolve the corresponding relationship between a node and an attribute, and the context word to be used to select the exact meaning of a word."
"From this representation, we observe that: a) Nodes “c” and “d” are closely related and may not be fully separable."
"In fact, it is sometimes difficult even for human experts to decide how to divide them into separate topics. b) The same word, such as “water”, may appear in both the context node and the basic semantic node. c)"
"Some words use context to resolve their meanings, while many do not need context."
"Although there exists many methods to derive the semantic correlations between words ([REF_CITE]; Karov &amp;[REF_CITE]), we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and context-based correlation."
"WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguati[REF_CITE]."
"In WordNet, the sense of a word is represented by a list of synonyms (synset), and the lexical information is represented in the form of a semantic network."
"However, it is well known that the granularity of semantic meanings of words in WordNet is often too fine for practical use."
We thus need to enlarge the semantic granularity of words in practical applications.
"For example, given a topic on “children education”, it is highly likely that the word “child” will be a key term."
"However, the concept “child” can be expressed in many semantically related terms, such as “boy”, “girl”, “kid”, “child”, “youngster”, etc."
"In this case, it might not be necessary to distinguish the different meaning among these words, nor the different senses within each word."
"It is, however, important to group all these words into a large synset {child, boy, girl, kid, youngster}, and use the synset to model the dominant but more general meaning of these words in the context."
"In general, it is reasonable and often useful to group lexically related words together to represent a more general concept."
"Here, two words are considered to be lexically related if they are related to by the “is_a”, “part_of”, “member_of”, or “antonym” relations, or if they belong to the same synset."
"Figure 3 lists the lexical relations that we considered, and the examples."
"Since in our experiment, there are many antonyms co-occur within the topic, we also group antonyms together to identify a topic."
"Moreover, if a word had two senses of, say, sense-1 and sense-2."
"And if there are two separate words that are lexically related to this word by sense-1 and sense- 2 respectively, we simply group these words together and do not attempt to distinguish the two different senses."
"The reason is because if a word is so important to be chosen as the keyword of a topic, then it should only have one dominant meaning in that topic."
The idea that a keyword should have only one dominant meaning in a topic is also suggested in Church &amp;[REF_CITE].
"Based on the above discussion, we compute the thesaurus-based correlation between the two terms t 1 and t 2 , in topic T i , as: 1 (t 1 andt 2 areinthesamesynset,ort 1 =t 2 ) 0.8 (t 1 and t 2 have“antonym”relation) (1) R (i) (t ,t )= 0..5 (t 1 andt 2 haverelationsof“is_a”, L 1 2 “part_of”,or“member_of”) 0 (others)"
Co-occurrence relationship is like the global context of words.
"Using co-occurrence statistics, Veling &amp; van der[REF_CITE]was able to find many interesting conceptual groups in the[REF_CITE]text corpus."
"Examples of the conceptual groups found include: {water, rainfall, dry}, {bomb, injured, explosion, injuries}, and {cola, PEP, Pepsi, Pespi-cola, Pepsico}."
"These groups are meaningful, and are able to capture the important concepts within the corpus."
"Since in general, high co-occurrence words are likely to be used together to represent (or describe) a certain concept, it is reasonable to group them together to form a large semantic node."
"Thus for topic T i , the co-occurrence-based correlation of two terms, t 1 and t 2 , is computed as: (i) R co (t 1 , t 2 ) = df (i) (t 1 ∧ t 2 ) / df (i) (t 1 ∨ t 2 ) (2) where df (i) (t 1 ∧ t 2 ) ( df (i)(t1∨ t2) ) is the fraction of documents in T i that contains t 1 and (or) t 2 ."
"Broadly speaking, there are three kinds of context: domain, topic and local contexts (Ide &amp;[REF_CITE])."
Domain context requires extensive knowledge of domain and si not considered in this paper.
Topic context can be modeled approximately using the co-occurrence relationships between the words in the topic.
"In this section, we will define the local context explicitly."
"The local context of a word t is often defined as the set of non-trivial words near t. Here a word wd is said to be near t if their word distance is less than a given threshold, which is set to be 5 in our experiment."
We represent the local context of term t j in topic T i by a context vector cv (i) (t j ).
"To derive cv (i) (t j ), we first rank all candidate context words of t i by their density values: (3) ρ (jki) = m (ji) (wd k )/ n (i) (t j ) where n (i) (t j ) is the number of occurrence of t j in T i , and m (ji) (wd k ) is the number of occurrences of wd k near t j ."
"We then select from the ranking, the top ten words as the context of t j in T i as: cv (i) (t j ) ={(wd (ji1) ,ρ (ji1) ),(wd (ji2) ,ρ (ji2) ),...,(wd (ji10) ,ρ (ji10) ) (4)"
"When the training sample is sufficiently large, the context vector will have good statistic meanings."
"Noting again that an important word to a topic should have only one dominant meaning within that topic, and this meaning should be reflected by its context."
"We can thus draw the conclusion that if two words have a very high context similarity within a topic, it will have a high possibility that they are semantic related."
Therefore it is reasonable to group them together to form a larger semantic node.
"We thus compute the context-based correlation between two term t 1 and t 2 in topic T i as: 10 (i) ∑ R co (wd 1(ik) , wd 2(im)(k) ) * ρ 1(ki) * ρ 2(im) (k) R c(i) (t 1 ,t 2 ) = k=1 (5) [∑(ρ 1(ki) ) 2 ] 1/2 * [∑(ρ (2ik) ) 2 ] 1/ 2 k k where m (k) = arg max R co(i) (wd 1(ki ) , wd 2(is) ) s"
"For example,[REF_CITE]corpus, “company” and “corp” are context-related words within the topic “acq”."
"This is because they have very similar context of “say, header, acquire, contract”."
There are many methods that attempt to construct the conceptual representation of a topic from the original data set (Veling &amp; van der[REF_CITE]; Baker &amp;[REF_CITE]).
"In this Section, we will describe our semantic-based approach to finding basic semantic groups and constructing the topic tree."
"Given a set of training documents, the stages involved in finding the semantic groups for each topic are given below."
"Extract all distinct terms {t 1 , t 2 , ..t n } from the training document set for topic T i ."
"For each term t j , compute its df (i) (t j ) and cv (i) (t j ), where df (i) (t j ) is defined as the fraction of documents in T i that contain t j ."
"In other words, df (i) (t j ) gives the conditional probability of t j appearing in T i ."
Derive the semantic group G j using t j as the main keyword.
Here we use the semantic correlations defined in Section 3 to derive the semantic relationship between t j and any other term t k .
"Thus: For each pair (t j ,t k ), k=1,..n, set Link(t j ,t k )=1 if R L(i) (t j ,t k )&gt;0, or, df (i) (t j )&gt;d 0 and R (i) (t, t )&gt;d 1 or co j k df (i) (t j )&gt;d 2 and R (i) (t, t )&gt;d 3 . c j k where d 0 , d 1 , d 2 , d 3 are predefined thresholds."
"For all t k with Link(t j ,t k )=1, we form a semantic group centered around t j denoted by:"
"G j = {t j 1 ,t j 2 ,...,t j k j }⊆ {t 1 ,t 2 ,...,t n } (6)"
Here t j is the main keyword of node G j and is denoted by main(G j )=t j .
C) Calculate the information value inf (i) (G j ) of each basic semantic group.
"First we compute the information value of each t j : inf (i) (t j ) = df (i) (t j )*max{0, p ij − 1 } (7) N where (i) (t j ) p ij = df N ∑df (i) (t k ) k=1 and N is the number of topics."
"Thus 1/N denotes the probability that a term is in any class, and p ij denotes the normalized conditional probability of t j in T i ."
Only those terms whose normalized conditional probability is higher than 1/N will have a positive information value.
"The information value of the semantic group G j is simply the summation of information value of its constituent terms weighted by their maximum semantic correlation with t j as: k j (8) inf (i) (G j ) = ∑ [w (i) * inf (i) (t k )] k =1 jk where w (jki) =max{R co(i) (t j ,t k ),R c(i) (t j ,t k ),R (Li) (t j ,t k )}"
Select the essential semantic groups using the following algorithm: a) Initialize:
"S ← {G 1 ,G 1 ,..., G n } , Groups ← Φ , b) Select the semantic group with highest information value: j ← arg max (inf (i) (G k )) k G k ∈S c) Terminate if inf (i) (G j ) is less than a predefined threshold d 4 . d) Add G j into the set Groups: S = S − G j , and Groups ← Groups ∪{G j } e) Eliminate those groups in S whose key terms appear in the selected group G j ."
"For each G k ∈ S , if main(G k )∈G j , then S ← S − {G k } f) Eliminate those terms in remaining groups in S that are found in the selected group G j ."
"For each G k ∈S , G k ← G k − G j , and if G k = Φ , then S ← S − {G k } g)"
If S = Φ then stop; else go to step (b).
"In the above grouping algorithm, the predefined thresholds d 0 ,d 1 ,d 2 ,d 3 are used to control the size of each group, and d 4 is used to control the number of groups."
The set of basic semantic groups found then forms the sub-topics of a 2-layered topic tree as illustrated in Figure 2.
The Combination of local perception and global arbitrator has been applied to solve perception problems (Wang &amp;[REF_CITE]; Liu &amp;[REF_CITE]).
Here we adopt the same strategy for topic spotting.
"For each topic, we construct a local perceptron net (LPN), which is designed for a particular topic."
We use a global expert (GE) to arbitrate all decisions of LPNs and to model the relationships between topics.
"Here we discuss the design of both LPN and GE, and their training processes."
We derive the LPN directly from the topic tree as discussed in Sectio n 2 (see Figure 2).
Each LPN is a multi-layer feed-forward neural network with a typical structure as shown in Figure 4.
"In Figure 4, x ij represents the feature value of keyword wd ij in the i th semantic group; x ijk ’s (where k=1,…10 ) represent the feature values of the context words wd ijk ‘s of keyword wd ij ; and a ij denotes the meaning of keyword wd ij as determined by its context."
A i corresponds to the i th basic semantic node.
"The weights w i , w ij , and w ijk and biases è i and è ij are learned from training, and y (i) (x) is the output of the network. x = {(x ij ,cv ij ) | i=1,2,…m, j=1,…i j } where m is the number of basic semantic nodes, i j is the number of key terms contained in the i th semantic node, and cv ij ={x ij1 ,x ij2 … x ijk } is the ij context of term x ij ."
The output y (i) =y (i) (x) is calculated as follows: (9) m y (i) = y (i) (x) = ∑ w i
A i i=1 1 where a ij = x ij * (10) 1+ exp[−( ∑ w ijk * x ijk − θ ij )] x ijk ∈cv ij i j and 1 − exp( − ∑ w i a ij ) (11) j=1
A i = i j 1 + exp( − ∑ w i a ij ) j=1
"For each topic T i , there is a corresponding net y (i) =y (i) (x) and a threshold θ (i) ."
"The pair of (y (i) (x), θ (i) ) is a local binary classifier for T i such that:"
"If y (i) (x)-θ (i) &gt; 0, then T i is present; otherwise T i is not present in document x."
"From the procedures employed to building the topic tree, we know that each feature is in fact an evidence to support the occurrence of the topic."
This gives us the suggestion that the activation function for each node in the LPN should be a non-decreasing function of the inputs.
"Thus we impose a weight constraint on the LPN as: w i &gt;0, w ij &gt;0, w ijk &gt;0 (12)"
"Since there are relations among topics, and LPNs do not have global information, it is inevitable that LPNs will make wrong decisions."
"In order to overcome this problem, we use a global expert (GE) to arbitrate al local decisions."
Figure 5 illustrates the use of global expert to combine the outputs of LPNs.
"Given a document x, we first use each LPN to make a local decision."
We then combine the outputs of LPNs as follows: (13) Y (i) =( y (i) −θ (i) )+ [ W ij (y (j) −θ (j) ) −Θ (i) ]∑ j≠i y(j) −θ(j) &gt;0 where W ij ’s are the weights between the global arbitrator i and the j th LPN; and Θ (i) ’s are the global bias.
"From the result[REF_CITE], we have:"
If Y (i) &gt; 0; then topic T i is present; otherwise T i is not present in document x
"The use[REF_CITE]implies that: a) If a LPN is not activated, i.e., y (i) ≤ θ (i) , then its output is not used in the GE."
"Thus it will not affect the output of other LPN. b) The weight W ij models the relationship or correlation between topic i and j. If W ij &gt; 0, it means that if document x is related to T j , it may also have some contribution (W ij ) to topic T j ."
"On the other hand, if W ij &lt; 0, it means the two topics are negatively correlated, and a document x will not be related to both T j and T i ."
The overall structure of SPN is as follows:
"In order to adopt SPN for topic spotting, we employ the well-known BP algorithm to derive the optimal weights and biases in SPN."
The training phase is divided to two stages.
"The first stage learns a LPN for each topic, while the second stage trains the GE."
"As the BP algorithm is rather standard, we will discuss only the error functions that we employ to guide the training process."
"In topic spotting, the goal is to achieve both high recall and precision."
"In particular, we want to allow y(x) to be as large (or as small) as possible in cases when there is no error, or when x∈ Ω + and y(x)&gt;θ (or x ∈Ω− and y(x) &lt;θ )."
Here Ω + and Ω − denote the positive and negative training document sets respectively.
"To achieve this, we adopt a new error function as follows to train the LPN: |Ω − | |Ω − |+ | Ω + | x∈ ∑ Ω ε + + (y(x),θ) E(w ijk ,θ ij ,w ij ,w i ,θ) = (14) |Ω + | |Ω − |+ |Ω + | x∈ ∑ Ω ε − − (y(x),θ) + where ε + (x,θ) = 12 (x −θ) 2 (x &lt;θ) , and  0 (x ≥θ) ε − (x,θ) =ε + (−x,−θ)"
The coefficients | Ω − | and |Ω − | +| Ω + | | Ω + | are used to ensure that the contributions |Ω − | +| Ω + | of positive and negative examples are equal.
"After the training, we choose the node with the biggest w i value as the common attribute node."
"Also, we trim the topic representation by removing those words or context words with very small w ij or w ijk values."
"We adopt the following error function to train GE: n (15) E(W ij ,Θ i )= ∑[ ∑ ε i+ (Y i (x),Θ i )+ ∑ ε i− (Y i (x),Θ i )] i=1 x∈Ω+i x∈Ω−i where Ω +i is the set of positive examples of T i ."
We employ the ModApte Split version[REF_CITE]corpus to test our method.
"In order to ensure that the training is meaningful, we select only those classes that have at least one document in each of the training and test sets."
This results in 90 classes in both the training and test sets.
"After eliminating documents that do not belong to any of these 90 classes, we obtain a training set of 7,770 documents and a test set of 3,019 documents."
"From the set of training documents, we derive the set of semantic nodes for each topic using the procedures outlined in Section 4."
"From the training set, we found that the average number of semantic nodes for each topic is 132, and the average number of terms in each node is 2.4."
"For illustration, Table 1 lists some examples of the semantic nodes that we found."
"From table 1, we can draw the following general observations. nodes."
Node 1 contains the common attribute set of the topic.
Node 2 is related to the “buying and selling of wheat”.
Node 3 is related to “wheat production”; and node 4 is related to “the effects of insect on wheat production”.
The results show that the automatically extracted basic semantic nodes are meaningful and are able to capture most semantics of a topic. b) Node 1 originally contains two terms “wheat” and “corn” that belong to the same synset found by looking up WordNet.
"However, in the training stage, the weight of the word “corn” was found to be very small in topic “wheat”, and hence it was removed from the semantic group."
This is similar to the discourse based word sense disambiguation. c)
The granularity of information expressed by the semantic nodes may not be the same as what human expert produces.
"For example, it is possible that a human expert may divide node 2 into two nodes {import} and {export, output}. d) Node 5 contains four words and is formed by analyzing context."
Each context vector of the four words has the same two components: “price” and “digital number”.
"Meanwhile, “rise” and “fall” can also be grouped together by “antonym” relation. “fell” is actually the past tense of “fall”."
"This means that by comparing context, it is possible to group together those words with grammatical variations without performing grammatical analysis."
Table 2 summarizes the results of SPN in terms of macro and micro F 1 values (see Yang &amp;[REF_CITE]for definitions of the macro and micro F 1 values).
"For comparison purpose, the Table also lists the results of other TC methods as reported in Yang &amp;[REF_CITE]."
"From the table, it can be seen that the SPN method achieves the best macF 1 value."
This indicates that the method performs well on classes with a small number of training samples.
"In terms of the micro F 1 measures, SPN out-performs NB, NNet, LSF and KNN, while posting a slightly lower performance than that of SVM."
The results are encouraging as they are rather preliminary.
"We expect the results to improve further by tuning the system ranging from the initial values of various parameters, to the choice of error functions, context, grouping algorithm, and the structures of topic tree and SPN."
"In this paper, we proposed an approach to automatically build semantic perceptron net (SPN) for topic spotting."
The SPN is a connectionist model in which context is used to select the exact meaning of a word.
"By analyzing the context and co-occurrence statistics, and by looking up thesaurus, it is able to group the distributed but semantic related words together to form basic semantic nodes."
"Experiments[REF_CITE]show that, to some extent, SPN is able to capture the semantics of topics and it performs well on topic spotting task."
"It is well known that human expert, whose most prominent characteristic is the ability to understand text documents, have a strong natural ability to spot topics in documents."
"We are, however, unclear about the nature of human cognition, and with the present state-of-art natural language processing technology, it is still difficult to get an in-depth understanding of a text passage."
We believe that our proposed approach provides a promising compromise between full understanding and no understanding.
We present a set of algorithms that en-able us to translate natural language sentences by exploiting both a trans-lation memory and a statistical-based translation model.
Our results show that an automatically derived transla-tion memory can be used within a sta-tistical framework to often find trans-lations of higher probability than those found using solely a statistical model.
"The translations produced using both the translation memory and the sta-tistical model are significantly better than translations produced by two com-mercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection, while the commercial systems translated per-fectly only 40-42% of them."
"Over the last decade, much progress has been made in the fields of example-based (EBMT) and statistical machine translation (SMT)."
"EBMT sys-tems work by modifying existing, human pro-duced translation instances, which are stored in a translation memory (TMEM)."
"Many methods have been proposed for storing translation pairs in a TMEM, finding translation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs."
"Others store phrases; new trans-lations are produced by optimally partitioning the input into phrases that match examples from the TMEM[REF_CITE], or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system[REF_CITE]."
"With a few exceptions[REF_CITE], most SMT systems are couched in the noisy chan-nel framework (see Figure 1)."
"In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source. [Footnote_1]"
"1 For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source lan-guage is the language into which the machine translation system translates."
"Most of the current statistical MT systems treat this source as a sequence of words[REF_CITE]. (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases[REF_CITE]or a syntactic tree[REF_CITE].)"
"In the noisy-channel framework, a mono-lingual corpus is used to derive a statistical lan-guage model that assigns a probability to a se-quence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not."
"A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model that explains how the source can be turned into the target and that assigns a probability to every way in which a source e can be mapped into a tar-get f. Once the parameters of the language and translation models are estimated using traditional maximum likelihood and EM techniques[REF_CITE], one can take as input any string in the target language f, and find the source e of highest probability that could have generated the target, a process called decoding (see Figure 1)."
It is clear that EBMT and SMT systems have different strengths and weaknesses.
"If a sen-tence to be translated or a very similar one can be found in the TMEM, an EBMT system has a good chance of producing a good translation."
"How-ever, if the sentence to be translated has no close matches in the TMEM, then an EBMT system is less likely to succeed."
"In contrast, an SMT sys-tem may be able to produce perfect translations even when the sentence given as input does not resemble any sentence from the training corpus."
"However, such a system may be unable to gener-ate translations that use idioms and phrases that reflect long-distance dependencies and contexts, which are usually not captured by current transla-tion models."
This paper advances the state-of-the-art in two respects.
"First, we show how one can use an ex-isting statistical translation model[REF_CITE]in order to automatically derive a statistical TMEM."
"Second, we adapt a decoding algorithm so that it can exploit information specific both to the statistical TMEM and the translation model."
Our experiments show that the automatically de-rived translation memory can be used within the statistical framework to often find translations of higher probability than those found using solely the statistical model.
The translations produced using f both the translation memory and the statisti-cal model are significantly better than translations produced by two commercial systems.
"For the work described in this paper we used a modified version of the statistical machine trans-lation tool developed in the context of the 1999 Johns Hopkins’ Summer Workshop[REF_CITE], which implements IBM translation model 4[REF_CITE]."
IBM model 4 revolves around the notion of word alignment over a pair of sentences (see Fig-ure 2).
The word alignment is a graphical repre-sentation of an hypothetical stochastic process by which a source string e is converted into a target string f.
The probability of a given alignment a and target sentence f given a source sentence e is given by
"P(a, f e) = n e t  e   d  ! #&quot; %$ (&amp;  e &quot; *+) %$ &amp;(  , where the factors delineated by symbols corre-spond to hypothetical steps in the following gen-erative process:"
"A Each English word e is assigned with prob-ability n e a fertility , which corre-sponds to the number of French words into which e is going to be translated."
"A Each English word e is then translated with probability t  e into a French word , where B ranges from 1 to the number of words (fertility of e ) into which e is translated."
"For example, the English word “no” in Figure 2 is a word of fertility 2 that is translated into “aucun” and “ne”."
"A The rest of the factors denote distorsion probabilities (d), which capture the proba-bility that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element (p ), etc."
See[REF_CITE]or[REF_CITE]for a detailed dis-cussion of this translation model and a de-scription of its parameters.
Companies that specialize in producing high-quality human translations of documentation and news rely often on translation memory tools to in-crease their productivity[REF_CITE].
Build-ing high-quality TMEM is an expensive process that requires many person-years of work.
"Since we are not in the fortunate position of having ac-cess to an existing TMEM, we decided to build one automatically."
"We trained IBM translation model 4 on 500,000 English-French sentence pairs from the Hansard corpus."
"We then used the Viterbi alignment of each sentence, i.e., the alignment of highest probability, to extract tuples of the form C D =) D )HGHGHGF)"
"D  ,KHL4),KHL*HGHGHGF) ),K #L E +I ML4&amp; )+&amp;ML*)"
"GHGHGF)+&amp; L*E  , where represents a contiguous English phrase, KHL4,) KHL*%),) K #L E represents a contiguous French phrase, and )+&amp; #)HGHGHGF)+&amp; *L E&amp; represents the Viterbi align-ment between the two phrases."
"We selected only “contiguous” alignments, i.e., alignments in which the words in the English phrase generated only words in the French phrase and each word in the French phrase was generated either by the NULL word or a word from the English phrase."
We extracted only tuples in which the English and French phrases contained at least two words.
"For example, in the Viterbi alignment of the two sentences in Figure 2, which was produced automatically, “there” and “.” are words of fertil-ity 0, NULL generates the French lexeme “.”, “is” generates “est”, “no” generates “aucun” and “ne”, and so on."
"From this alignment we extracted the six tuples shown in Table 1, because they were the only ones that satisfied all conditions mentioned"
"For example, the pair no one ; aucun syn-dicat particulier ne N does not occur in the transla-tion memory because the French word “syndicat” is generated by the word “union”, which does not occur in the English phrase “no one”."
"C D ,I KdI+eN&amp;"
"By extracting all tuples of the form from the training corpus, we ended up with many duplicates and with French phrases that were paired with multiple English translations."
We chose for each French phrase only one possible English translation equivalent.
"We tried out two distinct methods for choosing a translation equiv-alent, thus constructing two different probabilistic TMEMs:"
A The Frequency-based Translation MEMory (FTMEM) was created by associating with each French phrase the English equivalent that occurred most often in the collection of phrases that we extracted.
A The Probability-based Translation MEMory (PTMEM) was created by associating with each French phrase the English equivalent that corresponded to the alignment of high-est probability.
"In contrast to other TMEMs, our TMEMs explic-itly encode not only the mutual translation pairs but also their corresponding word-level align-ments, which are derived according to a certain translation model (in our case, IBM model 4)."
The mutual translations can be anywhere between two words long to complete sentences.
Both methods yielded translation memories that con-tained around 11.8 million word-aligned transla-tion pairs.
"Due to efficiency considerations and memory limitations — the software we wrote loads a complete TMEM into the memory — we used in our experiments only a fraction of the TMEMs, those that contained phrases at most 10 words long."
This yielded a working FTMEM of 4.1 million and a PTMEM of 5.7 million phrase translation pairs aligned at the word level using IBM statistical model 4.
"To evaluate the quality of both TMEMs we built, we extracted randomly 200 phrase pairs from each TMEM."
These phrases were judged by a bilingual speaker as
A perfect translations if she could imagine con-texts in which the aligned phrases could be mutual translations of each other;
A almost perfect translations if the aligned phrases were mutual translations of each other and one phrase contained one single word with no equivalent in the other lan-guage [Footnote_2] ;
"2 For example, the translation pair “final , le secrétaire de” and “final act , the secretary of” were labeled as almost perfect because the English word “act” has no French equiv-alent."
A incorrect translations if the judge could not imagine any contexts in which the aligned phrases could be mutual translations of each other.
The results of the evaluation are shown in Ta-ble 2.
A visual inspection of the phrases in our TMEMs and the judgments made by the evaluator suggest that many of the translations labeled as in-correct make sense when assessed in a larger con-text.
"For example, “autres régions de le pays que” and “other parts of Canada than” were judged as incorrect."
"However, when considered in a con-text in which it is clear that “Canada” and “pays” corefer, it would be reasonable to assume that the translation is correct."
Table 3 shows a few exam-ples of phrases from our FTMEM and their corre-sponding correctness judgments.
"Although we found our evaluation to be ex-tremely conservative, we decided nevertheless to stick to it as it adequately reflects constraints spe-cific to high-standard translation environments in which TMEMs are built manually and constantly checked for quality by specialized teams[REF_CITE]."
statistical TMEM and a statistical translation model
The results in Table 2 show that about 70% of the entries in our translation memory are correct or almost correct (very easy to fix).
"It is, though, an empirical question to what extend such TMEMs can be used to improve the performance of cur-rent translation systems."
"To determine this, we modified an existing decoding algorithm so that it can exploit information specific both to a statisti-cal translation model and a statistical TMEM."
The decoding algorithm that we use is a greedy one — see[REF_CITE]for details.
The decoder guesses first an English translation for the French sentence given as input and then at-tempts to improve it by exploring greedily alter-native translations from the immediate translation space.
We modified the greedy decoder described[REF_CITE]so that it attempts to find good translation starting from two distinct points in the space of possible translations: one point corresponds to a word-for-word “gloss” of the French input; the other point corresponds to a translation that resembles most closely transla-tions stored in the TMEM.
"As discussed[REF_CITE], the word-for-word gloss is constructed by aligning each French word f L with its most likely En-glish translation ef k (ef kml argmax n t(e f L ))."
"For example, in translating the French sentence “Bien entendu , il parle de une belle victoire .”, the greedy decoder initially assumes that a good translation of it is “Well heard , it talking a beauti-ful victory” because the best translation of “bien” is “well”, the best translation of “entendu” is “heard”, and so on."
A word-for-word gloss re - sults (at best) in English words written in French word order.
The translation that resembles most closely translations stored in the TMEM is constructed by deriving a “cover” for the input sentence using phrases from the TMEM.
"The derivation attempts to cover with translation pairs from the TMEM as much of the input sentence as possible, using the longest phrases in the TMEM."
The words in the input that are not part of any phrase extracted from the TMEM are glossed.
"For example, this approach may start the translation process from the phrase “well , he is talking a beautiful victory”"
"C if the TMEM contains the pairs well , ; bien en-"
"C tendu , N and he is talking; il parle N but no pair with the French phrase “belle victoire”."
"If the input sentence is found “as is” in the translation memory, its translation is simply re-turned and there is no further processing."
"Oth-erwise, once an initial alignment is created, the greedy decoder tries to improve it, i.e., it tries to find an alignment (and implicitly a translation) of higher probability by modifying locally the initial alignment."
"The decoder attempts to find align-ments and translations of higher probability by employing a set of simple operations, such as changing the translation of one or two words in the alignment under consideration, inserting into or deleting from the alignment words of fertility zero, and swapping words or segments."
"In a stepwise fashion, starting from the ini-tial gloss or initial cover, the greedy decoder iter-ates exhaustively over all alignments that are one such simple operation away from the alignment under consideration."
"At every step, the decoder chooses the alignment of highest probability, un-til the probability of the current alignment can no longer be improved."
"We extracted from the test corpus a collection of 505 French sentences, uniformly distributed across the lengths 6, 7, 8, 9, and 10."
"For each French sentence, we had access to the human-generated English translation in the test corpus, and to translations generated by two commercial systems."
"We produced translations using three versions of the greedy decoder: one used only the statistical translation model, one used the trans-lation model and the FTMEM, and one used the translation model and the PTMEM."
We initially assessed how often the translations obtained from TMEM seeds had higher proba- bility than the translations obtained from simple glosses.
Tables 4 and 5 show that the transla-tion memories significantly help the decoder find translations of high probability.
Only in about 15-18% of the cases the translations obtained from the gloss are better than the translations obtained from the TMEM seeds.
"It appears that both TMEMs help the decoder find translations of higher probability consistently, across all sentence lengths."
"In a second experiment, a bilingual judge scored the human translations extracted from the automatically aligned test corpus; the transla-tions produced by a greedy decoder that use both TMEM and gloss seeds; the translations produced by a greedy decoder that uses only the statistical model and the gloss seed; and translations pro-duced by two commercial systems (A and B)."
"A If an English translation had the very same meaning as the French original, it was con-sidered semantically correct."
"If the mean-ing was just a little different, the transla- tion was considered semantically incorrect."
"For example, “this is rather provision dis-turbing” was judged as a correct semantical translation of “voilà une disposition plotôt inquiétante”, but “this disposal is rather dis-turbing” was judged as incorrect."
"A If a translation was perfect from a gram-matical perspective, it was considered to be grammatical."
"Otherwise, it was considered incorrect."
"For example, “this is rather pro-vision disturbing” was judged as ungram-matical, although one may very easily make sense of it."
"We decided to use such harsh evaluation criteria because, in previous experiments, we repeatedly found that harsh criteria can be applied consis-tently."
"To ensure consistency during evaluation, the judge used a specialized interface: once the correctness of a translation produced by a system S was judged, the same judgment was automati-cally recorded with respect to the other systems as well."
"This way, it became impossible for a trans-lation to be judged as correct when produced by one system and incorrect when produced by an-other system."
"Table 6, which summarizes the results, displays the percent of perfect translations (both semanti-cally and grammatically) produced by a variety of systems."
Table 6 shows that translations produced using both TMEM and gloss seeds are much bet-ter than translations that do not use TMEMs.
The translation systems that use both a TMEM and the statistical model outperform significantly the two commercial systems.
The figures in Ta-ble 6 also reflect the harshness of our evaluation metric: only 82% of the human translations ex-tracted from the test corpus were considered per-fect translation.
"A few of the errors were gen-uine, and could be explained by failures of the sentence alignment program that was used to cre-ate the corpus[REF_CITE]."
"Most of the er-rors were judged as semantic, reflecting directly the harshness of our evaluation metric."
The approach to translation described in this pa-per is quite general.
It can be applied in con-junction with other statistical translation mod- els.
And it can be applied in conjunction with existing translation memories.
"To do this, one would simply have to train the statistical model on the translation memory provided as input, deter-mine the Viterbi alignments, and enhance the ex-isting translation memory with word-level align-ments as produced by the statistical translation model."
We suspect that using manually produced TMEMs can only increase the performance as such TMEMs undergo periodic checks for qual-ity assurance.
"The work that comes closest to using a sta-tistical TMEM similar to the one we propose here is that[REF_CITE], who au-tomatically derive from a parallel corpus a hier-archical TMEM."
The hierarchical TMEM con-sists of a set of transducers that encode a sim-ple grammar.
The transducers are automatically constructed: they reflect common patterns of us-age at levels of abstractions that are higher than the words.
"From a theoretical perspective, it appears though that the two approaches are complementary:[REF_CITE]identify abstract patterns of usage and then use them during translation."
This may address the data sparseness problem that is char-acteristic to any statistical modeling effort and produce better translation parameters.
"In contrast, our approach attempts to stir the statistical decoding process into directions that are difficult to reach when one relies only on the parameters of a particular translation model."
"For example, the two phrases “il est mort” and “he kicked the bucket” may appear only in one sentence in an arbitrary large corpus."
The pa-rameters learned from the entire corpus will very likely associate very low probability to the words “kicked” and “bucket” being translated into “est” and “mort”.
"Because of this, a statistical-based MT system will have trouble producing a trans-lation that uses the phrase “kick the bucket”, no matter what decoding technique it employs."
"How-ever, if the two phrases are stored in the TMEM, producing such a translation becomes feasible."
"If optimal decoding algorithms capable of searching exhaustively the space of all possible translations existed, using TMEMs in the style presented in this paper would never improve the performance of a system."
"Our approach works because it biases the decoder to search in sub-spaces that are likely to yield translations of high probability, subspaces which otherwise may not be explored."
"The bias introduced by TMEMs is a practical alternative to finding optimal transla-tions, which is NP-complete[REF_CITE]."
"It is clear that one of the main strengths of the TMEM is its ability to encode contextual, long-distance dependencies that are incongruous with the parameters learned by current context poor, reductionist channel models."
"Unfortunately, the criterion used by the decoder in order to choose between a translation produced starting from a gloss and one produced starting from a TMEM is biased in favor of the gloss-based translation."
"It is possible for the decoder to produce a perfect translation using phrases from the TMEM, and yet, to discard the perfect translation in favor of an incorrect translation of higher probability that was obtained from a gloss (or from the TMEM)."
"It would be desirable to develop alternative rank-ing techniques that would permit one to prefer in some instances a TMEM-based translation, even though that translation is not the best according to the probabilistic channel model."
The examples in Table 7 shows though that this is not trivial: it is not always the case that the translation of high- est probability is the perfect one.
The first French sentence in Table 7 is correctly translated with or without help from the translation memory.
"The second sentence is correctly translated only when the system uses a TMEM seed; and fortunately, the translation of highest probability is the one obtained using the TMEM seed."
The translation obtained from the TMEM seed is also correct for the third sentence.
"But unfortunately, in this case, the TMEM-based translation is not the most prob-able."
This work was supported by DARPA-ITO grant[REF_CITE]-00-1-9814.
"In this paper we discuss our approach toward establishing a model of the ac-quisition of English grammatical struc-tures by users of our English language tutoring system, which has been de-signed for deaf users of American Sign Language."
We explore the correlation between a corpus of error-tagged texts and their holistic proficiency scores as-signed by experts in order to draw ini-tial conclusions about what language errors typically occur at different levels of proficiency in this population.
"Since errors made at lower levels (and not at higher levels) presumably represent constructions acquired before those on which errors are found only at higher levels, this should provide insight into the order of acquisition of English grammatical forms."
"There have been many theories of language acqui-sition proposing a stereotypical order of acquisi-tion of language elements followed by most learn-ers, and there has been empirical evidence of such an order among morphological elements of lan-guage (cf.[REF_CITE]) and some syntac-tic structures (cf.[REF_CITE])."
"There is indication that these re-sults may be applied to any L1 group acquiring English[REF_CITE], and some research has focused on develop-ing a general account of acquisition across a broad range of morphosyntactic structures (cf.[REF_CITE])."
"In this work, we explore how our second language instruction sys-tem, ICICLE, has generated the need for model-ing such an account, and we discuss the results of a corpus analysis we have undertaken to fulfill that need."
ICICLE (Interactive Computer Identification and Correction of Language Errors) is an intelli-gent tutoring system currently under development[REF_CITE].
Its primary function is to tutor deaf students on their written English.
Essential to performing that function is the ability to correctly analyze user-generated language er-rors and produce tutorial feedback to student per-formance which is both correct and tailored to the student’s language competence.
"Our target learn-ers are native or near-native users of American Sign Language (ASL), a distinct language from English (cf.[REF_CITE]), so we view the acquisition of skills in written English as the acquisition of a second language for this pop-ulati[REF_CITE]."
"Our system uses a cycle of user input and sys-tem response, beginning when a user submits a piece of writing to be reviewed by the system."
"The system determines the grammatical errors in the writing, and responds with tutorial feedback aimed at enabling the student to perform correc-tions."
"When the student has revised the piece, it is re-submitted for analysis and the cycle begins again."
"As ICICLE is intended to be used by an individual over time and across many pieces of writing, the cycle will be repeated with the same individual many times."
Figure 1 contains a diagram of ICICLE’s over-all architecture and the cycle we have described.
"It executes between the User Interface, the Error Identification Module (which performs the syn-tactic analysis of user writing), and the Response Generation Module (which builds the feedback to the user based on the errors the user has commit-ted)."
"The work described in this paper focuses on the development of one of the sources of knowl-edge used by both of these processes, a compo-nent of the User Model representing the user’s grammatical competence in written English."
What currently exists of the ICICLE system is a prototype application implemented in a graph-ical interface connected to a text parser that uses a wide-coverage English grammar augmented by “mal-rules” capturing typical errors made by our learner population.
"It can recognize and label many grammatical errors, delivering “canned” one- or two-sentence explanations of each error on request."
The user can then make changes and resubmit the piece for additional analysis.
We have discussed[REF_CITE]the performance of our parser and mal-rule-augmented grammar and the unique challenges “She is teach piano on Tuesdays.”
Inappropriate use of auxiliary and verb morphology problems. “She teaches piano on Tuesdays.”
Intermediate: Missing appropriate +ing morphology. “She is teaching piano on Tuesdays.”
Advanced: Botched attempt at passive formation. “She is taught piano on Tuesdays.” faced when attempting to cover non-grammatical input from this population.
"In its current form, when the parser obtains more than one possible parse of a user’s sentence, the interface chooses arbitrarily which one it will assume to be representative of which structures the user was attempting."
"This is undesirable, as one challenge that we face with this particular population is that there is quite a lot of variabil-ity in the level of written English acquisition."
"A large percentage of the deaf population has read-ing/writing proficiency levels significantly below their hearing peers, and yet the population repre-sents a broad range of ability."
"Thus, even when focused on a subset of the deaf population (e.g., deaf high school or college students), there is significant variability in the writing proficiency."
"The impact of this variability is that a particular string of words may have multiple interpretations and the most likely one may depend upon the pro-ficiency level of the student, as illustrated in Fig-ure 2."
We are therefore currently developing a user model to address the system’s need to make these parse selections intelligently and to adapt tutoring choices to the individual[REF_CITE].
The model we are developing is called SLALOM.
"It is a representation of the user’s abil-ity to correctly use each of the grammatical “fea-tures” of English, which we define as incorpo-rating both morphological rules such as plural- izing a noun with +S and syntactic rules such as the construction of prepositional phrases and S V O sentence patterns."
"Intuitively, each unit in SLALOM corresponds to a set of grammar rules and mal-rules which realize the feature."
The information stored in each of these units repre-sents observations based on the student’s perfor-mance over the submission of multiple pieces of writing.
"These observations will be abstracted into three tags, representing performance that is consistently good (acquired), consistently flawed (unacquired), or variable (ZPD [Footnote_1] ) to record the user’s ability to correctly execute each structure in his or her written text."
1 Zone of Proximal Development: see[REF_CITE]for discussion. These are presumably the fea-tures the learner is currently in the process of acquiring and thus we expect to see variation in the user’s ability to execute them.
"A significant problem that we must face in gen-erating the tags for SLALOM elements is that we would like to infer tags on performance ele-ments not yet seen in a writer’s production, bas-ing those tags on what performance we have been able to observe so far."
"We have proposed[REF_CITE]that SLALOM be structured in such a way as to cap-ture these expectations by explicitly representing the relationships between grammatical structures in terms of when they are acquired; namely, indi-cating which features are typically acquired be-fore other features, and which are typically ac-quired at the same time."
"With this information available in the model, SLALOM will be able to suggest that a feature typically acquired be-fore one marked “acquired” is most likely also acquired, or that a feature co-acquired with one marked “ZPD” may also be something currently being mastered by the student."
The corpus anal-ysis we have undertaken is meant to provide this structure by indicating a partial ordering on the acquisition of grammatical features by this popu-lation of learners.
"Having the SLALOM model marked with gram-matical features as being acquired, unacquired, or ZPD will be very useful in at least two different ways."
The first is when deciding which possi-ble parse of the input best describes a particular sentence produced by a learner.
"When there are multiple parses of an input text, some may place the “blame” for detected errors on different con-stituents."
"In order for ICICLE to deliver relevant instruction, it needs to determine which of these possibilities most likely reflects the actual perfor-mance of the student."
We intend for the parse se-lection process to proceed on the premise that fu-ture user performance can be predicted based on the patterns of the past.
"The system can generally prefer parses which use rules representing well-formed constituents associated with “acquired” features, mal-rules from the “unacquired” area, and either correct rules or mal-rules for those fea-tures marked “ZPD.”"
A second place SLALOM will be consulted is in deciding which errors will then become the subjects of tutorial explanations.
This decision is important if the instruction is to be effective.
It is our wish for ICICLE to ignore “mistakes” which are slip-ups and not indicative of a gap in language knowledge[REF_CITE]and to avoid instruction on material beyond the user’s current grasp.
"It therefore will focus on features marked “ZPD”—those in that “narrow shifting zone di-viding the already-learned skills from the not-yet-learned ones”[REF_CITE], or the frontier of the learning process."
ICICLE will select those errors which involve features from this learner’s learning frontier and use them as the topics of its tutorial feedback.
"With the partial order of acquisition repre-sented in the SLALOM model as we have de-scribed, these two processes can proceed on the combination of the data contained in the previous utterances supplied by a given learner and the “in-tuitions” granted by information on typical learn-ers, supplementing empirical data on the specific user’s mastery of grammatical forms with infer-ences on what that means with respect to other forms related to those through the order of acqui-sition."
We have established the need for a description of the general progress of English acquisition as de-termined by the mastery of grammatical forms.
"We have undertaken a series of studies to estab-lish an order-of-acquisition model for our learner population, native users of American Sign Lan-guage."
"In our first efforts, we have been guided by the observation that the errors committed by learn-ers at different stages of acquisition are clues to the natural order that acquisition follows[REF_CITE]."
The theory is that one expects to find er-rors on elements currently being acquired; thus errors made by early learners and not by more advanced learners represent structures which the early learners are working on but which the ad-vanced learners have acquired.
"Having obtained a corpus of writing samples from 106 deaf indi-viduals, we sought to establish “error profiles”— namely, descriptions of the different errors com-mitted by learners at different levels of language competence."
These profiles could then be a piece of evidence used to provide an ordering struc-ture on the grammatical elements captured in the SLALOM model.
This is an overview of the process by which we developed our error profiles:
Goal : to have error profiles that indicate what level of acquisition is most strongly associ-ated with which grammatical errors.
It is important that the errors correspond to our grammar mal-rules so that the system can prefer parses which contain the errors most consistent with the student’s level of acqui-sition.
Method : 1.
Collect writing samples from our user population [Footnote_2].
2 We also discuss why we were satisfied with this score despite only being in the range of what Carletta calls “tenta-tive conclusions.”
Tag samples in a consistent manner with a set of error codes (where these codes have an established correspon-dence with the system grammar) [Footnote_3]. Divide samples into the levels of acqui-sition they represent 4. Statistically analyze errors within each level and compare to the magnitude of occurrence at other levels 5. Analyze resulting findings to determine a progression of competence
"3 Although these samples were relatively homogeneous with respect to the amount of English training and the age of the writer, we expected to see a range of demonstrated proficiency for reasons discussed above. We discuss later why the ratings were not as well spread-out as we expected."
"In[REF_CITE]we discuss the initial steps we took in this process, including the de-velopment of a list of error codes documented by a coding manual, the verification of our manual and coding scheme by testing inter-coder reliabil- ity in a subset of the corpus (where we achieved a Kappa agreement score[REF_CITE]of ) 2 , and the subsequent tagging of the en-tire corpus."
"Once the corpus was annotated with the errors each sentence contained, we obtained expert evaluations of overall proficiency levels performed by ESL instructors using the national Test of Written English (TWE) ratings [Footnote_3] ."
"3 Although these samples were relatively homogeneous with respect to the amount of English training and the age of the writer, we expected to see a range of demonstrated proficiency for reasons discussed above. We discuss later why the ratings were not as well spread-out as we expected."
The ini-tial analysis we go on to describe[REF_CITE]confirmed that clustering algorithms looking at the relative magnitude of different er-rors grouped the samples in a manner which cor-responded to where they appeared in the spectrum of proficiency represented by the corpus.
"The next step, the results of which we discuss here, was to look at each error we tagged and the ability of the level of the writer’s proficiency to predict which errors he or she would commit."
"If we found significant differences in the errors committed by writers of different TWE scores, then we could use the errors to help organize the SLALOM ele-ments, and through that obtain data on which er-rors to expect given a user’s level of proficiency."
"Although our samples were scored on the six-point TWE scale, we had sparse data at either end of the scale (only 5% of the samples occurring in levels 1, 5, and 6), so we concentrated our efforts on the three middle levels (2, 3, and 4), which we renamed low, middle, and high."
Our chosen method of data exploration was Multivariate Analysis of Variance (MANOVA).
An initial concern was to put the samples on equal footing despite the fact that they covered a broad range in length—from 2 to 58 sentences—and there was a danger that longer samples would tend to have higher error counts in every category sim-ply because the authors had more opportunity to make errors.
"We therefore used two dependent variables in our analysis: the TWE score and the length of the sample, testing the ability of the two combined to predict the number of times a given error occurred."
"We ran the MANOVA using both sentence count and word count as possible length variables, and in both runs we obtained many sta-tistically significant differences between the mag-nitude at which writers at different TWE levels committed certain errors."
"These differences are illustrated in Figure 3, which shows the results on a subset of the 47 error code tags for which we got discernible results [Footnote_4] ."
"4 “Activity” refers to the ability to correctly form a gerund-fronted phrase describing an activity, such as “I re-ally like walking the dog;” “comparison phrase” refers to the formation of phrases such as “He is smarter than she;” “voice” refers to the confusion between using active and pas-sive voice, such as “The soloist was sung.”"
"In the figure, a bar indicates that this level of proficiency committed this type of error more fre-quently than the others."
"If two of the three levels are both marked, it means that they both commit-ted the error more frequently than the third, but the difference between those two levels was unre-markable."
"Solid shading indicates results which were statistically significant (with an !#&quot; omnibus testyielding of significance level of ), and inten-sity differences (e.g., black for extra preposition in the low level, but grey in the middle level) in-dicate a difference that was not significant."
"In the example we just mentioned, the low-level writ-ers committed more extra preposition errors than the high-level writers with a significance level of 0.0082, and the mid-level writers also commit-ted more of these errors than the high-level writ-ers with a significance of .0083."
"The compari-son of the low and middle levels to each other, on the other hand, showed that the low-level learners committed more of this error, but that the result was strongly insignificant at .5831."
"The cross-hatched and diagonal-striped results in the figure indicate ! #&quot; results which did not satisfyfor significance, but were con-the cutoff of sidered both interesting and close enough to sig-nificance to be worth noting."
"The diagonal stripes have “less intensity” and thus indicate the same relationship to the cross-hatched bars as the gray does to the black—a difference in the data which indicates a lower occurrence of the error which is not significantly distinguished (e.g., high-level learners committed extra relative pronoun errors less often than mid-level learners, and both high-and mid-level learners committed it more often than the low-level learners), but, again, not to a significant extent."
Notice that the overall shape of the figure sup-ports the notion of an order of acquisition of fea-tures because one can see a “progression” of er-rors from level to level.
"Very strongly support-ive of this intuition are the first and last errors in the figure: “no parse,” indicating that the coder was unable to understand the intent of the sen-tence, statistically more often at the lowest level than the at the other two levels, while “no er-rors found” was significantly most prevalent at the highest ! level (both with a significance level).of"
Other data which is more relevant to our goals also presents itself.
"The lowest level exhibited higher numbers of errors on such elementary lan-guage skills as putting plural markers on nouns, placing adjectives before the noun they modify, and using conjunctions to concatenate clauses correctly."
"Both the low and middle levels strug-gled with many issues regarding forming tenses, and also exhibited “ASLisms” in their English, such as the dropping of constituents which are ei-ther not explicitly realized in ASL (such as de-terminers, prepositions, verb subjects and objects which are established discourse entities in focus, and the verb “TO BE”), or the treatment of certain discourse entities as they would be in ASL (e.g., using “here” as if it were a pronoun)."
"While be-ginning learners struggled with more fundamental problems with subordinate clauses such as miss-ing gaps, the more advanced learners struggled with using the correct relative pronouns to con-nect those clauses to their matrix sentence."
"Where the lower two levels committed more errors with missing determiners, the highest level among our writers had learned the necessity of determin-ers in English but was over-generalizing the rule and using them where they were not appropriate."
"Finally, the upper level learners were beginning to experiment with more complex verb construc-tions such as the passive voice."
All of this begins to draw a picture of the sequence in which these structures are mastered across these levels.
"While Figure 3 is meant to illustrate how the three different levels committed different sets of errors, it is clear that this picture is incomplete."
"The low and middle levels are insufficiently distinguished from each other, and there were very few errors committed most often by the highest level."
"Most importantly, many of the distinctions between lev-els were not achieved to a significant degree."
One of the reasons for these problems is the fact that our samples are concentrated in only three levels in the center of the TWE spectrum.
We hope to address this in the future by acquiring additional samples.
Another problem which addi-tional samples will help to solve is sparseness of data.
"Most of our insignificant differences come from error codes with very low frequency, sometimes occurring as infrequently as 7 times."
"What we have established is promising, how-ever, in that it does show statistically significant data spanning nearly every syntactic category."
"Additional samples must be collected and ana-lyzed to obtain more statistical significance; how-ever, the methodology and approach are proven solid by these results."
"If we were to stop here, then our user model de-sign would simply be to group the SLALOM con-tents addressed by these errors in an order accord-ing to how they fell into the distribution shown in Figure 3, assuming essentially that those errors falling primarily in the low-level group represent structures that are learned first, followed by those in the low/middle overlap area, followed by those which mostly the mid-level writers were strug-gling, followed finally by those which mostly posed problems for our highest-level writers."
"Given this structure, and a general classifica-tion of a given user, if we are attempting to select between competing parses for a sentence written by this user, we can prefer a sentence whose er-rors most closely fit those for the profile to which the user belongs."
"However, up until now we have only gathered information on the errors commit-ted by our learner population, and thus we still have no information on a great deal of gram-matical constructions."
Consider that some types of grammatical constructions may be avoided or used correctly at low levels but that the system would have no knowledge of this.
"By only mod-eling the errors, we fail to capture the acquisition order data provided by knowing what structures a writer can successfully execute at the different levels."
"Therefore, the sparse data problems we faced in this work are only partly explained by the small corpus and some infrequent error codes."
They are also explained by the fact that errors are only one half of the total picture of user perfor-mance.
"Although we experimented in this work with equalizing the error counts using different length measures, we did not have access to the num-bers that would have provided the most meaning-ful normalization: namely, the number of times a structure is attempted."
It is our belief that infor-mation on the successful structures in the users’ writing would give us a much clearer view of the students’ performance at each level.
"Tagging all sentences for the correct structures, however, is an intractable task for a human coder."
"On the other hand, while it is feasible to have this in-formation collected computationally through our parser, we are still faced with the problem of com-peting parses for many sentences."
Our methodol-ogy to address this problem is to use the human-generated error codes to select among the parses trees in order to gather statistics on fully-parsed sentences.
"We have therefore created a modified version of our user interface which, when given a sam-ple of writing from our corpus, records all com-peting parse trees for all sentences to a text file [Footnote_5] ."
5 Thanks are due to Greg Silber for his work on revising our interface and creating this variation.
Another application has been developed to com-pare these system-derived parse trees against the human-assigned error code tags for those same sentences to determine which tree is the closest match to human judgment.
"To do this, each tree is traversed and all constituents corresponding to mal-rules are recorded as the equivalent error code tag."
The competing lists of errors are then compared against the sequence determined by the human coder via a string alignment/comparison algorithm which we discuss[REF_CITE].
"With the “correct” parse trees indicated for each sentence, we will know which grammar con-stituents each writer correctly executed and which others had to be parsed using our mal-rules."
The same statistical techniques described above can then be applied to form performance profiles for capturing statistically significant differences in the grammar rules used by students within each level.
This will give us a much more detailed description of acquisition status on language ele-ments throughout the spectrum represented by our sample population.
"The implication of having such information is that once it is translated into the structure of our SLALOM user model, performance on a previously-unseen structure may be predicted based on what performance profile the user most closely fits and what tag that profile typically as-signs to the structure in question; as mentioned earlier in this text, features typically acquired be-fore a structure on which the user has demon-strated mastery can be assumed to be acquired as well."
Those structures which are well be-yond the user’s area of variable performance (his or her current area of learning) are most likely unacquired.
"Since we view the information in SLALOM as projecting probabilities onto the rules of the grammar, intuitively this will allow the user’s mastery of certain rules to project dif-ferent default probabilities on rules which have not yet been seen in the user’s language usage."
"With this information, ICICLE will then be able to make principled decisions in both pars-ing and tutoring tasks based on a hybrid of direct knowledge about the user’s exhibited proficiency on grammatical structures and the indirect knowl-edge we have derived from typical learning pat-terns of the population."
In this paper we have addressed an empirical ef-fort to establish a typical sequence of acquisition for deaf learners of written English.
Our initial results show much promise and are consistent in many ways with intuition.
"Future work will ap-ply the same methodology but expand beyond the analysis of user errors to the analysis of the com-plete image of user performance, including those structures which a user can successfully execute."
"When completed, our model will enable a com-plex tutoring tool to intelligently navigate through multiple competing parses of user text and to fo-cus language instruction where it will do the most good for the learner, exhibiting a highly desir-able adaptability to a broad range of users and ad-dressing a literacy issue in a population who could greatly benefit from such a tool."
¡J¦«ª3¬­§®a¯ ¤0¢¹¡¤º±»¢¤0Ja¼½²1¬¦«ª3¬¾±»¡¾²³¤|Ç T¥J°¼¦¶C¹¥J¬¤gTa°¢£] J¥J¬¤0 ©·¿¦¨¡¼­§0Â.Ê&apos;¬¤Ë¥JJ¼­©·¥°¦a¶}3¢W¸ ]  ] ¡°¦¶C¹ §JÎW§® ¥J¤0¼Ñ¥J­°©aª* Ó Ao³WCF&apos;IÌ@ ]\ cVYF³@ ÖØ×PÙ0ÚÜÛÝÙ0Úx×PÞ°´«¢mµW¶·²1´¸ ÔPÕxÔ °¤0©¡§J¡¶¸ ¦¶C¹ÒÇ:¥°¬¦«§9ÇC©¡ÇP¤0:§J¡¯]°¹¡¯¤E¥°¬C©·¥ÌTa°¢W¸ Jµº±»¡ ã a¶C§J¦¨¢¤0 ã ] íî0ïað ìmñÉòzó[*ø·ùoúYû·ü9ôaõû·ý þÿ ÿÿ     !&quot;  #$ % &amp;(*)
An approach to automatic detection of syllable boundaries is presented.
We demonstrate the use of several manu-ally constructed grammars trained with a novel algorithm combining the advan-tages of treebank and bracketed corpora training.
We investigate the effect of the training corpus size on the perfor-mance of our system.
The evaluation shows that a hand-written grammar per-forms better on finding syllable bound-aries than does a treebank grammar.
In this paper we present an approach to super-vised learning and automatic detection of sylla-ble boundaries.
The primary goal of the paper is to demonstrate that under certain conditions treebank and bracketed corpora training can be combined by exploiting the advantages of the two methods.
Treebank training provides a method of unambiguous analyses whereas bracketed corpora training has the advantage that linguistic knowl-edge can be used to write linguistically motivated grammars.
"In text-to-speech (TTS) systems, like those de-scribed[REF_CITE], the correct pronuncia-tion of unknown or novel words is one of the biggest problems."
In many TTS systems large pronunciation dictionaries are used.
"However, the lexicons are finite and every natural language has productive word formation processes."
The German language for example is known for its extensive use of compounds.
A TTS system needs a module where the words converted from graphemes to phonemes are syllabified before they can be further processed to speech.
The placement of the correct syllable boundary is es-sential for the application of phonological rules[REF_CITE].
Our approach of-fers a machine learning algorithm for predicting syllable boundaries.
Our method builds on two resources.
The first resource is a series of context-free gram-mars (CFG) which are either constructed manu-ally or extracted automatically (in the case of the treebank grammar) to predict syllable boundaries.
The different grammars are described in section 4.
The second resource is a novel algorithm that aims to combine the advantages of treebank and bracketed corpora training.
The obtained proba-bilistic context-free grammars are evaluated on a test corpus.
We also investigate the influence of the size of the training corpus on the performance of our system.
The evaluation shows that adding linguistic in-formation to the grammars increases the accuracy of our models.
"For instance, we coded the knowl-edge that (i) consonants in the onset and coda are restricted in their distribution, and (ii) the position inside of the word plays an important role."
"Fur-thermore, linguistically motivated grammars only need a small size of training corpus to achieve high accuracy and even out-perform the treebank grammar trained on the largest training corpus."
The remainder of the paper is organized as fol-lows.
Section 2 refers to treebank training.
In section 3 we introduce the combination of tree- bank and bracketed corpora training.
In section 4 we describe the grammars and experiments for German data.
Section 5 is dedicated to evaluation and in section 6 we discuss our results.
Treebank grammars are context-free grammars (CFG) that are directly read from production rules of a hand-parsed treebank.
"The probability of each rule is assigned by observing how often each rule was used in the training corpus, yielding a probabilistic context-free grammar."
"In syntax it is a commonly used method, e.g.[REF_CITE]extracted a treebank grammar from the Penn Wall Street Journal."
"The advantages of treebank train-ing are the simple procedure, and the good results which are due to the fact that for each word that appears in the training corpus there is only one possible analysis."
The disadvantage is that gram-mars which are read off a treebank are dependent on the quality of the treebank.
There is no free-dom of putting more information into the gram-mar.
"Bracketed Corpora Training introduced[REF_CITE]employs a context-free grammar and a training corpus, which is par-tially tagged with brackets."
The probability of a rule is inferred by an iterative training procedure with an extended version of the inside-outside al-gorithm.
"However, only those analyses are con-sidered that meet the tagged brackets (here sylla-ble brackets)."
Usually the context-free grammars generate more than one analysis.
BCT reduces the large number of analyses.
We utilize a spe-cial case of BCT where the number of analyses is always 1.
Our method used for the experiments is based on treebank training as well as bracketed corpora training.
The main idea is that there are large pro-nunciation dictionaries that provide information about how words are transcribed and how they are syllabified.
We want to exploit this linguis-tic knowledge that was put into these dictionar-ies.
"For our experiments we employ a pronun-ciation dictionary, CELEX[REF_CITE]that provides syllable boundaries, our so-called treebank."
We use the syllable boundaries as brackets.
The advantage of BCT can be uti-lized: writing grammars using linguistic knowl-edge.
With our method a special case of BCT is applied where the brackets in combination with a manually constructed grammar guarantee a single analysis in the training step with maximal linguis-tic information.
Figure 2 depicts our new algorithm.
We man-ually construct different linguistically motivated context-free grammars with brackets marking the syllable boundaries.
We start with a simple gram-mar and continue to add more linguistic informa-tion to the advanced grammars.
The input of the grammars is a bracketed corpus that was extracted from the pronunciation dictionary CELEX.
In a treebank training step we obtain a probabilistic context-free grammar (PCFG) by observing how often each rule was used in the training corpus.
The brackets of the input guarantee an unam-bigous analysis of each word.
"Thus, we can apply the formula of treebank training given by (Char- niak, 1996): if r is a rule, let j r j be the number of times r occurred in the parsed corpus and ( r ) be the non-terminal that r expands, then the proba-bility assigned to r is given by j r j p ( r ) ="
P r 0 2f r 0 j ( r 0 )= ( r ) g j r j 0
We then transform the PCFG by dropping the brackets in the rules resulting in an analysis grammar.
"The bracketless analysis grammar is used for parsing the input without brackets; i.e., the phoneme strings are parsed and the syllable boundaries are extracted from the most proba-ble parse."
We want to exemplify our method by means of a syllable structure grammar and an ex-emplary phoneme string.
"We experimented with a series of grammars, which are described in details in sec-tion 4.2."
In the following we will exemplify how the algorithm works.
"We chose the syllable struc-ture grammar, which divides a syllable into on-set, nucleus and coda."
The nucleus is obligatory which can be either a vowel or a diphtong.
All phonemes of a syllable that are on the left-hand side of the nucleus belong to the onset and the phonemes on the right-hand side pertain to the coda.
The onset or the coda may be empty.
The context-free grammar fragment in Figure 3 de-scribes a so called training grammar with brack-ets.
We use the input word “Forderung” (claim) [ fOR ][ d@ ][ RUN ] in the training step.
The unam-biguous analysis of the input word with the sylla-ble structure grammar is shown in Figure 1.
In the next step we train the context-free training grammar.
"Every grammar rule ap-pearing in the grammar obtains a probability de-pending on the frequency of appearance in the training corpus, yielding a PCFG."
A fragment [Footnote_1] of the syllable structure grammar is shown in Fig-ure 3 (with the recieved probabilities).
1 The grammar was trained on 389000 words
Rules (1.1)-(1.3) show that German disyllabic words are more probable than monosyllabic and trisyllabic words in the training corpus of 389000 words.
"If we look at the syllable structure, then it is more common that a syllable consists of an on-set, nucleus, and coda than a syllable comprising the onset and nucleus; the least probable struc-ture are syllables with an empty onset, and syl-lables with empty onset and empty coda."
"Rules (1.8)-(1.10) show that simple onsets are preferred over complex ones, which is also true for codas."
"Furthermore, the voiced stop [ d ] is more likely to appear in the onset than the voiceless fricative [ f ] ."
"Rules (1.19)-(1.20) show the Coda consonants with descending probability: [ R ] , [ N ] ."
In a further step we transform the obtained PCFG by drop-ping all syllable boundaries (brackets).
Rules (1.4)-(1.20) do not change in the fragment of the syllable structure grammar.
"However, the rules (1.1)-(1.3) of the analysis grammar are affected by the transformation, e.g. the rule (1.2.)"
Word ! [ Syl ] [ Syl ] would be transformed to (1.2.’)
"Syl Syl, dropping the brackets"
Predicting syllable boundaries.
Our system is now able to predict syllable boundaries with the transformed PCFG and a parser.
The input of the system is a phoneme string without brackets.
"The phoneme string [ fORd@RUN ] (claim) gets the following possible syllabifications according to the syllable structure grammar: [ fO ][ Rd@R ][ UN ] , [ fO ][ Rd@ ][ RUN ] , [ fOR ][ d@R ][ UN ] , [ fOR ][ d@ ][ RUN ] , [ fORd ][ @R ][ UN ] and [ fORd ] [ @ ][ RUN ] ."
The final step is to choose the most probable analysis.
"The subsequent tree depicts the most probable analysis: [ fOR ][ d@ ][ RUN ] , which is also the correct analysis with the overall word probability of 0.5114."
The probability of one analysis is defined as the product of the prob-abilities of the grammar rules appearing in the analysis normalized by the sum of all analysis probabilities of the given word.
"The category “Syl” shows which phonemes belong to the syllable, it indicates the beginning and the end of a syllable."
The syllable boundaries can be read off the tree: [ fOR ] [d@ ][ RUN ] .
"We experimented with a series of grammars: the first grammar, a treebank grammar, was automat-ically read from the corpus, which describes a syl-lable consisting of a phoneme sequence."
There are no intermediate levels between the syllable and the phonemes.
The second grammar is a phoneme grammar where only the number of phonemes is important.
The third grammar is a consonant-vowel grammar with the linguistic in-formation that there are consonants and vowels.
"The fourth grammar, a syllable structure gram-mar is enriched with the information that the con-sonant in the onset and coda are subject to certain restrictions."
"The last grammar is a positional syl-lable structure grammar which expresses that the consonants of the onset and coda are restricted ac-cording to the position inside of a word (e.g, ini-tial, medial, final or monosyllabic)."
These gram-mars were trained on different sizes of corpora and then evaluated.
In the following we first intro-duce the training procedure and then describe the grammars in details.
In section 5 the evaluation of the system is described.
"We use a part of a German newspaper corpus, the Stuttgarter Zeitung, consisting of 3 million words which are divided into 9/10 training and 1/10 test corpus."
"In a first step, we look up the words and their syllabification in a pronunciation dictionary."
The words not appearing in the dictionary are dis- carded.
Furthermore we want to examine the in-fluence of the size of the training corpus on the results of the evaluation.
"Therefore, we split the training corpus into 9 corpora, where the size of the corpora increases logarithmically from 4500 to 2.1 million words."
These samples of words serve as input to the training procedure.
In a treebank training step we observe for each rule in the training grammar how often it is used for the training corpus.
The grammar rules with their probabilities are transformed into the anal-ysis grammar by discarding the syllable bound-aries.
The grammar is then used for predicting syllable boundaries in the test corpus.
We started with an au-tomatically generated treebank grammar.
The grammar rules were read from a lexicon.
The number of lexical entries ranged from 250 items to 64000 items.
"The grammars obtained start with 460 rules for the smallest training corpus, increasing to 6300 rules for the largest training corpus."
The grammar describes that words are composed of syllables which consist of a string of phonemes or a single phoneme.
The following table shows the frequencies of some of the rules of the analysis grammar that are required to analyze the word [ fORd@RUN ] (claim):
Rule (3.1) describes a word that branches to three syllables.
The rules (3.2)-(3.6) depict that the syllables comprise different phoneme strings.
"For example, the word “Forderung” (claim) can result in the following two analyses:"
Word (0.9153) Word (0.0846)
Syl Syl Syl Syl Syl Syl f O R d @ R U N f O R d @ R U N
"The right tree receives the overall probability of (0.0846) and the left tree (0.9153), which means that the word [ fORd@RUN ] would be syllabified: [ fOR ] [ d@ ] [ RUN ] (which is the correct analysis)."
A second grammar is automatically generated where an abstract level is introduced.
"Every input phoneme is tagged with the phoneme label: P. A syllable consists of a phoneme sequence, which means that the number of phonemes and syllables is the decisive factor for calculating the probability of a word segmentation (into syllables)."
The following table shows a fragment of the analysis grammar with the rule frequencies.
The grammar consists of 33 rules.
Rule (4.1) describes a three-syllabic word.
The second and third rule describe that a three-phonemic syllable is preferred over two-phonemic syllables.
"Rules (4.4)-(4.6) show that P is re-written by the phonemes: [ f ] , [ O ] , and [ R ] ."
The word “Forderung” can be analyzed with the training grammar as follows (two examples out of 4375 possible analyses):
Word (0.2031) Word (0.0006)
Syl Syl Syl Syl Syl Syl P P P P P P P P P P P P P P P P f O R d @ R U N f O R d @ R U N
"In comparison with the phoneme grammar, the consonant-vowel (CV) grammar describes a syllable as a consonant-vowel-consonant (CVC) sequence[REF_CITE]."
"The linguistic knowledge that a syllable must contain a vowel is added to the CV grammar, which consists of 31 rules."
Rule (5.1) shows that a three-syllabic word is more likely to appear than a mono-syllabic word (rule (5.2)).
A CVC sequence is more probable than an open CV syllable.
The rules (5.5)-(5.8) depict some consonants and vowels and their probability.
The word “Forderung” can be analyzed as follows (two examples out of seven possible analyses):
Word (0.6864) Word (0.2166)
Syl Syl Syl Syl Syl Syl
C V C C V C V C C V C C V C V C f O R d @ R U N f O R d @ R U N
The correct analysis (left tree) is more probable than the wrong one (right tree).
Syllable structure grammar.
"We added to the CV grammar the information that there is an on-set, a nucleus and a coda."
This means that the con-sonants in the onset and in the coda are assigned different weights.
The grammar comprises 1025 rules.
The grammar and an example tree was al-ready introduced in section 3.
Positional syllable structure grammar.
Fur-ther linguistic knowledge is added to the syllable structure grammar.
"The grammar differentiate between monosyllabic words, syllables that occur in inital, medial, and final position."
Furthermore the syllable structure is defined recursively.
Another difference to the simpler grammar versions is that the syllable is devided into onset and rhyme.
"It is common wisdom that there are restrictions inside the onset and the coda, which are the topic of phonotactics."
"These restrictions are language specific; e.g., the phoneme sequence [ ld ] is quite frequent in English codas but it never appears in English onsets."
"Thus the feature position of the phonemes in the onset and in the coda is coded in the grammar, that means for example that an onset cluster consisting of 3 phonemes are ordered by their position inside of the cluster, and their position inside of the word, e.g. On.ini.1 (first onset consonant in an initial syllable), On.ini.2, On.ini.3."
A fragment of the analysis grammar is shown in the following table:
Rule (6.1) shows a monosyllabic word con-sisting of one syllable.
The second and third rules describe a bisyllabic word comprising an initial and a final syllable.
"The monosyllabic feature “one” is inherited to the daughter nodes, here to the onset, nucleus and coda in rule (6.4)."
Rule (6.5) depicts an onset that branches into two onset parts in a monosyllabic word.
The numbers represents the position inside the onset.
The subsequent rule displays the phoneme [ f ] of an initial onset.
In rule (6.7) the nucleus of an initial syllable consists of the phoneme [
O ] .
"Rule (6.8) means that the initial coda only comprises one consonant, which is re-written by rule (6.9) to a mono-phonemic coda which consists of the phoneme [ R ] ."
The first of the following two trees recieves a higher overall probabability than the second one.
The correct analysis of the transcribed word /claim/ [ fORd@RUN ] can be extracted from the most probable tree: [ fOR ][ d@ ][ RUN ] .
"Note, all other analyses of [ fORd@RUN ] are very unlikely to occur."
We split our corpus into a 9/10 training and a 1/10 test corpus resulting in an evaluation (test) corpus consisting of 242047 words.
Our test corpus is available on the World Wide Web [URL_CITE] .
"There are two different features that char-acterize our test corpus: (i) the number of un-known words in the test corpus, (ii) and the num-ber of words with a certain number of syllables."
The proportion of the unknown words is depicted in Figure 4.
"The percentage of unknown words is almost 100% for the smallest training corpus, decreasing to about 5% for the largest training corpus."
The “slow” decrease of the number of unknown words of the test corpus is due to both the high amount of test data (242047 items) and the “slightly” growing size of the training cor-pus.
"If the training corpus increases, the num-ber of words that have not been seen before (un-known) in the test corpus decreases."
"Figure 4 shows the distribution of the number of syllables in the test corpus ranked by the number of sylla-bles, which is a decreasing function."
"If the number of syllables increases, the number of words decreases."
"The test corpus without syllable boundaries, is processed by a parser[REF_CITE]and the probabilistic context-free grammars sustain-ing the most probable parse (Viterbi parse) of each word."
We compare the results of the parsing step with our test corpus (annotated with sylla-ble boundaries) and compute the accuracy.
"If the parser correctly predicts all syllable boundaries of a word, the accuracy increases."
We measure the so called word accuracy.
The accuracy curves of all grammars are shown in Figure 6.
"Comparing the treebank gram-mar and the simplest linguistic grammar we see that the accuracy curve of the treebank grammar monotonically increases, whereas the phoneme grammar has almost constant accuracy values (63%)."
The figure also shows that the simplest grammar is better than the treebank grammar un-til the treebank grammar is trained with a cor-pus size of 77.800.
The accuracy of both gram-mars is about 65% at that point.
"When the corpus size exceeds 77800, the performance of the tree-bank grammar is better than the simplest linguis-tic grammar."
The best treebank grammar reaches a accuracy of 94.89%.
The low accuracy rates of the treebank grammar trained on small corpora are due to the high number of syllables that have not been seen in the training procedure.
"Figure 6 shows that the CV grammar, the syllable struc-ture grammar and the positional syllable structure grammar outperform the treebank grammar by at least 6% with the second largest training corpus of about 1 million words."
"When the corpus size is doubled, the accuracy of the treebank grammar is still 1.5% below the positional syllable structure grammar."
"Moreover, the positional syllable structure grammar only needs a corpus size of 9600 to out-perform the treebank grammar."
Figure 5 is a sum-mary of the best results of the different grammars on different corpora sizes.
"We presented an approach to supervised learn-ing and automatic detection of syllable bound-aries, combining the advantages of treebank and bracketed corpora training."
The method exploits the advantages of BCT by using the brackets of a pronunciation dictionary resulting in an unam-bigous analysis.
"Furthermore, a manually con-structed linguistic grammar admit the use of max-imal linguistic knowledge."
"Moreover, the advan-tage of TT is exploited: a simple estimation pro-cedure, and a definite analysis of a given phoneme string."
"Our approach yields high word accu-racy with linguistically motivated grammars us-ing small training corpora, in comparison with the treebank grammar."
"The more linguistic knowl-edge is added to the grammar, the higher the accu-racy of the grammar is."
The best model recieved a 96.4% word accuracy rate (which is a harder cri-terion than syllable accuracy).
Comparison of the performance with other systems is difficult: (i) hardly any quantita-tive syllabification performance data is available for German; (ii) comparisons across languages are hard to interpret; (iii) comparisons across different approaches require cautious interpreta-tions.
Nevertheless we want to refer to sev- eral approaches that examined the syllabification task.
The most direct point of comparison is the method presented by Müller (to appear 2001).
"In one of her experiments, the standard probabil-ity model was applied to a syllabification task, yielding about 89.9% accuracy."
"However, syl-lable boundary accuracy is measured and not word accuracy."
Van den[REF_CITE]investi-gated the syllabification task with five induc-tive learning algorithms.
He reported a gener-alisation error for words of 2.22% on English data.
"However, in German (as well as Dutch and Scandinavian languages) compounding by concatenating word forms is an extremely pro-ductive process."
"Thus, the syllabification task is much more difficult in German than in En-glish."
Daelemans and van den[REF_CITE]re-port a 96% accuracy on finding syllable bound-aries for Dutch with a backpropagation learning algorithm.
Future work is to ap-ply our method to a variety of other languages.
"This paper considers three assumptions conventionally made about signatures in typed feature logic that are in po-tential disagreement with current prac-tice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semi-latticehood, unique feature introduc-tion, and the absence of subtype cover-ing."
It also discusses the conditions un-der which each of these can be tractably restored in realistic grammar signatures where they do not already exist.
"The logic of typed feature structures (LTFS,[REF_CITE]) and, in particular, its implementa-tion in the Attribute Logic Engine (ALE,[REF_CITE]), have been widely used as a means of formalising and developing gram-mars of natural languages that support computa-tionally efficient parsing and SLD resolution, no-tably grammars within the framework of Head-driven Phrase Structure Grammar (HPSG,[REF_CITE])."
"These grammars are formulated using a vocabulary provided by a finite partially ordered set of types and a set of features that must be specified for each grammar, and feature struc-tures in these grammars must respect certain con-straints that are also specified."
"These include ap-propriateness conditions, which specify, for each type, all and only the features that take values in feature structures of that type, and with which types of values (value restrictions)."
"There are also more general implicational constraints of the form   , where is a type, and is an expres-sion from LTFS’s description language."
"In LTFS and ALE, these four components, a partial order of types, a set of features, appropriateness declara-tions and type-antecedent constraints can be taken as the signature of a grammar, relative to which descriptions can be interpreted."
"LTFS and ALE also make several assump-tions about the structure and interpretation of this partial order of types and about appropriateness, some for the sake of generality, others for the sake of efficiency or simplicity."
"Appropriate-ness is generally accepted as a good thing, from the standpoints of both efficiency and representa-tional accuracy, and while many have advocated the need for implicational constraints that are even more general, type-antecedent constraints at the very least are also accepted as being necessary and convenient."
"Not all of the other assumptions are universally observed by formal linguists or gram-mar developers, however."
"This paper addresses the three most contentious assumptions that LTFS and ALE make, and how to deal with their absence in a tractable manner."
They are: 1.
Meet-semi-latticehood: every partial order of types must be a meet semi-lattice.
This implies that every consistent pair of types has a least upper bound. 2.
"Unique feature introduction: for every fea-ture, F , there is a unique most general type to which F is appropriate. 3."
No subtype covering: there can be feature structures of a non-maximally-specific type that are not typable as any of its maximally specific subtypes.
"When subtype covering is not assumed, feature structures themselves can be partially ordered and taken to repre-sent partial information states about some set of objects."
"When subtype covering is as-sumed, feature structures are discretely or-dered and totally informative, and can be taken to represent objects in the (linguistic) world themselves."
"The latter interpretation is subscribed to[REF_CITE], for example."
All three of these conditions have been claimed elsewhere to be either intractable or impossible to restore in grammar signatures where they do not already exist.
"It will be argued here that: (1) restoring meet-semi-latticehood is theoretically intractable, for which the worst case bears a dis-quieting resemblance to actual practice in current large-scale grammar signatures, but nevertheless can be efficiently compilable in practice due to the sparseness of consistent types; (2) unique feature introduction can always be restored to a signature in low-degree polynomial time, and (3) while type inferencing when subtype covering is assumed is intractable in the worst case, a very elegant con-straint logic programming solution combined with a special compilation method exists that can re-store tractability in many practical contexts."
Some simple completion algorithms and a corrected NP-completeness proof for non-disjunctive type infer-encing with subtype covering are also provided.
"In LTFS and ALE, partial orders of types are as-sumed to be meet semi-lattices: Definition 1 A partial order,  semi-lattice iff for any  ,  , is. a meet is the binary greatest lower bound, or meet op-eration, and is the dual of the join operation, , which corresponds to unification, or least upper bounds (in the orientation where corresponds to the most general type)."
"Figure 1 is not a meet semi-lattice because and do not have a meet, nor do and , for example."
"In the finite case, the assumption that every pair of types has a meet is equivalent to the assump-tion that every consistent set of types, i.e., types with an upper bound, has a join."
It is theoretically convenient when discussing the unification of fea-ture structures to assume that the unification of two consistent types always exists.
"It can also be more efficient to make this assumption as, in some representations of types and feature structures, it avoids a source of non-determinism (selection among minimal but not least upper bounds) dur-ing search."
"Just because it would be convenient for unifica-tion to be well-defined, however, does not mean it would be convenient to think of any empiri-cal domain’s concepts as a meet semi-lattice, nor that it would be convenient to add all of the types necessary to a would-be type hierarchy to ensure meet-semi-latticehood."
"The question then natu-rally arises as to whether it would be possible, given any finite partial order, to add some extra elements (types, in this case) to make it a meet semi-lattice, and if so, how many extra elements it would take, which also provides a lower bound on the time complexity of the completion."
"It is, in fact, possible to embed any finite partial order into a smallest lattice that preserves exist-ing meets and joins by adding extra elements."
The resulting construction is the finite restriction of the Dedekind-MacNeille completion ([REF_CITE]p. 41).
"Definition 2 Given a partially ordered set, ! , the #&quot; % Dedekind-MacNeille $&amp;(&apos; #&quot; %*$ ,) .+ -/ 4(567-98) completion of , , is given by:"
This route has been considered before in the context of taxonomical knowledge representati[REF_CITE].
"While meet semi-lattice completions are a practical step towards providing a semantics for arbitrary partial orders, they are generally viewed as an impractical preliminary step to performing computations over a partial order."
"Work on more efficient encoding schemes began[REF_CITE], and this seminal paper has"
Figure 2: A worst case for the Dedekind-in turn given rise to several interesting studies of incremental computations of the Dedekind-MacNeille completion in which LUBs are added as they are needed[REF_CITE].
This was also the choice made in the LKB parsing system for HPSG[REF_CITE]. which [Footnote_1]&gt; #&quot; %$(%1 )@?[Footnote_2]#&quot; ACBD*B3$ .
"1 It should be noted that while the common parlance for these sections of the type hierarchy is dimension, borrowed from earlier work[REF_CITE]on multi-dimensional Q inheritance, these are not dimensions in the sense of Q[REF_CITE]because not every -tuple of subtypes from an -dimensional classification is join-compatible."
"2 These are sometimes  * called the join density and meet density, respectively, of in ([REF_CITE]p. 42)."
As one family of
"There are partial orders of unbounded size for worst-case examples, parametrised by : , consider a set EFG) +(J (J L8: , and a partial order fined as all of the size NM: ;H subsets of E and allde-of the size H subsets of E , ordered by inclusion."
Fig-ure 2 shows the case where O: P) &lt; .
"Although the maximum subtype and supertype branching fac-tors in this family increase linearly with size, the partial orders can grow in depth instead in order to contain this."
"That yields something roughly of the form shown in Figure 3, which is an example of a recent trend in using type-intensive encodings of linguis-tic information into typed feature logic in HPSG, beginning[REF_CITE]."
These explicitly iso-late several dimensions 1 of analysis as a means of classifying complex linguistic objects.
"In Fig-ure 3, specific clausal types are selected from among the possible combinations of CLAUSAL-ting, the parameter : corresponds roughly to the ITY and HEADEDNESS subtypes."
"In this set-number of dimensions used, although an exponen-tial explosion is obviously not dependent on read-ing the type hierarchy according to this conven-tion."
"There is a simple algorithm for performing this completion, which assumes the prior existence of a most general element ( ), given in Figure 4."
"Most instantiations of the heuristic, “where there is no meet, add one”[REF_CITE], do not yield the Dedekind-MacNeille completi[REF_CITE], and other authors have proposed incremen-tal methods that trade greater efficiency in com-puting the entire completion at once for their in-crementality."
"The MSL completion algorithm is upon termination, it has produced !"
"R&quot; %$ . correct on finite partially ordered sets, , i.e., Proof: Let ;T #&quot; %$ be the partially ordered set pro-duced by the algorithm."
"Clearly, ;#&quot; %$ ."
"It suffices to show that (1) T;#&quot; %$ is a complete lattice (with W added), and (2) for all ,#&quot; %$ , there subsets \[  such that ]X )7^2a`_  -Y) exist c a`_  [ eih Suppose."
"Therethereis a leastare =;T &quot;#%$ element, so X suchandthat e have  . 2 more than one maximal lower bound, jSk&amp; and others."
"But then nj+ k  l 8 is upper-bounded and  , so the algorithm should not have termi-nated."
Suppose instead that !X  .
"Again, the algorithm should not have terminated."
"So T;&quot;#%$ with W added &quot;%$ is a complete  lattice, then. choose -is]) ist Given )unXv8+ ."
"Otherwise, if, the algorithm added X be- [ cause of a bounded set (+ w k  l 8 , with minimal up-per bounds, yzk{ (J J(J|~yd} , which H ."
"Indidthisnotcasehave, choosea least -upper s -i) bound # O-i , i.e.,and [ s ) kxCpdc } `_ [ 4( [is ther case, clearly XZ) ^ _` D b -is) ."
In ei-all &quot;%$ .  for
"Termination is guaranteed by considering, af-ter every iteration, the number of sets of meet-irreducible elements with no meet, since all com-pletion types added are meet-reducible by defini-tion."
"In LinGO[REF_CITE], the largest publicly-available LTFS-based grammar, and one which uses such type-intensive encodings, there are 3414 types, the largest supertype branching factor is 19, and although dimensionality is not distinguished in the source code from other types, the largest subtype branching factor is 103."
"Using supertype branching factor for the most conserva-tive estimate, this still implies a theoretical maxi- mum of approximately 500,000 completion types, whereas only 893 are necessary, 648 of which are inferred without reference to previously added completion types."
"Whereas incremental compilation methods rely on the assumption that the joins of most pairs of types will never be computed in a corpus before the signature changes, this method’s efficiency re-lies on the assumption that most pairs of types are join-incompatible no matter how the signa-ture changes."
"In LinGO, this is indeed the case: of the 11,655,396 possible pairs, 11,624,866 are join-incompatible, and there are only 3,306 that are consistent (with or without joins) and do not stand in a subtyping or identity relationship."
"In fact, the cost of completion is often dominated by the cost of transitive closure, which, using a sparse matrix representation, can be completed for LinGO in about 9 seconds on a 450 MHz Pentium II with 1GB memory[REF_CITE]."
"While the continued efficiency of compile-time completion of signatures as they further increase in size can only be verified empirically, what can be said at this stage is that the only reason that sig-natures like LinGO can be tractably compiled at all is sparseness of consistent types."
"In other ge-ometric respects, it bears a close enough resem-blance to the theoretical worst case to cause con-cern about scalability."
"Compilation, if efficient, is to be preferred from the standpoint of static error detection, which incremental methods may elect to skip."
"In addition, running a new signa-ture plus grammar over a test corpus is a frequent task in large-scale grammar development, and in-cremental methods, even ones that memoise pre-vious computations, may pay back the savings in compile-time on a large test corpus."
"It should also be noted that another plausible method is compi-lation into logical terms or bit vectors, in which some amount of compilation (ranging from linear-time to exponential) is performed with the remain-ing cost amortised evenly across all run-time uni-fications, which often results in a savings during grammar development."
"LTFS and ALE also assume that appropriateness guarantees the existence of a unique introducer for Definition 3 Given a type hierarchy, #( , and every feature: is a partial function, { "
"Feature introduction has been argued not to be appropriate for certain empirical domains either, although[REF_CITE]do otherwise ob-serve it."
"The debate, however, has focussed on whether to modify some other aspect of type infer-encing in order to compensate for the lack of fea-ture introduction, presumably under the assump-tion that feature introduction was difficult or im-possible to restore automatically to grammar sig-natures that did not have it."
"Just as with the condition of meet-semi-latticehood, however, it is possible to take a would-be signature without feature introduction and restore this condition through the addition of extra unique introducing types for certain appropriate features."
"The algorithm in Figure 5 completion type, X , can be used for different achieves this."
"In practice, the same signature are the same set, ¼ ."
"This clearly produces a features, provided that their minimal introducers partially ordered set with a unique introducing type for every feature."
"It may disturb meet-semi-latticehood, however, which means that this completion must precede the meet semi-lattice completion of Section 2."
"If generalisation has algorithm runs in ½&quot;S¾¦: $ , where ¾ is the number already been computed, the signature completion of features, and : is the number of types."
"In HPSG, it is generally assumed that non-maximally-specific types are simply a convenient shorthand for talking about sets of maximally specific types, sometimes called species, over which the principles of a grammar are stated."
"In a view where feature structures represent discretely ordered objects in an empirical model, every feature structure must bear one of these species."
"In particular, each non-maximally-specific type in a description is equivalent to the disjunction of the maximally specific subtypes that it subsumes."
"There are some good reasons not to build this assumption, called “subtype covering,” into LTFS or its implementations."
"Firstly, it is not an ap-propriate assumption to make for some empiri-cal domains."
"Even in HPSG, the denotations of parametrically-typed lists are more naturally in-terpreted without it."
"Secondly, not to make the as-sumption is more general: where it is appropriate, extra type-antecedent constraints can be added to the grammar signature of the form: : UË Ík ÌÏÎ(Î  Ë  type, : , and its"
"Ð for each non-maximally-specific Ë k J(J  Ë  maximal subtypes, ."
These con-straints become crucial in certain cases where the possible permutations of appropriate feature val-ues at a type are not covered by the permutations of those features on its maximally specific sub-types.
"This is the case for the type, verb, in the signature in Figure 6 (given in ALE syntax, where sub/2 defines the partial order of types, and intro/2 defines appropriateness on unique ÑMÒ in-troducers ÔÓ of features)."
"INV , is not attested by any of verb’s subtypes."
"While there are arguably better ways to represent this information, the extra type-antecedent con-straint:"
Ì verb aux verb main verb is necessary in order to decide satisfiability cor-rectly under the assumption of subtype covering.
We will call types such as verb deranged types.
Types that are not deranged are called normal types.
"Third, although subtype covering is, in the au-thor’s experience, not a source of inefficiency in practical LTFS grammars, when subtype cover-ing is implicitly assumed, determining whether a non-disjunctive description is satisfiable under ap-propriateness conditions is an NP-complete prob-lem, whereas this is known to be polynomial time without it (and without type-antecedent con-straints, of course)."
This was originally proven[REF_CITE].
"The proof, with cor-rections, is summarised here because it was never published."
Consider the translation of a 3SAT for-mula into a description relative to the signature given in Figure 7.
"The resulting description is al-ways non-disjunctive, since logical disjunction is encoded in subtyping."
Asking whether a formula is satisfiable then reduces to asking whether this description conjoined with trueform is satisfi-able.
"Every type is normal except for truedisj Ò ,for which the combination, DISJ 1 "
"DISJ 2  falseform falseform, is not attested in either of its subtypes."
Enforcing subtype covering on this one deranged type is the sole source of intractability for this problem.
"Instead of enforcing subtype covering along with type inferencing, an alternative is to suspend con-straints on feature structures that encode subtype covering restrictions, and conduct type inferenc-ing in their absence."
This restores tractability at the cost of rendering type inferencing sound but not complete.
This can be implemented very transparently in systems like ALE that are built on top of another logic programming language with support for constraint logic programming such as SICStus Prolog.
"In the worst case, an answer to a query to the grammar signature may contain vari- ables with constraints attached to them that must be exhaustively searched over in order to deter-mine their satisfiability, and this is still intractable in the worst case."
"The advantage of suspending subtype covering constraints is that other princi-ples of grammar and proof procedures such as SLD resolution, parsing or generation can add de-terministic information that may result in an early failure or a deterministic set of constraints that can then be applied immediately and efficiently."
The variables that correspond to feature structures of a deranged type are precisely those that require these suspended constraints.
"Given a diagnosis of which types in a signature are deranged (discussed in the next section), suspended subtype covering constraints can be implemented for the SICStus Prolog implemen-tation of ALE by adding relational attachments to ALE’s type-antecedent universal constraints that will suspend a goal on candidate feature structures with deranged types such as verb or truedisj."
"The suspended goal unblocks whenever the deranged type or the type of one of its appropriate features’ values is updated to a more specific subtype, and checks the types of the appropriate features’ values."
"Of particular use is the SICStus Constraint Handling Rules (CHR,[REF_CITE]) library, which has the ability not only to suspend, but to suspend until a particular variable is instantiated or even bound to another variable."
"This is the powerful kind of mechanism required to check these constraints efficiently, i.e., only when nec-essary."
"Re-entrancies in a Prolog term encoding of feature structures, such as the one ALE uses[REF_CITE], may only show up as the binding of two uninstantiated variables, and re-entrancies are often an important case where these con-straints need to be checked."
The details of this reduction to constraint handling rules are given[REF_CITE].
The relevant complexity-theoretic issue is the detection of deranged types.
The detection of deranged types themselves is also a potential problem.
"This is something that needs to be detected at compile-time when sub-type covering constraints are generated, and as small changes in a partial order of types can have drastic effects on other parts of the signature be-cause of appropriateness, incremental compila-tion of the grammar signature itself can be ex-tremely difficult."
"This means that the detection of deranged types must be something that can be per-formed very quickly, as it will normally be per-formed repeatedly during development."
"A naive algorithm would be, for every type, value types into the set, - , of all possible maxi-to expand the product of its features’ appropriate mally specific products, then to do Ð the same for the cific subtypes, forming sets i[ , and then to re-products on each of the type’s maximally spe-move the products in the [i from - ."
The type is main in -NÕd&quot;##[ $ .
"If the maximum number of deranged iff any maximally specific products re-features w types inappropriatethe signatureto any, thentypetheis cost, andoftherethisareis dominated wÖ , since inbythetheworstcostcaseof expandingall featuresthecouldproductshave, as their appropriate value."
A less naive algorithm would treat normal (non- deranged) subtypes as if they were maximally spe-cific when doing the expansion.
"This works be-cause the products of appropriate feature values of normal types are, by definition, covered by those of their own maximally specific subtypes."
"Maxi-mally specific types, furthermore, are always nor-mal and do not need to be checked."
Atomic types (types with no appropriate features) are also triv-ially normal.
"It is also possible to avoid doing a great deal of the remaining expansion, simply by counting the number of maximally specific products of types rather than by enumerating them."
"For exam-ple, in Figure ×M\Ò 6 ÔÓ , main ×M verb has one such prod-uct, AUX ÔÓ\Ò INV , and AUX ÔÓØÒ INV , and aux verb M has two, AUX INV . verb, on the other hand, has all four possible combina-tions, so it is deranged."
The resulting algorithm is thus given in Figure 8.
"Using the smallest normal subtype cover that we have for the product of w ’s feature values, we iteratively expand the feature value products for this cover until they partition their maximal feature products, and then count the maximal products using multiplication."
"A similar trick can be used to calculate maximal efficiently. is much better: &quot;mw  Ö $ , where is the weighted The complexity of this approach, in practice, mean subtype branching factor of a subtype of a value restriction of a non-maximal non-atomic type’s feature, and is the weighted mean length of the longest path from a maximal type to a sub-type of a value restriction of a non-maximal non-atomic type’s feature."
"In the Dedekind-MacNeille and the sum of Ö over all non-maximal types completion of LinGO’s signature, is 1.9, is 2.2, with arity is approximately H  ."
"The sum of maximal Ö mw&quot; $ over every non-maximal type, w , on the other hand, is approximately H  k ."
Practical performance is again much better because this al-gorithm can exploit the empirical observation that most types in a realistic signature are normal and that most feature value restrictions on subtypes do not vary widely.
Using branching factor to move the total number of types to a lower degree term is crucial for large signatures.
"Efficient compilation of both meet-semi-latticehood and subtype covering depends crucially in practice on sparseness, either of consistency among types, or of deranged types, to the extent it is possible at all."
Closure for unique feature introduction runs in linear time in both the number of features and types.
"Subtype covering results in NP-complete non-disjunctive type inferencing, but the postponement of these constraints using constraint handling rules can often hide that complexity in the presence of other principles of grammar."
This paper presents a method that as-sists in maintaining a rule-based named-entity recognition and classifi-cation system.
"The underlying idea is to use a separate system, constructed with the use of machine learning, to monitor the performance of the rule-based sys-tem."
"The training data for the second system is generated with the use of the rule-based system, thus avoiding the need for manual tagging."
The dis-agreement of the two systems acts as a signal for updating the rule-based sys-tem.
The generality of the approach is illustrated by applying it to large cor-pora in two different languages: Greek and French.
"The results are very en-couraging, showing that this alternative use of machine learning can assist sig-nificantly in the maintenance of rule-based systems."
Machine learning has recently been proposed as a promising solution to a major problem in lan-guage engineering: the construction of lexical resources.
"Most of the real-world language en-gineering systems make use of a variety of lexi-cal resources, in particular grammars and lexi-cons."
"The use of general-purpose resources is ineffective, since in most applications a special-ised vocabulary is used, which is not supported by general-purpose lexicons and grammars."
"For this reason, significant effort is currently put into the construction of generic tools that can quickly adapt to a particular thematic domain."
The adaptation of these tools mainly involves the adaptation of domain-specific semantic lexi-cal resources.
"Named-entity recognition and classification (NERC) is the identification of proper names in text and their classification as different types of named entity (NE), e.g. persons, organisations, locations, etc."
"This is an important subtask in most language engineering applications, in par-ticular information retrieval and extraction."
"The lexical resources that are typically included in a NERC system are a lexicon, in the form of gaz-etteer lists, and a grammar, responsible for rec-ognising the entities that are either not in the lexicon or appear in more than one gazetteer lists."
"The manual adaptation of those two re-sources to a particular domain is time-consuming and in some cases impossible, due to the lack of experts."
The exploitation of learning techniques to support this adaptation task has attracted the attention of researchers in language engineering.
"However, the adaptation of lexical resources to a specific domain at a certain point in time is not sufficient on its own."
The performance of a NERC system degrades over time[REF_CITE]due to the introduc-tion of new NEs or the change in the meaning of existing ones.
We need to find ways that facili-tate the maintenance of rule-based NERC sys-tems.
"This paper presents such a method, ex-ploiting machine learning in an innovative way."
Our method controls rule-based NERC systems with NERC systems constructed by a machine learning algorithm.
"The method comprises two stages: the training stage, during which a super- vised machine learning algorithm constructs a new system using data generated by the rule-based system, and the deployment stage, in which the results of the two systems are com-pared on new data and their disagreements are used as signals for change in the rule-based sys-tem."
"Note that, unlike most applications of su-pervised machine learning, the training data for the new system are not produced manually."
"In order to illustrate the generality of this ap-proach, we have tested it with two different NERC systems, one for Greek and another one for French."
The results are very encouraging and show that machine learning techniques can be used for the maintenance of rule-based systems.
Section 2 presents existing work on the do-main adaptation of NERC systems using ma-chine learning (ML) techniques.
Section 3 pre-sents the two rule-based NERC systems for Greek and French.
Section 4 explains our method and Section 5 describes the two experi-ments and presents the evaluation results.
"Fi-nally, Section 6 concludes and presents our fu-ture plans."
"As mentioned above, the exploitation of learning techniques to support the domain adaptation of NERC systems has recently attracted the atten-tion of several researchers."
Some of these ap-proaches are briefly discussed in this section.
Nymble[REF_CITE]uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text.
"Nymble did particularly well in the MUC-7 competiti[REF_CITE], due mainly to the use of the correct features in the encoding of words, e.g. capitalisation, and the probabilistic modelling of the recognition system."
Named-entity recognition in Alembic[REF_CITE]uses the transformation-based rule learning approach introduced in Brill’s work on part-of-speech tagging[REF_CITE].
An important aspect of this approach is the fact that the system learns rules that can be freely inter-mixed with hand-engineered ones.
The RoboTag system presented[REF_CITE]constructs decision trees that clas-sify words as being start or end points of a par-ticular named-entity type.
A variant of this ap-proach was used in the system presented by the
New York University (NYU) in the Multilingual Entity Task (MET-2) of MUC-7[REF_CITE].
"The system developed for Italian in ECRAN[REF_CITE], uses unsupervised learning to expand a manually constructed sys-tem and improve its performance."
The learning algorithm tries to supplement the manually con-structed system by classifying recognised but unclassified NEs.
"In[REF_CITE]the manually constructed system was replaced by the supervised tree induction algorithm C4.5[REF_CITE], reaching very good perform-ance on the MUC-6 corpora."
"The partially supervised multi-level boot-strapping approach presented[REF_CITE]induces a set of information extrac-tion patterns, which can be used to identify and classify NEs."
"The system starts by generating exhaustively all candidate extraction patterns, using an earlier system called AutoSlog[REF_CITE]."
"Given a small number of seed examples of NEs, the most useful patterns for recognising the seed examples are selected and used to ex-pand the set of classified NEs."
The end result is a dictionary of NEs and the extraction patterns that correspond to them.
Our method follows an alternative innovative approach to the use of learning for NERC.
"In-stead of using ML to construct a NERC system that will be used autonomously, the system con-structed by ML, according to our approach is used to monitor the performance of an existing rule-based NERC system."
"In this manner, the new system provides feedback on whether the rule-based system under control has become obsolete and needs to be updated."
"An important advantage of this approach is that no manual tagging of training data is needed, despite the use of a supervised learning algorithm."
Our method bears some similarities with sys-tems based on active learning[REF_CITE].
"According to this technique, multiple classifiers performing the same task are used in order to actively create training data, through their disagreements."
"Usually, this involves an iterative procedure."
"First a few initial labelled examples are used to train the classifiers and then, unlabelled examples are presented to the classifiers."
Examples that cause the classifiers to disagree are good candidates to retrain the clas-sifiers on.
The difference of active learning to our method is the use of a manually-constructed rule-based NERC system as the basic system.
"The ML method is used only to identify when the rule-based NERC system should be updated, but not for creating new training instances."
"An-other approach, which bears some similarity to ours, is presented[REF_CITE]where a heuristic algorithm is used to monitor the per-formance of web-page wrappers."
A typical NERC system consists of a lexicon and a grammar.
The lexicon is a set of NEs that are known beforehand and have been classified into semantic classes.
The grammar is used to recognize and classify NEs that are not in the lexicon and to decide upon the final classes of NEs in ambiguous cases.
"Manual construction of NERC systems is a complicated and time-consuming process, even for experts."
The meaning of a single sentence may vary a lot according to which category a NE is assigned to.
"For example, the sentence “Express group intends to sell Le[REF_CITE]MF” indicates a sale of a newspaper company, if “Le Point” is classified as an organisation."
"Whereas the following sentence, which is grammatically identical to the previous one, “Compagnie des Signaux intends to sell[REF_CITE]MF” gives only a price for an industrial product."
"In order for a NERC system to be able to recognise and categorise correctly NEs, both the lexicon and the grammar have to be validated on large corpora, testing their efficiency and their robustness."
"However, this process does not en-sure that the performance of the developed sys-tem will remain steady over time."
"Almost under all thematic domains, the introduction of new NEs or the change in the meaning of existing ones can increase the error rate of the system."
"Our approach tries to identify such cases, facili-tating the maintenance of the NERC system."
The following subsections briefly describe the Greek and French rule-based NERC systems that have been used in our experiments.
"The Greek NERC system[REF_CITE]used for the purposes of this experiment forms part of a larger Greek information extrac-tion system, being developed in the context of the R&amp;D project MITOS. [URL_CITE]"
"The NERC compo-nent of this system mainly consists of three processing stages: linguistic pre-processing, NE identification and NE classification."
"The linguis-tic pre-processing stage involves some basic tasks: tokenisation, sentence splitting, part-of-speech tagging and stemming."
"Once the text has been annotated with part of speech tags, a stemmer is used."
The aim of the stemmer is to reduce the size of the lexicon as well as the size and complexity of the NERC grammar.
"The NE identification stage involves the de-tection of their boundaries, i.e., the start and the end of all the possible spans of tokens that are likely to belong to a NE."
"Identification consists of three sub-stages: initial delimitation, separa-tion and exclusion."
Initial delimitation involves the application of general patterns.
"These pat-terns are combinations of a limited number of words, selected types of tokens (e.g. tokens con-sisting of capital characters), special symbols and punctuation marks."
"At the separation sub-stage, possible NEs that are likely to contain more than one NE or a NE attached to a non- NE, are detected and attachment problems are resolved."
"Finally, at the exclusion sub-stage two types of criteria are used for exclusion from the possible NE list: the context of the phrase and being part of an exclusion list."
"Suggestive con-text for exclusion consists of common names that refer to products, services or artifacts."
"The exclusion list includes capitalized abbreviations of common nouns, financial terms, capitalized person titles, which are not ambiguous, and nouns commonly found in names of products, artifacts and services."
"Once the possible NEs have been identified, the classification stage begins."
"Classification involves three sub-stages: application of classi-fication rules, gazetteer-based classification, and partial matching of classified named-entities with unclassified ones."
"Classification rules take into account both internal and external evidence[REF_CITE], i.e., the words and symbols that comprise the possible name and the context in which it occurs."
Gazetteer-based classifica-tion involves the look up of pre-stored lists of known proper names (gazetteers).
The gazet-teers contain stemmed forms and have been compiled from Web sites and an annotated train- ing corpus.
"The size of the gazetteers is rather small (3,059 names)."
"At the partial matching sub-stage, classified names are matched against unclassified ones aiming at the recognition of the truncated or variable forms of names."
The French NERC system has been imple-mented with the use of a rule-based inference engine[REF_CITE].
"It is based on a large knowledge base (lexicon) including 8,000 proper names that share 10,000 forms and con-sist of 11,000 words."
It has been used continu-ously since 1995 in several real-time document filtering applications[REF_CITE].
The uses of the NERC system in these applica-tions are the following: 1.
"Segmentation of NEs, in order to improve the performance of the syntactic analyser, par-ticularly in the case of long proper names which contain grammatical markers (e.g. prepositions, conjunctions, commas, full stops). 2."
Recognition of known NEs in order to sup-ply precise information to a document filtering module. 3.
Classification of NEs in order to feed a document filtering module with information dealing with the very nature of the NEs quoted in the documents.
"The NERC system tries to classify each NE in one of four different categories: association (non-commercial organisation), person, location or company."
"For the classification of known entities, a crucial problem appears when several NEs share a single form."
"To deal with these cases, two sets of rules have been implemented: 1."
"Local context: For instance, “Saint-Louis” may be interpreted in one of the following ways: the capital of Missouri, a French group in the food production industry, a small industry “les Cristalleries de Saint Louis”, a small town in France, a hospital in Paris, etc."
"Exploration of the local context using the proper name may enable, in certain cases, a choice to be made between these various interpretations."
"If the text speaks of “St-Louis (Missouri)”, only the first interpretation should be adopted."
"In order to do this the knowledge base should contain informa-tion that “Saint-Louis” is in Missouri, and a rule should exist to interpret the affixing of a paren-thesis. 2."
Global context: Abbreviated NEs and acro-nyms are much more frequent sources of ambi-guity and are almost always common to several NEs.
"In general, such ambiguous forms of NEs do not occur on their own in news but almost always together with non-ambiguous forms that enable the ambiguity to be removed."
"For in-stance, if the NEs “Saint-Louis” and “Hôpital Saint-Louis” appear in a single news item, the interpretation corresponding to the hospital is more likely to be the one that should be adopted."
"For unknown entities, three sets of rules have been implemented: 1."
Prototypes: Many NEs are constructed ac-cording to some prototypes.
These can be cate-gorised using pattern matching rules.
"Mr André Blavier, Kyocera Corp, Condé-sur-Huisne, Honda Motor, IBM-Asia, Bernard Tapie Finance, Siam Nissan Automobile Co Ltd are good examples of such prototypes. 2."
Local context: Many single-word unknown NEs (some known NEs as well) may also be categorised using the local context.
"For instance, the small sentences “Peskine, director of the group”, “the shareholders of Fibaly ” or “the mayor of Gisenyi” are used as categorisation rules. 3."
"Global context: After the first appearance of a NE in full, its head (e.g. family name, main company) is often used alone in the text instead of the full name."
"The company Kyocera Corp, for example, may be designated by the single word Kyocera in the remainder of the text."
"For each such unknown word, starting with a capital letter, a special rule examines whether it appears inside another NE in the text."
"Machine learning has been used successfully to control a rule-based system that performs a dif-ferent task, namely document filtering[REF_CITE]."
The learning method used in that case was a neural network[REF_CITE].
"In our present study, we control the rule-based NERC systems that have been presented in section 3, with NERC systems constructed by the C4.5 algorithm."
"Our method comprises two stages: the training stage, during which C4.5 constructs a new system using data generated by the rule-based system, and the deployment stage, in which the results of the two systems are com-pared on new data and their disagreements are used as signals for change in the rule-based sys-tem."
This section describes the basic principles of our control method.
The training stage of our method consists of the following processing steps (Figure 1):
Running the rule-based NERC system on a large training corpus (containing several thou-sands of NEs in our case).
The aim of this proc-ess is to recognise and classify the NEs in the corpus.
"The end product is a set of NEs, associ-ated with their class."
Constructing a separate NERC system by ap-plying C4.5 on the data generated by the rule-based system.
"In this process, the classified NEs are used as training data by C4.5, in order to construct the second NERC system (trained NERC)."
"For each classified NE a training exam-ple (vector) is created, containing information about the part of speech and gazetteer tags of the first and the last two words of the NE, as well as the two words preceding and the two following the NE."
"It is important to note that, unlike other uses of supervised machine learning methods, this approach does not require manual tagging of training data."
"In the deployment stage, the two NERC systems are compared on a new corpus to identify dis-agreements."
"Despite the fact that the second method is trained on data generated by the first, the different nature of the NERC system gener-ated by C4.5, i.e., a decision tree, leads to inter-esting disagreements between the two methods."
The deployment stage consists of the following processing steps (Figure 2): 1. Running the rule-based NERC system on a new corpus.
It should be stressed here that the documents in this corpus differ in some charac- teristic way from those in the training corpus.
"In our experiments the difference is chronological, i.e., the new corpus consists of recent news arti-cles."
The reason for adopting this approach is that we are interested in the maintenance of a rule-based system through time.
An alternative approach might be for the new corpus to be from a slightly different thematic domain.
"In that case, the goal of the process would be the cus-tomisation of the rule-based system to a new domain. 2. Running the trained NERC system on the same corpus. 3."
Comparing the results provided by both sys-tems to identify cases of disagreement.
"The re-sult is a set of data where the two systems dis-agree: in our case, disagreements deal with the different categories assigned by the NERC sys-tems to NEs (see Section 5 for detailed results)."
"These cases are then provided to the language engineer, who needs to evaluate them and de-cide on changes for the rule-based system."
"In order to evaluate the proposed method, two different experiments were contacted, one for each language."
The exact experimental settings as well as the evaluation results are presented in the following sections.
"For the experiment regarding the Greek lan-guage, we used three NE classes: organisations, persons and locations."
"For the purposes of the experiment, two corpora of financial news were used. [Footnote_2]"
2 The corpora were provided by the Greek publishing com-pany Kapa-TEL.
"The first corpus that was used for training purposes, consisted of 5,000 news articles from the years 1996 and 1997, containing 10,010 instances of NEs (1,885 persons, 1,781 loca-tions, 6,344 organisations)."
"The second corpus that was used for evaluation purposes consisted of 5,779 news from the years 1999 and 2000 and contained 11,786 instances of NEs (1,137 per-sons, 810 locations, 9,839 organisations)."
"A good way to give an overview of the cases of disagreement of the two systems is through a contingency matrix, as shown in Table 1."
"The rows of this table correspond to the classifica-tion of the rule-based system, while the columns to the classification of the system constructed by C4.5."
"As we can see from Table 1, in 95% of the cases the two systems are in agreement."
"This means, that in order to update the rule-based NERC system, we have to examine only 5% of the cases, where the two systems disagree."
Examin-ing these cases gave us important insight regard-ing problems of the rule-based NERC system.
Some examples are presented in the following sections.
The examination of cases in disagreement re-vealed some interesting problems regarding NE recognition.
These problems concern NEs that the rule-based system identified only partially and as a result classified them incorrectly.
"For example, in the stage of initial delimita-tion, the general patterns fail to identify NEs that contain numbers in their names, like the organi-sation “[REF_CITE]”[REF_CITE]represent-ing the organising committee of 2004 Olympics."
"In addition, during the separation phase some of the rules have not taken into account some inflexional endings, causing failures in separat-ing some NEs."
"For example, in the phrase “ο υφ."
"Πολιτισµού Γ. Φλωρίδης” (the under-secretary of Culture Γ. Φλωρίδης) the recogniser failed to separate the person name from its title, due to the last accented character of the word “Πολιτι- σµού”."
"Finally, we were able to locate several stop-words and update our exclusion list."
"For in-stance, the phrase “γραµµών ISDN” (ISDN lines) was recognised as an organisation (as the word “γραµµών” is a frequent constituent of airline or shipping companies), but in reality the text was referring to ISDN telephone lines."
"Except from the problems identified in the rec-ognition phase, the examination of the cases of disagreement revealed various problems regard-ing mainly the classification grammar."
"In fact, some of our classification rules were found to be too general, leading to wrong classifications."
"For example, according to one of the rules, a sequence of two words, starting with capital letters, constitutes a person name if it is pre-ceded by a definite article and the endings of these two words belong in a specific set that usually denote person names."
"This rule caused the classification of various non-NEs as persons, including “του Ολυµπιακού Χωριού” (the Olympic Village)."
"Another example of an overly general rule is a rule that classifies a sequence of abbreviations or nouns starting with capital letter as an organi-sation, if this sequence is preceded by a comma that in turn is preceded by a NE already classi-fied as an organisation."
"This rule caused the classification of few person names as organisa-tions, such as “ο διοικητής της Εθνικής Τράπε- ζας, Θ.Καρατζάς” (the director of National Bank, Θ.Καρατζάς)."
The corpus used for the French experiment con-tained dispatches from the Agence France-Presse[REF_CITE]until[REF_CITE].
The thematic domain of the corpus was shareholding events.
"This corpus contained six thousand documents, including 180,983 instances of NEs with the following distribution: companies (45%), locations (45%), persons (7%) and asso-ciations (non commercial organisations) (3%)."
"For the purposes of this experiment, the corpus was chronologically split in two parts."
"The part containing the chronologically earlier messages was used for training purposes while the second part, containing the most recent messages, was used in order to evaluate our approach."
"In this experiment, we mainly focused on four NE categories, instead of the three categories used for the Greek experiment."
This differentiation originates in the fact that the French NERC sys-tem further categorises organisations into asso-ciations (non-profit organisations) and compa-nies.
The contingency matrix giving an overview of the cases of disagreement of the two systems is shown in Table 2.
It appears that in 91% of the cases the two systems are in agreement.
Examining the disagreement cases gave us im-portant insight regarding problems of the rule-based system.
The following sections present some interesting examples.
"Similarly to the Greek experiment, the examina-tion of disagreements revealed some interesting problems in the recognition of NEs."
"For in-stance, “Europe 1” is a well-known French radio station, also written sometimes as “Europe Un” (Europe One)."
The rule-based system failed to identify “Europe Un” and only identified “Europe” as a location.
The source of the prob-lem is the lack of a mapping between fully writ-ten numbers and numerical figures.
"Another example is the phrase “Le Mans Re”, which is a shortened version of the com-pany name “Les mutuelles du Mans Reassurance” (a Reinsurance company)."
"The rule-based system recognised only “Le Mans” as a location, due to the well-known French city."
"What is needed here is an extension of the seg-mentation rules to include “Re” as a “company designator”, such as “Motor”, “Bank” or “Tele-com”."
Most of the classification problems that were identified concerned NEs already known to the system that meanwhile have acquired new meanings.
"For example, “Ariane II rachète” (Ariane II buys) is classified as a person, due to the word “Ariane” contained in the lexicon as a person forename."
"In reality, “Ariane II” is a new company that should also be included in the lexicon database."
Another example is “Orange” already included in the lexicon as an old French city.
"In the meanwhile, a new French company has been created having the same name, as in the example “Orange, valorisée par les analys-tes” (Orange, estimated by analysts)."
"Also in this case, the lexicon must be updated with a second entry for this entity, categorised as a company."
"Besides lexicon omissions, some problems regarding the classification grammar were also revealed."
"First, overly general rules were identi-fied, such as the one that classifies entities start-ing from “A” and followed by numbers as French highway names."
"This rule wrongly clas-sified the NE “A3XX” as a highway, while the text was referring to an airplane model: “L’A3XX, un avion” (The A3XX, an air plane)."
Our approach also succeeded in locating well-known NEs used in a new context.
"For example, the rule-based NERC system recog-nises “Taittinger” as a company while the sys-tem learned by C4.5 disagrees with this classifi-cation in the sentence “la famille Taittinger” (the family Taittinger)."
"In this case, the grammar should be updated with a rule saying that the word “family” in front of a proper name sug-gests a person name."
"In this paper, we have proposed an alternative use of machine learning in named-entity recog-nition and classification."
"Instead of constructing an autonomous NERC system, the system con-structed with the use of machine learning assists in the maintenance of a rule-based NERC sys-tem."
"An important feature of the approach is the use of a supervised learning method, without the need for manual tagging of training data."
The proposed approach was evaluated with success for two different languages: Greek and French.
On-going work aims at reducing the number of disagreements between the two systems down to those that are essential for the improvement of the system.
"Currently, there are many cases where the two systems disagree, but the rule-based system is correct."
"Another extension that we are examining is to train a NERC system to not only classify, but also recognise NEs."
We believe that this exten- sion will lead to the identification of more prob-lematic cases in the recognition phase.
"In conclusion, the method presented in this paper proposes a simple and effective use of machine learning for the maintenance of rule-based systems."
"The scope of this approach is clearly wider than that examined here, i.e., named-entity recognition."
"Techniques for automatically training modules of a natural language gener-ator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches."
In this paper We ex-perimentally evaluate a trainable sen-tence planner for a spoken dialogue sys-tem by eliciting subjective human judg-ments.
"In order to perform an ex-haustive comparison, we also evaluate a hand-crafted template-based genera-tion component, two rule-based sen-tence planners, and two baseline sen-tence planners."
"We show that the train-able sentence planner performs better than the rule-based systems and the baselines, and as well as the hand-crafted system."
The past several years have seen a large increase in commercial dialog systems.
"These systems typically use system-initiative dialog strategies, with system utterances highly scripted for style and register and recorded by voice talent."
How-ever several factors argue against the continued use of these simple techniques for producing the system side of the conversation.
"First, text-to-speech has improved to the point of being a vi-able alternative to pre-recorded prompts."
"Second, there is a perceived need for spoken dialog sys-tems to be more flexible and support user initia-tive, but this requires greater flexibility in utter- ance generation."
"Finally, systems to support com-plex planning are being developed, which will re-quire more sophisticated output."
"As we move away from systems with pre-recorded prompts, there are two possible ap-proaches to producing system utterances."
"The first is template-based generation, where ut-terances are produced from hand-crafted string templates."
Most current research systems use template-based generation because it is concep-tually straightforward.
"However, while little or no linguistic training is needed to write templates, it is a tedious and time-consuming task: one or more templates must be written for each combi-nation of goals and discourse contexts, and lin-guistic issues such as subject-verb agreement and determiner-noun agreement must be repeatedly encoded for each template."
"Furthermore, main-tenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases. [Footnote_1]"
"1 Although we are not aware of any software engineering studies of template development and maintenance, this claim is supported by abundant anecdotal evidence."
"The second approach is natural language gen-eration (NLG), which customarily divides the generation process into three modules[REF_CITE]: (1) Text Planning, (2) Sen-tence Planning, and (3) Surface Realization."
"In this paper, we discuss only sentence planning; the role of the sentence planner is to choose abstract lexico-structural resources for a text plan, where a text plan encodes the communicative goals for an utterance (and, sometimes, their rhetorical structure)."
"In general, NLG promises portability across application domains and dialog situations by focusing on the development of rules for each generation module that are general and domain- independent."
"However, the quality of the output for a particular domain, or a particular situation in a dialog, may be inferior to that of a template-based system without considerable investment in domain-specific rules or domain-tuning of gen-eral rules."
"Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge."
"Recently, several approaches for automatically training modules of an NLG system have been proposed[REF_CITE]."
"These hold the promise that the complex step of customiz-ing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates."
"While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough."
"In[REF_CITE]we propose a new model of sentence planning called SP O T. In SP O T, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals."
"In[REF_CITE], we evaluate the perfor-mance of the learning component of SP O T, and show that SP O T learns to select sentence plans that are highly rated by the two human judges."
"While this evaluation shows that SP O T has in-deed learned from the human judges, it does not show that using only two human judgments is sufficient to produce more broadly acceptable re-sults, nor does it show that SP O T performs as well as optimized hand-crafted template or rule-based systems."
In this paper we address these questions.
"Because SP O T is trained on data from a work-ing system, we can directly compare SP O T to the hand-crafted, template-based generation compo-nent of the current system."
"In order to perform an exhaustive comparison, we also implemented two rule-based and two baseline sentence-planners."
One baseline simply produces a single sentence for each communicative goal.
Another baseline randomly makes decisions about how to combine communicative goals into sentences.
We directly compare these different approaches in an evalua-tion experiment in which 60 human subjects rate each system’s output on a scale of 1 to 5.
The experimental design is described in section 2.
The sentence planners used in the evaluation are described in section 3.
"In section 4, we present our results."
We show that the trainable sentence planner performs better than both rule-based sys-tems and as well as the hand-crafted template-based system.
These four systems outperform the baseline sentence planners.
Section 5 summarizes our results and discusses related and future work.
"Our research concerns developing and evaluat-ing a portable generation component for a mixed-initiative travel planning system, AMELIA, de-veloped at AT&amp;T Labs as part of DARPA Com-municator."
"Consider the required generation ca-pabilities of AMELIA, as illustrated in Figure 1."
"Utterance System1 requests information about the caller’s departure airport, but in User2, the caller takes the initiative to provide information about her destination."
"In System3, the system’s goal is to implicitly confirm the destination (be-cause of the possibility of error in the speech recognition component), and request information (for the second time) of the caller’s departure air-port."
"This combination of communicative goals arises dynamically in the dialog because the sys-tem supports user initiative, and requires differ-ent capabilities for generation than if the system could only understand the direct answer to the question that it asked in System1."
"In User4, the caller provides this information but takes the initiative to provide the month and day of travel."
"Given the system’s dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and desti-nation cities and the month and day information, as well as to request information about the time of travel."
The system’s representation of its com- municative goals for System5 is in Figure 2.
"As before, this combination of communicative goals arises in response to the user’s initiative."
"Like most working research spoken dialog systems, AMELIA uses hand-crafted, template-based generation."
"Its output is created by choos-ing string templates for each elementary speech act, using a large choice function which depends on the type of speech act and various context con-ditions."
Values of template variables (such as ori-gin and destination cities) are instantiated by the dialog manager.
The string templates for all the speech acts of a turn are heuristically ordered and then appended to produce the output.
"In order to produce output that is not highly redundant, string templates must be written for every possible com-bination of speech acts in a text plan."
We refer to the output generated by AMELIA using this ap-proach as the TEMPLATE output.
"We perform an evaluation using human sub-jects who judged the TEMPLATE output of AMELIA against five NLG-based approaches: SP O T, two rule-based approaches, and two base- lines."
We describe them in Section 3.
An exam-ple output for the text plan in Figure 2 for each system is in Figure 3.
The experiment required human subjects to read 5 dialogs of real inter-actions with AMELIA.
"At times two or more of these variants coincided, in which case sentences were not repeated and fewer than six sentences were presented to the subjects."
"The subjects rated each variation on a 5-point Likert scale, by stating the degree to which they agreed with the state-ment The system’s utterance is easy to under-stand, well-formed, and appropriate to the dialog context."
Sixty colleagues not involved in this re-search completed the experiment.
This section describes the five sentence planners that we compare.
"SP O T, the two rule-based systems, and the two baseline sentence planners are all NLG based sentence planners."
"In Sec-tion 3.1, we describe the shared representations of the NLG based sentence planners."
"Section 3.2 describes the baselines, R ANDOM and N O A GG ."
"Section 3.3 describes SP O T. Section 3.4 de-scribes the rule-based sentence planners, RBS and ICF."
"In all of the NLG sentence planners, each speech act is assigned a canonical lexico-structural rep-resentation (called a DSyntS – Deep Syntactic Structure[REF_CITE])."
"We exclude issues of lexical choice from this study, and restrict our at-tention to the question of how elementary struc-tures for separate elementary speech acts are as-sembled into extended discourse."
"The basis of all the NLG systems is a set of clause-combining op-erations that incrementally transform a list of el-ementary predicate-argument representations (the DSyntSs corresponding to the elementary speech acts of a single text plan) into a list of lexico-structural representations of one or more sen-tences, that are sent to a surface realizer."
We uti-lize the RealPro Surface realizer with all of the sentence planners[REF_CITE].
DSyntSs are combined using the operations ex-emplified in Figure 4.
"The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves la-beled by all the elementary speech acts from the input text plan, and with its interior nodes la-beled with clause-combining operations."
"As an example, Figure 5 shows the sp-tree for utterance System5 in Figure 1."
Node soft-merge-general merges an implicit-confirmation of the destina-tion city and the origin city.
The row labelled SOFT - MERGE in Figure 4 shows the result when Args 1 and 2 are implicit confirmations of the ori-gin and destination.
See[REF_CITE]for more detail on the sp-tree.
The experimental sen-tence planners described below vary how the sp-tree is constructed.
In one obvious baseline system the sp-tree is con-structed by applying only the P ERIOD operation: each elementary speech act is realized as its own sentence.
"This baseline, N O A GG , was suggested[REF_CITE]."
"For N O A GG , we order the communicative acts from the text plan as follows: implicit confirms precede explicit confirms precede requests."
Figure 3 includes a N O A GG output for the text plan in Figure 2.
A second possible baseline sentence planner simply applies combination rules randomly ac-cording to a hand-crafted probability distribution based on preferences for operations such as the MERGE family over CONJUNCTION and PERIOD .
"In order to be able to generate the resulting sen-tence plan tree, we exclude certain combinations, such as generating anything other than a P ERIOD above a node labeled P ERIOD in a sentence plan."
The resulting sentence planner we refer to as R ANDOM .
Figure 3 includes a R ANDOM output for the text plan in Figure 2.
"In order to construct a more complex, and hopefully better, sentence planner, we need to en-code constraints on the application of, and order-ing of, the operations."
It is here that the remaining approaches differ.
"In the first approach, SP O T, we learn constraints from training material; in the second approach, rule-based, we construct con-straints by hand."
"For the sentence planner SP O T, we reconceptu-alize sentence planning as consisting of two dis-tinct phases as in Figure 6."
"In the first phase, the sentence-plan-generator (SPG) randomly gener-ates up to twenty possible sentence plans for a given text-plan input."
For this phase we use the R ANDOM sentence-planner.
"In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer."
The SPR is automatically trained by applying RankBoost[REF_CITE]to learn ranking rules from training data.
The training data was assembled by using R ANDOM to randomly generate up to 20 realizations for 100 turns; two human judges then ranked each of these realizations (using the setup described in Section 2).
"The SPR uses these rules to rank alternative sp-trees, and then selects the top-ranked output as input to the surface realizer."
"It has not been the object of our research to con-struct a rule-based sentence planner by hand, be it domain-independent or optimized for our do-main."
Our goal was to compare the SP O T sen-tence planner with a representative rule-based system.
"We decided against using an existing off-the-shelf rule-based system, since it would be too complex a task to port it to our application."
"In-stead, we constructed two reasonably representa-tive rule-based sentence planners."
"This task was made easier by the fact that we could reuse much of the work done for SP O T, in particular the data structure of the sp-tree and the implementation of the clause-combining operations."
"We developed the two systems by applying heuristics for pro- ducing good output, such as preferences for ag-gregation."
They differ only in the initial ordering of the communicative acts in the input text plan.
"In the first rule-based system, RBS (for “Rule-Based System”), we order the speech acts with explicit confirms first, then requests, then implicit confirms."
Note that explicit confirms and requests do not co-occur in our data set.
"The second rule-based system is identical, except that implicit con-firms come first rather than last."
This system we call ICF (for “Rule-based System with Implicit Confirms First”).
"In the initial step of both RBS and ICF, we take the two leftmost members of the text plan and try to combine them using the follow-ing preference ranking of the combination op-erations: A DJECTIVE , the M ERGE s, C ONJUNC - TION , R ELATIVE - CLAUSE , P ERIOD ."
The first operation to succeed is chosen.
"This yields a bi-nary sp-tree with three nodes, which becomes the current sp-tree."
"As long as the root node of the current sp-tree is not a P ERIOD , we iterate through the list of remaining speech acts on the ordered text plan, combining each one with the current sp-tree using the preference-ranked opera-tions as just described."
"The result of each iteration step is a binary, left-branching sp-tree."
"However, if the root node of the current sp-tree is a P ERIOD , we start a new current sp-tree, as in the initial step described above."
"When the text plan has been ex-hausted, all partial sp-trees (all of which except for the last one are rooted in P ERIOD ) are com-bined in a left-branching tree using P ERIOD ."
Cue words are added as follows: (1) The cue word now is attached to utterances beginning a new subtask; (2) The cue word and is attached to ut-terances continuing a subtask; (3) The cue words alright or okay are attached to utterances contain-ing implicit confirmations.
Figure 3 includes an RBS and an ICF output for the text plan in Fig-ure 2.
In this case ICF and RBS differ only in the verb chosen as a more general verb during the SOFT - MERGE operation.
We illustrate the RBS procedure with an ex-ample for which ICF works similarly.
"For RBS, the text plan in Figure 2 is ordered so that the re-quest is first."
"For the request, a DSyntS is cho-sen that can be paraphrased as What time would you like to leave?."
"Then, the first implicit-confirm is translated by lookup into a DSyntS which on its own could generate Leaving in September."
"We first try the A DJECTIVE aggregation opera-tion, but since neither tree is a predicative ad-jective, this fails."
We then try the M ERGE fam-ily.
"M ERGE -G ENERAL succeeds, since the tree for the request has an embedded node labeled leave."
"The resulting DSyntS can be paraphrased as What time would you like to leave in Septem-ber?, and is attached to the new root node of the resulting sp-tree."
"The root node is labeled M ERGE - GENERAL , and its two daughters are the two speech acts."
"The implicit-confirm of the day is added in a similar manner (adding an-other left-branching node to the sp-tree), yielding a DSyntS that can be paraphrased as What time would you like to leave on September the 1st? (us-ing some special-case attachment for dates within M ERGE )."
"We now try and add the DSyntS for the implicit-confirm, whose DSyntS might gener-ate Going to Dallas."
"Here, we again cannot use A DJECTIVE , nor can we use M ERGE or M ERGE - GENERAL , since the verbs are not identical."
"In-stead, we use S OFT - MERGE - GENERAL , which identifies the leave node with the go root node of the DSyntS of the implicit-confirm."
"When soft-merging leave with go, fly is chosen as a general-ization, resulting in a DSyntS that can be gener-ated as What time would you like to fly on Septem-ber the 1st to Dallas?."
The sp-tree has added a layer but is still left-branching.
"Finally, the last implicit-confirm is added to yield a DSyntS that is realized as What time would you like to fly on September the 1st to Dallas from Newark?."
"The experiment resulted in a total of 1200 judgements for each of the systems be-ing compared, since each subject judged 20 ut-terances by each system."
"We first discuss overall differences among the different systems and then make comparisons among the four different types of systems: (1) TEMPLATE , (2) SP O T, (3) two rule-based systems, and (4) two baseline systems."
All statistically significant results discussed here had p values of less than .01.
We first examined whether differences in hu-man ratings (score) were predictable from the type of system that produced the utterance be-ing rated.
A one-way ANOVA with system as the independent variable and score as the dependent variable showed that there were significant differ-ences in score as a function of system.
The overall differences are summarized in Figure 7.
"As Figure 7 indicates, some system outputs re-ceived more consistent scores than others, e.g. the standard deviation for TEMPLATE was much smaller than R ANDOM ."
"The ranking of the sys-tems by average score is TEMPLATE , SP O T, ICF, RBS, N O A GG , and R ANDOM ."
Posthoc compar-isons of the scores of individual pairs of systems using the adjusted Bonferroni statistic revealed several different groupings. [Footnote_2]
2 The adjusted Bonferroni statistic guards against acci-dentally finding differences between systems when making multiple comparisons among systems.
"The highest ranking systems were TEMPLATE and SP O T, whose ratings were not statistically significantly different from one another."
"This shows that it is possible to match the quality of a hand-crafted system with a trainable one, which should be more portable, more general and re-quire less overall engineering effort."
"The next group of systems were the two rule-based systems, ICF and RBS, which were not statistically different from one another."
However SP O T was statistically better than both of these systems (p .01).
Figure 8 shows that SP O T got more high rankings than either of the rule-based systems.
"In a sense this may not be that surprising, because[REF_CITE]point out, it is difficult to construct a rule-based sentence planner that handles all the rule interac-tions in a reasonable way."
Features that SPoT’s SPR uses allow SP O T to be sensitive to particular discourse configurations or lexical collocations.
"In order to encode these in a rule-based sentence planner, one would first have to discover these constraints and then determine a way of enforc-ing them."
"However the SPR simply learns that a particular configuration is less preferred, result-ing in a small decrement in ranking for the cor-responding sp-tree."
This flexibility of increment-ing or decrementing a particular sp-tree by a small amount may in the end allow it to be more sensi-tive to small distinctions than a rule-based system.
"Along with the TEMPLATE and RULE - BASED systems, SP O T also scored better than the base-line systems N O A GG and R ANDOM ."
"This is also somewhat to be expected, since the baseline sys-tems were intended to be the simplest systems constructable."
"However it would have been a pos-sible outcome for SP O T to not be different than either system, e.g. if the sp-trees produced by R ANDOM were all equally good, or if the ag-gregation rules that SP O T learned produced out-put less readable than N O A GG ."
"Figure 8 shows that the distributions of scores for SP O T vs. the baseline systems are very different, with SP O T skewed towards higher scores."
"Interestingly N O A GG also scored better than R ANDOM (p .01), and the standard deviation of its scores was smaller (see Figure 7)."
Remem-ber that R ANDOM ’s sp-trees often resulted in ar-bitrarily ordering the speech acts in the output.
"While N O A GG produced redundant utterances, it placed the initiative taking speech act at the end of the utterance in its most natural position, possibly resulting in a preference for N O A GG over R AN - DOM ."
Another reason to prefer N O A GG could be its predictability.
Other work has also explored automatically train-ing modules of a generator[REF_CITE].
"However, to our knowledge, this is the first re-ported experimental comparison of a trainable technique that shows that the quality of system utterances produced with trainable components can compete with hand-crafted or rule-based tech-niques."
"The results validate our methodology; SP O T outperforms two representative rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more eas-ily and quickly tuned to a new domain: the train- ing materials for the SP O T sentence planner can be collected from subjective judgements from a small number of judges with little or no linguistic knowledge."
Previous work on evaluation of natural lan-guage generation has utilized three different ap-proaches to evaluati[REF_CITE].
"The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources[REF_CITE]."
"Other work has evaluated template-based spoken dialog genera-tion with a task-based approach, i.e. the genera-tor is evaluated with a metric such as task com-pletion or user satisfaction after dialog comple-ti[REF_CITE]."
"This approach can work well when the task only involves one or two ex-changes, when the choices have large effects over the whole dialog, or the choices vary the con-tent of the utterance."
"Because sentence plan-ning choices realize the same content and only affect the current utterance, we believed it impor-tant to get local feedback."
A final approach fo-cuses on subproblems of natural language gener-ation such as the generation of referring expres-sions.
For this type of problem it is possible to evaluate the generator by the degree to which it matches human performance[REF_CITE].
"When evaluating sentence planning, this approach doesn’t make sense because many dif-ferent realizations may be equally good."
"However, this experiment did not show that trainable sentence planners produce, in general, better-quality output than template-based or rule-based sentence planners."
"That would be im-possible: given the nature of template and rule-based systems, any quality standard for the output can be met given sufficient person-hours, elapsed time, and software engineering acumen."
"Our prin-cipal goal, rather, is to show that the quality of the TEMPLATE output, for a currently operational dia-log system whose template-based output compo-nent was developed, expanded, and refined over about 18 months, can be achieved using a train-able system, for which the necessary training data was collected in three person-days."
"Furthermore, we wished to show that a representative rule-based system based on current literature, without massive domain-tuning, cannot achieve the same level of quality."
"In future work, we hope to extend SPoT and integrate it into AMELIA."
This work was partially funded by DARPA under contract[REF_CITE]-99-3-0003.
"The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised con-trolled clinical trial."
We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever per-formed on an NLG system.
"The de-tailed results of the clinical trial have been presented elsewhere, in the med-ical literature."
"In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG eval-uation techniques."
There is increasing interest in techniques for eval-uating Natural Language Generation ( NLG ) sys-tems.
"However, we are not aware of any previ-ously reported evaluations of NLG systems which have rigorously compared the task effectiveness of an NLG system to a non- NLG alternative."
"In this paper we discuss such an evaluation, a large scale (2553 subjects) randomised controlled clin-ical trial which evaluated the effectiveness of per-sonalised smoking-cessation letters generated by the STOP system[REF_CITE]."
"We be-lieve that this is the largest, most expensive, and perhaps most rigorous evaluation ever done of an NLG system; it was also a disappointing evalua-tion, as it showed that STOP letters in general were no more effective than control letters."
"The detailed results of the STOP evaluation have been presented elsewhere, in the medical lit- erature[REF_CITE]."
"The purpose of this paper is to discuss the clinical trial from an NLG evaluation perspective, in order to help future re-searchers decide when a clinical trial (or similar large-scale task effectiveness evaluation) would be an appropriate way to evaluate their systems."
"Evaluation is becoming increasingly important in NLG , as in other areas of NLP ; see[REF_CITE]for a summary of NLG evaluation."
"As Mellish and Dale point out, we can evalu-ate the effectiveness of underlying theories, gen-eral properties of NLG systems and texts (such as computational speed, or text understandability), or the effectiveness of the generated texts in an actual task or application context."
"Theory eval-uations are typically done by comparing predic-tions of a theory to what is observed in a human-authored corpus (for example,[REF_CITE])."
"Evaluations of text properties are typi-cally done by asking human judges to rate the quality of generated texts (for example,[REF_CITE]); sometimes human-authored texts are included in the rated set (without judges knowing which texts are human-authored) to pro-vide a baseline."
"Task evaluations (for example,[REF_CITE]) are typically done by showing hu-man subjects different texts, and measuring dif-ferences in an outcome variable, such as success in performing a task."
"However, despite the above work, we are not aware of any previous evaluation which has com-pared the effectiveness of NLG texts at meeting a communicative goal against the effectiveness of non-"
NLG control texts.
"Young’s task eval-uation, which may be the most rigorous previ-ous task evaluation of an NLG system, compared the effectiveness of texts generated by different NLG algorithms, while the IDAS task evaluati[REF_CITE]did not include a con-trol text of any kind."
"NLG alternatives, because they compared the impact of NLG argumentative texts to a no-text control (where users had access to the underlying data but were not given any texts arguing for a particular choice)."
Task evaluations that compare the effectiveness of texts from NLG systems to the effectiveness of non-
"NLG alternatives (mail-merge texts, human-written texts, or fixed texts) are expensive and difficult to organise, but we believe they are es-sential to the progress of NLG , both scientifically and technologically."
In this paper we describe such an evaluation which we performed on the STOP system.
"The evaluation was indeed expen-sive and time-consuming, and ultimately was dis-appointing in that it suggested STOP texts were no more effective than control texts, but we believe that this kind of evaluation was essential to the project."
We hope that our description of the STOP clinical trial and what we learned from it will en-courage other researchers to consider performing effectiveness evaluations of NLG systems against non-
The STOP system has been described elsewhere[REF_CITE].
"Very briefly, the system took as input a 4-page questionnaire about smoking history, habits, intentions, and so forth, and from this produced a small (4 pages of A5) person-alised smoking cessation letter."
"All interactions with the smoker were paper-based; he or she filled out a paper questionnaire which was scanned into the computer system, and the resultant letter was printed out and posted back to the smoker."
"The first page of a typical questionnaire is shown in Figure 1, and part of the letter produced from this questionnaire is shown in Figure 2. [Footnote_1]"
"1 To protect patient confidentiality, we have changed the name of the smoker and her medical practice, and typed her handwritten responses."
"We wish to emphasise that producing personalised health in-formation letters is not a new idea, many previous researchers have worked in this area; see[REF_CITE]for a comparison of STOP to previous work in this area."
"The STOP clinical trial, which is the focus of this paper, was organised as follows."
"We con-tacted 7427 smokers, and asked them to partici-pate in the trial. 2553 smokers agreed to partic-ipate, and filled out our smoking questionnaire."
These smokers were randomly split among three groups:
These smokers received the letter generated by STOP from their questionnaire.
These smokers received a fixed (non-tailored) letter.
"The non-tailored letter was essentially the letter produced by STOP from a blank questionnaire, with some manual post-editing and tidying up."
"In other words, during the course of developing STOP we created a set of default rules for han-dling incomplete or inconsistent question-naires; the non-tailored letter was produced by activating these default rules without any smoker data."
Part of the non-tailored letter is shown in Figure 3.
These smokers just received a let-ter thanking them for participating in our study.
"After six months we sent a followup question-naire asking participants if they had quit, and also other questions (for example, if they were intend-ing to try to quit even if they had not actually done so yet)."
"Smokers could also make free-text com-ments about the letter they received. 2045 smok-ers responded to the followup questionnaire, of which 154 claimed to have quit."
"Because people do not always tell the truth about their smoking habits, we asked these 154 people to give saliva samples, which were tested in a lab for nicotine residues. 99 smokers agreed to give such samples, and 89 of these were confirmed as non-smokers."
"The STOP clinical trial took 20 months to run (of which the first 4 months overlapped soft-ware development), and cost about UK£75,000 (US$110,000)."
We believe the STOP clinical trial was the longest and costliest evaluation ever done of an NLG system.
The length and cost of the clin-ical trial were primarily due to the large numbers of subjects.
"The cost of the trial was partially stationary and postage (we sent out over 10000 mailings to smokers, each of which included a reply-paid envelope), but mostly staff costs to set up the trial, perform the mailings, pro-cess and analyse the returns from smokers, and handle various glitches in the trial."
Another way of looking at the trial was that we spent about UK£30 (US$45) per subject (includ-ing staff time as well as materials).
"Perhaps the trial could have been done a bit more cheaply, but any experiment involving 2553 subjects is bound to be expensive and time-consuming."
The reason the trial needed to be so large was that we were measuring a binary outcome vari-able (laboratory-verified smoking cessation) with a very low positive rate (since smoking is a very difficult habit to quit).
"Young, in contrast, mea-sured numerical variables (such as the number of mistakes made by a user when following textual instructions) with substantial standard deviations."
"Another complication was that we wanted to use a representative sample of smokers in our trial, which meant that we could not (as Young and Levine and Mellish did) just recruit students and acquaintances."
"Instead, we contacted a repre-sentative set of GPs in our area, and asked them for a list of smokers from their patient record sys-tems."
This was the source of the 7427 initial smokers mentioned above.
"Detailed results of the STOP clinical trial, includ-ing statistical tables, have been published in the medical literature[REF_CITE]."
Here we just summarise the key findings which are of NLG (as well as medical) interest.
These broke down by group as follows: 3.5% (30 out of 857) of the tailored group stopped smoking 4.4% (37 out of 846) of the non-tailored group stopped smoking 2.6% (22 out of 850) of the no-letter group stopped smoking
"The non-tailored group had the lowest number of heavy (more than 20 cigarettes per day) smok-ers, who are less likely to stop smoking (because they are probably addicted to nicotine) than light smokers; the tailored group had the highest num-ber of heavy smokers."
"After adjusting for this fact, cessation rates were still higher in the non-tailored group than in the tailored group, but this difference was not statistically significant."
We can see this if we look just at cessation rates in light smokers (few heavy smokers from any cate-gory managed to stop smoking): 4.3% (25 out of 563) of the light smokers in the tailored group stopped smoking 4.9% (31 out of 597) of the light smokers in the non-tailored group stopped smoking 2.7% (16 out of 582) of the light smokers in the no-letter group stopped smoking
"The overall conclusion is therefore that recipi-ents of the non-tailored letters were more likely to stop than people who got no letter [Footnote_2] (p=.047 over-all unadjusted; p=.069 overall after adjusting for differences between groups, such as heavy/light smoker split; p=.049 for light smokers)."
"2 Note that while a 1% or 2% increase in cessation rates is small, it is medically useful if it can be achieved cheaply.[REF_CITE]for a discussion of success rates and cost-effectiveness of various smoking-cessation tech-niques, and[REF_CITE]for an analysis that shows that sending letters is very cost-effective compared to most other smoking-cessation techniques."
"How-ever, there was no evidence that the tailored let-ters were any better than the non-tailored ones in terms of increasing cessation rates."
There is some very weak evidence that the tai-lored letter may have been better than the non-tailored letter among smokers for whom quitting was especially difficult.
"For example, among dis-couraged smokers (people who wanted to quit but were not intending to quit, usually because they didn’t think they could quit), cessation rates were 60% higher among recipients of tailored let-ters than recipients of non-tailored letters, but the numbers were too small to reach statistical signif-icance, since (as with heavy smokers) very few such people managed to stop smoking."
"Further-more, among heavy smokers, recipients of the tai-lored letter were 50% more likely than recipients of the non-tailored letters to show increased inten-tion to quit (for example, say in their initial ques-tionnaire that they did not intend to quit, but say in the followup questionnaire that they did intend to quit) (p=.059)."
"It would be nice to test the hy-pothesis that tailored letters were effective among discouraged smokers or heavy smokers by run-ning another clinical trial, but such a trial would need to be even bigger and more expensive than the STOP trial, in order to have enough validated quitters from these categories to make it possible to draw statistically significant conclusions."
"Recipients of the tailored letters were more likely than recipients of non-tailored letters to re-member receiving the letter (67% vs 44%, signif-icant at p .01), to have kept the letter (30% vs 19%, significant at p .01), and to make a free-text comment about the letter (20% vs 12%, sig-nificant at p .01)."
"However, there was no statis-tically significant difference in perceptions of the usefulness and relevance of the tailored and non-tailored letters."
"Free-text comments on the tailored letters were varied, ranging from I carried mine with me all the time and looked at it whenever I felt like giving in to"
I found it patronising ...
Smoking obviously impairs my physical health — not my intelligence!
The most common complaint about content was that not enough information was given about practical ‘how-to-stop-smoking’ techniques.
STOP ’s tailoring rules only included such information in about one third of the letters; this was in accordance with the well-established Stages of Change model of smoking cessation (Prochaska and di[REF_CITE]).
Note that all recipients of the non-tailored letter received such information.
"If practical advice was useful to more than one third of smokers, then the Stages-of-Change based tailoring rules which decided when to include such information may have de-creased rather than increased letter effectiveness."
"One of the remarkable things about the NLG , NLP , and indeed AI literatures is that little men-tion is made of experiments with negative results."
"In more established fields such as medicine and physics, papers which report negative experimen-tal findings are common and are valued; but in NLP they are rare."
It seems unlikely that NLP ex-periments always produce positive results (unless the experiments are badly designed and biased to-wards demonstrating the experimenter’s desired outcome); what is probably happening is that peo-ple are choosing not to report negative results.
One reason for this may be that it can be diffi-cult to draw clear lessons from a negative result.
"In the case of STOP , for example, the clinical trial did not tell us why STOP failed."
"There are many possible reasons for the negative result, including: 1."
Tailoring cannot have much effect.
"That is, if a smoker receives a letter from his/her doctor about smoking, then the content of the let-ter is only of secondary importance, the im-portant thing is the fact of having received a communication from his/her doctor encour-aging smoking cessation. 2."
"Tailoring could have an impact, but only if it was based on much more knowledge about the smoker’s circumstances than is available via a 4-page multiple choice questionnaire. 3."
"Tailoring based on a multiple-choice ques-tionnaire can work, we just didn’t do it right in STOP , perhaps in part because we based our system on inappropriate theoretical mod-els of smoking cessation. 4."
"The STOP letters did in fact have an effect on some groups (such as heavy or discour-aged smokers), but the clinical trial was too small to provide statistically significant evi-dence of this."
"In other words, did we fail because (1) what we were attempting could not work; (2) what we were attempting could only work if we had a lot more knowledge available to us; or (3) we built a poor system?"
"Or (4) did the system actually work to some degree, but the evaluation didn’t show this because it was too small?"
"This is a key question for NLG researchers and developers (as opposed to doctors and health administrators who just want to know if they should use STOP as a black-box system), but the clinical trial does not distinguish between these possibilities."
Arguments can be made for all three of the above possibilities.
"For example, we could argue for (1) on the basis that brief discussions about smoking with a doctor have about a 2% success rate[REF_CITE], and this may be an up-per limit for the effectiveness of a brief letter from a doctor."
"If so, then letters cannot do much better that the 1.8% increase in cessation rates produced by the STOP non-tailored letter."
"Or we could ar-gue for (2) by noting that when we asked smok-ers to comment on STOP letters in a small pilot study, many of their comments were very specific to their particular circumstances For example, a single mother mentioned that a previous attempt to stop failed because of stress caused by dealing with a child’s tantrum, and an older woman dis-cussed the various stop-smoking techniques she had tried in the past and how they failed."
Per-haps tailoring according to such specific circum-stances would add value to letters; but such tai-loring would require much more information than can be obtained from a 4-page multiple-choice questionnaire.
"We could also argue for (3) be-cause there clearly are many ways in which the tailored letters could have been improved (such as having practical ‘how-to-stop’ tips in more let-ters, as mentioned at the end of Section 4); and for (4) on the basis of the weak evidence for this mentioned in Section 4."
"We do not know which of the above reason(s) were responsible for STOP ’s failure, so we can-not give clear lessons for future researchers or de-velopers."
"This is perhaps true of many negative experimental results, and may be a reason why people do not publish them in the NLP commu-nity."
"Again there is perhaps a different attitude in the medical community, where papers describ- ing experiments are taken as ‘data points’ and more theoretically minded researchers may look at a number of experimental papers and see what patterns and insights emerge from the collection as a whole."
"Under this perspective it is less im-portant to state what lessons or insights can be drawn from a particular negative result, what mat-ters is the overall pattern of positive and negative results in a group of related experiments."
"And like most such procedures, the process of infer-ring general rules from a collection of specific ex-perimental results will work much better if it has access to both positive and negative examples; in other words, if researchers publish their failures as well as their successes."
"We believe that negative results are also impor-tant in NLG , NLP , and AI , even if it is not possible to draw straightforward lessons from them; and we hope that more such results are reported in the future."
"The clinical trial was by far the biggest evaluation exercise in STOP , but we also performed some smaller evaluations in order to test our algorithms and knowledge acquisition methodology[REF_CITE]."
These included: 1.
"Asking smokers or domain experts to read two letters, and state which one they thought was superior; 2."
Statistical analyses of characteristics of smokers; and 3. Comparing the effectiveness of different al-gorithms at filling up but not exceeding 4 A5 pages.
"These evaluations were much smaller, simpler, and cheaper than the clinical trial, and often gave easier to interpret results."
"For example, the letter-comparison experiments suggested (al-though they did not prove) that older people pre-ferred a more formal writing style than younger people; the statistical analysis suggested (al-though again did not prove) that the tailoring rules should have been more influenced by level of ad-diction; and the algorithmic analysis showed that a revision architecture outperformed a conven-tional pipeline architecture."
"So, these experiments produced clearer results at a fraction of the cost of the clinical trial."
"But the cheapness of (1) and (2) were partially due to the fact that they were too small to produce sta-tistically solid findings, and the cheapness of (2) and (3) were partially due to the fact that they ex-ploited data sets and resources that were built as part of the clinical trial."
"Overall, we believe that these small-scale experiments were worth doing, but as a supplement to, not a replacement for, the clinical trial."
When is it appropriate to evaluate an NLG system with a large-scale task or effectiveness evaluation which compares the NLG system to a non-
"Certainly this should be done when a customer is seriously considering using the sys-tem, indeed customers may refuse to use a system without such testing."
"Controlled task/effectiveness evaluations are also scientifically important, because they provide a technique for testing applied hypotheses (such as ‘ STOP produces effective smoking-cessation letters’)."
"As such, they should be considered whenever a researcher is interested in testing such hypotheses."
"Of course, much research in NLG is primarily theoretical, and thus perhaps best tested by corpus studies or psycholinguistic ex-periments; and much work in applied NLG is con-cerned with pilot studies and other hypothesis for-mation exercises."
"But at the end of the day, re-searchers interested in applied NLG need to test as well as formulate hypotheses."
"While many speech recognition and natural-language understanding applications can be tested by comparing their out-put to a human-produced ‘gold standard’ (for ex-ample, speech recogniser output can be compared to a human transcription of a speech signal), this to date has been harder to do in NLG , especially in applications such as STOP where there are no hu-man experts[REF_CITE](there are many experts on personalised oral communication with smokers, but none on personalised written com-munication, because no one currently writes per-sonalised letters to smokers)."
"In such applica-tions, the only way to test hypotheses about the effects of systems on human users may be to run a controlled task/effectiveness evaluation."
"In other words, there’s probably no point in conducting a large-scale task/effectiveness evalu-ation of an NLG system if you’re interested in for-mulating hypotheses instead of testing them, or if you’re interested in theoretical instead of applied hypotheses."
"But if you want to test an applied hy-pothesis about the effect of an NLG system on hu-man users, the most rigorous way of doing this is to conduct an experiment where you show some users your NLG texts and other users control texts, and measure the degree to which the desired ef-fect is achieved in both groups."
"Large-scale evaluation exercises also have the benefit of forcing researchers and developers to make systems robust, and to face up to the messi-ness of real data, such as awkward boundary cases and noisy data."
"Indeed we suspect that STOP is one of the most robust non-commercial NLG sys-tems ever built, because the clinical trial forced us to think about issues such as what we should do with inconsistent or improperly scanned question-naires, or what we should say to unusual smokers."
"In conclusion, large-scale task/effectiveness evaluations are expensive, time-consuming, and a considerable hassle."
"But they are also an essential part of the scientific and technological process, especially in testing applied hypotheses about the effectiveness of systems on real users."
"We hope that more such evaluations are performed in the future, and that their results are reported whether they are positive or negative."
"In this paper we present a thorough evaluation of a corpus resource for Portuguese, CETEMPúblico, a 180-million word newspaper corpus free for R&amp;D in Portuguese processing."
"We provide information that should be useful to those using the resource, and to considerable improvement for later versions."
"In addition, we think that the procedures presented can be of interest for the larger NLP community, since corpus evaluation and description is unfortunately not a common exercise."
"CETEMPúblico is a large corpus of European Portuguese newspaper language, available at no cost to the community dealing with the processing of Portuguese. [Footnote_1]"
"1 CETEMPúblico stands for “Corpus de Extractos de Textos Electrónicos MCT / Público”, and its full reference is[URL_CITE]"
"It was created in the framework of the Computational Processing of Portuguese project, a government funded initiative to foster language engineering of the Portuguese language. [URL_CITE]"
"Evaluating this resource, we have two main goals in mind: To contribute to improve its usefulness; and to suggest ways of going about as far as corpus evaluation is concerned in general (noting that most corpora projects are simply described and not evaluated)."
"In fact, and despite the amount of research devoted to corpus processing nowadays, there is not much information about the actual corpora being processed, which may lead naïve users and/or readers to conclude that this is not an interesting issue."
"In our opinion, that is the wrong conclusion."
"There is, in fact, a lot to be said about any particular corpus."
"We believe, in addition, that such information should be available when one is buying, or even just browsing, a corpus, and it should be taken into consideration when, in turn, systems or hypotheses are evaluated with the help of that corpus."
"In this paper, we will solely be concerned with CETEMPúblico, but it is our belief that similar kinds of information could be published about different corpora."
Our intention is to give a positive contribution both to the whole community involved in the processing of Portuguese and to the particular users of this corpus.
"At the moment of writing, 160 people have ordered (and, we assume, consequently received) it [Footnote_3] ."
3 Although we also made available a CQP[REF_CITE]encoded version[REF_CITE]the vast majority of the users received the text-only version.
There have also been more than four thousand queries via the Web site which gives access to the corpus.
We want to provide evaluation data and describe how one can improve the corpus.
"We are genuinely interested in increasing its value, and have, since corpus release, [Footnote_4] made available four patches (e-mailing this information to all who ordered the corpus)."
"4 The corpus was ready[REF_CITE]; the first copies were sent out in October, with the information that version 1.0 creation date was 25[REF_CITE]."
We have also tried to considerably improve the Web page.
"We decided to concentrate on the evaluation of version 1.0, given that massive distribution was done of that particular version [Footnote_5] ."
"5 We have no estimate of how many users have actually succeeded, or even tried, to apply the patches made available later on. We have just launched a Web questionnaire in order to have a better idea of our user community."
Web access to the corpus[REF_CITE]will not be dealt with here.
Note that all trivial improvements described here have already been addressed in some patch.
"As described in detail[REF_CITE]and also in the FAQ at the corpus Web page, CETEMPúblico was built from the raw material provided by the Portuguese daily newspaper Público: text files in Macintosh format, covering approximately the years 1991 to 1998, and including both published news articles and those created but not necessarily brought to print."
"These files were automatically tagged with a classification based on, but not identical to, the one used by the newspaper to identify sections, and with the semester the article was associated to."
"In addition, sentence separation, and title and author identification were automatically created."
The texts were then divided in extracts with an average length of two paragraphs.
"These extracts were randomly shuffled (for copyright reasons) and numbered, and the final corpus was the ordered sequence of the extract numbers."
"To illustrate the corpus in text format, we present in Appendix A an extract that includes all possible tags with the exception of &lt;marca&gt;."
"We start by commenting on the distribution process, and then go on to analyse the corpus contents and the specific options chosen in its creation."
Let us first comment on the distribution options.
"While this resource is entirely free (one has just to register in a Web page in order to receive the corpus at the address of one’s choice), several critical remarks are not out of place:"
"First of all, when publicizing the resource, it was not clear for whom the CD distribution was actually meant: Later on, we discovered that many traditional linguists ordered it just to find out that they were much better off with the on-line version."
"Second, more accompanying information in the CD would not hurt, instead of pointing to a Web page as the only source: In fact, the assumption that everyone has access to the Web while working with CETEMPúblico is not necessarily true in Portugal or Brazil."
"Finally, we did not produce a medium-size technical description; in addition to the FAQ on the Web page, we provided only a full paper[REF_CITE]describing the whole project, arguably an overkill."
"About the corpus contents, several fundamental decisions can – and actually have, in previous conferences or by e-mail – be criticized, in particular the use of a single text source and the inclusion of sentence tags (by criteria so far not yet documented)."
"Still, we think that both are easy to defend, since 1) the time taken in copyright handling and contract writing with every copyright owner strongly suggests minimizing their number."
"And 2) although sentence separation is a controversial issue, it is straightforward to dispose of sentence separation tags."
"So, this option cannot really be considered an obstacle to users. [Footnote_6]"
"6 Since extract definition is based on paragraph and not sentence boundary, the option of marking &lt;s&gt; boundaries has no other consequences."
"We will concentrate instead on each annotation, after discussing the choice of texts and extracts."
"Looking at the final corpus, it is evident that many extracts should be discarded or, at least, rewritten."
"We tried to remove specific kinds of &quot;text&quot;, namely soccer classifications, citations from other newspapers, etc., but it is still possible to detect several other objects of dubious interest in the resulting corpus."
"In fact, using regular expression patterns of the kind “existence of multiple tabs in a line ending in numbers”, we identified 5270 extracts having some form of classification, as well as 662 extracts with no valid content."
"Now, it is arguable that classifications of other sports (e.g., athletics and motor races), solutions to crossword puzzles, film and book reviews, and TV programming tables, just to name a few, should have been extracted on the same grounds presented for removing soccer."
Our decision was obviously based on a question of extent. (Soccer results are much more frequent.)
"However, we now regret this methodological flaw and would like to clean up a little more (as done in the patches), or add back soccer results."
"Another problem detected, concerning the extract structure, was our unfortunate algorithm of appending titles to the previous extract, just like authors, instead of joining them to the next extract."
This means that 4.8% of the extracts end with a title in CETEMPúblico. (9.6% end with an author.)
"The worst problem presented by the CETEMPúblico corpus is the question of repeated material. (Incidentally, it is interesting to note that this is also a serious problem in searching the Web, as mentioned[REF_CITE].)"
Repeated articles [Footnote_7] can be due to two independent factors: - parallel editions of the local section of the newspaper in the two main cities of Portugal (Lisboa and Porto) - later publication of previously “rejected” articles
"7 Repeated sentences can also occur in the lead and in the body of an article, and (in the opinion section) to highlight parts of an article."
"In addition to manually inspecting rare items that one would not expect to appear more than a few times in the corpus (but which had higher frequency than expected), we used the following strategies to detect repeated extracts: 1. Record the first and last 40 characters of each extract, in a hash table, as well as their size in characters."
Then fully compare only the repeated extracts under this criterion. 2.
"Using the Perl module MD5 (useful for cryptographical purposes), we attributed to each extract a checksum of 32 bytes, and recorded it in a hash table."
"Repeated extracts have the same checksum, but it is extremely unlikely that two different ones will."
The results obtained for exactly equal extracts are displayed in Table 1 for both methods.
"Another related (and obviously more complicated) problem is what to do with quasi-duplicates, i.e. sentences or texts that are almost, but not, identical."
"An estimate of the number of approximately equal extracts, obtained with the 40 character-method but with relaxed size constraints (10%) yields some further 15,665 possibly repeated extracts."
"It is not obvious whether one can automatically identify which one is the revised version, or even whether it is desirable to choose that one."
"We have, anyway, compiled a list of these cases, thinking that they might serve as raw material for studying the revision process (and to obtain a list of errors and their correction)."
"In the CETEMPúblico corpus, newspaper titles and subtitles, as well as author identifications, have been marked up as result of heuristic processing."
"Given the corpus, we want to address precision and error rate (i.e., of all chunks tagged as titles, how many have been rightly tagged?, and how many are wrong?)."
"We reviewed manually the first 500 instances of &lt;t&gt; [Footnote_8] , of which 427 were undoubtedly titles, a further 4 wrongly tagged authors, and at least 15 belonged to book or film reviews, indicating title, author and publisher, or director and broadcasting date, etc."
"8[REF_CITE]th chunk of the corpus. This aparently naïve choice of test data does not bias evaluation, since the extracts are randomly placed in the corpus and do not reflect any order of time period or kind of text."
"We then looked into the following error-prone situation: After having noted that several paragraphs in a row including title and author tags were usually wrong (and should have been marked as list items instead), we looked for extracts containing sequences of four titles / authors and manually checked 200."
The precision in this case was very low:[REF_CITE]% were correctly tagged.
"Of the incorrect ones, as much as 34% were part of book reviews as described above."
This indicates clearly that we should have processed special text formats prior to applying our general heuristic rules.
"Regarding recall, we did the following partial inspection: We noted several short sentences ending in ? or ! (a criterion to parse a text chunk as a full sentence) that should actually be tagged as titles."
"We therefore looked at 200 paragraphs with one single sentence ending in question or exclamation mark containing less than 8 words, and concluded that 41 cases (20%) could definitively be marked as titles, while no less than 85 of these cases where questions taken from interviews."
Most other cases were questions inside ordinary articles.
"As far as authors are concerned, the phrase Leitor devidamente identificado (“duly identified reader”, used to sign reader&apos;s letters where the writer does not wish to disclose his or her identity) was correctly identified only in 78% of the cases (135 in 172)."
"From a list of 500 authors randomly extracted for evaluation purposes, only 395 (79%) were unambiguously so, while 8 (1.5%) could still be considered correct by somehow more relaxed criteria."
"We thus conclude that up to 21% of the author tags in the corpus may be wrongly attributed, a figure much higher than the originally estimated 4%."
"Among those cases, foreign names (generally in the context of film or music reviews, or book presentations) were frequently mistagged as authors of articles in Público, a situation highly unlikely and amenable to automatic correction."
Figure 1 is an example.
"In addition to paragraph separation coming from the original newspaper files, CETEMPúblico comes with sentence separation as an added-value feature."
"Now, sentence separation is obviously not a trivial question, and there are no foolproof rules for complicated cases[REF_CITE]."
"So, instead of trying to produce other subjective criteria for evaluating a particularly delicate area, we decided to look at the amount of work needed to revise the sentence separation for a given purpose, as reported in section 4.2."
But we did some complementary searches for cases we would expect to be wrong whatever the sentence separation philosophy.
"We thus found 6,358 sentences initiated by a punctuation mark (comma, closing quotes, period, question mark and exclamation mark, respectively amounting to 4053, 410, 1607, 227 and 61 occurrences), as well as a plethora of suspiciously small sentences, cf."
"Sentence separation marks some sentences as fragments (&lt;s frag&gt;); in addition, the &lt;li&gt; attribute was used to render list elements."
We are not sure now whether it was worthwhile to have two different markup elements.
"Finally, the sentence separation module also introduces the &lt;marca&gt; tag to identify meta-characters that are used for later coreference (eg. in footnotes)."
"The asterisk &quot;*&quot; was marked as such in CETEMPúblico, but not inside author or title descriptions, an undesirable inconsistency."
"An annoying detail is the amount of strange characters that have remained in the corpus after font conversion, such as non-Portuguese characters, hyphens, bullet list marking, and the characters &lt; &gt; instead of quotes."
"It is straightforward to replace these with other[REF_CITE]-1 characters or combinations of characters, as was done with dashes and quotes. [Footnote_9] Only the last line of Table 4 requires some care, since É is a otherwise valid Portuguese character that should only be replaced a few times."
9 Note that it is not always possible to have a one-to-one mapping from MacRoman[REF_CITE]-1.
CETEMPúblico extracts come with a subject classification derived from (but not equal to) the original newspaper section.
"Due to format differences of the original files, only 86% of the extracts have some classification associated."
The others carry the label ND (not determined).
"We evaluate here this classification, since for half of the corpus article separation had to be carried out automatically and thus chances exist that errors may have crept in."
The first thing we did was to check whether repeated extracts had been attributed the same classification.
"Astonishingly, there were many differences: of the 47,002 cases of multiple extracts, [Footnote_10],872 (23%) had different categories, even though only in 2% of the cases none of the conflicting categories was ND."
10 Glosses provided are not exhaustive.
Another experiment was to look at well-known polysemic or ambiguous items and see whether their meaning correlated with the kind of text it was purported to be in.
We thus inspected manually several thousand concordances dealing with the following middle frequency words 10 : 201 occurrences of vassoura (broom; last vehicle in a bicycle race); 124 of passador (sieve; drug seller; emigrant dealer); 314 of cunha (wooden object; corruption device); 599 of coxa (noun thigh; adjective lame); 205 of prego (nail; meat sandwich; pawnshop); 145 of garfo (fork; biking); 5505 of estrela (star; filmstar; success); 375 of dobragem (folding; dubbing; parachuting and F1 term); 573 of escravatura (slavery).
We could only find two cases of firm disagreement with source classification (in the two last mentioned queries).
"This is not such a good result as it seems, though, since it can be argued that subject classification is too high level (society, politics, culture) to allow for definite results."
The best way to evaluate a corpus resource is to see how well it fares regarding the tasks it is put to.
"We will not evaluate concordancing for human inspection, because we assume that this is a rather straightforward task for which CETEMPúblico is useful, especially because it requires direct scrutiny."
"Obviously, human inspection and judgement make the results more robust."
One of the authors developed proper name identification tools[REF_CITE]prior to the existence of CETEMPúblico.
We ran them on this corpus to see how they worked.
"We proceeded in the following way: We inspected manually the first 1,000 proper names obtained from CETEMPúblico and got less then 4% wrong, i.e., over 96% precision."
"Then, we computed the distribution of the 52,665 proper nouns identified by the program (23,401 types) on the first million words of the corpus as shown in Table 5, and inspected manually those 1,[Footnote_17] having a length larger or equal than four words."
"17 That is, words routinely used in Portuguese but which up to now have kept a distinctly foreign spelling, such as pullover."
Cases of merging two proper names and cases where it was easy to guess one missing (preceding or following) word accounted each for approximately 5% of the remaining instances.
"While use of CETEMPúblico allowed us to uncover cases not catered for by the program, it also illuminated some potential [Footnote_12] tokenization problems in the corpus, namely a large quantity of tokens ending in a dash (21,455 tokens, 6,458 types) or in a slash (7313 tokens, 4530 types), as well as up to 132,455 tokens including one single parenthesis (28,466 types)."
"12 Different tokenizers may have different strategies, but we assume that these will be hard cases for most."
"The first million words of CETEMPúblico was selected for the creation of a treebank for Portuguese (Floresta Sintá(c)tica [URL_CITE] ), given that its use is copyright cleared and the corpus is free."
"The treebank team engaged in a manual revision of the text prior to treebank coding, refining sentence separation with the help of syntactically-based criteria[REF_CITE]."
"We have tried to compute the amount of change produced by human intervention, which turned out to be a surprisingly complex task[REF_CITE]."
"This one million words subcorpus contained 8,043 extracts. [Footnote_14] Assuming that the first million is not different from the rest of the corpus, the results indicate an estimate of 17% of the corpus extracts in need of improvement."
"14 Numbered from 1 to 8067, since version 1.2 was used, and therefore 24 invalid extracts had been already removed. In addition, the treebank reviewers considered that further 129 should be taken out."
"Looking at sentences, 2,977 sentences of the 42,026 original ones had to be re-separated into 4,304 of the resulting 43,271."
Table 6 displays an estimate of what was actually involved in the revision of sentence tags (percentages are relative to the original number of sentences).
"The &quot;Other&quot; category includes changes among the tags &lt;t&gt;, &lt;a&gt;, &lt;li&gt; and &lt;s&gt;."
"One of the first and most direct uses of a large corpus is to study the coverage, evaluate, and especially improve a spelling checker and morphological analyser."
Our preliminary results of evaluating Jspell[REF_CITE]as far as type and token spelling is concerned are as follows:
"A superficial comparison showed that CETEMPúblico contains a higher percentage of unrecognized words, both types and tokens, than other Portuguese newspaper corpora."
Numbers for a 1.5-million word corpus of Diário do Minho (a regional newspaper) and for a 4-million word corpus of a political party newspaper are respectively 26.5% and 25.41% unrecognized types and 2.26% and 1.67% unrecognized tokens.
"These numbers may be partially explained by Público’s higher coverage of international affairs, together with its cinema and music sections, both bringing an increase in foreign proper names [Footnote_15] ."
15 The percentage of unrecognized tokens varies from 4.8% for culture to 2.0% for society extracts.
"Description Tokens Types Foreign first names 130 125 Portuguese first names 19 [Footnote_16][REF_CITE]208[REF_CITE]34[REF_CITE]45[REF_CITE]23[REF_CITE]48 48[REF_CITE]28 acronyms 81 77 foreign words 171 161 Portuguese foreign words 17 26 25 words missing in dict. 98101 incorrectly spelled 18 36 36 others 33 32 total 1,000 956"
"16 We classify as Portuguese or foreign the word, not the location: thus, Tanzânia is a Portuguese word."
"We investigated the “errors” found by the system, to see how many were real and how many were due to a defficient lexical (or rule) coverage."
"Table 7 shows the distribution of 1,000 “errors” randomly obtained from the 12 th corpus chunk."
The absolute frequencies of the most common spelling errors in CETEMPúblico is another interesting evaluation parameter.
"Applying Jspell to types with frequency &gt; 100 (excluding capitalized and hyphenated words), we identified manually the “real” errors."
"Strikingly, all involved lack or excess of accents."
"The most frequent appeared 840 times (juíz), the second one (saíu) 659, and the third (impôr) had 637 occurrences."
"Their correctly spelled variants (juiz, saiu, impor) appeared respectively 11896, 9892 and 5125 times."
"One can find excellent reports on the difficulties encountered in creating corpora (see e.g.[REF_CITE]and references therein), but it is significantly rarer to get an evaluation of the resulting objects."
It is thus not easy to compare CETEMPúblico with other corpora on the issues discussed here.
"For example, it was not easy to find a thorough documentation[REF_CITE]problems (although there is a mailing list and a specific e-mail address to report bugs), nor is similar information to be found in distribution agencies’ (such as LDC or ELRA)"
"It is obviously outside the scope of the present paper to do a thorough analysis of other corpora as well, but our previous experience shows that it is not at all uncommon to experience problems with characters and fonts, repeated texts or sentences, rubbish-like sections, wrong markup and/or lack of it."
All this independently of corpora being paid and/or distributed by agencies supposed to have performed validation checks.
The same happens for corpora that have been manually revised.
"As regards sentence separation,[REF_CITE]mention that proofreading of the automatic insertion of &lt;s&gt;-units was necessary for the ENPC corpus, but they do not report problems of human editors in deciding what an &lt;s&gt; should be."
"Let us, however, note that ENPC compilers were free to use an &lt;omit&gt; tag for complicated cases and, last but not least, were not dealing with newspaper text."
This paper can be read from a user’s angle as a complement to the documentation of the CETEMPúblico corpus.
"In addition, by showing several simple forms of evaluating a corpus resource, we hope to have inspired others to do the same for other corpora."
"While the work described in this paper already allowed us to publish several patches, improve our corpus processing library and contribute to new versions of other people’s programs, namely Jspell, our future plans are to do more extensive testing using more powerful techniques (e.g. statistical) to investigate other problems or features of the corpus."
"In any case, we believe that the work reported in this paper comes logically first."
"We are first of all grateful to the Público newspaper (especially José Vítor Malheiros, the responsible for the online edition) for making this resource possible."
"We thank José João Dias de Almeida for several suggestions, the team of Floresta Sintá(c)tica for their thorough revision of the first million words, Stefan Evert for invaluable CQP support, and Jan Engh for helpful comments."
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
"The summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection, removing redundant descriptions."
"The summarization components have been extensively evaluated for coherence, accuracy, and non-redundancy of the descriptions produced."
"The explosion of the World Wide Web has brought with it a vast hoard of information, most of it relatively unstructured."
This has created a demand for new ways of managing this often unwieldy body of dynamically changing information.
"The goal of automatic text summarization is to take a partially-structured source text, extract information content from it, and present the most important content in a condensed form in a manner sensitive to the needs of the user and task[REF_CITE]."
"Summaries can be ‘generic’, i.e., aimed at a broad audience, or topic-focused, i.e., tailored to the requirements of a particular user or group of users."
"Summarization (MDS) is, by definition, the extension of single-document summarization to collections of related documents."
"MDS can potentially help the user to see at a glance what a collection is about, or to examine similarities and differences in the information content in the collection."
Specialized multi-document summarization systems can be constructed for various applications; here we discuss a biographical summarizer.
"Biographies can, of course, be long, as in book-length biographies, or short, as in an author’s description on a book jacket."
"The nature of descriptions in the biography can vary, from physical characteristics (e.g., for criminal suspects) to scientific or other achievements (e.g., a speaker’s biography)."
"The crucial point here is that facts about a person’s life are selected, organized, and presented so as to meet the compression and task requirements."
"While book-quality biographies are out of reach of computers, many other kinds can be synthesized by sifting through large quantities of on-line information, a task that is tedious for humans to carry out."
We report here on the development of a biographical MDS summarizer that summarizes information about people described in the news.
"Such a summarizer is of interest, for example, to analysts who want to automatically construct a dossier about a person over time."
"Rather than determining in advance what sort of information should go into a biography, our approach is more data-driven, relying on discovering how people are actually described in news reports in a collection."
"We use corpus statistics from a background corpus along with linguistic knowledge to select and merge descriptions from a document collection, removing redundant descriptions."
The focus here is on synthesizing succinct descriptions.
The problem of assembling these descriptions into a coherent narrative is not a focus of our paper; the system currently uses canned text methods to produce output text containing these descriptions.
"Obviously, the merging of descriptions should take temporal information into account; this very challenging issue is also not addressed here."
"To give a clearer idea of the system’s output, here are some examples of biographies produced by our system (the descriptions themselves are underlined, the rest is canned text)."
"The biographies contain descriptions of the salient attributes and activities of people in the corpus, along with lists of their associates."
These short summaries illustrate the extent of compression provided.
"The first two summaries are of a collection of 1300 wire service news documents on the Clinton impeachment proceedings (707,000 words in all, called the ‘Clinton’ corpus)."
"In this corpus, there are 607 sentences mentioning Vernon Jordan by name, from which the system extracted 82 descriptions expressed as appositives (78) and relative clauses (4), along with 65 descriptions consisting of sentences whose deep subject is Jordan."
The 4 relative clauses are duplicates of one another: “who helped Lewinsky find a job”.
"The sentential descriptions are filtered in part based on the presence of verbs like “testify, “plead”, or “greet” that are strongly associated with the head noun of the appositive, namely “friend”."
The target length can be varied to produce longer summaries.
Vernon Jordan is a presidential friend and a Clinton adviser.
He helped Ms. Lewinsky find a job.
"He testified that Ms. Monica Lewinsky said that she had conversations with the president, that she talked to the president."
"He has numerous acquaintances, including Susan Collins, Betty Currie, Pete Domenici, Bob Graham, James Jeffords and Linda Tripp. 1,300 docs, 707,000 words (Clinton corpus) 607[REF_CITE]extracted appositives, 2 groups: friend, adviser."
Henry Hyde is a Republican chairman of House Judiciary Committee and a prosecutor in Senate impeachment trial.
He will lead the Judiciary Committee&apos;s impeachment review.
"Hyde urged his colleagues to heed their consciences , “the voice that whispers in our ear , ‘duty, duty, duty.’”"
"Victor Polay is the Tupac Amaru rebels&apos; top leader, founder and the organization&apos;s commander-and-chief."
He was arrested again in 1992 and is serving a life sentence.
"His associates include Alberto Fujimori, Tupac Amaru Revolutionary, and Nestor Cerpa. 73 docs, 38,000 words, 24[REF_CITE]extracted appositives, 3 groups: leader, founder and commander-in-chief."
"Each document in the collection to be summarized is processed by a sentence tokenizer, the Alembic part-of-speech tagger[REF_CITE], the Nametag named entity tagger[REF_CITE]restricted to people names, and the CASS parser[REF_CITE]."
"The tagged sentences are further analyzed by a cascade of finite state machines leveraging patterns with lexical and syntactic information, to identify constructions such as pre- and post-modifying appositive phrases, e.g., “Presidential candidate George Bush”, “Bush, the presidential candidate”, and relative clauses, e.g., “Senator ..., who is running for re-election this Fall,”."
"These appositive phrases and relative clauses capture descriptive information which can correspond variously to a person’s age, occupation, or some role a person played in an incident."
"In addition, we also extract sentential descriptions in the form of sentences whose (deep) subjects are person names."
"The classes of person names identified within each document are then merged across documents in the collection using a cross-document coreference program from the Automatic Content Extraction (ACE) research program[REF_CITE], which compares names across documents based on similarity of a window of words surrounding each name, as well as specific rules having to do with different ways of abbreviating a person’s name[REF_CITE]."
"The end result of this process is that for each distinct person, the set of descriptions found for that person in the collection are grouped together."
The appositive phrases usually provide descriptions of attributes of a person.
"However, the preprocessing component described in Section 2.1 does produce errors in appositive extraction, which are filtered out by syntactic and semantic tests."
"The system also filters out redundant descriptions, both duplicate descriptions as well as similar ones."
These filtering methods are discussed next.
"The appositive descriptions are first pruned to record only one instance of an appositive phrase which has multiple repetitions, and descriptions whose head does not appear to refer to a person."
The latter test relies on a person typing program which uses semantic information from WordNet 1.6[REF_CITE]to test whether the head of the description is a person.
A given string is judged as a person if a threshold percentage θ 1 (set to 35% in our work) of senses of the string are descended from the synset for Person in WordNet.
"For example, this picks out “counsel” as a person, but “accessory” as a non-person."
The pruning of erroneous and duplicate descriptions still leaves a large number of redundant appositive descriptions across documents.
"The system compares each pair of appositive descriptions of a person, merging them based on corpus frequencies of the description head stem, syntactic information, and semantic information based on the relationship between the heads in WordNet."
"The descriptions are merged if they have the same head stem, or if both heads have a common parent below Person in WordNet (in the latter case the head which is more frequent in the corpus is chosen as the merged head), or if one head subsumes the other under Person in WordNet (in which case the more general head is chosen)."
"When the heads of descriptions are merged, the most frequent modifying phrase that appears in the corpus with the selected head is used."
"When a person ends up with more than one description, the modifiers are checked for duplication, with distinct modifiers being conjoined together, so that “Wisconsin lawmaker” and “Wisconsin democrat” yields “Wisconsin lawmaker and Democrat”."
"Prepositional phrase variants of descriptions are also merged here, so that “chairman of the Budget Committee” and “Budget Committee Chairman” are merged."
Modifiers are dropped but their original order is preserved for the sake of fluency.
The system then weights the appositives for inclusion in a summary.
"A person’s appositives are grouped into equivalence classes, with a single head noun being chosen for each equivalence class, with a weight for that class based on the corpus frequency of the head noun."
The system then picks descriptions in decreasing order of class weight until either the compression rate is achieved or the head noun is no longer in the top θ 2 % most frequent descriptions ( θ 2 is set to 90% in our work).
"Note that the summarizer refrains from choosing a subsuming term from WordNet that is not present in the descriptions, preferring to not risk inventing new descriptions, instead confining itself to cutting and pasting of actual words used in the document."
"Once the relative clauses have been pruned for duplicates, the system weights the appositive clauses for inclusion in a summary."
"The weighting is based on how often the relative clause’s main verb is strongly associated with a (deep) subject in a large corpus, compared to its total number of appearances in the corpus."
The idea here is to weed out ‘promiscuous’ verbs that are weakly associated with lots of subjects.
The corpus statistics are derived from the Reuters portion of the North American News Text Corpus (called ‘Reuters’ in this paper) --nearly three years of wire service news reports containing 105.5 million words.
"Examples of verbs in the Reuters corpus which show up as promiscuous include “get”, “like”, “give”, “intend”, “add”, “want”, “be”, “do”, “hope”, “think”, “make”, “dream”, “have”, “say”, “see”, “tell”, “try”."
"In a test, detailed below in Section 4.2, this feature fired 40 times in 184 trials."
"To compute strong associations, we proceed as follows."
"First, all subject-verb pairs are extracted from the Reuters corpus with a specially developed finite state grammar and the CASS parser."
The head nouns and main verbs are reduced to their base forms by changing plural endings and tense markers for the verbs.
"Also included are ‘gapped’ subjects, such as the subject of “run” in “the student promised to run the experiment”; in this example, both pairs ‘student-promise’ and ‘student-run’ are recorded."
Passive constructions are also recognized and the object of the by-PP following the verb is taken as the deep subject.
Strength of association between subject i and verb j is measured using mutual informati[REF_CITE]:
"MI(i, j)=ln( N ⋅ tf ij ) . tf i ⋅ tf j"
"Here tf ij is the maximum frequency of subject-verb pair ij in the Reuters corpus, tf i is the frequency of subject head noun i in the corpus, tf j is the frequency of verb j in the corpus, and N is the number of terms in the corpus."
"The associations are only scored for tf counts greater than 4, and a threshold θ 3 (set to log score &gt; -21 in our work) is used for a strong association."
The relative clauses are thus filtered initially (Filter 1) by excluding those whose main verbs are highly promiscuous.
"Next, they are filtered (Filter 2) based on various syntactic features, as well as the number of proper names and pronouns."
"Finally, the relative clauses are scored conventionally (Filter 3) by summing the within-document relative term frequency of content terms in the clause (i.e., relative to the number of terms in the document), with an adjustment for sentence length (achieved by dividing by the total number of content terms in the clause)."
These descriptions are the relatively large set of sentences which have a person name as a (deep) subject.
We filter them based on whether their main verb is strongly associated with either of the head nouns of the appositive descriptions found for that person name (Filter 4).
The intuition here is that particular occupational roles will be strongly associated with particular verbs.
"For example, politicians vote and elect, executives resign and appoint, police arrest and shoot; so, a summary of information about a policeman may include an arresting and shooting event he was involved with. (The verb-occupation association isn’t manifest in relative clauses because the latter are too few in number)."
A portion of the results of doing this is shown in Table 1.
"The results for “executive” are somewhat loose, whereas for “politician” and “police”, the associations seem tighter, with the associated verbs meeting our intuitions."
"All sentences which survive Filter 4 are extracted and then scored, just as relative clauses are, using Filter 1 and Filter 3."
"Filter 4 alone provides a high degree of compression; for example, it reduces a total of 16,000 words in the combined sentences that include Vernon Jordan&apos; s name in the Clinton corpus to 578 words in 12 sentences; sentences up to the target length can be selected from these based on scores from Filter 1 and then Filter 3."
"However, there are several difficulties with these sentences."
"First, we are missing a lot of them due to the fact that we do not as yet handle pronominal subjects which are coreferential with the proper name."
"Second, these sentences contain lots of dangling anaphors, which will need to be resolved."
"Third, there may be redundancy between the sentential descriptions, on one hand, and the appositive and relative clause descriptions, on the other."
"Finally, the entire sentence is extracted, including any subordinate clauses, although we are working on refinements involving sentence compaction."
"As a result, we believe that more work is required before the sentential descriptions can be fully integrated into the biographies. particular classes of people in the Reuters corpus (negative log scores)."
Methods for evaluating text summarization can be broadly classified into two categories[REF_CITE].
"The first, an extrinsic evaluation, tests the summarization based on how it affects the completion of some other task, such as comprehension, e.g.,[REF_CITE], or relevance assessment[REF_CITE]."
"An intrinsic evaluation, on the other hand, can involve assessing the coherence of the summary[REF_CITE]."
"Another intrinsic approach involves assessing the informativeness of the summary, based on to what extent key information from the source is preserved in the system summary at different levels of compressi[REF_CITE],[REF_CITE]."
"Informativeness can also be assessed in terms of how much information in an ideal (or ‘reference’) summary is preserved in the system summary, where the summaries being compared are at similar levels of compressi[REF_CITE]."
"We have carried out a number of intrinsic evaluations of the accuracy of components involved in the summarization process, as well as the succinctness, coherence and informativeness of the descriptions."
"As this is a MDS system, we also evaluate the non-redundancy of the descriptions, since similar information may be repeated across documents."
"The component evaluation tests how accurately the tagger can identify whether a head noun in a description is appropriate as a person description The evaluation uses the WordNet 1.6 SEMCOR semantic concordance, which has files from the Brown corpus whose words have semantic tags (created by WordNet&apos; s creators) indicating WordNet sense numbers."
"Evaluation on 6,000 sentences with almost 42,000 nouns compares people tags generated by the program with SEMCOR tags, and provided the following results: right = 41,555, wrong = 1,298, missing = 0, yielding Precision, Recall, and F-Measure of 0.97."
This component evaluation tests the well-formedness of the extracted relative clauses.
"For this evaluation, we used the Clinton corpus."
"The relative clause is judged correct if it has the right extent, and the correct coreference index indicating which person the relative clause description pertains to."
The judgments are based on 36 instances of relative clauses from 22 documents.
"The results show 28 correct relative clauses found, plus 4 spurious finds, yielding Precision of 0.87, Recall of 0.78, and F-measure of .82."
"Although the sample is small, the results are very promising."
This component evaluation tests the system’s ability to accurately merge appositive descriptions.
The score is based on an automatic comparison of the system’s merge of system-generated appositive descriptions against a human merge of them.
We took all the names that were identified in the Clinton corpus and ran the system on each document in the corpus.
"We took the raw descriptions that the system produced before merging, and wrote a brief description by hand for each person who had two or more raw descriptions."
The hand-written descriptions were not done with any reference to the automatically merged descriptions nor with any reference to the underlying source material.
"The hand-written descriptions were then compared with the final output of the system (i.e., the result after merging)."
"The comparison was automatic, measuring similarity among vectors of content words (i.e., stop words such as articles and prepositions were removed)."
Here is an example to further clarify the strict standard of the automatic evaluation (words scored correct are underlined):
"System: E. Lawrence Barcella is a Washington lawyer, Washington white-collar defense lawyer, former federal prosecutor System Merge: Washington white-collar defense lawyer Human Merge: a Washington lawyer and former federal prosecutor Automatic Score: Correct=2; Extra-Words=2; Missed-Words=3"
"Thus, although ‘lawyer’ and ‘prosecutor’ are synonymous in WordNet, the automatic scorer doesn’t know that, and so ‘prosecutor’ is penalized as an extra word."
"The evaluation was carried out over the entire Clinton corpus, with descriptions compared for 226 people who had more than one description. 65 out of the 226 descriptions were Correct (28%), with a further 32 cases being semantically correct ‘obviously similar’ substitutions which the automatic scorer missed (giving an adjusted accuracy of 42%)."
"As a baseline, a merging program which performed just a string match scored 21% accuracy."
"The major problem areas were errors in coreference (e.g., Clinton family members being put in the same coreference class), lack of good descriptions for famous people (news articles tend not to introduce such people), and parsing limitations (e.g., “Senator Clinton” being parsed erroneously as an NP in “The Senator Clinton disappointed…”)."
"Ultimately, of course, domain-independent systems like ours are limited semantically in merging by the lack of world knowledge, e.g., knowing that Starr&apos; s chief lieutenant can be a prosecutor."
"To assess the coherence and informativeness of the relative clause descriptions 3 , we asked 4 subjects who were unaware of our research to judge descriptions generated by our system from the Clinton corpus."
"For each relative clause description, the subject was given the description, a person name to whom that description pertained, and a capsule description consisting of merged appositives created by the system."
"The subject was asked to assess (a) the coherence of the relative clause description in terms of its succinctness (was it a good length?) and its comprehensibility (was it and understandable by itself or in conjunction with the capsule?), and (b) its informativeness in terms of whether it was an accurate description (does it conflict with the capsule or with what you know?) and whether it was non-redundant (is it distinct or does it repeat what is in the capsule?)."
"The subjects marked 87% of the descriptions as accurate, 96% as non-redundant, and 65% as coherent."
"A separate [Footnote_3]-subject inter- annotator agreement study, where all subjects judged the same 46 decisions, showed that all three subjects agreed on 82% of the accuracy decisions, 85% of the non-redundancy decisions and 82% of the coherence decisions."
3 Appositives are not assessed in this way as few errors of coherence or informativeness were noticed in the appositive extraction.
"To learn rules for coherence for extracting sentential descriptions, we used the examples and judgments we obtained for coherence in the evaluation of relative clause descriptions in Section 4.5."
"Our focus was on features that might relate to content and specificity: low verb promiscuity scores, presence of proper names, pronouns, definite and indefinite clauses."
The entire list is as follows:
Table 2 provides information on different learning methods.
"The results are for a ten-fold cross-validation on 165 training vectors and 19 test vectors, measured in terms of Predictive Accuracy (percentage test vectors correctly classified)."
The best learning methods are comparable with rules created by hand by one of the authors (Barry’s rules).
"In the learners, the bestverb feature is used heavily in tests for the negative class, whereas in Barry’s Rules it occurs in tests for the positive class."
"Our work on measuring subject-verb associations has a different focus from the previous work.[REF_CITE], for example, examined verb-object pairs."
Their focus was on a method that would improve techniques for gathering statistics where there are a multitude of sparse examples.
"We are focusing on the use of the verbs for the specific purpose of finding associations that we have previously observed to be strong, with a view towards selecting a clause or sentence, rather than just to measure similarity."
We also try to strengthen the numbers by dealing with ‘gapped’ constructions.
"While there has been plenty of work on extracting named entities and relations between them, e.g.,[REF_CITE], the main previous body of work on biographical summarization is that[REF_CITE]."
"The fundamental differences in our work are as follows: (1) We extract not only appositive phrases, but also clauses at large based on corpus statistics; (2) We make heavy use of coreference, whereas they don’t use coreference at all; (3) We focus on generating succinct descriptions by removing redundancy and merging, whereas they categorize descriptions using WordNet, without a focus on succinctness."
This research has described and evaluated techniques for producing a novel kind of summary called biographical summaries.
"The techniques use syntactic analysis and semantic type-checking (from WordNet), in combination with a variety of corpus statistics."
"Future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole (i.e., including canned text)."
"In a headed tree, each terminal word can be uniquely labeled with a gov-erning word and grammatical relation."
"This labeling is a summary of a syn-tactic analysis which eliminates detail, reflects aspects of semantics, and for some grammatical relations (such as subject of finite verb) is nearly un-controversial."
"We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights."
"The quantity is computed in a parse for-est representation of the set of tree anal-yses for a given sentence, using vector sums and scaling by inside probability and flow."
"A labeled headed tree is one in which each non-terminal vertex has a distinguished head child, and in the usual way non-terminal nodes are la-beled with non-terminal symbols (syntactic cat-egories such as NP) and terminal vertices are labeled with terminal symbols (words such as reads). 1"
We work with syntactic trees in which terminals are in addition labeled with uninflected word forms (lemmas) derived from the lexicon.
"By percolating lemmas up the chains of heads, each node in a headed tree may be labeled with a lexical head."
"Figure 1 is an example, where lex-ical heads  are written as subscripts."
"We use the notation  for the lexical head of a vertex , and for the ordinary category or word label of ."
The governor label for a terminal vertex in such a labeled tree is a triple which represents the syntactic and lexical environment at the top of the chain of vertices headed by .
"Where is the maximal vertex of which is a head vertex, and is the parent of , the governor label for is the tuple &amp;[Footnote_1]   2 . 2 Governor labels for the example tree are given in Figure 2."
"1 Headed trees may be constructed as tree domains, which are sets of addresses of vertices. 0 is used as the relative ad-dress of the head vertex, negative integers are used as relative addresses of child vertices before the head, and positive in-tegers are used as relative addresses of child vertices after the head. A headed tree domain is a set of finite sequences of ,+. integers -0/ ( ! orsuch ( ./ that -0+. (i * ),ifthen $&quot; #$&amp;%&quot;! , - then %&amp;! . &apos;&quot; %&amp;! ; (ii) if &quot;$)( &amp;% ! and *"
"As observed[REF_CITE], grammatical relations such as subject and object may be re-constructed as ordered pairs of category labels, such as 1 NP , S [Footnote_2] for subject."
"2 In *A a @ headed tree domain, &gt; is a head of ? if &gt; is of the form ? for C( EB D ."
"So, a governor label encodes a grammatical relation and a governing lexical head."
"Given a unique tree structure for a sentence, governor markup may be read off the tree."
"How-ever, in view of the fact that robust broad coverage parsers frequently deliver thousands, millions, or thousands of millions of analyses for sentences of free text, basing annotation on a unique tree (such as the most probable tree analysis generated by a probabilistic grammar) appears arbitrary."
Note that different trees may produce the same governor labels for a given terminal position.
"Suppose for instance that the yield of the tree in Figure 1 has a different tree analysis in which the PP is a child of the VP , rather than NP ."
"In this case, just as in the original tree, the label for the fourth terminal position (with word label paper) is 1 NP , VP ,read 2 ."
"Supposing that there are only two tree analyses, this label can be assigned to the fourth word with certainty, in the face of syntac-tic ambiguity."
The algorithm we will define pools governor labels in this way.
Suppose that a probabilistic 398::8 :8 3 grammar licenses headed tree analyses 576
"5 for a sentence 398:8:8:3 ; , and assigns them probabilistic weights =&lt; 6 &lt; ."
We define a scheme which divides a count of 1 among the different governor labels.
"For a given governor tuple L , let def PRQ  &lt;4X 8 O L (1) Q  &lt; X"
"The definition sums the probabilistic weights of trees with markup L , and normalizes by the sum of the probabilities of all tree analyses of ; ."
The definition may be justified P] as \_ follows ^`a\ `b^ .
"We , work with \ a markup space [ where is the set of category labels and b is the set of lemma labels."
"For a given markup triple L , let c  [ fhg i be the function P which maps L to 1, and  to 0 for  L ."
"We define a random variate n d Trees fpo [ fhg irq c which maps a tree 5 to , where L is the gov-ernor markup for word position N which is de- termined by tree 5 ."
The random variate n is de-fined on labeled trees licensed by the probabilistic grammar.
"Note that o [ fRg irq is a vector space (with pointwise sums and scalar products), so that expectations and conditional O expectations may be defined."
"In these terms, is the conditional ex-pectation of n , conditioned on the yield being ; ."
"This definition, instead of a single governor la-bel for a given word position, gives us O a set of pairs of a markup L and a real number in L [0,1], such that the real numbers in the pairs sum to 1."
"In our implementation (which is based[REF_CITE]), we use O a cutoff of 0.1, and print only indices L where L is above the cutoff."
Figure 3 is an example.
"A direct implementation of the above definition O using an iteration over trees to compute would be unusable because in the robust grammar of En-glish we work with, the number of tree analyses for a sentence is frequently large, greater than  for about 1/10 of the sentences in the O British Na-tional Corpus."
We instead calculate in a parse forest representation of a set of tree analyses.
"A parse forest (see also[REF_CITE]) in labeled P grammar  3 notation { [Footnote_3]}| is a tu-ple v &amp;1  ~2 where &amp;1 3  { ~2 is a context free gram-mar y (consisting of non-terminals  , terminals { , rules i , and a start symbol ) and | is a function which maps elements of  to  P non-terminals  { in an underlying y grammar to termi- 1 w  &amp; i 2 and | elements of nals in ."
"3 We use multisets rather than set images to achieve cor-rectness of the inside algorithm in cases where ¤ represents some tree more than once, something which is possible given the definition of labeled grammars. A correct parser pro-duces a parse forest which represents every parse for the in-put sentence exactly once."
"By using on symbols on the left hand | and right hand sides of a parse forest rule, can be extended to map the set of parse forest rules |  to the set of underlying grammar rules i . is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar."
An example is given in figure 4.
"Where W} y  , 3 let {  be the set of trees licensed by &amp;1    which have root symbol  in the case of a symbol, and the set of trees which have  as the rule expanding the root in the case or a rule. g  is defined | to be the multiset image of g  under . g  is the multiset of inside trees represented by parse resenting two tree analyses of John reads every paper on markup. | The labeling P function drops subscripts, so that VP 6 VP. \ forest symbol { or rule  . 3 Let  be the set of trees in  \ which contain  as a symbol or use  as a \ rule.  is | defined \ to be the multiset image of  under  is the multiset. of complete trees represented by the parse forest symbol or rule  ."
"Where &lt; is a probability function on trees li-censed by the underlying grammar and  is a sym-bol or rule in v ,   def P  &lt; 5 (2)  7&amp;   def P Q  &lt; 5 8 = (3) Q &amp; :   &lt; 5   is called the inside probability for  and =  is called the flow for  . [Footnote_4]"
"4 These quantities can be given probabilistic interpreta-tions and/or definitions, for instance with reference to con-ditionally expected rule frequencies for flow."
"Parse forests are often constructed so that all inside trees represented by a parse forest nonter-minal  have the same span, as well as the same parent category."
"To deal with headedness and lexicalization of a probabilistic grammar, we construct parse forests so that, in addition, all in-side trees represented by a parse forest nontermi-nal have the same lexical head."
We add to the la-beled grammar a function £. which labels parse forest symbols with lexical heads.
"In our imple-mentation, an ordinary context free parse forest is first constructed by tabular parsing, and then in a second pass parse forest symbols are split accord-ing to headedness."
Such an algorithm is shown in appendix B.
This procedure gives worst case time and space complexity which is proportional to the fifth power of the length of the sentence.
"In practical ex-perience with broad-coverage context free gram-mars of several languages, we have not observed super-cubic average time or space requirements for our implementation."
"We believe this is be-cause, for our grammars and corpora, there is lim-ited ambiguity in the position of the head within a given category-span combination."
The governor algorithm stated in the next sec-tion refers to headedness in parse forest rules.
"This can be represented by constructing parse for-est rules (as well as ordinary grammar rules) with headed tree domains of depth one. 5 Where is a parse forest symbol on the right hand side of a parse forest rule n , we will simply state the con-dition “ is the head of n ”."
"The flow and governor algorithms  stated be-low call an algorithm PF-I NSIDE v which computes inside probabilities in v , where ¥ is a function giving probability parameters for the un-derlying grammar."
Any probability weighting of trees may be used which allows inside probabil-ities to be computed in parse forests.
The inside algorithm for ordinary ¥ PCFGs is given in figure 5.
"The parameter maps the set of underlying | grammar ,¼ rules i which is the image of on | to reals, with the | interpretation of rule proba- bilities."
"In step 5, | maps n the parse forest rule n to ¥ a grammar rule which is the argument of ."
"The functions lhs and rhs map rules to their left hand and right hand sides, respectively  ."
"Given an inside algorithm, the flow may be computed by the flow algorithm in Figure 6, or by the inside-outside algorithm."
The governor algorithm annotates parse forest symbols and rules with functions from governor labels to real numbers.
"Let 5 be a tree in the parse forest grammar, let be a symbol in 5 , let be the maximal symbol in 5 of which is a head, or itself if is a non-head child of its parent in 5 , and let  be the parent of in 5 ."
Recall that  ½  &amp;¾  : (4) is | a } vector |  mapping the markup triple 1  2 to 1 and other markups to 0. | We have }| constructed  parse forests such that 1  £ 2 agrees with the governor label for the | lexical head of the node corresponding to in 5 .
"A parse forest tree 5 and symbol in 5 thus de-termine the vector (4), where and are defined c as above  ."
"Call the vector determined in this way is a parse forest rule in v , let &amp;| c 3  Â  def P Q &amp;  &lt; 5 5 Q &apos;«ÀÃZ &lt; &amp;| (5) 5 { }3 | 2 is a parse forest representing each tree O analysis for a sentence exactly once, the quantity for termi-nal position N (as defined in section 1) is found by summing Â  for terminal symbols in y which have string position N . 6"
The algorithm PF - GOVERNORS is stated in Fig-ure 3.
"Working top down, if fills in an array Â oÆq which is supposed to agree with the quan-tity Â Æ defined above."
"Scaled governor vectors are created for non-head children in step 10, and summed down the chain of heads in step 9."
"In step [Footnote_6], vectors are divided in proportion to inside probabilities (just as in the flow algorithm), be-cause the set of complete trees for the left hand side of n are partitioned among the parse forest rules which expand the left hand side of n ."
"6 This procedure requires that symbols in Ç  correspond to a unique string position, something which is not enforced by our definition of parse forests. Indeed, such cases may arise if parse forest symbols are constructed as pairs of gram-mar symbols and strings[REF_CITE]rather than pairs of grammar symbols and spans. Our parser constructs parse forests organized according to span."
"Consider a parse forest rule n , and a parse for-est symbol on its right hand side which is not the head of n ."
"In each tree in \ n , is the top of a chain of heads, because is a non-head child in rule n ."
The scalar multi-plier o n q is
Q &amp;&apos;©} &lt; 5 3 Q &amp;&apos;©ÀÃZ &lt; 5 the relative weight of trees in \ n .
This is ap-propriate because Â as defined in equation (5) is to be scaled by the relative weight of trees in \.
"In line 9 of the algorithm, Â is summed into the head child \ n ."
"There is no \ scaling, because every tree in is a tree in. ¥"
A probability parameter vector is used in the inside algorithm.
"In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which condi-tions rules on parent category and parent lexical head, and conditions the heads of non-head chil-dren on child category, parent category, and par-ent head[REF_CITE]."
The requisite information \ is di-rectly represented in our parse forests by and  .
"Thus the call to PF- INSIDE in line 1 of PF-G OVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities."
"However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the ref-erence to £ in line 10."
Construction of head-marked parse forests is presented in the appendix.
The LoPar parser[REF_CITE]on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure.
Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses.
The worst-case space complexity of this representation is cubic (cmp.
"LoPar already provided functions for the com-putation of the head-marked parse forest, for the flow computation and for traversing the parse for-est in depth-first and topologically-sorted order (see[REF_CITE])."
"So it was only neces-sary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result."
The governor labels defined above are derived from the specific symbols of a context free gram-mar.
"In contrast, according to the general markup methodology of current computational linguis-tics, labels should not be tied to a specific gram-mar and formalism."
"The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using objective tests."
"As grammatical relation symbols, they use atomic labels such as dobj (direct object) an ncsubj (non-clausal subject)."
"The labels are arranged in a hier-archy, with for instance subj having subtypes nc-subj, xsubj, and csubj."
There is another problem with the labels we have used so far.
"Our grammar codes a variety of features, such as the feature VFORM on verb projections."
"As a result, instead of a single object grammatical relation 1 NP , VP 2 , we have grammati-cal relations 1 NP , VP . N 2 , 1 NP , VP ."
"FIN 2 , 1 NP , VP ."
"TO 2 , 1 NP , VP ."
"BASE 2 , and so forth."
This may result in frequency mass being split among different but similar labels.
"For instance, a verb phrase will have read every paper might have some analy-ses in which read is the head of a base form VP and paper is the head of the object of read, and others where read is a head of a finite form VP, and paper is the head of the object of read."
"In this case, frequencies would be split between 1 NP , VP ."
"BASE ,read 2 and 1 NP , VP ."
"FIN ,read 2 as gov-ernor labels for paper."
"To address  these problems, we employ a pool-ing function i which maps pairs of categories to symbols such  as  ncsubj or obj."
The gover-  2 is then replaced by nor  tuple 1&amp; 1  2 in the definition of the i governor label for a terminal vertex .
More flexibility could be gained by using a rule and the address of a constituent  on the right hand side as arguments of i .
This would allow the following assignments.  3 P  i VP .
FIN f VC .
NP NP s P dobj  i VP .
FIN f VC .
NP NP 3 P obj2  i VP .
FIN f VC .
TO 3 s P xcomp i VP .
FIN f VP .
TO s xmod
The head of a rule is marked with a prime.
"In the first pair, the objects in double object construction are distinguished using the address."
"In each case, the child-parent category pair is 1 NP , VP ."
"FIN 2 , so that the original proposal could not distinguish the grammatical relations."
"In the second pair, a VP ."
TO argument is distinguished from a VP .
TO modifier using the category of the head.
"In each case, the child-parent category pair is 1 VP ."
"TO , VP ."
FIN 2 .
"No-tice that[REF_CITE]of PF-G OVERNORS  , the rule n is available, so that the arguments of i could be changed in this way."
"The governor algorithm was designed as a com-ponent of Spot, a free-text question answering system."
"Current systems usually extract a set of candidate answers (e.g. sentences), score them and return the n highest-scoring candidates as possible answers."
The system described[REF_CITE]scores possible answers based on the overlap in the semantic represen-tations of the question and the answer candi-dates.
Their semantic representation is basically identical to the head-head relations computed by the governor algorithm.
"However, Harabagiu et al. extract this information only from maxi-mal probability parses whereas the governor al-gorithm considers all analyses of a sentence and returns all possible relations weighted with esti-mated frequencies."
"Our application in Spot works as follows: the question is parsed with a spe-cialized question grammar, and features including the governor of the trace are extracted from the question."
"Governors are among the features used for ranking sentences, and answer terms within sentences."
"In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it."
Expected governor markup summarizes syn-tactic structure in a weighted parse forest which is the product of exhaustive parsing and inside-outside computation.
This is a strategy of dumbing down the product of computation-ally intensive statistical parsing into unstructured markup.
Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im[REF_CITE]).
Governor markup differs in that it is reflective of higher-level syntax.
"The strat-egy has the advantage, in our view, that it allows one to base markup algorithms on relatively so-phisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probabil-ity model."
"Localizing markup on the governed word in-creases pooling of frequencies, because the span of the phrase headed by the governed item is ignored."
This idea could be exploited in other markup tasks.
"In a chunking task, categories and heads of chunks could be identified, rather than categories and boundaries."
The inside-outside  algorithm computes inside probabilities o q and outside probabilities Ë&apos;o q .
"We will show = that  these quantities are  related P to the  flow  {  { by the equation o q Ë&apos;o q o  o =q . o  is the inside probability of the root symbol, which is also the sum of the probabilities of all parse trees."
"According[REF_CITE], the outside probabilities in a parse forest are computed by:  n Ë&apos;o lhs n q  o q  Ë&apos;o q P } o q"
The outside probability of the start symbol is 1.
We prove by induction over the depth of the parse forest that the following relationship holds:   o q P Ë o q  {o q o
It is easy to see { that the assumption holds for the root symbol :  {  o {  P s P Ë&apos;o { q  o { q o
The flow in a parse forest is computed by:  o n q  o q P  o lhs n q    }} o lhs n q
"Now, we insert the induction hypothesis:"
Ë&apos;o lhs  {n q  o lhs n q  o n q   o q P  }} o   o lhs n q
"After a few transformations, we get the equation   o q P  {o q Ë&apos;o lhs  n q  on q  o  } o q which is equivalent to   o q P &apos;Ë o q  {o q o according to the definition of Ë&apos;o q ."
"So, the in-duction hypothesis is generally true."
"The function L EXICALIZE below takes an unlex-icalized parse forest as argument and returns a lexicalized parse forests, where each symbol is uniquely labeled with a lexical head."
Symbols are split if they have more than one lexical head.
L EXICALIZE creates new terminal symbols by calling the function N EW T.
The new symbols are linked to the original ones by means of Î oÆ q .
"For each rule in the old parse forest, the set of all possible combinations of the lexicalized daugh-ter symbols is generated."
The function LEM n returns the lemma associated with lexical rule n .
"For each combination of lexicalized daughter symbols, a new rule is inserted by calling A DD ."
A DD calls N EW NT to create new non-terminals and N EW R ULE to generate new rules.
A non-terminal is only created if no symbol with the same lexical head was linked to the original node.
"The standard pipeline approach to se-mantic processing, in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted, is a poor fit for ap-plications such as natural language in-terfaces."
"This is because the environ-ment information, in the form of the ob-jects and events in the application’s run-time environment, cannot be used to in-form parsing decisions unless the in-put sentence is semantically analyzed, but this does not occur until after pars-ing in the single-tree semantic architec-ture."
"This paper describes the compu-tational properties of an alternative ar-chitecture, in which semantic analysis is performed on all possible interpre-tations during parsing, in polynomial time."
"Shallow semantic processing applications, com-paring argument structures to search patterns or filling in simple templates, can achieve re-spectable results using the standard ‘pipeline’ ap-proach to semantics, in which sentences are mor-phologically and syntactically resolved to a single tree before being interpreted."
"Putting disambigua-tion ahead of semantic evaluation is reasonable in these applications because they are primarily run on content like newspaper text or dictated speech, where no machine-readable contextual informa-tion is readily available to provide semantic guid-ance for disambiguation."
"This single-tree semantic architecture is a poor fit for applications such as natural language inter-faces however, in which a large amount of contex-tual information is available in the form of the ob- jects and events in the application’s run-time en-vironment."
"This is because the environment infor-mation cannot be used to inform parsing and dis-ambiguation decisions unless the input sentence is semantically analyzed, but this does not occur until after parsing in the single-tree architecture."
"Assuming that no current statistical disambigua-tion technique is so accurate that it could not ben-efit from this kind of environment-based informa-tion (if available), then it is important that the se-mantic analysis in an interface architecture be ef-ficiently performed during parsing."
"This paper describes the computational prop-erties of one such architecture, embedded within a system for giving various kinds of conditional instructions and behavioral constraints to virtual human agents in a 3-D simulated environment[REF_CITE]."
"In one application of this system, users direct simulated maintenance personnel to repair a jet engine, in order to ensure that the maintenance procedures do not risk the safety of the people performing them."
"Since it is expected to process a broad range of maintenance instructions, the parser is run on a large subset of the Xtag English grammar (XTAG[REF_CITE]), which has been annotated with lex-ical semantic classes[REF_CITE]associ-ated with the objects, states, and processes in the maintenance simulation."
"Since the grammar has several thousand lexical entries, the parser is ex-posed to considerable lexical and structural ambi-guity as a matter of course."
"The environment-based disambiguation archi-tecture described in this paper has much in common with very early environment-based ap-proaches, such as those described by Winograd[REF_CITE], in that it uses the actual en-tities in an environment database to resolve am-biguity in the input."
This research explores two extensions to the basic approach however: 1.
It incorporates ideas from type theory to rep- resent a broad range of linguistic phenomena in a manner for which their extensions or po-tential referents in the environment are well-defined in every case.
This is elaborated in Section 2. 2.
"It adapts the concept of structure sharing, taken from the study of parsing, not only to translate the many possible interpretations of ambiguous sentences into shared logical ex-pressions, but also to evaluate these sets of potential referents, over all possible interpre-tations, in polynomial time."
This is elabo-rated in Section 3.
"Taken together, these extensions allow interfaced systems to evaluate a broad range of natural lan-guage inputs – including those containing NP/VP attachment ambiguity and verb sense ambiguity – in a principled way, simply based on the ob-jects and events in the systems’ environments."
"For example, such a system would be able to cor-rectly answer ‘Did someone stop the test at 3:00?’ and resolve the ambiguity in the attachment of ‘at 3:00’ just from the fact that there aren’t any 3:00 tests in the environment, only an event where one stops at 3:00. [Footnote_1]"
"1 It is important to make a distinction between this envi-ronment information, which just describes the set of objects and events that exist in the interfaced application, and what is often called domain information, which describes (usually via hand-written rules) the kinds of objects and events can exist in the interfaced application. The former comes for free with the application, while the latter can be very expensive to create and port between domains."
"Because it evaluates instructions before attempting to choose a single interpreta-tion, the interpreter can avoid getting ‘stranded’ by disambiguation errors in earlier phases of anal-ysis."
"The main challenge of this approach is that it requires the efficient calculation of the set of ob-jects, states, or processes in the environment that each possible sub-derivation of an input sentence could refer to."
"A semantic interpreter could al-ways be run on an (exponential) enumerated set of possible parse trees as a post-process, to fil-ter out those interpretations which have no en-vironment referents, but recomputing the poten-tial environment referents for every tree would re-quire an enormous amount of time (particularly for broad coverage grammars such as the one em-ployed here)."
"The primary result of this paper is therefore a method of containing the time com-plexity of these calculations to  lie within the com-plexity of parsing (i.e. within for a context-free grammar, where is the number of words in the input sentence), without sacrificing logi-cal correctness, in order to make environment-based interpretation tractable for interactive appli-cations."
"Existing environment-based methods (such as those proposed by Winograd) only calculate the referents of noun phrases, so they only consult the objects in an environment database when inter-preting input sentences."
But the evaluation of am-biguous sentences will be incomplete if the refer-ents for verb phrases and other predicates are not calculated.
"In order to evaluate the possible inter-pretations of a sentence, as described in the previ-ous section, an interface needs to define referent sets for every possible constituent. [Footnote_2]"
"2 This is not strictly true, as referent sets for constituents like determiners are difficult to define, and others (particu-larly those of quantifiers) will be extremely large until com-posed with modifiers and arguments. Fortunately, as long as there is a bound on the height in the tree to which the evaluation of referent sets can be deferred (e.g. after the first composition), the claimed polynomial complexity of refer-ent annotation will not be lost."
"The proposed solution draws on a theory of constituent types from formal linguistic seman-tics, in which constituents such as nouns and verb phrases are represented as composeable functions that take entitiess or situations as inputs and ulti-mately return a truth value for the sentence."
"Fol-lowing a straightforward adaptation of standard type theory, common nouns (functions from en-tities to truth values) define potential  referent   sets, of simple environment entities: and sentences (functions from situations or world states to truth values) define potential referent sets of  situations   in which."
"Dependingthose sentenceson the needsholdoftruethe: application, these situations can be represented as intervals along a time line[REF_CITE], or as regions in a three-dimensional space[REF_CITE], or as some com-bination of the two, so that they can be con-strained by modifiers that specify the situations’ times and locations."
Referents for other types of phrases may be expressed as tuples of enti-ties and situations: one for each argument of the corresponding logical function’s input (with the presence or absence of the tuple representing the boolean output).
"For example, adjectives, prepo-sitional phrases, and relative clauses, which are typically represented as situationally-dependent properties (functions from situations and entities to truth values) define potential referent sets of tu-ples  that   consist     of one  entity  and."
"Thisone represen-situation: tation can be extended to treat common nouns as situationally-dependent properties as well, in order to handle sets like ‘bachelors’ that change their membership over time."
Any method for using the environment to guide the interpretation of natural language sentences requires a tractable representation of the many possible interpretations of each input.
The representation described here is based on the polynomial-sized chart produced by any dynamic programming recognition algorithm.
"A record of the derivation paths in any dy-namic programming recognition algorithm (such as CKY[REF_CITE]or Earley[REF_CITE]) can be interpreted as a polynomial sized and-or graph with space complexity equal to the time complexity of recognition, whose disjunc-tive nodes represent possible constituents in the analysis, and whose conjunctive nodes represent binary applications of rules in the grammar."
"This is called a shared forest of parse trees, because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses[REF_CITE], and can be con-plexity (e.g.structed and traversed   in time of the same com-for context free grammars)."
"For example, the two parse trees for the noun phrase ‘button on handle beside adapter’ shown in Figure 1 can be merged into the single shared forest in Figure 2 without any loss of information."
"These shared syntactic structures can further be associated with compositional semantic func-tions that correspond to the syntactic elements in the forest, to create a shared forest of trees each representing a complete expression in some logical form."
"This extended sharing is similar to the ‘packing’ approach employed in the Core Language Engine[REF_CITE], except that the CLE relies on a quasi-logical form to under-specify semantic information such as quantifier scope (the calculation of which is deferred un-til syntactic ambiguities have been at least par-tially resolved by other means); whereas the ap-proach described here extends structure sharing to incorporate a certain amount of quantifier scope ambiguity in order to allow a complete eval-uation of all subderivations in a shared forest before making any disambiguation decisions in syntax. [Footnote_3] Various synchronous formalisms have been introduced for associating syntactic repre-sentations with logical functions in isomorphic or locally non-isomorphic derivations, includ-ing Categorial Grammars (CGs)[REF_CITE], Synchronous Tree Adjoining Grammars (TAGs)[REF_CITE], and Synchronous Description Tree Gram-mars (DTGs)[REF_CITE]."
3 A similar basis on (at least partially) disambiguated syn-tactic representations makes similar underspecified semantic representations such as hole semantics[REF_CITE]ill-suited for environment-based syntactic disambiguation.
"Most of these formalisms can be ex-tended to define semantic associations over entire shared forests, rather than merely over individual parse trees, in a straightforward manner, preserv-ing the ambiguity of the syntactic forest without exceeding its polynomial size, or the polynomial time complexity of creating or traversing it."
"Since one of the goals of this architecture is to use the system’s representation of its environ-ment to resolve ambiguity in its instructions, a space-efficient shared forest of logical functions will not be enough."
The system must also be able to efficiently calculate the sets of potential refer-ents in the environment for every subexpression in this forest.
"Fortunately, since the logical function forest shares structure between alternative anal-yses, many of the sets of potential referents can be shared between analyses during evaluation as well."
"This has the effect of building a third shared forest of potential referent sets (another and-or graph, isomorphic to the logical function forest and with the same polynomial complexity), where every conjunctive node represents the results of applying a logical function to the elements in that node’s child sets, and every disjunctive node rep-resents the union of all the potential referents in that node’s child sets."
"The presence or absence of these environment referents at various nodes in the shared forest can be used to choose a vi-able parse tree from the forest, or to evaluate the truth or falsity of the input sentence without dis-ambiguating it (by checking the presence or lack of referents at the root of the forest)."
"For example, the noun phrase ‘button on han-dle beside adapter’ has at least two possible in-terpretations, represented by the two trees in Fig-ure 1: one in which a button is on a handle and the handle (but not necessarily the button) is be-side an adapter, and the other in which a button is on a handle and the button (but not necessarily the handle) is beside an adapter."
"The semantic func-tions are annotated just below the syntactic cat-egories, and the potential environment referents are annotated just below the semantic functions in the figure."
"Because there are no handles next to adapters in the environment (only buttons next to adapters), the first interpretation has no envi-ronment referents at its root, so this analysis is dispreferred if it occurs within the analysis of a larger sentence."
"The second interpretation does have potential environment referents all the way up to the root (there is a button on a handle which is also beside an adapter), so this analysis is pre-ferred if it occurs within the analysis of a larger sentence."
"The shared forest representation effectively merges the enumerated set of parse trees into a single data structure, and unions the referent sets of the nodes in these trees that have the same la-bel and cover the same span in the string yield (such as the root node, leaves, and the PP cover-ing ‘beside adapter’ in the examples above)."
"The referent-annotated forest for this sentence there-fore looks like the forest in Figure 2, in which the sets of buttons, handles, and adapters, as well as the set of things beside adapters, are shared be-tween the two alternative interpretations."
"If there is a button next to an adapter, but no handle next to an adapter, the tree representing ‘handle beside adapter’ as a constituent may be dispreferred in disambiguation, but the NP constituent at the root is still preferred because it has potential referents in the environment due to the other interpretation."
The logical function at each node is defined over the referent sets of that node’s immediate children.
Nodes that represent the attachment of a modifier with referent set to a relation with referent set produce referent sets of the form:  
Nodes in a logical function forest that represent the attachment of an argument with referent set to a relation with referent set produce referent sets of the form:   effectively stripping off one of the objects in each tuple if the object is also found in the set of refer-ents for the argument. [Footnote_4]
"4 In order to show where the referents came from, the tu-ple objects are not stripped off in Figures 1 and 2. Instead, an additional bar is added to the function name to designate the effective last object in each tuple: the tuple ref-erenced by !&quot;# has as the last element, but the tuple referenced by !&quot;# # actually has as the last element since the complement $ has been already been attached."
"This is a direct application of standard type theory to the calculation of ref- erent sets: modifiers take and return functions of the same type, and arguments must satisfy one of the input types of an applied function."
"Since both of these ‘referent set composition’ operations at the conjunctive nodes – as well as the union operation at the disjunctive nodes – are linear in space and time on the number of ele-ments in each of the composed sets (assuming the sets are sorted in advance and remain so), the cal-culation of referent sets only adds a factor of to the size complexity of the forest and the time complexity of processing it, where is the num-ber of objects and events in the run-time environ-ment."
"Thus, the total space and time complexity of  the above  algorithm (on a context-free forest) is."
"If other operations are added, the com-plexity of referent set composition will be limited by the least efficient operation."
"Since the referent sets for situations are also well defined under type theory, this environment-based approach can also resolve attachment ambigui-ties involving verbs and verb phrases in addition to those involving only nominal referents."
"For example, if the interpreter is given the sentence “Coolant drained after test at 3:00,” which could mean the draining was at 3:00 or the test was at 3:00, the referents for the draining process and the testing process can be treated as time intervals in the environment history. [Footnote_5]"
"5 The composition of time intervals, as well as spatial re-gions and other types of situational referents, is more com-plex than that outlined for objects, but space does not permit a complete explanation."
"First, a forest is con-structed which shares the subtrees for “the test” and “after 3:00,” and the corresponding sets of referents."
Each node in this forest (shown in Fig-ure 3) is then annotated with the set of objects and intervals that it could refer to in the environment.
"Since there were no testing intervals at 3:00 in the environment, the referent set for the NP ‘test after 3:00’ is evaluated to the null set."
"But since there is an interval corresponding to a draining process (  ) at the root, the whole VP will still be pre-ferred as constituent due to the other interpreta-tion."
"The evaluation of referents for quantifiers also presents a tractability problem, because the func-tions they correspond to in the Montague analy-sis map two sets of entities to a truth value."
"This means that a straightforward representation of the potential referents of a quantifier such as ‘at least one’ would contain every pair of non-empty sub-sets of the set of all entities, with a cardinal-ity on the order of ."
"If the evaluation of ref-erents is deferred until quantifiers are composed with the common nouns they quantify over, the input sets would still be as large as the power sets of the nouns’ potential referents."
"Only if the eval-uation of referents is deferred until complete NPs are composed as arguments (as subjects or objects of verbs, for example) can the output sets be re-stricted to a tractable size."
"This provision only covers in situ quantifier scopings, however."
"In order to model raised scop-ings, arbitrarily long chains of raised quantifiers (if there are more than one) would have to be eval-uated before they are attached to the verb, as they are in a CCG-style function composition analy-sis of raising[REF_CITE]. [Footnote_6] Fortunately, univer-sal quantifiers like ‘each’ and ‘every’ only choose the one maximal set of referents out of all the pos-sible subsets in the power set, so any number of raised universal quantifier functions can be com-posed into a single function whose referent set would be no larger than the set of all possible en-tities."
"6 This approach is in some sense wedded to a CCG-style syntacto-semantic analysis of quantifier raising, inasmuch as its syntactic and semantic structures must be isomorphic in order to preserve the polynomial complexity of the shared forest."
"It may not be possible to evaluate the poten-tial referents of non-universal raised quantifiers in polynomial time, because the number of po-tential subsets they take as input is on the or-der of the power set of the noun’s potential ref-erents."
"This apparent failure may hold some ex-planatory power, however, since raised quantifiers other than ‘each’ and ‘every’ seem to be exceed-ingly rare in the data."
This scarcity may be a re-sult of the significant computational complexity of evaluating them in isolation (before they are composed with a verb).
"An implemented system incorporating this environment-based approach to disambiguation has been tested on a set of manufacturer-supplied aircraft maintenance instructions, using a computer-aided design (CAD) model of a portion of the aircraft as the environment."
"It contains several hundred three dimensional objects (buttons, handles, sliding couplings, etc), labeled with object type keywords and connected to other objects through joints with varying degrees of freedom (indicating how each object can be rotated and translated with respect to other objects in the environment)."
The test sentences were the manufacturer’s in- structions for replacing a piece of equipment in this environment.
"The baseline grammar was not altered to fit the test sentences or the environment, but the labeled objects in the CAD model were automatically added to the lexicon as common nouns."
"In this preliminary accuracy test, forest nodes that correspond to noun phrase or modifier cate-gories are dispreferred if they have no potential entity referents, and forest nodes corresponding to other categories are dispreferred if their argu-ments have no potential entity referents."
"Many of the nodes in the forest correspond to noun-noun modifications, which cannot be ruled out by the grammar because the composition operation that generates them seems to be productive (vir-tually any ‘N2’ that is attached to or contained in an ‘N1’ can be an ‘N1 N2’)."
"Potential referents for noun-noun modifications are calculated by a rudimentary spatial proximity threshold, such that any potential referent of the modified noun lying within the threshold distance of a potential ref-erent of the modifier noun in the environment is added to the composed set."
The results are shown below.
The average num-ber of parse trees per sentence in this set was before disambiguation.
"The average ratio of nodes in enumerated tree sets to nodes in shared forests for the instructions in this test set was  , a nearly tenfold reduction due to sharing."
Gold standard ‘correct’ trees were annotated by hand using the same grammar that the parser uses.
The success rate of the parser in this do-main (the rate at which the correct tree could be found in the parse forest) was  .
The reten-tion rate of the environment-based filtering mech-anism described above (the rate at which the cor-rect tree was retained in parse forest) was  of successfully parsed sentences.
The average reduction in number of possible parse trees due to the environment-based filtering mechanism de-scribed above was for successfully parsed and filtered forests. [Footnote_7]
7 Sample parse forests and other details of this application and environment are available[URL_CITE]
"This paper has described a method by which the potential environment referents for all possible in-terpretations of of an input sentence can be evalu-ated during parsing, in polynomial time."
The ar-chitecture described in this paper has been imple-mented with a large coverage grammar as a run-time interface to a virtual human simulation.
It demonstrates that a natural language interface ar-chitecture that uses the objects and events in an application’s run-time environment to inform dis-ambiguation decisions (by performing semantic evaluation during parsing) is feasible for interac-tive applications.
DQDO\]        $F\   ) $ 1)$ $W\     HVSHFLDOO\   OH[      1)$  1)$         ) $ &apos;)$   SURSHUW\      
"PLQLPDOLW\)$  FRQVLGHUDEO\    )$  ,QWURGXFWLRQ $F\)$ )6$  YHU\     OH[        YDULHW\ OH[          / 7KH\  YHU\    OH[    SUHIL[  VXIIL[   E\          "
VWULQJV DF\1)$            DF\ 1)$    1)$      ZD\
"DF\1)$    , H[  * OH[  1)$  E\   W\SLFDOO\  VLJQLILFDQWO\       &apos;)$                      OH[ +        QHFHVVDULO\  1)$          VL]        ) % 1)$  )    OH[  WKH\     E\    +) &gt;RQ@   )       &gt;LW@(YLGHQWO\1)$ \) $       KLJKO\ HYHU\ \     VL]      $  5   &apos;(/ (3(,,    E\ *0LQLVWU\   *     7HFKQRORJ\   )6$#\HVSHFLDOO\ *      *%:    1)$ PLQLPL]   %($0R5( /LQX["
9 §«­¬m¦¡&apos;¤®¡O¯r¡O°¦z± ²´³¥§©µ9¶r· ¿9À¡µ9£m¦z§¸7¦z¡+²´§³[±}°«¦ À °«¡R³¥§GÁÂ³¥§¨0¹­¶9»¼³¥²E±l³¥§«£99³[¹³[»¥¢ª1¤º¹6¤½²¾· ¢¡)ª1³¥±l£K³[¡¦z¨Å³¥²Æ¤&apos;»[¦1¶9¦z§«¤º°­¹°Ã¯ ±l³r¨r¦1¹¤º¶P¨e¤©¡¦z¤¥§«ªÅ¤º¹­»¥³[§°P±hÄÈÇ À £K¦1§«· ±l¦1¶`°«¡D³¥¶U°«9¦ÂÉÈ¤¥¹¹rÊ`°§«¦1¦1°6Ë[³¥µ9§«¶P¤¥¹¥¤º¶P¨%¢¡OmÑ°9¦ £9§«³¨r¦z±[°«¦1»)[[¤ [[³ ªGÄÓ9¦&apos;§«¦z¡µ9¹°«¡Ô¡µ9»¥»§«¤º°¦z¨Õ¡O¯r¡O°¦z±l¡0²´³¶P¡O°§¤½°¦0°«9¦ ¦ Òd¦zª[°§%­¸¥¦z¶9¦z¡«¡¹¦¤º§«¶9[¦z¡O°)/°«P¤½°p±³º²6°P­¡Æ¤º£r·)³[§¦ ¿9À­¶§«µ9¹¦¡¦1§¤º¹¦z¡Ô¤¥¶P¨e±%±Ô¤_¯ ©¬K¦Ô²´¦z¤)³[§£P9³¥»[¡O­¬9¹­¦¥Äh×Ø¶Ù¤¥¨9¨9[§«¤¥£99¦1±l¢ªp¤[¨_°«ÖÃµP¡O°±l¦1¶`°³[¶GÚ6¡¦1¸`· +¨r¦ ¿ ¶9°­³¥¶P¡[/[[³¥§«°«9±Ûªz¤º¶)¬m¦ ²´³¥§«±Ô¤º¹­Üz¦z¨Ô[§±l¦¨ ¬7¯Û¡µ9¬P¡¦  À ¦z¡Vµ9¶m¨r¦1§Å­¶Pª ¹­µP¡³[¶GÄ ²´µ9¹p²´³¥§e9[§±Ô¤º¹­¶7¸¥¦¡Ã°«¢¡O±Þ»`¤½°«­¡©¦¶9»ß¤¥¹°«¦1§«¶P¤½°«À ¸[¦Ó¡O¦¤º§ª · ¡
O°§¤½°¦z»¥­¦z¡p³½¸¥¦z§¼°«9¦Å¡¤¥±)¦V±l³¥§«£99³[¹³[»¥¢ª1¤¥¹ 7¯7£m³¥°9¦¡O¢¡/¡O£m¤¥ª ¦ [Ä à á
We propose a statistical method that finds the maximum-probability seg-mentation of a given text.
This method does not require training data because it estimates probabilities from the given text.
"Therefore, it can be applied to any text in any domain."
An experi-ment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation sys-tem.
Documents usually include various topics.
"Identi-fying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, in-cluding information retrieval[REF_CITE]and summarizati[REF_CITE]."
"In informa-tion retrieval, users are often interested in par-ticular topics (parts) of retrieved documents, in-stead of the documents themselves."
"To meet such needs, documents should be segmented into co-herent topics."
Summarization is often used for a long document that includes multiple topics.
A summary of such a document can be composed of summaries of the component topics.
Identifi-cation of topics is the task of text segmentation.
A lot of research has been done on text seg-mentati[REF_CITE].
A major characteristic of the methods used in this research is that they do not require training data to segment given texts.
"Consequently, these methods can be applied to any text in any domain, even if training data do not exist."
"This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents."
Another application of text segmentation is the segmentation of a continuous broadcast news story into individual stories[REF_CITE].
"In this application, systems relying on supervised learning[REF_CITE]achieve good performance because there are plenty of training data in the domain."
"These systems, however, can not be applied to domains for which no training data exist."
The text segmentation algorithm described in this paper is intended to be applied to the sum-marization of documents or speeches.
"Therefore, it should be able to handle domain-independent texts."
The algorithm thus does not use any train-ing data.
It requires only the given documents for segmentation.
"It can, however, incorporate train-ing data when they are available, as discussed in Section 5."
The algorithm selects the optimum segmen-tation in terms of the probability defined by a statistical model.
This is a new approach for domain-independent text segmentation.
Previous approaches usually used lexical cohesion to seg-ment texts into topics.
"The statistical model for the algorithm is de-scribed in Section 2, and the algorithm for ob-taining the maximum-probability segmentation is described in Section 3."
Experimental results are presented in Section 4.
"Further discussion and our conclusions are given in Sections 5 and 6, respec-tively."
We first define the probability of a segmentation of a given text in this section.
"In the next section, we then describe the algorithm for selecting the most likely  segmentation. be a  text beconsistinga segmen-of"
"Let words, and let tation of consisting of segments."
"Then the probability of the segmentation   &quot;! is defined by:    (1) The most likely %&amp;$ ) segmentation (*, &amp;$ + &quot; # ! &quot;. is given by: # /&quot; (2) is a constant &quot; for a given  text because ."
"The definitions of and are given below, in that  order &quot; .2.1"
Definition of We define a topic by the distribution of words in that topic.
We assume that different topics have different word distributions.
We further assume that different topics are statistically independent of each other.
We also assume that the words within the scope of a topic are statistically inde-pendent 1 ofbeeachthe othernumbergiven the topicin .segment 1
"If we define 1 , %:  ! and 1 correspond &lt;1 ; 1 then hold."
This means that and to each &quot; other.
"Under our assumptions, can be de-composed as follows:   = &quot; 9 &lt;1 ; &quot; 1 &lt;; &gt;5 &quot;[Footnote_1] ? 112 1  1 ?&lt;[Footnote_1] ; 12 2 ; 1 (3) as 1 E ."
1 4 1 as
1 4 1 as
"Next, we define : ? 21 1 @BA 1 C 1 2DFG (4) where A 1 ? 12 words in 1 that is the 1 number G are the same as 2 and is theof  number %&amp; of : different words in ."
"For example :&amp; , L&amp; if L&amp;NOL&amp;L)&amp;HKJ!H , then A &amp; , where A &amp;&quot;J&amp; , A &gt; and &quot; , A &gt;&quot; , , and ."
"Equation (4) is known as Laplace’s law (Manning Schütze ? 21 and A 1 , 1999)."
A 1  12 @%Z ? 21  1 651 can be defined as: (5)  165 [@ \ 65; ^] ? \1 . 21 &quot;. for
Z ? 21  1 (6) where ]
C 1\ .&quot; 12 _C`1 E&quot;. when 12 acb 1\ 1 and 2 are the same word Z &quot; and H &amp;J HK!J]
"HKd\ example &quot;HK. , ] &quot;%J&amp;."
R ] &quot; otherwise HK.
KH &quot;D ] !
J . .
HK For &quot;D ] .
Equations (5) and (6) are used in Section 3 to describe the algorithm for finding the maximum-probability segmentation  . 2.2 Definition of  The definition of can vary depending on our prior information about the possibility of seg-mentation .
"For example, we might know the average length  of the segments and want to incor-porate it into ."
"Our assumption, however, is that we do not have such prior information."
"Thus, we have to use some uninformative  prior probability."
"We define (7) Equation (7) is determined k ;  jmlon , p on the basis of its de-scription length, 1 i.e.,   t &apos; (8) where k bits. 2"
This description length is derived as follows:
"Suppose that there are two people, a sender and a receiver, both of whom know the text to be seg-mented."
"Only the sender knows the exact seg-mentation, and he/she should send a message so u that the receiver can segment the text correctly. vxwzy{v |zyz}z}~zyCv-} To this end, it is sufficient for the sender to send integers, i.e., , because these integers represent the lengths of segments and v u 5 v5!v thus uniquely determine the segmentation once !v v the text is known."
"A segment length can be encoded using bits, because is a number between 1 and ."
The total description length for all the segment lengths is thus bits. [Footnote_3]
"3 We have used | !v &lt;&lt;^  u ! v logarithm to the base 2. as before. But we use  | u !v!v in this paper, because it is easily interpreted as a description length and the experimental results obtained by using are slightly better than those obtained by us-  ing . An anonymous reviewer suggests using a Pois-son distribution whose parameter is , the average length of a segment (in words), as prior probability. We leave it for future work to compare the suitability of various prior probabilities for text segmentation."
"Generally speaking, takes a large value when the number   of segments is small."
"On the takes a large value  when the other hand, number of segments is large."
"If only is used to segment the text, then the resulting seg-mentation 0 will have too many segments."
"By usingboth and , we can get a reason-able number of segments."
"To # , wefindfirstthedefinemaximum-probability @f r t  ! &quot;. segmentation the cost of segmentation as (9) 1"
"Stolcke and Omohundro uses description length priors to induce the structure of hidden Markov models (Stolcke to obtain # , because  and we then minimize # %$&amp;)(*, $&amp;+ &quot; ! $!)*( ,   where"
L 2 ; r t &apos; A 1 ? 1 12iD G D r t &apos; @ (12)
"We further rewrite[REF_CITE]in the form by C  using 1 Equation &gt;1 5 of Equation (13 1 ) withbelow  (5) and  6z6 replacing  , where is the length of words, i.e.,the number of word tokens in words."
2 ‘Log’ denotes the  and[REF_CITE]).
This section describes an algorithm for finding the minimum-cost segmentation.
"First, we define the terms and symbols used  to describe  the algorithm. we define  1 consisting of 1 Given a text words &lt;1  ,  , so that e isasjustthe beforeposition . ¦¥¨§ and  between and is , where £ £ just after . is a set of nodes andNext, we define a graph ¥ is a set of edges. is defined as %ª  1 b«F¬h« [­ (14) and is defined as ¥ %ª&gt;¯ 1 2 b°«4¬²± 3 « d­ . (15) where the edges are ordered ¯ 1 2 ; the 1 initial vertex 2 are  and  , respec-and the terminal vertex of tively."
An example ¯ of 1 2¯ is shown 1&lt; in  Figure 1&lt;  1. 2 . that 1 2 We say that covers means 1C   2 represents a segment L 1 2 This 1&lt; ¯ 1 2 .
"Thus, we define the cost of edge by using[REF_CITE]:"
L 1 2 %? 1&lt; &lt; &lt;1   . ^&quot;. 2 (16) where is the number of different words in G .
"Given these definitions, we describe the algo-rithm to find the minimum-cost segmentation or"
L 12 ¯ 1 2 b¨« maximum-probability segmentation as follows:
Step ¬²± 1.
Calculate 3 « the cost of edge for by using[REF_CITE].
Step  2..Find the minimum-cost path from  to
Algorithms for finding the minimum-cost path in a graph are well known.
An algorithm that can provide a solution for Step 2 will be a simpler ver-sion of the algorithm used to find the maximum-probability solution in Japanese morphological analysis[REF_CITE].
"Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm. 4 DP algorithms have also been used for text segmentation by other researchers[REF_CITE]."
The path thus obtained represents the minimum-cost segmentation in when edges correspond ¯ with  &amp;? segments &quot;µ .
"In Figure 1, for then ¶ example  , ?· if ¶  ¶  is the minimum-cost path, is the minimum-cost segmentation."
The algorithm automatically determines the number of segments.
But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path.
"The algorithm allows the text to be segmented anywhere between words; i.e., all the positions between words are candidates for segment bound-aries."
"It is easy, however, to modify the algorithm so that the text can only be segmented at partic-ular positions, such as the ends of sentences or ¥ paragraphs."
This is done by using a subset of[REF_CITE].
"We use only the edges whose initial and terminal vertices are candidate bound-aries that meet particular conditions, such as be-ing the ends of sentences or paragraphs."
We then obtain the minimum-cost path by doing Steps 1 and 2.
The minimum-cost segmentation thus ob-tained meets the boundary conditions.
"In this pa-per, we assume that the segment boundaries are at the ends of sentences."
"Generally speaking, the number of segments ob-tained by our algorithm is not sensitive to the length of a given text, which is counted in words."
"In other words, the number of segments is rela-tively stable with respect to variation in the text length."
"For example, the algorithm divides a newspaper editorial consisting of about 27 sen-tences into [Footnote_4] to 6 segments, while on the other hand, it divides a long text consisting of over 1000 sentences into 10 to 20 segments."
4 A program that implements the algorithm described in this section is available[URL_CITE]
"Thus, the num-ber of segments is not proportional &apos; to text length."
This is due to the term  t[REF_CITE].
The value of this term increases as the number of words increases.
The term thus suppresses the di-vision of a text when the length of the text is long.
"This stability is desirable for summarization, because summarizing a given text requires select-ing a relatively small number of topics from it."
"If a text segmentation system divides a given text into a relatively small number of segments, then a summary of the original text can be composed by combining summaries of the component seg-ments[REF_CITE]."
"A finer segmentation can be obtained by applying our algorithm recursively to each segment, if neces-sary. [Footnote_5]"
"5 We segmented various texts without rigorous evaluation and found that our method is good at segmenting a text into a relatively small number of segments. On the other hand, the method is not good at segmenting a text into a large num-ber of segments. For example, the method is good at seg-menting a 1000-sentence text into 10 segments. In such a case, the segment boundaries seem to correspond well with topic boundaries. But, if the method is forced to segment the same text into 50 segments by specifying the number of"
We used publicly available data to evaluate our system.
"This data was used[REF_CITE]to compare various domain-independent &gt; text º seg-mentation systems. [Footnote_6] He evaluated[REF_CITE], TextTiling[REF_CITE], DotPlot[REF_CITE], and Segmenter (Kan  et al., 1998) byachieved theusing the data and reported that best performance among these systems."
6 The data is available[URL_CITE]C99-1.2-release.tgz.
The data description is as follows: “An artifi-cial test corpus of 700 samples is used to assess the accuracy and speed performance of segmen-tation algorithms.
A sample is a concatenation of ten text segments.
A segment is the first sen-tences of a randomly selected document from the Brown corpus.
A sample is characterised by the range .”[REF_CITE]
Table 1 gives the corpus statistics. probabilistic error metric Ã \
"Segmentation accuracy was measured by the man, et al. (1999). [Footnote_7] Low Ã \ proposed by Beefer-indicates high accu-edges in the minimum-cost path, then the resulting segmen-tation often contains very small segments consisting of only one or two sentences."
"7 Let Æ Ç9È /É Ê9Ë¦Ì which is contained in the package, for our experiment. be a correct segmentation and let be a seg-mentation proposed by a text segmentation system: Then the"
"We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of segments was somewhat larger than that of the minimum-cost path, whose number of segments was auto-matically determined by the algorithm. racy."
"The sample texts were preprocessed – i.e., punc-tuation and stop words were removed and the re-maining words were stemmed – by a program us-ing the libraries available in Choi’s package."
The texts were then segmented by the systems listed in Tables 2 and 3.
The segmentation boundaries were placed at the ends of sentences.
The seg-mentations were evaluated by applying an evalu- The results are listed in Tables 2 and 3.
Í &gt;b b ation program in Choi’s package. is the result for our system when the numbers b´b of nÎ p ments were determined by the system.
Í seg- is the result for our system when the  numbers  of &gt; seg- º nÎ p ments were given beforehand. [Footnote_8] and are the corresponding results for the systems de-scribed in Choi’s paper[REF_CITE]. [Footnote_9] » ¼¿½!Ñ Ñ½ » ¼¿Ñ Ñ  ¾ ments were determined by the systems.
"8 If two segmentations have the same cost, then our sys-"
"9 The results for Ò !Â Â/×Ø{Ù tems arbitrarily select one of them; i.e., the systems select the segmentation processed previously. in Table 3 are slightly different from those listed in Table 6 of Choi’s paper[REF_CITE]. This is because the original results in that paper were based on 500 samples, while the results in our Table 3 were based on 700 samples (Choi, personal communication)."
"In these tables, the symbol “ Ó&gt;Ó ” indicates that the difference in Ã \ between the two systems is ÔÕ9?Æ Ç9È yCÉ/Ê!Ë¦Ì9 statistically significant at the 1% level, based on a one-sided Ú -test of the null hypothesis of equal means."
The probability of the null hypothesis being true is displayed in the row indicated  by“prob”.
"The column labels, such as “ ”, in-erages of Ã \ dicate that the numbers in the column are the av- “Total” indicates the averages of Ã \ over the corresponding sample texts. over all the text samples."
These tables show statistically that our system  is more accurate than or at least as accurate as .
"This means that our system is more accurate than or at least as accurate as previous domain-independent  text segmentation systems, becausehas been shown to be more accurate than pre-vious domain-independent text segmentation sys-tems. 10"
Evaluation of the output of text segmentation sys-tems is difficult because the required segmenta-tions depend on the application.
"In this paper, we have used an artificial corpus to evaluate our sys-tem."
We regard this as appropriate for comparing relative performance among systems.
"It is important, however, to assess the perfor-mance of systems by using real texts."
These texts should be domain independent.
They should ä also be multi-lingual if we want to test the mul- 10 Speed performance Ï !
Ð Ð ÒÏ !
Ð Ð Ï Ð!Ð&amp;Ü is not our main concern in this pa-
Ò Â!Â per.
Our implementations of and are not opti-mum.
"However, and , which are implemented in C, run as fast as and , which are implemented in Java[REF_CITE], due to the difference in programming lan-"
Ï Ð!Ð!Â&amp;ÂÜßÝÝÝ ÏÒ Ð!Ð guages.
The average run times for a sample text were ½ »} !À&amp; ½ } Þ´Â&amp; sec. & amp;½ »} !à sec.
Ò !Â Â ÜßÝ ½&amp;}Þ´¾ sec. sec. tilinguality of systems.
"For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans[REF_CITE]."
"But, there are no such corpora for other languages."
"We are planning to build a segmen-tation corpus for Japanese, based on a corpus of speech transcriptions[REF_CITE]."
Our proposed algorithm finds the maximum-probability segmentation of a given text.
This is a new approach for domain-independent text segmentation.
"A probabilistic approach, however, has already been proposed by Yamron, et al. for domain-dependent text segmentation (broadcast news story segmentation)[REF_CITE]."
"They trained a hidden Markov model (HMM), whose states correspond to topics."
"Given a word sequence, their system assigns each word a topic so that the maximum-probability topic sequence is obtained."
"Their model is basically the same as that used for HMM part-of-speech (POS) taggers[REF_CITE], if we regard topics as POS tags. [Footnote_11] Finding topic boundaries is equiv-alent to finding topic transitions; i.e., a continuous topic or segment is a sequence of words with the same topic."
"11 The details are different, though."
"Their approach is indirect compared with our approach, which directly finds the maximum-probability segmentation."
"As a result, their model can not straightforwardly incorporate features pertaining to a segment itself, such as the average length of segments."
"Our model, on the other hand, Suppose that the length of a ./ segment  á can incorporate this information quite naturally. a normal distribution â  å follows , with a mean of and standard deviation[REF_CITE]."
L æ¿ n w  |   5p rt &apos; Z ?
C1 1  1 1  1&gt;5651 DiDWG E Dhè r t &apos; 2 ;
Dhê rt &apos; â  ?   1 E &gt;5  . . 1 (17) where :æ .
"Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does."
"Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available."
"It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available."
"In such a case, our model should be extended to incor-porate various features such as the average seg-ment length, clue words, named entities, and so[REF_CITE]."
Our proposed algorithm naturally estimates the probabilities of words in segments.
"These prob-abilities, which are called word densities, have been used to detect important descriptions of words in texts[REF_CITE]."
This method is based on the assumption that the den-sity of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth.
It would be interesting to apply our method to this application.
We have proposed a statistical model for domain-independent text segmentation.
This method finds the maximum-probability segmentation of a given text.
The method has been shown to be more accurate than or at least as accurate as previous methods.
We are planning to build a segmenta-tion corpus for Japanese and evaluate our method against this corpus.
Multi-processor systems are becom-ing more commonplace and afford-able.
"Based on analyses of ac-tual parsings, we argue that to ex-ploit the capabilities of such ma-chines, unification-based grammar parsers should distribute work at the level of individual unification oper-ations."
"We present a generic ap-proach to parallel chart parsing that meets this requirement, and show that an implementation of this tech-nique for LinGO achieves consider-able speedups."
The increasing demand for accuracy and ro-bustness for today’s unification-based gram-mar parsers brings on an increasing demand for computing power.
"In addition, as these systems are increasingly used in applications that require direct user interaction, e.g. web-based applications, responsiveness is of major concern."
"In the mean time, small-scale desk-top multiprocessor systems (e.g. dual or even quad Pentium machines) are becoming more commonplace and affordable."
"In this paper we will show that exploiting the capabilities of these machines can speed up parsers con-siderably, and can be of major importance in achieving the required performance."
There are certain requirements the design of a parallel parser should meet.
"Over the past years, many improvements to existing parsing techniques have boosted the perfor-mance of parsers by many factors (Oepen and"
"If a design of a parallel parser is tied too much to a particular ap-proach to parsing, it may be hard to incorpo-rate such improvements as they become avail-able."
"For this reason, a solution to parallel parsing should be as general as possible."
One obvious way to ensure that optimizations for sequential parsers can be used in a parallel parser as well is to let a parallel parser mimic a sequential parser as much as possible.
This is basically the approach we will take.
The parser that we will present in this pa-per uses the LinGO grammar.
LinGO is an HPSG-based grammar which was developed at Stanford[REF_CITE].
It is currently used by many research institutions.
This al-lows our results to be compared with that of other research groups.
"In Section 2, we explore the possibilities for parallelism in natural language parsing by an-alyzing the computational structure of pars-ings."
Section 3 and 4 discuss respectively the design and the performance of our system.
"Finally, we compare our work with other re-search on parallel parsing."
To analyze the possibilities for parallelism in computations they are often represented as task graphs.
"A task graph is a directed acyclic graph, where the nodes represent some unit of computation, called a task, and the arcs represent the execution dependencies between the tasks."
"Task graphs can be used to an-alyze the critical path, which is the mini-mal time required to complete a computa-tion, given an infinite amount of processors."
"T P ≤ T 1 /P + T ∞ , (1) where T 1 is the total work, or the execution time for the one processor case, and T ∞ is the critical path."
"Furthermore, to effectively use P processors, the average parallelism P̄ ="
T 1 /T ∞ should be larger than P.
The first step of the analysis is to find an appropriate graph representation for parsing computations.
"According[REF_CITE], performing a complexity analysis solely at the level of grammars and parsing schemata can give a distorted image of the parsing pro-cess in practice."
"For this reason, we based our analysis on actual parsings."
"The experi-ments were based on the fuse test suite, which is a balanced extract from four appointment scheduling (spoken) dialogue corpora (incl."
Fuse contains over 2000 sen-tences with an average length of 11.6.
We define a task graph for a single pars-ing computation as follows.
"First, we distin-guish two types of tasks: unification tasks and match tasks."
A unification task executes a single unification operation.
A match task is responsible for all the actions that are taken when a unification succeeds: matching the resulting edge with other edges in the chart and putting resulting unification tasks on the agenda.
The match task is also responsible for applying filtering techniques like the quick check[REF_CITE].
The tasks are connected by directed arcs that indicate the execution dependencies.
We define the cost of each unification task as the number of nodes visited during the unification and successive copying operation.
Unification operations are typically responsi-ble for over 90% of the total work.
"In addi-tion, the cost of the match tasks are spread out over succeeding unification tasks."
"We therefore simply neglect the cost for match op-erations, and assume that this does not have a significant impact on our measurements."
The length of a path in the graph can now be de-fined as the sum of the costs of all nodes on the path.
The critical path length T ∞ can be defined as the longest path between any two nodes in the graph.
"The presented model resembles a very fine-grained scheme for distributing work, where each single unification tasks to be scheduled independently."
"In a straightforward imple-mentation of such a scheme, the scheduling overhead can become significant."
Limiting the scheduling overhead is crucial in obtaining considerable speedup.
It might therefore be tempting to group related tasks into a single unit of execution to mitigate this overhead.
For this reason we also analyzed a task graph representation where only match tasks spawn a new unit of execution.
The top graph in Figure 1 shows an example of a task graph for the first approach.
The bottom graph of Figure 1 shows the corresponding task graph for the second approach.
"Note that because a unification task may depend on more than one match task, a choice has to be made in which unit of execution the unification task is put."
Table 1 shows the results of the critical path analysis of both approaches.
"For the first ap-proach, the critical path is uniquely defined."
"For the second approach we show both the worst case, considering all possible schedul-ings, and an average case."
"The results for T 1 , T ∞ , and P̄ are averaged over all sentences. 1"
"The results show that, using the first ap-proach, there is a considerable amount of par-allelism in the parsing computations."
"The re-sults also show that a small change in the de-sign of a parallel parser can have a signifi-cant impact on the value for P̄. To obtain a speedup of P, in practice, there should be a safety margin between P and P̄. This sug-gests that the first approach is a considerably saver choice, especially when one is consider-ing using more than a dozen of processors."
"Based on the discussion in the preceding sec-tions, we can derive two requirements for the design of a parallel parser: it should be close in design to a sequential parser and it should allow each single unification operation to be scheduled dynamically."
The parallel parser we will present in this section meets both require-ments.
Let us first focus on how to meet the first requirement.
"Basically, we let each processor, run a regular sequential parser augmented with a mechanism to combine the results of the different parsers."
Each sequential parser component is contained in a different thread.
"By using threads, we allow each parser to share the same memory space."
"Initially, each thread is assigned a different set of work, for example, resembling a different part of the in-put string."
"A thread will process the unifica-tion tasks on the agenda and, on success, will perform the resulting match task to match the new edge with the edges on its chart."
"After completing the work on its agenda, a thread will match the edges on its chart with the edges derived so far by the other threads."
"This may produce new unification tasks, which the thread puts on its agenda."
"After the commu-nication phase is completed, it returns to nor-mal parsing mode to execute the work on its agenda."
"This process continues until all edges sults for P̄ turn out slightly lower than might have 1 Note that since PT 1 /PT ∞ 6= PT 1 /T ∞ , the re-been expected from the values of T 1 and T ∞ . of all threads have been matched against each other and all work has been completed."
Figure 2 shows an outline of our approach in terms of data structures.
"Each thread con-tains an agenda, which can be seen as a queue of unification tasks, a chart, which stores the derived edges, and a heap, which is used to store the typed-feature structures that are ref-erenced by the edges."
"Each thread has full ac-cess to its own agenda, chart, and heap, and has read-only access to the respective struc-tures of all other threads."
Grammars are read-only and can be read by all threads.
"In the communication phase, threads need read-only access to the edges derived by other threads."
This is especially problematic for the feature structures.
Many unification al-gorithms need write access to scratch fields in the graph structures.
Such algorithms are therefore not thread-safe. 2
"For this reason we use the thread-safe unification algorithm pre-sented[REF_CITE], which is com-parable in performance to Tomabechi’s algo-rithm[REF_CITE]."
Note that each thread also has its own agenda.
Some parsing systems require strict control over the order of evaluation of tasks.
The distributed agendas that we use in our approach may make it hard to implement such a strict control.
One solution to the problem would be to use a centralized agenda.
The dis-advantage of such a solution is that it might increase the synchronization overhead.
Tech-niques to reduce the synchronization overhead 1. newWork ← not IsEmpty(agenda). [Footnote_2]. Process the agenda as in the sequential case.
"2 In this context, thread safe means that the same data structure can be involved in more than one op-eration, of more than one thread, simultaneously."
"In addition, stamp each newly de-rived I edge by setting I.generation to the current value for threadGen and add I to this thread’s edge list. 3. Examine all the other threads for newly derived edges."
"For each new edge I and for each edge J on the chart for which holds I.generation &gt; J.generation, add the cor-responding task to the agenda if it passes the filter."
"If any edge was processed, set newWork to true. 4. if not newWork then in such a setup can be found[REF_CITE]."
"At startup, each thread calls the scheduling algorithm shown in Figure 3."
This algorithm can be seen as a wrapper around an existing sequential parser that takes care of combin-ing the results of the individual threads.
The functionality of the sequential parser is em-bedded in step 2.
"After this step, the agenda will be empty."
The communication between threads takes place in step 3.
"Each time a thread executes this step, it will proceed over all the newly derived edges of other threads (foreign edges) and match them with the edges on its own chart (local edges)."
Checking the newly derived edges of other threads can simply be done by proceeding over a linked list of derived edges maintained by the respective threads.
Threads record the last visited edge of the list of each other thread.
This ensures that each newly derived item needs to be vis-ited only once by each thread.
"As a result of step 3, the agenda may be-come non-empty."
"In this case, newWork will be set and step 2 is executed again."
This cycle continues until all work is completed.
"The remaining steps serve several purposes: load balancing, preventing double work, and detecting termination."
We will explain each of these aspects in the following sections.
Note that step 6 and 7 are protected by a lock.
This ensures that no two threads can execute this code simultaneously.
This is necessary because Step 6 and 7 write to variables that are shared amongst threads.
"The overhead incurred by this synchronization is minimal, as a thread typically iterates over this part only a small number of times."
"This is because the depth of the derivation graph of any edge is limited (average 14, maximum 37 for the fuse test set)."
"In the design as presented so far, each thread exclusively executes the unification tasks on its agenda."
"Obviously, this violates the re-quirement that each unification task should be scheduled dynamically."
"In[REF_CITE], it is shown that for any multi-threaded compu-tation with work T 1 and task graph depth T ∞ , and for any number P of processors, a scheduling will achieve T P ≤ T 1 /P +T ∞ if for the scheduling holds that whenever there are more than P tasks ready, all P threads are executing work."
"In other words, as long as there is work on any queue, no thread should be idle."
An effective technique to ensure the above requirement is met is work stealing[REF_CITE].
"With this technique, a thread will first attempt to steal work from the queue of another thread before denouncing itself to be idle."
"If it succeeds, it will resume nor-mal execution as if the stolen tasks were its own."
"Work stealing incurs less synchroniza-tion overhead than, for example, a centralized work queue."
"In our implementation, a thread becomes a thief by calling Steal, at step 4 of Sched."
"Steal allows stealing from two types of queues: the agendas, which contain outstand-ing unification tasks, and the unchecked for-eign edges, which resemble outstanding match tasks between threads."
A thief first picks a random victim to steal from.
It first attempts to steal the victim’s match tasks.
"If it succeeds, it will perform the matches and put any resulting unification tasks on its own agenda."
"If it cannot gain exclusive access to the lists of unchecked for-eign edges, or if there were no matches to be performed, it will attempt to steal work from the victim’s agenda."
A thief will steal half of the work on the agenda.
This balances the load between the two threads and minimizes the chance that either thread will have to call the expensive steal operation soon thereafter.
Note that idle threads will keep calling Steal until they either obtain new work or all other threads become idle.
"Obviously, stealing eliminates the exclusive ownership of the agenda and unchecked for-eign edge lists of the respective threads."
"As a consequence, a thread needs to lock its agenda and edge lists each time it needs to access it."
"We use an asymmetric mutual exclusion scheme, as presented[REF_CITE], to minimize the cost of locking for normal pro-cessing and move more of the overhead to the side of the thief."
"When two matching edges are stored on the charts of two different threads, it should be prevented that both threads will perform the corresponding match."
Failing to do so can cause the derivation of duplicate edges and eventually a combinatorial explosion of work.
Our solution is based on a generation scheme.
"Each newly derived edge is stamped with the current generation of the respective thread, threadGen (see step 2)."
"In addition, a thread will only perform the match for two edges if the edge on its chart has a lower generation than the foreign edge (see step 3)."
"Obviously, because the value of threadGen is unique for the thread (see step 6), this scheme prevents two edges from being matched twice."
Sched also ensures that two matching edges will always be matched by at least one thread.
"After a thread completes step 3, it will always raise its generation."
The new gen-eration will be greater than that of any for-eign edge processed before.
"This ensures that when an edge is put on the chart, no for-eign edge with a higher generation has been matched against the respective chart before."
"A thread may terminate when all work is com-pleted, that is, if and only if the following conditions hold simultaneously: all agendas of all threads are empty, all possible matches between edges have been processed, and all threads are idle."
Step 7 of Sched enforces that these conditions hold before any thread leaves Sched.
"Basically, each thread deter-mines for itself whether its queues are empty and raises the global counter NrThreadsIdle accordingly."
"When all threads are idle simul-taneously, the parser is finished."
A thread’s agenda is guaranteed to be empty whenever newWork is false at step 7.
The same does not hold for the unchecked foreign edges.
"Whenever a thread derives a new edge, all other edges need to perform the corresponding matches."
The following mecha-nism enforces this.
The first thread to become idle raises the global generation and records it in IdleGen.
Subsequent idle threads will adopt this as their idle generation.
"When-ever a thread derives a new edge, it will raise Generation and reset NrThreadsIdle (step 6)."
This invalidates IdleGen which implicitly re-moves the idle status from all threads.
Note that step 7 lets each thread perform an addi-tional iteration before raising NrThreadsIdle.
This allows a thread to check for foreign edges that were derived after step 3 and before 7.
"Once all work is done, detecting termination"
The implementation of the system con-sists of two parts: MACAMBA and CaLi.
MACAMBA stands for Multi-threading Ar-chitecture for Chart And Memoization-Based Applications.
The MACAMBA framework provides a set of objects that implement the scheduling technique presented in the previ-ous section.
It also includes a set of sup-port objects like charts and a thread-safe uni-fication algorithm.
CaLi is an instance of a MACAMBA application that implements a Chart parser for the LinGO grammar.
"The design of CaLi was based on PET[REF_CITE], one of the fastest parsers for LinGO."
"It implements the quick check[REF_CITE], which, together with the rule check, takes care of filtering over 90% of the failing unification tasks before they are put on the agenda."
"MACAMBA and CaLi were both im-plemented in Objective-C and currently run on Windows NT, Linux, and Solaris."
The performance of the sequential version of CaLi is comparable to that of PET. 4
"In ad-dition, for the single-processor parallel ver-sion of CaLi the total overhead incurred by scheduling is less than 1%."
"The first set of experiments consisted of running the fuse test suite on a SUN Ultra Enterprise with 8 nodes, each with a 400 MHz"
"UltraSparc processor, for a varying number of processors."
Table 2 shows the results of these experiments. [Footnote_5] The execution times for each parse are measured in wall clock time.
"5 Because the system was shared with other users, only 6 processors could be utilized."
The time measurement of a parse is started be-fore the first thread starts working and ends only when all threads have stopped.
The fuse test suite contains a large number of small sentences that are hard to parallelize.
These results indicate that deploying multiple pro-cessors on all input sentences unconditionally still gives a considerable overall speedup.
The second set of experiments were run on a[REF_CITE]with 64 250 MHz Ul-traSparc II processors.
"To limit the amount of data generated by the experiments, and to in-crease the accuracy of the measurements, we selected a subset of the sentences in the fuse suite."
The parser is able to parse many sen-tences in the fuse suite in fewer than several milliseconds.
Measuring speedup is inaccu-rate in these cases.
We therefore eliminated such sentences from the test suite.
From the remaining sentences we made a selection of 500 sentences of various lengths.
The results are shown in Figure 4.
"The fig-ure includes a graph for the maximum, mini-mum, and average speedup obtained over all sentences."
The maximum speedup of 31.[Footnote_4] is obtained at 48 processors.
"4[REF_CITE]s and 1339s on a 500MHz P-III, where both parsers used the same parsing schema."
The overall peak is reached at 32 processors where the average speedup is 17.[Footnote_3].
3 No locking is required once a thread is idle.
One of the reasons for the decline in speedup after 32 processors is the overhead in the scheduling algorithm.
"Most notably, the total number of top-level itera-tions of Sched increases for larger P. The minimum speedups of around 1 are obtained for, often small, sentences that contain too lit-tle inherent parallelism to be parallelized ef-fectively."
"Figure 4 shows a graph of the parallel ef-ficiency, which is defined as speedup divided by the number of processors."
The average ef-ficiency remains close to 80% up till 16 pro-cessors.
"Note that super linear speedup is achieved with up to 12 processors, repeat-edly for the same set of sentences."
Super lin- ear speedup can occur because increasing the number of processors also reduces the amount of data handled by each node.
This reduces the chance of cache misses.
Parallel parsing for NLP has been researched extensively.
"For example,[REF_CITE]presented some implementations of parallel chart parsers."
A survey of parallel processing in NLP is given[REF_CITE].
"Nevertheless, many of the presented solu-tions either did not yield acceptable speedup or were very specific to one application."
"Re-cently, several NLP systems have been par-allelized successfully."
"The disadvantage of this approach, though, is that it can only be applied to parsers developed in Prolog."
"This solution exploits coarse-grained parallelism of the kind that is unusable for many parsing applications, including our own (see also Görz et. al. (1996))."
"Although their best results were obtained with random grammars, speedups for the English grammar were also considerable."
"Yoshida et. al.[REF_CITE]pre-sented a 2-phase parallel FB-LTAG parser, where the operations on feature structures are all performed in the second phase."
"The speedup ranged up to 8.8 for 20 processors, Parallelism is mainly thwarted by a lack of parallelism in the first phase."
"Finally,[REF_CITE]developed an agent-based parallel parser that achieves speedups of up to 13.2."
It is implemented in ABCL/f and LiLFeS.
They also provide a generic solution that could be applied to many parsers.
The main difference with our system is the distribution of work.
This system uses a tabular chart like distribution of matches and a randomized distribution of unification tasks.
Experiments we conducted show that the choice of distribution scheme can have a significant influence on the cache utilization.
"It should be mentioned, though, that it is in general hard to compare the performance of systems when different grammars are used."
"On the scheduling side, our approach shows close resemblance to the Cilk-5 system[REF_CITE]."
It implements work stealing using similar techniques.
"An important dif-ference, though, is that our scheduler was designed for chart parsers and tabular algo-rithms in general."
These types of applications fall outside the class of applications that Cilk is capable of handling efficiently.
We showed that there is sufficient parallelism in parsing computations and presented a par-allel chart parser for LinGO that can effec-tively exploit this parallelism by achieving considerable speedups.
"Also, the presented techniques do not rely on a particular parsing schema or grammar formalism, and can there-fore be useful for other parsing applications."
This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the[REF_CITE]Darpa Communicator data collection.
We describe results based on the stan-dard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme.
"We show that per-formance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics im-proved the models by an absolute 5%."
The objective of the DARPA COMMUNICATOR program is to support research on multi-modal speech-enabled dialogue systems with advanced conversational capabilities.
"In order to make this a reality, it is important to understand the con-tribution of various techniques to users’ willing-ness and ability to use a spoken dialogue system."
In[REF_CITE]we conducted an exploratory data collection experiment with nine participating communicator systems.
All systems supported travel planning and utilized some form of mixed-initiative interaction.
"However the systems var-ied in several critical dimensions: (1) They tar-geted different back-end databases for travel in-formation; (2) System modules such as ASR, NLU, TTS and dialogue management were typ-ically different across systems."
"The Evaluation Committee chaired by Walker[REF_CITE], with representatives from the nine COMMUNICATOR sites and from NIST, de-veloped the experimental design."
A logfile stan-dard was developed by MITRE along with a set of tools for processing the logfiles[REF_CITE]; the standard and tools were used by all sites to collect a set of core metrics for making cross system comparisons.
"The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be imple-mented consistently across systems."
NIST’s con-tribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee.
"The experiment was designed to make it possi-ble to apply the PARADISE evaluation framework[REF_CITE], which integrates and unifies previous approaches to evaluati[REF_CITE]."
The framework posits that user satisfaction is the overall objective to be maximized and that task success and various in-teraction costs can be used as predictors of user satisfaction.
Our results from applying PARADISE include that user satisfaction differed consider-ably across the nine systems.
"Subsequent model-ing of user satisfaction gave us some insight into why each system was more or less satisfactory; four variables accounted for 37% of the variance in user-satisfaction: task completion, task dura-tion, recognition accuracy, and mean system turn duration."
"However, when doing our analysis we were struck by the extent to which different aspects of the systems’ dialogue behavior weren’t captured by the core metrics."
"For example, the core met-rics logged the number and duration of system turns, but didn’t distinguish between turns used to request or present information, to give instruc- tions, or to indicate errors."
Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of character-izing dialogue behaviors[REF_CITE].
"Several research efforts have explored the use of dialogue act tag-ging schemes for tasks such as improving recog-nition performance[REF_CITE], identifying important parts of a dialogue[REF_CITE], and as a con-straint on nominal expression generati[REF_CITE]."
Thus we decided to explore the applica-tion of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems.
Section 2 describes the corpus.
Section 3 de-scribes the dialogue act tagging scheme we de-veloped and applied to the evaluation of COM - MUNICATOR dialogues.
"Section 4 first describes our results utilizing the standard logged metrics, and then describes results using the DATE met-rics."
Section 5 discusses future plans.
The corpus consists of 662 dialogues from nine different travel planning systems with the num-ber of dialogues per system ranging between 60 and 79.
The experimental design is described[REF_CITE].
"Each dialogue consists of a recording, a logfile consistent with the stan-dard, transcriptions and recordings of all user ut-terances, and the output of a web-based user sur-vey."
Metrics collected per call included:
"Dialogue Efficiency: Task Duration, System turns, User turns, Total Turns"
"Dialogue Quality: Word Accuracy, Response latency, Response latency variance"
Task Success: Exact Scenario Completion
"User Satisfaction: Sum of TTS performance, Task ease, User expertise, Expected behavior, Future use."
The objective metrics focus on measures that can be automatically logged or computed and a web survey was used to calculate User Satisfac-ti[REF_CITE].
"A ternary definition of task completion, Exact Scenario Completion (ESC) was annotated by hand for each call by an-notators at AT&amp;T."
"The ESC metric distinguishes between exact scenario completion (ESC), any scenario completion (ANY) and no scenario com-pletion (NOCOMP)."
This metric arose because some callers completed an itinerary other than the one assigned.
"This could have been due to users’ inattentiveness, e.g. users didn’t correct the system when it had misunderstood them."
"In this case, the system could be viewed as having done the best that it could with the information that it was given."
This would argue that task completion would be the sum of ESC and ANY.
"However, examination of the dialogue transcripts suggested that the ANY category sometimes arose as a ratio-nal reaction by the caller to repeated recognition error."
"Thus we decided to distinguish the cases where the user completed the assigned task, ver-sus completing some other task, versus the cases where they hung up the phone without completing any itinerary."
The hypothesis underlying the application of di-alogue act tagging to system evaluation is that a system’s dialogue behaviors have a strong ef-fect on the usability of a spoken dialogue sys-tem.
"However, each COMMUNICATOR system has a unique dialogue strategy and a unique way of achieving particular communicative goals."
"Thus, in order to explore this hypothesis, we needed a way of characterizing system dialogue behaviors that could be applied uniformly across the nine different communicator travel planning systems."
We developed a dialogue act tagging scheme for this purpose which we call DATE (Dialogue Act Tagging for Evaluation).
"In developing DATE, we believed that it was important to allow for multiple views of each dialogue act."
"This would allow us, for ex-ample, to investigate what part of the task an utterance contributes to separately from what speech act function it serves."
"Thus, a cen-tral aspect of DATE is that it makes distinc-tions within three orthogonal dimensions of ut-terance classification: (1) a SPEECH - ACT dimen-sion; (2) a TASK - SUBTASK dimension; and (3) a CONVERSATIONAL - DOMAIN dimension."
We be-lieve that these distinctions are important for us-ing such a scheme for evaluation.
Figure 1 shows a COMMUNICATOR dialogue with each system ut- terance classified on these three dimensions.
The tagset for each dimension are briefly described in the remainder of this section.
See[REF_CITE]for more detail.
"In DATE, the SPEECH - ACT dimension has ten cat-egories."
"We use familiar speech-act labels, such as OFFER , REQUEST - INFO , PRESENT - INFO , AC - KNOWLEDGE , and introduce new ones designed to help us capture generalizations about commu-nicative behavior in this domain, on this task, given the range of system and human behavior we see in the data."
"One new one, for example, is STATUS - REPORT ."
Examples of each speech-act type are in Figure 2.
The CONVERSATIONAL - DOMAIN dimension in-volves the domain of discourse that an utterance is about.
Each speech act can occur in any of three domains of discourse described below.
The ABOUT - TASK domain is necessary for evaluating a dialogue system’s ability to collab-orate with a speaker on achieving the task goal of making reservations for a specific trip.
"It supports metrics such as the amount of time/effort the sys-tem takes to complete a particular phase of mak-ing an airline reservation, and any ancillary ho-tel/car reservations."
The ABOUT - COMMUNICATION domain re-flects the system goal of managing the verbal channel and providing evidence of what has been understood[REF_CITE].
"Utterances of this type are frequent in human-computer dialogue, where they are moti-vated by the need to avoid potentially costly er-rors arising from imperfect speech recognition."
All implicit and explicit confirmations are about communication; See Figure 1 for examples.
The SITUATION - FRAME domain pertains to the goal of managing the culturally relevant framing expectations[REF_CITE].
The utterances in this domain are particularly relevant in human-computer dialogues because the users’ expecta-tions need to be defined during the course of the conversation.
"About frame utterances by the sys-tem attempt to help the user understand how to in-teract with the system, what it knows about, and what it can do."
Some examples are in Figure 1.
The TASK - SUBTASK dimension refers to a task model of the domain task that the system sup-ports and captures distinctions among dialogue acts that reflect the task structure. [Footnote_1]
1 This dimension elaborates of each speech-act type in other tagging schemes[REF_CITE].
The motiva-tion for this dimension is to derive metrics that quantify the effort expended on particular sub-tasks.
"This dimension distinguishes among 14 sub-tasks, some of which can also be grouped at a level below the top level task. [Footnote_2] , as described in Figure 3."
2 In[REF_CITE]we didn’t distinguish the price subtask from the itinerary presentation subtask.
"The TOP - LEVEL - TRIP task de-scribes the task which contains as its subtasks the ORIGIN , DESTINATION , DATE , TIME , AIRLINE , TRIP - TYPE , RETRIEVAL and ITINERARY tasks."
The GROUND task includes both the HOTEL and CAR subtasks.
Note that any subtask can involve multiple speech acts.
"For example, the DATE subtask can consist of acts requesting, or implicitly or explic-itly confirming the date."
"A similar example is pro-vided by the subtasks of CAR (rental) and HOTEL , which include dialogue acts requesting, confirm-ing or acknowledging arrangements to rent a car or book a hotel room on the same trip."
We implemented a dialogue act parser that clas-sifies each of the system utterances in each dia-logue in the COMMUNICATOR corpus.
"Because the systems used template-based generation and had only a limited number of ways of saying the same content, it was possible to achieve 100% ac-curacy with a parser that tags utterances automat-ically from a database of patterns and the corre-sponding relevant tags from each dimension."
A summarizer program then examined each di-alogue’s labels and summed the total effort ex-pended on each type of dialogue act over the dialogue or the percentage of a dialogue given over to a particular type of dialogue behavior.
These sums and percentages of effort were calcu-lated along the different dimensions of the tagging scheme as we explain in more detail below.
"We believed that the top level distinction be-tween different domains of action might be rel-evant so we calculated percentages of the to-tal dialogue expended in each conversational do-main, resulting in metrics of TaskP, FrameP and CommP (the percentage of the dialogue devoted to the task, the frame or the communication do-mains respectively)."
We were also interested in identifying differ-ences in effort expended on different subtasks.
The effort expended on each subtask is repre-sented by the sum of the length of the utterances contributing to that subtask.
"These are the met- rics: TripC, OrigC, DestC, DateC, TimeC, Air-lineC, RetrievalC, FlightinfoC, PriceC, GroundC, BookingC. See Figure 3."
We were particularly interested developing metrics related to differences in the system’s di-alogue strategies.
One difference that the DATE scheme can partially capture is differences in con-firmation strategy by summing the explicit and implicit confirms.
"This introduces two metrics ECon and ICon, which represent the total effort spent on these two types of confirmation."
Another strategy difference is in the types of about frame information that the systems pro-vide.
"The metric CINSTRUCT counts instances of instructions, CREQAMB counts descriptions provided of what the system knows about in the context of an ambiguity, and CNOINFO counts the system’s descriptions of what it doesn’t know about."
SITINFO counts dialogue initial descrip-tions of the system’s capabilities and instructions for how to interact with the system
"A final type of dialogue behavior that the scheme captures are apologies for misunderstand-ing (CREJECT), acknowledgements of user re-quests to start over (SOVER) and acknowledg-ments of user corrections of the system’s under-standing (ACOR)."
"We believe that it should be possible to use DATE to capture differences in initiative strate-gies, but currently only capture differences at the task level using the task metrics above."
"The TripC metric counts open ended questions about the user’s travel plans, whereas other subtasks typi-cally include very direct requests for information needed to complete a subtask."
"We also counted triples identifying dialogue acts used in specific situations, e.g. the utterance Great!"
"I am adding this flight to your itinerary is the speech act of acknowledge, in the about-task domain, contributing to the booking subtask."
This combination is the ACKBOOKING metric.
"We also keep track of metrics for dialogue acts of acknowledging a rental car booking or a hotel booking, and requesting, presenting or confirm-ing particular items of task information."
Below we describe dialogue act triples that are signifi-cant predictors of user satisfaction.
We initially examined differences in cumulative user satisfaction across the nine systems.
"An ANOVA for user satisfaction by Site ID using the modified Bonferroni statistic for multiple com-parisons showed that there were statistically sig-nificant differences across sites, and that there were four groups of performers with sites 3,2,1,4 in the top group (listed by average user satisfac-tion), sites 4,5,9,6 in a second group, and sites 8 and 7 defining a third and a fourth group."
See[REF_CITE]for more detail on cross-system comparisons.
"However, our primary goal was to achieve a better understanding of the role of qualitative as-pects of each system’s dialogue behavior."
We quantify the extent to which the dialogue act metrics improve our understanding by applying the PARADISE framework to develop a model of user satisfaction and then examining the extent to which the dialogue act metrics improve the model[REF_CITE].
Section 4.1 describes the PARADISE models developed using the core metrics and section 4.2 describes the models de-rived from adding in the DATE metrics.
We applied PARADISE to develop models of user satisfaction using the core metrics; the best model fit accounts for 37% of the variance in user sat-isfaction.
"The learned model is that User Sat-isfaction is the sum of Exact Scenario Comple-tion, Task Duration, System Turn Duration and Word Accuracy."
"Table 1 gives the details of the model, where the coefficient indicates both the magnitude and whether the metric is a positive or negative predictor of user satisfaction, and the P value indicates the significance of the metric in the model."
The finding that metrics of task completion and recognition performance are significant predic-tors duplicates results from other experiments ap-plying PARADISE[REF_CITE].
The fact that task duration is also a significant predictor may indicate larger differences in task duration in this corpus than in previous studies.
Note that the PARADISE model indicates that system turn duration is positively correlated with user satisfaction.
We believed it plausible that this was due to the fact that flight presentation utter-ances are longer than other system turns.
Thus this metric simply captures whether or not the sys-tem got enough information to present some po-tential flight itineraries to the user.
We investigate this hypothesis further below.
"Next, we add in the dialogue act metrics extracted by our dialogue parser, and retrain our models of user satisfaction."
"We find that many of the dia-logue act metrics are significant predictors of user satisfaction, and that the model fit for user sat-isfaction increases from 37% to 42%."
The dia-logue act metrics which are significant predictors of user satisfaction are detailed in Table 2.
"When we examine this model, we note that sev-eral of the significant dialogue act metrics are cal-culated along the task-subtask dimension, namely TripC, BookingC and PriceC. One interpretation of these metrics are that they are acting as land-marks in the dialogue for having achieved a par-ticular set of subtasks."
The TripC metric can be interpreted this way because it includes open ended questions about the user’s travel plans both at the beginning of the dialogue and also after one itinerary has been planned.
Other signif-icant metrics can also be interpreted this way; for example the ReqDate metric counts utterances such as Could you tell me what date you wanna travel? which are typically only produced after the origin and the destination have been under-stood.
"The ReqTripType metric counts utterances such as From Boston, are you returning to Dal-las? which are only asked after all the first infor-mation for the first leg of the trip have been ac-quired, and in some cases, after this information has been confirmed."
The AckRental metric has a similar potential interpretation; the car rental task isn’t attempted until after the flight itinerary has been accepted by the caller.
"However, the predic-tors for the models already include a ternary exact scenario completion metric (ESC) which speci-fies whether any task was achieved or not, and whether the exact task that the user was attempt-ing to accomplish was achieved."
The fact that the addition of these dialogue metrics improves the fit of the user satisfaction model suggests that per-haps a finer grained distinction on how many of the subtasks of a dialogue were completed is re-lated to user satisfaction.
"This makes sense; a user who the system hung up on immediately should be less satisfied than one who never could get the system to understand his destination, and both of these should be less satisfied than a user who was able to communicate a complete travel plan but still did not complete the task."
Other support for the task completion related nature of some of the significant metrics is that the coefficient for ESC is smaller in the model in Table 2 than in the model in Table 1.
Note also that the coefficient for Task Duration is much larger.
"If some of the dialogue act metrics that are significant predictors are mainly so because they indicate the successful accomplishment of partic-ular subtasks, then both of these changes would make sense."
"Task Duration can be a greater nega-tive predictor of user satisfaction, only when it is counteracted by the positive coefficients for sub-task completion."
The TripC and the PriceC metrics also have other interpretations.
The positive contribution of the TripC metric to user satisfaction could arise from a user’s positive response to systems with open-ended initial greetings which give the user the initiative.
"The positive contribution of the PriceC metric might indicate the users’ positive response to getting price information, since not all systems provided price information."
"As mentioned above, our goal was to de-velop metrics that captured differences in dia-logue strategies."
The positive coefficient of the Econ metric appears to indicate that an explicit confirmation strategy overall leads to greater user satisfaction than an implicit confirmation strategy.
"This result is interesting, although it is unclear how general it is."
The systems that used an ex-plicit confirmation strategy did not use it to con-firm each item of information; rather the strategy seemed to be to acquire enough information to go to the database and then confirm all of the param-eters before accessing the database.
The other use of explicit confirms was when a system believed that it had repeatedly misunderstood the user.
We also explored the hypothesis that the rea-son that system turn duration was a predictor of user satisfaction is that longer turns were used to present flight information.
"We removed sys-tem turn duration from the model, to determine whether FlightInfoC would become a significant predictor."
However the model fit decreased and FlightInfoC was not a significant predictor.
Thus it is unclear to us why longer system turn dura-tions are a significant positive predictor of user satisfaction.
We showed above that the addition of dialogue act metrics improves the fit of models of user satis-faction from 37% to 42%.
Many of the significant dialogue act metrics can be viewed as landmarks in the dialogue for having achieved particular sub-tasks.
"These results suggest that a careful defi-nition of transaction success, based on automatic analysis of events in a dialogue, such as acknowl-edging a booking, might serve as a substitute for the hand-labelling of task completion."
In current work we are exploring the use of tree models and boosting for modeling user satisfac-tion.
Tree models using dialogue act metrics can achieve model fits as high as 48% reduction in error.
"However, we need to test both these mod-els and the linear PARADISE models on unseen data."
"Furthermore, we intend to explore methods for deriving additional metrics from dialogue act tags."
"In particular, it is possible that sequential or structural metrics based on particular sequences or configurations of dialogue acts might capture differences in dialogue strategies."
We began a second data collection of dialogues with COMMUNICATOR travel systems[REF_CITE].
"In this data collection, the subject pool will use the systems to plan real trips that they intend to take."
"As part of this data collection, we hope to develop additional metrics related to the qual-ity of the dialogue, how much initiative the user can take, and the quality of the solution that the system presents to the user."
This work was supported under DARPA[REF_CITE]99 3 0003 to AT&amp;T Labs Research.
"Thanks to the evaluation committee members: J. Aberdeen, E. Bratt, J. Garofolo, L. Hirschman, A. Le, S. Narayanan, K. Papineni, B. Pellom, A. Potamianos, A. Rudnicky, G. Sanders, S. Sen-eff, and D. Stallard who contributed to 2000 COMMUNICATOR data collection."
We present a syntax-based statistical translation model.
Our model trans-forms a source-language parse tree into a target-language string by apply-ing stochastic operations at each node.
These operations capture linguistic dif-ferences such as word order and case marking.
Model parameters are esti-mated in polynomial time using an EM algorithm.
The model produces word alignments that are better than those produced by IBM Model 5.
A statistical translation model (TM) is a mathe-matical model in which the process of human-language translation is statistically modeled.
Model parameters are automatically estimated us-ing a corpus of translation pairs.
"TMs have been used for statistical machine translati[REF_CITE], word alignment of a translation cor-pus[REF_CITE], multilingual document re-trieval[REF_CITE], automatic dictionary constructi[REF_CITE], and data preparation for word sense disambiguation programs[REF_CITE]."
Developing a bet-ter TM is a fundamental issue for those applica-tions.
Researchers at IBM first described such a sta-tistical TM[REF_CITE].
Their mod-els are based on a string-to-string noisy channel model.
The channel converts a sequence of words in one language (such as English) into another (such as French).
"The channel operations are movements, duplications, and translations, ap-plied to each word independently."
"The movement is conditioned only on word classes and positions in the string, and the duplication and translation are conditioned only on the word identity."
Math-ematical details are fully described[REF_CITE].
One criticism of the IBM-style TM is that it does not model structural or syntactic aspects of the language.
The TM was only demonstrated for a structurally similar language pair (English and French).
It has been suspected that a language pair with very different word order such as En-glish and Japanese would not be modeled well by these TMs.
"To incorporate structural aspects of the lan-guage, our channel model accepts a parse tree as an input, i.e., the input sentence is preprocessed by a syntactic parser."
The channel performs oper-ations on each node of the parse tree.
"The oper-ations are reordering child nodes, inserting extra words at each node, and translating leaf words."
Figure 1 shows the overview of the operations of our model.
"Note that the output of our model is a string, not a parse tree."
"Therefore, parsing is only needed on the channel input side."
"The reorder operation is intended to model translation between languages with different word orders, such as SVO-languages (English or Chi-nese) and SOV-languages (Japanese or Turkish)."
The word-insertion operation is intended to cap-ture linguistic differences in specifying syntactic cases.
"E.g., English and French use structural po-sition to specify case, while Japanese and Korean use case-marker particles."
"Both also tried to incorporate structural as-pects of the language, however, neither handles nested structures."
"The way we handle syntactic parse trees is in-spired by their work, although their approach is not to model the translation process, but to formalize a model that generates two languages at the same time."
Our channel operations are also similar to the mechanism in Twisted Pair Grammar[REF_CITE]used in their knowledge-based system.
"Following[REF_CITE]and the other literature in TM, this paper only focuses the de-tails of TM."
"Applications of our TM, such as ma-chine translation or dictionary construction, will be described in a separate paper."
Section 2 de-scribes our model in detail.
Section 3 shows ex-perimental results.
"We conclude with Section 4, followed by an Appendix describing the training algorithm in more detail."
We first introduce our translation model with an example.
Section 2.2 will describe the model more formally.
We assume that an English parse tree is fed into a noisy channel and that it is trans-lated to a Japanese sentence. 1
Figure 1 shows how the channel works.
"First, child nodes on each internal node are stochas-tically reordered."
A node with / children has 10 possible reorderings.
The probability of tak- / ing a specific reordering is given by the model’s r-table.
Sample model parameters are shown in Table 1.
We assume that only the sequence of child node labels influences the reordering.
"In Figure 1, the top VB node has a child sequence PRP-VB1-VB2."
The probability of reordering it into PRP-VB2-VB[Footnote_1] is 0.723 (the second row in the r-table in Table 1).
1 The parse tree is flattened to work well with the model. See Section 3.1 for details.
"We also reorder VB-TO into TO-VB, and TO-NN into NN-TO, so there-fore the probability of the second tree in Figure 1 is 2436587:&lt;9 ;=24365?A&gt; @&lt;[REF_CITE]DC; :@:9[REF_CITE]HACI&gt; &gt; ."
"Next, an extra word is stochastically inserted at each node."
"A word can be inserted either to the left of the node, to the right of the node, or nowhere."
"Here, we instead decide the position on the basis of the nodes of the in-put parse tree."
The insertion probability is deter-mined by the n-table.
"For simplicity, we split the n-table into two: a table for insert positions and a table for words to be inserted (Table 1)."
The node’s label and its parent’s label are used to in-dex the table for insert positions.
"For example, the PRP node in Figure 1 has parent VB, thus parent=VB ¡ node=PRP ¢ is the conditioning in-dex."
"Using this label pair captures, for example, the regularity of inserting case-marker particles."
"When we decide which word to insert, no condi-tioning variable is used."
"That is, a function word like ga is just as likely to be inserted in one place as any other."
"In Figure 1, we inserted four words (ha, no, ga and desu) to create the third tree."
"The top VB node, two TO nodes, and the NN node inserted nothing."
"Therefore, the probability of obtaining ££ 243D¤243D7:::¥¥:7¦7[REF_CITE]¬2;;§243D74¨:2=:@ª©«2A5:©¯; ;[REF_CITE]D7::¥ :7¦¥F§243¬2ª;[REF_CITE]I2ª; @I­©«&gt; @&lt;;[REF_CITE]D; 243D7@82::¥:2°7¦[REF_CITE]; ;®243¬2ª¤::2±E7ª©«; the third £ tree given the £ second tree is 3.498e-9."
"Finally, we apply the translate operation to each leaf."
We assume that this operation is depen-dent only on the word itself and that no context is consulted. [Footnote_2]
"2 When a TM is used in machine translation, the TM’s role is to provide a list of possible translations, and a lan-guage model addresses the context. See[REF_CITE]."
The model’s t-table specifies the probability for all cases.
Suppose we obtained the translations shown in the fourth tree of Figure 1.
The probability of the translate operation here is 243D:@ :¥ 7²;[REF_CITE]D@82:2°=; 243¬2ª9:C&lt;=; 243D9::9 &lt;9 A¨83¬2; ::2 2³[REF_CITE]¬2´¨µ2ªC .
"The total probability of the reorder, insert and translate operations in this example is 243H&gt;ACI1&gt; ; 3.498e-9 ;­243¬2´¨µ2ªC1E 1.828e-11."
Note that there are many other combinations of such operations that yield the same Japanese sentence.
"Therefore, the probability of the Japanese sentence given the English parse tree is the sum of all these probabil-ities."
"We actually obtained the probability tables (Ta-ble 1) from a corpus of about two thousand pairs of English parse trees and Japanese sentences, completely automatically."
Section 2.3 and Ap-pendix 4 describe the training algorithm.
This section formally describes our translation model.
"To make this paper comparable[REF_CITE], we use English-French notation in this section."
We assume that an English parse tree ¶ is transformed into a French sentence · .
"Let ¸ª¹ ¡ ¸Bº the 3µ3µ3= English ¡ ¸B» , andparselet thetreeoutput ¶ consist of nodes consist of French words ¼ ¹ ¡½¼ º ¡µ3µ3µ3 French ?¡½¼I¾ . sentence Three random variables, ¿ , À , and Á are chan-nel operations applied to each node."
Insertion ¿ is an operation that inserts a French word just be-fore or after the node.
"The insertion can be none, left, or right."
Also it decides what French word to insert.
Reorder À is an operation that changes the order of the children of the node.
"If a node has three children, e.g., there are 9Â0FEÃ¤ ways to reorder them."
This operation applies only to non-terminal nodes in the tree.
Translation Á is an operation that translates a terminal English leaf word into a French word.
This operation applies only to terminal nodes.
Note that an English word can be translated into a French ÇÆ ¡ÉÈÊ¡ÌËÍ¢
The notation ÅÄ E of values of ¿Î¡ÌÀÏ¡½ÁÐ¢ .
ÄBÑ1E stands ÇÆ ÑÒ¡ÉÈAÑÓ¡ÌËÔÑÇ¢ for a set is a set ¸Ñ . ofAndvalues ÕÖE×Ä of random ¹ ¡ÌÄ º ¡µ3µ3µ3? variables ¡ÌÄ » is theassociatedset of allwithran-dom :¸ ¹ ¡ ?¸ º variables ¡µ3µ3µ3B¡ =¸ » . associated with a parse tree ¶ØE
The probability of getting a French sentence · given an English parse tree ¶ is 4 ä â ä¬å§æçæçè:é Ù â ÜÝ4Þ ÙÛÚ=Ü ÝÂÞàß â ã á P P Str £ £ where Str Õ ¶ê ©É© is the sequence of leaf words of a tree transformed by Õ from ¶ .
The probability of having a particular set of values of random variables in a parse tree is
Ù â Ü ÝÂÞëß ôö÷ ÙíìÔîÒïðì½ñ½÷ ïÒòÓòÒòÉïóìÌô­Ü õµîÉïÇõ§÷ñ½ïÒòÓòÉòÒïÇõ½ôIÞ P P ß èî
Ùíì ÜìÔîÉïóì½ñ§ïÒòÒòÒòÒïÇì ø îÉïùõ=îÓïÇõ§ñ§ïÒòÒòÒòÒïùõ§ô?Þúò P
This is an exact equation.
"Then, we assume that a transform operation is independent from other transform operations, and the random variables of each node are determined only by the node itself."
"So, we obtain"
Ù â Ü ÝÂÞàß ôö÷Ùíì î ïóì ñ÷ ïÒòÓ÷òÒòÒïóì ô
Üõ î ïÇõ ñ ïÒòÓòÒòÒïÇõ ô
Þ P P ß èî
Ùíì Ü õ
The random variables ?Ä &lt;Ñ E ÇÆ ÑÓ¡ÉÈ­Ñú¡ÌËÔÑð¢ are as-sumed to be independent of each other.
We also assume that they are ¸ dependent on particular fea-tures of ÷ ÷ the node ÷Ñ .
"Then ÷ ÷ , ÷"
Ùíü ÷÷ Ü õ
Þ ÷Ùçý÷ Ü õ Þ ÷÷ ÙÛþ ÷
Ü õ÷÷ ÷ Þ Ùíü ÷ ïùý ÷ ïÇþ Üõ ÷ Þ ÷
Ùíì Ü õ Þûßßß ÙíüÙíü ÜÜÿÿ ÙÛõÙÛõ ÞÇÞÞÇÞ BÙçýÙçý ÜÜ
ÙÛõÙÛõ ÞÇÞÞÇÞ  ÒÙÛþÙÛþ ÷ ÷
"Ü Ü ÙÛõÙÛõ ÷ ÷ ÞÇÞÞÇÞ P P P P P ß P P P ¿ ,where À , and Á , , respectively, and are.theForrelevantexamplefeatures, we sawto that the parent node label and the node label were used for , and the syntactic category sequence of children was used for ."
"The last line in the above formula introduces a change in notation, meaning that £ÇÆ those © £ probabilities © , and £Ë are © the, wheremodel,pa-,, È rameters and are the possible values for , , and , respectively."
"In summary, the probability of getting a French sentence · given an English â and P Ë model, and these are the probabilities we want to estimate from a training corpus."
"To estimate the model parameters, we use the EM algorithm[REF_CITE]."
The algorithm iteratively updates the model parameters to max-imize the likelihood of the training corpus.
"First, the model parameters are initialized."
"We used a uniform distribution, but it can be a distribution taken from other models."
"For each iteration, the number of events are counted and weighted by the probabilities of the events."
The probabilities of events are calculated from the current model pa-rameters.
"The model parameters are re-estimated based on the counts, and used for the next itera-tion."
"In our case, an event Æ is a pair of a value of a random variable (such as , È , or Ë ) and a feature value (such as , , or )."
A separate counter is used for each event.
"Therefore £ÇÆ ¡ © , £ we need the £ same , ÈÍ¡ © , and Ë4¡ © , number of counters, as £ÇÆ the © number £ © £Ë of entries in © .the probability tables,, È , and"
The training procedure is the following: íÙ Âü Ü Þ ?
"ÓÙÛþ4Ü Þ 1. Initialize all probability tables: , , and ."
"ÝÍï Ú  §Ùíü?ï Þ §Ùçý:ï Þ ®ÙÛþ?ï Þ 2. Reset all counters: , , and . â ÚêßÙ â Ü ÝÂÞ Ù â"
ÙíÝÂÞÇÞâ4ã ä â ä¬å§æçæçè:é Ù â Ü ÝÂÞ 3.
"For each pair in the training corpus,"
"For all , such that Str ,"
"Let cnt = P PStr very expensive, since there Æ are . possi-ble combinations, where Æ and È are the num-ber of possible values for Æ and È , respectively ( Ë is uniquely decided when and È are given for a particular ¶¯¡Ì·I¢ )."
"Appendix describes an efficient implementation that estimates the probability in polynomial time. [Footnote_3] With this efficient implemen-tation, it took about 50 minutes per iteration on our corpus (about two thousand pairs of English parse trees and Japanese sentences."
"3 Note that the algorithm performs full EM counting, whereas the IBM models only permit counting over a sub-set of possible alignments."
See the next section).
"To experiment, we trained our model on a small English-Japanese corpus."
"To evaluate perfor-mance, we examined alignments produced by the learned model."
"For comparison, we also trained IBM Model 5 on the same corpus."
These sentences were mostly short ones.
The average sentence length was 6.9 for English and 9.7 for Japanese.
"However, many rare words were used, which made the task difficult."
"The vocabulary size was 3463 tokens for English, and 3983 tokens for Japanese, with 2029 tokens[REF_CITE]tokens for Japanese occurring only once in the corpus."
Brill’s part-of-speech (POS) tagger[REF_CITE]and Collins’ parser[REF_CITE]were used to obtain parse trees for the English side of the corpus.
The output of Collins’ parser was modified in the following way.
"First, to reduce the number of parameters in the model, each node was re-labelled with the POS of the node’s head word, and some POS labels were collapsed."
"For example, labels for different verb endings (such as VBD for -ed and VBG for -ing) were changed to the same label VB."
"There were then 30 differ-ent node labels, and 474 unique child label se-quences."
"Second, a subtree was flattened if the node’s head-word was the same as the parent’s head-word."
"For example, (NN1 (VB NN2)) was flat-tened to (NN1 VB NN2) if the VB was a head word for both NN1 and NN2."
This flattening was motivated by various word orders in different lan-guages.
"An English SVO structure is translated into SOV in Japanese, or into VSO in Arabic."
"These differences are easily modeled by the flat-tened subtree (NN1 VB NN2), rather than (NN1 (VB NN2))."
"IBM Model 5 was se-quentially bootstrapped with Model 1, an HMM Model, and Model [Footnote_3][REF_CITE]."
"3 Note that the algorithm performs full EM counting, whereas the IBM models only permit counting over a sub-set of possible alignments."
Each preceding model and the final Model 5 were trained with five iterations (total 20 iterations).
The training procedure resulted in the tables of es-timated model parameters.
Table 1 in Section 2.1 shows part of those parameters obtained by the training above.
"To evaluate performance, we let the models generate the most probable alignment of the train-ing corpus (called the Viterbi alignment)."
The alignment shows how the learned model induces the internal structure of the training data.
Figure 2 shows alignments produced by our model and IBM Model 5.
Darker lines indicates that the particular alignment link was judged cor-rect by humans.
"Three humans were asked to rate each alignment as okay (1.0 point), not sure (0.5 point), or wrong (0 point)."
The darkness of the lines in the figure reflects the human score.
We obtained the average score of the first 50 sentence pairs in the corpus.
We also counted the number of perfectly aligned sentence pairs in the 50 pairs.
Perfect means that all alignments in a sentence pair were judged okay by all the human judges.
Our model got a better result compared to IBM Model 5.
Note that there were no perfect align-ments from the IBM Model.
"Errors by the IBM Model were spread out over the whole set, while our errors were localized to some sentences."
We expect that our model will therefore be easier to improve.
"Also, localized errors are good if the TM is used for corpus preparation or filtering."
We also measured training perplexity of the models.
"The perplexity of our model was 15.79, and that of IBM Model 5 was 9.84."
"For reference, the perplexity after 5 iterations of Model 1 was 24.01."
Perplexity values roughly indicate the pre-dictive power of the model.
"Generally, lower per-plexity means a better model, but it might cause over-fitting to a training data."
"Since the IBM Model usually requires millions of training sen-tences, the lower perplexity value for the IBM Model is likely due to over-fitting."
We have presented a syntax-based translation model that statistically models the translation pro-cess from an English parse tree into a foreign- language sentence.
The model can make use of syntactic information and performs better for lan-guage pairs with different word orders and case marking schema.
"We conducted a small-scale ex-periment to compare the performance with IBM Model 5, and got better alignment results."
This appendix describes an efficient implemen-tation of the EM algorithm for our translation model.
This implementation uses a graph struc-ture for a pair ¶ ¡Ì·I¢ .
A graph node is either a major-node or a subnode.
A major-node shows a pairing of a subtree of ¶ and a substring ÇÆ of · .
A subnode shows a selection of a value ¡ÉÈÍ¡ÌËÍ¢ for the subtree-substring ¼ 2 3µ3µ3Ô¼  pair (Figure ¹ 87 be a3substring). of · Let 1 · 20 E from the word ¼ 2 with length 065 9 .
"Note this notation is ¸Ñ differentis a subtreefrom ¶ (Brown etthealnode., 1993 ¸Ñ .).WeA assumesubtree that a subtree ¸: of ¹ is below ¶£ ¸ ."
Ñ ¡Ì· ;20 © is a pair of a subtree ¸
A major-node : · 20 .
"The root of the graph is and ¹ ¡Ì· 1 ¹ &lt; a © ,substringwhere = is the length"
Æ · of .
Each Ç£ Æ ?&gt; ¸ major-
"ÒÑ ¡Ì· 20 © : node connects to several -subnodes Æ : , showing which £ ¸ ÑÒ¡Ì· 20 © value of £ÇÆ ?&gt; is ¸ ÑÒ¡Ì· 20 © selected."
The arc £ÇÆ between ¸
Ñð©Æ : and : has weight
A -subnode : £ÇÆ ? £ &gt; ¸ Ñ ¡Ì· 120 © P . node with weight P Ë ¸
Ñó© if ¸ connects Ñ is a terminalto a final-node in ¶ .
"If ¸Ñ is a non-terminal node, a £ Æ½Æ connects to several È -subnodes :"
È &gt; -subnode ¡ ¸
"ÑÉ¡Ì· ;20 © , showing a selection £ ¸ of a value È ."
The weight of the arc is P È Ñú© . : £ @ A &gt; ÈÊÈ ¡
Æ ¡ ¸
ÑÉ¡Ì· ;20 © -subnode is then connected to @ -subnodes .
"The partition variable, @ , shows a particular way of £ partitioning &gt; Æ ¸ · 20"
"A @ -subnode : @ ÈÍ¡ ¡ ÑÉ¡Ì· 20 © is.then connected to major-nodes which correspond to the ÇÆ children of ¸Ñ and the substring of · 120 , decided by ¡ÉÈÍ¡ /@ ¢ ."
A major-node can be connected from different @ -subnodes.
The arc weights between È -subnodes and major-nodes are always 1.0. £Õ
This graph structure makes it easy to obtain P ¶ £ © for a particular Õ and Õ M Str 4 Õ 4ON 7P7PQSR P Õ ¶ê© .
"A trace starting from the graph root Æ , selecting one of the arcs from major-nodes, -subnodes, and È -subnodes, and all the arcs from @ -subnodes, corresponds to a particular Õ , and the product £ of the weight on the trace corresponds to P Õ ¶ê© ."
"Note that a trace forms a tree, making branches at the @ -subnodes."
"We define an alpha probability and a beta prob-ability for each major-node, in analogy with the measures used in the inside-outside algorithm for probabilistic context free grammars[REF_CITE]."
The alpha probability (outside probability) is a path probability from the graph root to the node and the side branches of the node.
The beta proba-bility (inside probability) is a path probability be-low the node.
Figure 4 shows formulae for alpha-beta probabilities.
"From these definitions,"
"Õ M Str 4 Õ 4ON 7P7TQSR £ÇÆ P ¡£ Õ © ¶ © £ EÈÊ¡ © ¡Ì· © £ Ë4¡ © VU £ ¸ª¹ ¹ &lt; . pair ¶ ¡Ì·I¢ are also in, the figure, and."
"Those formulaefor each The counts replace the step 3 (in Section 2.3) for each training pair, and these counts are used in the step 4."
The graph structure £ ¸:¹ ¡Ì· ; ¹ &lt; © is generated by expanding the root node : .
"The beta probability for each node is first calculated bottom-up, then the alpha probability for each node is calculated top-down."
"Once the alpha and beta probabilities for each node are obtained, the counts are calculated as above and used for updating the parameters."
"In this paper, a new language model, the Multi-Class Composite N-gram, is pro-posed to avoid a data sparseness prob-lem for spoken language in that it is difficult to collect training data."
"The Multi-Class Composite N-gram main-tains an accurate word prediction ca-pability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi-Classes."
"In the Multi-Class, the statisti-cal connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent the positional attributes."
"Fur-thermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams."
"In experiments, the Multi-Class Composite N-grams re-sult in 9.5% lower perplexity and a 16% lower word error rate in speech recogni-tion with a 40% smaller parameter size than conventional word 3-grams."
Word N-grams have been widely used as a sta-tistical language model for language processing.
Word N-grams are models that give the transition probability of the next word from the previous N 1 word sequence based on a statistical analy-sis of the huge text corpus.
"Though word N-grams are more effective and flexible than rule-based grammatical constraints in many cases, their per-formance strongly depends on the size of training data, since they are statistical models."
"In word N-grams, the accuracy of the word prediction capability will increase according to the number of the order N, but also the num-ber of word transition combinations will exponen-tially increase."
"Moreover, the size of training data for reliable transition probability values will also dramatically increase."
This is a critical problem for spoken language in that it is difficult to col-lect training data sufficient enough for a reliable model.
"As a solution to this problem, class N-grams are proposed."
"In class N-grams, multiple words are mapped to one word class, and the transition probabilities from word to word are approximated to the proba-bilities from word class to word class."
The perfor-mance and model size of class N-grams strongly depend on the definition of word classes.
"In fact, the performance of class N-grams based on the part-of-speech (POS) word class is usually quite a bit lower than that of word N-grams."
"Based on this fact, effective word class definitions are re-quired for high performance in class N-grams."
"In this paper, the Multi-Class assignment is proposed for effective word class definitions."
"The word class is used to represent word connectiv-ity, i.e. which words will appear in a neigh-boring position with what probability."
"In Multi-Class assignment, the word connectivity in each position of the N-grams is regarded as a differ-ent attribute, and multiple classes corresponding to each attribute are assigned to each word."
"For the word clustering of each Multi-Class for each word, a method is used in which word classes are formed automatically and statistically from a cor-pus, not using a priori knowledge as POS infor-mation."
"Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are ex-tended to Multi-Class Composite N-grams."
Word N-grams are models that statistically give the transition probability of the next word from the previous N 1 word sequence.
This transition probability is given in the next formula. p ( w i j w i N +1 ;:::;w i 2 ;w i 1 ) (1)
"In word N-grams, accurate word prediction can be expected, since a word dependent, unique connec-tivity from word to word can be represented."
"On the other hand, the number of estimated param-eters, i.e., the number of combinations of word transitions, is V N in vocabulary V ."
"As V N will exponentially increase according to N , reliable estimations of each word transition probability are difficult under a large N ."
Class N-grams are proposed to resolve the problem that a huge number of parameters is re-quired in word N-grams.
"In class N-grams, the transition probability of the next word from the previous N 1 word sequence is given in the next formula. p ( c i j c i N +1 ;:::;c i 2 ;c i 1 ) p ( w i j c i ) (2)"
"Where, c i represents the word class to which the word w i belongs."
"In class N-grams with C classes, the number of estimated parameters is decreased from V N to C N ."
"However, accuracy of the word predic-tion capability will be lower than that of word N-grams with a sufficient size of training data, since the representation capability of the word depen-dent, unique connectivity attribute will be lost for the approximation base word class."
"In class N-grams, word classes are used to repre-sent the connectivity between words."
"In the con-ventional word class definition, word connectiv-ity for which words follow and that for which word precedes are treated as the same neighbor-ing characteristics without distinction."
"Therefore, only the words that have the same word connec-tivity for the following words and the preceding word belong to the same word class, and this word class definition cannot represent the word connec-tivity attribute efficiently."
Take ”a” and ”an” as an example.
"Both are classified by POS as an Indef-inite Article, and are assigned to the same word class."
"In this case, information about the differ-ence with the following word connectivity will be lost."
"On the other hand, a different class assign-ment for both words will cause the information about the community in the preceding word con-nectivity to be lost."
This directional distinction is quite crucial for languages with reflection such as French and Japanese.
"As in the previous example of ”a” and ”an”, fol-lowing and preceding word connectivity are not always the same."
Let’s consider the case of dif-ferent connectivity for the words that precede and follow.
Multiple word classes are assigned to each word to represent the following and preced-ing word connectivity.
"As the connectivity of the word preceding ”a” and ”an” is the same, it is ef-ficient to assign them to the same word class to represent the preceding word connectivity, if as-signing different word classes to represent the fol-lowing word connectivity at the same time."
"To apply these word class definitions to formula (2), the next formula is given. p ( c ti j c fNi N +11 ; :::; c fi 22 ; c fi 11 ) p ( w i j c ti ) (3)"
"In the above formula, c ti represents the word class in the target position to which the word w i be-longs, and c fNi represents the word class in the N-th position in a conditional word sequence."
"We call this multiple word class definition, a Multi-Class."
"Similarly, we call class N-grams based on the Multi-Class, Multi-Class N-grams[REF_CITE]."
"For word clustering in class N-grams, POS in-formation is sometimes used."
"Though POS in-formation can be used for words that do not ap-pear in the corpus, this is not always an optimal word classification for N-grams."
The POS in-formation does not accurately represent the sta-tistical word connectivity characteristics.
Better word-clustering is to be considered based on word connectivity by the reflection neighboring charac-teristics in the corpus.
"In this paper, vectors are used to represent word neighboring characteris-tics."
The elements of the vectors are forward or backward word 2-gram probabilities to the clus-tering target word after being smoothed.
And we consider that word pairs that have a small distance between vectors also have similar word neighbor-ing characteristics[REF_CITE].
"In this method, the same vector is assigned to words that do not appear in the cor-pus, and the same word cluster will be assigned to these words."
"To avoid excessively rough cluster-ing over different POS, we cluster the words un-der the condition that only words with the same POS can belong to the same cluster."
Parts-of-speech that have the same connectivity in each Multi-Class are merged.
"For example, if differ-ent parts-of-speeche are assigned to ”a” and ”an”, these parts-of-speeche are regarded as the same for the preceding word cluster."
Word clustering is thus performed in the following manner. 1. Assign one unique class per word.s. 2. Assign a vector to each class or to each word X .
"This represents the word connectivity at-tribute. v t ( x ) = [ p t ( w 1 j x ) ; p t ( w 2 j x ) ; :::; p t ( w N j x )] (4) v f ( x ) = [ p f ( w 1 j x ) ; p f ( w 2 j x ) ; :::; p f ( w N j x )] (5) Where, v t ( x ) represents the preceding word connectivity, v f ( x ) represents the following word connectivity, and p t is the value of the probability of the succeeding class-word 2-gram or word 2-gram, while p f is the same for the preceding one. 3. Merge the two classes."
"We choose classes whose dispersion weighted with the 1-gram probability results in the lowest rise, and U new = X ( merge these two classes: p ( w ) D ( v ( c new ( w )) ; v ( w )))"
U old = X w ( (6) p ( w ) D ( v ( c old ( w )) ; v ( w ))) w (7) where we merge the classes whose merge cost U new U old is the lowest.
"D ( v c ;v w ) represents the square of the Euclidean dis-tance between vector v c and v w , c old repre-sents the classes before merging, and c new represents the classes after merging. 4."
Repeat step 2 until the number of classes is reduced to the desired number.
"To apply the multiple clustering for 2-grams to 3-grams, the clustering target in the conditional part is extended to a word pair from the single word in 2-grams."
"Number of clustering targets in the preceding class increases to V 2 from V in 2-grams, and the length of the vector in the succeed-ing class also increase to V 2 ."
"Therefore, efficient word clustering is needed to keep the reliability of 3-grams after the clustering and a reasonable calculation cost."
"To avoid losing the reliability caused by the data sparseness of the word pair in the history of 3-grams, approximation is employed using distance-2 2-grams."
The authority of this ap-proximation is based on a report that the asso-ciation of word 2-grams and distance-2 2-grams based on the maximum entropy method gives a good approximation of word 3-grams[REF_CITE].
The vector for clustering is given in the next equation. v f 2 ( x ) = [ p f 2 ( w 1 j x ) ; p f 2 ( w 2 j x ) ; :::; p f 2 ( w N j x )] (8)
"Where, p f 2 ( y j x ) represents the distance-2 2-gram value from word x to word y ."
And the POS con-straints for clustering are the same as in the clus-tering for preceding words.
Introducing Variable Length Word Sequences Let’s consider the condition such that only word sequence ( A;B;C ) has sufficient frequency in sequence ( X; A; B; C; D ) .
"In this case, the value of word 2-gram p ( B j A ) can be used as a reli-able value for the estimation of word B , as the frequency of sequence ( A; B ) is sufficient."
The value of word 3-gram p ( C j A;B ) can be used for the estimation of word C for the same rea-son.
"For the estimation of words A and D , it is reasonable to use the value of the class 2-gram, since the value of the word N-gram is unreli-able (note that the frequency of word sequences ( X; A ) and ( C; D ) is insufficient)."
"Based on this idea, the transition probability of word sequence ( A; B; C; D ) from word X is given in the next equation in the Multi-Class 2-gram."
P = p ( c t ( A ) j c f ( X )) p ( A j c t ( A ))) p ( B j A ) p ( C j A; B ) p ( c t ( D ) j c f ( C )) p ( D j c t ( D )) (9)
"When word succession A + B + C is introduced as a variable length word sequence ( A; B; C ) , equa-tion (9) can be changed exactly to the next equa-ti[REF_CITE]."
P = p ( c t ( A ) j c f ( X )) p ( A + B + C j c t ( A )) p ( c t ( D ) j c f ( C )) p ( D j c t ( D )) (10)
"Here, we find the following properties."
The pre-ceding word connectivity of word succession A + B +
"C is the same as the connectivity of word A , the first word of A + B + C ."
The following con-nectivity is the same as the last word C .
"In these assignments, no new cluster is required."
But con-ventional class N-grams require a new cluster for the new word succession. c t ( A + B + C ) = c t ( A ) (11) c f ( A + B + C ) = c f ( C ) (12)
"Applying these relations to equation (10), the next equation is obtained."
P = p ( c t ( A + B + C ) j c f ( X )) p ( A + B + C j c t ( A + B + C )) p ( c t ( D ) j c f ( A + B + C )) p ( D j c t ( D )) (13)
"We call Multi-Class Composite 2-grams that are created by par-tially introducing higher order word N-grams by word succession, Multi-Class 2-grams."
"In addi-tion, equation (13) shows that number of param-eters will not be increased so match when fre-quent word successions are added to the word en-try."
Only a 1-gram of word succession A + B + C should be added to the conventional N-gram pa-rameters.
"Multi-Class Composite 2-grams are created in the following manner. 1. Assign a Multi-Class 2-gram, for state ini-tialization. 2. Find a word pair whose frequency is above the threshold. 3. Create a new word succession entry for the frequent word pair and add it to a lexicon."
"The following connectivity class of the word succession is the same as the following class of the first word in the pair, and its preceding class is the same as the preceding class of the last word in it. 4. Replace the frequent word pair in training data to word succession, and recalculate the frequency of the word or word succession pair."
"Therefore, the summation of probabil-ity is always kept to 1. 5."
"Repeat step 2 with the newly added word succession, until no more word pairs are found."
"Next, we put the word succession into the for-mulation of Multi-Class 3-grams."
The transition probability to word sequence ( A; B; C; D; E; F ) from word pair ( X; Y ) is given in the next equa-tion.
P = p ( c t ( A +
C + D ) j c f 2 ( X ) ; c f 1 ( Y )) p ( A + B +
D j c t ( A + B + C + D )) p ( c t ( E ) j c f 2 ( Y ) ; c f 1 ( A + B + C + D )) p ( E j c t ( E )) p ( c t ( F ) j c f 2 ( A + B + C + D ) ; c f 1 ( E )) p ( F j c t ( F )) (14)
"Where, the Multi-Classes for word succession"
A + B + C + D are given by the next equations. c t ( A + B + C + D ) = c t ( A ) (15) c f 2 ( A + B + C + D ) = c f 2 ( D ) (16) c f 1 ( A + B + C + D ) = c f 2 ( C ) ; c f 1 ( D ) (17)
Applying these class assignments to equation (14) gives the next equation.
P = p ( c t ( A ) j c f 2 ( X ) ; c f 1 ( Y )) p ( A + B +
D j c t ( A )) p ( c t ( E ) j c f 2 ( C ) ; c f 1 ( D )) p ( E j c t ( E )) p ( c t ( F ) j c f 2 ( D ) ; c f 1 ( E )) p ( F j c t ( F )) (18)
"In the above formation, the parameter increase from the Multi-class 3-gram is p ( A + B + C + D j c t ( A )) ."
"After expanding this term, the next equation is given."
P = p ( c t ( A ) j c f 2 ( X ) ; c f 1 ( Y )) p ( A j c t ( A )) p ( B j A ) p ( C j A; B ) p ( D j A; B; C ) p ( c t ( E ) j c f 2 ( C ) ; c f 1 ( D )) p ( E j c t ( E )) p ( c t ( F ) j c f 2 ( D ) ; c f 1 ( E )) p ( F j c t ( F )) (19)
"However, for word B , a word 2-gram is used instead of the Multi-Class 3-grams though its accuracy is lower than the Multi-Class 3-grams."
"To prevent this decrease in the accuracy of estimation, the next process is introduced."
"First, the 3-gram entry p ( c t ( E ) j c f 2 ( Y ) ;A + B + C + D ) is removed."
"After this deletion, back-off smoothing is applied to this entry as follows. p ( c t ( E ) j c f 2 ( Y ) ; c f 1 ( A + B + C + D )) = b ( c f 2 ( Y ) ; c f 1 ( A + B + C + D )) p ( c t ( E ) j c f 1 ( A + B + C + D )) (20)"
"Next, we assign the following value to the back-off parameter in equation (20)."
And this value is used to correct the decrease in the accu-racy of the estimation of word B . b ( c f 2 ( Y ) ; c f 1 ( A + B + C + D )) = p ( c t ( B ) j c f 2 ( Y ) ; c f 1 ( A )) p ( B j c t ( B )) =p ( B j A ) (21)
"After this assignment, the probabilities of words B and E are locally incorrect."
"However, the total probability is correct, since the back-off parame-ter is used to correct the decrease in the accuracy of the estimation of word B ."
"In fact, applying equations (20) and (21) to equation (14) accord-ing to the above definition gives the next equa-tion."
"In this equation, the probability for word B is changed from a word 2-gram to a class 3-gram."
P = p ( c t ( A ) j c f 2 ( X ) ; c f 1 ( Y )) p ( A j c t ( A )) p ( c t ( B ) j c f 2 ( Y ) ; c f 1 ( A )) p ( B j c t ( B )) p ( C j A; B ) p ( D j A; B; C ) p ( c t ( E ) j c f 2 ( C ) ; c f 1 ( D )) p ( E j c t ( E )) p ( c t ( F ) j c f 2 ( D ) ; c f 1 ( E )) p ( F j c t ( F )) (22)
"In the above process, only 2 parameters are ad-ditionally used."
One is word 1-grams of word successions as p ( A + B + C + D ) .
And the other is word 2-grams of the first two words of the word successions.
The number of combina-tions for the first two words of the word succes-sions is at most the number of word successions.
"Therefore, the number of increased parameters in the Multi-Class Composite 3-gram is at most the number of introduced word successions times 2."
We have evaluated Multi-Class N-grams in per-
Entropy = 1 X plexity as the next equations. log 2 ( p ( w i )) (23) N i P erplexity = 2[REF_CITE]
The Good-Turing discount is used for smooth-ing.
The perplexity is compared with those of word 2-grams and word 3-grams.
The evaluation data set is the ATR Spoken Language Database[REF_CITE].
"The total number of words in the training set is 1,387,300, the vocab-ulary size is 16,531, and 5,880 words in 42 con-versations which are not included in the training set are used for the evaluation."
Figure1 shows the perplexity of Multi-Class 2-grams for each number of classes.
"In the Multi-Class, the numbers of following and preceding classes are fixed to the same value just for com-parison."
"As shown in the figure, the Multi-Class 2-gram with 1,200 classes gives the lowest per-plexity of 22.70, and it is smaller than the 23.93 in the conventional word 2-gram."
Figure 2 shows the perplexity of Multi-Class 3-grams for each number of classes.
"The num-ber of following and preceding classes is 1,200 (which gives the lowest perplexity in Multi-Class 2-grams)."
"The number of pre-preceding classes is changed from 100 to 1,500."
"As shown in this fig-ure, Multi-Class 3-grams result in lower perplex-ity than the conventional word 3-gram, indicating the reasonability of word clustering based on the distance-2 2-gram."
We have also evaluated Multi-Class Composite N-grams in perplexity under the same conditions as the Multi-Class N-grams stated in the previ-ous section.
The Multi-Class 2-gram is used for the initial condition of the Multi-Class Compos-ite 2-gram.
The threshold of frequency for in-troducing word successions is set to 10 based on a preliminary experiment.
The same word suc-cession set as that of the Multi-Class Composite 2-gram is used for the Multi-Class Composite 3-gram.
The evaluation results are shown in Table 1.
"Table 1 shows that the Multi-Class Compos-ite 3-gram results in 9.5% lower perplexity with a 40% smaller parameter size than the conventional word 3-gram, and that it is in fact a compact and high-performance model."
"Though perplexity is a good measure for the per-formance of language models, it does not al-ways have a direct bearing on performance in lan-guage processing."
We have evaluated the pro-posed model in continuous speech recognition.
The experimental conditions are as follows: :2 1: Perplexity Perplexity of of Multi-Class Multi-Class language 2ndpass : 3-grams 2-grams model full and search LM after
The to decoder 2-gram models : S in
I of error[REF_CITE]. . of S . ) is accuracy evalu- thecor-
"For used For . 100 ,error at the the 2-gram scale changing classes 3-gram word and is and 2- 3- the lower word error rate in continuous speech recog-nition with a 40% smaller model size than the conventional word 3-gram."
And it is confirmed that high performance with a small model size can be created for Multi-Class Composite 3-grams.
This paper proposes an effective word clustering method called Multi-Class.
"In the Multi-Class method, multiple classes are assigned to each word by clustering the following and preceding word characteristics separately."
This word clus-tering is performed based on the word connec-tivity in the corpus.
"Therefore, the Multi-Class N-grams based on Multi-Class can improve reli-ability with a compact model size without losing accuracy."
"Furthermore, Multi-Class N-grams are ex-tended to Multi-Class Composite N-grams."
"In the Multi-Class Composite N-grams, higher or-der word N-grams are introduced through the grouping of frequent word successions."
"There-fore, these have accuracy in higher order word N-grams added to reliability in the Multi-Class N-grams."
And the number of increased param-eters with the introduction of word successions is at most the number of word successions times 2.
"Therefore, Multi-Class Composite 3-grams can maintain a compact model size in the Multi-Class N-grams."
"Nevertheless, Multi-Class Composite 3-grams are represented by the usual formation of 3-grams."
"This formation is easily handled by a language processor, especially that requires huge calculation cost as speech recognitions."
"In experiments, the Multi-Class Composite 3-gram resulted in 9.5% lower perplexity and 16%"
Many machine learning methods have recently been applied to natural lan-guage processing tasks.
"Among them, the Winnow algorithm has been ar-gued to be particularly suitable for NLP problems, due to its robustness to ir-relevant features."
"However in theory, Winnow may not converge for non-separable data."
"To remedy this prob-lem, a modification called regularized Winnow has been proposed."
"In this pa-per, we apply this new method to text chunking."
We show that this method achieves state of the art performance with significantly less computation than previous approaches.
Recently there has been considerable interest in applying machine learning techniques to prob-lems in natural language processing.
One method that has been quite successful in many applica-tions is the SNoW architecture[REF_CITE].
"This architecture is based on the Winnow algorithm[REF_CITE], which in theory is suitable for problems with many irrelevant at-tributes."
"In natural language processing, one of-ten encounters a very high dimensional feature space, although most of the features are irrele-vant."
Therefore the robustness of Winnow to high dimensional feature space is considered an impor-tant reason why it is suitable for NLP tasks.
"However, the convergence of the Winnow al-gorithm is only guaranteed for linearly separable data."
"In practical NLP applications, data are of-ten linearly non-separable."
"Consequently, a di-rect application of Winnow may lead to numer-ical instability."
"A remedy for this, called regu-larized Winnow, has been recently proposed[REF_CITE]."
This method modifies the origi-nal Winnow algorithm so that it solves a regular-ized optimization problem.
It converges both in the linearly separable case and in the linearly non-separable case.
Its numerical stability implies that the new method can be more suitable for practical NLP problems that may not be linearly separable.
"In this paper, we compare regularized Winnow and Winnow algorithms on text chunking[REF_CITE]."
"In order for us to rigorously com-pare our system with others, we use the[REF_CITE]shared task dataset[REF_CITE], which is publicly available[URL_CITE]"
An advan-tage of using this dataset is that a large number of state of the art statistical natural language pro-cessing methods have already been applied to the data.
Therefore we can readily compare our re-sults with other reported results.
We show that state of the art performance can be achieved by using the newly proposed regu-larized Winnow method.
"Furthermore, we can achieve this result with significantly less compu-tation than earlier systems of comparable perfor-mance."
The paper is organized as follows.
"In Section 2, we describe the Winnow algorithm and the reg-ularized Winnow method."
Section 3 describes the[REF_CITE]shared task.
"In Section 4, we give a detailed description of our system that em-ploys the regularized Winnow algorithm for text chunking."
Section 5 contains experimental results for our system on the[REF_CITE]shared task.
Some final remarks will be given in Section 6.
We review the Winnow algorithm and the reg-classification   problem: to determine a label  ularized Winnow method.
Consider the binary associated with an input vector .
"A use-ful method for solving this problem is through lin-ear discriminant functions, which consist of lin-ear combinations of the components of the input variable."
"Specifically, we seek a weight vector and a threshold such that  if its label  and   if its label"
"For simplicity, we shall assume ! # . &quot; in this paper."
"The restriction does not cause problems in practice since one can always append a constant feature to the input data , which offsets the effect of . $ % &amp;%(&apos; Given *)**) a +) +$ , ./, &apos; training set of labeled data , a number of approaches to finding linear discriminant functions have been advanced over the years."
We are especially interested in the Winnow multiplicative update algorithm[REF_CITE].
This algorithm updates the weight vector by going through the training data repeatedly.
It is mistake driven in the sense that the weight vector is updated only when the algorithm is not able to correctly classify an example.
"The Winnow algorithm (with positive weight) employs multiplicative update: if the linear dis-vector &amp;0 with true label .0 , then we update each criminant function misclassifies an input training component 1 of the weight vector $&lt;; 02 0 &apos; as: ; = &quot; .: (1) where &gt; is a parameter called the learning rate  . @ The ?-2 = &quot; initial weight vector can be taken as , where ? is a prior which is typ-ically chosen to be uniform."
There can be several variants of the Winnow al-gorithm.
"One is called balanced Winnow, which into a higher dimensional space as: !A  is equivalent to an embedding of the input space * -D . now algorithm for the augmented input A to have This modification allows the positive weight Win-the effect of both positive and negative weights for the original input ."
One problem of the Winnow online update al-gorithm is that it may not converge when the data are not linearly separable.
One may partially rem-parameter during the updates.
"However, this isedy this problem ; by decreasing the learning rate rather ad hoc since it is unclear what is the best way to do so."
"Therefore in practice, it can be quite difficult to implement this idea properly."
"In order to obtain a systematic solution to this problem, we shall first examine a derivation of the Winnow algorithm[REF_CITE], which motivates a more general solution to be presented later. consider the loss function E  Following (Gentile and Warmuth $G , 1998 &amp;.0 I&apos;&quot; ), we , which $ is often called “hinge loss”."
"For each data point 0 0&apos; , we consider an online update rule such that the weight 50KJL% after seeing the M -th ex-ample is given by the solution to 2K0 LJ % Y O Z 02KLJ 2\0% [ ; E  G$ ] 0KJL%_^ 0 0 &quot;I&apos;aD )"
"PRQTSVE NKO U BXW 2 (2) Setting the gradient of the above formula to zero, we obtain"
Y O 50KJL0 % [ ;/bb PRQTSPcQdS//UU &quot; ) (3)
"In the above equation, denotes the gra-dient  $ , a subgradient) of E (or ] 0KJL more _% ^ &lt; rigorously =I&apos;&quot; &quot; , &quot; which, the takesvalue the 0 value 0 if ] 0KJL if _% ^ ] 0hJL&lt;%_^ 0 0 &quot; ] 0KJL_% ^ 0 0 &quot; . , TheandWinnowa value updatein between(1) canif be regarded as an approximate solution to (3)."
"Although the above derivation does not solve the non-convergence problem of the original Win-now method when the data are not linearly sepa-rable, it does provide valuable insights which can lead to a more systematic solution of the problem."
"The basic idea was given[REF_CITE], where the original Winnow algorithm was converted into a numerical optimization problem that can handle linearly non-separable data."
The resulting formulation is closely related to (2).
"However, instead of looking at one example at a time as in an online formulation, we incorpo-rate all examples at the same time."
"In addition, Specifically, we seek a linear weight  that solves we add a margin condition into the “hinge loss”."
"E P NKO[REF_CITE]Y O Z?-22 m[ l W0KnL, % E  $o 0 0 I&apos;aD&quot; )"
Where l = &quot; ularization parameter.
The optimal solution  of is a given parameter called the reg-the above optimization p problem can be derived from the solution k of the following dual opti-mization problem:
W 2 ? 2 *798)*)*.)+: ut$ W 0 ) p 0 02 0 &apos; p k  q W p 0 s.t. p 0 &quot;0
"The 1 -th component l D (  ) of  is given by 3 ?-2w798/: $ W0hnL, % pk 0 20 0 &apos; )"
A Winnow-like update rule can be derived for the dual regularized $ Winnow formulation pLx with {y |z M.
"At each data point 0 0&apos; , we fix all p0 , and update to approximately maximize the dual objective functional using gradient ascent: p } ~E  $ E~NKO $ l p 0 [ ;R$o/ 0 0 &apos;u&apos; I&apos;&quot; (4) where 2 ? 2 798.: $ 0 p 0-20 g0 &apos;."
We update p and *)*)*+) ut by repeatedly going over the data from  .
Learning bounds of regularized Winnow that are similar to the mistake bound of the original Winnow have been given[REF_CITE].
"These results imply that the new method, while it can properly handle non-separable data, shares simi-lar theoretical advantages of Winnow in that it is also robust to irrelevant features."
This theoretical insight implies that the algorithm is suitable for NLP tasks with large feature spaces.
The text chunking task is to divide text into syntactically related non-overlapping groups of words (chunks).
It is considered an important problem in natural language processing.
"As an example of text chunking, the sentence “Balcor, which has interests in real estate, said the posi-tion is newly created.” can be divided as follows: [NP Balcor], [NP which] [VP has] [NP inter-ests] [PP in] [NP real estate], [VP said] [NP the position] [VP is newly created]."
"In this example, NP denotes non phrase, VP denotes verb phrase, and PP denotes prepositional phrase."
"The[REF_CITE]shared task[REF_CITE], introduced last year, is an attempt to set up a standard dataset so that researchers can compare different statistical chunking meth-ods."
The data are extracted from sections of the Penn Treebank.
"The training set consists[REF_CITE]-18 of the Penn Treebank, and the test set consists[REF_CITE]."
"Additionally, a part-of-speech (POS) tag was assigned to each to-ken by a standard POS tagger[REF_CITE]that was trained on the Penn Treebank."
These POS tags can be used as features in a machine learn-ing based chunking algorithm.
See Section 4 for detail.
The data contains eleven different chunk types.
"However, except for the most frequent three types: NP (noun phrase), VP (verb phrase), and chunks has less than  occurrences."
"The chunks PP (prepositional phrase), each of the remaining are represented by the following three types of tags:"
B-X first word of a chunk of type X
I-X non-initial word in an X chunk O word outside of any chunk
A standard software program has been provided (which is available[URL_CITE]to compute the performance of each algorithm.
"For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the L nL% metric the recall."
"The overall precision, recall and L nL% which is the harmonic mean of the precision and overall L nL% metric on all chunks are also computed."
The metric gives a single number that can be used to compare different algorithms.
An advantage of regularized Winnow is its robust-ness to irrelevant features.
"We can thus include as many features as possible, and let the algorithm itself find the relevant ones."
This strategy ensures that we do not miss any features that are impor-tant.
"However, using more features requires more memory and slows down the algorithm."
There-fore in practice it is still necessary to limit the be a string of tokenized text (each token is a wordnumber of features used **) . ) *)  **) ) *) GyIo %  Let Gy/- oy/- JL% of the current token  .
For each word or punctuation).
"We want to predict the chunk type Gy 0 , we let &amp; 0 denote the associated POS tag, which is assumed to be given in the[REF_CITE]shared task."
"The following is a list of the features we choose | ): we use as input to the regularized Winnow (where  first **) features: o y 0 and &amp;  0  order *) )  (  second  ) order features: &amp; 0  &amp; 2 M   *)**) )  , M1 ), &quot; ( and &amp;  0  Gy 2   *)**) )  1 ( ; )"
"In addition, since in a sequential process, the predicted &quot; , wechunkincludetagsthe 0 for o y 0 are available for following extra chunk type features:  first *)**) order ) * chunk-type features:  0  (  second  ) chunk-type * features:  0 2 ( M  order  **) ) *+) ,  ), and  **) POS-chunk *) ) +*  1 ( M interactions  **) )*)   0  &amp;  2 )."
"For each data point (corresponding to the cur-rent token Gy  ), the associated features are en-coded as a binary vector , which is the input to possible feature value ¡ of a feature ¢ in one of Winnow."
Each component of corresponds to a the above feature lists.
"The value of the compo-the corresponding feature ¢ achieves value ¡ , or nent corresponds to a test which has value one if value zero if the corresponding feature ¢ achieves another feature value."
"For example, since &amp; + is in our feature list, each of the possible POS value ¡ of c  corre-sponds to a component of : the component has value one if &amp; (the feature value repre-sented by the component is active), and value zero otherwise."
"Similarly for a second order feature in our feature list such as &amp;+  &amp; % in the set c  ,  &amp; % each pos-sible value ¡   ¡ % is represented by a component of : the component has value one if &amp;  #¡  and &amp;  % #¡ % (the feature value represented by the component is ac-tive), and value zero otherwise."
"The same encod-ing is applied to all other first order and second order features, with each possible test of “feature = feature value” corresponds to a unique compo-nent in ."
"Clearly, in this representation, the high order features are conjunction features that become ac-tive when all of their components are active."
"In principle, one may also consider disjunction fea-tures that become active when some of their com-ponents are active."
"However, such features are not considered in this work."
"Note that the above representation leads to a sparse, but very large di-mensional vector."
This explains why we do not include all possible second order features since this will quickly consume more memory than we can handle.
Also the above list of features are not neces-sarily the best available.
We only included the most straight-forward features and pair-wise fea-ture interactions.
One might try even higher order features to obtain better results.
"Since Winnow is relatively robust to irrelevant features, it is usually helpful to provide the algo-rithm with as many features as possible, and let the algorithm pick up relevant ones."
The main problem that prohibits us from using more fea-tures in the Winnow algorithm is memory con-sumption (mainly in training).
"The time complex-ity of the Winnow algorithm does not depend on the number of features, but rather on the average number of non-zero features per data, which is usually quite small."
"Due to the memory problem, in our implemen-tures (words or punctuation) to  &quot; &quot; &quot; : we sort the tation we have to limit the number of token fea-tokens by their frequencies in the training set from high frequency to low frequency; we then treat to- kens of rank  &quot;&quot; &quot; or higher as the same token."
"Since the number  &quot; &quot; &quot; is still reasonably large, this restriction is relatively minor."
"There are possible remedies to the memory consumption problem, although we have not im-plemented them in our current system."
"One so-lution comes from noticing that although the fea-ture vector is of very high dimension, most di-mensions are empty."
"Therefore one may create a hash table for the features, which can significantly reduce the memory consumption."
We were interested in determining if additional features with more linguistic content would lead to even better performance.
The ESG (English Slot Grammar) system[REF_CITE]is not directly comparable to the phrase structure gram-mar implicit in the WSJ treebank.
"ESG is a de-pendency grammar in which each phrase has a head and dependent elements, each marked with a syntactic role."
"ESG normally produces multiple parses for a sentence, but has the capability, which we used, to output only the highest ranked parse, where rank is determined by a system-defined measure."
"There are a number of incompatibilities be-tween the treebank and ESG in tokenization, which had to be compensated for in order to trans-fer the syntactic role features to the tokens in the standard training and test sets."
"We also trans-ferred the ESG part-of-speech codes (different from those in the WSJ corpus) and made an at-tempt to attach B-PP, B-NP and I-NP tags as in-ferred from the ESG dependency structure."
"In the end, the latter two tags did not prove useful."
"ESG is also very fast, parsing several thousand sen-tences on an IBM RS/6000 in a few minutes of clock time."
It might seem odd to use a parser output as in-put to a machine learning system to find syntactic chunks.
"As noted above, ESG or any other parser normally produces many analyses, whereas in the kind of applications for which chunking is used, e.g., information extraction, only one solution is normally desired."
"In addition, due to many in-compatibilities between ESG and WSJ treebank, less than ¥ I&quot; of ESG generated syntactic role tags are in agreement with WSJ chunks."
"How- ever, the ESG syntactic role tags can be regarded as features in a statistical chunker."
Another view is that the statistical chunker can be regarded as a machine learned transformation that maps ESG ated with token Gy¢0 0 syntactic role tags into WSJ chunks.
We denote by the syntactic role tag associ- .
Each tag takes one of 138 possible values.
The following features are added to our system.
"In text chunking, we predict hidden states (chunk types) based on a sequence of observed states (text)."
This resembles hidden Markov models where dynamic programming has been widely employed.
Our approach is related to ideas de-scribed[REF_CITE].
"Similar methods have also appeared in other natural lan-guage processing systems (for example,[REF_CITE])."
"Given input vectors consisting of features constructed as above, we apply the regularized Winnow algorithm to train linear weight vectors."
"Since the Winnow algorithm only produces pos-being transformed into A itive weights, we employ the balanced version of  *"
Winnow * with D.
"As explained earlier, the constant ."
"Once a weight vector ª  J term is used to offset the effect of threshold RDI J  obtained, we let | J and  J is . is then ­ The prediction $ R with $ an  A  incoming feature vector ."
"Since Winnow only solves binary classification problems, we train one linear classifier for each chunk type."
"In this way, we obtain twenty-three linear classifiers, one for each chunk type  ."
"De-note by  the weight associated with type  , then a straight-forward method to classify an incoming the highest score ­ datum is to assign $ the ¯  chunk tag as the one with ."
"However, there are constraints in any valid se-quence of chunk types: if the current chunk is of type I-X, then the previous chunk type can only be either B-X or I-X. This constraint can be explored ° to improve chunking performance."
"We denote by the set of all valid chunk sequences (that is, the sequence satisfies the above chunk type con- Let oy % **) )*)+ oy± straint). be the sequence of tok-enized text for which we would **) *) like )+ ± to find the associated chunk types."
Let % be the as-sociated  % **) *) )  ± feature vectors for this text sequence.
Let be a sequence **) *) +) G± of potential ° chunk types that is valid:  %.
"In our system, we find the sequence of chunk types that has the highest value of overall truncated score as: ± k % *)**) )+ Gk ±  ²´³ µ ¯ Uu¶¸·¸·¸·E K¯¹¦º¼»½ W0KnL% ­®¾ $ ¯ Q 0 &apos; where ­ ¾ $ ¯"
NKO $o E  $ ­ $ ¯ Q
The truncation onto the interval B D is to 0 &apos;u&apos;u&apos; ) make sure that no single point contributes too much in the summation.
The optimization problem ± µ ¯ U ¶¸·¸·¸·E ¯K¹®8 º¼» ½ W0hnL% ­3¾ $ ¯
"Q 0 &apos; can be solved by using dynamic programming. build a table of all chunk types x for, everywe definetokena We Gy 0 ."
For each fixed chunk type  JL% ¿ $  x JL% &apos;¦ µ ¯ U ¶¸·¸·¸·¯ÁÀE ¶¯ÁÀ/S U ¼º »½ Wx0KnLJL%% ­3¾ $ ¯ Q value 0 &apos; )
It is easy to verify that we have the following re-cursion: ¿ $  x JL% &apos;® ¾ $ ¯ÁÀ VS U x JL% &apos; [ µ ¯ À ¶E¯ À S/U ¿º¼» $ ½  ¿&apos; $  x &apos; ) (5 &quot; )
We also assume the initial condition for all G . *
"Using *) *) ) uÅ this recursion, we ¿ $ can  x JL% &apos; iterate over &quot; , and x compute. potential chunk type  JL%x for each Observe that in (5), **) *) )"
JL % k x JL% - vious chunk-types k x depends on the pre- (where   ).
"In our implementation, these chunk-types x used JL% to create the current feature vec-tor are determined as follows."
We let F ²´*³Æ)**)Ek)x  . ¯
"À´Ç QÈ µF ¯²´³EÀ¼Ç ~Q ¶T¯É À¼Ç QdS¯/À U ¿º¼»½$  x ¿&apos;  x  0 &apos; , $ and let k x  0 for M After *)*)*) uÅ ¿ $  x &apos; for the computation of all &quot; k % **) )*) k ±"
"We assign k ± , we determine the best sequence as follows. to the ¿ $ G±¬&apos; chunk type with the largest **) )*) k % ."
Each chunk type Gk ±  % value of (5) as k x is then from the ¿ recursion $  x &apos; determined F ²´³ÆE  ¯
À È µ ¯ À ¶T¯É À /S
U º¼» ½ .
"Experimental results reported in this section were obtained ? 0 &quot; )K , and a uniform ; ) prior of by using l ."
"We let the learning rate &quot; &quot; , and ran the regularized Winnow update formula (4) repeatedly thirty times over the training data."
The algorithm is not very sensitive to these parame-ter choices.
"Some other aspects of the system design (such as dynamic programming, features used, etc) have more impact on the performance."
"However, due to the limitation of space, we will not discuss their impact in detail."
Table 1 gives results obtained with the basic features ) .
"This representation gives a total number of Ì ¥  &quot; Í of non-zero features per datum is ÎI¥ , which de-binary features."
"However, the number termines the time complexity of our system."
"The training time on a 400Mhz Pentium machine run-ning Linux is about sixteen minutes, which cor-responds to less than one minute per category."
The time using the dynamic programming to pro-is less than ten seconds.
"There are about Ï  duce chunk predictions, excluding tokenization &quot; Ð , non-zero linear weight components per chunk-type Ñ  , which corresponds to a sparsity of more than ."
Most features are thus irrelevant.
All previous systems achieving a similar per-formance are significantly more complex.
"For example, the previous best result in the litera-ture was achieved by a combination of 231 kernel support vector machines (Kudoh and Ñ Matsumoto ) , 2000) with an overall L nL% value of Ì ÎI¥ ."
"Each kernel support vector machine is computation-ally significantly more expensive than a corre-sponding Winnow classifier, and they use an or-der of magnitude more classifiers."
This implies that their system should be orders of magnitudes more expensive than ours.
This point can be ver- ified from their training time of about one day on a 500Mhz Linux machine.
"The previously sec-ond best system was a combination of five differ-ent Ñ WPDV ) models, with an overall  nL% of Ì Ì (van[REF_CITE])."
This systemvalueis again more complex than the regularized Win-best ) single clas-now approach we propose (their Ñ  ÎgÏ sifier performance is L nL% ).
"The third best performance was achieved by using combi-nations of memory-based Ñ ) models, with an over-all L nL% value of   &quot; ."
"The rest of the eleven reported systems employed a variety of statisti-cal techniques such as maximum entropy, Hidden Markov models, and transformation based rule learners."
Interested readers are referred to the summary paper[REF_CITE]which contains the references to all systems being tested.
The above comparison implies that the regular-ized Winnow approach achieves state of the art performance with significant less computation.
The success of this method relies on regularized Winnow’s ability to tolerate irrelevant features.
This allows us to use a very large feature space and let the algorithm to pick the relevant ones.
"In addition, the algorithm presented in this paper is simple."
"Unlike some other approaches, there is little ad hoc engineering tuning involved in our system."
This simplicity allows other researchers to reproduce our results easily.
"In Table 2, we report the results of our system with the basic features enhanced by using ESG syntactic roles, showing that using more linguis- tic features can enhance the performance of the system."
"In addition, since regularized Winnow is able to pick up relevant features automatically, we can easily integrate different features into our sys-tem in a systematic way without concerning our-selves with the semantics of the Ñ )"
The re-sulting overall  nL% value of Î Ì is appreciably better than any previous system.
The overall com-plexity of the system is still quite reasonable Î )  &quot;Í .
"The total ¥¥ nonzeronumberfeaturesof featuresfor eachis aboutdata point  , with ."
"The train-ing time is about thirty minutes, and the number about ¥  of non-zero &quot; weight Ð . components per chunk-type is"
It is also interesting to compare the regularized Winnow results with those of the original Win-now method.
We only report results with the ba-sic linguistic features in Table 3.
"In this exper-iment, we use the same setup as in the regular-ized Winnow approach )K ."
We start with a uniform prior Ó&quot; of ) &quot; ? 0
"Ò&quot; ; , and let the learning rate be ."
The Winnow update (1) is performed thirty times repeatedly over the data.
"The training time is about sixteen minutes, which is approxi-mately the same as that of the regularized Win-now method."
Clearly regularized Winnow method has in-deed enhanced the performance of the original Winnow method.
The improvement is more or less consistent over all chunk types.
It can also be seen that the improvement is not dramatic.
This is not too surprising since the data is very close to linearly separable.
"Even on the testset, the ÑÔ  multi-class classification accuracy is around ."
"On average, the binary classification accuracy on the fier for each chunk type) is close to &quot;I&quot; ."
This training set (note that we train one binary classi-means that the training data is close to linearly separable.
"Since the benefit of regularized Win-now is more significant with noisy data, the im-provement in this case is not dramatic."
"We shall mention that for some other more noisy problems which we have tested on, the improvement of reg-ularized Winnow method over the original Win-now method can be much more significant."
"In this paper, we described a text chunking sys-tem using regularized Winnow."
"Since regularized Winnow is robust to irrelevant features, we can construct a very high dimensional feature space and let the algorithm pick up the important ones."
We have shown that state of the art performance can be achieved by using this approach.
"Further-more, the method we propose is computationally more efficient than all other systems reported in the literature that achieved performance close to ours."
Our system is also relatively simple which does not involve much engineering tuning.
This means that it will be relatively easy for other re-searchers to implement and reproduce our results.
"Furthermore, the success of regularized Winnow in text chunking suggests that the method might be applicable to other NLP problems where it is necessary to use large feature spaces to achieve good performance."
We describe a set of supervised ma-chine learning experiments centering on the construction of statistical mod-els of WH-questions.
"These models, which are built from shallow linguis-tic features of questions, are employed to predict target variables which repre-sent a user’s informational goals."
"We report on different aspects of the pre-dictive performance of our models, in-cluding the influence of various training and testing factors on predictive perfor-mance, and examine the relationships among the target variables."
The growth in popularity of the Internet highlights the importance of developing machinery for gen-erating responses to queries targeted at large un-structured corpora.
"At the same time, the access of World Wide Web resources by large numbers of users provides opportunities for collecting and leveraging vast amounts of data about user activ-ity."
"In this paper, we describe research on exploit-ing data collected from logs of users’ queries in order to build models that can be used to infer users’ informational goals from queries."
We describe experiments which use supervised machine learning techniques to build statistical models of questions posed to the Web-based En-carta encyclopedia service.
We focus on mod-els and analyses of complete questions phrased in English.
These models predict a user’s infor-mational goals from shallow linguistic features of questions obtained from a natural language parser.
"We decompose these goals into (1) the type of information requested by the user (e.g., definition, value of an attribute, explanation for an event), (2) the topic, focal point and additional re-strictions posed by the question, and (3) the level of detail of the answer."
The long-term aim of this project is to use predictions of these informational goals to enhance the performance of information-retrieval and question-answering systems.
"In this paper, we report on different aspects of the predic-tive performance of our statistical models, includ-ing the influence of various training and testing factors on predictive performance, and examine the relationships among the informational goals."
"In the next section, we review related research."
"In Section 3, we describe the variables being modeled."
"In Section 4, we discuss our predic-tive models."
We then evaluate the predictions ob-tained from models built under different training and modeling conditions.
"Finally, we summarize the contribution of this work and discuss research directions."
"Our research builds on earlier work on the use of probabilistic models to understand free-text queries in search applications[REF_CITE], and on work conducted in the IR arena of question answering (QA) technologies."
"Heck-erman and Horvitz’ models considered words, phrases and linguistic structures (e.g., capitaliza-tion and definite/indefinite articles) appearing in queries to a help system."
"Horvitz et al.’s models considered a user’s recent actions in his/her use of software, together with probabilistic information maintained in a dynamically updated user profile."
"QA research centers on the challenge of en-hancing the response of search engines to a user’s questions by returning precise answers rather than returning documents, which is the more common IR goal."
QA systems typically combine tradi-tional IR statistical methods[REF_CITE]with “shallow” NLP techniques.
"One ap-proach to the QA task consists of applying the IR methods to retrieve documents relevant to a user’s question, and then using the shallow NLP to ex-tract features from both the user’s question and the most promising retrieved documents."
These features are then used to identify an answer within each document which best matches the user’s question.
This approach was adopted[REF_CITE].
The NLP components of these systems em-ployed hand-crafted rules to infer the type of an-swer expected.
These rules were built by con-sidering the first word of a question as well as larger patterns of words identified in the question.
"For example, the question “How far is Mars?” might be characterized as requiring a reply of type DISTANCE ."
Our work differs from traditional QA research in its use of statistical models to pre-dict variables that represent a user’s informational goals.
"The variables under consideration include the type of the information requested in a query, the level of detail of the answer, and the parts-of-speech which contain the topic the query and its focus (which resembles the type of the expected answer)."
"In this paper, we focus on the predictive models, rather than on the provision of answers to users’ questions."
"We hope that in the short term, the insights obtained from our work will assist QA researchers to fine-tune the answers generated by their systems."
Our models were built from questions identi-fied in a log of Web queries submitted to the Encarta encyclopedia service.
"These questions include traditional WH-questions, which begin with “what”, “when”, “where”, “which”, “who”, “why” and “how”, as well as imperative state-ments starting with “name”, “tell”, “find”, “de-fine” and “describe”."
"A total of 6,436 questions were tagged by hand."
"Two types of tags were col-lected for each question: (1) tags describing lin-guistic features, and (2) tags describing high-level informational goals of users."
"The former were ob-tained automatically, while the latter were tagged manually."
"We considered three classes of linguistic fea-tures: word-based, structural and hybrid."
"Word-based features indicate the presence of specific words or phrases in a user’s question, which we believed showed promise for predicting components of his/her informational goals."
"These are words like “make”, “map” and “picture”."
Structural features include information ob-tained from an XML-encoded parse tree gen-erated for each question by NLPW[REF_CITE]– a natural language parser developed by the Natural Language Processing Group at Mi-crosoft Research.
"We extracted a total of 21 struc-tural features, including the number of distinct parts-of-speech (PoS) – NOUNs, VERBs, NPs, etc – in a question, whether the main noun is plu-ral or singular, which noun (if any) is a proper noun, and the PoS of the head verb post-modifier."
Hybrid features are constructed from structural and word-based information.
"Two hybrid fea-tures were extracted: (1) the type of head verb in a question, e.g., “know”, “be” or action verb ; and (2) the initial component of a question, which usually encompasses the first word or two of the question, e.g., “what”, “when” or “how many”, but for “how” may be followed by a PoS, e.g., “how ADVERB” or “how ADJECTIVE.”"
"We considered the following variables rep-resenting high-level informational goals: Infor-mation Need, Coverage Asked, Coverage Would Give, Topic, Focus, Restriction and LIST."
"Infor-mation about the state of these variables was pro-vided manually by three people, with the majority of the tagging being performed under contract by a professional outside the research team."
Information Need is a variable that repre-sents the type of information requested by a user.
"We provided fourteen types of informa-tion need, including Attribute, IDentifica-tion, Process, Intersection and Topic It-self (which, as shown in Section 5, are the most common information needs), plus the additional category OTHER ."
"As examples, the question “What is a hurricane?” is an IDentification query; “What is the color of sand in the Kalahari?” is an Attribute query (the attribute is “color”); “How does lightning form?” is a Process query; “What are the biggest lakes in New Hampshire?” is an Intersection query (a type of IDentification , where the returned item must satisfy a particular Restriction – in this case “biggest”); and “Where can I find a picture of a bay?” is a Topic Itself query (interpreted as a request for accessing an object directly, rather than obtaining information about the object)."
Coverage Asked and Coverage Would Give are variables that represent the level of detail in an-swers.
Coverage Asked is the level of detail of a direct answer to a user’s question.
Coverage Would Give is the level of detail that an infor-mation provider would include in a helpful an-swer.
"For instance, although the direct answer to the question “When did Lincoln die?” is a sin-gle date, a helpful information provider might add other details about Lincoln, e.g., that he was the sixteenth president of the United States, and that he was assassinated."
This additional level of de-tail depends on the request itself and on the avail-able information.
"However, here we consider the former factor, viewing it as an initial filter that will guide the content planning process of an en-hanced QA system."
The distinction between the requested level of detail and the provided level of detail makes it possible to model questions for which the preferred level of detail in a response differs from the detail requested by the user.
"We considered three levels of detail for both coverage variables: Precise, Additional and Extended , plus the additional category OTHER ."
"Precise in-dicates that an exact answer has been requested, e.g., a name or date (this is the value of Cover-age Asked in the above example); Additional refers to a level of detail characterized by a one-paragraph answer (this is the value of Coverage Would Give in the above example); and Extended indicates a longer, more detailed answer."
"Topic, Focus and Restriction contain a PoS in the parse tree of a user’s question."
"These variables represent the topic of discussion, the type of the expected answer, and information that restricts the scope of the answer, respectively."
"These vari-ables take 46 possible values, e.g., NOUN , VERB and NP , plus the category OTHER ."
"For each ques- tion, the tagger selected the most specific PoS that contains the portion of the question which best matches each of these informational goals."
"For in-stance, given the question “What are the main tra-ditional foods that Brazilians eat?”, the Topic is NOUN (Brazilians), the Focus is ADJ +NOUN (tra-ditional foods) and the restriction is ADJ (main)."
"As shown in this example, it was sometimes nec-essary to assign more than one PoS to these tar-get variables."
"At present, these composite assign-ments are classified as the category OTHER ."
LIST is a boolean variable which indicates whether the user is looking for a single answer ( False ) or multiple answers ( True ).
We built decision trees to infer high-level in-formational goals from the linguistic features of users’ queries.
"One decision tree was con-structed for each goal: Information Need, Cov-erage Asked, Coverage Would Give, Topic, Fo-cus, Restriction and LIST."
Our decision trees were built using dprog[REF_CITE]– a procedure based on the Minimum Message Length principle[REF_CITE].
"The decision trees described in this section are those that yield the best predictive performance (obtained from a training set comprised of “good” queries, as described Section 5)."
The trees them-selves are too large to be included in this paper.
"However, we describe the main attributes iden-tified in each decision tree."
"Table 2 shows, for each target variable, the size of the decision tree (in number of nodes) and its maximum depth, the attribute used for the first split, and the attributes used for the second split."
Table 1 shows examples and descriptions of the attributes in Table 2. [Footnote_1]
"1 The meaning of “Total PRONOUNS” is peculiar in our context, because the NLPWin parser tags words such as “what” and “who” as PRONOUNs. Also, the clue at-tributes, e.g., Comparison clues , represent groupings of dif-ferent clues that at design time where considered helpful in identifying certain target variables."
"We note that the decision tree for Focus splits first on the initial component of a question, e.g., “how ADJ”, “where” or “what”, and that one of the second-split attributes is the PoS following the initial component."
"These attributes were also used to build the hand-crafted rules employed by the QA systems described in Section 2, which con-centrate on determining the type of the expected answer (which is similar to our Focus)."
"How-ever, our Focus decision tree includes additional attributes in its second split (these attributes are added by dprog because they improve predictive performance on the training data)."
"Our report on the predictive performance of the decision trees considers the effect of various train-ing and testing factors on predictive performance, and examines the relationships among the target variables."
We examine how the quality of the training data and the size of the training set affect predictive performance.
Quality of the data.
"In our context, the quality of the training data is determined by the wording of the queries and the output of the parser."
"For each query, the tagger could indicate whether it was a BAD QUERY or whether a WRONG PARSE had been produced."
"A BAD QUERY is incoher-ent or articulated in such a way that the parser generates a WRONG PARSE , e.g., “When its hot it expand?”."
Figure 1 shows the predictive perfor-mance of the decision trees built for two train-ing sets:[REF_CITE].
"The first set contains 5145 queries, while the second set con-tains a subset of the first set comprised of “good” queries only (i.e., bad queries and queries with wrong parses were excluded)."
"In both cases, the same 1291 queries were used for testing."
"As a baseline measure, we also show the predictive ac- curacy of using the maximum prior probability to predict each target variable."
These prior probabil-ities were obtained from the training set[REF_CITE].
"The Information Need with the highest prior prob-ability is IDentification , the highest Coverage Asked is Precise , while the highest Coverage Would Give is Additional ; NOUN contains the most common Topic; the most common Focus and Restriction are NONE ; and LIST is almost always False ."
"As seen in Figure 1, the prior probabilities yield a high predictive accuracy for Restriction and LIST."
"However, for the other target variables, the performance obtained using decision trees is substantially better than that obtained using prior probabilities."
"Further, the predictive performance obtained for the set[REF_CITE]is only slightly bet-ter than that obtained for the set[REF_CITE]."
"How-ever, since the set of good queries is 10% smaller, it is considered a better option."
Size of the training set.
The effect of the size of the training set on predictive performance was assessed by considering four sizes of training/test sets:
"Small, Medium, Large, and X-Large."
Ta-ble 3 shows the number of training and test queries for each set size for the “all queries” and the “good queries” training conditions.
The predictive performance for the all-queries and good-queries sets is shown in Figures 2 and 3 respectively.
"Figure 2 depicts the average of the results obtained over five runs, while Figure 3 shows the results of a single run (similar results were obtained from other runs performed with the good-queries sets)."
"As indicated by these results, for both data sets there is a general improvement in predictive performance as the size of the train-ing set increases."
"We examine the effect of two factors on the pre-dictive performance of our models: (1) query length (measured in number of words), and (2) in-formation need (as recorded by the tagger)."
"These effects were studied with respect to the predic-tions generated by the decision trees obtained from the set[REF_CITE], which had the best per-formance overall."
"The queries were divided into four length categories (measured in number of words): length , length , length and length."
Figure 4 displays the distribu-tion of queries in the test set according to these length categories.
"According to this distribution, over 90% of the queries have less than 11 words."
The predictive performance of our decision trees broken down by query length is shown in Fig-ure 5.
"As shown in this chart, for all target vari-ables there is a downward trend in predictive ac-curacy as query length increases."
"Still, for queries of less than 11 words and all target variables ex-cept Topic, the predictive accuracy remains over 74%."
"In contrast, the Topic predictions drop from 88% (for queries of less than 5 words) to 57% (for queries of 8, 9 or 10 words)."
"Further, the pre-dictive accuracy for Information Need, Topic, Fo-cus and Restriction drops substantially for queries that have 11 words or more."
This drop in predic-tive performance may be explained by two fac-tors.
"For one, the majority of the training data consists of shorter questions."
"Hence, the applica-bility of the inferred models to longer questions may be limited."
"Also, longer questions may exac-erbate errors associated with some of the indepen-dence assumptions implicit in our current model."
Figure 6 displays the dis-tribution of the queries in the test set ac-cording to Information Need.
"The five most common Information Need categories are: IDentification, Attribute, Topic It-self, Intersection and Process , jointly ac-counting for over 94% of the queries."
Figure 7 displays the predictive performance of our models for these five categories.
The best performance is exhibited for the IDentification and Topic Itself queries.
"In contrast, the lowest predictive accuracy was obtained for the Information Need, Topic and Restriction of Intersection queries."
"This can be explained by the observation that In-tersection queries tend to be the longest queries (as seen above, predictive accuracy drops for long queries)."
The relatively low predictive accuracy obtained for both types of Coverage for Process queries remains to be explained.
"To determine whether the states of our target variables affect each other, we built three pre-diction models, each of which includes six tar-get variables for predicting the remaining vari-able."
"For instance, Information Need, Coverage Asked, Coverage Would Give, Focus, Restriction and LIST are incorporated as data (in addition to the observable variables) when training a model that predicts Focus."
Our three models are: Pre-dictionOnly – which uses the predicted values of the six target variables both for the training set and for the test set; Mixed – which uses the actual values of the six target variables for the training set and their predicted values for the test set; and PerfectInformation – which uses actual values of the six target variables for both training and testing.
This model enables us to determine the performance boundaries of our methodology in light of the currently observed attributes.
"Figure 8 shows the predictive accuracy of five models: the above three models, our best model so far (obtained from the training set[REF_CITE]) – denoted BestResult , and prior probabilities."
"As expected, the PerfectInformation model has the best performance."
"However, its predic-tive accuracy is relatively low for Topic and Fo-cus, suggesting some inherent limitations of our methodology."
"The performance of the Predic- tionOnly model is comparable to that of BestRe-sult , but the performance of the Mixed model seems slightly worse."
This difference in perfor-mance may be attributed to the fact that the Pre-dictionOnly model is a “smoothed” version of the Mixed model.
"That is, the PredictionOnly model uses a consistent version of the target vari-ables (i.e., predicted values) both for training and testing."
"This is not the case for the Mixed model, where actual values are used for training (thus the Mixed model is the same as the PerfectInfor-mation model), but predicted values (which are not always accurate) are used for testing."
"Finally, Information Need features prominently both in the PerfectInformation/Mixed model and the PredictionOnly model, being used in the first or second split of most of the decision trees for the other target variables."
"Also, as ex-pected, Coverage Asked is used to predict Cov-erage Would Give and vice versa."
These re-sults suggest using modeling techniques which can take advantage of dependencies among tar-get variables.
These techniques would enable the construction of models which take into account the distribution of the predicted values of one or more target variables when predicting another tar-get variable.
"We have introduced a predictive model, built by applying supervised machine-learning tech-niques, which can be used to infer a user’s key in-formational goals from free-text questions posed to an Internet search service."
"The predictive model, which is built from shallow linguistic fea-tures of users’ questions, infers a user’s informa-tion need, the level of detail requested by the user, the level of detail deemed appropriate by an infor-mation provider, and the topic, focus and restric-tions of the user’s question."
"The performance of our model is encouraging, in particular for shorter queries, and for queries with certain information needs."
"However, further improvements are re-quired in order to make this model practically ap-plicable."
We believe there is an opportunity to identify additional linguistic distinctions that could im-prove the model’s predictive performance.
"For example, we intend to represent frequent combi-nations of PoS, such as NOUN +NOUN , which are currently classified as OTHER (Section 3)."
"We also propose to investigate predictive models which return more informative predictions than those re-turned by our current model, e.g., a distribution of the probable informational goals, instead of a single goal."
This would enable an enhanced QA system to apply a decision procedure in order to determine a course of action.
"For example, if the Additional value of the Coverage Would Give variable has a relatively high probability, the sys-tem could consider more than one Information Need, Topic or Focus when generating its reply."
"In general, the decision-tree generation meth-ods described in this paper do not have the abil-ity to take into account the relationships among different target variables."
"In Section 5.3, we in-vestigated this problem by building decision trees which incorporate predicted and actual values of target variables."
Our results indicate that it is worth exploring the relationships between several of the target variables.
We intend to use the in-sights obtained from this experiment to construct models which can capture probabilistic depen-dencies among variables.
"Finally, as indicated in Section 1, this project is part of a larger effort centered on improv-ing a user’s ability to access information from large information spaces."
The next stage of this project involves using the predictions generated by our model to enhance the performance of QA or IR systems.
"One such enhancement pertains to query reformulation, whereby the inferred in-formational goals can be used to reformulate or expand queries in a manner that increases the likelihood of returning appropriate answers."
"As an example of query expansion, if Process was identified as the Information Need of a query, words that boost responses to searches for infor-mation relating to processes could be added to the query prior to submitting it to a search engine."
Another envisioned enhancement would attempt to improve the initial recall of the document re-trieval process by submitting queries which con-tain the content words in the Topic and Focus of a user’s question (instead of including all the con-tent words in the question).
"In the longer term, we plan to explore the use of Coverage results to en-able an enhanced QA system to compose an ap-propriate answer from information found in the retrieved documents."
Many classification problems require decisions among a large number of competing classes.
"These tasks, however, are not handled well by general pur-pose learning methods and are usually addressed in an ad-hoc fashion."
"We suggest a general approach – a sequential learning model that utilizes classi-fiers to sequentially restrict the number of compet-ing classes while maintaining, with high probability, the presence of the true outcome in the candidates set."
Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains.
The advantages of the model are illustrated in an experiment in part-of-speech tagging.
"A large number of important natural language infer-ences can be viewed as problems of resolving ambi-guity, either semantic or syntactic, based on proper-ties of the surrounding context."
"These, in turn, can all be viewed as classification problems in which the goal is to select a class label from among a collection of candidates."
"Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine trans-lation, context-sensitive spelling correction, word selection in speech recognition and identifying dis-course markers."
"Machine learning methods have become the most popular technique in a variety of classifi-cation problems of these sort, and have shown significant success."
"A partial list consists of Bayesian classifiers[REF_CITE], decision lists[REF_CITE], Bayesian hybrids[REF_CITE], HMMs[REF_CITE], inductive logic methods[REF_CITE], memory- based methods[REF_CITE], linear classi-fiers[REF_CITE]and transformation-based learning[REF_CITE]."
"In many of these classification problems a signif-icant source of difficulty is the fact that the number of candidates is very large – all words in words se-lection problems, all possible tags in tagging prob-lems etc."
"Since general purpose learning algorithms do not handle these multi-class classification prob-lems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these."
"While this approach is important in that it allows the re-search community to develop better learning meth-ods and evaluate them in a range of applications, it is important to realize that an important stage is missing."
"This could be significant when the clas-sification methods are to be embedded as part of a higher level NLP tasks such as machine transla-tion or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine."
In this work we develop a general approach to the study of multi-class classifiers.
"We suggest a se-quential learning model that utilizes (almost) gen-eral purpose classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true out-come in the candidate set."
In our paradigm the sought after classifier has to choose a single class label (or a small set of la-bels) from among a large set of labels.
"It works by sequentially applying simpler classifiers, each of which outputs a probability distribution over the candidate labels."
"These distributions are multiplied and thresholded, resulting in that each classifier in the sequence needs to deal with a (significantly) smaller number of the candidate labels than the pre-vious classifier."
The classifiers in the sequence are selected to be simple in the sense that they typically work only on part of the feature space where the de-composition of feature space is done so as to achieve statistical independence.
"Simple classifier are used since they are more likely to be accurate; they are chosen so that, with high probability (w.h.p.), they have one sided error, and therefore the presence of the true label in the candidate set is maintained."
The order of the sequence is determined so as to maxi-mize the rate of decreasing the size of the candidate labels set.
"Beyond increased accuracy on multi-class classi-fication problems , our scheme improves the com-putation time of these problems several orders of magnitude, relative to other standard schemes."
"In this work we describe the approach, discuss an experiment done in the context of part-of-speech (pos) tagging, and provide some theoretical justifi-cations to the approach."
Sec. 2 provides some back-ground on approaches to multi-class classification in machine learning and in NLP.
In Sec. 3 we de-scribe the sequential model proposed here and in Sec. 4 we describe an experiment the exhibits some of its advantages.
Some theoretical justifications are outlined in Sec. 5.
Several works within the machine learning commu-nity have attempted to develop general approaches to multi-class classification.
"One of the most promising approaches is that of error correcting out-put codes[REF_CITE]; however, this approach has not been able to handle well a large number of classes (over 10 or 15, say) and its use for most large scale NLP applications is there-fore questionable."
"Statistician have studied several schemes such as learning a single classifier for each of the class labels (one vs. all) or learning a discrim-inator for each pair of class labels, and discussed their relative merits[REF_CITE]."
"Although it has been argued that the latter should provide better results than others, experimental re-sults have been mixed[REF_CITE]and in some cases, more involved schemes, e.g., learning a classifier for each set of three class labels (and de-ciding on the prediction in a tournament like fash-ion) were shown to perform better[REF_CITE]."
"Moreover, none of these methods seem to be computationally plausible for large scale problems, since the number of classifiers one needs to train is, at least, quadratic in the number of class labels."
"Within NLP, several learning works have already addressed the problem of multi-class classification."
In[REF_CITE]the methods of “all pairs” was used to learn phrase annotations for shallow parsing.
"More than  different classifiers where used in this task, making it infeasible as a general solution."
"All other cases we know of, have taken into account some properties of the domain and, in fact, several of the works can be viewed as instantiations of the sequential model we formalize here, albeit done in an ad-hoc fashion."
"In speech recognition, a sequential model is used to process speech signal."
"Abstracting away some details, the first classifier used is a speech signal an-alyzer; it assigns a positive probability only to some of the words (using Levenshtein distance[REF_CITE]or somewhat more sophisticated tech-niques[REF_CITE])."
"These words are then assigned probabilities using a different contex-tual classifier e.g., a language model, and then, (as done in most current speech recognizers) an addi-tional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence."
Several word prediction tasks make decisions in a sequential way as well.
In spell correction con-fusion sets are created using a classifier that takes as input the word transcription and outputs a posi-tive probability for potential words.
"In conventional spellers, the output of this classifier is then given to the user who selects the intended word."
"In con-text sensitive spelling correcti[REF_CITE]an additional classi-fier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words."
"In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers."
Other word predictions tasks have also con-structed manually the list of confusion sets[REF_CITE]and justifications where given as to why this is a reasonable way to construct it.[REF_CITE]present a similar task in which the con-fusion sets generation was automated.
Their study also quantified experimentally the advantage in us-ing early classifiers to restrict the size of the confu-sion set.
"Many other NLP tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers."
"In several of these cases the number of classes could be very large (e.g., pos tag-ging in some languages, pos tagging when a finer proper noun tag is used)."
The sequential model sug-gested here is a natural solution.
"We study the problem of learning a multi-class clas-sifier,  ,where  !#&quot;$ and % is typically large, on the order of &amp;)(*+ ."
We address this problem using the Sequential Model (SM) in which simpler classifiers are sequentially used to filter subsets of out of consideration.
"The sequential model is formally defined as a , -tuple: -/. $/ 2 :;&lt; 2 = where & gt; 2 is a decomposition of the do- @6A?  "
CE main D  2 (not  necessarily). disjoint; it could be that &gt; is the set of class labels. &gt; 7H!J  !
J which the classifiers &amp; A aredetermineslearned andtheevaluatedorder in.
"For convenience we denote ! &amp;  &gt; 2 A is the set of classifiers used by the model, 2 TSU 2 X6VZY9\[]_4V^ X6V . & gt; &lt; 2  is a set of constant thresholds."
"Given D `bac [Footnote_2] and a set 2Ud of class labels, [Footnote_1] the e th classifier S f  outputs   a mh&quot; probability `nYRY over distributionlabels in (where 2 f 2 [Footnote_2] `nY e is the probability [Footnote_2] assigned to class by 2 ), and 2 satisfies that if aqo 2Ud then f [Footnote_2] `nY/ ."
2 We use the terms class and target interchangeably.
"1 The output of many classifiers can be viewed, after appro-priate normalization, as a confidence measure that can be used as our xWy ."
2 We use the terms class and target interchangeably.
2 We use the terms class and target interchangeably.
2 We use the terms class and target interchangeably.
The set of remaining candidates e after the th clas-sification stage is determined by 2 and &lt; 2 : 2 f 2 Sg:h `iYvuw&lt; [Footnote_2]
2 We use the terms class and target interchangeably.
"The sequential process can be viewed as a mul-tiplication of distributions.[REF_CITE]argues that a product of distributions (or, “experts”, PoE) is an efficient way to make decisions in cases where several different constrains play a role, and is ad-vantageous over additive models."
"In fact, due to the thresholding step, our model can be viewed as a se-lective PoE."
"The thresholding ensures that the SM has the following monotonicity property:  2 Sl`nYvuw&lt; 2 ){f 2Ud :h`iYvuw&lt; 2Ud # that is, as we evaluate the classifiers sequentially, smaller or equal (size) confusion sets are consid-ered."
"A desirable design goal for the SM is that, w.h.p., the classifiers have one sided error (even at the price of rejecting fewer classes)."
"That is, if #| is the true target 2 , then we would like to have that f 2 _| h `iY}u~&lt; 2 ."
"The rest of this paper presents a concrete instantiation of the SM, and then pro-vides a theoretical analysis of some of its properties (Sec. 5)."
"This work does not address the question of acquiring SM i.e., learning &lt; [Footnote_2] :_7 ."
2 We use the terms class and target interchangeably.
"This section describes a two part experiment of pos tagging in which we compare, under identical con-ditions, two classification models: A SM and a sin-gle classifier."
Both are provided with the same input features and the only difference between them is the model structure.
"In the first part, the comparison is done in the context of assigning pos tags to unknown words – those words which were not presented during train-ing and therefore the learner has no baseline knowl-edge about possible POS they may take."
This ex-periment emphasizes the advantage of using the SM during evaluation in terms of accuracy.
The second part is done in the context of pos tagging of known words.
"It compares processing time as well as accu-racy of assigning pos tags to known words (that is, the classifier utilizes knowledge about possible POS tags the target word may take)."
This part exhibits a large reduction in training time using the SM over the more common one-vs-all method while the ac-curacy of the two methods is almost identical.
Two types of features – lexical features and contextual features may be used when learning how to tag words for pos.
Contextual features cap-ture the information in the surrounding context and the word lemma while the lexical features capture the morphology of the unknown word. [Footnote_3] Several is- sues make the pos tagging problem a natural prob-lem to study within the SM. (i)
3 Lexical features are used only when tagging unknown words.
"A relatively large number of classes (about 50). (ii) A natural decom-position of the feature space to contextual and lexi-cal features. (iii) Lexical knowledge (for unknown words) and the word lemma (for known words) pro-vide, w.h.p, one sided error[REF_CITE]."
"The domain in our experiment is defined using the following set of features, all of which are computed relative to the target word  2 ."
Contextual Features (as[REF_CITE]):
"Let  2Ud    be the tags of the word preceding, (following) the target word, respectively. 1.  2d . 2.  2  . 3.  2d &amp; . 4.  2  &amp; . 5.  2d   . 6.  2d &amp; z 2Ud . 7.  2    &amp; . 8."
Baseline tag for word  2 .
"In case  2 is an unknown word, the baseline is proper singular noun “NNP” for capitalized words and common singular noun “NN” otherwise. (This feature is introduced only in some of the experiments.) 9.The target word  2 ."
Lexical Features: Let   be any three characters observed in the examples. 10.
"Target word is capitalized. 11.  2 ends with  and length(  2 Y uw . 12.  2 ends with  and length(  2 YuL . 13.  2 ends with E and length(  2 Y uG, ."
"In the following experiment, the SM used for un-known words makes use of three different classifiers  &amp; and  or  , defined as follows:   : a classifier based on the lexical feature  . : a classifier based on lexical features ( &amp;  :  a classifier based on contextual features ( .  : a classifier based on all the features, ( ."
The SM is compared with a single classifier – either  or  .
Notice that  is a single classifier that uses the same information as used by the SM.
Contextual and Lexical features in a Sequential Model.
The input for capitalized classifier has 2 values and therefore 2 ways to create confusion sets.
"There are at most  &amp;F    ! +  different in-puts for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may emit up to   R&amp;   "
R+ confusion sets.
"All the classifiers in the sequential model, as well as the single classifier, use the SNoW learn-ing architecture[REF_CITE]with the Winnow up-date rule."
"SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tai-lored for learning in domains in which the poten-tial number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features."
SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.
SNoW has already been used successfully on several tasks in natural language processing ([REF_CITE];
"Specifically, for each class label SNoW learns a function  [ _^ that maps a feature based representation ` of the input instance to a number   a[]#^ which can be interpreted as the prob- ability of being the class label corresponding to ` ."
"At prediction time, given `3a , SNoW outputs / -  J&apos;¡SU`nY/%  `    Sl`iY_: (1)"
"All functions – in our case, , target nodes are used, one for each pos tag – reside over the same feature space, but can be thought of as autonomous functions (networks)."
"That is, a given example is treated autonomously by each target subnetwork; an example labeled  is considered as a positive exam-ple for the function learned for  and as a negative example for the rest of the functions (target nodes)."
The network is sparse in that a target node need not be connected to all nodes in the input layer.
"For ex-ample, it is not connected to input nodes (features) that were never active with it in the same sentence."
"Although SNoW is used with , different targets, the SM utilizes by determining the confusion set dy-namically."
"That is, in evaluation (prediction), the maximum in Eq. 1 is taken only over the currently applicable confusion set."
"Moreover, in training, a given example is used to train only target networks that are in the currently applicable confusion set."
"That is, an example that is positive for target  , is viewed as positive for this target (if it is in the con-fusion set), and as negative for the other targets in the confusion set."
All other targets do not see this example.
The case of POS tagging of known words is han-dled in a similar way.
"In this case, all possible tags are known."
"In training, we record, for each word  2 , all pos tags with which it was tagged in the training corpus."
"During evaluation, whenever word  2 oc-curs, it is tagged with one of these pos tags."
"That is, in evaluation, the confusion set consists only of those tags observed with the target word in train-ing, and the maximum in Eq. 1 is taken only over these."
"This is always the case when using  (or  ), both in the SM and as a single classifier."
"In training, though, for the sake of this experiment, we treat  (  ) differently depending on whether it is trained for the SM or as a single classifier."
"When trained as a single classifier (e.g.,[REF_CITE]),  uses each  -tagged example as a positive exam-ple for  and a negative example for all other tags."
"On the other hand, the SM classifier is trained on a  -tagged example of word  , by using it as a posi-tive example for  and a negative example only for the effective confusion set."
"That is, those pos tags which have been observed as tags of  in the train-ing corpus."
The data for the experiments was extracted from the Penn Treebank WSJ and Brown corpora.
The train-ing corpus consists of  words.
"The test corpus consists of  words of which , are unknown words (that is, they do not occur in the training corpus. (Numbers (the pos “CD”), are not included among the unknown words)."
Table 1 summarizes the results of the experiments with a single classifier that uses only contextual fea-tures.
Notice that adding the baseline POS signifi-cantly improves the results but not much is gained over the baseline.
The reason  is that the baseline feature is almost perfect ( ]5¥ ) in the training data.
"For that reason, in the next experiments we do not use the baseline at all, since it could hide the phenomenon addressed. (In practice, one might want to use a more sophisticated baseline, as[REF_CITE].) contextual and lexical Features (accuracy in per-cent).  is based only on contextual features,  is based on contextual and lexical features."
SM( 2 _#§ ) denotes that § follows 2 in the sequential model.
Table 2 summarizes the results of the main exper-iment in this part.
"It exhibits the advantage of using the SM (columns 3,4) over a single classifier that makes use of the same features set (column 2)."
"In both cases, all features are used."
"In  , a classifier is trained on input that consists of all these features and -/. chooses  _ _ a  Y labelthe samefrom featuresamong allareclassusedlabelsas input."
"In, but different &amp; classifiers are used sequentially – using only part of the feature space and restricting the set of possible outcomes available to the next classifier in the sequence – 2 chooses only from among those left as candidates."
"It is interesting to note that further improvement Given that the last stage incan be achieved, as shown in -/ the .   right :_ most  Y columnis iden-. tical to the single classifier  , this shows &amp; the con-tribution of the filtering done in the first two stages using and &amp; ."
"In addition, this result shows that the input spaces of the classifiers need not be dis-joint."
"In this section, we discuss some of the theoretical aspects of the SM and explain some of its advan-tages."
"In particular, we discuss the following issues: 1."
"Domain Decomposition: When the input fea-ture space can be decomposed, we show that it is advantageous to do it and learn several clas-sifiers, each on a smaller domain. 2."
Range Decomposition: Reducing confusion set size is advantageous both in training and testing the classifiers. (a) Test:
Smaller confusion set is shown to yield a smaller expected error. (b) Training:
"Under the assumptions that a small confusion set (determined dynam-ically by previous classifiers in the se-quence) is used when a classifier is eval-uated, it is shown that training the classi-fiers this way is advantageous. 3."
Expressivity: SM can be viewed as a way to generate an expressive classifier by building on a number of simpler ones.
"We argue that the SM way of generating an expressive clas-sifier has advantages over other ways of doing it, such as decision tree. (Sec 5.3)."
"In addition, SM has several significant computa-tional advantages both in training and in test, since it only needs to consider a subset of the set of can-didate class labels."
We will not discuss these issues in detail here.
Decomposing the domain is not an essential part of the SM; it is possible that all the classifiers used ac-tually use the same domain.
"As we shown below, though, when a decomposition is possible, it is ad-vantageous to use it."
"It is shown in Eq. 2-7 that when it is possible to decompose the domain to subsets that are condition-ally independent given the class label, the SM with classifiers defined on these subsets is as accurate as the optimal single classifier. (In fact, this is shown for a pure product of simpler classifiers; the SM uses a selective product.)"
In the following we assume that sA provide a decomposition of the domain (Sec. 3) and that SU`  A Y¬aLSU  A Y .
By condi-tional independence we mean that
CED   § h/ ¯ § fSU` ° h# ° B±2 where ` ° is the input for the ² th classifier. ! µv¶} :h `iY/ !µv¶F¸ }X   `    A Y (2) F¸ X  }¶  f¹Sl`     (3) F¸ X f¹SU`   A Y ³ ´Rµ }¶  f¹Sl`    
A h  (4) F¸ X ³ ´Rµ ¶} f¹Sl` h  A h  (5) F¸ X ³ R´ µ ¶}³ · :h ` Ylf¹Sl` Y  :h `EA1YUfSU`±A=Y »! F¸ X   (6)  }¶  :h ` :h ` A Y6» (7) f lS KY
A d F¸ X  f¹Sl`    A Y in Eq. 3 is identical C  and there-fore can be treated as a constant.
Eq. 5 is derived by applying the independence assumption.
Eq. 6 is de-rived by using the Bayes rule for each term :h ` 2 Y separately.
"We note that although the conditional indepen-dence assumption is a strong one, it is a reasonable assumption in many NLP applications; in particu-lar, when cross modality information is used, this assumption typically holds for decomposition that is done across modalities."
"For example, in POS tag-ging, lexical information is often conditionally in-dependent of contextual information, given the true"
"POS. (E.g., assume that word is a gerund; then the context is independent of the “ing” word ending.)"
"In addition, decomposing the domain has signif-icant advantages from the learning theory point of view[REF_CITE]."
"Learning over domains of lower dimensionality implies better generalization bounds or, equivalently, more accurate classifiers for a fixed size training set."
The SM attempts to reduce the size of the candidates set.
We justify this by considering two cases: (i) Test: we will argue that prediction among a smaller set of classes has advantages over predicting among a large set of classes; (ii) Training: we will argue that it is advantageous to ignore irrelevant examples. 5.2.1 Decomposing the range during Test The following discussion formalizes the intuition that a smaller confusion set in preferred.
Let  ½ be the true target function and  § h `nY the probability assigned by the final classifier to class  given example `¿ap .
"Assuming that the prediction is done, naturally, by choosing the most likely class label, we see that the expected er-ror when using a confusion set of size ² is:"
J Á ° ÀÃÂ [ÄS   %  `  § h`nYRY=Ç ^ !
Æ °     %  `  § h`nYRY=Ç !Y (8) !Æ §
Now we have:
Claim 1 Let *  ° *  ! °  ¨ for example ` .
Thenbe two sets of class À labels $ J Á °Ì and )À assume  J Á _°
Í . 6US `ntY a0É
Denote: fTÎ5S     %  `  § h`iY!=Y Ç !
Y  § ÆEÐ
À $ J ÁKÑ Í
ÀÃÂ [ S   %  `  § h`nYRY=Ç {^ !
Æ ° ¨ ! $Ô Á 
À)!Á Ñ ÔÕS!ËfTÎ5S( )À  !
Á _Ñ _! $Á Ô
J ÔrS!( J YlfTÎ5Sk²$)Ô
Ö À ) J Á Ñ
"Claim 1 shows that reducing the size of the con-fusion set can only help; this holds under the as-sumption that the true class label is not eliminated from consideration by down stream classifiers, that is, under the one-sided error assumption."
"Moreover, it is easy to see that the proof of Claim 1 allows us to relax the one sided error assumption and assume instead that the previous classifiers err with a prob-ability which is smaller than: ( À$ J Á Ñ Y6»RfTÎ5S{$² $Ô Á"
"We will assume now, as suggested by the previous discussion, that in the evaluation stage the small-est possible set of candidates will be considered by each classifier."
"Based on this assumption, Claim 2 shows that training this way is advantageous."
"That is, that utilizing the SM in training yields a better classifier."
Let × be a learning algorithm that is trained to minimize: Ø
"Â  SlÜ »KÝ¹Sl`iY!YUfSU`nYÞ: where ` is an example, ( _ is the true class, Ý is the hypothesis, is a loss function and f¹Sl`iY e is the probability of seeing Ú example ` when `sà (see[REF_CITE]). (Notice that in this section we are using general loss function ; we could use, in particular, binary loss function Ú used in Sec 5.2.)"
"We phrase and prove the next claim, w.l.o.g, the case of vs.  class labels."
"Claim 2 - Let  &amp; ! be the set of D class la-bels, let 2 be the set of examples for class ."
Assume a sequential model in which class  does not - com-pete with class  .
"That is, whenever `áa the SM filters out  such that the final classifier ( A ) considers only  and &amp; ."
"Then, the × (errorfor of)the- whenhy-pothesis - produced by algorithm -  - &amp; is no larger A than trained on examples in the error produced by the hypothesis - - -  it.produceswhen trained on examples in &amp;"
"Assume that - the algorithm × , when trained on a sample , produces a - hypothesis that minimizes the empirical e error over ."
Denote `Ûà X when ` is sampled according to a distribution - that supports only examples with label in .
"Let be a sample set of size % , according to e !â&amp; , and  the hypothesis produced by × ."
"Then, for all ÝÛÇ Ý  , % ã  Ú SlÜÝ  SU`nYRY Ì %pãÂ  Ú !Y (9) äÂ"
"In the Ø limit, as æ% qç Ø"
SUÜÝ  Sl`iY!YUf¹Sl`iYFÞ:` Ì ÂKèié Ú !
ÂKèié {O Úê Q {ê QO
"In particular this holds if Ý- is a hypothesis pro-duced by × when e  trained &amp; â  . on  , that is sampled ac-cording to `ëà"
"The SM is a decision process that is conceptually similar to a decision tree processes[REF_CITE], especially if one allows more general classifiers in the decision tree nodes."
In this section we show that (i) the SM can express any DT. (ii) the SM is more compact than a decision tree even when the DT makes used of more expressive internal nodes[REF_CITE].
"The next theorem shows that for a fixed set of functions (queries) over the input features, any bi-nary decision tree can be represented as a SM."
Ex-tending the proof beyond binary decision trees is straight-forward.
Theorem 3 Let ì be a binary decision tree with  internal - nodes - .
"Then, there exist a sequential model such that and ì have the same size, and they produce the same predictions."
"Proof (Sketch): Given a decision tree ì on  nodes we show how to construct a SM that produces equivalent predictions.  1. Generate a confusion set the consists of classes, each representing an internal node in ì . 2."
"For each internal node in Þta3ì&quot; , assign a clas-sifier: 2 &apos;\[]#^ d nï . 3. Order the classifiers  A such that a clas-sifier that is assigned to node Þ is processed before any classifier that was assigned to any of the children of Þ . 4. Define each classifier 2 that was assigned to node  ì to have an influence on the outcome iff node"
Þða ì lies in the path ( Ï_! !
"Ï ° d ) from the root to the predicted class. 5. Show that - using steps 1-4, the predicted target of ì and are identical."
This completes that proof and shows that the result-ing SM is of equivalent size to the original decision tree.
"We note that given a SM, it is also relatively easy (details omitted) to construct a decision tree that produces the same decisions as the final classifier of the SM."
"However, the simple construction results in a decision tree that is exponentially larger than the original SM."
Theorem 4 shows that this difference in expressivity is inherent.  Theorem 4 Let be - the number of classifiers in a sequential model and the number of internal nodes a in decision tree ì - .
Let % be the set of classes in the output of and also the maxi-mum ñ Slì= degree ñ S - Y theof thenumberinternalof functionsnodes in ì representable.
Denote by by  - respectively.
"Then, when ñ òu% )u  , ñ S - Y is exponentially larger than Slì=Y ."
"Proof (Sketch): The proof follows by counting the number of functions that  can be represented using a decision tree with internal nodes[REF_CITE], and the number of functions that  can be rep-resented using a sequential model on intermedi-ate classifier."
"Given the exponential gap, it follows that one may need exponentially large decision  trees to represent an equivalent predictor to an size SM."
"A wide range and a large number of classifica-tion tasks will have to be used in order to perform any high level natural language inference such as speech recognition, machine translation or question answering."
"Although in each instantiation the real conflict could be only to choose among a small set of candidates, the original set of candidates could be very large; deriving the small set of candidates that are relevant to the task at hand may not be immedi-ate."
"This paper addressed this problem by developing a general paradigm for multi-class classification that sequentially restricts the set of candidate classes to a small set, in a way that is driven by the data ob-served."
"We have described the method and provided some justifications for its advantages, especially in NLP-like domains."
Preliminary experiments also show promise.
Several issues are still missing from this work.
In our experimental study the decomposition of the feature space was done manually; it would be nice to develop methods to do this automatically.
"Bet-ter understanding of methods for thresholding the probability distributions that the classifiers output, as well as principled ways to order them are also among the future directions of this research."
£O¢¡¤£O?¥¢¦..§ ±·´¡¨£O©ª|² £O{£c´Q©µ§ ¶©µ´\©{|«s±T¹`´|®¢²Q±1­T§O³²Q±1¡ | |´ :£c»*¸µ±:£c»*¸µ±¶´|.³  ®¾­T§ . ® ±p²Q±1d´¯ºl²Q§ ¡ .§ `¿(­T§ ?\´ |« ±1d´|±1{?´|| Ã §O² ¦`¿L»*£O«|±1¦¼«Q´Q£c´Q©ª«|´Q©ª­p£O¸vºl±1£c´Q¬.²Q±1« Ã  ®Ä¡ ±1£O« .¬ ²Q± |« ±1¡¨£O . ® ±p²Q±1·»]|±1¦u´|§s±1* |±1.±p±1¦B´|.±p² £c´|±1¦5§O²v²Q±1­T§O³Ã  . ® *±p²Q±1©µÇp±1¦$ÀvÁ¯±|±1`¿ £Od¥Å¸ª£O.³ ¬**¸ª©ª­p£c´Q©ª§ ´ ´ ² £O©ª5£·¦.±1­p©ª«Q©ª§ h´|²Q±p±¨¬*«Q©ª.³u´Q®.\± ­T§ *«||´ ² |±1¦5ºl±1£8¿| ´Q¬­T§.²Q±B«®.±p²Q±1|±p´¨´|.§O´pÀIÁ¯±sÉ|§ ¡¨£c´Q©ª­p£O¸ª¸µ¥]:­p¸ª£O« «Q©µºl¥È«.¬ ²v­T§|±1d´|±1*{©  §Oº]]» §D§ «||´  \´|²Q±p±1«3£O . ® ±p²Q±1* £O­ *® ©µ±pÊO±1«Ë£O¯£O­p­p¬.² ?Î Ã .® ±1|´Q©{.³ .¿ ©ª.³ |´ ² ©ª³O² £O¡ ¿L³O±1.±p² £c´|±1¦B«|±1d´|±1.§ .(¿ ­T§ . ® ±p²Q±1 ºl²Q§ ¡Ñ´Q®.§ «|± ©ª Ò²Q§ £O¦*­p£O«|´ËÓI± Ã *£c´Q£O«|±p´¨Ï­T§c¿ .±p²Q±1® Ô Õ ÖNO) =  (K 47)  G§O² . ¦ ±p²Ø´| ?ÊO±1d´Q ©µ§ |´Q£c´Q ©ª«Q´Q©ª­p£O¸ ¸ª£O.³ *¬ £c³O±Ü¡  Ã ±Ü.±p±1¦Þ´|§ÄÉt*¦Ø£O«|
"=Í.ÎÏÐÈÎÑ,* .ÕÌÐÈÃ×:ÐÔ×, =ÐÚÖ ÐÈ×É&amp;ÝÓ5ÀàÏÐçæ .  "
"Å.= ,, &quot;&gt;é ÄÆÃ5ÏZäïËØÄÆÃÛäïé&gt;.,ÐÈ× ÇÈÄÞé_á \  ÖÛÝÃÕÆÇ]ËØÁÕÞÖÛÝÃÛÁ×&quot;@,ÅZß _.&gt;Á"
ÀÄÞé ÖÛÀÁ&quot;ËØÁÕÞÖÛÝÃÛÁ× ÐÈÉÂÃÛÄÞÙjÁ_ÖÛÀÁ&quot;&amp;&amp;  %\:ÂÃÛÄÞÙZÐÔÏÁ ÖÛÀÁ_ÐÚÎËØÄjÃÛÉ:ÕÌÖÛÐÈÄÆÎ _*
"ÂÕÌÃÛÖ×MäïÄÆËòä ×ÛÂ.Á&apos;Á&apos;Ó5À\ó{ô&gt;,ÖÛÃ5ÕÌÎ×ÛÇÈÕÌä ÖÛÐÈÄÆÎá;:ÕÞÒÐÈÉ&quot;ÝÉ%\. ×MÖÛÃÝÓÖÝÃÕÆÇËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×]&amp; &amp;Å.ÁÏÏÁÏ\ÐÈÎ\{á ûüÖ:ÖÛÝÃÛÎ×ÄjÝÖÖÛÀÕÌÖ:  :ÄÆËpëpÄjÃÛÁÕÌÎ¼ÕÌÎÏ¼ì]"
ÇÈÐÈÑÆÎÁÏ  ÕÆÃÛÁ .Ý \ËØÄjÃ$ÖÀÁ
"ÏÁ&apos;ÃÛÐÈÙÞÕÞÖÐÚÄjÎ ÄÌË : ËØÄjÃíÉ:ÕÌä Ó5ÀÐÈÎÁèÖÛÃ5ÕÌÎ×ÛÇÔÕÞÖÛÐÈÄÆÎKÕÌÎÏ%.ÕÌÑÆÁ× ÐÚÎËØÄjÃÛÉ:ÕÌÖÛÐÈÄÆÎ  ÇÈÄÌÖpÄÌË&gt;ÕÌÂÂÃÄjÕjÓ5ÀÁ&apos;×0ÀÕuÙÆÁ&amp;Å. ÑÆÁ×êÖÁ&apos;Ï%ÖÄ²Í.ÎÏ \.ÄjÎÏÐÈÎÑ &gt;é ÄÆÃ5Ï%.× ó¬ù&gt;&amp;ÕÌÇïáÈå  yù&gt;_&quot;ÕÆÇ{áÈå     Æ÷åÂÀÃÕj×MÁ óïöZÀÐÈÎ\å    ó¬ëpÝä ÂÐÈÁ&apos;ÓÆå  {öZÉ:ÕÆÏ MÕ&amp;å    & gt;ÄÆÃýZ×sÀ. ËØÄÆÃ]&gt;.ó pÕÌÇÈÁ ÕÆÎÏ FÀZÝÃÓ5Àå  ÕÞÖÐÚÄjÎ,ÆâÝÎÑÉÁ&apos;Õj×MÝÃÛÁ×FÓ&apos;ÕÌÎ&quot;ÕÌÎÏ ,ÅRÁpÉÐÈ×ÛÇÚÁÏ  ,Z÷á 0ÄÞé&gt; &quot;é&gt;ÄÆÃ5Ï ÐÈÎíÕ&quot;.],ÉÄÆÃÁ ÖÛÀ.&gt;ÄÆÃ5Ï  ÐÈÎÏÐÈÃÛÁÓÖ Î ó{å   Æ÷á :ÕÌÖÛÐÈÄÆÎ 5. 4%6B0&quot;! % # 2 $%54$. &amp;%(*&apos;6C0,) + *25D./+10&amp;5[REF_CITE]+ =) 4[REF_CITE]. %=&amp;5HI&amp;=G=.#%9J;) &lt;: ;4&lt;: %4 +%6 ))=)=+?&gt; %# =+) #4B7.5@ 2 +**954. .54%%6B0&apos;*@A/. +25D:&lt;&amp;(4 Q* &lt;: @&lt;@ 5. &lt;@ *&apos; &amp;;GN&amp;5HR&amp; =G =G #%9S&gt; (&amp; S) +*.54T) $ )NGL+ =) *54. %=G )XW Q %- :&lt;GY-T:&lt;&apos;9 )=$%=) &apos;*)=4Z+ )=6B5. ] [ \_^ F`[ &quot;b5cSeNe=_f bZgd[ %c5gf[ hM\"
"ÖÛÁÓ5ÀÎÐÔøgÝÁ&apos;×:.Å  %ikjml   &gt; ? å   ,&gt;:Ý×ÛÝÕÌÇÈÇÚß    =ëpÝÂÐÈÁ&apos;ÓÌ$åÐÈÎKÖÛÀÁ  @=ìFú:ÕÆÇÚÑjÄÆÃÐçÖÀÉ9óÏ on { q å  r FõAÓ5À\ÕÌÇïáÚå &gt;ÕÌÇïáÚå _ r á s 0ÄÞé&gt;&gt;,×ÛÄÆÉÁ ÏÐÚæ    .ÕÆ×ÛÁ&apos;Ï  "
"ÇÔÕÌÎÑjÝÕÌÑjÁWÂÕÆÐÚÃá \  ÖÐÚÉÁ  :ÕÞÖÛÐÈÄÆÎKÕÌÎÏ²ÀÐÈÑÆÀ% &amp;:×&amp;Õj×Û×ÛÝÉÁ&apos;Ï@{ÖÄÌäüÄÆÎÁ, ×ÛÂ."
ÐÚÖêßÆ t á 0ÄÞé&gt;&gt;.
"ÖÃÕÆÎ×Mä ÇÔÕÞÖÁ&apos;Ï &quot; ..ÄjÝÎÏ&amp;ÎÄÆÝÎ.&amp;ÕÆÉqä &gt;ÄÆÃ5Ï×   ÏÁ&apos;ÎgÖÄjÎ@ÖÀÁ @ÃÁ&apos;øgÝÐÈÃÁ, &quot;:×pÝ×ÛÁÇÚÐÚÖMÖÇÚÁ ÄjÃ_ p ÁÏÑÆÁ:ÖÄÜ×êÖÃÛÝ.ÓÖÛÝÃÛÁ:ÖÀÁ ÝÎ. ÏÐÔ×MÖÛÄÆÃÛÖÛÐÈÄÆÎqÂÃÄÆÅÕÆÅÐÈÇÚÐÚÖêß&quot;ÕÌÎ.&quot;ÂÃÛÄjÅÕÌÅÐÈÇÚä .ÏÐÚÎÑ é&gt;ÄÆÃ5Ï\._Õíé&gt;Á&apos;ÕÌý ,&gt; ÇÔÕÌÎä"
ÑjÝÕÌÑjÁ&apos;×pÕÌÎ u Ï v Î É: ×pÖÛÀÁ: ×ÛÝÇÚÖ&apos;å&gt;ÇÚÄÆÖ×qÄÌËAÝÎÑÆÃ5ÕÌÉÉ:.:ÕÆÎÏ ÖÄZÄ :É ÕÌÎZßÜÂ..
"Á:Á×êÖÐÚÉ:ÕÌÖÛÁ&apos;Ï ÕÆÃÛÁÕÌÇÈÇÚÄÞé&gt; ,:É ÕÆÎgß éFÄjÃÏ× ÕÆÃÛÁ ÕÆÇÚÐÈÑÆÎÁÏ ÖÛÄ%ÖÛÀÁ Á&apos;ÉqÂÖêßAé&gt;=ÖÛÄ ÖÛÀÁ&gt;ÄÞÙÆÁ&apos;ÃMÍÖMÖÛÐÈÎÑ0ÂÃÛÄjÅÇÚÁ&apos;É óüõAÓ5À  ÕÌÇïáÚå  g÷á & gt;ÖÄ&amp;Á×êÖÐÚÉ:&quot;ÙjÄÆÇÈÝÉÁqÄÆËFÅÐÈÇÈÐÚÎÑjÝÕÌÇ]ÂÃÛÄjÂ.=ÕÆÇÚÐÈÑÆÎÁ&apos;Ï."
"Á&apos;Ã&apos;åsé&gt;&amp; w x_]y zZ{ d |} ~  \ y]  |  }k  c yR  |&quot;5(}^ y  |  { *{  } C\ :&lt;&apos;*9 )=+4%%GY-?) EF)=&amp;(+ :&lt;*9 &lt;@: IVB+&quot;WZcP$%(&amp; .5U%&lt;: &lt;@ : IVB+ +/. + 54h4. %( @&lt;&lt;: &apos;*- Q &amp;((=) %4 )=9*/. +*)=&apos;  (&lt;@ &lt;: &apos;*- Q&amp;(s(=) %4 :&lt;)=&apos;*9 +*./*-%+ ) =) &apos;Q + &amp;%()9*,6 5.&lt;)@&lt;: (%4 &gt; =) (&amp;4Z9*+ 6 %$ (&amp;54. 5U.%%61:&lt;&lt;@ B:^"
IV1+ :&lt;&apos; + *+ /. ) + %+*%-&lt;: &apos;H) + &amp;(+ :&lt;&amp;(4$Q *(&amp; .5U%:&lt;&lt;@ : ++*-%./+ .54h4% (&lt;@ &lt;: &apos;*- Q Q&amp;(9*:&lt;41 .PGN)=9+Y.5:&lt;41$ &amp;(&apos;*: + :&lt;(&amp; 4
"Q :&lt;&lt;@ @ (=) %4 )=*9 ./*+ ) .  ) Q &amp;(,&lt;: 4;. ==) *+ .5&lt;: 4k$ &amp;(&apos;*: + &lt;: (&amp; Z/  ¡ § /¦ /©Sª « &quot;¤¥L¦&quot;*¬Z­ &quot;¨   (¬ &quot;¨  *± ± &quot;¨ ³´ · /£ ¤¡ &quot;®®¹Xº &quot;, (µ ¶L¨N®¯*¾N­gµ(*« ( «Y¾YÀ8¨&quot;"
Â  Â &quot;¦ /Ä =¥ ¨NÅÆ ¹X£&quot;&quot; ©SªsÍ ÎJ£(¸Z¦/&quot;»ZÏ8   ¦/¤IÐ/ ²AÈYÉ&quot;¼%&quot;¬ZÊ¥L¤hZ£&quot;/»Z% *¹hÍÒ5¥L¤¡Ó&quot;®¯%° ¼Z¦/¤
Â  §  ¥  %¥LZ¸Z¥L Â &quot;% &quot; ¦ ¸&quot;Ô¡¥ § Â  Ò/&quot; Z8Ñ ®®Á]® Í ¼Z¤¡£/¼%¥L¤ Z£/ Ò/ ° Í ¦/&quot;ÎJ Z¦/% ;¿ ÕÌÅÇÈÁ  ..=ÎÑÆÇÈÐÈ×ÛÀ_ÕÌÎÏ_ëpÄÆä ÃÁ&apos;ÕÌÎ  .
"ÕÆÇÚÐÈÑÆÎä × Ø G ÚÙ C W½D= ; » Û êCL a PÜÝÛ D êC  0JM¾sY JÛ &quot; Û Y ÛJêd]EWL CEFX JMEªH²D] àß  qÛ ÛJMaZG »WJ­XC  . ]ÐÚÎ  &quot; q å _ =×MÝ.Ó5À&amp;ÕÆ×*ëpÄÆä ÃÁ&apos;ÕÌÎðÕÌÎÏðì=ÎÑjÇÚÐÔ×MÀá  {:á N ó   ÖÛÀÁ&gt;*]&amp;ëpÄjÃÛÁÕÌÎ ÕÌÎ.:Ï ì=ÎÑjÇÚÐÔ×MÀ:]ÕÆÂÂÃÄuÒZÐÈÉ: X ß  ÕÆÎÏÉ:ÕÌÎZßgä ÖÛÄÆäïÉ:ÕÆÎgßpÉ:ÕÌÂÂÐÚÎÑj×; X Ï  ²ÐÈÎ&amp; ù&gt;Á&apos;×ÛÐÈÏÁ&apos;×&apos;åyëpÄÆÃÁ&apos;ÕÆÎèÕÆÎÏ ì=ÎÑjÇÚÐÔ×ÛÀ ×ÛÀÄÞé É&quot;ÝÓ5À  ÐÈÎ :_× &quot;ÝÖÝÕÌÇ]ÖÛÃ5ÕÌÎ×ÛÇÈÕÌÖÛÐÈÄÆÎ ÕÆ×&quot; ÐÈÎ ; Á Æá¿ ÕÌÅÇÈ 3 .&gt;];:û­Î,ÕÌÖÛÐÈÄÆÎ :ÕÞÖÐÚÄjÎàÓ&apos;ÕÌÎàÅ.@ÅZß ÖÛÀÁ]ËØÄjÃÛÉãÄÆËÅÐÈÇÚÐÈÎÑÆÝ..= ÖÛÝÃÛÁ\ó¬ñèÕÌÖÕÆÎÕÌÅRÁ,  å    ÖÛÝÃÛÁ: I ó â ]ÕÆÉ:ÕÌÉÄÌÖÄ h å  g÷á ¿ÄÍÎ.,Ï .&gt;"
Õ .Ó&apos;ÕÞÖÐÚÄjÎäïÅ.:ÕÌÃÜó¬ú\ÕÞÖMä ×ÛÝÉÄÌÖÛÄ.å  g÷;ÄÆÃFÅÐÚÇÈÐÈÎÑÆÝÕÆÇÑjÃÕÆÉqÉ:  å _ Ì÷; ÅRÁ]Ý×ÛÁ&apos;Ïá*]ÖÛÃß ÖÄ0ÍÎÏ&quot;:ÕÞÖÓ5À&quot;ÄÌË _*.
ÍÃ5×êÖá J 0ÄÞé&gt;.ÕÌÇÈßZ×ÛÐÔ×]ÄÆËÁ&apos;ÕjÓ5À   Õ ÀÕÌÃ5Ï_ÕÌÎ._Ï   û­Î\ÖÀÐÈ×AÂÕÆÂ.Á&apos;Ã&apos;åé&gt;ÁqÂÃÛÄjÂ.Äg×MÁqô&gt;õpö\ &quot;.@ .*&gt;:ÕÌÖÛÐÈÄÆÎ_ ÐÔ×:ÐÚÉÂRÄÆÃÛÖÕÆÎjÖ ÐÈÎ$@     ñèÁ\. ËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×5÷;ÅZß&amp;.ÕÆ×ÛÁ&apos;ÏqÄÆÎqÖÛÀÁ úÜì$ËØÃÕÆÉqÁ&apos;éFÄjÃÛýsápâ; &amp;.Å   \Õ :]ÎÑÆÇÈÐÈ×ÛÀ ÕÌÎ.ÏðëpÄjÃÛÁÕÌÎ\ õWÝÃ_
ÕÆÏÙuÕÆÎgÖÕÌÑjÁ&apos;× ]ÖÀÁ0ÇÈÐÚÉÐÚÖÕÌÖÛÐÈÄÆÎÄÌËé&gt;ÄÆÃ5ÏZä{ËØÄjÃMäüé&gt;ÄÆÃ5Ï  ÕjÏÙÞÕÌÎgÖÕÆÑÆÁ  p ]×ÛÄÆÉÁ_.ÄjÝÖ ÖÀÁ&quot;ÇÔÕÌÎä ÑjÝÕÌÑjÁÆá* _&amp;Õ & gt;
"ÖÃÕÆÎ×ÛÇÈÕÌÖÛÐÈÄÆÎ ÅZß:&amp;ä ÐÈÎÕÞÖÐÚÎÑ\    =.Äg×Û×ÛÐÚÅÇÚÁAÖÛÄÁ×êÖÐÚÉ,Õ :ÕÞÖÁAÖÛÀÁ_ÂÃÄÆÅ:.ÕÆÇÚÇÕÌÅÐÈÇÚÐÚÖêß=×Û F Ð ã &apos;Á&amp;:ÄÆËÄÌË&gt;; ÂRÄÆÃ5Õá :ÕÞÖÐÚÄjÎ&quot;ÐÔ×; ×MÖÛÃÝÓÖÐÚÄjÎíÄÆË*ÕqÅÐÈÇÈÐÚÎÑjÝÕÌÇÑjÃÕÆÉÉÕÆÃ&apos;á ä åçæ  æ JêC  PÜ D ,XG CKL D 0C sÛ  ÕÆÉÄÆÎÑ¼ô&gt; .:;."
"ÐÚÎ@ëpÄjÃÛÁÕÌÎèÐÈ×pÖÃÕÆÎ×ÛÇÈÕÌÖÛÁ&apos;ÏðÖÄÜÕ ÎÄjÝÎ@ÄÆÃÕÜÎÄÆÝÎ@ ÅZß ,ÐÚÎàì=ÎÑjÇÚÐÔ×MÀá &gt;õpö:&gt;ÄÆÝÇÔÏ::ÕÌÖÛÐÈÄÆÎ ÐÈÎ _ \&gt;=ÕÆ×_Õ"
ÇÈÁ&apos;××   & amp;ÕÌÇÈÑÆÄjÃÛÐÚÖÛÀÉ ÖÛÀ.ÕÞÖ ÉÕÌÒÐÚÉ ]
"Ð ã \&amp;Å.Á&apos;ÃðÄÆË ÉÕÌÖÓ5ÀÐÈÎÑKô&gt;õpö%ÐÈÎ  ,ÕÌÇïáÚ , å   ÕÌÇÔ×ÛÄÜÕÆ××MÝÉÁÖÀÕÞÖ&amp;ÖÛÀÁ :"
"É ÕÆÂÂÐÈÎÑÜÐÈÎËØÄÆÃÉ:ÕÞÖÛÐÈÄÆÎ ÄÆË0ô&gt;õpö .]Õ:ÇÔÕÌÎÑjÝÕÌÑjÁpÂÕÌÐÈÃ \Õ: :ÕÆÓ5ÀÐÈÎÁAÖÛÃ5ÕÌÎ×ÛÇÔÕÞÖÛÐÈÄÆÎá $×MÐÈÉÐÚÇÔÕÌÃÐçÖÐÚÁ×@&gt;.ÄjÎÏÐÈÎÑ &gt;ô  ÐÚÎ  ×êÖÃÛÝ. é&gt;: é á 0ÄÞé&gt; &amp; ÐÚÎ ì] &amp;ÖÛÄ&amp;ÕpÖÛÄÆä ,_ÐÈÎÜëpÄÆÃÁ&apos;ÕÆÎíÕÆ×  Á v § ¦//©Sªëêíìî«Y¬Z­ ¨N®¯h°  (¬ ¨&quot; ² *± ± &quot;¨ ³´5©S³mµ(¶L¨N®¯h  "
"ÅZß¼ÖÀÁ .&gt;$Öêé&gt; ÑÆÝ. ëpÄÆÃÁ&apos;ÕÌÎ\ÐÈ×_ÕÆÎðÕÌÑjÑÆÇÈÝÖÛÐÈÎÕÌÖÛÐÈÙÆÁqÇÈÕÆÎÑÆÝÕÆÑÆÁjá  ÐÈÎ&amp;ëpÄÆÃÁ&apos;ÕÆÎ_.×MÐÔ×êÖ5×ÄÆËÕ =ÝÎÐçÖ5×yÓ&apos;ÕÌÇÈÇÚÁÏ  á Ä   .Äg×MÁÏ ÄÆËÕ &gt;ÄÆÃ5Ï%ÕÌÎÏ ËØÝÎ.ÓÖÛÐÈÄÆÎ%é&gt;ÄÆÃ5Ï×&apos;á _:É  _,Ä {á=_ &gt;é ÄÆÃ5Ï×0ÐÈÎèì]"
"ÖÄ Ä {áÁÆá ÖÛÀÁ×Ó5ÀÄZÄÆÕ, * Ç õ ]ÈåRÐÈÎðì]."
Çå ]ôô ö ÖÄ ÷ùø  * ó . ú   &quot;ñ åR×ÛÓ5ÀÄgÄjÇçäïÖÛÄZ ( ÷ õ $:ÕÌÂÂÐÚÎÑàô&gt; ..
ÄÌËRô&gt;;ÕÌÃÁ]ÄÆÅÖÕÌÐÈÎÁÏ_:ÕÞÖÐÈÓ]ËØÁ&apos;ÕÌä ÖÛÝÃÛÁ&quot; ÖÛÀÁ&quot;ú\&gt;ÄÆÃýRá .Õ ËØÁ&apos;ÕÌÖÛÝÃÁ .
ÖÕÆÑ . ... .&gt; Z × v ;&gt; :×MÝÂ. :
"ÕÆÃÛÁ @ËØÃÄÆÉ Õè×ÛÉ:ÕÌÇÈÇ0ÂRÄÆÃÛÖÛÐÈÄÆÎ &gt;:ÕÆÃÛÁ ÖÃÕÆÐÚÎÁÏàÅgß@ÖÛÀÁÜûÛûMö ÕÌÇÈÑÆÄjÃÛÐÚÖÛÀÉ ó¬ù&gt;_ÕÆÇ{áÈ P å  = P å    _ÕÌÃÁ_Ï  û ó  û ð 8ð  û /ÿ ð ×&apos;á û­;&gt;ËØÁ&apos;ÕÌÖÛÝÃÁ= ÖÛÃ5ÕÌÐÈÎÐÈÎÑÜÏÕÌÖÕá * ÇÔÕÌÃÑÆÁ_ÑjÕÆÐÚÎ ÖÄ:ÖÛÀÁ&amp;_ÍÎ.ÕÌÇÄjÝÖMä ÕÌÎ.Ï ÖÀÁ_..&gt;õpö: ð 8ð  û &gt;ÖÛÀÕÌÖ ÿ/  ð : _ÄÌË Cû    :  &quot;&quot;ÇÚÄZÄÆý ÕÞÖFÖÛÀÁAé&gt;&quot;.×MÝÃÁWÕ&amp; ÖÛÀÁ  ,ÐÈÎ@ & gt;é _&gt;ÐÔ×0ÕÆ×&gt;ËØÄjÇÚÇÈÄÞé0× û­ÎÂÝÖ    ¼ÄÆË*  Ö  ó  Æ÷_ËØÃÄÆÉ Õð×MÉ:..\ÄÌË Ö5ÕÌÑ  %ÅZß"
Ö p ÐÈÎjÖÄ:å :á   á FÄjÉqÂÝÖÛÁ ÖÛÀÁíé&gt; ÖÝÃÛÁ× :ûMûMö   á FÃÁ&apos;ÕÌÖÛÁpÕ&quot;ËØÁÕÞÖÝÃÛÁpÂRÄZÄÆÇ &quot;!  ;Â.Äg×Û×ÛÐÚä &amp;% ÂÕÆÐÚÃ5×&apos;á j ; # Ã !
ÊÝ×MÐÈÎÑ .\_ÕÆÎÏè×MÐÈÉÐÚÇÔÕÌÃÐçÖêß  $ À :á á  FÄjÉqÂÝÖÛÁWÖÛÀÁ_ÕÆÂÂÃÄuÒZÐÈÉ:ÕÞÖÁ ÑjÕÆÐÚÎ.×]
"ÐÈ % Î ! á  &apos; ó &amp; $ ÇÈÕÆÃÛÑjÁÑjÕÌÐÈÎ ÙÞÕÌÇÈÝÁ&apos;×&apos;åyÕÆÎÏ ÕjÏ ( Ï á )  á FÄjÉqÂÝÖÛÁ * + ó , ÷  : .0"
"ÖÀÁ / , / íó +  ] 5 ÷   á &quot;&quot;Ý×ÛÐÚÎÑ"
".ÄÞÙjÁÆågËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×&gt;:ÕÌÂä ÂÐÈÎÑj×AÄÆË&gt;&gt;ô &gt;ÄíÇÔÕÌÎÑjÝÕÌÑjÁ&apos;×WÕÌÃÁqÕÆÝÖÛÄÆä :ÕÞÖÐÈÓ&apos;ÕÌÇÈÇÚßqÇÚÁÕÌÃÎÁ&apos;ÏqËØÃÄÆÉ =&quot;É &gt;× v í N ó ÷_ @ÕÆÎÏ ó p j÷AÝÎ×ÛÝÂ.Á&apos;ÃMä ÙZÐÈ×ÛÁ&apos;Ï, û­ÎðÖÀÁ, ;&gt;é Á:ÉÕÆÎZÝÕÌÇÈÇÚßðÕÌÇÈÐÚÑjÎÁ&apos;Ï =ì Îä ."
"ÖÀÁ&quot;É:ÕÌÎZÝÕÆÇÚÇÈßíÕÆÇÚÐÈÑÆÎÁ&apos;Ï &gt;&amp;ÕÜËØÁ&apos;ÕÌÖÛÝÃÁ,ÂRÄZÄÆÇ]@À.ÕÆ×&amp;ÐÚÎÐÚÖÛÐÔÕÌÇ ô&gt;õpö _ÝÎ×ÛÝÂ.Á&apos;ÃÛÙZÐÔ×MÁÏ ËØÁ&apos;ÕÌÖÛÝÃÁ&amp;&amp; ÕÌÃÁ_ÕÆÏÏÁ&apos;Ï,ÖÛÄÖÛÀÁpËØÁ&apos;ÕÞÖÝÃÁAÂRÄZÄÆÇïá 9;: &lt; ?= &gt;;@BADC2EGF HIF J0KL=GMIAD@ û Î ÖÀÁ   @Õ ×ÛÉ:ÕÌÇÈÇ\ÂRÄÆÃÛÖÛÐÈÄÆÎ ÄÆË@ÅÐçä­  ù&gt;ÃÐÚÇÈÇ *õ &gt;ÃÛÐÈÇÈÇ{  å j÷0ÕÆÎÏ j ô"
"Ýõ p I ó = â ÄZÄÆÎ  ÕÆÇ{áÈå  $ÄÆË&quot; ÐÔ×0ËØÄjÃpì]ÎÑÆÇÈÐÔ×MÀÜÕÆÎÏ\, :É ÕÌÎZÝÕÆÇÚÇÈßàÕÆÇÚÐÈÑÆÎÁ&apos;Ï ÁÕÆÓ5À$,ÐÈÎ  ÕÌÎ._Ï . âÄjÃ_ % êÝ.×êÖÁ&apos;Ï\×ÛÄÆÉÁqÂÕÌÃÛÖpÄÌË&gt;&gt;ù ÃÐÚÇÈÇ *õ ×WÖÕÆÑ ..*ÄÌËì=Îä ÑÆÇÈÐÔ×MÀ²ÕÆÎÏ$ $]ì ÎÑÆÇÈÐÔ×MÀ$ÕÆÎÏ .\ÖÛÀÁqÅÕj×MÐÔ× ÄÆË=ÖÛÀÁ&amp; ÇÔ×ÛÄåé&gt;ÁAÉ:ÕjÏÁAÖÕÌÑ:.ÓÖÛÐÈÄÆÎ  ÃÁ&apos;×ÛÂ.ÁÓÖ ÖÛÄàÁ&apos;ÕÆÓ5À ÇÈÕÆÎÑÆÝ.ÕÌÑÆÁðÅZß$  &gt;&amp;ÝÎÐçÖ&quot;ÕÆÃÛÁ:  ÃÝÇÚÁ×&quot;ÕÆÃÛÁ Ý.×MÁÏ  É:ÕÆýgÐÈÎÑ\ ÂRÄgÄjÇ{á"
"Õj× Õ ,ÄÆËpÐÈÎÐÚÖÛÐÔÕÌÇAÕÆÓÖÐÚÙjÁ ËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×&apos;å :á 9;:ZY SJ\K;HIM ]C _&gt; ^ \?AebfM2?&gt; C]\i[ :=&gt;é Á  : ÖÛÄ@,Õ ÂÃÛÄjÅÕÌÅÐÈÇÚä"
ÐÚÖêß\ * .&quot;.ÄjÎÏÐÈÎÑ ëpÄjÃÛÁÕÌÎ
Á djek  $=ì ÎÑjÇÚÐÔ×MÀ${åyéFÁÝ×ÛÁ: jel Zá  Ö Ö U .èÅRÁÕÌÇÈÇ*:.Â ÕÌÖÛÐÈÄÆÎÄg×Û×ÛÐÚÅÇÚÁ:&gt;õpö
ÖÕÌÑ:ÄÆË= .   o å n  &amp;ËØÁ&apos;ÕÌÖÛÝÃÁAËØÝÎÓÖÐÚÄjÎ y |%v  óØÄÆÃ0ÕqËØÁÕÞÖÝÃÛÁu÷ ~}   &gt; } ÕÆ×&gt;ËØÄÆÇÈÇÈÄÞé0× Zv    ó ? ÷
Bx z {
ËØÁ&apos;ÕÞÖÝÃÁ p q r sq u   r *0D. 1/  + ó  / ÷ $4  Öêé&gt; m yå]&amp;ÐÚÎËØÄjÃÛÉ:ÕÌÖÛÐÈÄÆÎ ËØÄÆÃWÂÃÁ&apos;ÏÐÔÓÖÐÚÎÑ ÖÛÀÕÌÖAÕÌÎðì=  É: : # gá ­:_ÂRÄZÄÆÇïåÑÆÐÈÙÆÁ&apos;Î Õ:û ÂÕÆÐÚÃåséFÁ&quot;ÍÃ×MÖ_.;Â.&quot;ÅÐÈÎÕÌÖÛÐÈÄÆÎ× ÄÌË =ÎÑÆÇÈÐÈ×ÛÀ &gt;ô õpö&amp;Ö5ÕÌÑ&quot;]ÕÌÎ.:Ï ëpÄÆÃÁ&apos;ÕÌÎ:&amp;ÖÕÌÑ&amp;ì .
"ÉÄÆÎÑ&quot; p] ÖÀÕÞÖ0×ÕÞÖÛä   ÐÈÎjÖÄ:ÖÛÀÁ ËØÁ&apos;ÕÌÖÛÝÃÁpÂ.ÄZÄj  Ç ! á  \  p q r sq u ÷  Ú  ×ÛÖÀÐÚÉÐÈ:ÁÇÈÕÆÃÛÐÚÒÖêÐÔß\×êÖ # ÙÞÕÆ  ÇÚÝ  Áð×MÝ.ó¬×ÛÓ5ÐÚÀðÉÂÖÀÇÈßèÕÞÖ,  ó ÝÎg  Ö  ÐÚÎ    ÷ 4Q ÄÆË0ª×ÛÕÆÉqÁÖ5ÕÌÑZ÷ÕÌÎÏðÖÛÀÁ &gt; % &amp;ÕÌÎ. ( Ï   r á ;9 : c AebfMI;&gt; ]"
C AH2ADi Ae^ J0K; &gt; !
"ÐÈ×;ÖÄgÄpÙÞÕÆ×MÖ&apos;åjÕ &quot;ÂÃÛÄÆä Ó _;;âÄjÃÖÀÐÈ×&apos;åÌÁÕÆÓ5À ËØÁ&apos;ÕÌÖÛÝÃÁ=.ÕÞÖÛÁÏ_&gt;ÖÛÄ0ÀÄÞéèÉ&amp;ÝÓ5ÀAÖÛÀÁ&apos;ß_ ÖÛÃÐÈÅÝÖÛÁ,ÖÛÄ\ÖÛÀÁ ÇÚÐÈýÆÁ&apos;ÇÚÐÈÀÄZÄÏ *@ÐÈ×   ] á @ËØÁÕÞÖÝÃÛÁ\ÑjÕÌÐÈÎ²ÕÆÎÏ$Õ ,ÄÌË &gt;Á .ÓÖÛÐÈÄÆÎ,ÄÌË úÜì áû­Î úÜì åÕ&amp;ËØÁ&apos;ÕÞÖÝÃÁAÑjÐÚÙjÁ&apos;×&gt;ÐÈÎËØÄÆÃÉ:ÕÞÖÛÐÈÄÆÎíÖÛÄ:ÕqÂÃÄÆÅÕÌä ÅÐÈÇÚÐÚÖêß,&gt; ÍÎÁÏá].*ÖÀÁpËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×0ÕÆÃÛÁAÖÃÕÆÐÚÎÁÏ]"
ÅZß²ÖÀÁàÐÚÉÂÃÄÞÙÆÁÏ  ó¬ûMûMö÷ðÕÌÇÈÑÆÄjÃÛÐÚÖÛÀÉ ÖÛÀ.ÕÞÖpÉqÐÈÎÐÈÉ F Ð ã &apos;Á&apos;×WÖÛÀÁ:ëpÝÇÈÇÈÅÕÆÓ5ýg L ä Ö
"Öêé&gt;  ÖÛÀÁpÖÛÃ5ÕÌÐÈÎÐÚÎÑÏÕÞÖÕá _,ÖÄàÕà×ÛÐÚÉÂÇÈÁ\ÖêßZÂRÁ ÄÌË &quot;Ó&apos;ÕÌÎèÅ.:Á  ÕqÃÕÌÖÛÐÈÄ"
"Ö  : 0* .Å ÁWÖÛÀÁ ::&amp;Õ  ÄÌËÐÈÎÐÚÖÛÐÔÕÌÇÆÕjÓÖÛÐÈÙÆÁ*ËØÁÕÞÖÝÃÛÁ G × :_= ÐÈÎ N ó *  + ó w ,v s÷"
"Bx  * + ó w , v R÷ /     s  ó u÷ =   ó +v s÷ Bx¡  * + ó w ,v R÷ /     s ¿ &gt;"
R÷0ÄÆË= ÖÛÀÁ_ Ö ûMûMöíÅ 5 Ö . p]  ¢ .Å £ Á  ]p ¢ åÕÌÎÏ *    Å.Á ÖÛÀÁWÄÆÂÖÐÚÉ:
ÕÆÇ. ;ÂÃÛÄjÅÕÌÅÐÈÇÈÐçÖêß  ÕjÏÏÐÈÎÑ ËØÁ&apos;ÕÌÖÛÝÃÁ ]p ¢ ;á  *    = Ã B¦ FågÐÚÎ:;ÑÆÐÈÙÆÁ&apos;ÎqÅZßqÕÆÓÖÐÚÙjÁ &gt; ]p ¢ áû­ÎpÄÆÃ5ÏÁ&apos;Ã ÖÛÄAÉ:
"ÕÌýjÁFÐÚÖyÖÃÕjÓÖ5ÕÌÅÇÈÁ]]&gt;Á ÕÆ××ÛÝÉÁ ÖÀÕÞÖ&gt;ÖÛÀÁ_ÕjÏÏÐÚÖÛÐÈÄÆÎ,ÄÆËyÕ&quot;ËØÁ&apos;ÕÞÖÝÃÁ p ¢ &gt;ÄÆÎÇÚß ÖÛÀÁq×ÛÐÚÎÑÆÇÈÁ&amp;Ã ¦ Fá&quot;   ó p j÷&gt;ÐÈ×"
"Fá ¦ 0  x B § ó  R÷ 0 *  ó ,v s÷ / §     s * ó j÷   B§ ó R v ¨x© ÷  *   ó ,v s÷ / §     s ¿ .=ÕjÏÏÐÈÎÑpÕ ×ÛÐÚÎÑjÇÚÁWËØÁ&apos;ÕÌÖÛÝÃÁ ]p ¢ ,Å.ÁpÁ&apos;×MÖÛÐÈÉ:ÕÞÖÁ&apos;ÏíÅgß:ÉÁÕÆ×ÛÝÃÛÐÈÎÑqÏÐçËòä ._ÄÌË*:É"
ÕÞÒÐÈÉ&quot;ÝÉ ÇÚÄjÑÌäüÇÚÐÈýÆÁ&apos;ÇÚÐÈÀÄZÄÏ &gt; $ Î ª Wó *    ÷ ÕÌÎ. « Ï Wó ª *0 ÷á$ p ¢ ÅZß ¬ ó p ¢   ÐÈÎ  ó ° ¬  ó    ó p] * ¢ ®­ ÷ ®­ ÷ ¯ª Wó * v\ §0°   ÷      ó *ó * ÷  ÷ ó g÷  ÇÈÄÆÑ *0 *    x
"ÀÁ ÕÌÇÈÑÆÄÆÃÐÚÖÛÀÉ]@ÃÁ&apos;×ÛÂRÁ&apos;ÓÖ/&quot;ÖÛÄ p ¢ ,ÕÆÏÄjÂÖÛÁÏÖÛÀÁ ù&gt;Ã õ ×   ÑjÕÆÐÚÎ.×²ó¬ù&gt;@ ÕÌÇïáÈ X å   g÷á âÄÆÃíÖÛÀÁ   {{áÈå  ô=&amp; ÕÌÇïáÈå _"
Æá    ±   p¶* µ0. ó 2/ p 12¢ - ÷ · / 0*  ó p ¢ ÷ p  ~?¦ ¸Sx r á º¹ .ÁÕÞÖ:
"ÝÎgÖÐÚÇ ~°   ó _» ÷qÀÕÆ×,  v FÄjÉqÂÝÖÛÁ U¦  ËØÃÄÆÉ ½¦ » Ý ×MÐÈÎÑ ¦  x`U°¦ »U¾   ¿ ó ¦ ÇÚÄjÑ  = ó &gt;÷ º± .Ý ×MÐÈÎÑ ¿;ÀÀ;ÁÁÂ Ã   §Å§ÅÄÄ  ÷ FÄjÉqÂÝÖÛÁ °#° Ì   ó  ó * ¦ ; BxÆ± ÷ ¨x ÷ * µ ó  p ¢ È ÷ _É±Ç     &quot;£   * µ ó  R÷   ¼ §   ó É  Ç   å ° Ì Ì.  /21 ] ó /¦ ; Í¨x¡Î ÷ ó v R"
"Ò­ ÷ _   * ÉÇ§ å    É p ó Ë]p¢ ¢Ã , v s÷Aå   &quot;Î Ï p  ¢ + å +"
"Ð Ñ    ;i ¬ ª W *   ó § p]¢ ó BÔÕ° ÷  , v R÷"
B­  `Ó
É Ë         s  ó ¦ »  ÷ Ã ¿ ÀÐÈ×&gt;ÕÌÇÈÑÆÄjÃÛÐÚÖÛÀÉ  Ý×ÛÐÈÎÑ n  &gt;é ÄÆÎ   &gt; Î ]
Ð ã * $É&amp;ÝÓ5À ÖÀÁAËØÁ&apos;ÕÌä :ÖÀÁ ÉÄÏÁ&apos;Ç{á &gt;×  ÇÈÁ&apos;ÓÖ&gt;.ËØÁ&apos;ÕÌÖÛÝÃÁ&apos;×]
ÀÐÈÑÆÀ:ÑjÕÆÐÚÎ:ÙÞÕÆÇÚÝÁ×]ÕÆÎÏÄÆÅÖ5ÕÌÐÈÎ ;ô]&gt;Á  = ÂÃÛÄjÅÕÌÅÐÚÇÈÐçÖÐÚÁ×=ÄÆËô&gt;õpö&amp;ÖÕÆÑ .
Ö Û  êC 0JM¾sY JÛ &quot; Û Y ÛJêd]EWL CEFX &gt; .. &gt;_ÅÐÈÇÚÐÈÎÑÆÝ.&amp;ä ÅRÁ&apos;ÏÏÁÏ ÐÈÎ ;û­ÎíëpÄjÃÛÁÕÌÎä­ì=ÎÑÆÇÈÐÈ×ÛÀ&amp;:É ÕÌä ..
"ÃÁ&apos;×ÛÂ.ÄjÎÏÐÈÎÑ: A ÑÆÐÈÙÆÁ&apos;Î  £ Á ×  ËØÝÎÏÕÌÉÁ&apos;ÎjÖ5ÕÌÇRÁ&apos;øgÝÕÌÖÛÐÈÄÆÎ ÄÆËÉ:ÕÆÓ5ÀÐÈÎÁ0ÖÃÕÆÎ×ÛÇÈÕÌÖÛÐÈÄÆÎ @ÅRÁ, ÐÈÎ² ¡ ó ,ÖÛÀÁ ÃÕÆÎÏÄjÉ7ÙÞÕÌÃÐÚä ÕÌÅÇÚÁ"
Ù × Ø ÕÌÎ.
L Ï Ú cÕÆÃÛÁ\.Ï =Îä ÑÆÇÈÐÔ×MÀ : ÕpÖÛÃ5ÕÌÎ×ÛÇÔÕÞÖÛÐÈÄÆÎåjÕÌÎ.ÏqÖÛÀÁ0Ã5ÕÌÎä
"ÏÄjÉIÙuÕÆÃÛÐÔÕÌÅÇÚ o Á Û 3ÐÔ×FÕÆÎíÕÆÇÚÐÈÑÆÎÉÁ&apos;ÎjÖ&gt;.Å ,ÖÛÀÁ&apos;É á= Á&apos;øgÝÕÌÖÛÐÈÄÆÎ ,  ÐÚÖêßÆå Ü :ó A  ÖÛÃ5ÕÌÎ×ÛÇÔÕÞÖÛÐÈÄÆÎ&amp;ß U : Ü + ó × ,A ÷á*û­Î"
"Ù ß Ü :ó × ,A"
"Ü A ,× Bx Ü :ó A Ü :  ÷ :  ó  + ó × ÷ , A ÷ :ó ÷ AÝ x ÕÆÃÛÑ]É l ÕÌÒ Ü :ó A  ÷ : + ó × ,A ÷ ó Z÷ û­_ûêù&gt;ú  :*Á×êÖÐÚÉ:ÕÞÖÐÚÎÑWÖÃÕÆÎ×ÛÇÈÕÌÖÛÐÈÄÆÎ ÂÃÄÆÅÕÆÅÐÈÇÚÐÚÖÛÐÈÁ&apos;×&apos;á .×MÇÔÕÞÖÐÚÄjÎ\ .Ó_ f å × ÕÌÎÏ A ÐÔ×"
"Ü +× , A ÒxLÞ ÷ à3á  ß ã¢ á â   ó É à ,/ É ¢ Ü ÷ :  ó   , å   ÷ :ó  ó j÷ û­Î  ó j÷åàÕÌÎ7ì ²/ É / É ] / É å=ÕÌÎÏ ÕðëpÄÆÃÁ&apos;ÕÆÎÕÆ"
A ÎÏíÀÕj×ÖÛÀ_Á Â ó
ÀÂRÄj××MÐÈÅÇÈÁ è× ÃÕj$×MÁ×&gt;ÀÕj×ÄÌË ÂÀÃ5ÕÆ×ÛÁ&apos;×&apos;å éA
ÂRÄj××MÐÈÅÇÈÁpÂÀÃ5 l ÕÆ ç ×Û çç Á&apos;×&apos;å ä É ä É l çç&apos;ç ä É  .À  ÕuÙÆ  :Á æ  Ð ÄjÃÛÃ å Á&apos;×ÛÂ æ[Footnote_2]ë .Äj]å
"2 2 ! $&quot; &amp;# %((&apos; &apos; )&apos; &apos;(+* $&apos; , ./ 10325432,"
"ÎÕÌÎÏÏÐÈ@Î â ×MÁøjÝÎÁ&apos;× ß _Á×&quot;ÄÆÄÆË ~  × ²ÀÕuÙjÁö\Ó&apos;ÕÞÖÛÁ&apos;ÑÆÄjÃÛÐÈÁ&apos;×&apos;å . ç&apos;çç ô&gt;õpö   hå  å  Ð çç&apos;çå   íÞ á \ÐÈ×  ó  É ,/ É  "
A Òx`ö÷ß ÷ à3á \ø   ó É  É à ÷ ¢ á â ;ø ó / É / É ¢ ÷ öhx q  æ Ñ æ[Footnote_2]ê + ú &apos;û u æq Ñû ær
"2 2 ! $&quot; &amp;# %((&apos; &apos; )&apos; &apos;(+* $&apos; , ./ 10325432,"
Ñæ êæ +¼²üýú &apos;üýûü ¼ u æq 
Ñû æ rÑæ æ2ë + ú &apos;û u æ
"Ñû r æ ë  ó Æ÷ * m  ó ä É ,/ É  A þ ÷ A  þ ÷ ó ) ÷  ä É ,/ É Òx ÷ 3æ þ á  ó ä É ,/ É   ÿ  _.JML"
CEFXa èÁ&amp;\=ì ñ ÖÛÀ.:ËØÃÛÄjÉ ÖÀÁWé&gt;
"Zõ ÕÌÎ.ÏíÕÉ:ÕÌÑgÕ &apos;ÐÚÎÁWËØÄÆÃ0ì ã ]ÎÑÆÇÈÐÔ×MÀ ÇÚÁÕÌÃÎÐÈÎÑ ó{¿yÕÆÅÇÈ X Á p Æ÷á %ÕÌÇÈÐÈÑÆÎÁÏ  ²ô&gt;õpögäïÖÕÆÑÆÑÆÁÏ  ÂÕÆÐÚÃ5×@ÖÄ ÄjÅÖÕÆÐÚÎ @.ÄjÎä  ÄÌË   ÐÈÎ ¿;ÕÌÅÇÚÁ å &gt;ÄÆÃ5ÏZäïËØÄÆÃÛäïé&gt;. ì=ÎÑÆÇÈÐÈ×ÛÀä­ëpÄÆÃÁ&apos;ÕÌÎ,&gt;ÄÆÎÇÈ ë ß Æ &lt; á p á ,á $× Õ ÃÁ&apos;×ÛÝÇçÖå jå ) . .&gt;"
"ÖÛÀÁèÉ:ÕÌÎZÝÕÌÇÈÇÈß¼ÕÆÇÚÐÈÑÆÎÁ&apos;Ï &quot;{ó ¿yÕÆÅÇÈ B Á g÷á :&gt;é $Õj× ÐÈÎÐÚÖÛÐÔÕÌÇpÕÆÓÖÐÚÙjÁ $ÖÀÁ éFÁ&apos;ÐÚÑjÀgÖ× ÄÌË&amp;ÖÛÀÁ  ÖÛÝÃÛÁ×\é&gt;%ûMûMö  ¿;ÕÌÅÇÚÁ j =ÎÑjÇÚÐÔ×MÀ &gt;é ÄÆÃ5Ï×ì ¹ ÕÞÖÛÐÈÄëpÄÆÃÁ&apos;ÕÆÎ,ÉÄÆÃÂÀÁ&apos;ÉÁ&apos;× p  Æá &lt; á p  á p   áá ¿yÕÆÅÇÚÁ  v =*  ÏÐÔ×   ÕjÓÖÛÐÈÙÆÁAËØÁÕÞÖÛÝÃÛÁ× jå ) )"
F å _p ô ÍÇÚÖÛÁ&apos;ÃÛÁÏíËØÁ&apos;ÕÞÖÝÃÁAÂRÄZÄÆÇ n  ÎÁ&apos;é$ËØÁÕÞÖÛÝÃÛÁ× ¿yÕÆÅÇÈÁ ; =âÁÕÞÖÛÝÃÛÁ×&apos;ó º /£ ¤¡¤¡¥L¡¼%/£ Z¸Z¥L § ¥L£ _/  Z¥  (»Z¥L § ¥ ÷ ×ÛÀÄÞé0×qéFÁ&apos;ÐÚÑjÀjÖ5×   
"ÖÛÀ.ÕÞÖ p q    q u  :á ) ] å  ÜËØÁ&apos;ÕÞÖÝÃÁ&apos;×óòÖÕÆÑ@=\.&amp;ÓÖÛÐÈÄÆÎåé&gt; /ÖÀÁ  å r  \$&gt;Á  &quot;ÕÌÎ.. ÖÛÐÈÄÆÎ,ÃÛÝÇÈÁ&apos;×&apos;åjÖÛÀÁp×M ]"
"Ð ã  ÄÌËÖÛÀÁ ËØÁÕÞÖÛÝÃÛÁ0ÂRÄZÄÆÇRé&gt;Õj×=ÎÄÆÖFÇÔÕÌÃÑÆÁjá &quot;ËØÁ&apos;ÕÌÖÛÝÃÁ&amp;&gt;&quot;Á Ó5ÀÄg×MÁ&quot;;ËØÁ&apos;ÕÌä &quot;ÖÀÃÛÁ×MÀÄÆÇÔÏ&amp;ÄÆ  Ë r á r r ) á ×= &gt;Á ÄÆÅÖÕÌÐÈÎÁÏqÖÛÀÁAÂÃÄÆÅÕÆÅÐÚÇÈÐÚÖêß&amp; ÖÛÀ.ÕÞÖ:   ]ì ÎÑÆÇÈÐÔ×MÀàô&gt;õpö Ö5ÕÌÑ . &quot;&gt;ô ;ÖÄ ¿yÕÆÅÇÚ 1 Á &quot;.ÕÌÇÂÃÄÆÅä  \×MÀÄÞé0×:ÖÛÀ.ÕÞÖ  =ì   ; Ä n  ,ÖÖ íÄÆÃ*ÕÆÏÎÄjÉqä &gt;ÄÆÃ5Ï\ÐÈÎ  .   .&gt;_ &gt;é .×MÁÏ .  "
"Ú  Ae^ \a~=  JGJ0M  _ ×yÉqÁ&apos;ÎgÖÛÐÈÄÆÎÁÏ_Å. :ÕÌÎZß,&gt;é _ÕÌÇÈÐÚÑjÎÁ&apos;Ï,,é&gt;ÄÆÃ5ÏíÕÆÎÏ Ã5ÕÌÃÁqé&gt;:*éFÁÖÁ&apos;×MÖÛÁ&apos;Ï  ×MÖÛÃÝÓÖÝÃÕÆÇ]ËØÁÕÞÖÝÃÛÁ×&amp; ÐÚÎà×MÉÄZÄÌÖÀÐÚÎÑá âÄjÃ ÖÛÀÐÈ×&apos;å&quot; ÄÌË ..&gt;Õj× &quot;ÅRÄÌÖÀqÄjÎÇÚÄÞé &quot;&amp; $&quot; #&amp;%(&apos;(&apos; )&apos;)$&quot; 78%((&apos; &apos;(+* &apos;$,6 . /;32: $&amp;# (% &apos;(&apos;&quot; )&apos;)(&apos; &lt;=(% (&apos; &apos;(&quot;=,6 .&gt;32;-?4?: &quot;$#&amp;%(&apos;(&apos; &apos; )&apos; @* =&apos; ,?=% ( .?: ?4 / / &quot; #&amp;(% &apos;(&apos;$ &apos;)(&apos; +* $&apos; ,?%(&apos;)&apos;(+* &apos;$, .-?/,?, $&quot; #&amp;%(&apos;(&apos; (&apos; &apos;(*+$&apos; , =% A8AG6H&lt; .-F?/"
"This paper presents an empirical evaluation of stacked generalization, a scheme for combining automatically induced classifiers, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization."
"The increasing popularity and low cost of e-mail have intrigued direct marketers to flood the mailboxes of thousands of users with unsolicited messages, advertising anything, from vacations to get-rich schemes."
"These messages, known as spam or more formally Unsolicited Commercial E-mail, are extremely annoying, as they clutter mailboxes, prolong dial-up connections, and often expose minors to unsuitable content (Cranor &amp;[REF_CITE])."
National Centre for Scientific Research “Demokritos”[REF_CITE]Ag.
"Paraskevi, Athens, Greece e-mail: {ionandr, paliourg, vangelis, costass}@iit.demokritos.gr"
"Legal and simplistic technical counter-measures, like blacklists and keyword-based filters, have had a very limited effect so far. [Footnote_1]"
"1 Consult[URL_CITE]spam.abuse.net, and[URL_CITE]"
"The success of machine learning techniques in text categorizati[REF_CITE]has recently led to alternative, learning-based approaches ([REF_CITE]; Pantel &amp;[REF_CITE])."
"A classifier capable of distinguishing between spam and non-spam, hereafter legitimate, messages is induced from a manually categorized learning collection of messages, and is then used to identify incoming spam e-mail."
"Initial results have been promising, and experiments are becoming more systematic, by exploiting recently introduced benchmark corpora, and cost-sensitive evaluation measures ([REF_CITE];"
"Stacked generalizati[REF_CITE], or stacking, is an approach for constructing classifier ensembles."
"A classifier ensemble, or committee, is a set of classifiers whose individual decisions are combined in some way to classify new instances[REF_CITE]."
Stacking combines multiple classifiers to induce a higher-level classifier with improved performance.
The latter can be thought of as the president of a committee with the ground-level classifiers as members.
Each unseen incoming message is first given to the members; the president then decides on the category of the message by considering the opinions of the members and the message itself.
Ground-level classifiers often make different classification errors.
"Hence, a president that has successfully learned when to trust each of the members can improve overall performance."
We have experimented with two ground-level classifiers for which results on a public benchmark corpus are available: a Naïve Bayes classifier ([REF_CITE]c) and a memory-based classifier[REF_CITE].
"Using a third, memory-based classifier as president, we investigated two versions of stacking and two different cost-sensitive scenarios."
"Overall, our results indicate that stacking improves the performance of the ground-level classifiers, and that the performance of the resulting anti-spam filter is acceptable for real-life applications."
Section 1 below presents the benchmark corpus and the preprocessing of the messages; section 2 introduces cost-sensitive evaluation measures; section 3 provides details on the stacking approaches that were explored; section 4 discusses the learning algorithms that were employed and the motivation for selecting them; section 5 presents our experimental results followed by conclusions.
Text categorization has benefited from public benchmark corpora.
"Producing such corpora for anti-spam filtering is not straightforward, since user mailboxes cannot be made public without considering privacy issues."
"A useful public approximation of a user’s mailbox, however, can be constructed by mixing spam messages with messages extracted from spam-free public archives of mailing lists."
"The corpus that we used, Ling-Spam, follows this approach ([REF_CITE]b;"
"It is a mixture of spam messages and messages sent via the Linguist, a moderated list about the science and profession of linguistics."
The corpus consists of 2412 Linguist messages and 481 spam messages.
"Spam messages constitute 16.6% of Ling-Spam, close to the rates reported[REF_CITE], and[REF_CITE]."
"Although the Linguist messages are more topic-specific than most users’ e-mail, they are less standardized than one might expect."
"For example, they contain job postings, software availability announcements and even flame-like responses."
"Moreover, recent experiments with an encoded user mailbox and a Naïve Bayes (NB) classifier[REF_CITE]yielded results similar to those obtained with Ling-Spam[REF_CITE]."
"Therefore, experimentation with Ling-Spam can provide useful indicative results, at least in a preliminary stage."
"Furthermore, experiments with Ling-Spam can be seen as studies of anti-spam filtering of open unmoderated lists."
"Each message of Ling-Spam was converted into a vector x = x 1 , x 2 , x 3 ,h, x n , where x 1 , , x n are the values of attributes X 1 ,h, X n ."
Each attribute shows if a particular word (e.g. “adult”) occurs in the message.
All attributes are binary: X i =1 if the word is present; otherwise X i = 0 .
"To avoid treating forms of the same word as different attributes, a lemmatizer was applied, converting each word to its base form."
"To reduce the dimensionality, attribute selection was performed."
"First, words occurring in less than 4 messages were discarded."
"Then, the Information Gain (IG) of each candidate attribute X was computed: ∑ P(x,c) ⋅ log P(Px()x⋅,Pc()c) IG(X ,C) = x∈{0,1}, c∈{spam,legit}"
"The attributes with the m highest IG-scores were selected, with m corresponding to the best configurations of the ground classifiers that have been reported for Ling-Spam[REF_CITE]; see Section 4."
Blocking a legitimate message is generally more severe an error than accepting a spam message.
"Let L → S and S → L denote the two error types, respectively, and let us assume that L → S is λ times as costly as S → L ."
"Previous research has considered three cost scenarios, where λ = 1, 9, or 999 ([REF_CITE]b, c;[REF_CITE])."
"In the scenario where λ = 999, blocked messages are deleted immediately."
"L → S is taken to be 999 times as costly as S → L , since most users would consider losing a legitimate message unacceptable."
"In the scenario where λ = 9, blocked messages are returned to their senders with a request to resend them to an unfiltered address."
"In this case, L → S is penalized more than S → L , to account for the fact that recovering from a blocked legitimate message is more costly (counting the sender’s extra work) than recovering from a spam message that passed the filter (deleting it manually)."
"In the third scenario, where λ = 1, blocked messages are simply flagged as possibly spam."
"Hence, L → S is no more costly than S → L ."
Previous experiments indicate that the Naïve Bayes ground-classifier is unstable when λ = 999[REF_CITE].
"Hence, we have considered only the cases where λ = 1 or 9."
"Let W L (x) and W S (x) be the confidence of a classifier (member or president) that message x is legitimate and spam, respectively."
The classifier classifies x as spam iff:
W S (x) &gt; λ W L (x)
"If W L (x) and W S (x) are accurate estimates of P(legit | x) and P(spam | x) , respectively, the criterion above achieves optimal results (Duda &amp;[REF_CITE])."
"To measure the performance of a filter, weighted accuracy (WAcc) and its complementary weighted error rate (WErr = 1 – WAcc) are used ([REF_CITE]b, c;[REF_CITE]):"
"WAcc = λ ⋅ N L → L + N S → S λ ⋅ N L + N S where N Y → Z is the number of messages in category Y that the filter classified as Z , N L = N L → L + N L → S , N S = N S → S + N S → L ."
"That is, when a legitimate message is blocked, this counts as λ errors; and when it passes the filter, this counts as λ successes."
"We consider the case where no filter is present as our baseline: legitimate messages are never blocked, and spam messages always pass."
The weighted accuracy of the baseline is:
WAcc b = λ ⋅ λ N ⋅ N L L + N S
The total cost ratio (TCR) compares the performance of a filter to the baseline:
TCR = WErr b = N S WErr λ ⋅ N L → S + N S → L
Greater TCR values indicate better performance.
"For TCR &lt; 1, not using the filter is better."
Our evaluation measures also include spam recall (SR) and spam precision (SP):
N S → S SR = N S → S + N S → L N S → S SP = N S → S + N L → S
"SR measures the percentage of spam messages that the filter blocks (intuitively, its effectiveness), while SP measures how many blocked messages are indeed spam (its safety)."
"Despite their intuitiveness, comparing different filter configurations using SR and SP is difficult: each configuration yields a pair of SR and SP results; and without a single combining measure, like TCR, that incorporates the notion of cost, it is difficult to decide which pair is better."
"In all the experiments, stratified 10-fold cross-validation was used."
"That is, Ling-Spam was partitioned into 10 equally populated parts, maintaining the original spam-legitimate ratio."
"Each experiment was repeated 10 times, each time reserving a different part S j (j = 1, …, 10) for testing, and using the remaining 9 parts as the training set L j ."
"In the first version of stacking that we explored[REF_CITE], which we call cross-validation stacking, the training set of the president was prepared using a second-level 3-fold cross-validation."
"Each training set L j was further partitioned into three equally populated parts, and the training set of the president was prepared in three steps."
"At each step, a different part LS i (i = 1, 2, 3) of L j was reserved, and the members were trained on the union LL i of the other two parts."
"Each x = x 1 , ,x m of LS i was enhanced with the members’ confidence W S1 (x) and W S2 (x) that x is spam, yielding an enhanced LS i &apos; with vectors x&apos;= x 1 , ,x m ,W S1 (x),W S2 (x) ."
"At the end of the 3-fold cross-validation, the president was trained on L j &apos; = LS 1 &apos; LS 2 &apos; LS 3 &apos; ."
"It was then tested on S j , after retraining the members on the entire L j and enhancing the vectors of S j with the predictions of the members."
"The second stacking version that we explored, dubbed holdout stacking, is similar to Kohavi’s (1995) holdout accuracy estimation."
"It differs from the first version, in two ways: the members are not retrained on the entire L j ; and each partitioning of L j into LL i and LS i leads to a different president, trained on LS i &apos; , which is then tested on the enhanced S j ."
"Hence, there are 3×10 presidents in a 10-fold experiment, while in the first version there are only 10."
"In each case, WAcc is averaged over the presidents, and TCR is reported as WErr b over the average WErr."
"Holdout stacking is likely to be less effective than cross-validation stacking, since its classifiers are trained on smaller sets."
"Nonetheless, it requires fewer computations, because the members are not retrained."
"Furthermore, during classification the president consults the same members that were used to prepare its training set."
"In contrast, in cross-validation stacking the president is tested using members that have received more training than those that prepared its training set."
"Hence, the model that the president has acquired, which shows when to trust each member, may not apply to the members that the president consults when classifying incoming messages."
"As already mentioned, we used a Naïve Bayes (NB) and a memory-based learner as members of the committee[REF_CITE]."
"For the latter, we used TiMBL, an implementation of the k-Nearest Neighbor algorithm[REF_CITE]."
"With NB, the degree of confidence W S ( x ) that x is spam is:"
W SNB ( x ) =
"P ( spam | x ) = m P(spam) ⋅ ∏ P(x i | spam) i=1 = m ∑ P(k) ⋅ ∏ P(x i | k) k ∈{spam,legit} i=1"
"NB assumes that X 1 , ,X m are conditionally independent given the category (Duda &amp;[REF_CITE])."
"With k-NN, a distance-weighted method is used, with a voting function analogous to the inverted cube of distance[REF_CITE]."
"The k nearest neighbors x i of x are considered: k ∑ δ (spam,C(x i )) d(x, x i ) 3 W Sk − NN (x) = i = 1 , k ∑ 1 d(x, x i ) 3 i = 1 where C(x i ) is the category of neighbor x i , d(x i , x j ) is the distance between x i and x j , and δ (c 1 ,c 2 ) = 1 , if c 1 = c 2 , and 0 otherwise."
"This formula weighs the contribution of each neighbor by its distance from the message to be classified, and the result is scaled to [0,1]."
"The distance is computed by an attribute-weighted functi[REF_CITE], employing Information Gain (IG): n d(x i , x j ) ≡ ∑ IG t ⋅δ (x ri , x rj ) , t=1 where x i = x 1i , l ,x im , x j = x 1j , l ,x mj , and IG t is the IG score of X t (Section 1)."
"In Tables 1 and 2, we reproduce the best performing configurations of the two learners on Ling-Spam[REF_CITE]."
These configurations were used as members of the committee.
The same memory-based learner was used as the president.
"However, we experimented with several configurations, varying the neighborhood size (k) from 1 to 10, and providing the president with the m best word-attributes, as in Section 1, with m ranging from 50 to 700 by 50."
"The same attribute- and distance-weighting schemes were used for the president, as with the ground-level memory-based learner."
Our motivation for combining NB with k-NN emerged from preliminary results indicating that the two ground-level learners make rather uncorrelated errors.
"Table 3 shows the average percentages of messages where only one, or both ground-level classifiers fail, per cost scenario (λ) and message category."
The figures are for the configurations of Tables 1 and 2.
It can be seen that the common errors are always fewer than the cases where both classifiers fail.
"Hence, there is much space for improved accuracy, if a president can learn to select the correct member."
"Tables 4 and 5 summarize the performance of the best configurations of the president in our experiments, for each cost scenario."
Comparing the TCR scores in these tables with the corresponding scores of Tables 1 and 2 shows that stacking improves the performance of the overall filter.
"From the two stacking versions, cross-validation stacking is slightly better than holdout stacking."
"It should also be noted that stacking was beneficial for most of the configurations of the president that we tested, i.e. most sub-optimal presidents outperformed the best configurations of the members."
"This is encouraging, since the optimum configuration is often hard to determine a priori, and may vary from one user to the other."
There was one interesting exception in the positive impact of stacking.
"The 1-NN and 2-NN (k = 1, 2) presidents were substantially worse than the other k-NN presidents, often performing worse than the ground-level classifiers."
"We witnessed this behavior in both cost scenarios, and with most values of m (number of attributes)."
"In a “postmortem” analysis, we ascertained that most messages misclassified by 1-NN and 2-NN, but not the other presidents, are legitimate, with their nearest neighbor being spam."
"Therefore, the additional errors of 1-NN and 2-NN, compared to the other presidents, are of the L → S type."
"Interestingly, in most of those cases, both members of the committee classify the instance correctly, as legitimate."
"This is an indication, that for small values of the parameter k the additional two features, i.e., the members’ confidence W S1 (x) and W S2 (x) , do not enhance but distort the representation of instances."
"As a result, the close neighborhood of the unclassified instance is not a legitimate, but a spam e-mail."
This behavior of the memory-based classifier is also noted[REF_CITE].
"The suggested solution there was to use a larger value for k, combined with a strong distance weighting function, such as the one presented in section 4."
"In this paper, we propose a new method of text categorization based on feature space restruc-turing for SVMs."
"In our method, independent components of document vectors are extracted using ICA and concatenated with the original vectors."
This restructuring makes it possible for SVMs to focus on the latent semantic space without losing information given by the original feature space.
"Using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data."
The task of text categorization has been exten-sively studied in Natural Language Processing.
Most successful works rely on a large number of classi ed data.
"However, it is hard to collect classi ed data, so considering real applications, text categorization must be realized even with a small number of labeled data."
"Several methods to realize it have been proposed so far[REF_CITE], but they need to be further developed."
"For that purpose, we have to take advantage of invaluable information o ered by the property of unlabeled data."
"In this paper, we propose a new categorization method based on Sup-port Vector Machines (SVMs)[REF_CITE]and Independent Component Analysis (ICA)[REF_CITE]."
"SVM is gaining popularity as a classi-er with high performance, and ICA is one of the most prospective algorithms in the eld of signal processing, which extracts independent components from mixed signals."
SVM has been applied in many applications such as Image Processing and Natural Language Processing.
The idea to apply SVM for text cat-egorization was rst introduced[REF_CITE].
"However, when the number of labeled data are small, SVM often fails to produce a good result, although several e orts against this problem have been made."
There are two strate-gies for improving performance in the case of a limited number of data.
"One is to modify the learning algorithm itself[REF_CITE], and the other is to process training data[REF_CITE], including the selection of features."
"In this pa-per, we focus on the latter, especially on fea-ture space restructuring ."
"For processing train-ing data, Principal Component Analysis (PCA) is often adopted in classi ers such as k-Nearest Neighbor method[REF_CITE]."
But the con-ventional dimension-reduction methods fail for SVM as shown by experiments in Section 6.
"Un-like the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space."
ICA is built on the assumptions that the sources are independent of each other and that the signals observed at multiple-points are lin-ear mixtures of the sources.
"While the theoret-ical aspects of ICA are being studied, its pos-sibility to applications is often pointed out as[REF_CITE]."
The idea of us-ing ICA for text clustering is adopted in sev-eral works such as[REF_CITE].
"In those works, vector representation model is adopted (i.e. each text is represented as a vector with the word-frequencies as the elements)."
"It is reported however that the independent com-ponents do not always correspond to the desired classes, but represent some kind of characteris-tics of texts[REF_CITE]."
"In[REF_CITE], they showed that the num-ber of potential components were larger than that of human-annotated classes."
These facts imply that it is not easy to apply ICA directly for text classi cation.
"Taking these observations into consideration, we take the following strategy: rst we perform ICA on input document vectors, and second, create the restructured information by concate-nating the reduced vectors (i.e. the values of the independent components) and the original feature vectors."
PCA is an alternative restructuring method.
"So we conducted experiments using SVM with various input vectors: original feature vectors, reduced feature vectors and restructured fea-ture vectors (reduction and restructuring are performed by PCA and ICA)."
"For comparison, we conducted experiments using Transductive SVM (TSVM)[REF_CITE]as well, which is designed for the case of a small number of labeled data."
"Using the proposed method (SVM with ICA), we obtain better results than ordinary SVM and TSVM, with both small and large numbers of labeled data."
Support Vector Machine (SVM) is one of the large-margin classi ers[REF_CITE].
"Given a set of pairs, ( x 1 ; y 1) ; ( x 2 ; y 2) ; ; ( x n ; y n ) (1) 8 i; x 2 i R d ; y i 2 f 1 ; 1 g of a feature vector and a label, SVM constructs a separating hyperplane with the largest margin (the distance between the hyperplane and the vectors, see Figure 1): f ( x ) = w x + b: (2)"
"Finding the largest margin is equivalent to min-imizing the norm k w k , which is expressed as: min : 1 k w k 2 ; (3) s : t : 8 i; y ( 2 i x i w + b ) 1 0 :"
This is realized by solving the quadratic pro-gram (dual problem of (3)): max : P 1 P i i P 2 i;j i j y y x x i j i (4) j s : t : i i y i = 0 ; 8 i; 0 ; i where i &apos;s are Lagrange multipliers.
"Using the i &apos;s that maximize (4), w is expressed as w = X y i i x i : (5) i"
"Substituting (5) into (2), we obtain f ( x ) = X y i i x i x + b: (6) i"
Unlabeled data are classi ed according to the signs of (6).
SVM is a linear classi er and its separating abil-ity is limited.
"To compensate this limitation, Kernel Method is usually combined with SVM[REF_CITE]."
"In Kernel Method, the dot-product in (4) and (6) is replaced by a more general inner-product K ( x i ; x ), called the kernel function."
Polynomial kernel ( x i x j + 1) d ( d 2 N +) and RBF ker-nel exp f k x x j k 2 = 2 2 g are often used.
Us- i ing kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there.
"This map-ping structure makes non-linear separation pos-sible, although SVM is basically a linear classi-er."
"Another advantage of kernel method is that although it deals with a high dimensional (pos-sibly in nite) Hilbert space, there is no need to compute high dimensional vectors explicitly."
Only the general inner-products of two vectors are needed.
This leads to a relatively small com-putational overhead.
"The Transductive Support Vector Machine (TSVM) is introduced[REF_CITE], which is one realization of transductive learning[REF_CITE]."
It is designed for the classi -cation with a small number of labeled data.
"Its algorithm is approximately as follows: 1. construct a hyperplane using labeled data in the same way as the ordinary SVMs. 2. classify the unlabeled (test) data according to the current hyperplane. 3. select the pair of a positively classi ed sam-ple and a negatively classi ed sample that are nearest to the hyperplane. 4. exchange the labels of those samples, if the margin gets larger by exchanging them. 5. terminate if a stopping-criterion is satis ed."
"Otherwise, go back to step 2."
"This is one way to search for the largest mar-gin, permitting the relabeling of test data that have already been labeled by the classi er in the previous iteration."
Independent Component Analysis (ICA) is a method by which source signals are extracted from mixed signals.
It is based on the assump-tions that the sources s 2 R m are statisti-cally independent of each other and that the observed signals x 2 R are linear mixtures of n the sources: x = A s : (7)
Here the matrix A is called a mixing matrix .
We observe x as a time series and estimate both A and s = ( s 1 ; ; s m ).
So our purpose here is to nd a demixing matrix W such that s 1 ; ; s m are as independent of each other as possible: s = W x : (8)
The computation proceeds by way of descent learning with an objective function indicating independence.
"There are several criteria of independence and their learning rules, among which we take here Infomax approach[REF_CITE], but with natural gradi-ent[REF_CITE]."
Its learning rule is
ÆW = ( I + (
I 2 g ( W x ))( W x ) t ) W; (9) where; g ( u ) = 1 = (1 + exp( u )) : 4 Text Categorization Enhanced with Feature Space Restructuring
"As in most previous works, we adopt Vector Space Model[REF_CITE]for representing documents."
"In this framework, each document d is represented as a vector ( f 1 ; ;f d ) with word-frequencies as its ele-ments."
First we reduce the dimension of document vec-tors using PCA or ICA.
"As for PCA, we fol-low the previous work described in , e.g.,[REF_CITE]."
"In[REF_CITE], they use ICA for dimension reduction and ob-tain a good result in Information Retrieval."
"At the rst step of our method, where the reduced vectors are obtained, we follow their method."
"In this framework, each document d is consid-ered as a linear mixture of sources s representing topics ."
Each word plays a role of &quot;microphone&quot; and receives a word-frequency in the document as a mixed signal at each time unit.
This for-mulation is represented by the equation: d = A s ; (10) where A is a mixing matrix.
"Although both A and s are unknown, they can be obtained using the independence assumption."
The source sig-nals s are considered as a reduced expression of this document.
"In the case of PCA, the restruc-turing is processed in the same way."
The only di erence is that independent components cor-respond to principal components for the PCA case.
"After computing a reduced vector s with PCA or ICA, we concatenate the original vector d and the reduced vector s : d ^ = d : (11) s"
"This transformation means that we do not rely only on the reduced information, but make use of both the reduced and the original informa-tion, that is, the restructured information ."
"Regarding ^ d as the input feature vector of a document, we use SVM for categorization."
"Since SVMs are binary classi ers themselves, so we take here the one-versus-rest method to apply them for multi-class classi cation tasks."
The proposed feature restructuring method can be considered as the use of a certain kernel for the pre-restructured feature space.
We give an explanation for the linear case.
"Given two vec-tors, d 1 and d 2, the kernel function K in the restructured space is expressed as,"
K ( d ^1 ; d ^2) = ^ d 1 d 2 t ^ = d t 1 d 2 + s t 1 s 2 = d 1 t d 2 + d 1 t A t
A d 2 : (12)
"Considering the fact that each of two terms above is a kernel and that the sum of two kernels is also a kernel[REF_CITE], the proposed re-structuring is equivalent to using a certain ker-nel in the pre-restructured space."
The criterion of meaningfulness depends on which of ICA and PCA is used.
Note that weighting is di er-ent from reducing.
"In the dimension-reduction methods, only the latent semantic space is con-sidered, but in our method, the original feature space still directly in uences the classi cation result."
"This property of our method makes it pos-sible to focus on the information given by the latent semantic space, without losing informa-tion given by the original feature space."
"In text categorization, classes to be predicted are sometimes characterized by local informa-tion such as the occurrence of a certain word, but sometimes dominated by global information such as the total frequency of a certain group of words."
"Considering this situation and the above property of our method, it is not surprising that out method gives a good result."
"To evaluate the proposed method, we conducted several experiments."
The data used here is the[REF_CITE]dataset.
The most frequent 6 categories are ex-tracted from the training-set of the corpus.
Some part of them is used as training data and the rest is used as test data.
Only the words occurring more than twice are used.
Both stemming and stop-word removal are performed.
"For compu-tation, we used SVM- light[REF_CITE]."
We conducted two kinds of experiments.
"The rst one focuses on evaluating the performance of the proposed method for each category, with a xed number of labeled data (Section 6.1)."
The second one is conducted to show that the proposed method gives a good result also when the number of labeled data increases (Section 6.2).
The results are evaluated by F-measures.
"To evaluate the performance across categories, we computed Micro-average and Macro-average[REF_CITE]of F-measures."
Micro-average is obtained by rst computing precision and re-call for all categories and then using them to compute the F-measure.
Macro-average is com-puted by rst calculating F-measures for each category and then averaging them.
"Micro-average tends to be dominated by large-sized categories, and Macro-average by small-sized ones."
The kernel function used here is a linear ker-nel.
The number of independent or principal components extracted by ICA or PCA is set to 50.
"In this experiment, we treated 100, 500, 1000 and 2000 samples as labeled respectively and kept the other 4772, 4372, 3872 and 2872 sam-ples unlabeled."
The experiment was conducted 10 times for each sample-size repeatedly with randomly selected labeled samples and their av-erage values are computed.
"The result is shown in Tables 2, 3, 4 and 5."
"In the row of &quot;Method&quot;, combinations of restructuring methods are writ-ten. &quot;Original&quot; means the data of original docu-ment vectors. & quot;PCA&quot; and &quot;ICA&quot ; mean the data of only reduced vectors, respectively. &quot;Orig-inal+PCA&quot; and &quot;Original+ICA&quot; are the re-structured data explained in Section 4."
The proposed method yields a high F-measure in all the categories for 1000 and 2000 labeled data and in most categories for 100 and 500 labeled data.
"The last two rows of Tables 2, 3, 4 and 5 show that both Micro-average and Macro-average are the highest for the pro-posed method."
"This means that the proposed method performs well both for large-sized cat-egories (e.g., earn) and small-sized categories (e.g., interest), regardless with the number of labeled data."
"To investigate how each method behaves when the number of labeled data increases, we con-ducted this experiment."
The number of labeled data ranges from 100 to 2000.
The results are shown in Figure 2 and Figure 3. &quot;PCA&quot; gives a good score only with a small number of data and &quot;Original&quot; gives a good score only with a large number of data.
"In contrast to them, the pro-posed method produces high performance both with small and large numbers of data."
We proposed a new method of feature space re-structuring for SVM.
"In our method, indepen-dent components are extracted using ICA and concatenated with the original vectors."
"Using this new vectors in the restructured space, we achieved high performance both with small and large numbers of labeled data."
The proposed method can be applied also to other machine learning algorithms provided that they are robust against noise and can han-dle a high-dimensional feature space.
"From this point of view, it is expected that the proposed method is useful for kernel-based methods, to which SVM belongs."
"As a future work, we need to nd a way to de-cide the number of independent components to be extracted."
"In this paper, we set the number to 50 in an ad-hoc way."
"However, the appropri-ate number must be predicted based on a theo- 



 retical reason."
"Toward this problem, theories of model selection such as Minimum Description Length[REF_CITE]or Akaike Information Criteri[REF_CITE]could be a good theo-retical basis."
"As explained in Section 4, two terms d t 1 d 2 and d t 1 A t A d 2 are simply concatenated in our method."
But either of these terms can be mul-tiplied with a certain constant.
This means that either of the original space and the Latent Se-mantic Space can be weighted.
Searching for the best weighting scheme is one of the future works.
Ð¸¡¹ÅÂ¼ÊÉ²´Ì¹ÂQ¹Å&quot;³ Ä²´Ë{&quot;É Ì¡Ë{»8¶PÌ¡²´»8µÃÉÅ¶P²´µÃ±kÉÅ±W¾Ñ¹º³±k¶P¼Ê»¾¿)²´¼Ê³¡µÆ±Ç¹Å±)Î ¶´¾L³¡»¿»8¼Ê¶²Q¹Å¹º¶´¼ÊÏ0»¿²´µÃ¶#É&quot;¹Å±Òu»¶´¾LÐ »8¼´ÉÅÌµÃ±¡±ÓÇ µÃ±ÓÉÌ¡ÀW{± ²Ê.¶ µÃ±¹iÄ·¹º¼ÊÇÅ»0²Ê» Ô²0Ó¡¹Õ²Q¹ºÒ¹¶P»Í&quot;ÖY²Ê¼Ê¹ÅµÆ±¡»mÓ ÉÅÌ¶P×¶PÀØÉÅ±7ÙÅÚÛ2Ë{Ì¡»m¶Ü²ÊµÆÉ±¶ Ð ¼´ÉÀÝ²Ê³¡»0ÞÑ»mÓµÃ¹W¼´È ¸¡Ì¶8½ ¹Å&quot;¶ ¾¿¹Å¶&quot;ÙÛ7ÎÑß¿à6È_Óá¸¡ÀW»8±²${Ë Ì¡»m¶Ü²ÊµÆÉ±¶8Í Ö »â²´³¡»8!± áÕ¹ÅÄÆÌ¹º²´»mÓ@ÉÅÌ¡$¼ ¶´×¶Ü²ÊÀ-ÉÅ±äã8ÚÛ7Ë{Ì¡»8¶P²´µÃÉÅ±¶0É Ð ²´Ä³»ÎÑß)à6ÈåÛ0{Ë Ì¡»m¶Ü²ÊµÆÉ±æ¹º±¶´¾¿¼ÊµÆ±¡Ç¨²Q¹Å¶´ÏqÍÄè²Ê³¡ÉÅÌ¡Ç³æÉÅÌ¼Ã»8¹º¼Ê±¡µÃ±¡Çé¹Å¸¡¸¡¼ÊÉ¹ÂQ³7ÉÅ±¡Ì¶P»m.¶ ê Ð »m¹Õ²´Ì¼´»m½q¾¿»2¹º¼Ê»2¹ÅÒ¡ÄÃ» ²´ÉY¹ÅÂQ³¡µÃ»WË{Ì¡µÆ²´»éÂ ÉÅÀW¸u» ²Êµè²ÊµÆá*» ¹ÅÂ8Â Ì¡¼Q¹Å×ÅÍé)Î ³»æ¼´»m¶PÌÄè²Q¶ µÃ¹2±Ó¸¡¼´É¹ÕÀW²Ê»iµÃ¶´µÆ²´±³Ç2¹º²*¾)¶PÌ¹wÂQ²´³ÉW¹Ò¡Ì¡ÀâµÃÄÃÓi¹ÅÂQ³¡µÃ¶Ü±¡²Q¹Õ²´ÄÆ»mÈo¹ºÉ ¼ÊÐ ±¡Èy²ÊµÃ±¡³¡Ç» È_¹º¹Å¼´¸¡²¿¸¡{Ë ¼ÊÌ¡É¹»m¶ÜÂQ²Ê³!µÆÉµÃ± ¶
The paper presents a data-driven approach to infor-mation extraction (viewed as template lling) using the structured language model (SLM) as a statistical parser.
The task of template lling is cast as con-strained parsing using the SLM.
The model is auto-matically trained from a set of sentences annotated with frame/slot labels and spans.
"Training pro-ceeds in stages: rst a constrained syntactic parser is trained such that the parses on training data meet the speci ed semantic spans, then the non-terminal labels are enriched to contain semantic information and nally a constrained syntactic+semantic parser is trained on the parse trees resulting from the pre-vious stage."
"Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad | personal infor-mation management | task."
Information extraction from text can be character-ized as template lling[REF_CITE]: a given template or frame contains a certain num-ber of slots that need to be lled in with segments of text.
Typically not all the words in text are rele-vant to a particular frame.
"Assuming that the seg-ments of text relevant to lling in the slots are non-overlapping contiguous strings of words, one can rep-resent the semantic frame as a simple semantic parse tree for the sentence to be processed."
The tree has two levels: the root node is tagged with the frame label and spans the entire sentence; the leaf nodes are tagged with the slot labels and span the strings of words relevant to the corresponding slot.
"Consider the semantic parse S for a sentence W presented in Fig. 1. CalendarTask is the frame tag, ( CalendarTask schedule meeting with ( ByFullName*Person megan hokins) about ( SubjectByWildCard*Subject internal lecture) at ( PreciseTime*Time two thirty p.m.))"
Figure 1: Sample sentence and semantic parse spanning the entire sentence; the remaining ones are slot tags with their corresponding spans.
In the MiPad scenario[REF_CITE]| es-sentially a personal information management (PIM) task | there is a module that is able to convert the information extracted according to the semantic parse into speci c actions.
In this case the action is to schedule a calendar appointment.
We view the problem of information extraction as the recovery of the two-level semantic parse S for a given word sequence W .
We propose a data driven approach to information extraction that uses the structured language model (SLM)[REF_CITE]as an automatic parser.
The parser is constrained to explore only parses that contain pre-set constituents | spanning a given word string and bearing a tag in a given set of semantic tags.
"The constraints available dur-ing training and test are di erent, the test case con-straints being more relaxed as explained in Section 4."
The main advantage of the approach is that it doesn&apos;t require any grammar authoring expertise.
The approach is fully automatic once the annotated training data is provided; it does assume that an application schema | i.e. frame and slot structure | has been de ned but does not require seman-tic grammars that identify word-sequence to slot or frame mapping.
"However, the process of convert-ing the word sequence coresponding to a slot into actionable canonical forms | i.e. convert half past two in the afternoon into 2:30 p.m. | may require grammars."
"The design of the frames | what infor-mation is relevant for taking a certain action, what slot/frame tags are to be used, see[REF_CITE]| is a delicate task that we will not be concerned with for the purposes of this paper."
The remainder of the paper is organized as fol-lows: Section 2 reviews the structured language model (SLM) followed by Section 3 which describes in detail the training procedure and Section 4 which de nes the operation of the SLM as a constrained parser and presents the necessary modi cations to the model.
"Section 5 compares our approach to oth-ers in the literature, in particular that[REF_CITE]."
Section 6 presents the experiments we have carried out.
We conclude with Section 7.
We proceed with a brief review of the structured language model (SLM); an extensive presentation of the SLM can be found[REF_CITE].
The model assigns a probability P ( W;T ) to every sentence W and its every possible binary parse T .
"The terminals of T are the words of W with POStags, and the nodes of T are annotated with phrase headwords and non-terminal labels."
W be a sentence of length n words to which we have prepended the sentence beginning marker &lt;s&gt; and appended the sentence end marker &lt;/s&gt; so that w 0 = &lt;s&gt; and w n +1 = &lt;/s&gt; .
Let W k = w 0 :::w k be the word k -pre x of the sentence | the words from the begining of the sentence up to the current po-sition k | and W k T k the word-parse k -pre x .
"Fig-ure 2 shows a word-parse k -pre x; h_0 .. h_{-m} are the exposed heads , each head being a pair (head-word, non-terminal label), or (word, POStag) in the case of a root-only tree."
The exposed heads at a given position k in the input sentence are a function of the word-parse k -pre x.
The joint probability P ( W; T ) of a word sequence W and a complete parse T can be broken into:
P ( Q W; Q T ) [ P =( w =W n +1 k =1 k k 1
T k 1 ) P ( t k =W k 1 T k 1 ;w k ) i =1 N k P ( p ki =W k 1 T k 1 ; w k ; t k ; p k 1 : : : p ki 1 )] where:
W k 1 T k 1 is the word-parse ( k 1)-pre x w k is the word predicted by WORD-PREDICTOR t k is the tag assigned to w k by the TAGGER
N k 1 is the number of operations the PARSER executes at sentence position k before passing control to the WORD-PREDICTOR (the N k -th operation at position k is the null transition); N k is a function of T p ki denotes the i -th PARSER operation carried out at position k in the word string; the opera-tions performed by the PARSER are illustrated
"Our model is based on three probabilities, each estimated using deleted interpolation and parame-terized (approximated) as follows:"
P ( w k =W k 1 T k 1 ) = : P ( w k =h 0 ;h 1 ) P ( t k =w k ; W k 1 T k 1 ) = : P ( t k =w k ;h 0 ; h 1 ) P ( p ki =W k T k ) = : P ( p ki =h 0 ; h 1 )
It is worth noting that if the binary branching struc-ture developed by the parser were always right-branching and we mapped the POStag and non-terminal label vocabularies to a single type then our model would be equivalent to a trigram lan-guage model.
"Since the number of parses for a given word pre x W k grows exponentially with k , jf T k gj"
"O (2 k ), the state space of our model is huge even for relatively short sentences, so we had to use a search strategy that prunes it."
Our choice was a synchronous multi-stack search algorithm which is very similar to a beam search.
The language model probability assignment for the word at position k + 1 in the input sentence is made using:
P ( w k +1 =W k ) =
P ( w k +1 =W k T k ) ( W k T k ) ; ( W k T k ) =
"P ( W k T k ) = X P ( W T k 2 S k k T k ) (1) T k 2 S k which ensures a proper probability over strings W , where S k is the set of all parses present in our stacks at the current stage k ."
"Each model component | WORD-PREDICTOR, TAGGER, PARSER | is initialized from a set of parsed sentences after undergoing headword percolation and binarization."
Separately for each model component we: gather counts from \main&quot; data | about 90% of the training data estimate the interpolation coeÆcients on counts gathered from \check&quot; data | the remaining 10% of the training data.
An N-best EM[REF_CITE]variant is then employed to jointly re-estimate the model pa-rameters such that the likelihood of the training data under our model is increased.
This section describes the training procedure for the SLM when applied to information extraction and introduces the modi cations that need to be made to the SLM operation.
The training of the model proceeds in four stages: 1. initialize the SLM as a syntactic parser for the domain we are interested in.
A general purpose parser (such as NLPw[REF_CITE]) can be used to generate a syntactic treebank from which the SLM parameters can be initialized.
Another possibility for initializing the SLM is to use a treebank for out-of-domain data (such as the UPenn Treebank[REF_CITE]) | see Section 6.1. 2. train the SLM as a matched constrained parser .
"At this step the parser is going to propose a set of N syntactic binary parses for a given word string (N-best parsing), all matching the con-stituent boundaries speci ed by the semantic parse: a parse T is said to match the seman-tic parse S , denoted T 3 S , if and only if the set of un-labeled constituents that de ne S is included in the set of constituents that de ne T ."
"At this time only the constituent span informa-tion in S is taken into account. 3. enrich the non-terminal and pre-terminal labels of the resulting parses with the semantic tags (frame and slot) present in the semantic parse, thus expanding the vocabulary of non-terminal and pre-terminal tags used by the syntactic parser to include semantic information along-side the usual syntactic tags. 4. train the SLM as a L(abel)-matched constrained parser to explore only the semantic parses for the training data."
"This time the semantic con-stituent labels are taken into account too, which means that a parse P | containing both syn-tactic and semantic information | is said to L(abeled)-match S if and only if the set of la-beled semantic constituents that de nes S is identical to the set of semantic constituents that de nes P ."
"If we let SEM ( P ) denote the func-tion that maps a tree P containing both syntac-tic and semantic information to the tree con-taining only semantic information, referred to as the semantic projection of P , then all the parses P i ; 8 i &lt; N , proposed by the SLM for a given sentence W , L-match S and thus satisfy SEM ( P i ) = S; 8 i &lt; N ."
"The semantic tree S has a two level structure so the above requirement can be satis ed only if the parses SEM ( P ) proposed by the SLM are also on two levels, frame and slot level respec-tively."
We have incorporated this constraint into the operation of the SLM | see Section 4.2.
The model thus trained is then used to parse test sentences and recover the semantic parse us-ing S = SEM (argmax P i P ( P i ; W )).
"In principle, one should sum over all the parses P that yield the same P semantic parse S and then choose S = arg max S P i s:t: SEM ( P i )="
S P ( P i ; W ).
A few iterations of the N-best EM variant | see Section 2 | were run at each of the second and fourth step in the training procedure.
The con-strained parser operation makes this an EM variant where the hidden space | the possible parse trees for a given sentence | is a priori limited by the se-mantic constraints to a subset of the hidden space of the unrestricted model.
At test time we wish to recover the most likely subset of the hidden space consistent with the constraints imposed on the sen-tence.
"To be more speci c, during the second training stage, the E-step of the reestimation procedure will only explore syntactic trees (hidden events) that match the semantic parse; the fourth stage E-steps will consider hidden events that are constrained even further to L-match the semantic parse."
"We have no proof that this procedure should lead to better per-formance in terms of slot/frame accuracy but intu-itively one expects it to place more and more proba-bility mass on the desirable trees | that is, the trees that are consistent with the semantic annotation."
"This is con rmed experimentally by the fact that the likelihood of the training word sequence (observ-able) | calculated by Eq. (1) where the sum runs over the parse trees that match/L-match the seman-tic constraints | does increase [Footnote_1] at every training step, as presented in Section 6, Table 1."
1 Equivalent to a decrease in perplexity
"However, the increase in likelihood is not always correlated with a decrease in error rate on the training data, see Tables 2 and 3 in Section 6."
We now detail the constrained operation of the SLM | matched and L-matched parsing | used at the second and fourth steps of the training procedure described in the previous section.
A semantic parse S for a given sentence W con-sists of a set of constituent boundaries along with semantic tags.
"When parsing the sentence using the standard formulation of the SLM, one obtains binary parses that are not guaranteed to match the seman-tic parse S , i.e. the constituent proposed by the SLM may cross semantic constituent boundaries; for the constituents matching the semantic constituent boundaries, the labels proposed may not be the de-sired ones."
"To x terminology, we de ne a constrained con-stituent | or simply a constraint | c to be a span together with a set 2 of allowable tags for the span: c = &lt; l;r;Q &gt; where l is the left boundary of the constraint, r is the right boundary of the constraint and Q is the set of allowable non-terminal tags for the constraint."
"A semantic parse can be viewed as a set of con-straints; for each constraint the set of allowable non-terminal tags Q contains a single element, respec-tively the semantic tag for each constituent."
An ad-ditional fact to be kept in mind is that the semantic parse tree consists of exactly two levels: the frame level (root semantic tag) and the slot level (leaf se-mantic tags).
"During training, we wish to constrain the SLM op-eration such that it considers only parses that match the constraints c i ; i = 1 : : :C as it proceeds left to right through a given sentence W ."
"In light of the training procedure sketched in the introduction, we consider two avors of constrained parsing, one in which we only generate parses that match the con-straint boundaries and another in which we also en-force that the proposed tag for every matching con-stituent is among the constrained set of non-terminal tags c i :Q | L(abeled)-match constrained parsing."
The only constraints available for the test sen-tences are: the semantic tag of the root node | which spans the entire sentence | must be in the set of frame tags.
"If it were a test sen-tence the example in Figure 1 would have the following semantic parse (constraints): ( {CalendarTask,ContactsTask,MailTask} schedule meeting with megan hokins about internal lecture to two thirty p.m.) the semantic projection of the trees proposed by the SLM must have exactly two levels; this constraint is built in the operation of the L-match parser."
The next section will describe the constrained parsing algorithm.
"Section 4.2 will describe fur-ther changes that the algorithm uses to produce only parses P whose semantic projection SEM ( P ) has exactly two levels, frame (root) and slot (leaf) level, respectively | only in the L-match case."
We conclude with Section 4.3 explaining how the con-strained parsing algorithm interacts with the prun-ing of the SLM search space for the most likely parse.
The trees produced by the SLM are binary trees.
The tags annotating the nodes of the tree are purely syntactic | during the second training stage | or syntactic+semantic | during the last training stage or at test time.
"It can be proved that satisfying the following two conditions at each position k in the input sentence ensures that all the binary trees generated by the SLM parsing algorithm match the pre-set constraints c i ; i = 1 : : : C as it proceeds left to right through the input sentence W = w 0 : : : w n +1 . for a given word-parse k -pre x W k T k (see Sec-tion 2) accept an adjoin transition if and only if: 1. the resulting constituent does not violate [Footnote_3] any of the constraints c i ; i = 1 : : : C [Footnote_2]. L-match parsing only: if the seman-tic projection of the non-terminal tag SEM ( NTtag ) proposed by the adjoin op-eration is non-void then the newly created constituent must L-match an existing con-straint, 9 c i s:t: SEM ( NTtag ) 2 c i :Q . for a given word-parse k -pre x W k T k (see Sec-tion 2) accept the null transition if and only if all the constraints c i whose right boundary is equal to the current word index k , c i :r = k , have been matched."
3 A constraint is violated by a constituent if the span of the constituent crosses the span of the constraint.
2 The set of allowable tags must contain at least one ele-ment
If these constraints remain un-matched they will be broken at a later time during the process of completing the parse for the current sentence W : there will be an adjoin operation involving a constituent to the right of the current position that will break all the constraints ending at the current position k .
"The two-layer structure of the semantic trees need not be enforced during training, simply L-matching the semantic constraints will implicitly satisfy this constraint."
"As explained above, for test sentences we can only specify the frame level constraint, leav-ing open the possibility of generating a tree whose semantic projection would contain more than two levels | nested slot level constituents."
"In order to avoid this, each tree in a given word-parse has two bits that describe whether the tree already con-tains a constituent whose semantic projection is a frame/slot level tag, respectively."
An adjoin opera-tion proposing a tag that violates the correct layer-ing of frame/slot level tags can now be detected and discarded.
In the absence of pruning the search for the most likely parse satisfying the constraints for a given sentence becomes computationally intractable 4 .
"In practice, we are forced to use pruning techniques in order to limit the size of the search space."
"However, it is possible that during the left to right traversal of the sentence, the pruning scheme will keep alive only parses whose continuation cannot meet constraints that we have not encountered yet and no complete parse for the current sentence can be returned."
"In such cases, we back-o to unconstrained parsing | regular SLM usage."
"In our experiments, we noticed that this was necessary for very few training sen-tences (1 out of 2,239) and relatively few test sen-tences (31 out of 1,101)."
The use of a syntactic parser augmented with se-mantic tags for information information from text is not a novel idea.
The basic approach we described is very similar to the one presented[REF_CITE]however there are a few major di erences: in our approach the augmentation of the syn-tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5 .
The approach[REF_CITE]needs to insert additional nodes in the syntactic tree to account for the seman-tic constituents that do not have a correspond-ing syntactic one.
We believe our approach en-sures tighter coupling between the syntactic and the semantic information in the nal augmented trees. our constraint de nition allows for a set of se-mantic tags to be matched on a given span. the two-level layering constraint on semantic trees is a structural constraint that is embed-ded in the operation of the SLM and thus can be guaranteed on test sentences.
The semantic annotation required by our task is much simpler than that employed[REF_CITE].
"One possibly bene cial extension of our work suggested[REF_CITE]would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level."
"However, this would complicate the task of data annotation making it more expensive."
The same constrained EM variant employed for reestimating the model parameters has been used[REF_CITE]for training a purely syntactic parser showing increase in likelihood but no improvement in parsing accuracy.
We have evaluated the model on manually annotated data for the MiPad[REF_CITE]task.
"We have used 2,239 sentences (27,119 words) for train-ing and 1,101 sentences (8,652 words) for test."
"There were 2,239/[Footnote_5],431 semantic frames/slots in the train-ing data and 1,101/1,698 in the test data, respec-tively."
5 This is a consequence of the fact that the SLM generates binary trees
"The word vocabulary size was 1,035, closed over the test data."
"The slot and frame vocabulary sizes were 79 and 3, respectively."
"The pre-terminal (POStag) vocabulary sizes were 64 and 144 for train-ing steps 2 and 4 (see Section 3), respectively; the non-terminal (NTtag) vocabulary sizes were 61 and 540 for training steps 2 and 4 (see Section 3), re-spectively."
We have used the NLPw[REF_CITE]parser to obtain the MiPad syntactic treebank needed for initializing the SLM at training step 1.
"Although not guaranteed theoretically, the N-best EM variant used for the SLM parameter reestima-tion increases the likelihood of the training data with each iteration when the parser is run in both matched (training step 2) and L-matched (training step [Footnote_4]) constrained modes."
"4 It is assumed that the constraints for a given sentence are consistent, namely there exists at least one parse that meets all of them."
Table 1 shows the evo- lution of the training and test data perplexities (cal-culated using the probability assignment in Eq. 1) during the constrained training steps 2 and 4.
The training data perplexity decreases monoton-ically during both training steps whereas the test data perplexity doesn&apos;t decrease monotonically in ei-ther case.
We attribute this discrepancy between the evolution of the likelihood on the training and test corpora to the di erent constrained settings for the SLM.
The most important performance measure is the slot/frame error rate.
"To measure it, we use man-ually created parses which consist of frame-level la-bels and slot-level labels and spans as reference."
A frame-level error is caused by a frame label of the hypothesis parse which is di erent from the frame label of the reference.
"In order to calculate the slot-level errors, we create a set of slot label and slot span pairs for the reference and hypothesis parse, respectively."
"The number of slot errors is then the minimum edit distance between these 2 sets using the substitution, insertion and deletion operations on the elements of the set."
Table 2 shows the error rate on training and test data at di erent stages during training.
The last col-umn of test data results (Test-L1) shows the results obtained by assuming that the user has speci ed the identity of the frame | and thus the frame level con-straint contains only the correct semantic tag.
This is a plausible scenario if the user has the possibility to choose the frame using a di erent input modality such as a stylus.
The error rates on the training data were calculated by running the model with the same constraint as on the test data | constraining the set of allowable tags at the frame level.
This could be seen as an upper bound on the performance of the model (since the model parameters were estimated on the same data).
Our model signi cantly outperforms the baseline model | a simple semantic context free grammar authored manually for the MiPad task | in terms of slot error rate (about 35% relative reduction in slot error rate) but it is outperformed by the latter in terms of frame error rate.
When running the mod-els from training step 2 on test data one cannot add any constraints; only frame level constraints can be used when evaluating the models from training step 4 on test data.
"N-best reestimation at either train-ing stage (2 or 4) doesn&apos;t improve the accuracy of the system, although the results obtained by intial-izing the model using the reestimated stage 2 model | iteration 2- f 0,1,2 g models tend to be slightly bet-ter than their 0- f 0,1,2 g counterparts."
"Constraining the frame level tag to have the correct value doesn&apos;t signi cantly reduce the slot error rate in either ap-proach, as can be seen from the Test-L1 column of results [Footnote_6] ."
"6 The frame error rate in this column should be 0; in prac-tice this doesn&apos;t happen because some test sentences could not be L-match parsed using the pruning strategy employed by the SLM, see Section 4.3"
Recent results[REF_CITE]on the portability of syntactic structure within the SLM framework show that it is possible to initialize the SLM parameters from a treebank for out-of-domain text and main-tain the same language modeling performance.
We have repeated the experiment in the context of in-formation extraction.
"Similar to the approach[REF_CITE]we initialized the SLM statistics from the UPenn Tree-bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3."
The remain-ing part of the training procedure was the same as in the previous set of experiments.
"The word, slot and frame vocabulary were the same as in the previous set of experiments."
"The pre-terminal (POStag) vocabulary sizes were 40 and 204 for training steps 2 and 4 (see Section 3), respec-tively; the non-terminal (NTtag) vocabulary sizes were 52 and 434 for training steps 2 and 4 (see Sec-tion 3), respectively."
"The results are presented in Table 3, showing im-proved performance over the model initialized from in-domain parse trees."
"The frame accuracy increases substantially, almost matching that of the baseline model, while the slot accuracy is just slightly in-creased."
We attribute the improved performance of the model initialized from the UPenn Treebank to the fact that the model explores a more diverse set of trees for a given sentence than the model initial-ized from the MiPad automatic treebank generated using the NLPwin parser.
We have also evaluated the impact of the training data size on the model performance.
"The results are presented in Table 4, showing a strong dependence of both the slot and frame error rates on the amount of training data used."
"This, together with the high accuracy of the model on training data (see Table 3), suggests that we are far from saturation in perfor-mance and that more training data is very likely to improve the model performance substantially."
"As a summary error analysis, we have investigated the correlation between the semantic frame/slot er-ror rate and the number of semantic slots in a sen-tence."
We have binned the sentences in the test set according to the number of slots in the manual an- notation and evaluated the frame/slot error rate in each bin.
The results are shown in Table 5.
The frame/slot accuracy increases with the num-ber of slots per sentence | except for the 5+ bin where the frame error rate increases | showing that slot co-ocurence statistics improve performance; sen-tences containing more semantic slots tend to be less ambiguous from an information extraction point of view.
"We have presented a data-driven approach to infor-mation extraction that, despite the small amount of training data used, is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad | personal infor-mation management | task."
"The performance of the baseline model could be improved with more authoring e ort, although this is expensive."
"The big di erence in performance between train-ing and test and the fact that we are using so little training data, makes improvements by using more training data very likely, although this may be expensive."
A framework which utilizes the vast amounts of text data collected once such a system is deployed would be desirable.
"Statistical model-ing techniques that make more e ective use of the training data should be used in the SLM, maximum entropy[REF_CITE]being a good candidate."
"As for using the SLM as the language understand-ing component of a speech driven application, such as MiPad, it would be interesting to evaluate the im-pact of incorporating the semantic constraints on the word-level accuracy of the system."
Another possible research direction is to modify the framework such that it nds the most likely semantic parse given the acoustics | thus treating the word sequence as a hidden variable.
"We are developing corpus-based techniques for iden-tifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in tra-ditional knowledge representation systems)."
In this paper we describe a classification algorithm for iden-tifying relationships between two-word noun com-pounds.
"We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from train-ing instances, performing better on previously un-seen words than a baseline consisting of training on the words themselves."
We are exploring empirical methods of determin-ing semantic relationships between constituents in natural language.
"Our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make inferences about propositions that hold between sci-entific concepts within biomedical texts[REF_CITE]."
"One of the important challenges of biomedical text, along with most other technical text, is the proliferation of noun compounds."
A typical article title is shown below; it consists a cascade of four noun phrases linked by prepositions:
"Open-labeled long-term study of the effi-cacy, safety, and tolerability of subcuta-neous sumatriptan in acute migraine treat-ment."
"The real concern in analyzing such a title is in de-termining the relationships that hold between differ-ent concepts, rather than on finding the appropriate attachments (which is especially difficult given the lack of a verb)."
"And before we tackle the prepo-sitional phrase attachment problem, we must find a way to analyze the meanings of the noun com-pounds."
"Our goal is to extract propositional information from text, and as a step towards this goal, we clas- sify constituents according to which semantic rela-tionships hold between them."
"For example, we want to characterize the treatment-for-disease relation-ship between the words of migraine treatment ver-sus the method-of-treatment relationship between the words of aerosol treatment."
"These relations are intended to be combined to produce larger proposi-tions that can then be used in a variety of interpreta-tion paradigms, such as abductive reasoning[REF_CITE]or inductive logic programming[REF_CITE]."
"Note that because we are concerned with the se-mantic relations that hold between the concepts, as opposed to the more standard, syntax-driven com-putational goal of determining left versus right as-sociation, this has the fortuitous effect of changing the problem into one of classification, amenable to standard machine learning classification techniques."
We have found that we can use such algorithms to classify relationships between two-word noun com-pounds with a surprising degree of accuracy.
A one-out-of-eighteen classification using a neural net achieves accuracies as high as 62%.
"By taking ad-vantage of lexical ontologies, we achieve strong re-sults on noun compounds for which neither word is present in the training set."
"Thus, we think this is a promising approach for a variety of semantic label-ing tasks."
"The reminder of this paper is organized as follows: Section 2 describes related work, Section 3 describes the semantic relations and how they were chosen, and Section 4 describes the data collection and on-tologies."
"In Section 5 we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method."
Section 6 concludes the paper and discusses future work.
Several approaches have been proposed for empiri-cal noun compound interpretation.
"Several researchers have tackled the syntactic analysis[REF_CITE], usually using a variation of the idea of find-ing the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured."
"We are interested in the third task, interpretation of the underlying semantics."
Most related work re - lies on hand-written rules of one kind or another.
1 Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation.
"In the related sub-area of information extracti[REF_CITE], the main goal is to find every instance of particular entities or events of in-terest."
"These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates."
"Our goals are more general than those of information extraction, and so this work should be helpful for that task."
"How-ever, our approach will not solve issues surrounding previously unseen proper nouns, which are often im-portant for information extraction tasks."
"There have been several efforts to incorporate lex-ical hierarchies into statistical processing, primar-ily for the problem of prepositional phrase (PP) attachment."
"The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, deter-mine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with."
"Because the data is sparse, empirical methods that train on word occurrences alone[REF_CITE]have been supplanted by algorithms that gen-eralize one or both of the nouns according to class-membership measures[REF_CITE], but the statistics are computed for the par-ticular preposition and verb."
It is not clear how to use the results of such anal-ysis after they are found; the semantics of the rela- tionship between the terms must still be determined.
"In our framework we would cast this problem as finding the relationship R(p, n2) that best character-izes the preposition and the NP that follows it, and then seeing if the categorization algorithm deter-mines their exists any relationship R (n1, R(p, n2)) or R (v, R(p, n2))."
The algorithms used in the related work reflect the fact that they condition probabilities on a particular verb and noun.
Resnik (1993; 1995) use classes in Wordnet[REF_CITE]and a measure of concep-tual association to generalize over the nouns.
Our approach differs from these in that we are us-ing machine learning techniques to determine which level of the lexical hierarchy is appropriate for gen-eralizing across nouns.
"In this work we aim for a representation that is in-termediate in generality between standard case roles (such as Agent, Patient, Topic, Instrument), and the specificity required for information extraction."
"We have created a set of relations that are sufficiently general to cover a significant number of noun com-pounds, but that can be domain specific enough to be useful in analysis."
"We want to support relation-ships between entities that are shown to be impor-tant in cognitive linguistics, in particular we intend to support the kinds of inferences that arise from Talmy’s force dynamics[REF_CITE]."
"It has been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or op-posed to, a proposal)[REF_CITE]."
"In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chem-ical removes an entity that is blocking the passage of a fluid through a channel."
The problem remains of determining what the ap-propriate kinds of relations are.
"In theoretical lin-guistics, there are contradictory views regarding the semantic properties of noun compounds (NCs)."
Be-tween these two extremes lies Warren’s (1978) tax-onomy of six major semantic relations organized into a hierarchical structure.
We have identified the 38 relations shown in Ta-ble 1.
"We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren, but in many cases these are inappropriate."
"Levi’s classes are too general for our purposes; for example, she collapses the “location” and “time” relationships into one single class “In” and there-fore field mouse and autumnal rain belong to the same class."
"Warren’s classification schema is much more detailed, and there is some overlap between the top levels of Warren’s hierarchy and our set of relations."
"For example, our “Cause (2-1)” for flu virus corresponds to her “Causer-Result” of hay fever, and our “Person Afflicted” (migraine patient) can be thought as Warren’s “Belonging-Possessor” of gunman."
"Warren differentiates some classes also on the basis of the semantics of the constituents, so that, for example, the “Time” relationship is di-vided up into “Time-Animate Entity” of weekend guests and “Time-Inanimate Entity” of Sunday pa-per."
Our classification is based on the kind of re-lationships that hold between the constituent nouns rather than on the semantics of the head nouns.
"For the automatic classification task, we used only the 18 relations (indicated in bold in Table 1) for which an adequate number of examples were found in the current collection."
"Many NCs were ambigu-ous, in that they could be described by more than one semantic relationship."
"In these cases, we sim-ply multi-labeled them: for example, cell growth is both “Activity” and “Change”, tumor regression is “Ending/reduction” and “Change” and bladder dys-function is “Location” and “Defect”."
Our approach handles this kind of multi-labeled classification.
Two relation types are especially problematic.
"Some compounds are non-compositional or lexical-ized, such as vitamin k and e[Footnote_2] protein; others defy classification because the nouns are subtypes of one another."
"2 The percentage of the word pairs extracted that were not true NCs was about 6%; some examples are: treat migraine, ten patient, headache more. We do not know, however, how many NCs we missed. The errors occurred when the wrong label was assigned by the tagger (see Section 4)."
"This group includes migraine headache, guinea pig, and hbv carrier."
We placed all these NCs in a catch-all category.
We also included a “wrong” category containing word pairs that were incorrectly labeled as NCs. 2
The relations were found by iterative refinement based on looking at 2245 extracted compounds (de-scribed in the next section) and finding commonal-ities among them.
Labeling was done by the au-thors of this paper and a biology student; the NCs were classified out of context.
"We expect to con-tinue development and refinement of these relation-ship types, based on what ends up clearly being use- ful “downstream” in the analysis."
"The end goal is to combine these relationships in NCs with more that two constituent nouns, like in the example intranasal migraine treatment of Sec-tion 1."
"To create a collection of noun compounds, we per-formed searches from MedLine, which contains ref-erences and abstracts from 4300 biomedical journals."
"We used several query terms, intended to span across different subfields."
We retained only the titles and the abstracts of the retrieved documents.
On these titles and abstracts we ran a part-of-speech tagger[REF_CITE]and a program that extracts only sequences of units tagged as nouns.
"We ex-tracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents."
The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine[REF_CITE].
We use the MetaThe-saurus component to map lexical items into unique concept IDs (CUIs). [Footnote_3]
3 In some cases a word maps to more than one CUI; for the work reported here we arbitrarily chose the first mapping in all cases. In future work we will explore how to make use of all of the mapped terms.
The UMLS also has a map-ping from these CUIs into the MeSH lexical hier-archy[REF_CITE]; we mapped the CUIs into MeSH terms.
"There are about 19,000 unique main terms in MeSH, as well as additional modifiers."
"For example, tree A cor-responds to Anatomy, tree B to Organisms, and so on."
"The longer the name of the MeSH term, the longer the path from the root and the more precise the description."
"For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Ner-vous System Diseases) and so on."
We use the MeSH hierarchy for generalization across classes of nouns; we use it instead of the other resources in the UMLS primarily because of MeSH’s hierarchical structure.
"For these experiments, we considered only those noun compounds for which both nouns can be mapped into MeSH terms, re-sulting in a total of 2245 NCs."
"Because we have defined noun compound relation determination as a classification problem, we can make use of standard classification algorithms."
"In particular, we used neural networks to classify across all relations simultaneously. shown in boldface are those used in the experiments reported on here."
Relation ID numbers are shown in parentheses by the relation names.
The second column shows the number of labeled examples for each class; the last row shows a class consisting of compounds that exhibit more than one relation.
The notation (1-2) and (2-1) indicates the directionality of the relations.
"For example, Cause (1-2) indicates that the first noun causes the second, and Cause (2-1) indicates the converse."
We ran the experiments creating models that used different levels of the MeSH hierarchy.
"For example, for the NC flu vaccination, flu maps to the MeSH term D4.808.54.79.429.154.349 and vaccination to G3.770.670.310.890."
Flu vaccination for Model 4 would be represented by a vector consisting of the concatenation of the two descriptors showing only the first four levels: D4.808.54.79 G3.770.670.310 (see Table 2).
"When a word maps to a general MeSH term (like treatment, Y11) zeros are appended to the end of the descriptor to stand in place of the missing values (so, for example, treatment in Model 3 is Y 11 0, and in Model 4 is Y 11 0 0, etc.)."
The numbers in the MeSH descriptors are cate-gorical values; we represented them with indicator variables.
"That is, for each variable we calculated the number of possible categories c and then repre-sented an observation of the variable as a sequence of c binary variables in which one binary variable was one and the remaining c − 1 binary variables were zero."
We also used a representation in which the words themselves were used as categorical input variables (we call this representation “lexical”).
For this col-lection of NCs there were 1184 unique nouns and therefore the feature vector for each noun had 1184 components.
In Table 3 we report the length of the feature vectors for one noun for each model.
The en-tire NC was described by concatenating the feature vectors for the two nouns in sequence.
The NCs represented in this fashion were used as input to a neural network.
We used a feed-forward network trained with conjugate gradient descent.
"The network had one hidden layer, in which a hy-perbolic tangent function was used, and an output layer representing the 18 relations."
"A logistic sig-moid function was used in the output layer to map the outputs into the interval (0, 1)."
The number of units of the output layer was the number of relations (18) and therefore fixed.
The network was trained for several choices of numbers of hidden units; we chose the best-performing networks based on training set error for each of the models.
We subsequently tested these networks on held-out testing data.
We compared the results with a baseline in which logistic regression was used on the lexical features.
"Given the indicator variable representation of these features, this logistic regression essentially forms a table of log-odds for each lexical item."
We also com-pared to a method in which the lexical indicator vari-ables were used as input to a neural network.
"This approach is of interest to see to what extent, if any, the MeSH-based features affect performance."
Note also that this lexical neural-network approach is fea-sible in this setting because the number of unique words is limited (1184) – such an approach would not scale to larger problems.
In Table 4 and in Figure 1 we report the results from these experiments.
Neural network using lex-ical features only yields 62% accuracy on average across all 18 relations.
A neural net trained on Model 6 using the MeSH terms to represent the nouns yields an accuracy of 61% on average across all 18 relations.
"Note that reasonable performance is also obtained for Model 2, which is a much more gen-eral representation."
Table 4 shows that both meth-ods achieve up to 78% accuracy at including the cor-rect relation among the top three hypothesized.
Multi-class classification is a difficult problem[REF_CITE].
"In this problem, a baseline in which the algorithm guesses yields about 5% accuracy."
"We see that our method is a significant improvement over the tabular logistic-regression-based approach, which yields an accuracy of only 31 percent."
"Addi-tionally, despite the significant reduction in raw in-formation content as compared to the lexical repre-sentation, the MeSH-based neural network performs as well as the lexical-based neural network. (And we again stress that the lexical-based neural network is not a viable option for larger domains.)"
Figure 2 shows the results for each relation.
"MeSH-based generalization does better on some re-lations (for example 14 and 15) and Lexical on others (7, 22)."
It turns out that the test set for relation-ship 7 (“Produces on a genetic level”) is dominated by NCs containing the words alleles and mrna and that all the NCs in the training set containing these words are assigned relation label 7.
"A similar situa-tion is seen for relation 22, “Time(2-1)”."
"In the test set examples the second noun is either recurrence, season or time."
"In the training set, these nouns ap-pear only in NCs that have been labeled as belonging to relation 22."
"On the other hand, if we look at relations 14 and 15, we find a wider range of words, and in some cases the words in the test set are not present in the train-ing set."
"In the training set, NCs with vaccine in it have also been classified as “Instrument” (anti-gen vaccine, polysaccharide vaccine), as “Object” (vaccine development), as “Subtype of” (opv vac-cine) and as “Wrong” (vaccines using)."
"Other words in the test set for 14 are varicella which is present in the trainig set only in varicella serology labeled as “Attribute of clinical study”, drainage which is in the training set only as “Location” (gallbladder drainage and tract drainage) and “Activity” (bile drainage)."
Other test set words such as immunisa-tion and carcinogen do not appear in the training set at all.
"In other words, it seems that the MeSHk-based categorization does better when generalization is re-quired."
"Additionally, this data set is “dense” in the sense that very few testing words are not present in the training data."
This is of course an unrealistic situation and we wanted to test the robustness of the method in a more realistic setting.
The results reported in Table 4 and in Figure 1 were obtained splitting the data into 50% training and 50% testing for each relation and we had a total of 855 training points and 805 test points.
"Of these, only 75 ex-amples in the testing set consisted of NCs in which both words were not present in the training set."
We decided to test the robustness of the MeSH-based model versus the lexical model in the case of unseen words; we are also interested in seeing the relative importance of the first versus the second noun.
"Therefore, we split the data into 5% training (73 data points) and 95% testing (1587 data points) and partitioned the testing set into 4 subsets as fol-lows (the numbers in parentheses are the numbers of points for each case): for these test set partitions."
Figure 3 shows that the MeSH-based models are more robust than the lexical when the number of unseen words is high and when the size of training set is (very) small.
"In this more realistic situation, the MeSH models are able to generalize over previously unseen words."
"For unseen words, lexical reduces to guessing. [Footnote_4]"
"4 Note that for unseen words, the baseline lexical-based logistic regression approach, which essentially builds a tabular representation of the log-odds for each class, also reduces to random guessing."
Figure 4 shows the accuracy for the MeSH based-model for the the four cases of Table 5.
It is interest-ing to note that the accuracy for Case 1 (first noun not present in the training set) is much higher than the accuracy for Case 2 (second noun not present in the training set).
This seems to indicate that the second noun is more important for the classification that the first one.
We have presented a simple approach to corpus-based assignment of semantic relations for noun compounds.
The main idea is to define a set of rela-tions that can hold between the terms and use stan-dard machine learning techniques and a lexical hi-erarchy to generalize from training instances to new examples.
The initial results are quite promising.
In this task of multi-class classification (with 18 classes) we achieved an accuracy of about 60%.
These results can be compared[REF_CITE]who reports an accuracy of 52% with 13 classes and[REF_CITE]whose algorithm achieves about 80% accuracy for a much simpler binary clas-sification.
We have shown that a class-based representation performes as well as a lexical-based model despite the reduction of raw information content and de- spite a somewhat errorful mapping from terms to concepts.
We have also shown that representing the nouns of the compound by a very general represen-tation (Model 2) achieves a reasonable performance of aout 52% accuracy on average.
This is particu-larly important in the case of larger collections with a much bigger number of unique words for which the lexical-based model is not a viable option.
Our results seem to indicate that we do not lose much in terms of accuracy using the more compact MeSH representation.
We have also shown how MeSH-besed models out perform a lexical-based approach when the num-ber of training points is small and when the test set consists of words unseen in the training data.
This indicates that the MeSH models can generalize successfully over unseen words.
Our approach han-dles “mixed-class” relations naturally.
"For the mixed class Defect in Location, the algorithm achieved an accuracy around 95% for both “Defect” and “Loca-tion” simultaneously."
Our results also indicate that the second noun (the head) is more important in determining the relationships than the first one.
In future we plan to train the algorithm to allow different levels for each noun in the compound.
"We also plan to compare the results to the tree cut algo-rithm reported[REF_CITE], which allows different levels to be identified for different subtrees."
We also plan to tackle the problem of noun com-pounds containing more than two terms.
In this paper we describe a morphological analy-sis method based on a maximum entropy model.
This method uses a model that can not only consult a dictionary with a large amount of lex-ical information but can also identify unknown words by learning certain characteristics.
The model has the potential to overcome the un-known word problem.
Morphological analysis is one of the basic tech-niques used in Japanese sentence analysis.
"A morpheme is a minimal grammatical unit, such as a word or a , and morphological analysis is the process segmenting a given sentence into a row of morphemes and assigning to each mor-pheme grammatical attributes such as a part-of-speech (POS) and an in ection type."
"One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two sta-tistical approaches to this problem."
"One is to acquire unknown words from corpora and put them into a dictionary (e.g.,[REF_CITE]), and the other is to estimate a model that can identify unknown words correctly (e.g.,[REF_CITE])."
We would like to be able to make good use of both approaches.
"If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem."
Mori and Nagao proposed a statisti-cal model that can consult a dictionary[REF_CITE].
"In their model the proba-bility that a string of letters or characters is z New[REF_CITE]Broadway, 7th oor New York,[REF_CITE]USA sekine@cs.nyu.edu a morpheme is augmented when the string is found in a dictionary."
"The improvement of the accuracy was slight, however, so we think that it is  to  integrate the mecha-nism for consulting a dictionary into an n-gram model."
In this paper we therefore describe a morphological analysis method based on a max-imum entropy (M.E.) model.
This method uses a model that can not only consult a dictionary but can also identify unknown words by learn-ing certain characteristics.
"To learn these char-acteristics, we focused on such information as whether or not a string is found in a dictio-nary and what types of characters are used in a string."
The model estimates how likely a string is to be a morpheme according to the informa-tion on hand.
"When our method was used to identify morpheme segments in sentences in the Kyoto University corpus and to identify the ma-jor parts-of-speech of these morphemes, the re-call and precision were respectively 95.80% and 95.09%."
This section describes a model which estimates how likely a string is to be a morpheme.
We implemented this model within an M.E. frame-work.
"Given a tokenized test corpus, the problem of Japanese morphological analysis can be re-duced to the problem of assigning one of two tags to each string in a sentence."
A string is tagged with a 1 or a 0 to indicate whether or not it is a morpheme.
"When a string is a mor-pheme, a grammatical attribute is assigned to it."
"The 1 tag is thus divided into the num-ber, n , of grammatical attributes assigned to morphemes, and the problem is to assign an at-tribute (from 0 to n ) to every string in a given sentence."
The ( n +1) tags form the space of \fu- tures&quot; in the M.E. formulation of our problem of morphological analysis.
"The M.E. model, as well as other similar models, enables the com-putation of P ( f j h ) for any future f from the space of possible futures, F , and for every his-tory, h , from the space of possible histories, H ."
A \history&quot; in M.E. is all of the conditioning data that enable us to make a decision in the space of futures.
"In the problem of morphologi-cal analysis, we can reformulate this in terms of nding the probability of f associated with the relationship at index t in the test corpus:"
P ( f j h t ) =
P ( f j Information derivable from the test corpus related to relationship t )
The computation of P ( f j h ) in any M.E. models is dependent on a set of \features&quot; which would be helpful in making a prediction about the fu-ture.
"Like most current M.E. models in com-putational linguistics, our model is restricted to those features which are binary functions of the history and future."
"For instance, one of our fea-tures is 8 &gt; 1 : if has( h;x ) = true ; g ( h;f ) = &lt;&gt;: 0 : otherwise&amp; x = \[REF_CITE](Major) : verb ; (1) 00 f = 1 :"
"Here \has( h , x )&quot; is a binary function that re-turns true if the history h has feature x ."
"In our experiments, we focused on such information as whether or not a string is found in a dictionary, the length of the string, what types of characters are used in the string, and the part-of-speech of the adjacent morpheme."
"Given a set of features and some training data, the M.E. estimation process produces a model in which every feature g i has an associ-ated parameter i ."
This enables us to compute the conditional probability as follows[REF_CITE]:
P ( f j h ) =
Q i gi i ( h;f ) (2) Z ( h ) =
X Z Y ( h ) gi ( i h;f ) : (3) f i
"The M.E. estimation process guarantees that for every feature g i , the expected value of g i accord-ing to the M.E. model will equal the empirical expectation of g i in the training corpus."
"In other words,"
X P ~( h; f ) 1 g i ( h; f ) h;f = X P ~( h ) 1 X P M:E: ( f j h ) 1 g i ( h; f ) : (4) h f
Here P ~ is an empirical probability and P M:E: is the probability assigned by the model.
We de ne part-of-speech and bunsetsu boundaries as grammatical attributes.
Here a bunsetsu is a phrasal unit consisting of one or more morphemes.
"When there are m types of parts-of-speech, and the left-hand side of each morpheme may or may not be a bunsetsu boundary, the number, n , of grammatical at-tributes assigned to morphemes is 2 2 m . 1"
We propose a model which estimates the likelihood that a given string is a morpheme and has the grammatical attribute i ([Footnote_1] i n ).
1 Not only morphemes but also bunsetsus can be iden-ti ed by considering the information related to their bun-setsu boundaries.
We call it a morpheme model .
"This model is represented by Eq. (2), in which f can be one of ( n + 1) tags from 0 to n ."
"A given sentence is divided into morphemes, and a grammatical attribute is assigned to each morpheme so as to maximize the sentence prob-ability estimated by our morpheme model."
Sen-tence probability is de ned as the product of the probabilities estimated for a particular division of morphemes in a sentence.
We use the Viterbi algorithm to nd the optimal set of morphemes in a sentence and we use the method proposed by Nagata[REF_CITE]to search for the N-best sets.
The part-of-speech categories that we used fol-low those of JUMAN[REF_CITE].
The number of grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary.
We do not identify in ection types probabilistically since they can be almost perfectly identi ed by check-ing the spelling of the current morpheme after a part-of-speech has been assigned to it.
"There-fore, f in Eq. (2) can be one of 107 tags from 0 to 106."
"We used the Kyoto University text corpus (Version 2)[REF_CITE], a tagged corpus of the Mainichi newspaper."
"For training, we used 7,958 sentences from newspa-per articles appearing from January 1 to Jan-uary 8, 1995, and for testing, we used 1,246 sentences from articles appearing[REF_CITE]."
"Given a sentence, for every string consisting of ve or less characters and every string ap-pearing in the JUMAN dictionary[REF_CITE], whether or not the string is a morpheme was determined and then the gram-matical attribute of each string determined to be a morpheme was identi ed and assigned to that string."
The maximum length was set at ve because morphemes consisting of six or more characters are mostly compound words or words consisting of katakana characters.
The stipula-tion that strings consisting of six or more char-acters appear in the JUMAN dictionary was set because long strings not present in the JUMAN dictionary were rarely found to be morphemes in our training corpus.
"Here we assume that compound words that do not appear in the JU-MAN dictionary can be divided into strings con-sisting of ve or less characters because com-pound words tend not to appear in dictionar-ies, and in fact, compound words which con-sist of six or more characters and do not ap-pear in the dictionary were not found in our training corpus."
"Katakana strings that are not found in the JUMAN dictionary were assumed to be included in the dictionary as an entry having the part-of-speech \Unknown(Major), Katakana(Minor).&quot; An optimal set of mor-phemes in a sentence is searched for by em-ploying the Viterbi algorithm under the con-dition that connectivity rules de ned between parts-of-speech in JUMAN must be met."
"The assigned part-of-speech in the optimal set is not always selected from the parts-of-speech at-tached to entries in the JUMAN dictionary, but may also be selected from the 53 categories of the M.E. model."
"It is  to select an appro-priate category from the 53 when there is little training data, so we assume that every entry in the JUMAN dictionary has all possible parts-of-speech, and the part-of-speech assigned to each morpheme is selected from those attached to the entry corresponding to the morpheme string."
The features used in our experiments are listed in Table 1.
"Each row in Table 1 contains a feature type, feature values, and an experimen-tal result that will be explained later."
Each fea-ture consists of a type and a value.
The features are basically some attributes of the morpheme itself or those of the morpheme to the left of it.
"We used the 31,717 features that were found three or more times in the training corpus."
The notations \(0)&quot; and \(-1)&quot; used in the feature type column in Table 1 respectively indicate a target string and the morpheme on the left of it.
The terms used in the table are the following:
String: Strings which appeared as a morpheme ve or more times in the training corpus Length: Length of a string POS: Part-of-speech. \Major&quot; and \Minor&quot; respectively indicate major and minor part-of-speech categories as de ned in JUMAN.
"In ection type as de ned in JUMAN Dic: We use the JUMAN dictionary, which has about 200,000 entries[REF_CITE]. \Major&amp;Minor&quot; indicates pos-sible combinations between major and mi-nor part-of-speech categories."
"When the target string is in the dictionary, the part-of-speech attached to the entry correspond-ing to the string is used as a feature value."
"If an entry has two or more parts-of-speech, the part-of-speech which leads to the high-est probability in a sentence estimated from our model is selected as a feature value."
"JUMAN has another type of dictionary, which is called a phrase dictionary ."
"Each entry in the phrase dictionary consists of one or more morphemes such as \ と ( to , case marker), は ( wa , topic marker), いえ ( ie , say).&quot; JUMAN uses this dictionary to detect morphemes which need a longer con-text to be identi ed correctly."
"When the target string corresponds to the string of the left most morpheme in the phrase dic-tionary in JUMAN, the part-of-speech at- 



 tached to the entry plus the information that it is in the phrase dictionary (such as \Verb&amp;Phrase&quot;) is used as a feature value."
Types of characters used in a string. \(Beginning)&quot; and \(End)&quot; respectively represent the leftmost and rightmost char-acters of a string.
"When a string con-sists of only one character, the \(Begin-ning)&quot; and \(End)&quot; are the same charac-ter. \TOC(0)(Transition)&quot; represents the transition from the leftmost character to the rightmost one in a string. \TOC(- 1)(Transition)&quot; represents the transition from the rightmost character in the adja-cent morpheme on the left to the leftmost one in the target string."
"For example, when the adjacent morpheme on the left is \ 先 生 ( sensei , teacher)&quot; and the target string is \ に ( ni , case marker),&quot; the feature value \Kanji !"
Hiragana&quot; is selected.
BB: Indicates whether or not the left side of a morpheme is a bunsetsu boundary.
Some results of the morphological analysis are listed in Table 2.
Recall is the percentage of morphemes in the test corpus whose segmen-tation and major POS tag are identi ed cor-rectly.
Precision is the percentage of all mor-phemes identi ed by the system that are iden-ti ed correctly.
F represents the F-measure and is de ned by the following equation.
F 0 measure = 2 2 RecallRecall + 2 PrecisionP recision
"Table 2 shows results obtained by using our method, by using JUMAN, and by using JU-MAN plus KNP[REF_CITE]."
We show the result obtained using JUMAN plus KNP because JUMAN alone assigns an \Unknown&quot; tag to katakana strings when they are not in the dictionary.
All katakana strings not found in the dictionary are therefore evaluated as er-rors.
KNP improves on JUMAN by replacing the \Unknown&quot; tag with a \Noun&quot; tag and dis-ambiguating part-of-speech ambiguities which arise during the process of parsing when there is more than one JUMAN analysis with the same score.
The accuracy in segmentation and major POS tagging obtained with our method and that obtained with JUMAN were about 3% worse than that obtained with JUMAN plus KNP.
We think the main reason for this was an  amount of training data and fea-ture sets and the inconsistency of the corpus.
"The number of sentences in the training cor-pus was only about 8,000, and we did not use as many combined features as were proposed in Ref.[REF_CITE]."
"We were unable to use more training data or more feature sets be-cause every string consisting of ve or less char-acters in our training corpus was used to train our model, so the amount of tokenized train-ing data would have become too large and the training would not have been completed on the available machine if we had used more training data or more feature sets."
The inconsistency of the corpus was due to the way the corpus was made.
"The Kyoto University corpus was made by manually correcting the output of JU-MAN plus KNP, and it is  to manually correct all of the inconsistencies in the output."
The use of JUMAN plus KNP thus has an ad-vantage over the use of our method when we evaluate a system&apos;s accuracy by using the Ky-oto University corpus.
"For example, the num-ber of morphemes whose rightmost character is \ 者 &quot; was 153 in the test corpus, and they were all the same as those in the output of JUMAN plus KNP."
There were three errors (about 2%) in the output of our system.
"There were several inconsistencies in the test corpus such as \ 生産 ( seisan , Noun), 者 ( sha , )(producer),&quot; and \ 消費者 ( shouhi-sha , Noun)(consumer).&quot; They should have been corrected in the corpus-making process to \ 生産 ( seisan , Noun), 者 ( sha , )(producer),&quot; and \ 消費 ( shouhi , Noun), 者 ( sha , )(consumer).&quot; It is  for our model to discriminate among these with-out over-training when there are such incon-sistencies in the corpus."
"Other similar incon-sistencies were, for example, \ 芸術家 ( geijutsu-ka , Noun)(artist)&quot; and \ 工芸 ( kougei , Noun), 家 ( ka , )(craftsman),&quot; \ 警視庁 ( keishi-cho , Noun)(the Metropolitan Police Board)&quot; and \ 検 察 ( kensatsu , Noun), 庁 ( cho , Noun)(the Pub-lic Prosecutor&apos;s ),&quot; and \ 現実的 ( genjitsu-teki , Adjective)(realistic)&quot; and \ 理想 ( risou , Noun), 的 ( teki , )(ideal).&quot;."
"If these had been corrected consistently when making the corpus, the accuracy obtained by our method could have been better than that shown in Ta-ble 2."
A study on corpus revision should be un-dertaken to resolve this issue.
We believe it can be resolved by using our trained model.
"There is a high possibility that a morpheme lacks con-sistency in the training corpus when its proba-bility, re-estimated by our model, is low."
Thus a method which detects morphemes having a low probability can identify those lacking con-sistency in the training corpus.
We intend to try this in the future.
"In our model, dictionary information and cer-tain characteristics of unknown words are re-ected as features, as shown in Table 1. \String&quot; and \Dic&quot; re ect the dictionary in-formation, [Footnote_2] and \Length&quot; and \TOC&quot;(types of characters) re ect the characteristics of un-known words."
2 \String&quot; indicates strings that make up a morpheme and were found ve or more times in the training corpus. Using this information as features in our M.E. model corresponds to consulting a dictionary constructed from the training corpus.
"Therefore, our model can not only consult a dictionary but can also detect un-known words."
Table 1 shows the results of an analysis without the complete feature set.
Al-most all of the feature sets improved accuracy.
The contribution of the dictionary information was especially signi cant.
"There were cases, however, in which the use of dictionary information led to a decrease in the accuracy."
"For example, we found these er-roneous segmentations: \ ／海 ( umi , sea) ／に ( ni , case marker) ／かけ た ( kaketa , bet) ／ ロマンは ( romanha , the Ro-mantic school) ／ &quot; and \ ／荒波 ( aranami , rag-ing waves) ／に ( ni , case marker) ／負け ( make , lose) ／ ない心 ( naishin , one&apos;s inmost heart) ／と ( to , case marker) ／ &quot; (Underlined strings were errors.) when the correct segmentations were: \ ／海 ( umi , sea) ／に ( ni , case marker) ／かけ た ( kaketa , bet) ／ロマン ( roman , romance) ／は ( wa , topic marker) ／ &quot; and \ ／荒波 ( aranami , raging waves) ／に ( ni , case marker) ／負けない ( makenai , not to lose) ／心 ( kokoro , heart) ／と ( to , case marker) ／ &quot; (\ ／ &quot; indicates a morpho-logical boundary.)."
These errors were caused by nonstandard en- tries in the JUMAN dictionary.
"The dictio-nary had not only the usual notation using kanji characters, \ ロマン派 &quot; and \ 内心 ,&quot; but also the uncommon notation using hiragana strings, \ ロ マンは &quot; and \ ない心 &quot;."
"To prevent this type of error, it is necessary to remove nonstandard en-tries from the dictionary or to investigate the frequency of such entries in large corpora and to use it as a feature."
The accuracies (F-measures) for the training corpus and the test corpus are shown in Figure 1 plotted against the number of sentences used for training.
The learning curve shows that we can expect improvement if we use more training data.
The strength of our method is that it can iden-tify morphemes when they are unknown words and can assign appropriate parts-of-speech to them.
"For example, the nouns \ 漱石 (Souseki)&quot; and \ 露伴 (Rohan)&quot; are not found in the JU-"
"JUMAN plus KNP analyzes them simply as \ 漱 (Noun) 石 (Noun)&quot; and \ 露 (Adverb) 伴 (Noun),&quot; whereas our system ana-lyzes both of them correctly."
Our system cor-rectly identi ed them as names of people even though they were not in the dictionary and did not appear as features in our M.E. model.
"Since these names, or proper nouns, are newly coined and can be represented by a variety of expres-sions, no proper nouns can be included in a dic-tionary, nor can they appear in a training cor-pus; this means that proper nouns could easily be unknown words."
"We investigated the accu-racy of our method in identifying morphemes when they are unknown words, and the re-sults are listed in Table 3."
The rst row in each section shows the recall for the morphemes that were unknown words.
The second row in each section shows the percentage of morphemes whose segmentation and \minor&quot; POS tag were identi ed correctly.
"The di erence between the rst and second lines, the third and fourth lines, and fth and sixth lines is the de nition of un-known words."
"Unknown words were de ned re-spectively as words not found in the dictionary nor in our training corpus, as words not found in the dictionary nor in our features, and as words not found in the dictionary."
"Our accu-racy, shown as the second rows in Table 3 was more than 5% better than that of JUMAN plus KNP for each de nition."
"These results show that our model can  learn the char-acteristics of unknown words, especially those of proper nouns such as the names of people, organizations, and locations."
Several methods based on statistical models have been proposed for the morphological anal-ysis of Japanese sentences.
An F-measure of about 96% was achieved by a method based on a hidden Markov model (HMM)[REF_CITE]and by one based on a variable-memory Markov model[REF_CITE].
"Al-though the accuracy obtained with these meth-ods was better than that obtained with ours, their accuracy cannot be compared directly with that of our method because their part-of-speech categories di er from ours."
"And an advantage of our model is that it can handle unknown words, whereas their models do not handle unknown words well."
"In their models, unknown words are divided into a combination of a word consisting of one character and known words."
"Haruno and Matsumo[REF_CITE]achieved a recall of about 96% when using trigram or greater information, but achieved a recall of only 94% when using bi-gram information."
This leads us to believe that we could obtain better accuracy if we use tri-gram or greater information.
We plan to do so in future work.
"Two approaches have been used to deal with unknown words: acquiring unknown words from corpora and putting them into a dictionary (e.g.,[REF_CITE]) and develop-ing a model that can identify unknown words correctly (e.g.,[REF_CITE])."
Nagata reported a recall of about 40% for unknown words[REF_CITE].
"As shown in Table 3, our method achieved a recall of 69.90% for unknown words."
Our accuracy was about 30% better than his.
"It is  to compare his method with ours directly because he used a di erent corpus (the EDR corpus), but the part-of-speech categories and the def-inition of morphemes he used were similar to ours."
"Thus, this comparison is helpful in evalu-ating our method."
There are no spaces between morphemes in Japanese.
"In general, therefore, detecting whether a given string is an unknown word or is not a morpheme is  when it is not found in the dictionary, nor in the train-ing corpus."
"However, our model learns whether or not a given string is a morpheme and has a huge amount of data for learning what in a cor-pus is not a morpheme."
"Therefore, we believe that the characteristics of our model led to its good results for identifying unknown words."
Mori and Nagao proposed a model that can consult a dictionary[REF_CITE]; they reported an F-measure of about 92 when using the EDR corpus and of about 95 when using the Kyoto University corpus.
"Their slight improvement in accuracy by using dictionary in-formation resulted in an F-measure of about 0.2, while our improvement was about 1.7."
"Their accuracy of 95% when using the Kyoto Univer-sity corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus."
"Therefore, their exper-iment had to deal with fewer unknown words than ours did."
"With regard to the morphological analy-sis of English sentences, methods for part-of-speech tagging based on an HMM[REF_CITE], a variable-memory Markov model (Schutze and[REF_CITE]), a decision tree model[REF_CITE], an M.E. model[REF_CITE], a neural network model[REF_CITE], and a transformation-based error-driven learning model[REF_CITE]have been proposed, as well as a combined method ( and , 1997; van[REF_CITE])."
"On available machines, however, these models cannot handle a large amount of lex-ical information."
"We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning cer-tain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in En-glish."
We plan to apply our model to English sentences.
This paper described a method for morpho-logical analysis based on a maximum entropy (M.E.) model.
This method uses a model that can not only consult a dictionary but can also identify unknown words by learning cer-tain characteristics.
"To learn these characteris-tics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string."
The model estimates how likely a string is to be a morpheme according to the information on hand.
"When our method was used to iden-tify morpheme segments in sentences in the Ky-oto University corpus and to identify the ma-jor parts-of-speech of these morphemes, the re-call and precision were respectively 95.80% and 95.09%."
"In our experiments without each fea-ture set shown in Tables 1, we found that dic-tionary information signi cantly contributes to improving accuracy."
"We also found that our model can  learn the characteristics of unknown words, especially proper nouns such as the names of people, organizations, and lo-cations."
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.
We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement.
"We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach."
A multiword unit (MWU) is a connected collocation: a sequence of neighboring words “whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components”[REF_CITE].
"In other words, MWUs are typically non-compositional at some linguistic level."
"For example, phonological non-compositionality has been observed (Finke &amp;[REF_CITE]) where words like “got” [g &lt; t] and “to” [tu] change phonetically to “gotta” [g &lt;rF ] when combined."
"We have interest in inducing headwords for machine-readable dictionaries (MRDs), so our interest is in semantic rather than phonological non-compositionality."
"As an example of semantic non-compositionality, consider “compact disk”: one could not deduce that it was a music medium by only considering the semantics of “compact” and “disk.”"
MWUs may also be non-substitutable and/or non-modifiable[REF_CITE].
Non-substitutability implies that substituting a word of the MWU with its synonym should no longer convey the same original content: “compact disk” does not readily imply “densely-packed disk.”
"Non-modifiability, on the other hand, suggests one cannot modify the MWU’s structure and still convey the same content: “compact disk” does not signify “disk that is compact.”"
MWU dictionary headwords generally satisfy at least one of these constraints.
"For example, a compositional phrase would typically be excluded from a hard-copy dictionary since its constituent words would already be listed."
These strategies allow hard-copy dictionaries to remain compact.
"As mentioned, we wish to find MWU headwords for machine-readable dictionaries (MRDs)."
"Although space is not an issue in MRDs, we desire to follow the lexicographic practice of reducing redundancy."
"As Sproat indicated, &quot;simply expanding the dictionary to encompass every word one is ever likely to encounter is wrong: it fails to take advantage of regularities&quot; (1992, p. xiii)."
"Our goal is to identify an automatic, knowledge-free algorithm that finds all and only those collocations where it is necessary to supply a definition. “Knowledge-free” means that the process should proceed without human input (other than, perhaps, indicating whitespace and punctuation)."
This seems like a solved problem.
"Many collocation-finders exist, so one might suspect that most could suffice for finding MWU dictionary headwords."
"To verify this, we evaluate nine existing collocation-finders to see which best identifies valid headwords."
We evaluate using two completely separate gold standards: (1) WordNet and (2) a compendium of Internet dictionaries.
"Although web-based resources are dynamic and have better coverage than WordNet (especially for acronyms and names), we show that WordNet-based scores are comparable to those using Internet MRDs."
Yet the evaluations indicate that significant improvement is still needed in MWU-induction.
"As an attempt to improve MWU headword induction, we introduce several algorithms using Latent Semantic Analysis (LSA)."
LSA is a technique which automatically induces semantic relationships between words.
We use LSA to try to eliminate proposed MWUs which are semantically compositional.
"Unfortunately, this does not help."
"Yet when we use LSA to identify substitutable MWUs, we do show modest performance gains."
"For decades, researchers have explored various techniques for identifying interesting collocations."
There have essentially been three separate kinds of approaches for accomplishing this task.
"These approaches could be broadly classified into (1) segmentation-based, (2) word-based and knowledge-driven, or (3) word-based and probabilistic."
We will illustrate strategies that have been attempted in each of the approaches.
"Since we assume knowledge of whitespace, and since many of the first and all of the second categories rely upon human input, we will be most interested in the third category."
Some researchers view MWU-finding as a natural by-product of segmentation.
One can regard text as a stream of symbols and segmentation as a means of placing delimiters in that stream so as to separate logical groupings of symbols from one another.
A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.
"In such cases, these larger units may be MWUs."
"The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996;[REF_CITE]; de[REF_CITE]) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography ([REF_CITE]; and many others)."
"Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression."
"Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not."
These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by the space character).
"However, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the delimiters."
"This suggests that in a language with whitespace, one might prefer to begin at the word level and identify appropriate word combinations."
"Some researchers start with words and propose MWU induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure[REF_CITE]."
"For example, Justeson and Katz indicated that the patterns NOUN NOUN and ADJ NOUN are very typical of MWUs."
"Daille also suggests that in French, technical MWUs follow patterns such as “ NOUN de NOUN &quot; (1996, p. 50)."
To find word combinations that satisfy such patterns in both of these situations necessitates the use of a lexicon equipped with part of speech tags.
"Since we are interested in knowledge-free induction of MWUs, these approaches are less directly related to our work."
"Furthermore, we are not really interested in identifying constructs such as general noun phrases as the above rules might generate, but rather, in finding only those collocations that one would typically need to define."
The third category assumes at most whitespace and punctuation knowledge and attempts to infer MWUs using word combination probabilities.
Table 1 (see next page) shows nine commonly-used probabilistic MWU-induction approaches.
"In the table, f X and P X signify frequency and probability of a word X. A variable XY indicates a word bigram and XY indicates its expected frequency at random."
An overbar signifies a variable’s complement.
"For more details, one can consult the original sources as well[REF_CITE]and[REF_CITE]."
"Prior to applying the algorithms, we lemmatize using a weakly-informed tokenizer that knows only that whitespace and punctuation separate words."
Punctuation can either be discarded or treated as words.
"Since we are equally interested in finding units like “Dr.” and “U. S.,” we opt to treat punctuation as words."
"Once we tokenize, we use Church’s (1995) suffix array approach to identify word n-grams that occur at least T times (for T=10)."
"We then rank-order the 



"
"One approximation redefines X and Y to be, respectively, the word sequences w 1 w 2 ... w i and w i+1 w i+2 ...w n, where i is chosen to maximize P X P Y ."
This has a natural interpretation of being the expected probability of concatenating the two most probable substrings in order to form the larger unit.
"Since it can be computed rapidly with low memory costs, we use this approximation."
Two additional issues need addressing before evaluation.
The first regards document sourcing.
"If an n-gram appears in multiple sources (eg., Congressional Record versus Associated Press), its likelihood of accuracy should increase."
This is particularly true if we are looking for MWU headwords for a general versus specialized dictionary.
"Phrases that appear in one source may in fact be general MWUs, but frequently, they are text-specific units."
"Hence, precision gained by excluding single-source n-grams may be worth losses in recall."
We will measure this trade-off.
"Second, evaluating with punctuation as words and applying no filtering mechanism may unfairly bias against some algorithms."
Pre- or post-processing of n-grams with a linguistic filter has shown to improve some induction algorithms’ performance[REF_CITE].
"Since we need knowledge-poor induction, we cannot use human-suggested filtering rules as in Section 2.2."
Yet we can filter by pruning n-grams whose beginning or ending word is among the top N most frequent words.
This unfortunately eliminates acronyms like “U. S.” and phrasal verbs like “throw up.”
"However, discarding some words may be worthwhile if the final list of n-grams is richer in terms of MRD headwords."
"We therefore evaluate with such an automatic filter, arbitrarily (and without optimization) choosing N=75."
A natural scoring standard is to select a language and evaluate against headwords from existing dictionaries in that language.
"Others have used similar standards[REF_CITE], but to our knowledge, none to the extent described here."
We evaluate thousands of hypothesized units from an unconstrained corpus.
"Furthermore, we use two separate evaluation gold standards: (1) WordNet[REF_CITE]and (2) a collection of Internet MRDs."
Using two gold standards helps valid MWUs.
It also provides evaluation using both static and dynamic resources.
We choose to evaluate in English due to the wealth of linguistic resources.
"In particular, we use a randomly-selected corpus consisting of a 6.7 million word subset of the TREC databases ([REF_CITE]-1997)."
"Table 2 illustrates a sample of rank-ordered output from each of the different algorithms (following the cross-source, filtered paradigm described in section 3)."
Note that algorithms in the first four columns produce results that are similar to each other as do those in the last four columns.
"Although the mutual information results seem to be almost in a class of their own, they actually are similar overall to the first four sets of results; therefore, we will refer to the first five columns as “information-like.”"
"Similarly, since the last four columns share properties of the frequency approach, we will refer to them as “frequency-like.”"
One’s application may dictate which set of algorithms to use.
"Our gold standard selection reflects our interest in general word dictionaries, so results we obtain may differ from results we might have obtained using terminology lexicons."
"If our gold standard contains K MWUs with corpus frequencies satisfying threshold (T=10), our figure of merit (FOM) is given by 1 K K M i 1 P i , where P i (precision at i) equals i/H i , and H i is the number of hypothesized MWUs required to find the i th correct MWU."
This FOM corresponds to area under a precision-recall curve.
WordNet has definite advantages as an evaluation resource.
"It has in excess of 50,000 MWUs, is freely accessible, widely used, and is in electronic form."
"Yet, it obviously cannot contain every MWU."
"For instance, our corpus contains 177,331 n-grams (for 2 n 10) satisfying T 10, but WordNet contains only 2610 of these."
"It is unclear, therefore, if algorithms are wrong when they propose MWUs that are not in WordNet."
We will assume they are wrong but with a special caveat for proper nouns.
WordNet includes few proper noun MWUs.
Yet several algorithms produce large numbers of proper nouns.
This biases against them.
"One could contend that all proper nouns MWUs are valid, but we disagree."
"Although such may be MWUs, they are not necessarily MRD headwords; one would not include every proper noun in a dictionary, but rather, those needing definitions."
"To overcome this, we will have two scoring modes."
"The first, “S” mode (standing for some) discards any proposed capitalized n-gram whose uncapitalized version is not in WordNet."
The second mode “N” (for none) disregards all capitalized n-grams.
Table 3 illustrates algorithmic performance as compared to the 2610 MWUs from WordNet.
"The first double column illustrates “out-of-the-box” performance on all 177,331 possible n-grams."
"The second double column shows cross-sourcing: only hypothesizing MWUs that appear in at least two separate datasets (124,952 in all), but being evaluated against all of the 2610 valid units."
"Double columns 3 and 4 show effects from high-frequency filtering the n-grams of the first and second columns (reporting only 29,716 and 17,720 n-grams) respectively."
"As Table 3 suggests, for every condition, the information-like algorithms seem to perform best at identifying valid, general MWU headwords."
"Moreover, they are enhanced when cross-sourcing is considered; but since much of their strength comes from identifying proper nouns, filtering has little or even negative impact."
"On the other hand, the frequency-like approaches are independent of data source."
They also improve significantly with filtering.
"Overall, though, after the algorithms are judged, even the best score of 0.265 is far short of the maximum possible, namely 1.0."
"Since WordNet is static and cannot report on all of a corpus’ n-grams, one may expect different performance by using a more all-encompassing, dynamic resource."
The Internet houses dynamic resources which can judge practically every induced n-gram.
"With permission and sufficient time, one can repeatedly query websites that host large collections of MRDs and evaluate each n-gram."
"Having approval, we queried: (1) onelook.com, (2) acronymfinder.com, and (3) infoplease.com."
The first website interfaces with over 600 electronic dictionaries.
The second is devoted to identifying proper acronyms.
The third focuses on world facts such as historical figures and organization names.
"To minimize disruption to websites by reducing the total number of queries needed for evaluation, we use an evaluation approach from the information retrieval community (Sparck-Jones and van[REF_CITE])."
Each algorithm reports its top 5000 MWU choices and the union of these choices (45192 possible n-grams) is looked up on the Internet.
Valid MWUs identified at any website are assumed to be the only valid units in the data.
Algorithms are then evaluated based on this collection.
"Although this strategy for evaluation is not flawless, it is reasonable and makes dynamic evaluation tractable."
"Table 4 shows the algorithms’ performance (including proper nouns). 



"
Can performance be improved?
Numerous strategies could be explored.
"An idea we discuss here tries using induced semantics to rescore the output of the best algorithm (filtered, cross-sourced Zscore) and eliminate semantically compositional or modifiable MWU hypotheses."
"It forms high-dimensional vectors using word counts and uses singular value decomposition to project those vectors into an optimal k-dimensional, “semantic” subspace (see[REF_CITE])."
"Following an approach[REF_CITE], we showed how one could compute latent semantic vectors for any word in a corpus[REF_CITE]."
"Using the same approach, we compute semantic vectors for every proposed word n-gram C=X 1 X 2 ...X n. Since LSA involves word counts, we can also compute semantic vectors (denoted by ) for C’s subcomponents."
These can either include ( {X i } n ) or exclude ( {X i } n ) C’s counts.
We seek to see i 1 if induced semantics i 1 can help eliminate incorrectly-chosen MWUs.
"As will be shown, the effort using semantics in this nature has a very small payoff for the expended cost."
"Non-compositionality is a key component of valid MWUs, so we may desire to emphasize n-grams that are semantically non-compositional."
Suppose we wanted to determine if C (defined above) were non-compositional.
"Then given some meaning function, , C should satisfy an equation like: g( (C) , h( (X 1 ),..., (X n ) ) ) 0, (1) where h combines the semantics of C’s subcomponents and g measures semantic differences."
"If C were a bigram, then if g(a,b) is defined to be |a-b|, if h(c,d) is the sum of c and d, and if (e) is set to -log P e , then equation (1) would become the pointwise mutual information of the bigram."
"If g(a,b) were defined to be (a-b)/b ½ , and if h(a,b)=ab/N and (X)=f X , we essentially get Z-scores."
These formulations suggest that several of the probabilistic algorithms we have seen include non-compositionality measures already.
"However, since the probabilistic algorithms rely only on distributional information obtained by considering juxtaposed words, they tend to incorporate a significant amount of non-semantic information such as syntax."
Can semantic-only rescoring help?
"To find out, we must select g, h, and ."
"Since we want to eliminate MWUs that are compositional, we want h’s output to correlate well with C when there is compositionality and correlate poorly otherwise."
"Frequently, LSA vectors are correlated using the cosine between them: cos(X,Y) X # Y . ||X|| ||Y||"
"A large cosine indicates strong correlation, so large values for g(a,b)=1-|cos(a,b)| should signal weak correlation or non-compositionality. h could represent a weighted vector sum of the components’ semantic vectors with weights (w i ) set to either 1.0 or the reciprocal of the words’ frequencies."
Table 5 indicates several results using these settings.
"As the first four rows indicate and as desired, non-compositionality is more apparent for X * (i.e., the vectors derived from excluding C’s counts) than for X ."
"Yet, performance overall is horrible, particularly considering we are rescoring Z-score output whose score was 0.269."
Rescoring caused five-fold degradation!
What happens if we instead emphasize compositionality?
Rows 5-8 illustrate the effect: there is a significant recovery in performance.
"The most reasonable explanation for this is that if MWUs and their components are strongly correlated, the components may rarely occur except in context with the MWU."
It takes about 20 hours to compute the X * for each possible n-gram combination.
"Since the probabilistic algorithms already identify n-grams that share strong distributional properties with their components, it seems imprudent to exhaust resources on this LSA-based strategy for non-compositionality."
These findings warrant some discussion.
Why did non-compositionality fail?
"Certainly there is the possibility that better choices for g, h, and could yield improvements."
"We actually spent months trying to find an optimal combination as well as a strategy for coupling LSA-based scores with the Z-scores, but without avail."
"Another possibility: although LSA can find semantic relationships, it may not make semantic decisions at the level required for this task."
This seems to be a significant component.
Yet there is still another: maybe semantic compositionality is not always bad.
"Interestingly, this is often the case."
"Consider vice_president, andorganized crime, Marine_Corps."
"Although these are MWUs, one would still expect that the first is related to president, the second relates to crime, and the last relates to Marine."
"Similarly, tokens such as Johns_Hopkins and Elvis are anaphors for Johns_Hopkins_University and Elvis_Presley, so they should have similar meanings."
This begs the question: can induced semantics help at all?
The answer is “yes.”
The key is using LSA where it does best: finding things that are similar — or substitutable.
"For every collocation C=X 1 X 2 ..X i-1 X i X i+1 ..X n , we attempt to find other similar patterns in the data, X 1 X 2 ..X i-1 YX i+1 ..X n ."
"If X i and Y are semantically related, chances are that C is substitutable."
"Since LSA excels at finding semantic correlations, we can compare Xi and Y to see if C is substitutable."
"We use our earlier approach[REF_CITE]for performing the comparison; namely, for every word W, we compute cos( w, R ) for 200 randomly chosen words, R. This allows for computation of a correlaton mean ( µ W ) and standard deviation ( 1 W ) between W and other words."
"As before, we then compute a normalized cosine score ( cos) between words of interest, defined by cos( Xi , Y ) µ k . min cos(X i ,Y) k {X ,Y} 1 k i"
"With this set-up, we now look for substitutivity."
Note that phrases may be substitutable and still be headword if their substitute phrases are themselves MWUs.
"For example, dioxide in carbon_dioxide is semantically similar to monoxide in carbon_monoxide."
"Moreover, there are other important instances of valid substitutivity: &amp; Abbreviations"
Al Albert &lt; Al_Gore Albert_Gore &amp;
Rico Rican &lt; Puerto_Rico Puerto_Rican &amp; Taxonomic relationships bachelor master &lt; bachelor_’_s_degree master_’_s_degree.
"However, guilty and innocent are semantically related, but pleaded_guilty and pleaded_innocent are not MWUs."
We would like to emphasize only n-grams whose substitutes are valid MWUs.
"To show how we do this using LSA, suppose we want to rescore a list L whose entries are potential MWUs."
"For every entry X in L, we seek out all other entries whose sorted order is less than some maximum value (such as 5000) that have all but one word in common."
"For example, suppose X is “bachelor_’_s_degree.”"
The only other entry that matches in all but one word is “master_’_s_degree.”
"If the semantic vectors for “bachelor” and “master” have a normalized cosine score greater than a threshold of 2.0, we then say that the two MWUs are in each others substitution set."
"To rescore, we assign a new score to each entry in substitution set."
Each element in the substitution set gets the same score.
The score is derived using a combination of the previous Z-scores for each element in the substitution set.
"The combining function may be an averaging, or a computation of the median, the maximum, or something else."
The maximum outperforms the average and the median on our data.
"By applying in to our data, we observe a small but visible improvement of 1.3% absolute to .282 (see Fig. 1)."
It is also possible that other improvements could be gained using other combining strategies.
This paper identifies several new results in the area of MWU-finding.
We saw that MWU headword evaluations using WordNet provide similar results to those obtained from far more extensive web-based resources.
"Thus, one could safely use WordNet as a gold standard for future evaluations."
"We also noted that information-like algorithms, particularly Z-scores, SCP, and $ 2, seem to perform best at finding MRD headwords regardless of filtering mechanism, but that improvements are still needed."
We proposed two new LSA-based approaches which attempted to address issues of non-compositionality and non-substitutivity.
"Apparently, either current algorithms already capture much non-compositionality or LSA-based models of non-compositionality are of little help."
LSA does help somewhat as a model of substitutivity.
"However, LSA-based gains are small compared to the effort required to obtain them."
",ÅwÉrÉr× &gt; { ÃÚÙKÛ!"
"ÅwÔ ÈÞÝßÃµÅwÁ , !Ô3È {*ÅwÔ ÅwÒêØ ÂÓÃ*Â &gt; ö ÷ Ow. /&lt; 2,,ø ¿M= &gt;&lt;   $ #  *"
È ÿ ÒkÅw× Ô1Îrà*ô ! *ô
"Qä ,ÅwÅ`ÇµÂæà  Ô   ô  × Ô È  È ÂkÔQÖQàZÅwÔ Ù ¡Ï  ô çCà&amp;ÒÓÅwÔ Ö× )"
We present a hybrid text mining method for finding abbreviations and their definitions in free format texts.
"To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words."
The pattern-based rules describe how abbreviations are formed from definitions.
Rules can be generated automatically and/or manually and can be augmented when the system processes new documents.
"The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition."
"Many organizations have a large number of on-line documents -- such as manuals, technical reports, transcriptions of customer service calls or telephone conferences, and electronic mail --which contain information of great potential value."
"In order to utilize the knowledge these data contain, we need to be able to create common glossaries of domain-specific names and terms."
"While we were working on automatic glossary extraction, we noticed that technical documents contain a lot of abbreviated terms, which carry important knowledge about the domains."
"We concluded that the correct recognition of abbreviations and their definitions is very important for understanding the documents and for extracting information from them [1, 6, 9, 11]."
An abbreviation is usually formed by a simple method: taking zero or more letters from each word of its definition.
"However, the tendency to make unique, interesting abbreviations is growing."
"So, it is easy to find new kinds of abbreviations which cannot be processed by hard-coded heuristics-based algorithms [1, 6, 7, 13, 14], since they are formed in ways not anticipated when the algorithms were devised."
We propose a hybrid text mining approach to deal with these problems.
"We use three kinds of knowledge: pattern-based abbreviation rules, text markers, and linguistic cue words."
"An abbreviation rule consists of an abbreviation pattern, a definition pattern and a formation rule."
The formation rule describes how an abbreviation is formed from a definition.
There may exist multiple formation rules for a given pair of abbreviation and definition patterns.
Abbreviation rules are described in Section 3.
Text markers are special symbols frequently used to imply the abbreviation-definition relationship in texts.
"They include characters such as ‘(…)’, ‘[…]’, and ‘=’."
"Cue words are particular words occurring in the local contexts of abbreviations and the definitions, which strongly imply the abbreviation relationship."
"They include words such as “or”, “short”, “acronym” and “stand”."
Text markers and cue words are discussed in section 2.4.
"This system has 5 components --abbreviation recognizer, definition finder, rule applier, abbreviation matcher and best match selector -- as shown in Figure 1."
The abbreviation seeksrecognizer candidate abbreviations in a text and generates their patterns (Section 1).
"When an abbreviation candidate is found, the system determines the contexts within which to look for a definition."
"When it finds a candidate definition, it generates a pattern for it also (Section 2)."
"Having generated the abbreviation pattern and the definition pattern, the system first searches the rulebase for a rule which would generate the abbreviation from the definition."
The rules for the given candidates are applied in the order of rule priorities (Section 4.1).
"If the rulebase is empty or if no existing rule matches the candidate abbreviation with the candidate definition, the system runs the abbreviation matcher and generates a new abbreviation rule."
The abbreviation matcher consists of 5 layered matching algorithms (Section 4.2).
"If the matcher succeeds, new rules may be added to the rulebase, allowing it to grow as the system processes new documents."
System Overview 1.
An abbreviation is defined as a shortened form of a written word or phrase used in place of the full form [2].
Acronyms are a special case of abbreviations which are devoted to multi-word full forms.
"In this work, we consider a string of alphabetic, numeric and special characters as a candidate abbreviation if it satisfies the following three conditions: (1) Its first character is alphabetic or numeric (2)"
Its length is between 2 and 10 characters (3)
It contains at least one capital letter and if the string meets the following restrictions: (1) It is not a known (dictionary) word containing an initial capital letter and appearing as the first word in a sentence. (2) It is not a member of a predefined set of person and location names. (3) It is not a member of user-defined list of stopwords.
The first restriction keeps many common words from being treated as possible abbreviations.
Many proper names have the same characteristics above and may be recognized as abbreviations.
"To reduce generating false candidates and thus improve system performace, we use a list of proper names created by the Talent system[3, 10]."
We also provide users with a way to create a user defined list of stopwords.
"Based on these conditions, AI (Artificial Intelligence), Baracuda (Boldly Advanced and Refined Aircraft Concept Under Development for AGATE), SgRP (seating reference point), 2MASS (Two-Micron All Sky Survey), ACIS (Advanced CCD Imaging Spectrometer), W3C (World Wide Web Consortium), T/C/F (Trim/Chassis/Final) are recognized as candidate abbreviations."
Once a candidate abbreviation is identified an abbreviation pattern is generated from it.
An abbreviation pattern is a string of ‘c’ and ‘n’ characters.
"An alphabetic character is replaced with a ‘c’ and a sequence of numeric characters (including ‘.’ and ‘,’) is replaced with an ‘n’ regardless of its length."
"Non-alphanumeric characters such as hyphen, slash, and ampersand are not reflected in abbreviation patterns."
Some examples of candidate abbreviations and their patterns are in Table 1.
This system searches for a possible definition of a candidate abbreviation in its left and right contexts.
The size of the search space is a function of the length of the abbreviation and the maximally allowed distance (offset) between a definition and its abbreviation.
"We have analyzed about 4,500 abbreviations and their definitions in computer science texts."
The maximum number of skipped words (words in a definition that are not reflected in the abbreviation) was 4 in our sample data.
"Based on this analysis, we decided that, for relatively short abbreviations (from two to four characters), the length of a definition in words should not be greater than twice the abbreviation length."
"For long abbreviations (five or more characters), the definition should not be longer than the abbreviation length plus 5."
"Thus, the maximum length of a definition D of an abbreviation A is calculated as follows. max. |D| = min {|A| + 5, |A| * 2}"
The maximum offset means the longest distance of a definition from an abbreviation.
"If a definition is in the left context, the distance is the number of words from the last word of the definition to the abbreviation."
"If a definition is in the right context, the distance is the number of words from the abbreviation to the first word of the definition."
We set the maximum offset to 10 in this experiment.
"Therefore, the size of each search space is {max. |D| + 10} words to the left and right of the candidate abbreviation as shown in Fig 2. (2) All words in a definition are in the same sentence. (3) The first word and the last word of a definition are not prepositions, be-verbs, modal verbs, conjunctions or pronouns. (4) Some symbols such as (, ), [, ], {, }, =, !, ? may not be inside of a definition."
"Next, we preprocess the candidate definition as follows to generate a pattern for the candidate . (1) replace special symbols with spaces."
Input/Output =&gt; Input Output (2) separate strings of numerical characters.
"1) We currently have 60 prefixes such as anti, bi, electro, inter, pre, sub, trans, un."
"A definition pattern consists of the characters ‘w’ (word), ‘s’ (stopword), ‘p’ (prefix), ‘h’ (headword) and ‘n’ (number)."
Some examples of definitions and their patterns are in Table 2. max. length of a definition max. offset D A D search search
Fig. 2 Search Space for Definitions
The system searches for candidate definitions within the search space.
A sequence of words in the contexts is considered as a candidate definition if it satisfies the following conditions. (1) The first character of the first word of a definition is matched with the first character of the abbreviation (including ‘replacement match (Section 3)’).
"In the above examples, the definition pattern for ‘product database’ is ‘phw’, which is not morphologically correct."
This happens because ‘pro’ is included in our prefix list and ‘duct’ is found in the dictionary and we don’t do any semantic-level processing.
We extract some orthographic and syntactic structure information as well as possible definitions from the contexts.
"If there exist text markers and/or cue words in the contexts of a candidate abbreviation and its candidate definition, the pair is highly likely to be valid and to be useful for augmenting the abbreviation rulebase."
The structures we take into account include: (1) (abbr) or [abbr]
An abbreviation rule describes how an abbreviation is formed from its definition.
"An abbreviation rule, R, consists of an abbreviation pattern (A_Pattern), a definition pattern (D_Pattern) and a formation rule (F_Rule)."
"We constructed an initial rulebase from our analysis of 4,500 abbreviations in the field of computer science, which were collected from the Web."
We ran the Abbreviation Matcher routine to generate patterns and formation rules for the abbreviations and their definitions and selected frequent rules for the initial rulebase.
"The initial rulebase currently contains 45 abbreviation rules, some of which are shown in Table 4. 4."
Matching Abbreviations and Definitions
"When the system has found a candidate abbreviation with a candidate definition, it generates the A_pattern and the D_pattern , respectively, and then looks up the pattern pair in the rulebase."
"If the pair exists, the system applies the associated formation rules in priority order."
"If any rule can generate the given abbreviation from the definition, the pair is regarded as valid."
"Suppose, for example, that the abbreviation “5GL” and the definition “fifth generation language” are found in a text."
The system preprocesses them and generates their patterns.
"In this case, the A_Pattern is “ncc” and the D_Pattern is “[URL_CITE]"
"A formation rule &lt;(1, R) (2, F) (3, F)&gt; is associated with this pattern pair in the rulebase."
"Thus, the system applies the rule to determine the validity of the abbreviation/definition pair."
"The first word (‘fifth’) can be replaced into ‘5’ [(1, R)]; the first character of the second word is ‘G’ [(2, F)]; and the first character of the third word is ‘L’ [(3, F)]."
Hence the pair is valid and ‘fifth generation language’ is considered to be a definition of ‘5GL’.
"If the rulebase does not have rules for the pattern pair or if no rule successfully generates the abbreviation from the definition, and if the pair occurs in one of the cue environments described in section 2.4, the system activates the Abbreviation Matcher routine."
This routine is also used for creating the initial rulebase.
The Abbreviation Matcher contains five layered matching algorithms.
"We categorized abbreviations into five different types, one for each layer, based on the relationship between the abbreviation length and the length of the corresponding definition."
"Abbreviations of type 1 are the most frequent in our 4,500 item sample."
"Type 2 is the next most frequent, and so on. |A| represents the length of an abbreviation pattern. |D| is the length of a definition pattern. |S| indicates the number of stopwords in a definition."
"This system processes an &lt;A, D&gt; pair by applying the algorithms in layer order, beginning at layer 1."
"If the pair is matched at any layer, matching stops and the successful formation rule is returned."
"If a match is not found at any layer, the candidate pair is discarded."
"The system may generate multiple definition candidates in many cases, but we assume that there exists only one definition in a local context."
"In order to select the best candidate, we employ several weighting features. (1) syntactic cues A definition has a higher weight than other candidates if it has syntactic cues. (2) rule priority A definition has a higher weight if it was matched by a higher priority rule. (3) distance"
The definition closest to the abbreviation is favored over other candidate definitions. (4) capitalization
"A definition with initial capital letters is preferred. (5) number of words A definition is preferred based on the following sequence of length comparisons: |A| = |D|, |A| &lt; |D| and |A| &gt; |D|. (6) number of stopwords"
A definition having fewer stopwords is preferred.
"If multiple candidate definitions are found in a document for an abbreviation, these six features are tested on the definitions in the order given, until one definition remains."
"If ambiguity still remains at the end of the test, the first definition is selected."
Users can specify whether they want to update the rulebase with the results of processing a new document.
"If an existing rule successfully matches an abbreviation/definition pair, then that rule’s frequency is updated in the rulebase, thereby increasing the rule’s priority."
Users may also specify a rule threshold; new rules which occur with a frequency exceeding the threshold will be added to the rulebase.
"We have conducted experiments with three documents: a book about automotive engineering (D1), a technical book from a pharmaceutical company (D2), and NASA press releases for 1999 (D3)."
The data used in the experiments and experimental results are shown in Table 5.
Performance is evaluated using recall and precision.
"For D1, the system found 32 abbreviations and their definitions but among them 1 abbreviation is incorrect."
"Thus, it shows 93.9% recall and 96.9% precision."
"For D2, it found 60 pairs and missed 3 pairs showing 95.2% recall and 100% precision."
"For D3, it found 78 pairs with 2 incorrect results and missed 5 pairs."
The recall rate is 93.8 % and precision is 97.4 %.
The for missing somereasons abbreviations are (a) the definitions fell outside of the search space (b) misinterpretation by the part-of-speech tagger (c) matches beyond system’s current capability.
Some examples of missed abbreviations are: (1) DEHP di-2-ethylhexylphthalate (2)
ALT alanine aminotransferase (3) ASI Italian Space Agency (4) MIDEX medium-class Explorer (5) CAMEX-3 Third Convection and Moisture Experiment
"For (1), we would need to add the domain-specific prefixes “ethyl and “hexyl” to the prefix list."
"In general, adaptation of our method to new technical domains will probably involve the addition of domain-specific prefixes to the prefix list. (2) failed because there was no first letter match for “aminotransferase”."
The abbreviation in (3) is an acronym of the Italian translation of the definition.
"In (4), there is no credible source for the “I” in the abbreviation."
"In (5), the numeric replacement in the abbreviation is permuted."
These and other phenomena such as compound word processing will be the subject of further investigation.
AFP (Acronym Finding Program) is an early attempt to automatically find acronyms and their definitions in free text [13].
"In this work, however, an acronym candidate is simply an upper-case word from 3 to 10 characters in length."
AFP looks for candidate expansions in two sub-windows – the pre-window and the post-window - of the acronym by applying an LCS (longest common subsequence) algorithm.
Each subwindow’s length in words is set to twice the number of characters in the acronym and it looks for matching letters occurring at word beginnings or after hyphens.
"However, AFP does not support 2-letter acronyms that are very common in texts (e.g., AI, DB, and IP) and it does not allow interior- letter matches that are not uncommon in abbreviations."
"TLA (Three Letter Acronym) [14] removes all non-alphabetic characters and breaks the text into chunks based on the occurrences of ‘(’, ‘)’ and ‘.’ characters."
It looks for candidate acronyms in each chunk and attempts to find matching definitions in the preceding and following chunks.
Candidate acronyms and candidate definitions are compared by matching up to the first three letters of each word in the chunks.
"The potential matches are passed through a number of ad-hoc heuristics below, each of which can reject any candidate acronyms. · Acronyms are shorter than their definitions · Acronyms contain initial characters of most of the words in their definitions · Acronyms are given in upper case · Shorter acronyms tend to have longer words in their definition · Longer acronyms tend to have more stop words"
"As part of a larger study of the topology of relations across the World-Wide Web, Sundaresan and Yi [12] explore specific relations involving acronyms and their definitions."
"Similar to other work on mining the Web for relations (e.g., Kleinberg [5] for hyperlinks and Larson [8] for bibliometrics), their work uses duality-based methods to build networks of interrelated syntactic cues, acronym-definition pairs, and formation rules."
"It develops iterative techniques for finding new acronym-definition pairs, given a set of syntactic cues, and for finding new syntactic cues, given a set of known pairs."
It can also learn new formation rules.
"While the overall system frameworks are quite different, our hybrid text mining method and the duality-based method both use similar underlying machinery: syntactic cues, abbreviation-definition pairs, and formation rules."
"Differences include the hybrid method&apos;s use of a more abstract representation for formation rules, the central use of abbreviation patterns and definition patterns as the organizing principle for the rule base, and the use of cue words among the syntactic cues."
The developers of the Acrophile system at UMass Amherst [7] evaluated four different acronym extraction algorithms against manually-analyzed test documents and against hand-crafted acronym dictionaries.
"Their &quot;canonical-contextual&quot; algorithm, which shares elements with our hybrid method, was the most successful one."
"In particular, Acrophile uses a fixed 40-word search space for their &quot;contextual&quot; definition search and has a set of syntactic cues similar to ours for defining the &quot;canonical&quot; environments in which abbreviation-definition pairs may be found."
"Beyond special handling for numeric characters in acronyms, however, there is no provision for replacement matches; for explicit lists of prefixes, known words, and proper names; or for adaptively learning new acronym patterns."
"Acrophile&apos;s system environment and experimental results are quite interesting; by directed search of the World-Wide Web, the system was able to exceed the coverage of the largest publicly available hand-crafted on-line acronym dictionary."
Conclusions and Future Work
We have introduced a new hybrid approach for finding abbreviations and their definitions in unstructured texts.
The problem of abbreviation processing has attracted relatively little attention in NLP field.
"However, technical documents use a lot of abbreviations to represent domain-specific knowledge."
"Thus, the ability to find correct abbreviations and their definitions is very important to being able to utilize the information contained in those documents."
"It is also very useful for many NLP applications such as information retrieval [1] and glossary extraction [4, 9, 11]."
The proposed method has the following advantages: (1) It is simple and fast.
A small number of formation rules can describe many abbreviations.
"By keeping these rules in the rulebase, this system can process most abbreviations by simple pattern matches."
"Furthermore, the abbreviation matcher consists of 5 simple match routines and each routine is dedicated to a certain type of abbreviations."
"Thus, it is conceptually simple and fast. (2) It shows high recall and precision rates. (3) It provides for flexible user customization."
"For example, users can specify rule thresholds for updating the rulebase. (4) It is trainable."
The rulebase may be automatically refined as the system processes new documents. (5) It is adaptable to new styles and editorial conventions.
It can process new types of abbreviations by inserting appropriate rules in the rulebase without modifying the system.
"Rules are symbolic, so users can easily add, modify, or delete the rules by hand. (6) It can be adapted to new technical domains."
"The dictionary, set of replacement matches, stopword list, and prefix list, can be tailored for new domains."
"In addition to the lacunae mentioned in Section 5, we are aware that there are classes of abbreviations which our current method does not handle adequately."
These are typically written with all lower-case characters and are almost never introduced with text markers or cue words.
Examples are : • cu – customer • hw – hardware • mgr – manager • pgm – program • sw – software
"Mechanisms for processing these abbreviations, which tend to occur in informal text such as email, chat rooms, or customer service call records, are the subject of ongoing research in our project."
"@,© ,°¿¾d¯^¬,°³·_¶¥§ÁÀµÂÄÃ¶¥Å ¦ÇÆÈÀÉ¶^,± ° ³¯^ ¯^§D·ËÅ ,± @ÎdÏ±,°­D±,°³¬,°³§Q©,¯©, ¾d¯^Ã¶¥Å¬°³·0¶¥§¦U«Ñ¬Ò¯ÔÓÕ½DÖ»©[,,«»Â×©,,°³¬°³§t©^_° ,¯©,«»¶¥§Á¬¸v¬©°³Ó^§D¬,@¼  °Ù¯^^°³§t©^­D©,,¬®«Ñ§º¯¬ÜÃF¶¥Å^¦µ¯^¬É¯^^§D¼@°^Ð*,°­d±,°³¬°³§t©,¯©,«»¶¥§ @\\ßd°@^ ´®«»©,ªH©,^§D·_·D¶¥Ó,°³¼@+©o­D±¯^«Ñ§aàp©,,¯^¬ÌÞ­ ^^¶,± ©,¯¾a«ÑÖÑ«»©,«»¶¥§áÃIÃ&lt;¸4¼@%ÚT¬¸v¬¨Â,°³· ©°³Ó«Ñ¬Ù+^°±,¸Ü°@Îv­ °³§D¬°Ù^,°±³²©«»Ù^°ã¶,ªD°^&gt;±+­D±l¯¼@¶¥¬©É¶^@¼ ©,«Ñ¼¯^ã ^¼@ÖäÂÄÖ»°Ù^¶ ^ °±,,{ã ^¶ +^¯,§D¼«»¶¥§@° ¾½D¯^°³¼¯ÖÑÖÑ¸Õ¼^@¶¥§D¬,,@©°³·+++Ó ^¯ +@¼ ¶^±,,­d½D¬ªD¯^¬@¾ °°³§ÛÓ°±,,© ¬¯+^¯^§D·§vÂ ©^}^¯ ÖÑÖ»¸},,^¹ °Ü¯^+Ó ¶¥½D§Q©®¶^ã-·D¯©,¯}ã¶^,¯BÂ ,«Ñ¬©,æ «¼¯^Ö ­D±,,,°Ùv«»¶¥½D¬cÃ¶¥Å ¦ÄÂ×¾a¯^,©°³Ó+¬² ©  °±%­D±,¶^­ ¶¥¬°³¬%,^§vÂ ,,¬«Ñ¼¯¼@^,^±,°³¯^§Á¬­ )Ð @¶¥Ó+­dÖ»° ©°³Ö»¸Ë¯^°³Ö»¶^^­°³·Á¯}^¯©¨Â½vÂ ©¶¥Ó+¯©,«Ñ¼Ô¼@¶^,± ­d½D¬Â×¾d¯^ ¦ìÖÑ¯¾  ¯^,°³·D«Ñ¼@,© ,&gt;¾a¯^+¬°Ù^°±l¯^ÖdÖ»°@@¶Â ¬¸v§Q©,^¯ @¼ ©,«¼%,©,«Ñ¼ã°³¯©,½D±,°³¬ ã¶^,°°µ«Ñ§vÂ ·D½D¼@©,«Ñ¶¥§)Ð ¶^ãIî&gt;ï#^¹ ,+^}¯©,,«Ñ¼¯^°±,ã{¶^+¯^@§D¼©°³·@°"
ticWecontext-freeinvestigatedgrammarsthe applicabilityto of probabilis-and grapheme-to-phoneme conversion.
The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries.
"However, our results indicate that the standard probability model does not solve grapheme-to-phoneme con-version  although, we varied all free parameters of the probabilistic reestimation pro-cedure."
In this paper we present an approach to un-supervised learning and automatic detection grapheme-to-phonemeof syllable boundariesconversionas well asusingautomaticprob-abilistic context-free grammars (PCFGs).
"A text-to-speech system (TTS), like those de-scribed[REF_CITE], needs a module where thephonemeswords, iare.e. itsconvertedtranscriptionfrom , graphemesand one thatto  the obtained phoneme string before they can be further processed to speech."
The andtwo taskswith canprobabilisticbe solvedmethodsboth with.
"Rule-basedrule-based methods are facing the problem that they have bleto returnanalysesonethenanalysisthe .rule-basedIf there aresystemseveralhaspossi-the odsproblem, howeverof disambiguation, yield the most."
Probabilisticprobable analy-meth-sis according to the training corpus.
Our ap-proach builds on two resources.
The  re-source are manually constructed context-free grammars (CFGs) for both  and grapheme-to-phoneme conversion (G2P).
The CFG generates for the G2P task all sequences ofgraphicphonemesinput wordcorresponding.
"For the to a given ortho-task, the CFG generates all possible syllable bound-aries."
"We use context-free grammars for gen-erating transcriptions, and  phoneme strings, because grammars are expressive and writing grammar-rules is easy and intuitive."
We trained the CFGs on a training corpus that was extracted from a large newspaper corpus.
"The second resource consists of the inside-outside al-gorithm that was used for the training proce-dure, sustaining probabilistic context-free gram-mars. testThecorpusobtained."
"Themodelsresultswereof ourevaluatedexperimentson a show that PCFGs are good in predicting sylla-ble boundaries, but simple PCFGs do not yield good results for grapheme-to-phoneme conver-sion."
"A  splits a sequence of phonemes to syllables, e.g. the German phoneme se-quence [ lUftlOx ] ( air pocket ) can be syllabi-  as [ lU ] [ ftlOx ] , [ lUf ] [ tlOx ] , [ lUft ][ lOx ] , and [ lUftl ][ Ox ] . scribedOur methodin this,paperused, foris basedthe experimentson a manuallyde-constructed context-free grammar with about 50 rules which returns for a given phoneme string all possible analyses."
"Our grammar de-scribes how words are composed of syllables and syllables branch into onset, nucleus and coda."
"These syllable parts are re-written by the gram-mar as sequences of natural phone classes, e.g. stops, fricatives, nasals, liquids, as well as long and short vowels, and diphtongs."
The phone classes are then re-interpreted as the individual phonemes that they are made up of.
Figure 1 shows some of the rules of the context-free gram-marThe.  rule (1.1) in  1 describes a (1.1) Word !
Syl (1.2) Word ! (1.3) Word ! (1.4) Syl ! (1.5) Syl ! (1.6) Syl ! (1.7) Syl ! (1.8) Onset ! (1.9) Onset ! (1.10)
On ! (1.11) Nucleus ! (1.12) Vow ! (1.13) Coda ! (1.14) Coda ! (1.15)  ! (1.16)  ! (1.17) Liquid ! (1.18) Fricative ! (1.20) Plosiv(1.19)
"Fricative !! (1.21) SVowel ! (1.22) SVowel ! monosyllabicword consistingwordof twoandandrulethree(1.2)syllablesand (1.3, )re-a spectively."
"The subsequent rule (1.4)  a syllable without an onset and coda, whereas in rule (1.5) the coda is missing and in (1.7) the ofonsetan. onsetRule,(1nucleus.6) depictsandhowcodaa.syllableThe nextconsiststwo rules (1.8)-(1.9) describe the complexity of the ityonsetof andthe codathe rules, each(1consisting.13)-(1.14)oftheonecomplex-or two phonemes."
"An onsets consists of a liquid, which is shown in rules (1.10)."
"Rule (1.12) describe a short vowel, which is re-written in rule (1.21)- (1.22) as the vowels [ U ] and [ O ] ."
"According to rules (1.15)-(1.20) a coda consonant can be re-interpreted as a plosive or a fricative, which are in Figureturn re-written2 depictsasonea [ ofp ] four, a [ f ] possible, a [ x ] , oranalysesa [ t ] . of Wethe transformphoneme stringthe context-free [ lUftlOx ] . grammar by a training procedure to a probabilistic CFG."
We then choose the analysis with the highest proba-bility.
The probability of one analysis is  as the product of the probabilities of the gram-mar rules appearing in the analysis.
"In our ex-ample the correct syllable segmentation received theWehighestuse theprobabilityinside-outside: [ lUft ][ lOxalgorithm ] . devel- grammar rules are assigned random probabili-ties, which are reestimated during several itera-tions yielding the rule probabilities."
"There are three free parameters that can be varied: (1) the training corpus, (2) the number of iterations, and (3) the start parameters."
"We used the freely available lopar parser, implemented[REF_CITE]. formingFigurePCFG3 showswitha fragmentthe rule probabilitiesof the best usedper-for ."
"Rules (2.1)-(2.3) show that the most probable word structure is a word con-sisting of one syllable, a two-syllabic word is less probable and the least probable structure is a three-syllabic word."
Rules (2.5)-(2.6) show that syllables with empty onsets are preferred to open syllables.
"Sim-ple onsets consisting of one consonant are more probable than complex ones, which is also true for codas (rules (2.7)-(2.8) and (2.13)-(2.14))."
Rules (2.9)-(2.10) show that fricatives are more probable than liquids in the onset.
"Moreover, it is more likely that a nasal appears in the coda than a liquid (rule (2.15)-(2.16))."
"Our experiments are based on two  cor-pora: (i) a spoken news corpus of 1.5 h and (ii) the Sternzeit corpus, a feature series consist-ing of 2 hours read text."
"The corpus comprise 96165 words, which are automatically looked up in a pronunciation dictionary, CELEX[REF_CITE]."
The transcribed words are devided into 9/10 training and 1/10 test corpus.
"The training corpus does not include syllable bound-aries, whereas the test corpus is annotated with syllable Training boundaries ."
"We utilized. the following training procedure: we initialize the[REF_CITE]times with ran-domized rule probabilities (10 start gram-mars), each of the start grammars is re-estimated 10 times on the training corpus with the inside-outside algorithm. a test Evaluation set of almost ."
We evaluated the obtained models after each training iter-ation by calculating the most probable anal-ysis (viterbi-parse) and extracting the syllable boundaries from the analysis.
"The yielded syl- phoneme strings are compared with the annotated evaluation corpus and the syllable accuracy is measured, which is calculated as the number of the correctly predicted syllable boundaries devided by the number of all sylla-ble boundaries."
The results are shown in  4.Each accuracy curve show how the current model performs on the test data.
"At the value 0 of the x-axis, the accuracy of the randomly initialized grammar is depicted."
"At the value 1, the accuracy after the  iteration, and at value 2 the accuracy after the second iteration is displayed, and so forth."
The curves point out that  iterations would be enough for the syl- task.
"Furthermore, it is quite im-portant to experiment with a high number of start grammars."
"The accuracy usually increases of about 30% until the maxima is reached, inde-pendently which start grammar was used."
The range between the best and the worst grammar of the start grammars is about 30%.
The large range indicates that it is worthwhile to search for a start grammar with a high start precision value.
The described training procedure  a number of grammars that vary in their per-formance.
"The next step is to choose the best grammar with the highest performance, which yield a syllable accuracy of almost 90% on the test data. rors Qualitative that were made Evaluation on the . evaluati[REF_CITE]er-by the best model."
"The trained model showed main  in predicting the coda structure. 60.1% (i.e. 564) of the errors were made when the model predicts that the consonants belongs to the onset, however they are part of the coda. 39.9% (i.e. 374) of the errors were made when the models predicts the onset structure. with83%alveolarof theconsonantswrong coda."
"The rest the errors of the following list occured less than 10 times: [ p; If d; we m; analyze ks ] . how the errors could be avoided, bythewritingmost importantrules forimprovement and could beinmadethe grammar similar to those suggested by Meng (2001and) for108Englishon . 189 errors."
A furtherwere improvementmade on pre-could be made by modelling numerals seperately in120a pre-processingerrors.
"Namesprocedureand acronyms, whicharecouldfoundavoidto be a minor source of errors (27, and 19 respec-tively)."
Another problem are morpheme bound-aries in compound words.
"It would be very in-icalterestinglevel,towhereinsertwordsin thearehierarchysplit toa morpholog-, root and , and where a recursive rule allows this again, as compounding in German is a very pro-ductive Evaluation process. of the perplexitiy."
The per-plexity of a PCFG is measured after each train-ingiterationas a monotonouslyon the trainingdecreasingcorpus andfunctionis de-.
"Moreover, the inside-outside algorithm tries to reduce the perplexity of a PCFG during the training procedure."
It is very interesting if the perplexity correlates with the accuracy.
We ex-tracted for each of the 10 start grammars the grammar with the best accuracy values and the perplexity measured at that point.
The upper curve of  5 shows the best precision val-ues of all 10 grammars and the lower one shows the measured perplexity.
The grammars were ordered on the x-axis by decreasing accuracy.
Figure 5 shows that there is no correlation be-tween the accuracy and the perplexity. shows a fragment of the expanded grammar.
The example word Luftloch (air pocket) yields 41 analyses according to the new grammar.
Fig-ure 7 depicts the correct analysis.
The main idea is to use the standard probability model for dis-ambiguation of the analyses.
"In this section, we present the application of the standard probability model to grapheme-to-phoneme (G2P) conversion (i) using a CFG to produce all possible phonemic correspondences of a given grapheme string, (ii) predicting pro-nunciation by choosing the most probable anal-ysis, and (iii) reading  the transcription from the phoneme tier."
A fragment of the grammar is already described in section 2.
"We used. the same training proce-dure like in section 2.1: we initialize the[REF_CITE]times with ran-domized rule probabilities (10 start gram- mars), each of the start grammars is re-estimated 10 times on the training corpus with the inside-outside algorithm.[REF_CITE]words, ."
Thenot appearingevaluation incorpusthe trainingconsists corpus.
The words were extracted from 295105 words of the CELEX dictionary not appearing in the newspaper corpus.
"From this test set we manually eliminated (i) foreign words, (ii) acronyms, (iii) proper names, (iv) verbs, and (v) words that did not exactly consist of two syllables."
The ambiguity expressed as the aver-age number of analyses per word was 289.
Each ofcomparingthe grammarsthe mostis evaluatedprobable transcriptionlinguisticallyofbya wordtionarywith. ThethewordtranscriptionaccuracyofwasthemeasuredCELEX dic-by: the number of correctly analyzed words devided by the number of all words appearing in the test corpus.
Figure 8 shows the results of the train-ing procedure.
Almost all of the 10 grammars reached the maximum of the accuracy after one iteration.
"The best grammar yields a word accu-racy of almost 40%, which is nonsatisfying."
"The worse result can be due to the various parame-trainingters thatcorpusplay a,rolenumberwith ofPCFGsstart,grammarse.g. size of. theWe systematically varied these parameters in addi-tional Qualitative experiments evaluation , presented ."
"Note, that there could be several errors in a word."
"Predict-ing vowel quality was a main error source, how-ever it was not easy to  main error sources for consonants. 906 errors (i.e. 67.3%) are due to vowel quality."
A further problem was to decide whether a /e/ is transcribed as a schwa [@] or a [ E ] .
Somethatproblems/e/ is tran-ap-peared when two adjacent identical graphemes were found e.g. Schneeball ( snow ball ).
"Between the two /ee/s, there could either be a morpheme boundary i.e. the  vowel belongs to the  syllable and the second vowel to the succeeding syllable, or the two vowels refer to a long vowel ( [ e :] ). tifariousThe errors. 155regardingout of 440theerrorsconsonantsare madeareonmul-the feature voiced vs. unvoiced."
"The model pre-dicted a [ d ] instead of a [ t ] in 53 cases. [ p ] instead of [ b ] was predicted 11 times, and [ b ] instead of [ p ] 12 times. 39 times a [ v ] was predicted instead of a [ f ] , and 20 times a [ s ] instead of a [ z ] , and 12 times a [ z ] instead of a [ s ] ."
"Another problem was that the algorithm transcribed 66 times a /s/ as a [ S ] in the coda, which can be avoided if the rule s comes !"
S fromis restrictedthe to/theig/ onsete.g. in.
Afurther( liquid error).
The model transcribed a /g/ 62 times as a [ x ] [Footnote_1] instead of a [ k ] .
"1 Note, that CELEX transcribe both the velar frica-tive [ x ] and the palatal fricative [ C ] as [ x ]. They suggest that the correct fricative can be chosen in a pre-lexical step."
Another source of error (48 times) is that the algorithm transcribes an /n/ preceding a /k/ as an [ n ] instead of a [ N ] .
This rule have to be applied except when there is a morpheme boundary.
Figure 9 shows the results of the accuracy and the per-plexity.
"The accuracy is a decreasing function, whereas the perplexity did not change remark-ably."
"Thus, there is no correlation between ac-curacy and perplexity."
"The results correspond to the  shown in section 2.1 for cation. 3.2 Additional experiments  Start  parameters , as . theWegrammarsvaried theareparameterrandomly initialized in the beginning of the training proce-dure and the inside-outside algorithm can only detect local maxima."
"We experimented with 50 randomly initialized start grammars yielding a 3% Size increase of training in accuracy corpus to 42 . .In5%further. experi-ments we varied the size of the training corpus systematically: 4500, 9600, 15000, 33000, 77000, 182000, 398000 and 1000000 words."
We initial-ized 50 start grammars and trained each gram-mar with 10 iterations on the  corpora.
The best grammar achieved a 42.6% accuracy on a corpus size of 182000 words.
It is quite inter-esting that a corpus size of 398000 and 1000000 did not yield better results than a smaller cor-pus Type . training.
"In another experiment we investigate if the accuracy can be increased by using a typenized training corpus, i.e. a training corpus where a word appears only once."
"The size of the training corpus was systematically varied: 250, 500, 1000, 2000, 4000, 8000, 16000, 32000 and 64000."
The accuracy increased from 38.01% with 250 types to 41.25% with 32000 types and then started to decrease again.
We have presented an approach to unsuper-vised learning and automatic detection of sylla-ble boundaries and grapheme-to-phoneme con-version using the standard probability model.
"Automatic conversion of a string of characters, i.e. a word, into a string of phonemes, i.e. its pronunciation, is essential for applications such as speech synthesis from unrestricted text in-put, which can be expected to contain words that are not in the system&apos;s pronunciation dic-tionary or otherwise unknown to the system."
The phoneme string received by grapheme-to-phoneme conversion has to be  before it can be further processed to speech.
In our  experiment a phoneme string was segmented to syllables.
The best model achieved a syllable ac-curacy of almost 90%.
In a further experiment we added supplemantary grapheme-to-phoneme rules to the context-free grammar and applied the CFG to G2P.
The results of 42.5% show that the G2P task cannot be solved  with a simple PCFG.
"The variation of the pa-rameters: size of the training corpus, number of start grammars, and type training did not note-worthyWe assumeincreasethatthethewordgrammaraccuracymodels. syllab-   quite well but grapheme-to-phoneme conversion needs a more elaborate grammar that expresses e.g. the position of the syllable in a word, a  treatment of onset and coda consonants, the position of the consonant in the consonantwith grammarclusterrules."
"Alternativelycould improve, wethesupposeperformancethat another probability model performs better on the G2P task."
"Although, it is  to compare the per-formancewant to referwith otherto severalsystemsapproachesand methodsthat, ex-we amined the  and grapheme-to-phoneme conversion task."
They used a CFG to generate all possible phonemic corresponcences of a given grapheme string and then applied a probabilistic sylla-ble model predicting pronunciation by choos-ing the most probable analysis.
The proba-bilistictranscribedsyllabledatabasemodel. wasThe trainedBell
LabsonGermana large TTS system[REF_CITE]performed at bet-ter than 94% word accuracy on our test set.
This TTS system relies on an annotation of mor-andphologicalit performsstructurea morphologicalfor the wordsanalysisin its lexiconof un-known words[REF_CITE]; the pronunciation rules draw on this structural information.
"However, she trained and eval-uatedpearingoninaboutthe[REF_CITE].000 mostCorpusfrequent."
Damperwordsetap-al. (1999) reported a 72% word accuracy on un-aligned English data.
Van dentaskBoschwith (1997inductive) investigatedlearningthe algorithms.
He reported a generalisation error for words of 2.2% on English data.
"However, in German (as well as Dutch and Scandinavian languages) compounding by concatenating word forms is an extremely productive process."
"Thus, the  task is much more  in German than in English."
Daelemans and van den[REF_CITE]report a 96% accuracy on  syllable boundaries for Dutch with a backpropagation learning algorithm.
We have presented an approach to unsuper-vised learning and automatic detection of sylla-ble boundaries and grapheme-to-phoneme con-version using the standard probability model.
In our  experiment a phoneme string was segmented to syllables.
We achieved a syllable accuracy of 90%.
In a further experiment we added supplementary rules to the context-free grammar and applied the CFG to grapheme-to-phoneme conversion.
The results of 42.5% show that the G2P task cannot be solved ciently.
"The variation of the parameters: size of the training corpus, number of start grammars, and type training did not noteworthy increase the word accuracy."
"A more elaborate grammar that models morphological structure, might in-crease the accuracy."
"Moreover, another prob-ability model increases the performance of the task."
"E¯I¨°«m¢¡ ±MO²!~&amp;&amp;Um¶·O²&amp;¤2¨IO¤&amp;S¶ ¨U²!¦¬E/¯¸­®/O²!¯I~¯U¹GE­®¹E¢E²!¦¤&amp;I¡&lt;®/&amp;¤7¢E£ /«!¼¤&amp;O¹E¹/¨M ¦¥&amp;* ¿ ªU E­®¹E¢E²!®¤&amp;M¡&lt;¥A¦¯[¶ «a­¦§I¨IÁÇSÈ[/¦¡ª§I¡¨w¦¯´¤&amp;I&amp;&amp;§I¨U©¢E±i©EÅÁ7O²&amp;/¡ ¦¨I¨U¢E²&amp;/¯ÂÁ7O²/¨ÌË!ÃE¢Ä¬ÂÁ7¢/ [¦¯U¹UÅ¨U/­ÆÅ E¯I¨2K²/¡&lt;¥4O²!!&lt; a¬SE­~&lt;O¤&amp;/®¢¯¨Í£v²[&amp;¢¡,/¨2Ë / !O²!¯I~/«m¤&amp;¥a¿&lt;[&amp;¥( ¶ &amp;/«m¤&amp;&amp;&amp;/«m¤&amp;&amp;&amp;®Ïa¤ ! E¦¯I~¯U¹ ¨IO¤&amp;&lt;O²&amp;4mÈUE¡&lt;¦¯I/*¨ ! ¦¥ «aE­¦«a§I­~/¨ÐE¥ /­¦­nE¥ &amp;¤ Ua²&amp;&amp;² ¢E²   [Ã ¯U¢c¾&apos;¯ E¯I¨z§I¯UÃ[/¯I¥a¿µUw²!/&amp;¥ §I­®¤&amp;¥Ñ¥&amp;U¢Ä¾Ò¨I  ¯I«m/¥?/¯Õ¤&amp;UO±I±I²&amp;/&amp;Iµ¨I¦£~¶/ £va²&amp;/¯Ö¤4­~¦¯U¹§I¦¥&amp;¤&amp; ¦«]¦¯I£f¢E²!¡wO¤&amp;®¢¯µÔM§I¦­®¤&amp;U&lt;[¥(¶ /&lt;¡ ¥a¿ Ý× ¯4¤&amp;ØcÙ"
CEÚ}/N Ú Ù &lt;/O²!¯I¦¯I¹E­®¹E¢O¶
¸E¹[º/\:¼ : \\:¼ ::¼ÌÊÍ»ÏÎJÅ:ÊË»JÐ Å ¼HÈcºÁJÄ£Å:¼HÊÍºÑÄ£Ò8ÒJÃHÈ\Ä£ºÁc¹[Å)ÊË»*.Õ;È`Ö ::¹: ÁJÄZÅrÆg¹4¹[»Ø½ËÊË¼H¼H½Ë¹fÃ:::ÈÌÚÛÄZÃ¢È\»Ø¼ÂÁc¹BÜ~:¼HÊËÈZ»ØÈÓÚ :~[:ÃÂÄÓÊÍ»cÊÍ»cÐYº/È\Ã:ÒgÈZÃÂÄ ÊÍ»~ágÎc¹[»Jº/¹r¼:~ÎJÄÓ½ËÊË¼Ý¾È£Ú;ÅH¼:ÈcºÁJÄ£Å:¼HÊÍº*[Ã:Ä£¼HÊËÈZ»Bº/È\ ÒgÈZ»c¹[=?Ö¹*ÊÍ»\×£¹[ÅH¼:Ê¿Ð\ÄÓ¼H¹(:¹*ÊÍÅ:ÅÂÎc¹[Å ÎJÅ:
"ÊË»JÐÌ¼:ÁJ¹ä *éØÅH¾cÅ:¼H¹[É¢Ôêä *éÙÎ8ÅH¹[ÅY¼ÝÖ È ßcÊÍÅH¼:ÊË»8º/¼,Å:¼HÈcºÁJÄZÅH¼:::Ã  \Ã:\Ä£»Jß*,$¹3ÎJÅ:¹ Ä£ÎJ¼HÈZÉÄ£¼HÊÍº,\Ð Ã:/íc¼ÂÃ:ÄZº/~Îc¹[)Å ¼HÈ3¹/íc¼:ÃÂÄ£º4¼ ÐZÃÂÄ£ÉÉÄ£ÃÂÅ ÚÛÃ::\¼HâðÅ::Ã:~:Ác¹ [» ÎJÅ:¹Ñ¼:$Ác¹[ÅH¹Ñ¹4íc¼::ÃÂÄ£º\/:¼ :¼HÊËÐZÄ£¼H¹3¼:?::~ÎJÄÓ½ËÊË¼Ý¾(~ÈÓÚâ :ÁJ¹ÄZ»J»cÈ£¼ÂÄÓ¼::ÊÍ»cÐÄÁJÄ£»JßcâðÄZ»J»cÈ£¼ÂÄÓ¼:¼ /È\:Ã Ò8ÎJÅ( ¹4½Ë½;Ä£Å(ÄZ»ÄZÎc¼HÈ\:Ä£¼H¹[ßº4ÈZÃHâº ;ô3ÎJÃ;Ã:::Á8ÄÓ¼;ÄZÎc¼HÈ\~â ¼:ÃÂÄ£º4¼HÊËÈZ»àÊËÅ)Äµ×\ÊÍÄ£ÆJ½¿¹òÄÓ½Ë¼H¹[:Ã : ÉÄ£ÃÂÅ#ÚëÈ\Ã :¼:\ÃH¹ZÀcÄ£Å ¹//:¼  Ü~ÎJÄÓ½ËÊË¼Ý¾YÄ£»JßrÅ::ÃÂÄÓÊÍ»cÊÍ»cÐ(º/È\:Ã : ö ÷ 9¶c6JGÑø;;ù ·S¶J7HG,9 ¸E¹[º/\:¼ : \\:¼ ::¼ÌÊÍ»ÏÎJÅ:ÊË»JÐ :Å ¼HÈcºÁJÄ£Å:¼HÊÍºØÄZÒJÒJÃ:[:Ã \¼ÄZ»Jß :Ê¿½ËÈ£ÐZ½¿È\ÎtÀ8þ[ÿ£ÿ Jõ  ?"
Ä£»cÐ\\ñ Ê¿½Íßc¹;ÄZ»Jß(\[ÿ£ÿ 8õ ÄZ»cÐZñ\ÊË½ËßJ¹£À  ;ÎJßJ»cÊÍºñ\¾£À  Jõ§¸;Ä£¼:»JÄÓâ ÒJÄZÃ:ñ~
"ÁcÊmÀ  Jõ òÄZ»cÐZÄ£½¿È\  :Ô 8È\Ã Ð£¹[:Ê¿È\»tÀ8Å:¼HÈcºÁJÄZÅH¼::ÁcÈcßJÅ3ÒJÃ:::ÎJÒg¹/â Ã :\¼Ñº/\È »\¼H¹/íc¼ÂÅ :Ê¿È\Ã,¼:ÈÁ8Ä£»Jß~âÝº4ÃÂÄ`Úë¼:([Ã:Ä£¼HÈ\Ã:,Å ÊË»$ûÄ£¼ ¼ ? ¼ÝÖ ÈßcÊïÚ â ìóÁc¹[»® ¼:Ác¹ ÃÂÄ£»JÐ£¹rÈÓÚ§ÈZÎc¼ÂÒJÎc¼(¼HÈfÆg¹NÐ£¹[: ÖÑÊÍßc¹£À,/[\ ¼ÂÃ:ÄZ»JÅH½ÍÄÓ¼:Ê¿È\»YÅ:¾cÅH¼: ìóÁc¹[» Ä([:Ã :¼H¹[;[ÄÓ¼: :¾*Ü\ÎJÊËºñ\½Ë¾£Ô"
Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
"While this has allowed for quanti-tative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable pars-ing models are across corpora."
"We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser&apos;s probability model are particularly tuned to the corpus on which it was trained."
This leads us to a technique for pruning parameters to reduce the size of the parsing model.
"The past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data."
"The techniques[REF_CITE],[REF_CITE], and[REF_CITE]achieved roughly comparable results using the same sets of training and test data."
"In each case, the cor-pus used was the Penn Treebank&apos;s hand-annotated parses of Wall Street Journal articles."
"Relatively few quantitative parsing results have been reported on other corpora (though see[REF_CITE]for results on Switchboard, as well[REF_CITE]for results[REF_CITE]for bootstrapping from WSJ to ATIS)."
The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across cor-pora.
In this paper we examine the following ques-tions:
"To what extent is the performance of statistical parsers on the WSJ task due to its relatively uniform style, and how might such parsers fare on the more varied Brown corpus?"
Can training data from one corpus be applied to parsing another?
"What aspects of the parser&apos;s probability model are particularly tuned to one corpus, and which are more general? a surprisingOur investigationresult aboutof theseparsingquestionsthe WSJleadscorpusus to: over a third of the model&apos;s parameters can be elim-inated with little impact on performance."
"Aside from cross-corpus considerations, this is an impor-tant nding if a lightweight parser is desired or mem-ory usage is a consideration."
A great deal of work has been done outside of the parsing community analyzing the variations between corpora and di erent genres of text.
Of particular importance to statistical parsers is the investigation of frequencies for verb subcategoriza-tions such[REF_CITE].
"Ar-gument structure is essentially the task that auto-matic parsers attempt to solve, and the frequencies of various structures in training data are re ected in a statistical parser&apos;s probability model."
The varia-tion in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing an-other.
"The probability models of modern parsers include not only the number and syntactic type of a word&apos;s arguments, but lexical information about their llers."
"Although we are not aware of previous comparisonswe can only assumeof the thatfrequenciesthey varyof argumentat least as muchllers, as the syntactic subcategorization frames."
We take as our baseline parser the statistical model of Model 1[REF_CITE].
"The model is a history-based, generative model, in which the probability for a parse tree is found by expanding each node in the tree in turn into its child nodes, and multiplying the probabilities for each action in the derivation."
"It can betic thoughtcontext-freeof asgrammara variety, withof lexicalizedthe rule probabilitiesprobabilis-factored into three distributions."
"The rst distribu-tion gives probability of the syntactic category H of the head child of a parent node with category P , head word Hhw with the head tag (the part of speech tag of the head word)"
P h ( H j P; Hht; Hhw )
The head word and head tag of the new node H are de ned to be the same as those of its parent.
The remaining two distributions generate the non-head children one after the other.
A special #STOP# symbol is generated to terminate the sequence of children for a given parent.
Each child is gener-ated in two steps: rst its syntactic category C and child&apos;s features and a functionhead tag Cht are chosen given the parent&apos;srepresentingand headthe distance from the head child:
P c ( C; Cht j P; H; Hht; Hhw; )
Then the new child&apos;s head word Chw is chosen:
P cw ( Chw j P; H; Hht; Hhw; ; C; Cht )
"For each of the three distributions, the empirical dis-tribution of the training data is interpolated with less speci c backo distributions, as we will see in Section 5."
"Further details of the model, including the distance features used and special handling of punctuation, conjunctions, and base noun phrases, areThedescribedfundamentalin Collinsfeatures(1999of). used in the proba-bility distributions are the lexical heads and head tags of each constituent, the co-occurrences of par-ent nodes and their head children, and the co-occurrences of child nodes with their head siblings and parents."
"The probability models[REF_CITE],[REF_CITE]and[REF_CITE]di er in their details but are based on similar fea-tures."
"Models 2 and 3[REF_CITE]add some slightly more elaborate features to the probability model, as do the additions[REF_CITE]to theOurmodelimplementationof Charniak (of1997Collins&apos;)."
Model 1 performs at 86% precision and recall of labeled parse con-stituents on the standard Wall Street Journal train-ing and test sets.
"While this does not re ect the state-of-the-art performance on the WSJ task achievedniak (2000by) theandmoreCollinsthe(complex2000), wemodelsregardofitChar-as a reasonable baseline for the investigation of corpus e ects on statistical parsing."
"Wedata,conductedBrown dataseparate, and aexperimentscombinationusingof theWSJtwo as training material."
"For the WSJ data, we ob-served the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets."
"For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training."
This may underestimate the dif-fromcultytheof samethe Browndocumentscorpusinbytrainingincludingand sentencestest sets.
"However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative."
Only the subset of the Brown corpus available in the Treebank II bracket-ing format was used.
This subset consists primarily of various ction genres.
Corpus sizes are shown in Table 1.
"Table 2: Parsing results by training and test corpus resultsResultsfor forcomparisonthe Brown, arecorpusshown, alongin Tablewith2."
WSJThe basic mismatch between the two corpora is shown in the signi cantly lower performance of the WSJ-trained model on Brown data than on WSJ data (rows 1 and 2).
"A model trained on Brown data only does signi cantly better, despite the smaller size of the training set."
"Combining the WSJ and Brown training data in one model improves performance further, but by less than 0.5% absolute."
"Similarly, adding the Brown data to the WSJ model increased performance on WSJ by less than 0.5%."
"Thus, even a large amount of additional data seems to have rel-atively little impact if it is not matched to the test material. seemsThetomoreimpactvariedresultsnature, asofallthetheBrownresultscorpuson"
Brownalso are lower than the WSJ result.
The parsers cited above all use some variety of lexical dependency feature to capture statistics on the co- occurrence of pairs of words being found in parent-child relations within the parse tree.
"These word pair relations, also called lexical bigrams[REF_CITE], are reminiscent of dependency grammars such as  (1988) and the link grammar[REF_CITE]."
"In Collins&apos; Model 1, the word pair statistics occur in the distribution"
P cw ( Chw j P; H; Hht; Hhw; ; C; Cht ) where Hhw represent the head word of a parent node in the tree and Chw the head word of its (non-head) child. (The head word of a parent is the same as the head word of its head child.)
"Because this is the only part of the model that involves pairs of words, it is also where the bulk of the parameters are found."
The large number of possible pairs of words in the vocab-ulary make the training data necessarily sparse.
"In order to avoid assigning zero probability to unseen events, it is necessary to smooth the training data."
The Collins model uses linear interpolation to es-timate probabilities from empirical distributions of varying speci cities:
"P cw ( Chw j P; H; Hht; Hhw; ; C; Cht ) = 1 P ~( Chw j P; H; Hht; Hhw; ; C; Cht ) + (1 , 1 ) 2 P ~( Chw j P; H; Hht; ; C; Cht )+ (1 , 2 ) P ~( Chw j Cht ) (1) where P ~ represents the empirical distribution de-rived directly from the counts in the training data."
"The interpolation weights 1 , 2 are chosen as a function of the number of examples seen for the con-ditioning events and the number of unique values seen for the predicted variable."
"Only the rst distri-bution in this interpolation scheme involves pairs of words, and the third component is simply the prob-ability of a word given its part of speech. ci Becausec in the modelthe word, it ispairlikelyfeatureto be isthethemostmostcorpus-spe-speci c."
"The vocabularies used in corpora vary, as do the word frequencies."
It is reasonable to ex-pect word co-occurrences to vary as well.
"In or-der to test this hypothesis, we removed the distribu-tion P ~( Chw j P; H; Hht; Hhw; C; Cht ) from the pars-ing model entirely, relying on the interpolation of the two less speci c distributions in the parser:"
"P cw 2 ( Chw j P; H; Hht; ; C; Cht ) = 2 P ~( Chw j P; H; Hht; ; C; Cht ) + (1 , 2 ) P ~( Chw j Cht ) (2) toWedetermineperformedwhethercross-corpusthe simplerexperimentsparsingas modelbefore might be more robust to corpus e ects."
Results are shown in Table 3. thePerhapseliminationthe mostof lexicalstrikingbigramsresulta isectsjustthehowbaselinelittle system: performance on the WSJ corpus decreases by less than 0.5% absolute.
"Moreover, the perfor-mance of a WSJ-trained system without lexical bi-grams on Brown test data is identical to the WSJ-trained system with lexical bigrams."
Lexical co-occurrence statistics seem to be of no bene t when attempting to generalize to a new corpus.
The relatively high performance of a parsing model with no lexical bigram statistics on the WSJ task led us to explore whether it might be possible to signi cantly reduce the size of the parsing model bycingselectivelyperformanceremoving.
"Suchparametersa techniquewithoutreducessacri-the parser&apos;s memory requirements as well as the over-head of loading and storing the model, which could be desirable for an application where limited com-puting resources are available. niquesSigniforcantpruninge ort hasstatisticalgone intolanguagedevelopingmodelstech-for speech recognition, and we borrow from this work, using the weighted di erence technique[REF_CITE]."
"This technique applies to any statistical model which estimates probabilities by backing o , that is, using probabilities from a less speci c distribution when no data are available are available for the full distribution, as the following equations show for the general case:"
P ( e j h ) =
"P 1 ( e j h ) if e 62 BO( h ) = ( h ) P 2 ( e j h ) if e 2 BO( h ) 0 conditioningHere e is theeventseventorto history be predicted, is a backo, h is theweightset of, and h 0 is the subset of conditioning events used for othesetlessofspecieventsc backofor whichdistributionno data are."
BO presentis theinback-the speci c distribution P 1 .
"In the case of n-gram lan-guage modeling, e is the next word to be predicted, and the conditioning events are the n , 1 preceding words."
"In our case the speci c distribution P 1 of the backo model is P cw of equation 1, itself a linear in-terpolation of three empirical distributions from the training data."
"The less speci c distribution P 2 of the backo model is P cw 2 of equation 2, an interpolation of two empirical distributions."
"The backo weight is simply 1 , 1 in our linear interpolation model."
The Seymore/Rosenfeld pruning technique can be used to prune backo probability models regardless of whether the backo weights are derived from lin-ear interpolation weights or discounting techniques such as Good-Turing.
"In order to ensure that the model&apos;s probabilities still sum to one, the backo weight must be adjusted whenever a parameter is removed from the model."
"In the Seymore/Rosenfeld approach, parameters are pruned according to the following criterion:"
"N ( e; h )(log p ( e j h ) , log p ( e j h )) (3) 0 0 0 ( e j h ) represents the new backed o proba-bilitywhereestimate p after removing p ( e j h ) from the model 0 and adjusting the backo weight, and N ( e; h ) is the count in the training data."
"This criterion aims to prune probabilities that are similar to their back-o estimates, and that are not frequently used."
"As shown[REF_CITE], this criterion is an approx-imation of the relative entropy between the original and pruned distributions, but does not take into ac-count the e ect of changing the backo weight on otherAdjustingevents&apos;theprobabilitiesthreshold. below which parameters areand prunedmore parametersallows us. toResultssuccessivelyfor di erentremovevaluesmoreof are shown in Table 4."
The lexical bigrams are contained in the most speci c distribution for P cw .
Removing all these parameters reduces the total model size by 43%.
The results show a gradual degradation as more parameters are pruned. theThepruningten lexicalmetricbigramsare shownwith thein Tablehighest5scoresfor WSJfor and Table 6.
The pruning metric of equation 3 has been normalized by corpus size to allow compari-sonbetweenbetweenthe twoWSJsetsandis forBrownpairs. ofTheunknownonly overlapword tokens.
"The WSJ bigrams are almost all speci c to nance, are all word pairs that are likely to ap-pear immediately adjacent to one another, and are all children of the base NP syntactic category."
"The Brown bigrams, which have lower correlation val-ues by our metric, include verb/subject and prepo-sition/object relations and seem more broadly ap-plicable as a model of English."
"However, the pairs are not strongly related semantically, no doubt be-cause the rst term of the pruning criterion favors the most frequent words, such as forms of the verbs \be&quot; and \have&quot;."
Our results show strong corpus e ects for statistical parsing models: a small amount of matched train-ing data appears to be more useful than a large amount of unmatched data.
The standard WSJ task seems to be simpli ed by its homogenous style.
"Adding training data from from an unmatched cor-pus doesn&apos;t hurt, but doesn&apos;t help a great deal either. beIncorpus-speciparticular, lexicalc, and bigramour resultsstatisticsshow appearthat theyto are of no use when attempting to generalize to new training data."
"In fact, they are of surprisingly little bene t even for matched training and test data | removing them from the model entirely reduces per-formance by less than 0.5% on the standard WSJ parsing task."
"Our selective pruning technique al-lows for a more ne grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration."
"In our im-plementation, pruning allowed models to run within 256MBThe parsingthat, unprunedmodels, required[REF_CITE]and."
"An area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the pe-culiarities of the Wall Street Journal."
Of particu-lar interest are the automatic clusterings of lexical co-occurrences used[REF_CITE]and[REF_CITE].
Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser&apos;s use.
"Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132."
BmpE0apHP^.o0.P0paH.HB Q.plBHQ~.HQH &apos;aHP [. £0¥mHP^a¡¦p0P£[ac.§¢pB¨~¨0HpE^©ª¢&apos;.«BH 0HBP¬HkE¨0[£B.C­ 1aap¨kB1£B¢Ppp®°¯A .aH~¨~m¢&apos;±1²³§p0P£¡´HV¢£00¤.BpµHª¨0¨0HVHµHB B¢9Hp£¢£E.~mV¢taH.~¨[~ac.
BHVCp..  0¡P«P^[ac.·H©~1aPº¢p¡¡cV¨ »&apos;¼ ½g¾ ®B¿s0¹00a..¢£QBCV³¨~. «c0ÀH.P0pa [ ~aQ[a¢p¡¡c©­¨0Pac.p¨la# HPQ^H¢P¡c¡© £.[¡B[a HB .
Ã Ä 2³-~60Å Æ$$Ç /CÅg2 ap¢&apos;p^ ..
ÌxÍ ^Ha~~¨ B¢&apos;V¨Êc.^HËB[a0a¡.C.¡0.B.A.P0paHÁmB¨ AÔÓÕBÖ×PØHÙÚPÖiÔxÛÕAÛCÜ Ý×£ÚPÔÞ£ÔxÛÕ0Þ [. ~p¡B² cHQ^H¡¡c©«¢&apos;.~ßB¢tH0 ¢PBCH£^ap®  a³1©^B¢p¡¡c© ~0BHp¡c© H©I B0H^¢£0VP® Í P¸^[.×AÙÚ£åPæ~ÔÞ£ÔÓÖoÔxÛ[Õ ç [Û ÖoÖiã×&apos;ÕX×aÚ£à´Q.p¹b$^£·B¨·&apos;¸IHpB¨ BB¨I¢&apos;£[ s .P0paH0«HP¸^mV¢&apos;cÁm¢ C1©^¡¶  &apos;¸0«.H.0a0¡P# HmV¢&apos;c©^0a05ä.[éXæ~[²4¢P}¢&apos;¡cp^ B0¡c¢PHAm^HH0¡c©5^B[[HBH«[¡0p  .a¨~p¹H ¦¨~ac. ^170a«0P^7&apos;¸0«0¡³mCa[aCH¢P¡ Í
£¢S­( (  £µq³¹¢ £(¤f¥§q­(§¨¬qk­ ¨º¢q(¤f¨»L¡ ¥µq¯§£·¶\¨L( ¼ f¸ ¯§£¤f­((£L¯(?­(%( ? ¸ (¤¡µq¯?&apos;¬ (¡¶\¡(¥¬ ¿¨¬  ( ­ (¡¿µq¯?¥§­(  £¬q(§¨¬Ä¨L¶)(­  Å\ÆPÇ ÈÉ |¸ ­ £³Ê­ ¨·¿L£¬ ¡¥?­  $ ¤¡?4? ¡( ¿ ³·µq¯§­(?(­  
"Ì Í 3KLUCÏÎ$+$O9K@GC3-%$3 ÎÐ&lt; %K@ÒÑ;@GC3 çèÓqÔÒÓ¬·L¢q¢q§­(§¨¬­ ¨(¡¶\¡(¥¬ ¿ ¶Üµq¬q¤f­\(§¨¬%¾?¬ ¨µq¬k4 ¥L £ Lq(¤¡L¬DL¯¿J¬¨N¡(\¶ ¨L?(­ §¨¬M??­(¦;§»L¿L¨L¯½¨µ ­ ­( 1((¡¶\µq¤¥¡ ¸ (£((¬ ¿¼­( ½ ;  £?êL¡£ë (§¨¬qL¯$?­|¸ ­(§­(µq¢ k­ ¨©a?¥¢qÂ­(  (¡¶\¡(£¬P­ èÜì q;£¯§­¡¾Fí£îLïLð9ñFòL¨L ¸ ¢qL¬%¾ó?ôLôLôPé¥À ç ¬nõÏÃ  è ;%(­   4?(­¬ §­(L¯¤¡&apos;(¡¶\¡¥&apos;­   £¤f­¬1 ³·µq (  4?(;¨¯¢ ¶ÜL¤f$q( £L¢q¢q§­(§¨¬qL¯P¬ \¶ ¨L?­(§¨¬ ?¦;¨µ ­§­¡À ÷Gøfù)?   ÿ ! &quot; # $ &amp;% () !* )+ , /.10 :9/; &gt;&lt; =@68? :ACB(!D ;; &lt; /&lt; 9GFH3  9/; &lt;: &lt;:MN3 :&lt;:?69P8&gt;M);@Q  R qµ ¥§­ ­ £¬V­ (¡À ìË?¬qq?;­(£µ ¥L¯¯L¬?½¶\( ¿µq?¿Lk¿L£¬ ¡¥h¸ &gt; 9µ S £¬P­("
"Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences."
"In this paper, we present SPoT, a sentence planner, and a new methodology for automatically train-ing SPoT on the basis of feedback provided by human judges."
We reconceptualize the task into two distinct phases.
"First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of pos-sible sentence plans for a given text-plan input."
"Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan."
The SPR uses ranking rules automatically learned from train-ing data.
We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan.
"Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. [Footnote_1]"
"1 We would like to thank Michael Collins and Rob Schapire for their help, comments, and encouragement, and Noemie Elhadad and three anonymous reviewers for very useful feedback. This work was partially funded by DARPA under contract[REF_CITE]-99-3-0003."
"For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1:"
What airport would you like to fly out of?
I need to go to Dallas.
Flying to Dallas.
What departure airport was that?
User4: from Newark on September the 1st.
What time would you like to travel on Septem-ber the 1st to Dallas from Newark?
"Utterance System1 requests information about the caller’s departure airport, but in User2, the caller takes the initiative to provide information about her destina-tion."
"In System3, the system’s goal is to implicitly con-firm the destination (because of the possibility of error in the speech recognition component), and request in-formation (for the second time) of the caller’s departure airport."
"In User4, the caller provides this information but also provides the month and day of travel."
"Given the system’s dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and desti-nation cities and the month and day information, as well as to request information about the time of travel."
The system’s representation of its communicative goals for utterance System5 is in Figure 1.
The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.
Some exam-ple alternative realizations are in Figure 2. [Footnote_2]
2 The meaning of the human ratings and RankBoost scores in Fig-ure 2 are discussed below.
"In this paper, we present SPoT, for “Sentence Plan-ner, Trainable”."
We also present a new methodology for automatically training SPoT on the basis of feed-back provided by human judges.
"In order to train SPoT, we reconceptualize its task as consisting of two distinct phases."
"In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input."
"In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer."
Our primary contribution is a method for training the SPR.
"The SPR uses rules au-tomatically learned from training data, using techniques similar[REF_CITE]."
"Our method for training a sentence planner is unique in neither depending on hand-crafted rules, nor on the existence of a text or speech corpus in the domain of the sentence planner obtained from the interaction of a hu-man with a system or another human."
We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan.
"In the remainder of the paper, section 2 describes the sentence planning task in more detail."
"We then describe the sentence plan generator (SPG) in sec-tion 3, the sentence plan ranker (SPR) in section 4, and the results in section 5."
The term “sentence planning” comprises many distinct tasks and many ways of organizing these tasks have been proposed in the literature.
"In general, the role of the sen-tence planner is to choose abstract linguistic resources (meaning-bearing lexemes, syntactic constructions) for a text plan."
"In our case, the output of the dialog manager of a spoken dialog system provides the input to our sentence planner in the form of a single spoken dialog text plan for each of the turns. (In contrast, the dialog managers of most dialog systems today simply output completely formed utterances which are passed on to the TTS mod-ule.)"
"Each text plan is an unordered set of elementary speech acts encoding all of the system’s communicative goals for the current turn, as illustrated in Figure 1."
"Each elementary speech act is represented as a type (request, implicit confirm, explicit confirm), with type-specific pa-rameters."
The sentence planner must decide among al-ternative abstract linguistic resources for this text plan; surface realizations of some such alternatives are in Fig-ure 2.
"As already mentioned, we divide the sentence plan-ning task into two phases."
"In the first phase, the sentence-plan-generator (SPG) generates 12-20 possible sentence plans for a given input text plan."
"Each speech act is assigned a canonical lexico-structural representation (called a DSyntS – Deep Syntactic Structure (Mel’čuk, 1988))."
The sentence plan is a tree recording how these elementary DSyntS are combined into larger DSyntSs; the DSyntS for the entire input text plan is associated with the root node of the tree.
"In the second phase, the sentence plan ranker (SPR) ranks sentence plans gener-ated by the SPG, and then selects the top-ranked out-put as input to the surface realizer, RealPro[REF_CITE]."
The architecture is summarized in Fig-ure 3.
The research presented here is primarily concerned with creating a trainable SPR.
"A strength of our ap-proach is the ability to use a very simple SPG, as we explain below."
"The basis of our SPG is a set of clause-combining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations."
Examples can be found in Fig-ure 4.
M ERGE .
Two identical main matrix verbs can be iden-tified if they have the same arguments; the adjuncts are combined.
M ERGE - GENERAL .
"Same as M ERGE , except that one of the two verbs may be embedded."
S OFT - MERGE .
"Same as M ERGE , except that the verbs need only to be in a relation of synonymy or hyperonymy (rather than being identical)."
S OFT - MERGE - GENERAL .
"Same as M ERGE - GENERAL , except that the verbs need only to be in a relation of syn-onymy or hyperonymy."
C ONJUNCTION .
This is standard conjunction with con-junction reduction.
R ELATIVE - CLAUSE .
This includes participial adjuncts to nouns.
A DJECTIVE .
This transforms a predicative use of an ad-jective into an adnominal construction.
P ERIOD .
Joins two complete clauses with a period.
"These operations are not domain-specific and are sim-ilar to those of previous aggregation components[REF_CITE], al-though the various M ERGE operations are, to our knowl-edge, novel in this form."
"The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts from the input text plan, and with its interior nodes la-beled with clause-combining operations 3 ."
Each node is also associated with a DSyntS: the leaves (which corre-spond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).
The interior nodes are associated with DSyntSs by executing their clause-combing operation on their two daughter nodes. (A P E - RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)
"If a clause combination fails, the sp-tree is discarded (for ex-ample, if we try to create a relative clause of a struc-ture which already contains a period)."
"As a result, the DSyntS for the entire turn is associated with the root node."
"This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes)."
"The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized."
Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- tem5 in Dialog D1.
"Sp-trees for alternatives 0, 5 and 8 are in Figures 5, 6 and 7."
"For example, consider the sp-tree in Figure 7."
Node soft-merge-general merges an implicit-confirmations of the destination city and the origin city.
The row labelled SOFT - MERGE in Figure 4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the ori-gin and destination cities.
Figure 8 illustrates the rela-tionship between the sp-tree and the DSyntS for alter-native 8.
"The labels and arrows show the DSyntSs as-sociated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations."
"The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a sin-gle high quality sentence plan."
"In our approach, we do not need to encode such constraints."
"Rather, we gener-ate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sen-tence plans, by randomly selecting among the operations according to some probability distribution. [Footnote_4]"
"4 Here the probability distribution is hand-crafted based on assumed preferences for operations such as SOFT - MERGE and SOFT - MERGE - GENERAL over CONJUNCTION and PERIOD . This allows us to bias the SPG to generate plans that are more likely to be high quality, while generating a relatively smaller sample of sentence plans."
The sentence-plan-rankerSPR takes as input a set of sen-tence plans generated by the SPG and ranks them.
"In order to train the SPR we applied the machine learning program RankBoost[REF_CITE], to learn from a labelled set of sentence-plan training examples a set of rules for scoring sentence plans."
RankBoost is a member of a family of boosting algo-rithms[REF_CITE].
"For example, one such indicator function might be    if  DSYNT - TRAVERSAL - PRONOUN ( ) otherwise ! $% if the number of pronouns in is &amp; . &quot;#"
"To apply RankBoost, we require a set of example sp-trees, each of which have been rated, and encoded in terms of a set of features (see below)."
We started with a corpus of 100 text plans generated in context in 25 di-alogs by the dialog system.
"We then ran the SPG, param-eterized to generate at most 20 distinct sp-trees for each text plan."
"Since not all text plans have 20 valid sp-trees (while some have many more), this resulted in a corpus of 1868 sentence plans."
The ratings given by the judges were then averaged to provide a rat-ing between 1 and 5 for each sentence plan alternative.
"The ratings assigned to the sentence plans were roughly normally distributed, with a mean of 2.86 and a median of 3."
"Each sp-tree provided an example input to Rank-Boost, and each corresponding rating was the feedback for that example."
"Rankboost, like other machine learning programs of the boosting family, can handle a very large number of fea-tures."
"Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones."
"In total, we used 3,291 features in training the SPR."
"Features were discovered from the actual sentence plan trees that the SPG gener-ated through the feature derivation process described be-low, in a manner similar to that used[REF_CITE]."
The motivation for the features was to capture declar-atively decisions made by the randomized SPG.
We avoided features specific to particular text plans by dis-carding those that occurred fewer than 10 times.
Features are derived from two sources: the sp-trees and the DSyntSs associated with the root nodes of sp-trees.
The feature names are prefixed with “sp-” or “dsynt-” depending on the source.
There are two types of features: local and global.
"Local features record struc-tural configurations local to a particular node, i.e., that can be described with respect to a single node (such as its ancestors, its daughters, etc.)."
"The value of the fea-ture is the number of times this configuration is found in the sp-tree or DSyntS. Each type of local feature also has a corresponding parameterized or lexicalized ver-sion, which is more specific to aspects of the particular dialog in which the text plan was generated. [Footnote_5] Global fea-tures record properties of the entire tree."
"5 Lexicalized features are useful in learning lexically specific restric-tions on aggregation (for example, for verbs such as kiss)."
Features and examples are discussed below.
"Traversal features: For each node in the tree, fea-tures are generated that record the preorder traversal of the subtree rooted at that node, for all subtrees of all depths (up to the maximum depth)."
"Feature names are constructed with the prefix “traversal-”, followed by the concatenated names of the nodes (starting with the cur-rent node) on the traversal path."
"As an example, consider the sp-tree in Figure 5."
Feature SP - TRAVERSAL - SOFT - MERGE * IMPLICIT - CONFIRM *
"IMPLICIT - CONFIRM has value 1, since it counts the number of subtrees in the sp-tree in which a soft-merge rule dominates two implicit-confirm nodes."
"In the DSyntS tree for alternative 8 (Fig-ure 8), feature DSYNT - TRAVERSAL -PRONOUN, which counts the number of nodes in the DSyntS tree labelled PRONOUN (explicit or empty), has value 4."
Sister features: These features record all con-secutive sister nodes.
"Names are constructed with the prefix “sisters-”, followed by the concatenated names of the sister nodes."
"As an example, con-sider the sp-tree shown in Figure 7, and the DSyntS tree shown in Figure 8."
Feature DSYNT - SISTERS - PRONOUN-ON1 counts the number of times the lexi-cal items PRONOUN and ON1 are sisters in the DSyntS tree; its value is 1 in Figure 8.
"Another example is feature SP - SISTERS - IMPLICIT - CONFIRM * IMPLICIT - CONFIRM , which describes the configuration of all im-plicit confirms in the sp-trees in; its value is 2 for all three sp-trees in Figures 5, 6 and 7."
"For each node in the tree, these features record all the initial subpaths of the path from that node to the root."
"Feature names are constructed with the prefix “ancestor-”, followed by the concatenated names of the nodes (starting with the current node)."
"For example, the feature SP - ANCESTOR * IMPLICIT - CONFIRM - ORIG - CITY * SOFT - MERGE - GENERAL * SOFT - MERGE - GENERAL counts the number of times that two soft-merge-general nodes dominate an implicit confirm of the origin city; its value is 1 in the sp-trees of Figures 5 and 6, but 0 in the sp-tree of Figure 7."
Leaf features: These features record all initial substrings of the frontier of the sp-tree (recall that its frontier consists of elementary speech acts).
"Names are prefixed with “leaf-”, and are then followed by the concatenated names of the frontier nodes (starting with the current node)."
The value is always 0 or 1.
"For example, the sp-trees of Figure 5, 6 and 7 have value 1 for features LEAF - IMPLICIT - CONFIRM AND LEAF - IMPLICIT - CONFIRM * IMPLICIT - CONFIRM , representing the first two sequences of speech acts on the leaves of the tree."
"Figure 5 sp-tree has value 1 for features LEAF - IMPLICIT - CONFIRM * IMPLICIT - CONFIRM * REQUEST , and LEAF - IMPLICIT -"
CONFIRM * IMPLICIT - CONFIRM * REQUEST * IMPLICIT - CONFIRM .
"Each of these has a corresponding param-eterized feature, e.g. for LEAF - IMPLICIT - CONFIRM , there is a corresponding parameterized feature of LEAF - IMPLICIT - CONFIRM - ORIG - CITY ."
"Global Features: The global sp-tree features record, for each sp-tree and for each operation labeling a non-frontier node (i.e., rule such as C ONJUNCTION or M ERGE - GENERAL ), (1) the minimal number of leaves (elementary speech acts) dominated by a node labeled with that rule in that tree (M IN ); (2) the maximal num-ber of leaves dominated by a node labeled with that rule (M AX ); and (3) the average number of leaves dominated by a node labeled with that rule (A VG )."
"For example, the sp-tree for alternative 8 in Figure 7 has value 2 for SOFT - MERGE - GENERAL - MAX - MIN , and - AVG , but a PERIOD - MAX of 5, PERIOD - MIN of 2 and PERIOD - AVG of 3.5."
"To train and test the SPR we partitioned the corpus into 5 disjoint folds and performed 5-fold cross-validation, in which at each fold, 80% of the examples were used for training an SPR and the other unseen 20% was used for testing."
This method ensures that every example occurs once in the test set.
We evaluate the performance of the the trained SPR on the test sets of text plans by compar-ing for each text plan:
The score of the top human-ranked sentence plan(s);
"The score of SPoT’s selected sentence plan; highest ranked sp-tree for each of the 100 text plans, ac-cording to the human experts, according to SPoT, and ac-cording to random choice."
"The human rankings provide a topline for SPoT (since SPoT is choosing among op-tions ranked by the humans, it cannot possibly do better), while the random scores provide a baseline."
The BEST distribution shows that 97% of text plans had at least one sentence plan ranked 4 or better.
The RANDOM distri-bution approximates the distribution of rankings for all sentence plans for all examples.
"Because each text plan is used in some fold of 5-fold cross validation as a test element, we assess the signif-icance of the ranking differences with a paired t-test of SPOT to BEST and SPOT to RANDOM."
A paired t-test of SPOT to BEST shows that there are cb significant differences in performance ( \[ ]9 _+ a`^ ).
Perfect performance would have meant that ! there would ! be no significant difference.
"However, the mean of BEST is 4.82 as compared with the mean of SPOT of 4.56, for a mean difference of 0.26 on a scale of 1 to 5."
This is only a 5% difference in performance.
Figure 5 also shows that the main differences are in the lower half of the distribution of rankings; both distributions have a median of 5.
A paired t-test of SPOT to RANDOM shows that there are also M significant Mb differences in performance (  5d + ^_ ).
The median of the RANDOM distri- ! ! bution is 2.50 as compared to SPoT’s median of 5.0.
"The mean of RANDOM is 2.76, as compared to the mean of SPOT of 4.56, for a mean difference of 1.8 on a scale of 1 to 5."
"The performance difference in this case is 36%, showing a large difference in the performance of SPoT and RANDOM."
We then examined the rules that SPoT learned in training and the resulting RankBoost scores.
"Figure 2 shows, for each alternative sentence plan, the BEST rat-ing used as feedback to RankBoost and the score that RankBoost gave that example when it was in the test set in a fold."
"Recall that RankBoost focuses on learning rel-ative scores, not absolute values, so the scores are nor-malized to range between 0 and 1."
"Figure 9 shows some of the rules that were learned on the training data, that were then applied to the alternative sentence plans in each test set of each fold in order to rank them."
We include only a subset of the rules that had the largest impact on the score of each sp-tree.
"We discuss some particular rule examples here to help the reader understand how SPoT’s SPR works, but leave it to the reader to examine the thresholds and feature values in the remainder of the rules and sum the increments and decrements."
Rule (1) in Figure 9 states that an implicit confirma-tion as the first leaf of the sp-tree leads to a large (.94) increase in the score.
Thus all three of our alternative sp-trees accrue this ranking increase.
"Rules (2) and (5) state that the occurrence of 2 or more PRONOUN nodes in the DSyntS reduces the ranking by 0.85, and that 3 or more PRONOUN nodes reduces the ranking by an additional 0.34."
Alternative 8 is above the threshold for both of these rules; alternative 5 is above the threshold for Rule (2) and alternative 0 is always below the thresholds.
Rule (6) on the other hand increases only the scores of alter-natives 0 and 5 by 0.33 since alternative 8 is below the threshold for that feature.
Note also that the quality of the rules in general seems to be high.
"Although we provided multiple instantiations of features, some of which included parameters or lexical items that might identify particular discourse contexts, most of the learned rules utilize general properties of the sp-tree and the DSyntS. This is probably partly due to the fact that we eliminated features that appeared fewer than 10 times in the training data, but also partly due to the fact that boosting algorithms in general appear to be resistant to overfitting the data[REF_CITE]."
Previous work in sentence planning in the natural lan-guage generation (NLG) community uses hand-written rules to approximate the distribution of linguistic phe-nomena in a corpus (see[REF_CITE]for a recent exam-ple with further references).
"This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions[REF_CITE], and it is difficult to develop new applications quickly."
"Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning."
Most dialog systems today use template-based gener-ation.
The template outputs are typically concatenated to produce a turn realizing all the communicative goals.
"It is hard to achieve high quality output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system."
"For these reasons,[REF_CITE]use e -gram models and[REF_CITE], maximum entropy to choose templates, using hand-written rules to score different candidates."
"But syntactically simplistic approaches may have qual-ity problems, and more importantly, these approaches only deal with inform speech acts."
"And crucially, these approaches suffer from the need for training data."
"In general there may be no corpus available for a new ap-plication area, or if there is a corpus available, it is a transcript of human-human dialogs."
"Human-human di-alogs, however, may not provide a very good model of sentence planning strategies for a computational system because the sentence planner must plan communicative goals such as implicit confirmation which are needed to prevent and correct errors in automatic speech recogni-tion but which are rare in human-human dialog."
"Other related work deals with discourse-related as-pects of sentence planning such as cue word placement[REF_CITE], clearly a crucial task whose integration into our approach we leave to future work."
"Using hand-crafted evaluation metrics, they show that a genetic algorithm achieves good results in finding discourse trees."
"However, they do not address clause-combining, and we do not use hand-crafted metrics."
"We have presented SPoT, a trainable sentence planner."
"SPoT re-conceptualizes the sentence planning task as consisting of two distinct phases: (1) a very simple sen-tence plan generator SPG that generates multiple candi-date sentence plans using weighted randomization; and (2) a sentence plan ranker SPR that can be trained from examples via human feedback, whose job is to rank the candidate sentence plans and select the highest ranked plan."
Our results show that:
Y SPoT’s SPR selects sentence plans that on aver-age are only 5% worse than the sentence plan(s) se-lected as the best by human judges.
Y SPoT’s SPR selects sentence plans that on average are 36% better than a random SPR that simply se-lects randomly among the candidate sentence plans.
We validated these results in an independent experi-ment in which 60 subjects evaluated the quality of differ-ent realizations for a given turn. (Recall that our train-able sentence planner was trained on the scores of only two human judges.)
This evaluation revealed that the choices made by SPoT were not statistically distinguish-able from the choices ranked at the top by the two hu-man judges.
"More importantly, they were also not dis-tinguishable statistically from the current hand-crafted template-based output of the AT&amp;T Communicator sys-tem, which has been developed and fine-tuned over an extended period of time (whereas SPoT is based on judg-ments that took about three person-days to make)."
SPoT also was rated better than two rule-based versions of our SPG which we developed as baselines.
All systems out-performed the random choice.
We will report on these results in more detail in a future publication.
"In future work, we intend to build on the work reported in this paper in several ways."
"First, we believe that we could utilize additional features as predictors of the qual-ity of a sentence plan."
"These include features based on the discourse context, and features that encode relation-ships between the sp-tree and the DSyntS. We will also expand the capabilities of the SPG to cover additional sentence planning tasks in addition to sentence scoping, and duplicate the methods described here to retrain SPoT for our extended SPG."
} ~ (]:*~ /*/ F~ :}UP/Ar 6/ (~:X¨©&quot;ª/}4«t~S¢t¬ ¢­/FH¢®¯t~ 6&quot;¡7ª//( 6((/ (H°F6H¦±A~ A~]¡²&lt;S(A¤~ ((tS¢6m¡ª/&lt;4µªX} t(t¡F­P~ /A6(¢¨or} ~¶ª/AkF¢PH´S£¥¯6ª/S£·} ­PX]( A&lt;¢Ptm¶~ /tF~ :A/tH¦±(~ ((ª/S£z(ªFS¥}4 (/ª ºX](A&lt;¢]£¯A»ª*(((~/¾¬ ~À¢¡/ Á½/At­F®Â*¬ÄÃ7A­/]®¢°~ (/ ¾®¢­/A4»~/¬ ®¢AA¿¦&lt;( Z~ /~ ~/»X¢(Aº¡²/F¼~ (A~F~¦±~/¢&quot;­P / ­­/Att ¦](A­/] tX~¨1©&quot;/ª &lt;X]( (A~z/­/¬ AÅA¯:AX((A¢(¦&lt;(Z&lt;¢ÆAZD*~ ( (X­/ ¸¹¶6ª/S£µ(ª¢¯ª/À}4¯(/ª ½/( V£¥t»Xª*ºX((] :  t¬ S ¦±£º¾ª!¶£¯®¦¬a¡²¢(/ A~!¢!~ t/~ (A~:(AS (: 6t~»±£ºª/(¶£¯¶(HS4®ªÇ¦//*~ 4Xºª/*/( (/ªFS°~*:((AXt¡È¬a:kt(AS(t¡¨-©v¨ &quot;ªS-´*4¤} ](A&lt;¢/¡²X°¦]]¼¬
"É Ê 5¥]{ ,/+P| /{ 6% °5+"
"6ã&quot;ñ&lt;&gt;í æ&apos;ëWí  5é&quot;è6ëGá, &lt; à &quot;áBí&gt;    .îBë á,) B Yè¼é&quot;è6ëMì&amp;&quot;  ä &amp;  £ì6í C &amp;å4ë)à«é&quot;æé&apos;ímæ&quot;)ë îBë Ié &amp;ñ &quot;)æPí  4é&apos;ëIþ5é  Ií&gt;àé&quot;,.à á,.à â®á,à &lt; &quot;áBí&gt;@à ã&quot;)ë îBë #&quot; &quot;è6ë  &quot; è6ëQì.&quot;  ä &amp;&quot;  ë .æ&apos;ë)ì4á,à  GFH ý &quot; I .è   ì. C í  .å2ë)à«é&quot;æ  &quot;&gt;í åÎæ&quot;  ë &quot;  í 6ã Ië)æ½á,à O&quot;  .&amp;ì á,à6â &quot;ë D «&lt; Q &gt; P 5 í 6ãYà."
"RD 6 P í&gt;æ&quot; B 6ã&quot; S ½á,.à  "
"A  á å4ë) T O&quot; , æ £í&gt;æ à )îBë)æ álå4ë)æ"
D .ìé&quot;è6ë US .)ë &quot;í&gt; A  &lt; à  V&quot; &quot;áBí&gt; 5á
"Ië XWYS ½úGï&apos;õ AZ 0ü è6ëPì.&quot;ä  [ Ií&gt;à.æ&quot;álæ&apos;é&quot;æ í ..ñ &quot;ã íþ5á,)îBê M ü ]K \ ú¶í Mé&apos;ëIþ5é D G.&quot;ã ë)æ&apos;ë)àé&quot;á,à6â _^ å`á,,î î,áBí&gt;. ` ô .ãálà &lt; &quot;áBí&gt;&quot;  &lt; &quot;é&apos;ë)å a &amp;æ&apos;ë) é÷Gí &quot; }ñ&amp;&quot;.ñ.ãYí&gt;ä YèUü ï à é&quot;è6ë b .ã0æ&apos;é¦ñ&amp;&quot;æ"
D ÷Gë æ&apos;ë) 0è6ë))
"A à Iê O &lt;)ë ì.,á &amp;&quot;ä &amp; è6ë¼è.áBâ&gt; O è &quot; )ë æ&apos;éæ ,.à â ñ&amp;&quot;)&quot;ëeé&quot;è6ë) c à .æ&apos;ë)ìdé&apos;í d IãYë) ëIþ5ñ&amp;.ì6ë) e ì  ,ë)æ D M&amp;&amp;ñ ,î áBë)ì¨á,àé&quot;è.ë æ&apos;ë Ií&gt;à.&quot;æ æ  ]&quot;é .è ë  G ý F &amp;ñ &quot;æ&quot;) è6ëLì.&quot;äñ.&quot;ã # ë &quot; .&quot;ã"
"O ñ í Ië)&quot;æ æ&quot;&amp;ì¼ãYë)îBë [ Iëæ ,à6âné&apos;ë"
"Yè&amp;à.á  6ë)&quot;ë æ&quot;,á å`á,,î  &quot;é è6ëí&gt;à6ë)&amp;ñ&amp;î,áBë)ìFá,àé&quot;è6ë  GF ý O&quot; ì ÿií C Uõ5û E . fFQ ì T £ï E ñ&amp;&quot;á &amp;&quot;áBí&gt;à&amp; gWY æ S 6 &lt;$ n.ì E í  &gt;í   æ  ZBDiWY 6 S  &lt;$    Z 0ü"
"E )ë îBë  [  ,  &amp;)ìí&gt;&amp; ! è ..à k á &quot; . d ì &amp; l ë) 6ã&quot;ë)æ D ëIþ5é&apos;ãYä Ié&apos;ë) ! ì ã&quot;í&gt;å¬é&quot;è6ë é&apos;ëIþ5éì&amp;&quot;äá,à®é&quot;è6ë f &gt;í î,,î í÷Dá,à.âH÷Gäê m ä &quot;è6ëfá,à. k á &quot; &quot;é á, 3 &amp;,à6â &lt;D ÷ëé&apos;í )ë à.á $ )ë ìé&quot;è6ëì6í O .å4ë)àé&quot; G æ &amp;&quot;æ ,á à6â &quot;&quot;á,&quot;æ &quot;é á  )ë .à á $  è6ëné&apos;í )ë à.á $ ë) .&quot;ã"
"O ñ í Ië)&quot;æ æ&apos;ë)ì ] êHäæ&apos;é&quot;&quot;á,æ&apos;é&quot; n á &amp;&quot;é$í ] æ&apos;ñ&lt; Yè hW óQôõ AZ é&quot; oW 9,,ì6í &lt;  0 Z )ìLí&gt;à¼é&quot;è6ëæ&apos;ñ&lt;ë)î,,î ,á à6â&gt;æ .ìé&quot;$õHé&quot;&gt;æ"
D é&quot;&quot;ñè.æ$&quot;ë 5 í .à.
"X ì ê   álà6â 6&quot;è6ë$&quot;ñ&amp; U è &quot;ãYë)æ&apos;ñ&lt;í&gt;à..ì á,à6âxé&apos;í¼ä2â&gt;á )ë à .ìóGôõ®é&quot;,&quot;ä &amp;îBë D D â6ü M D é&quot; r ã .à&amp;.à á,à6â 5s &quot;é )ì  t  Î f Ií&gt; = à &quot;é&apos;ë)ì ,á àé&apos;í  . ,&quot;  á &quot;áBí&gt;àëIþ5ñ&lt;,å4ë)àé&quot;æ r ã .  à &gt;&quot;ë)&quot;è6ëDæ&quot;]  ) _u &apos;ë v&quot; &quot;  é á     .à"
"A 0è.)ìUü &amp;   ..à ì ,á àé&quot;&quot;&amp;ñ èé&quot;ä &amp;&quot; w ë &amp;énálà é&quot;è6ë),á &gt;,á à.  ,î½é&quot;.&quot;ë  #&quot;6 &gt;í î,ì6ë)ìä &quot;è6ëî &quot;ñè6í&gt;&gt;á .&quot;,á &gt;&amp;.è ë)à.)ì .&quot;ëé&quot;è6ë)àPæ&apos;ñ&amp;,î áBéQá,àé&apos;í2é&quot;è6ë)áBã Ií&gt;å2ñ_&gt;í à.)ë à«é&quot; ç  ë .æ&apos;ë)ìPä¼å4í5ì.   á .)ë ìPô  &amp;á zW E  í &lt;&quot;é&quot;æ&apos;í&gt;      .î,ä®á,à¨é&quot;è6ë { &amp;ãYæ&apos;é +&quot; }&amp;ñ &quot;ææ ,à.â6üØßMà.á k&quot; . | ì &amp;,àé&quot;è6ë &quot;ë Ié&quot;áBí&gt;à¨í n&quot;é è6ë  &quot;."
O .å4ë)àé Ií&gt;à«é&apos;ã0á  6é&apos;ë) &quot;ëí } `~  álì &lt;# W +V ! #     +  á æ½é&quot;.è ]ë  
Ií  .àé z .
"C í  .å2ë)à«é = /  Uá,æ, &quot;é è6ëMì6í C+  &amp;å4ë)à«éîBë)&quot;è DC=C= £á,&quot;è6ëiä )&quot;è í &lt;&quot;é è6ëDì6í O .å4ë)àé&quot;,àné&quot;è.  ë Ií&gt;î,îBë Ié&quot;áBí&gt;.ì2á, A ì ½álæWé&quot;è6ë  ,à  O .å4ë)àé &quot;ã ë / .)ë"
A à Iê  Ií&gt;å4ñ .é&apos;ë)   á / ø Hà  qOL
"ZB  álì &lt;    à  qOL   ²á,æxé&quot;&quot;] / à &amp;  å &lt;  iì6í O .å4ë)àé&quot;æná,à &quot;é è6ë  &quot;  ñ .. d ì dálæ$&quot;é è6ëà  .   å &lt;  Dì6í"
"Ií&gt;àé&quot;,à.,á à6âeäPâ&gt;  á )àà &lt;&quot; } z ü nW+ ÷Gë  .æ&apos;ë)ì   qOL  v  X ..à ,á  L æ ,à..ì  qOq5L  # qOq5L &quot;.è  ë &amp;)ü è  .ë .ãYæ&apos;é ñ&amp;&quot;ækæ &quot;]ä4î,á,à6ë) Ií&gt;å &amp;,á .à &quot;,á &gt;í   .à&amp; .  ì  &quot;)ë &gt;á )ë  à êxý H &amp; nW ü + &quot;èné&quot;.è  ë .à O&quot;   æ )æW÷ë),á â&gt; qOK . x ì &amp; ) æ ÷Gë)áBâ&gt;è«éDë / . qOM ü ç ë ; Ií&gt;å4ñ 6é&apos;ë) 1 ì .ãYæ&quot; + é &quot; }ñ&amp;&quot;æ2ãYë)îBë [ Iëeæ )æ   D  í ,.&amp;ñ &quot;æ&quot;)æ D )ë ä 0è "
"Ií&gt;à«é&quot;,&amp;à á,à6â .. ñ ãYí)þ6á,)îBê    à.&gt;í O à &quot; .æ D x Ié&apos;ë)ì &quot;&gt;í  å K D"
I q ë)
"A à Iê C ,_ë)ì&amp;,á &quot;é&quot;á ) )ì¬í&gt;à¬é&quot;è6ë  &amp;&quot;&quot;æ&quot; &lt; 5,á à6â &lt;D ÷Gë  Ií&gt;à.æ&quot;é&apos;ã  Ié&apos;ë).ì6ë) ] ì  ,)ë o æ .&quot;æ &quot;è6ëîBí &quot;  G Ií&gt;&amp;,  æ Z é&apos;ë Yè.à.á n/ 6 fW¡ ë  .ì F G&quot;ã í  é  "
"Z 0&quot;è6ëkæ&apos;ë Ií&gt;.à ì4ñ&amp; ,à6â &lt;D é&quot;è6ëQëIþ C&quot; &amp;ñ ..ì ë)  ì  ,ë) ` &amp;æ&apos;ë)ìé&apos;í4æ &quot;ë M D  D =J ñ&amp; +&quot; &quot;æ ) y æ &amp;)ìeí&gt;àé&quot;è6ë  QFH ý &quot; I ¤ &quot;ñ . è6ë ñ &quot;)&quot;ëQæ&apos;ë)îBë Ié&apos;ë)ìxé&apos;í Ií&gt;à«é&quot;&amp;.ñ &quot;ã ) í þ6á,)î,ê&amp;  à.í&gt; O à &quot; ."
N¬  &lt;· &lt; &lt;·dª ±=³ /&lt;+Ë &lt;# ° &lt;#_Ë &lt;/&lt;· ¥  ¦  ¦ &lt;ÐÒÑ ¥t¦ ¦`Ó &lt; # Ó )&lt;&lt;&lt;/Ð
Ë &lt;Ö #¼5¸/&lt;Ö &lt;&lt;&lt;Ö &lt; ° &lt;&lt;&lt;_Ë &lt;&lt;  #&lt;Ö[&lt;Ö &lt;·U¬  ËØÕ{ÙgÚ  ° &lt;#&lt;· ¹#¼5¸/&lt;·]&lt;/Å&lt;#ÝCË  &lt; ¦­G®  ° &lt;&lt;· &lt;&lt;&lt;# &lt;{· &lt; &lt; /&lt;Å #&lt;&lt;[#  ° â ]ä  &gt;ïhêã ð ¨ {/&gt;ºÜÑ ðy­GÓ &lt; &lt;{§  &lt;·UÂV·#!&lt;·[&lt; °  &lt;;¼5Å&lt;#&lt;· ¥tô`® &lt;&lt;&gt;)&lt;Ö  &lt; °Ìõ &lt;#ÝOÂz¹#  &lt;·÷hø ® &lt;_&lt;Ý  //º
Æ¼O½&lt; ° õ &lt; +  Ó { /  ­ « ¥ ü ðy­iÿ « ® Û ¦ ø ð Ûy÷)« ¥t­ Û ý  «`û  ¬  ¥t­  ôy¦ Ûi¬   ð ¹  #  &lt;·Â&gt;ÍG¼ [¹# ° &lt;· &lt; # ¯=Ó &lt;_Ë  ¿C° &lt;;· Í¾n¸A½&lt; {&lt; ;&lt;_Ý&lt;=&lt;_Ë  &lt;&lt;&lt; &lt;· &lt;= ° ; ù Ó  ¨;&lt;· &lt;&lt;#&lt;&lt; &lt;Ö  ° Ûy½OË    ù µ Ó &lt; ! &lt;# °X­  ;&lt;× !    #&lt;&lt;· _&lt;Ý /[Æ¼O½&lt; ° &lt;· &lt; {{#Ë
Æ¼O½&lt;#&lt; ¯=BÓ ° &lt;];=#  û ­H¦ ýAü ð &lt;« &lt;· &lt; /&gt;º ° #   È/ &lt;&lt;&lt;Ë &gt;º{# &gt;  &lt;·yÍÈ&lt; /#· ° #&lt;· /&gt;º ½&lt;## &lt;#  = °  ¶5Ó ÍÈ&lt;{&lt;· _Ý&lt; //  °áõ ·  &lt;&lt;]# #&lt;Âo¸&lt; ¾n¸{&lt; &lt;·oÊCÅ&lt; &lt;&lt; ± ¼5Å&lt; ³_±  #]ÂV¼{&lt;· = &lt;&lt;· µ   ° îáê  æ  Ìæ  6æ  Cë5ì&gt;ïhê õ G# &lt;·GÊ/ _« &lt;· ;&lt;· ÊCÅ&lt; &lt;)  =¯ Ó &lt;[&lt;· ¥tô`®  ° [/#;·   &lt;·wÊCÅ&lt; ¸&lt;&lt;Ö5È/Â °   &lt;]#¹ /   ÀCº/  ÀCº/ &lt;=#· °  
"È&lt; ª °H­ {= # ° µ °  ;/#· &lt; ¿ =# Ó ¿C° ¬È&lt;&lt;Ö # #¹ ¼5Æ§mÅ&lt; Á ¥ &lt;Ö õ &lt;&lt;ÃBÅA© ¥  « ¥ +Ë ¥  õ  «| ®  õ  ° &lt;&lt; ° &lt;° ¬È&lt;;#&lt;·½&lt;± &lt;&lt; # ° ´C°H­ ¨  &lt;# / &lt;· =#&lt; &lt; ´ =#  & quot;!$#&amp;% ()&apos; % +* , . +/ ! #&amp;3! # #&amp;%-028. (-9! :&quot;8; &lt; = A? @ EDFHGJI) ,&gt; ,ON6PRQSN !V%+#54$YZ% &quot;&amp;# !J\ [ #. %-T ]#&quot;. ! (^[ +_^ [ # .-8a02. &quot;! +!   # &quot;. :* V%+54-YZ# % &quot;8(#&amp;t! [\# ON6PrQSN, . ] %-T #&quot;.wv! l BnmVM&quot;=&gt;&lt; = A? @ , ! ^ [ +[\# .-8t% !  &quot;%-8ST% ]%! x { #.-;| + (eV-! #&amp;!bewT]~_} +&quot;* &lt;CM:`&lt;&gt;= ?A@ ) , ,ON6PrQSN %-T# V! % +^546YZ# % &quot;&amp;# !9:&amp;# ! (&quot;![^# .-8\.-02. -! # ]! ! (^[ +*ihvrk CM&quot; , V! % +# ! ^ [ +\[ # .-802! +!--02. . (&quot;! (&quot;-9! :#V9++9 % +[\+&quot;! -x! ]YZ% &quot;&amp;# ! $\[ # .-8+*;hvrk  oDM+?AmVK] , V! %+#54\-8! ([\+^[ # -8. 0d!:&quot;!+!-WX!.. # ((&quot;!(&quot;!-9:%&quot;(8 &amp;# ! #V9+!-9+%.&quot;!-8.([\.wv+ +! .-02. (;* 9  . ! -9: [\&quot;01x&amp;&quot;.!-. 9:# &quot;#&amp;! -02. ! x &amp;$!   #A;[ ! # ( &quot;. (# +* , ]:#]4# ! ! ^[ + ]%-T54# ^ -028Rv. -9!, :#8&quot;(&quot;!-&quot;.9:$(5&apos;# $(# &quot;!(&quot;-9! :&quot;(-8! ([\&quot;!q+#&amp;%(%-%! +¡ * &amp;#  &quot;8( 8(&quot;! (&quot;!-9:&quot;+8  c1&amp;# ! &quot;]] &quot;.&quot;:9 &quot;8(+(&quot;[^T£02+&quot;!. &quot;!]/&quot;!-8. (\[ + 8((! ! . % &quot;&amp;# ¦! *"
"P  &quot;!! +&quot;. (+-8! (\[ #+! $. ]% &quot;(8[ #&amp;!%(]# !/ -! . [; %+W &quot;:9 :+&quot;!wvv 5W &quot;:9 -T% &amp;# ! # (&quot;! (&quot;! :* , (ew8((&quot;T#   &quot;!f&amp;# % (%&quot; + % W  &quot;9:##5W  &quot;:9 # (v #&amp;!z[ ++. ]&quot;(] &quot;["
"YV% ­&quot;(8 -8!(&amp;#\[ !¦+*(&quot;&quot;9:&amp;# ! -. #&amp;! ] # % (&quot;.. #w9 -! . &quot;%(_.-028(! :]+(+(#8(++!!wv (8 [\+&quot;* N % % [\# % # [; A# [ # [ (# (&quot; &quot;8(.(!-9:]T +(+&quot;% &quot;!-. (#$. +&quot;c1# ((+ #V.&quot;cn4#-8! (\[ +&quot;&quot;9®v &amp;# $! 4# ;$! # &quot;! .-&amp;# T]! ¦* ¯ °z± ^ ¸ºz·  { [! (&quot;.(#HT]! ! .](&quot;(++%;02! ,O¿# . À+v¤YZ(#y. %#8&quot;((8 &amp;# T #&amp;! &apos;X#&amp;(8 +%&amp;# ! ! + \ ^ [  #&amp;_% (+]#54  % &quot;#&amp;!.+&quot;c1# &quot;! +#! : &quot;&quot;* , +[^+[\9# +&quot;8$#54YV% &quot;8(#&amp;!-8$02!  &amp;ÃÄ&amp;Uw+Äw[REF_CITE]Æ&gt;5Ãwf5Ã5ÇUpÆb   .+&quot;c1 # &quot;! (&quot;(8 +* , $::(#54tYV% &quot;8Rv #&amp;-8! [^-8% (&quot;. 4# &amp;# % O¿, À v¤t. +&quot;c1#5&apos;)T &quot;! (&quot;8( [\8(+#&amp;(8 A# [;!y02! # ;[, 02!! * , +8+!-.y[^!\YZ-. % . & quot;&quot;.#&amp;+-87Zv! +(+&quot;! % &quot;&amp;# 8! (¡w01&apos;-&apos;)02!   #&amp;! &quot;8302! # +(#&quot;cd. :ÆbÈ]YV% &quot;8(#&amp;-8! +* ­ (8 +54# +x&amp;% :&quot;&amp;# -8Oh! -8! (\[ +^&apos;)((+!-8`k[;]. :v (&quot;9:c1#&quot;.4&quot;# &quot;-8! (# (8 +% ! ! -8([\+ !%-. &quot;(T(&quot;!-x3! [/ . +,# &quot;. &quot;}WZe. ;¿ ¿ | 4P(# Ýw, * &quot;! ((&quot;TÞ4#  , ((ew8(* (&quot;, ! &quot;.]% 54# #&amp;% ((ew8((ew8(&quot;((TÍ0d&quot;[REF_CITE]!\A#5à39# [;302!+! +% #&amp;! !-.. +&quot;c2# &quot;!(++% (&quot;. %! x¥&apos;)((+!%&quot;+c2#&amp;(8 !fWX #  -! .(!V% ++* N % + # &quot;%  &quot;-9! :ew9+c1# &quot;. (#&amp;% :( #$:-! .02-! . +(&amp;(#&amp;c2% (\} ¿i¿ 02T]#&quot;]T &quot;!#%&amp;02!! x YV% ­&quot;(8 !&amp;# !-8&quot;* ^#:&amp;# ]T &apos;#&amp;!&quot;!"
"A# [;02! ! ,  * ­ !+#(()% (&quot;. (# :#&amp;T &apos;X&amp;# ! &quot;! pþ . þ Vý &amp;!&apos;# / ~ú Vý &amp;$! # ç6õå ü î1(ãå å+ä ø ê\ì ø&amp;5ù ] æ ]ôø&amp;ü ö-åâVä&quot;(öbäOå(âwã(ä(øå&quot;ìiìæ ý (å  ø ì ä  åçOè; \÷1ä÷2ãø ì\ä  ÷dö / å 510  ì ää(ã  ä (:ø &amp;ø æ ÷ ôXøZä&amp;ö ÷2ö&quot;å öZää&amp;ø í  (ã  âwã(+ä(õ5í-ã&quot;å æªõ(å * ä  -÷1äå  (å * :õ5÷2î2í ìå å ý ç   ã 1 \ì õ5ã  +õ  ÷1å * øì &amp;&amp;özä  ìå (ä ø5ô / ú õ5öì   ã \å ì ã(ä ì ÷2ö / &amp; 5 ã ý ç Ví- 6 ã 5 : õ ; õ5÷2î2í ì å ø  +í ì ã6÷ &lt; -ä  å ì  ø  -÷2ãOå   ø ì ä &amp;ø í  aö å+å * _ö ø5ä &gt; :ø  å * :äiä å ?   0 ö 9 äõ5ö-ã \å ì ô ì 9 ø -í :å * ü âzä åã(âwã(ä(å&quot; õ5î2î ý = ÷ ó ÷2ö /5 ã \å ì ]õAâa÷2ö ø 4 å õ5ö_å  ø ì  ü (ã å * Zí-å @ &quot;öbä ä:ø&amp;æ;å æqôXø&amp;õ"
B öRø A å&quot;öZä+ý(åOã ø^Xôå ì ì (ãø å&quot;ü î1î2åå * &quot;:æ]&amp;ã÷2öuø_ö ÷2ã;&amp;íì @ Ví (å&quot;+å (ã å&quot;öaä&amp;ö](ø õ5üö 9 å  ã \å ì ÷2 / ö 5 ] ì (ä(å&quot;æ ý ö ø5ä å ì ÷1å 6 OôXø&amp;÷2öZäS÷2ãSä(øã(å+å ä å å X C å * :ø \ä å ü(ã âwã&quot;å (î1øä(å&quot;J÷2öæJø&amp;ö = ä 7 ÷ -å 5 &amp;í ì ( åè Vì ý ? õ5ö = 4 ÷2ö÷2 / ö -÷d 5 ö / ì ] 5 å&quot;ä å ý ç âVä(å&quot;ã 1 ã ø B ì &amp;ø;æ ö ã ø B ; ã¬ã(&amp;ø æ å 3 å 65 ì õ &amp;ö F: ü í ä
G í ì ä å ì ùVýÿ 0 öþ -÷2£ü E ö 5 OâVää(&quot;å ãå õ
D 5å 5 ä ø ÷ &lt; +õ5î;î1å&quot;ö / 5 5 ä  âVä(å\õ5ö-ã ^å ì -õ5ã
H :&amp;ø  &lt; ÷ å ì õ ü î1å I å 65 ì õ &amp;ö ý ( ö øç6õì ü ä î1å&quot;åã ì õ5öõ5ö 4 ì ø&amp;æ ^ ( å ã -ø Jõ D ä 5 5å&quot;å ã;ä ä(ì øqä å K &amp; JML öæ](ã âwì ÷1óã ) ä &quot;å æ ( ä ì ì å&quot;&amp;ö-ã ü å+ý ä O \ñiø5ä+å &quot;å (özåHä ä å ( è ä ì åõ5öì  4 õ5ì . ö zä 7 ÷ &amp; 5 ö-å P ÷ 7JML +õ5öbä ì õ5ö 4 : ü í ã ä øiãä å * :å ì&amp;õ5î2îö)ø ã(ä (å&quot; &apos; å æÍôXå 0 ì øì &quot;åì ]æ õ5ö :å = ÷ 7 &amp; 5 í ì å üâZä(å ã(âwã((äì &quot;å (æ¢õ5ä ø$ä -åôXåö + å 6 ì 5 øì õì æ]å * õ5ö . :ø ( è âVä(ø&quot;å ã ì ý ä ( ö å = ÷ 7 í ì å /: \ä  ü å ì ø  Zí-å @ ì ÷1å&quot;ã R&gt; -÷ &lt;% -õ ôì ø 9 -í ã:^å * å ì ü â $  T ì úVýõ5ö ç 4  å&quot;ú ìõ5å&quot;ö z÷2öã U \ &lt; ÷ +å ü (åå&quot;î2÷1åö øõ5ö-åôXø&amp;ã \å ì (ä ;å ôø&amp; ü ÷2îd÷1äâtø ¦æ]õ ÷2ö /  5 ø ì ;å ÷2æ ô ì ø å&quot;æ &quot;å  ä÷2ö$õ5ö-ã ^å ì &quot;î1å * :&amp;ö ü â ì 4 å ì õ5ö 4 ÷2ö / u 5 ä å ì &quot;å  ý :  Y +Z N[ \^`] _ . [  ( 3ö ä  \ä  ^ø â5å&quot;õ ì ã 6: (ã å å ì õ5îXå  Xø&amp;ø ìî &lt; ø õ5öõ5öZí-å @  &quot;ã(¥å+&amp;äö ýõ5ö-ã  \å ì ÷2ö   ø ì ø5ôXå&quot;ö  -ø&amp;]æ õ5÷2ö õ5î ý : ë5ë5ë 5ðøZø j ì å+&quot;å ãDõ5ö .  &lt; :å : ë5ë5ë # õ5ö  =  ø  ]
Síì   ì íø&amp;õì 4  G åVí @ +&quot;åä ã(ý õ5î&amp;ý : ë5ë  \å # ì -õ÷2ö /5; -õ5ã ü &quot;+å &quot;åõ ì å * (ø ý æ n ø&amp;î2î1ø ä ] å î2å&quot;õ ø ä åuø5ä å ì ô)õ ì  &lt; +  å çOèié\êÁå &amp;ö ü í ä ü õ5ã(åuø&amp;í ì :&amp;ø æ ôXø&amp;ö å&quot;&amp;ö æ]õpó ÷2æí-æå&quot;öZä ì   ø -å&quot;î2î2÷2ö /5 ý Po å ü &quot;å î2÷2å å ä  :ø ì  ü õ5ã(å * ã((å&quot;æ]ãuõ5î2î1ø ßä(å * -ö &amp;ø î1ø &amp; 5 ÷1å&quot;ãuä(ø ü å :&amp;ø æ   ì å * ÷2öõHã(âwã(ä(å&quot;]æ ÷ &lt; _ ì ø&amp;õ : ä Ví-ã $ í ì  å ì ÷dö / u 5 ä å ? -å 0 &quot; &lt; î ø  Ví @ å&quot;ã(&amp;ã ^å ì ÷2ö /5 ýä p q dsrut/hvuwDx\ ^dHr :&amp;ø æå ôXøô &amp;ì &quot;åö ã&quot;å(å&quot;öZäöZä ) ( ^ ;å * ¢õ÷2ã(å&gt;ü å ø åJø&amp;í ì&amp;öÜøõ ì  Hä(  å * :åã (ìâwãåJõ5ö((ä &quot;å   æ ¢õ÷2ö o ä  å y Zí-å @ &quot;(ã &amp;ö«õ5ö-ã  \å ì ÷2ö /
Äô 5 ì ø ü î1å&quot;æ ý o  å å &quot;å î { ø5ôXVí @ å&quot;å * æ(ã &amp;]ö N õpó ~ ÷2æí-æÞåã ^å ì &quot;+öbäì ø5ôZâ÷ 70.| +ø ì&amp;ö . &amp;ö-ã] } å * øì &quot;üø5ä  æ õ]ùõ ! ì  ÷2õö / ü 5 (ãý &amp;ø î2íç (ä åå ì &quot;å èièf÷dæ ôì å&quot;ì ø(ã å&quot;åöZä&quot;æ(å * å&quot;zõü ø ö / åq÷d 5 öä -÷ &lt; +&quot;ö 9 (å :â 9 &amp;+ö-ãî1ø5ôXý å * ç -÷2õ-åtä$(ã øì&amp;í ì :åqä&amp;(øaåöæ:] ì ö ÷ ó$øä ¬äåqøå ì &lt;( ÷ &amp; 5 èÁä÷2ö-õ5(ø  î 
Ví @JML &quot;å ã ) /-8¢/&gt;;/*;&lt;$ 7¤*/$/K¥D &lt;*i&lt;.*7*.9 /*.&lt; ª&gt;&lt; &lt; ; /&lt;&lt; &lt;¤6+$¥©  &lt; *&quot;  ¬¨*/*}&lt; /.*&lt; * *&gt;&gt;&lt;&lt; */6/*.*7¬ &lt;. D®.71¯$.7*6¡ ± ^ºN»I¼Uº.½¾º.¶H ¿ À ¢ 8Â&lt;/. *Ã®&quot;¬Ã ;&gt;ÄW ./6 Å ÄWHÆ+;&quot;&quot;.§®W6/*É9¡ À /¢ ; /// Å &lt;Ð&gt;/?&lt; /8&gt;ÄQD/ &lt;/ * ./  À  .¦ÆÓD8$ &lt; ?&lt; })   *7.&lt;/&gt;/* 7I*/  &lt; Ô )..× ;/* $ &lt;&lt;$D * ¡Ò?/ B&gt;7*/ *¡
Transformation-based learning has been successfully employed to solve many natural language process-ing problems.
It achieves state-of-the-art perfor-mance on many natural language processing tasks and does not overtrain easily.
"However, it does have a serious drawback: the training time is often in-torelably long, especially on the large corpora which are often used in NLP."
"In this paper, we present a novel and realistic method for speeding up the train-ing time of a transformation-based learner without  performance."
"The paper compares and contrasts the training time needed and performance achieved by our  learner with two other systems: a standard transformation-based learner, and the ICA system[REF_CITE]."
The results of these experiments show that our system is able to achieve a  improvement in training time while still achieving the same performance as a stan-dard transformation-based learner.
This is a valu-able contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.
Much research in natural language processing has gone into the development of rule-based machine learning algorithms.
These algorithms are attractive because they often capture the linguistic features of a corpus in a small and concise set of rules.
Transformation-based learning (TBL)[REF_CITE]is one of the most successful rule-based ma-chine learning algorithms.
"It is a  method which is easily extended to various tasks and do-mains, and it has been applied to a wide variety of NLP tasks, including part of speech tagging[REF_CITE], noun phrase chunking[REF_CITE], parsing[REF_CITE], phrase chunking[REF_CITE], spelling correcti[REF_CITE], prepositional phrase attachment[REF_CITE], dialog act tagging[REF_CITE], segmentation and message understand-ing[REF_CITE]."
"Furthermore, transformation-based learning achieves state-of-the-art performance on several tasks, and is fairly resistant to overtrain-ing[REF_CITE]."
"Despite its attractive features as a machine learn-ing algorithm, TBL does have a serious draw-back in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks."
"For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to  training on a 1 million word cor-pus."
"This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple it-erations of estimation and application of the base learner."
"In this paper, we present a novel method which enables a transformation-based learner to re-duce its training time dramatically while still retain-ing all of its learning power."
"In addition, we will show that our method scales better with training data size."
The central idea of transformation-based learning (TBL) is to learn an ordered list of rules which progressively improve upon the current state of the training set.
"An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made."
The following  and notations will be used throughout the paper:
The sample space is denoted by S ;
C denotes the set of possible  of the samples;
"C [ s ] denotes the  associated with a sample s , and T [ s ] denotes the true tion of s ; p will usually denote a predicate  on S ;"
"A rule r is  as a predicate - class label pair, ( p; t ) , where t 2 C is called the target of r ; R denotes the set of all rules;"
"If r = ( p; t ) , pr will denote p and tr will denote t ;"
A rule r = ( pr;tr ) applies to a sample s if pr ( s ) = true and tr = 6 C [ s ] ; the resulting sam-ple is denoted by r ( s ) .
Using the TBL framework to solve a problem as-sumes the existence of:
An initial class assignment.
"This can be as sim-ple as the most common class label in the train-ing set, or it can be the output of another clas-."
A set of allowable templates for rules.
These templates determine the types of predicates the rules will test; they have the largest impact on the behavior of the system.
An objective function f for learning.
"Unlike in many other learning algorithms, the objec-tive function for TBL will directly optimize the evaluation function."
A typical example is the  in performance resulting from apply-ing the rule: f ( r ) = good ( r ) bad ( r ) where good ( r ) = jf s j C [ s ] = 6 T [ s ] ^
C [ r ( s )] =
T [ s ] gj bad ( r ) = jf s j C [ s ] =
T [ s ] ^
C [ r ( s )] 6 = T [ s ] gj
"Since we are not interested in rules that have a nega-tive objective function value, only the rules that have a positive good ( r ) need be examined."
"This leads to the following approach: 1. Generate the rules (using the rule template set) that correct at least an error (i.e. good ( r ) &gt; 0 ), by examining all the incorrect samples ( s s.t. C [ s ] = 6 T [ s ] ); 2. Compute the values bad ( ) for each rule r such that good ( r ) &gt; f ( b ) , storing at each point in time the rule b that has the highest score; while computing bad ( r ) , skip to the next rule when f ( r ) &lt; f ( b )"
"The system thus learns a list of rules in a greedy fashion, according to the objective function."
"When no rule that improves the current state of the train-ing set beyond a pre-set threshold can be found, the training phase ends."
"During the application phase, the evaluation set is initialized with the initial class assignment."
The rules are then applied sequentially to the evaluation set in the order they were learned.
The   is the one attained when all rules have been applied.
"As was described in the introductory section, the long training time of TBL poses a serious prob-lem."
"Various methods have been investigated to-wards ameliorating this problem, and the following subsections detail two of the approaches."
One of the most time-consuming steps in transformation-based learning is the updating step.
"The iterative nature of the algorithm requires that each newly selected rule be applied to the corpus, and the current state of the corpus updated before the next rule is learned."
Ramshaw &amp;[REF_CITE]attempted to reduce the training time of the algorithm by making the up-date process more .
"Their method requires each rule to store a list of pointers to samples that it applies to, and for each sample to keep a list of pointers to rules that apply to it."
"Given these two sets of lists, the system can then easily: 1. identify the positions where the best rule applies in the corpus; and 2. update the scores of all the rules which are af-fected by a state change in the corpus."
"These two processes are performed multiple times during the update process, and the  re-sults in a  reduction in running time."
The disadvantage of this method consists in the system having an unrealistically high memory re-quirement.
"For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration."
"The additional memory space required to store the lists of pointers associ-ated with these rules is about 450 MB, which is a rather large requirement to add to a system. [Footnote_1]"
"1 We need to note that the 200k-word corpus used in this experiment is considered small by NLP standards. Many of the available corpora contain over 1 million words. As the size of the corpus increases, so does the number of rules and the additional memory space required."
The ICA system[REF_CITE]aims to reduce the training time by introducing independence assump-tions on the training samples that dramatically re-duce the training time with the possible downside of  performance.
"To achieve the speedup, the ICA system disallows any interaction between the learned rules, by enforc-ing the following two assumptions:"
Sample Independence a state change in a sample (e.g. a change in the current part-of-speech tag of a word) does not change the context of surrounding samples.
"This is cer-tainly the case in tasks such as prepositional phrase attachment, where samples are mutually independent."
"Even for tasks such as part-of-speech tagging where intuition suggests it does not hold, it may still be a reasonable assump-tion to make if the rules apply infrequently and sparsely enough."
Rule Commitment there will be at most one state change per sample.
"In other words, at most one rule is allowed to apply to each sample."
"This mode of application is similar to that of a decision list[REF_CITE], where an sample is  by the  rule that applies to it, and not  again thereafter."
"In general, this assumption will hold for problems which have high initial accuracy and where state changes are infrequent."
"The ICA system was designed and tested on the task of part-of-speech tagging, achieving an impres-sive reduction in training time while  only a small decrease in accuracy."
The experiments pre-sented in Section 4 include ICA in the training time and performance comparisons 2 .
The -TBL sys-tem described[REF_CITE]attempts to cut down on training time with a more  Prolog imple-mentation and an implementation of  learning.
"The application of a transformation-based learning can be considerably sped-up if the rules are compiled in a  transducer, as described[REF_CITE]."
"The approach presented here builds on the same foundation as the one[REF_CITE]: instead of regenerating the rules each time, they are stored into memory, together with the two values good ( r ) and bad ( r ) ."
The following notations will be used throughout this section:
G ( r ) = f s 2
"Sj pr ( s ) = true and C [ s ] = 6 tr and tr = T [ s ] g the samples on which the rule applies and changes them to the correct ; therefore, good ( r ) = j G ( r ) j ."
B ( r ) = f s 2
Sj pr ( s ) = true and C [ s ] = 6 tr and
C [ s ] =
"T [ s ] g the samples on which the rule applies and changes the  from correct to incorrect; similarly, bad ( r ) = j B ( r ) j ."
"Given a newly learned rule b that is to be applied to S , the goal is to identify the rules r for which at least one of the sets G ( r ) ; B ( r ) is  by the application of rule b ."
"Obviously, if both sets are not  when applying rule b , then the value of the objective function for rule r remains unchanged."
"The presentation is complicated by the fact that, in many NLP tasks, the samples are not indepen-dent."
"For instance, in POS tagging, a sample is de-pendent on the  of the preceding and succeeding 2 samples (this assumes that there ex-ists a natural ordering of the samples in S )."
"Let V ( s ) denote the  of a sample the set of samples on whose  the sample s might depend on (for consistency, s 2 V ( s ) ); if samples are independent, then V ( s ) = f s g ."
Let s be a sample on which the best rule b applies (i.e. [ b ( s )] = 6 C [ s ] ).
We need to identify the rules r that are  by the change s ! b ( s ) .
Let r be such a rule. f ( r ) needs to be updated if and only if there exists at least one sample s 0 such that s 0 2 G ( r ) and b ( s ) 2 = G ( r ) or 0 (1) s 0 2 B ( r ) and b ( s ) 2 = B ( r ) or 0 ([Footnote_2]) s 0 2 = G ( r ) and b ( s ) 2 G ( r ) or 0 (3) s 0 2 = B ( r ) and b ( s ) 2 B ( r ) 0 (4)
"2 The algorithm was implemented by the the authors, fol-lowing the description[REF_CITE]."
Each of the above conditions corresponds to a spe- update of the good ( r ) or bad ( r ) counts.
"We will discuss how rules which should get their good or bad counts decremented (subcases (1) and (2)) can be generated, the other two being derived in a very similar fashion."
"The key observation behind the proposed algo-rithm is: when investigating the  of applying the rule b to sample s , only samples s 0 in the set V ( s ) need to be checked [."
Any sample s 0 that is not in the set
V ( s ) f s j b changes s g can be ignored since s 0 = b ( s ) . 0
Let s 0 2 V ( s ) be a sample in the vicinity of s .
There are 2 cases to be examined one in which b applies to s 0 and one in which b does not:
Case I : c ( s ) = c ( b ( s )) ( b does not modify the 0 0  of sample s 0 ).
We note that the condition s 0 2 G ( r ) and b ( s ) 2 = G ( r ) 0 is equivalent to prt ( rs =) T = [ trues ] ^^ prC ( b [ s ( s ] =) 6 ) t = r ^ false 0 0 0 (5) 0 and the formula s 0 2 B ( r ) and b ( s ) 2 = B ( r ) 0 is equivalent to prC ( s [ s )]== trueT [ s ^ ] ^
"Cp [ sr (] b = 6 ( st ) r ) ^ = false 0 0 0 (6) 0 0 (for the full details of the derivation, inferred from the  of G ( r ) and B ( r ) , please refer[REF_CITE])."
These formulae  us a method of generating the rules r which are  by the  s 0 ! b ( s ) : 0 1. Generate all predicates p (using the predicate templates) that are true on the sample s 0 . 2.
"If C [ s ] = 6 T [ s ] then 0 0 (a) If p ( b ( s )) = false then decrease good ( r ) , 0 where r is the rule created with predicate p s.t. target T [ s ] ; 0 3."
Else (a) If p ( b ( s )) = false then for all the rules 0 r whose predicate is p [Footnote_3] and tr = 6 C [ s ] de- 0 crease bad ( r ) ;
"3 This can be done with an appropriate data structure - for example, using a double hash."
"The algorithm for generating the rules r that need their good counts (formula (3)) or bad counts (for-mula (4)) increased can be obtained from the formu-lae (1) (respectively (2)), by switching the states s 0 and b ( s ) 0 , and making sure to add all the new pos-sible rules that might be generated (only for (3))."
Case II : C [ s ] = 6 C [ b ( s )] 0 ( b does change the clas- 0  of sample s 0 ).
"In this case, the formula (5) is transformed into: pr ( s ) = ( truepr ( b ^ ( s )) = false _ tr = C [ b ( s )])"
"C [ s ] = 6 tr ^ tr = T [ s ] ^ 0 0 0 0 0 (7) (again, the full derivation is presented[REF_CITE])."
"The case of (2), however, is much simpler."
"It is easy to notice that C [ s ] = 6 C [ b ( s )] 0 0 and s 0 2 B ( r ) implies that b ( s ) 2 = B ( r ) ; indeed, 0 a necessary condition for a sample s 0 to be in a set B ( r ) is that s is  correctly,"
C [ s ] =
T [ s ] . 0 0 0
"Since T [ s ] = 6 C [ b ( s )] 0 , results C [ b ( s )] = 6 T [ s ] 0 0 0 and therefore b ( s ) 2 = B ( r ) ."
"Condition (3) is, therefore, 0 equivalent to pr ( s ) = true ^"
C [ s ] = 6 tr ^ C [ s ] =
T [ s ] 0 0 0 0 (8) The algorithm is  by replacing the test p ( b ( s )) = false with the test pr ( b ( s )) = false _
C [ b ( s )] = tr in formula (1) and removing the test altogether for case of (2).
The formulae used to gen-erate rules r that might have their counts increased (equations (3) and (4)) are obtained in the same fashion as in Case I.
"At every point in the algorithm, we assumed that all the rules that have at least some positive outcome ( good ( r ) &gt; 0 ) are stored, and their score computed."
"Therefore, at the beginning of the algorithm, all the rules that correct at least one wrong  need to be generated."
"The bad counts for these rules are then computed by generation as well: in every position that has the correct , the rules that change the  are generated, as in Case 4, and their bad counts are incremented."
The entire FastTBL algorithm is presented in Figure 1.
"Note that, when the bad counts are computed, only rules that already have positive good counts are se-lected for evaluation."
This prevents the generation of useless rules and saves computational time.
The number of examined rules is kept close to the minimum.
"Because of the way the rules are gen-erated, most of them need to modify either one of their counts."
Some additional space (besides the one needed to represent the rules) is necessary for repre-senting the rules in a predicate hash in order to have a straightforward access to all rules that have a given predicate; this amount is considerably smaller than the one used to represent the rules.
"For exam-ple, in the case of text chunking task described in section 4, only approximately 30Mb additional mem-ory is required, while the approach[REF_CITE]would require approximately 450Mb."
"As mentioned before, the original algorithm has a number of  that cause it to run slowly."
Among them is the drastic slowdown in rule learning as the scores of the rules decrease.
"When the best rule has a high score, which places it outside the tail of the score distribution, the rules in the tail will be skipped when the bad counts are calculated, since their good counts are small enough to cause them to be discarded."
"However, when the best rule is in the tail, many other rules with similar scores can no longer be discarded and their bad counts need to be computed, leading to a progressively longer running time per iteration."
"Our algorithm does not  from the same prob-lem, because the counts are updated (rather than recomputed) at each iteration, and only for the sam-ples that were  by the application of the lat-est rule learned."
"Since the number of  sam-ples decreases as learning progresses, our algorithm actually speeds up considerably towards the end of the training phase."
"Considering that the number of low-score rules is a considerably higher than the number of high-score rules, this leads to a dramatic reduction in the overall running time."
This has repercussions on the scalability of the al-gorithm relative to training data size.
"Since enlarg-ing the training data size results in a longer score dis-tribution tail, our algorithm is expected to achieve an even more substantial relative running time im-provement over the original algorithm."
Section 4 presents experimental results that validate the su-perior scalability of the FastTBL algorithm.
"Since the goal of this paper is to compare and con-trast system training time and performance, extra measures were taken to ensure fairness in the com-parisons."
"To minimize implementation , all the code was written in C++ and classes were shared among the systems whenever possible."
"For each task, the same training set was provided to each system, and the set of possible rule templates was kept the same."
"Furthermore, extra care was taken to run all comparable experiments on the same ma-chine and under the same memory and processor load conditions."
"To provide a broad comparison between the sys-tems, three NLP tasks with  properties were chosen as the experimental domains."
"The  task, part-of-speech tagging, is one where the commitment assumption seems intuitively valid and the samples are not independent."
"The second task, prepositional phrase attachment, has examples which are independent from each other."
"The last task is text chunking, where both independence and commitment assumptions do not seem to be valid."
"A more detailed description of each task, data and the system parameters are presented in the following subsections."
Four algorithms are compared during the follow-ing experiments:
"The regular TBL, as described in section 2;"
"An improved version of TBL, which makes ex-tensive use of indexes to speed up the rules&apos; up-date;"
The FastTBL algorithm;
The ICA algorithm[REF_CITE].
The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech.
"A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches."
"The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used[REF_CITE]."
The training set contained approximately 1M words and the test set approximately 200k words.
Table 1 presents the results of the experiment [Footnote_4] .
4 The time shown is the combined running time for both the lexical tagger and the contextual tagger.
All the algorithms were trained until a rule with a score of 2 was reached.
"The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster."
"The two assumptions made by the ICA algorithm result in considerably less training time, but the performance is also degraded (the  in performance is sta-tistically , as determined by a signed test, at a  level of 0 : 001 )."
Also present in Ta-ble 1 are the results of training
Brill&apos;s tagger on the same data.
The results of this tagger are presented to provide a performance comparison with a widely used tagger.
Also worth mentioning is that the tag-ger achieved an accuracy of 96 : 76% when trained on the entire data [Footnote_5] ; a Maximum Entropy tagger[REF_CITE]achieves 96 : 83% accuracy with the same training data/test data.
5 We followed the setup from Brill&apos;s tagger: the contextual tagger is trained only on half of the training data. The train-ing time on the entire data was approximately 51 minutes.
Prepositional phrase attachment is the task of decid-ing the point of attachment for a given prepositional phrase (PP).
"As an example, consider the following two sentences: 1."
I washed the shirt with soap and water. 2.
I washed the shirt with pockets.
"In Sentence 1, the PP  soap and  de-scribes the act of washing the shirt."
"In Sentence 2, however, the PP   is a description for the shirt that was washed."
Most previous work has concentrated on situa-tions which are of the form VP NP1 P NP2 .
"The problem is cast as a  task, and the sen-tence is reduced to a 4-tuple containing the preposi-tion and the  base forms of the head words of the verb phrase VP and the two noun phrases NP1 and NP2 ."
"For example, the tuple cor-responding to the two above sentences would be: 1. wash shirt with soap 2. wash shirt with pocket"
"Many approaches to solving this this problem have been proposed, most of them using standard ma-chine learning techniques, including transformation-based learning, decision trees, maximum entropy and  estimation."
The transformation-based learning system was originally developed[REF_CITE].
"The data used in the experiment consists of ap-proximately 13,000 quadruples ( VP NP1 P NP2 ) extracted from Penn Treebank parses."
"The set is split into a test set of 500 samples and a training set of 12,500 samples."
The templates used to generate rules are similar to the ones used[REF_CITE]and some include WordNet features.
All the systems were trained until no more rules could be learned.
Table 2 shows the results of the experiments.
"Again, the ICA algorithm learns the rules very fast, but has a slightly lower performance than the other two TBL systems."
"Since the samples are inherently independent, there is no performance loss because of the independence assumption; therefore the per-formance penalty has to come from the commitment assumption."
"The Fast TBL algorithm runs, again, in a order of magnitude faster than the original TBL while preserving the performance; the time ratio is only 13 in this case due to the small training size (only 13000 samples)."
"Text chunking is a subproblem of syntactic pars-ing, or sentence diagramming."
Syntactic parsing at-tempts to construct a parse tree from a sentence by identifying all phrasal constituents and their attach-ment points.
"Text chunking  the task by dividing the sentence into non-overlapping phrases, where each word belongs to the lowest phrasal con-stituent that dominates it."
The following exam-ple shows a sentence with text chunks and part-of-speech tags: [NP A.P. NNP Green NNP ] [ADVP currently RB ] [VP has ] [[REF_CITE]098 CD shares NNS ] [ADJP outstanding JJ ] .
The problem can be transformed into a  task.
"Following Ramshaw &amp; Marcus&apos; (1999) work in base noun phrase chunking, each word is assigned a chunk tag corresponding to the phrase to which it belongs ."
The following table shows the above sentence with the assigned chunk tags:
Word POS tag Chunk Tag A.P. NNP B-NP Green NNP I-NP currently RB B-ADVP has VBZ B-[REF_CITE]098 CD B-NP shares NNS I-NP outstanding JJ B-ADJP . .
The data used in this experiment is the[REF_CITE]phrase chunking corpus (Tjong[REF_CITE]).
The training corpus consists of sections 15-18 of the Penn Treebank[REF_CITE]; section 20 was used as the test set.
"The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by Brill&apos;s tagger[REF_CITE]."
All the systems are trained to completion (until all the rules are learned).
Table 3 shows the results of the text chunking ex-periments.
"The performance of the FastTBL algo-rithm is the same as of regular TBL&apos;s, and runs in an order of magnitude faster."
"The ICA algorithm again runs considerably faster, but at a cost of a cant performance hit."
There are at least 2 reasons that contribute to this behavior: 1.
"The initial state has a lower performance than the one in tagging; therefore the independence assumption might not hold. 25% of the samples are changed by at least one rule, as opposed to POS tagging, where only 2.5% of the samples are changed by a rule. 2."
The commitment assumption might also not hold.
"For this task, 20% of the samples that were  by a rule are also changed again by another one."
A question usually asked about a machine learning algorithm is how well it adapts to larger amounts of training data.
"Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data."
The experiment was performed with the part-of-speech data set.
The four algorithms were trained on training sets of  sizes; training times were recorded and averaged over 4 trials.
The results are presented in Figure 2(a).
"It is obvious that the Fast TBL algorithm is much more scalable than the reg-ular TBL displaying a linear dependency on the amount of training data, while the regular TBL has an almost quadratic dependency."
The explanation for this behavior has been given in Section 3.3.
"Figure 2(b) shows the time spent at each iteration versus the iteration number, for the original TBL and fast TBL systems."
"It can be observed that the time taken per iteration increases dramatically with the iteration number for the regular TBL, while for the FastTBL, the situation is reversed."
"The con-sequence is that, once a certain threshold has been reached, the incremental time needed to train the FastTBL system to completion is negligible."
We have presented in this paper a new and im-proved method of computing the objective function for transformation-based learning.
"This method al-lows a transformation-based algorithm to train an observed 13 to 139 times faster than the original one, while preserving the  performance of the algorithm."
"The method was tested in three ent domains, each one having  characteris-tics: part-of-speech tagging, prepositional phrase at-tachment and text chunking."
"The results obtained indicate that the algorithmic improvement gener-ated by our method is not linked to a particular task, but extends to any  task where transformation-based learning can be applied."
"Fur-thermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks."
"The increased speed of the Fast TBL algorithm also enables its usage in higher level machine learn-ing algorithms, such as adaptive boosting, model combination and active learning."
"Recent work[REF_CITE]has shown how a TBL frame-work can be adapted to generate  on the output, and our algorithm is compatible with that framework."
"The stability, resistance to overtraining, the existence of probability estimates and, now, rea-sonable speed make TBL an excellent candidate for solving  tasks in general."
"The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anony-mous reviewers for useful suggestions, observations and connections with other published material."
The work presented here was supported by NSF grants[REF_CITE].
We present two methods for learning the struc-ture of personal names from unlabeled data.
"The first simply uses a few implicit constraints governing this structure to gain a toehold on the problem — e.g., descriptors come before first names, which come before middle names, etc."
The second model also uses possible coreference information.
We found that coreference con-straints on names improve the performance of the model from 92.6% to 97.0%.
"We are in-terested in this problem in its own right, but also as a possible way to improve named entity recognition (by recognizing the structure of dif-ferent kinds of names) and as a way to improve noun-phrase coreference determination."
We present two methods for the unsupervised learning of the structure of personal names as found in Wall Street Journal text.
"More specif-ically, we consider a “name” to be a sequence of proper nouns from a single noun-phrase (as in-dicated by Penn treebank-style parse trees)."
"For example, “Defense Secretary George W. Smith” would be a name and we would analyze it into the components “Defense Secretary” (a descrip-tor), “George” (a first name), “W.” (a middle name, we do not distinguish between initials and “true” names), and “Smith” (a last name)."
We consider two unsupervised models for learning this information.
"The first simply uses a few implicit constraints governing this struc-ture to gain a toehold on the problem — e.g., descriptors come before first names, which come before middle names, etc."
We henceforth call this the “name” model.
The second model also uses possible coreference information.
"Typically the same individual is mentioned several times in the same article (e.g., we might later en-counter “Mr. Smith”), and the pattern of such references, and the mutual constraints among them, could very well help our unsupervised methods determine the correct structure."
We call this the “coreference” model.
We were at-tracted to this second model as it might offer a small example of how semantic information like coreference could help in learning structural in-formation.
To the best of our knowledge there has not been any previous work on learning personal structure.
"We are aware of one previous case of unsupervised learning of lexical information from possible coreference, namely that of Ge et. al. [5] where possible pronoun coreference was used to learn the gender of nouns."
In this case a program with an approximately 65% accuracy in determining the correct antecedent was used to collect information on pronouns and their possible antecedents.
The gender of the pro-noun was then used to suggest the gender of the noun-phrase that was proposed as the an-tecedent.
"The current work is quite different in both goal and methods, but similar in spirit."
"More generally this work is part of a growing body of work on learning language-related in-formation from unlabeled corpora [1,2,3,8,9,10, 11]."
We assume that people’s names have six (op-tional) components as exemplified in the follow-ing somewhat contrived example:
"Our models make the following assumptions about personal names: • all words of label l (the label number) must occur before all words of label l + 1 • with the exception of descriptors, a maxi-mum of one word may appear for each label • every name must include either a first name or a last name • in a loose sense, honorifics and closes are “closed classes”, even if we do not know which words are in the classes."
"We im-plement this by requiring that words given these labels must appear in our dictionary only as proper names, and that they must have appeared at least three times in the training set used for the lexicon (sections 2-21 of the Penn Wall Street Journal tree-bank)"
Section 5 discusses these constraints and as-sumptions.
The input to the name model is a noisy list of personal names.
"This list is approximately 85% correct; that is, about 15% of the word sequences are not personal names, but rather non-names, or the names of other types of en-tities."
We obtained these names by running a program inspired by that of Collins and Singer [4] for unsupervised learning of named entity recognition.
This program takes as input pos-sible names plus contextual information about their occurrences.
"It then categorizes each name as one of person, place, or organization."
A possi-ble name is considered to be a sequence of one or more proper nouns immediately dominated by a noun-phrase where the last of the proper nouns is the head (rightmost noun) of the noun phrase.
We used as input to this program the parsed text found in the[REF_CITE]-89 WSJ Corpus — Release 1 [6].
"Because of a mi-nor error, the parser used in producing this cor-pus had a unwarranted propensity to label un-capitalized words as proper nouns."
To correct for this we only allowed capitalized words to be considered proper nouns.
In section 5 we note an unintended consequence of this decision.
The coreference model for our tasks is also given a list of all personal names (as de-fined above) in each Wall Street Journal arti-cle.
"Although the BLLIP corpus has machine-generated coreference markers, these are ig-nored."
"The output of both programs is an assign-ment from each name to a sequence of labels, one for each word in the name."
Performance is measured by the percent of words labeled cor-rectly and percent of names for which all of the labels are correct.
We now consider the probability models that underlie our learning mechanisms.
Both models are generative in that they assign probabilities to all possible labelings of the names. (For the coreference model the model generates all pos-sible labelings given the proposed antecedent.)
Let ~l be a sequence of label assignments to the name ~n (a sequence of words).
"For the name model we estimate arg max l~ p(~l | ~n) = arg max ~l p(~l, n~) (1)"
"We estimate this latter probability by assum-ing that the number of words assigned label l, n(l), is independent of which other labels have appeared."
"Our assumptions imply that with the exception of descriptor, all labels may occur zero or one times."
We arbitrarily assume that there may be zero to fourteen descriptors.
We then assume that the words in the name are inde-pendent of one another given their label.
"Thus we get the following equation: p(~l, n~) ="
Y p(N(l) = n(l))
"Y p(w(i) | l) l=0,5 i=0,n(l) (2)"
"Here w(i) is the ith word from ~n assigned the label l in ~l and N (l) is a random variable whose value is the number of words in the name with label l. To put this slightly differently, we first guess the number of words with each label l according to the distribution p(N(l) = n(l))."
"Given the ordering constraints, this completely determines which words in ~n get which label."
We then guess each of the words according to the distribution p(w(i) | l).
"The name model does not use information concerning how often each name occurs. (That is, it implicitly as-sumes that all names occur equally often.)"
"We have also considered somewhat more com-plex approximations to p(~l,n~)."
See section 5.
The coreference model is more complicated.
"Here we estimate arg max l~ p(~l | n~, c~) = arg max ~l p(~l, ~n | c~) (3)"
"That is, for each name the program identifies zero or one possible antecedent name."
It does this using a very crude filter.
"The last word of the proposed antecedent (unless that word is “Jr.”, in which case it looks at the second to last word) must also appear in n~ as well."
"If no such name exists, then c~ = 4 and we estimate the distribution according to equation 2."
"If more than one such name exists, we choose the first one appearing in the article."
"Even if there is such a name, the program does not assume that the two names are, in fact, coreferent."
"Rather, a hidden random variable R determines how the two names relate."
"There are three possibilities: • ~c is not coreferent (R = 4), in which case the probability is estimated according to the c~ = 4 case. • c~ is not coreferent but is a member of the same family as ~n (e.g., “John Sebas-tian Bach” and “Carl Philipp Emmanuel Bach”)."
"This case (R = f) is computed as the non-coreference case, but the sec-ond occurrence is given “credit” for the last name. • ~c is coreferent to ~n, in which case we com-pute the probability as described below."
"In this case we assume that any words shared by the two names must appear with the same label, and except for descriptions, la-bels may not change between them (e.g., if c~ has a first name, then ~n can be given a first name only if it is the same word as that in ~c)."
"This does not allow for nick-names and other such cases, but they are rare in the Wall Street Journal."
"More formally, we have p(~n,~l | ~c) = X p(R)p(n~,~l | R,~c) (4) R"
"We then estimate p(~n,~l | R,~c) as follows: • if R = 4, compute p(n~,~l) as estimated from equation 2. • if R = f, then p(n~,~l)/p(s | ~l(s)) where s is the word in common that caused the previ-ous name to be selected as a possible coref-erent and ~l(s) is the label assigned to s ac-cording to ~l. • if R = ~c, use equation 8 below."
In equation 4 the R = 4 case is reasonably straight-forward: we simply use equation 2 as the non-coreferent distribution.
"For R = f, as we noted earlier, we want to claim that the new name is a member of the same family as that of the earlier name."
"Thus, as we said earlier, we get “credit” for the repeated family name."
This is why we take the non-coreferent probability and divide by the probability of what we take to be the family name.
This leaves the coreferent case.
The basic idea is that we view the labeling of new name (~l) as a transformation of the labeling of the old one (~l 0 ).
"However, we do not know l~ 0 so we have to sum over all possible former labelings L 0 ."
"This is expressed as p(~n,~l | c~) ="
"X p(l~ 0 | c~)p(n~,~l | l~ 0 , c~) (5) l ∈L~ 0 0"
"The first term, p(~l 0 | ~c), is easy to compute from equation 2 using Bayes law."
We now turn our attention to the second term.
"To establish a more detailed relationship be-tween the old and new names we compute pos-sible correspondences between the two names, where a correspondence specifies for each word in the old name if it is retained in the new name, and if so, the word in the new name to which it corresponds."
Two words may correspond only if they are the same lexical item. (The converse does not hold.)
"Since in principle there can be multiple correspondences, we introduce the cor-respondences κ by summing the probability over all of them: p(n~,~l | l~ 0 , c~) = X p(~n,~l, κ | ~l 0 , c~) (6) κ ≈ max κ p(n~,~l, κ | ~l 0 ,~c) (7)"
"In the second equation we simplify by making the assumption that the sum will be dominated by one of the correspondences, a very good as-sumption."
"Furthermore, as is intuitively plau-sible, one can identify the maximum κ with-out actually computing the probabilities: it is the κ with the maximum number of words re-tained from ~c."
Henceforth we use κ to denote this maximum-probability correspondence.
"By specifying κ we divide the words of the old name into two groups, those (R) that are retained in the new name and those (S) that are subtracted when going to the new name."
"Simi-larly we have divided the words of the new name into two classes, those retained and those added (A)."
"We then assume that the probability of a word being subtracted or retained is indepen-dent of the word and depends only on its label (e.g., the probability of a subtraction given the label l is p(s | l))."
"Furthermore, we assume that the labels of words in R do not change between ~ 0 l and ~l."
"Once we have pinned down R and S, any words left in ~l must be added."
"However, we do not yet “know” the labels of those, so we need a probability term p(l | a)."
"Lastly, for words that are added in the new name, we need to guess the particular word corresponding to the label type."
"This gives us the following dis-tribution: p(n~,~l, κ | c~,~l 0 ) ="
Y p(s | l~ 0 (w)) w∈S Y p(r | l~ 0 (w)) w∈R Y p(l | a) w∈A p(w | l) (8)
"Taken together, equations 2 — 8 define our probability model."
"From the work on named entity recognition we obtained a list of 145,670 names, of which 87,809 were marked as personal names."
A second pro-gram creates an ordered list of names that ap-pear in each article in the corpus.
"The two files, names and article-name occurrences, are the in-put to our procedures."
"With one exception, all the probabilities re-quired by the two models are initialized with flat distributions — i.e., if a random variable can take n possible values, each value is 1/n. The probabilites so set are: [Footnote_1]. p(N(l) = n(l)) from equation 2 (the prob-ability that label l appears n(l) times), 2. p(w(i) | l) from equation 2 (the probability of generating w(i) given it has label l), 3. p(s | l~ 0 (w)), p(r | ~l 0 (w)), and p(a | ~l(w)) from equation 8, the probabilities that a the label ~l(w) will be subtracted, retained, or added when going from the old name to the new name."
"1 These were the first values we tried and, as they worked satisfactorily, we simply left them alone."
We then used the expectation-maximization (EM) algorithm to re-estimate the values.
We initially decided to run[REF_CITE]iterations as our benchmark.
In practice no change in perfor-mance was observed after about 15 iterations.
"The one exception to the flat probability dis-tribution rule is the probability distribution p(R), the probability of an antecedent being coreferent, a family relation, or non-coreferent."
"This distribution was set at .993, .002, and .005 respectively for the three alternatives and the values were not re-estimated by EM. 1 Figure 1 show some of the probabilities for individual words given the possible labels."
"The result shown in Figure 1 are basically correct, with “Director” having a high proba-bility as a descriptor, (0.0059), “Ms.” having a high probability as honorific (0.058), etc."
"Some of the small non-zero probabilities are due to genuine ambiguity (e.g., Fisher does occur as a first name as well as a last name) but more of it is due to small confusions in particular cases (e.g., “Director” as a last-name, or “John” as descriptor)."
After EM training we evaluated the program on 309 personal names from our names list that we had annotated by hand.
These names were obtained by random selection of names labeled as personal names by the name-entity recog-nizer.
If the named entity recognizer had mis-takenly classified something as a personal name it was not used in our test data.
For the name model we straightforwardly used equation 2 to determine the most probable label sequence ~l for each name.
"Note, however, that the testing data does not itself include any information on whether or not the test name was a first or subsequent occurrence of an indi-vidual in the text."
"To evaluate the coreference model we looked at the possible coreference data to find if the test-data name was most common as a first occurrence, or if not, which possible antecedent was the most common."
"If first occur-rence prevailed, ~l was determined from equation 2, and otherwise it was determined using equa-tion 3 with c~ set to the most common possible coreferent for this name."
We compare the most probable labels ~l for a test example with the hand-labeled test data.
We report percentage of words that are given the correct label and percentage of names that are completely correct.
The results of our ex-periments are as follows:
Model Label% Name%
"As can be seen, information about possible coreference was a decided help in this task, lead-ing to an error reduction of 59% for the number of labels correct and 63% for names correct."
"The errors tend to arise from three situations: the name disobeys the name structure assump-tions upon which the program is based, the name is anomalous in some way, or sparse data."
We consider each of these in turn.
Many of the names we encounter do not obey our assumptions.
"Probably the most common situation is last names that, contrary to our as-sumption, are composed of more than word e.g., “Van Dam”."
"Actually, a detail of our process-ing has caused this case to be under-represented in our data and testing examples."
"As noted in Section 2, uncapitalized proper nouns were not allowed."
"The most common extra last name is probably “van,” but all of these names were ei-ther truncated or ignored because of our pro-cessing step."
"In principle, it should be possible to allow for multiple last names, or alternatively have a new label for “first of two last names”."
"In practice, it is of course the case that the more parameters we give EM to fiddle with, the more mischief it can get into."
"However, for a practical program this is probably the most important extension we envision."
Names may be anomalous while obeying our restrictions at least in the letter if not the spirit.
"Chinese names have something very much like the first-middle-last name structure we assume, but the family name comes first."
"This is partic-ularly relevant for the coreferent model, since it will be the family name that is repeated."
"There is nothing in our model that prevents this, but it is sufficiently rare that the program gets “con-fused”."
"In a similar way, we marked both “Dr.” and “Sir” as honorifics in our test data."
"How-ever, the Wall Street Journal treats them very differently from “Mr.” in that the former tend to be included even in the first mention of a name, while the latter is not."
Thus in some cases our program labeled “Dr.” and “Sir” as descriptors.
"Lastly, there are situations where we imag-ine that if the program had more data (or if the learning mechanisms were somehow “better”) it would get the example right."
"For example, the name “Mikio Suzuki” appears only once in our corpus, as does the word “Mikio”. “Suzuki” appears two times, the first being in “Yotaro Suzuki” who is mentioned earlier in the same article."
"Unfortunately, because “Mikio” does not appear elsewhere, the program is at a loss to decide which label to give it."
"However, be-cause Yotaro is assumed to be a first name, the program makes “Mikio Suzuki” coreferent with “Yotaro Suzuki” by labeling “Mikio” descriptor."
"As noted briefly in section 3, we have consid-ered more complicated probabilities models to replace equation 2."
"The most obvious of these is to allow the distribution over numbers of words for each label to be conditioned on the previ-ous label — e.g., a bi-label model."
"This model generally performed poorly, although the coref-erence versions often performed as well as the coreference model reported here."
Our hypoth-esis is that we are seeing problems similar to those that have bedeviled applying EM to tasks like part-of-speech tagging [7].
In such cases EM typically tries to lower probabilities of the cor-pus by using the tags to encode common word-word combinations.
"As the models correspond-ing to equations 2 and 8 do not include any label-label probabilities, this problem does not appear in these models."
"It is probably clear to most readers that the structure and probabilities learned by these models, particularly the coreferent model, could be used for tasks other than assigning structure to names."
"For starters, we would imagine that a named entity recognition program that used information about name structure could do a better job."
"The named entity recognition pro-gram used to create the input looks at only a few features of the context in which the name appears, the complete name, and the individ-ual words that appear in the name irrespective of the other words."
"Since the different kinds of names (person, company and location) differ in structure from one another, a program that simultaneously establishes both structure and type would have an extra source of information, thus enabling it to do a better job."
Our name-structure coreferent model is also learning a lot of information that would be use- ful for a program whose primary purpose is to detect coreference.
One way to see this is to look at some of the probabilities that the pro-gram learned.
Consider the probability that we will have an honorific in a first occurrence of a name: p(n(1) = 1) = .000044 (9)
This is very low.
Contrast this with the prob-ability that we add an honorific in the second occurrence: p(a | honorific) = 1 (10)
"These dramatic probabilities are not, in fact, accurate, as EM tends to exaggerate the effects by moving words that do not obey the trend out of the honorific category."
"They are, however, in-dicative of the fact that in the Wall Street Jour-nal names are introduced without honorifics, but subsequent occurrences tend to have them (a fact we were not aware of at the start of this research)."
"Another way to suggest the usefulness of this research for coreference and named-entity recognition is to consider the cases where our program’s crude filter suggests a possible an-tecedent, but the probabilistic model of equa-tion 4 rejects this analysis."
"As can be seen, except for “Mr. President” and “President Reagan”, all of the examples are either not coreferent or are not people at all."
We have presented two methods for the un-supervised discovery of personal-name struc-ture.
"The two methods differ in that the second uses multiple, possibly coreferent, occurrences of names to constrain the problem."
The meth-ods perform at a level of 92.6 and 97.0 percent accuracy respectively.
"The methods are of potential interest in their own right, e.g., to improve the level of detail provided by Penn Treebank style parses."
"As we have also noted, we should also be able to use this research in the quest for better unsu-pervised learning of named-entity recognition, and the model that attends to coreference in-formation can potentially be useful for programs aimed directly at this latter problem."
"Finally, many of us believe that the power of unsupervised learning methods for linguistic information will be proportional to the depth of semantic or pragmatic analysis that goes into the features they consider."
"The vastly superior performance of the coreference model over the 4. Collins, M. and Singer, Y. Unsuper- based approach for building semantic lexi-cons."
"In Proceedings of the Second Confer-ence on Empirical Methods in Natural Lan-guage Processing. 1997, 117–124. basic name model moves this believe some small 10."
"Roark, B. and Charniak, E. Noun- distance from hope to hypothesis."
"Berland, M. and Charniak, E. Finding parts in very large corpora."
In Proceedings phrase co-occurrence statistics for semi-automatic semantic lexicon construction.
"Yarowsky, D. Unsupervised word sense 1999, 57–64."
°·µ«-¬®­j¯a°²±®°´³jµ¡­&gt;!&gt;&gt;!¿F¸Wµ&gt;{±®­j¹%ÂW¸Ã¿®­±®°·¿F¼½¸!µ&gt;{!ÄÄ¼ ¿¾Ï¡¹ÐÓ+|ÕWµÚ&gt;¬®³ºµ¡»ºÑ&gt;³Ö)¶´»n¯aÅº»×°·¿Ø±¾°·È%&gt;.#Ö`»ÛÚ&gt;&gt;È°·µ&gt;{±Ü­;ÕWµ&gt;³Ö&gt;)¶´±­ºµ¡¯ È°´µ¡°´È¨­!!µ&gt;µ&gt;³º±®­)!Ó&gt;°À¹ÐÓÊ°À¿&gt;@!¼BÈ°´µ!¼fÕWµ&gt;&gt;³Ö°·µ&gt;)!{µ ±¾°À¹ ¬¾»nÔ{Ï&gt;:¬¾Ï¡¶´»n¿ÂW¸ &gt; ±¾æ¯a°´±¾°·³ºµ¡­º¶¨±®éÑëFìÐíníèzîÐè#|ï¾éÐðºð\ñóòWô ­!¶·Åº³º¬®°´±¾Ó&gt;³Ö)&gt;Èõ­&gt;!Ú&gt;&gt;;Â{¸å­ ö ø)Qã2=&gt;&lt; ebE@ .Ú\³º¬¾±®­ºµ{±u±®­j¿FÕú¼½³j¬0¯a°À¿ÝÄ ±¾|è ñóèzÓ¡&gt;þ °·¬B¬Ð¿F»R³j¬ä¯a°À­­º¶Àµj­±®±¾°·³ºµq¿&apos;!Âqµj±Ð±Ý¿¥ÖD°À¿-Ú&gt;­º¬®µ¡!­Ú&gt;{Ô Ó¡Ï&gt;&gt;³º°À¬®¿F°·°´¹`±¾»&apos;±®³ÞW±¾)±¾Ï¡Ó¡­º»D¶¡&gt;Ï µqµj±®°²¯a±®¬Ð°´»n¿Ý±Ð¿b­!­ºµ¡µ¡¯a¯ Ä °·µ&gt;;Å ³º¼.%«U¬Ð­º¯a°´±¾°·³ºµq­!¶·¶´¸jË# &gt;\&gt;°´µ&gt;Ä °·µ&gt;&gt;ÅjÏ&gt;°·¿F±¾°À¹¨­!.ÕWµ&gt;³Ö)&gt;Åº»³!&gt;ÅºÏq­!Åº»jÑ
"In this paper we investigate polysemous adjectives whose meaning varies depending on the nouns they modify (e.g., fast )."
We acquire the meanings of these adjectives from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpre-tations.
We identify lexical semantic information auto-matically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning.
We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model’s ranking of meanings correlates reliably with hu-man intuitions: meanings that are found highly probable by the model are also rated as plausible by the subjects.
"Much recent work in lexical semantics has been con-cerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to."
"Adjectives, more than other cat-egories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify (see[REF_CITE]and the references therein)."
"The adjective fast in (1) receives different interpre-tations when modifying the nouns programmer , plane and scientist ."
"A fast programmer is typically a program-mer who programs quickly, a fast plane is typically a plane that flies quickly, a fast scientist can be a scien-tist who publishes papers quickly, who performs exper-iments quickly, who observes something quickly, who reasons, thinks, or runs quickly."
"Interestingly, adjectives like fast are ambiguous across and within the nouns they modify."
"A fast plane is not only a plane that flies quickly, but also a plane that lands, takes off, turns, or travels quickly."
Even the more restrictive fast programmer al-lows more than one interpretation.
"One can easily think of a context where a fast programmer thinks, runs or talks quickly. (1) a. fast programmer b. fast plane c. fast scientist (2) a. easy problem b. difficult language c. good cook d. good soup"
Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since[REF_CITE].
The meaning of adjective-noun combinations like those in (1) and (2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.
"For example, an easy problem is “a problem that is easy to solve” or “a problem that one can solve easily”."
"In order to account for the meaning of these combinations Vendler (1968, 92) points out that “in most cases not one verb, but a fam-ily of verbs is needed”."
Vendler further observes that the noun figuring in an adjective-noun combination is usu-ally the subject or object of the paraphrasing verb.
"Al-though fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpre-tations (see (2a,b))."
"An easy problem is usually a prob-lem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write."
Adjec-tives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.
"Pustejovsky treats nouns as having a qualia structure as part of their lexical en-tries, which among other things, specifies possible events associated with the entity."
"For example, the telic (pur-pose) role of the qualia structure for problem has a value equivalent to solve ."
"When the adjective easy is combined with problem , it predicates over the telic role of prob-lem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve."
"Furthermore, in cases where more than one interpretations are pro-vided (see[REF_CITE]), no information is given with respect to the likelihood of these interpretations."
"Out-of context, the number of interpretations for fast scien-tist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly."
In this paper we focus on polysemous adjective-noun combinations (see (1) and (2)) and attempt to address the following questions: (a) Can the meanings of these adjective-noun combinations be acquired automatically from corpora? (b) Can we constrain the number of inter-pretations by providing a ranking on the set of possible meanings? (c) Can we determine if an adjective has a preference for a verb-subject or verb-object interpreta-tion?
We provide a probabilistic model which combines distributional information about how likely it is for any verb to be modified by the adjective in the adjective-noun combination or its corresponding adverb with in-formation about how likely it is for any verb to take the modified noun as its object or subject.
"We obtain quantitative information about verb-adjective modifica-tion and verb-argument relations from the British Na-tional Corpus (BNC), a 100 million word collection of samples of written and spoken language from a wide range of sources designed to represent current British En-glish[REF_CITE]."
We evaluate our results by com-paring the model’s predictions against human judgments and show that the model’s ranking of meanings correlates reliably with human intuitions.
"In order to come up with the meaning of “plane that flies quickly” for fast plane we would like to find in the cor-pus a sentence whose subject is the noun plane or planes and whose main verb is fly , which in turn is modified by the adverbs fast or quickly ."
In the general case we want to paraphrase the meaning of an adjective-noun combi-nation by finding the verbs that take the head noun as their subject or object and are modified by an adverb corresponding to the modifying adjective.
"This can be expressed as the joint probability P(a,n,v,rel) where v is the verbal predicate modified by the adverb a (derived from the adjective present in the adjective-noun combi-nation) bearing the argument relation rel (i.e., subject or object) to the head noun n."
"We rewrite P(a,n,v,rel) using the chain rule in (3). (3) P(a,n,v,rel) = P(v)·P(n|v)·P(a|v,n)·P(rel|v,n,a)"
"Although the parameters P(v) and P(n|v) can be straight-forwardly estimated from the BNC, the estimation of P(rel|v,n,a) and P(a|v,n) is somewhat problematic."
"In order to obtain P(rel|v,n,a) we must estimate the fre-quency f(v,n,a,rel) (see (4)). (4) P(rel|v,n,a) = f(v,n,a,rel) f(v,n,a)"
"One way to acquire f(v,n,a,rel) would be to fully parse the corpus so as to identify the verbs which take the head noun n as their subject or object and are modified by the adverb a."
"Even if we could accurately parse the corpus, it is questionable whether we can find enough data for the estimation of f(v,n,a,rel)."
"There are only six sentences in the entire BNC that can be used to es-timate f(v,n,a,rel) for the adjective-noun combination fast plane (see (5a)–(5f))."
"The interpretations “plane that swoops in fast”, “plane that drops down fast” and “plane that flies fast” are all equally likely, since they are at-tested in the corpus only once."
This is rather counter-intuitive since fast planes are more likely to fly than swoop in fast.
"For the adjective-noun combination fast programmer there is only one sentence relevant for the estimation of f(v,n,a,rel) in which the modifying ad-verbial is not fast but the semantically related quickly (see (6))."
"The sparse data problem carries over to the es-timation of the frequency f(v,n,a). (5) a. Three planes swooped in, fast and low. b."
The plane was dropping down fast towards Bangkok. c.
The unarmed plane flew very fast and very high. d.
The plane went so fast it left its sound behind. e.
And the plane’s going slightly faster than the Hercules or Andover. f. He is driven by his ambition to build a plane that goes faster than the speed of sound. (6) It means that programmers will be able to develop new applications more quickly.
We avoid these estimation problems by reducing the pa-rameter space.
"In particular, we make the following in-dependence assumptions: (7) P(a|v,n) ≈ P(a|v) (8) P(rel|v,n,a) ≈ P(rel|v,n)"
We assume that the likelihood of an adverb modifying a verb is independent of the verb’s arguments (see (7)).
"Accordingly, we assume that knowing that the adverb a modifying the verb v will contribute little information to the likelihood of the relation rel which depends more on the verb and its argument n (see (8))."
"By substituting (7) and (8) into (3), P(a,n,v,rel) can be written as: (9) P(a,n,v,rel) ≈ P(v)·P(n|v)·P(a|v)·P(rel|v,n)"
"We estimate the probabilities P(v), P(n|v), P(a|v), and"
"P(rel|v,n) as follows: f(v) (10) P(v) = ∑ f(v i ) i (11) P(n|v) = f(n,v) f(v) (12) P(a|v) = f(a,v) f(v) (13) P(rel|v,n) = f(rel,v,n) f(v,n)"
"By substituting equations (10)–(13) into (9) and simpli-fying the relevant terms, (9) is rewritten as follows: (14) P(a,n,v,rel) ≈ f(rel,v,n)· f(a,v) f(v)·∑ f(v i ) i"
"Depending on the data (noisy or not) and the task at hand we may choose to estimate the probability P(v,n,a,rel) from reliable corpus frequencies only (e.g., f(a,v) &gt; 1 and f(rel,v,n) &gt; 1)."
"If we know the interpretation pref-erence of a given adjective (i.e., subject or object), we may vary only the term v, keeping the terms n, a and rel constant."
"Alternatively, as we show in Experiment 1 (see Section 3), we may acquire the interpretation preferences automatically by varying both the terms rel and v."
We estimated the parameters described in the previous section from a part-of-speech tagged and lemmatized version of the BNC (100 million words).
The estimation of the terms f(v) and ∑ i f(v i ) (see (14)) reduces to the number of times a given verb is attested in the corpus.
"In order to estimate the terms f(rel,v,n) and f(a,v) the corpus was automatically parsed by Cass[REF_CITE], a robust chunk parser designed for the shallow analysis of noisy text."
We used the parser’s built-in function to ex-tract tuples of verb-subjects and verb-objects (see (15)).
The tuples obtained from the parser’s output are an im-perfect source of information about argument relations.
Bracketing errors as well as errors in identifying chunk categories accurately result in tuples whose lexical items do not stand in a verb-argument relationship.
"For exam-ple, the verb is missing from (16a) and the noun is miss-ing from (16b). (15) a. change situation SUBJ b. come off heroin OBJ c. deal with situati[REF_CITE]a. isolated people SUBJ b. smile good SUBJ"
In order to compile a comprehensive count of verb-argument relations we discarded tuples containing verbs or nouns attested in a verb-argument relationship only once.
"Non-auxiliary instances of the verb be (e.g., OBJ be embassy) were also eliminated since they contribute no semantic information with respect to the events or states that are possibly associated with the noun with which the adjective is combined."
Particle verbs (see (15b)) were retained only if the particle was adjacent to the verb.
Verbs followed by the preposition by and a head noun were considered instances of verb-subject relations.
The verb-object tuples also included prepositional objects (see (15c)).
"It was assumed that PPs adjacent to the verb headed by either of the prepositions in, to, for, with, on, at, from, of, into, through, upon were prepositional objects."
"This resulted in 737,390 distinct types of verb-subject pairs and 1,077,103 distinct types of verb-object pairs."
"Generally speaking, the frequency f(a,v) represents not only a verb modified by an adverb derived from the adjective in question (see (17a)) but also constructions like the ones in (17b,c) where the adjective takes an in-finitival VP complement whose logical subject can be re-alized as a for -PP (see (17c))."
"It is relatively straight-forward to develop an automatic process which maps an adjective to its corresponding adverb, modulo excep-tions and idiosyncrasies."
However in the experiments de-scribed in the following sections this mapping was man-ually specified. (17) a. comfortable chair → a chair on which one sits comfortably b. comfortable chair → a chair that is comfort-able to sit on c. comfortable chair → a chair that is comfort-able for me to sit on
"We estimated the frequency f(a,v) by collapsing the counts from cases where the adjective was followed by an infinitival complement (see (17b,c)) and cases where the verb was modified by the adverb corresponding to the related adjective (see (17a))."
We focused only on in-stances where the verb and the adverbial phrase modify-ing it (AdvP) were adjacent and extracted the verb and the head of the AdvP immediately following or preced-ing it.
From constructions with adjectives immediately followed by infinitival complements with an optionally intervening for -PP (see (17c)) we extracted the adjective and the main verb of the infinitival complement.
In what follows we explain the properties of the model by applying it to a small number of adjective-noun combina-tions taken from the lexical semantics literature.
Table 1 gives the interpretations of eight adjective-noun com-binations discussed[REF_CITE]and[REF_CITE].
"Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v 1 is the most likely interpreta-tion, v 2 is the second most likely interpretation, etc.)."
First notice that our model predicts variation in mean-ing when the same adjective modifies different nouns by providing different interpretations for easy problem and easy planet (see Table 2).
Our model agrees[REF_CITE]in the interpretation of easy problem (see Tables 1 and 2).
"Furthermore, it provides the additional meanings “a problem that is easy to deal with, identify, tackle, and handle”."
"Although the model does not derive Vendler’s interpretation of easy planet , it produces complementary meanings such as “a planet that is easy to predict, iden-tify, plunder, work with”."
"Similarly, although the model does not discover the suggested interpretation for good umbrella it comes up with the plausible meaning “an um-brella that covers well”."
"In fact the latter can be consid-ered as a subtype of the meaning suggested[REF_CITE]: an umbrella functions well if it opens well, closes well, covers well, etc."
"Although Pustejovsky suggests only a subject-related interpretation for good umbrella , the model also derives plausible object-related interpretations: “an umbrella that is good to keep, good for waving, good to hold, good to run for, good to leave”."
"The model additionally acquires the fairly plausible mean-ings “a shoe that is good to keep, to buy, and get” for good shoe and “a horse that learns, goes, comes and rises fast” for fast horse ."
The model’s interpretations for dif-ficult language are a superset of the meanings suggested by Vendler (see Table 1).
The model’s interpretations for careful scientist seem intuitively plausible (even though they don’t overlap with those suggested by Vendler).
"Fi-nally, note that the meanings derived for comfortable chair are also plausible (the second most likely meaning is the one suggested by Vendler, see Table 1)."
The examples in Table 1 may not be entirely represen-tative of the types of polysemous adjective-noun combi-nations occurring in unrestricted text since they are taken from linguistic texts where emphasis is given on explain-ing polysemy with examples that straightforwardly illus-trate it.
"In other words, the adjective-noun combinations in Table 1 may be too easy for the model to handle."
"In Experiment 1 (see Section 3) we test our model on poly-semous adjective-noun combinations randomly sampled from the BNC, and formally evaluate our results against human judgments."
The ideal test of the proposed model of adjective-noun polysemy will be with randomly chosen materi-als.
We evaluate the acquired meanings by comparing the model’s rankings against judgments of meaning para- phrases elicited experimentally from human subjects.
"By comparing the model-derived meanings against human intuitions we are able to explore: (a) whether plausi-ble meanings are ranked higher than implausible ones; (b) whether the model can be used to derive the argu-ment preferences for a given adjective, i.e., whether the adjective is biased towards a subject or object interpre-tation or whether it is equi-biased; (c) whether there is a linear relationship between the model-derived likelihood of a given meaning and its perceived plausibility, using correlation analysis."
We chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns ran-domly selected from the BNC.
We chose the adjectives as follows: we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature[REF_CITE].
"From these we ran-domly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong )."
"These adjectives had to be unambiguous with respect to their part-of-speech: each adjective was unambiguously tagged as “adjective” 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC."
"We identified adjective-noun pairs using Gsearch[REF_CITE], a chart parser which detects syn-tactic patterns in a tagged corpus by exploiting a user-specified context free grammar and a syntactic query."
Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.
From the syntactic analysis provided by the parser we extracted a table containing the adjective and the head of the noun phrase following it.
"In the case of compound nouns, we only included sequences of two nouns, and considered the rightmost occurring noun as the head."
We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.
"We employed no threshold on the frequencies f(a,v) and f(rel,v,n)."
"In order to obtain the frequency f(a,v) the adjective was mapped to its corresponding adverb."
"In par-ticular, good was mapped to good and well , fast to fast , easy to easily , hard to hard , right to rightly and right , safe to safely and safe , slow to slowly and slow and wrong to wrongly and wrong ."
"The adverbial function of the adjective difficult is expressed only periphrastically (i.e., in a difficult manner, with difficulty)."
"As a result, the frequency f(difficult,v) was estimated only on the basis of infinitival constructions (see (17))."
"We estimated the probability P(a,n,v,rel) for each adjective-noun pair by varying both the terms v and rel."
"In order to generate stimuli covering a wide range of model-derived paraphrases corresponding to differ-ent degrees of likelihood, for each adjective-noun com-bination we divided the set of the derived meanings into three probability “bands” (High, Medium, and Low) of equal size and randomly chose one interpretation from each band."
The division ensured that the experimen-tal stimuli represented the model’s behavior for likely and unlikely paraphrases and enabled us to test the hy-pothesis that likely paraphrases correspond to high rat-ings and unlikely paraphrases correspond to low rat-ings.
"We performed separate divisions for object-related and subject-related paraphrases resulting in a total of six interpretations for each adjective-noun combination, as we wanted to determine whether there are differences in the model’s predictions with respect to the argument function (i.e., object or subject) and also because we wanted to compare experimentally-derived adjective bi-ases against model-derived biases."
Example stimuli (with object-related interpretations only) are shown in Table 3 for each of the nine adjectives.
"Our experimental design consisted of the factors adjective-noun pair (Pair), grammatical function (Func) and probability band (Band)."
The factor[REF_CITE]adjective-noun combinations.
"The factor Func had two levels, subject and object, whereas the factor Band had three levels, High, Medium and Low."
This yielded a to-tal of Pair × Func × Band = 90 × 2 × 3 = 540 stim-uli.
The number of the stimuli was too large for sub-jects to judge in one experimental session.
"We limited the size of the design by selecting a total of 270 stimuli as follows: our initial design created two sets of stimuli, 270 subject-related stimuli and 270 object-related stim-uli."
"For each stimuli set we randomly selected five nouns for each of the nine adjectives together with their cor-responding interpretations in the three probability bands (High, Medium, Low)."
This yielded a total of Pair × Func × Band = 45×2×3= 270 stimuli.
"This way, stim-uli were created for each adjective in both subject-related and object-related interpretations."
We administered the 270 stimuli to two separate sub-ject groups.
Each group saw 135 stimuli consisting of interpretations for all adjectives with both subject-related and object-related interpretations.
Each exper-imental item consisted of an adjective-noun pair and a sentence paraphrasing its meaning.
"Paraphrases were created by the experimenter by converting the model’s output to a simple phrase, usually a noun modified by a relative clause."
A native speaker of English was asked to confirm that the paraphrases were syntactically well-formed.
"The experimental paradigm was Magnitude Estima-tion (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli[REF_CITE], which[REF_CITE]and"
"ME has been shown to provide fine-grained measurements of linguis-tic acceptability which are robust enough to yield statis-tically significant results, while being highly replicable both within and across speakers."
ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion.
"Subjects are first exposed to a modulus item, to which they assign an arbitrary number."
All other stimuli are rated propor-tional to the modulus.
"In this way, each subject can es-tablish their own rating scale, thus yielding maximally fine-grained data and avoiding the known problems with the conventional ordinal scales for linguistic data[REF_CITE]."
"In the present experiment, the subjects were instructed to judge how well a sentence paraphrases an adjective-noun combination proportional to a modulus item."
The experiment was conducted remotely over the Inter-net.
"Subjects accessed the experiment using their web browser, which established an Internet connection to the experimental server running WebExp 2.1[REF_CITE], an interactive software package for administer-ing web-based psychological experiments."
"Subjects first saw a set of instructions that explained the ME tech-nique and included some examples, and had to fill in a short questionnaire including basic demographic infor-mation."
"Each subject group saw 135 experimental stim-uli (i.e., adjective-noun pairs and their paraphrases)."
"Sub-jects were assigned to subject groups at random, and a random stimulus order was generated for each subject."
"The experiment was completed by 60 unpaid volunteers, all native speakers of English."
Subjects were recruited via postings to local Usenet newsgroups.
"As is standard in magnitude estimation studies[REF_CITE], statistical tests were done using geometric means to normalize the data (the geometric mean is the mean of the logarithms of the ratings)."
We first performed an analysis of variance (A NOVA ) to determine whether there is a relation between the para-phrases derived by the model and their perceived likeli-hood.
"In particular, we tested the hypothesis that mean-ings assigned high probabilities by the model are per-ceived as better paraphrases by the subjects and cor-respondingly that meanings with low probabilities are perceived as worse paraphrases."
The descriptive statis-tics for log-transformed model-derived probabilities are shown in Table 4.
"The A NOVA revealed that the Prob-ability Band effect was significant, in both by-subjects and by-items analyses: F 1 (2,118) = 101.46, p &lt; .01; F 2 (2,88) = 29.07, p &lt; .01."
"The geometric mean of the ratings in the High band was −.0005, compared to Medium items at −.1754 and Low items at −.2298 (see Table 5)."
Post-hoc Tukey tests indicated that the differ-ences between all pairs of conditions were significant at α = .01 in the by-subjects analysis.
"The difference be-tween High and Medium items as well as High and Low items was significant at α = .01 in the by-items analysis, whereas the difference between Medium and Low items did not reach significance."
These results show that mean-ing paraphrases derived by the model correspond to hu-man intuitions: paraphrases assigned high probabilities by the model are perceived as better than paraphrases that are assigned low probabilities.
"We further explored the linear relationship between the subjects’ rankings and the corpus-based model, using correlation analysis."
The elicited judgments were com- pared with the interpretation probabilities which were obtained from the model described in Section 2 to exam-ine the extent to which the proposed interpretations cor-relate with human intuitions.
"A comparison between our model and the human judgments yielded a Pearson corre-lation coefficient of .40 (p &lt; .01, N = 270)."
"This verifies the Probability Band effect discovered by the A NOVA , in an analysis which compares the individual interpretation likelihood for each item with elicited interpretation pref-erences, instead of collapsing all the items in three equiv-alence classes (i.e., High, Medium, Low)."
"In order to evaluate whether the grammatical function has any effect on the relationship between the model-derived meanings and the human judgments, we split the items into those that received a subject interpretation versus those that re-ceived an object interpretation."
"A comparison between our model and the human judgments yielded a corre-lation of r = .53 (p &lt; .01, N = 135) for object-related items and a correlation of r = .21 (p &lt; .05, N = 135) for subject-related items."
Note that a weaker correlation is obtained for subject-related interpretations.
"One expla-nation for that could be the parser’s performance, i.e., the parser is better at extracting verb-object tuples than verb-subject tuples."
"Another hypothesis (which we test be-low) is that most adjectives included in the experimental stimuli have an object-bias, and therefore subject-related interpretations are generally less preferred than object-related ones."
An important question is how well humans agree in their paraphrase judgments for adjective-noun combina-tions.
Inter-subject agreement gives an upper bound for the task and allows us to interpret how well the model is doing in relation to humans.
For each subject group we performed correlations on the elicited judgments using leave-one-out resampling[REF_CITE].
"For the first group, the average inter-subject agreement was .67 (Min = .03, Max = .82, SD = .14), and for the second group .65 (Min = .05, Max = .82, SD = .14)."
This means that our model performs satisfactorily given that humans do not perfectly agree in their judgments.
"The elicited judgments can be further used to derive the grammatical function preferences (i.e., subject or ob-ject) for a given adjective."
"In particular, we can determine which is the preferred interpretation for individual adjec-tives and compare these preferences against the ones pro-duced by our model."
Argument preferences can be easily derived from the model’s output by comparing subject-related and object-related paraphrases.
For each adjective we gathered the subject and object-related interpretations derived by the model and performed an A NOVA in order to determine the significance of the Grammatical Func-tion effect.
We interpret a significant effect as bias towards a par-ticular grammatical function.
"We classify an adjective as object-biased if the mean of the judgments for the object interpretation of this particular adjective is larger than the mean for the subject interpretation; subject-biased adjec-tives are classified accordingly, whereas adjectives for which no effect of Grammatical Function is found are classified as equi-biased."
Table 6 shows the biases for the nine adjectives√as derived by our model.
The presence of the symbol indicates significance of the Grammat-ical Function effect as well as the direction of the bias.
Argument preferences were elicited from human sub-jects in a similar fashion.
For each adjective we gathered the elicited responses pertaining to subject- and object-related interpretations and performed an A NOVA .
The bi-ases and√ the significance of the Grammatical Function effect ( ) are shown in Table 6.
"Comparison of the biases derived from the model with ones derived from the elicited judgments shows that the model and the humans are in agreement for all adjec-tives but slow , wrong and safe ."
"On the basis of human judgments slow has a subject bias, whereas wrong has an object bias."
Although the model could not reproduce this result there is a tendency in the right direction (see
Note that in our correlation analysis reported above the elicited judgments were compared against model-derived paraphrases without taking argument preferences into ac-count.
We would expect a correct model to produce intu-itive meanings at least for the interpretation a given ad-jective favors.
We further examined the model’s behav-ior by performing separate correlation analyses for pre-ferred and dispreferred biases as determined previously by the A NOVA s conducted for each adjective.
"Since the adjective good was equi-biased we included both biases (i.e., object-related and subject-related) in both correla-tion analyses."
"The comparison between our model and the human judgments yielded a Pearson correlation co-efficient of .52 (p &lt; .01, N = 150) for the preferred in-terpretations and a correlation of .23 (p &lt; .01, N = 150) for the dispreferred interpretations."
The result indicates that our model is particularly good at deriving meanings corresponding to the argument-bias for a given adjective.
"However, the dispreferred interpretations also correlate significantly with human judgments, which suggests that the model derives plausible interpretations even in cases where the default argument bias is overridden."
The probabilistic model described in Section 2 explic-itly takes adjective/adverb and verb co-occurrences into account.
"However, one could derive meanings for poly-semous adjective-noun combinations by solely concen-trating on verb-noun relations, ignoring thus the adjec-tive/adverb and verb dependencies."
"For example, in or-der to interpret the combination easy problem we could simply take into account the types of activities which are related with problems (i.e., solving them, setting them, etc.)."
"This simplification is consistent with Puste-jovsky’s (1995) claim that polysemous adjectives like easy are predicates, modifying the events associated with the noun."
"A “naive” or “baseline” model would be one which simply takes into account the number of times the noun in the adjective-noun pair acts as the subject or ob-ject of a given verb, ignoring the adjective completely."
Given an adjective-noun combination we are interested in finding the verbs whose object or subject is the noun appearing in the adjective-noun combination.
"This can be simply expressed as P(v|rel,n), the conditional probabil-ity of a verb v given an argument-noun relation rel,n: (18) P(v|rel,n) = f(v,rel,n) f(rel,n)"
The model in (18) assumes that the meaning of an adjective-noun combination is independent of the ad-jective in question.
The model in (18) would come up with the same probabilities for fast plane and wrong plane since it does not take the identity of the modi-fying adjective into account.
"We estimated the frequen- cies f(v,rel,n) and f(rel,n) from verb-object and verb-subject tuples extracted from the BNC using Cass[REF_CITE]."
Using the naive model we calculated the meaning prob-ability for each of the 270 stimuli included in Experi-ment 1.
Through correlation analysis we explored the linear relationship between the elicited judgments and the naive baseline model.
"We further directly compared the two models, our initial, linguistically more informed model, and the naive baseline."
Using correlation analysis we explored which model performs better at deriving meanings for adjective-noun combinations.
"A comparison between the naive model’s probabilities and the human judgments yielded a Pearson correlation coefficient of .25 (p &lt; .01, N = 270)."
"Recall that we obtained a correlation of .40 (p &lt; .01, N = 270) when comparing our original model to the human judg-ments."
"Not surprisingly the two models are intercorre-lated (r = .38, p &lt; .01, N = 270)."
An important question is whether the difference between the two correlation co-efficients (r = .40 and r = .25) is due to chance.
"Compar-ison of the two correlation coefficients revealed that their difference was significant (t(267) = 2.42, p &lt; .01)."
This means that our original model performs reliably better than a naive baseline at deriving interpretations for poly-semous adjective-noun combinations.
We further compared the naive baseline model and the human judgments separately for subject-related and object-related items.
"The comparison yielded a correla-tion of r = .29 (p &lt; .01, N = 135) for object interpreta-tions."
Recall that our original model yielded a correlation coefficient of .53.
"The two correlation coefficients were significantly different (t(132) = 3.03, p &lt; .01)."
"No cor-relation was found for the naive model when compared against elicited subject interpretations (r = .09, p = .28, N = 135)."
In this paper we showed how adjectival meanings can be acquired from a large corpus and provided a probabilis-tic model which derives a preference ordering on the set of possible interpretations.
Our model does not only ac-quire clusters of meanings (following Vendler’s (1968) insight) but furthermore can be used to obtain argument preferences for a given adjective.
We rigorously evaluated the results of our model by eliciting paraphrase judgments from subjects naive to lin-guistic theory.
"Comparison between our model and hu-man judgments yielded a reliable correlation of .40 when the upper bound for the task (i.e., inter-subject agree-ment) is approximately .65."
"Furthermore, our model per-formed reliably better than a naive baseline model, which only achieved a correlation of .25."
"Although adjective-noun polysemy is a well researched phenomenon in the theoretical linguistics literature, the experimental ap-proach advocated here is new to our knowledge."
"Furthermore, the proposed model can be viewed as complementary to linguistic theory: it automatically de-rives a ranking of meanings, thus distinguishing likely from unlikely interpretations."
Even if linguistic theory was able to enumerate all possible interpretations for a given adjective (note that in the case of polysemous ad-jectives we would have to take into account all nouns or noun classes the adjective could possibly modify) it has no means to indicate which ones are likely and which ones are not.
Our model fares well on both tasks.
It recasts the problem of adjective-noun polysemy in a probabilistic framework deriving a large number of in-terpretations not readily available from linguistic intro-spection.
The information acquired from the corpus can be also used to quantify the argument preferences of a given adjective.
These are only implicit in the lexical semantics literature where certain adjectives are exclu-sively given a verb-subject or verb-object interpretation.
We have demonstrated that we can empirically derive ar-gument biases for a given adjective that correspond to human intuitions.
This paper describes a lexicon organized around sys-tematic polysemy : a set of word senses that are related in systematic and predictable ways.
The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut .
"We compare our lexicon to WordNet cousins , and the inter-annotator disagreement ob-served between WordNet Semcor and DSO corpora."
"In recent years, the granularity of word senses for computational lexicons has been discussed fre-quently in Lexical Semantics (for example, (Kilgar-ri , 1998a;[REF_CITE]))."
"This issue emerged as a prominent problem after previous studies and ex-ercises in Word Sense Disambiguation (WSD) re-ported that, when ne-grained sense de nitions such as those in WordNet[REF_CITE]were used, en-tries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarri , 1998b;[REF_CITE])."
"In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Informa-tion Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the di erence in the correct sense assignments a ects re-call, precision and other evaluation measures. proposedIn responsewhichto groupthis, severalne-grainedapproacheswordhavesensesbeenin various ways to derive coarse-grained sense groups."
"Some approaches utilize an abstraction hierarchy de-utilizened insurfacea dictionarysyntactic([REF_CITE]of the), whilefunctionalothers structures (such as predicate-argument structure for verbs) of words[REF_CITE]."
"Also, the current version of WordNet (1.6) encodes groupings of sim-ilar/related word senses (or synsets) by a relation called cousin . utilizeAnothera linguisticapproachphenomenonto grouping wordcalledsenses systematic is to polysemy : a set of word senses that are related in sys- tematic and predictable ways. [Footnote_1]"
"1 Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy[REF_CITE]or logical polysemy 2 Note (th[REF_CITE]olysemy). should be contrasted with homonymy , which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word \bank&quot;)."
"For example, ANIMAL and MEAT meanings of the word \chicken&quot; are re-lated because chicken as meat refers to the esh of a chicken as a bird that is used for food. 2"
"This rela-tion is systematic, since many ANIMAL words such as \duck&quot; and \lamb&quot; have a MEAT meaning."
Another example is the relation QUANTITY-PROCESS observed in nouns such as \increase&quot; and \supply&quot;. lexico-semanticallySense grouping basedmotivatedon systematicin that itpolysemyexpressesis general human knowledge about the relatedness of word meanings.
Such sense groupings have advan-tages compared to other approaches.
"First, related senses of a word often exist simultaneously in a discourse (for example the QUANTITY and PROCESS meanings of \increase&quot; above)."
"Thus, systematic polysemy can be e ectively used in WSD (and WSD evaluation) to accept multiple or alternative sense tags (Buitelaar, personal communication)."
"Second, many systematic relations are observed between senses which belong to di erent semantic categories."
"So if a lexicon is de ned by a collection of sepa-rate trees/hierarchies (such as the case of Word-Net), systematic polysemy can express similarity be-tween senses that are not hierarchically proximate."
"Third, by explicitly representing (inter-)relations be-tween senses, a lexicon based on systematic poly-semy can facilitate semantic inferences."
"Thus it is useful in knowledge-intensive NLP tasks such as dis-course analysis, IE and MT."
"More recently,[REF_CITE]also discusses potential usefulness of sys-tematic polysemy for clustering word senses for IR. largeHoweversense,inventoriesextractingissystematica  taskrelations."
"Mostfromof-ten, this procedure is done manually."
"For example, WordNet cousin relations were identi ed manually by the WordNet lexicographers."
A similar e ort was also made in the EuroWordnet project[REF_CITE].
"The problem is not only that manual inspection of a large, complex lexicon is very time-consumingIn this paper, it is,alsowe pronedescribesto inconsistenciesa lexicon organized. around systematic polysemy."
The lexicon is derived by a fully automatic extraction method which uti-lizes a clustering technique called tree-cut[REF_CITE].
"In our previous work[REF_CITE], we applied this method to a small subset of Word-Net nouns and showed potential applicability."
"In the currentand verbsworkin ,WordNetwe applied, andthebuiltmethoda lexiconto allin nounswhich word senses are partitioned by systematic polysemy."
We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator dis-agreement observed between two semantically an-notated corpora: WordNet Semcor[REF_CITE]and DSO[REF_CITE].
"The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense parti-letta, 1996) than arbitrary sense groupingsvalueson(Car-the tions in our lexicon yielded better agreement data."
Thetechniquetree-cutwhichtechniquepartitionsis andataunsuperviseditems organizedlearningin a tree structure into mutually-disjoint clusters.
"It was originally proposed[REF_CITE], and then adopted in our previous method for automatically extracting systematic polysemy[REF_CITE]."
"In this section, we give a brief summary of this tree-cut technique using examples[REF_CITE]&apos;s original work."
The tree-cut technique is applied to data items that are organized in a structure called a thesaurus tree .
"A thesaurus tree is a hierarchically organized lexicon where leaf nodes encode lexical data (i.e., words) and internal nodes represent abstract semantic classes."
"Aa list tree-cut of internalis a partition/leaf nodesof ainthesaurusthe tree,treeand."
Iteachis node represents a set of all leaf nodes in a subtree rooted by the node.
"Such a set is also considered as a cluster . 3 Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint."
"For instance, Figure 1 shows an example thesaurus tree and one possible tree-cut [ AIRCRAFT , ball, kite, puzzle], which is indicated by a thick curve in the forgurethis. treeThere: [airplaneare also, fourhelicopterother, ballpossible, kitetree-cuts, puzzle], [airplane, helicopter, TOY ], [ AIRCRAFT , TOY ] and [ ARTIFACT ]. wasInapplied(Li andtoAbethe, problem1998), theof tree-cutacquiringtechniquegeneral- ized case frame patterns from a corpus."
"Thus, each node/word in the tree received as its value the num-ber of instances where the word occurred as a case role (subject, object etc.) of a given verb."
"Then the acquisition of a generalized case frame was viewed as a problem of selecting the best tree-cut model that estimates the true probability distribution, given a sample corpus data. of Formallya tree-cut, a; tree-cut and a probability model M isparametera pair consistingvector of the same length,"
"M = (; ; ) (1) where ; and are: ; = [ C 1 ; ::; C k ] ; = [ P ( C 1 ) ; ::; P ( C k )] (2) where C i (1 cut, P ( C i ) is the i probability k ) is a ofclustera clusterin the C i ,tree-and P k P ( C i ) = 1."
"Note that P ( C ) is the prob-ability i =1 of cluster C = f n 1 ;::;n m g as a whole, that is, P ( C ) ="
P mj =1 P ( n j ).
"For example, sup-pose a corpus contains 10 instances of verb-object relation for the verb \ y&quot;, and the frequencies of object nouns n , denoted f ( n ), are as follows: f ( airplane ) = 5 ; f ( helicopter ) = 3 ; f ( ball ) = 0 ; f ( kite ) = 2 ; f ( puzzle ) = 0."
"Then, the set of tree-cut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY ], [.5, .3, .2]) and ([ AIRCRAFT , TOY ], [.8, .2])."
"To select the best tree-cut model,[REF_CITE]uses the Minimal Description Length (MDL) ."
"The MDL is a principle of data compression in Informa-tion Theory which states that, for a given dataset, the best model is the one which requires the min-imum length (often measured in bits) to encode the model (the model description length ) and the data (the data description length )[REF_CITE]."
"Thus, the MDL principle captures the trade-o be-tween the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the good-ness of t to the data, which is measured by the estimationThe calculationaccuracyofofthethe descriptionprobability distributionlength for .a tree-cut model is as follows."
"Given a thesaurus tree T and a sample S consisting of the case frame in-stances, the total description length L ( M;S ) for a tree-cut model M = (; ; ) is"
L ( M; S ) =
L (;) + L ( j ;) +
"L ( S j ; ; ) ([Footnote_3]) where L (;) is the model description length, L ( j ;) shortlythe),parameterand L ( S j ;description ; ) is thelengthdata description(explained is length."
3 A leaf node is also a cluster whose cardinality is 1.
Note that L (;) + L ( j ;) essentially corre-sponds to the usual notion of the model description length.
TheEachmodellengthdescriptionin L ( M; length S ) is calculated L (;) is as follows. [Footnote_4]
"4 For justi cation and detailed explanation of these formu-las, 5 seeIn our(Li andpreviousAbe, work1998),.we used entropy instead of MLE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate."
"L (;) = log 2 j G j (4) where G is the set of all cuts in T , and j G j denotes the size of G ."
"This value is a constant for all mod-els, thus it is omitted in the calculation of the total lengthThe. parameter description length L ( j ;) indi-cates the complexity of the model."
It is the length required to encode the probability distribution of the clusters in the tree-cut ;.
It is calculated as
"L ( j ;) = k 2 log j S j 2 (5) where k is the length of , and j S j is the size of S . theFinallylength, requiredthe datatodescriptionencode thelengthwhole sample L ( S j ; ; data) is."
It is calculated as
"L ( S j ; ; ) = ; X log 2 P ( n ) (6) n 2 S where, for each n 2 C and each C 2 ;,"
P ( n ) = P j ( CC j ) and P ( C ) = f j ( SC j ) (7)
Note the equation (7) essentially computes the Max-imum A table Likelihood in Figure Estimate 1 shows (MLE the MDL ) for lengthsall n . for all 5 theve tree-cuttree-cut [models AIRCRAFT .
"The, bestball,modelkite, puzzleis the].one with"
"Using the tree-cut technique described above, our previous work[REF_CITE]extracted systematic polysemysummary offromthisWordNetmethod., Inandthisdescribesection,theweclustergive a pairs obtained by the method."
"In our previous work, systematically related word senses are derived as binary cluster pairs, by apply-ing the extraction procedure to a combination of two WordNet (sub)trees."
This process is done in the fol-lowing three steps.
"In the rst step, all leaf nodes of the two trees are assigned a value of either 1, if a node/word appears in both trees, or 0 otherwise. [Footnote_6]"
"6 Prior to this, each WordNet (sub)tree is transformed into a thesaurus tree, since WordNet tree is a graph rather than a tree, and internal nodes as well as leaf nodes carry data. In the transformation, all internal nodes in a WordNet tree are copied 7 Removingas leaf nodesnodes, withand shared0 is alsosubtreeswarrantedaresinceduplicatedwe are. not estimating values for those nodes (as explained in footnote 5)."
"In the second step, the tree-cut technique is applied to each tree separately, and two tree-cuts (or sets of clusters) are obtained."
"To search the best tree-cut for a tree (i.e., the model which requires the mini-mum total description length), a greedy algorithm called Find-MDL described[REF_CITE]is used to speed up the search."
"Finally in the third step, clusters in those two tree-cuts are matched up, and the pairs which have substantial overlap (more than three overlapping words) are selected as sys-tematicFigurepolysemies2 shows parts. of the nal tree-cuts for the ARTIFACT and MEASURE classes."
"Note in the gure, bold letters indicate words which are polysemous in the two trees (i.e., assigned a value 1)."
"In the current work, we made a minor modi cation to the extraction method described above, by re-moving nodes that are assigned a value 0 from the trees."
The purpose was to make the tree-cut tech-nique less sensitive to the structure of a tree and produce more speci c clusters de ned at deeper lev-els. 7
The MDL principle inherently penalizes a com-plex tree-cut by assigning a long parameter length.
"Therefore, shorter tree-cuts partitioned at abstract levels are often preferred."
"This causes a problem when the tree is bushy, which is the case with Word-Net trees."
"Indeed, many tree-cut clusters obtained in our previous work were from nodes at depth 1 (counting the root as depth 0) { around 88% (122 out of total 138 clusters) obtained for 5 combinations of WordNet noun trees."
"Note that we did not allow a cluster at the root of a tree; thus, depth 1 is the highest level for any cluster."
"After the modi cation above, the proportion of depth 1 clusters decreased to 49% (169 out of total 343 clusters) for the same tree combinations."
We applied the modi ed method described above to all nouns and verbs in WordNet.
We rst parti-tioned words in the two categories into basic classes .
"A basic class is an abstract semantic concept, and it corresponds to a (sub)tree in the WordNet hier-archies."
Those basic classes exhaustively cover all words in the two categories encoded in Word-Net.
"For example, basic classes for nouns include ARTIFACT , SUBSTANCE and LOCATION , while basic classes for verbs include CHANGE , MOTION and STATE . extractionFor eachmethodpart-of-speechto all combinationscategory, we appliedof two ourba-sic classes."
"Here, a combined class, for instance ARTIFACT-SUBSTANCE , represents an underspeci ed semantic class."
"We obtained 2,377 cluster pairs in 99 underspeci ed classes for nouns, and 1,710 cluster pairs in 59 underspeci ed classes for verbs."
"Table 1 shows a summary of the number of basic and under-speci ed classes and cluster pairs extracted by our method. nationsAlthough, the theaccuracyresults(precisionvary among) of thecategoryderivedcombi-clus-ter pairs was rather low: 50 to 60% on average, based on our manual inspection using around 5% randomly chosen samples. [Footnote_8]"
"8 Note that the relatedness between clusters was deter-mined solely by our subjective judgement. That is because there is no existing large-scale lexicon which encodes related senses completely for all words in the lexicon. (Note that WordNet cousin relation is encoded only for some words). Although the distinction between related vs. unrelated mean-ings is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example,[REF_CITE]). A comparison with WordNet 9 Actuallycousin, cousinis discussedis one ofinthethe nextthreesectionrelations4. which in-dicate the grouping of related senses of a word. Others are sister and twin . In this paper, we use cousin to refer to all relations listed in \cousin.tps&quot; le (available in a WordNet distribution)."
This means our automatic method over-generates possible relations.
"We speculate that this is because in general, there are many homony-mous relations that are &apos;systematic&apos; in the English language."
"For example, in the ARTIFACT-GROUP Wordsclass, awhichpair [ LUMBER are common, SOCIAL in GROUP the two] wasclustersextractedare. \picket&quot;, \board&quot; and \stock&quot;."
"Since there are enough number of such words (for our purpose), our automatic method could not di erentiate them from true systematic polysemy."
"To test our automatic extraction method, we com-pared the cluster pairs derived by our method to WordNet cousins."
"The cousin relation is relatively new in WordNet, and the coverage is still incom-plete."
Currently a total of 194 unique relations are encoded.
"A cousin relation in WordNet is de ned between two synsets, and it indicates that senses of a word that appear in both of the (sub)trees rooted by those synsets are related. 9"
"The cousins were man- ually identi ed by the WordNet lexicographers. pairsTo tocompareWordNetthecousinsautomatically, we used derivedthe hypernym-cluster hyponym relation in the trees, instead of the number or ratio of the overlapping words."
"This is because the levels at which the cousin relations are de ned di er quite widely, from depth 0 to depth 6, thus the number of polysemous words covered in each cousin relation signi cantly varies."
"Therefore, it was cult to decide on an appropriate threshold value for either criteria."
"Using the hypernym-hyponym relation, we checkedat least,onefor clustereach cousinpair relationthat subsumed, whetherortherewas sub-was sumed by the cousin."
"More speci cally, for a cousin relation de ned between nodes c 1 and c 2 in trees T 1 and T 2 respectively and a cluster pair de ned between nodes r 1 and r 2 in the same trees, we de-cided on the correspondence if c 1 is a hypernym or hyponym of r 1, and c 2 is a hypernym or hyponym r 2 at the same time. cating[REF_CITE]thisoutcriteriaof the, we194obtainedcousin arelationsresult indi-had corresponding cluster pairs."
"This makes the recall ratio 89%, which we consider to be quite high. maticIn additionextractiontomethodthe"
"WordNetdiscoveredcousinsseveral, ourinterest-auto-ing relations."
Table 2 shows some examples.
"Using the extracted cluster pairs, we partitioned word senses for all nouns and verbs in WordNet, and produced a lexicon."
"Recall from the previous section that our cluster pairs are generated for all possible binary combinations of basic classes, thus one sense could appear in more than one cluster pair."
"For ex-ample, Table 3 shows the cluster pairs (and a set of senses covered by each pair, which we call a sense cover ) extracted for the noun \table&quot; (which has 6 senses in WordNet)."
"Also as we have mentioned ear-lier in section accuracy-result, our cluster pairs con-tain many false positives ones."
"For those reasons, we took a conservative approach, by disallowing transi-tivity of cluster pairs. senseTo partitioncover a valuesenseswhichof awewordcall, wea connectedness rst assign each."
It is de ned as follows.
"For a given word w which has n senses, let S be the set of all sense covers generated for w ."
"Let c ij denote the number of sense covers in in S (where c ii = 0 for all 1which sense i ( s i ) and sense j ( s j ) i occurred n ), andtogether d ij = P n c ik + c kj , where k 6 = i , k 6 = j , c ik &gt; 0, c kj &gt; 0, and k = C 1 = C P i;j c ij ."
"A connectedness of a sense cover sc 2 S , denoted CN sc , where sc = ( s l ;::;s m ) (1 l &lt; m n ) is de ned as:"
CN sc = X m X m c ij + d ij (8) i = l j =1
"Intuitively, c ij represents the weight of a direct re-lationrelation, andbetween d ij representsany two sensesthe weight i andof j an."
Theindirectidea behind this connectedness measure is to favor sense covers that have strong intra-relations.
This mea-sure also e ectively takes into account a one-level transitivity in d ij .
"As an example, the connectedness of (2 3 4) is the summation of c 23 ; c 34 ; c 24 ; d 23 ; d 34 and d 24 ."
"Here, c 23 = 4 because sense 2 and 3 co-occur in four sense covers, and c 34 = c 24 = 1."
"Also, d 23 = ( c 24 + c 43 )+( c 25 C + c 53 )+( c 26 + c 63 ) = 2+2+2 = : 429 (omitting cases where either or both c ik 14 and c kj are zero), and similarly d 34 = : 5 and d 24 = : 5."
"Thus,[REF_CITE]= 4 + 1 + 1 + : 429 + : 5+ : 5 = 7 : 429."
"Table 3 shows the connectedness values for all sense covers for \table&quot;. non-overlappingThen, we partitionsensethecoverssenseswhichby selectingmaximizesa settheof total connectedness value."
"So in the example above, the set f (1 4) , (2 3 5) g yields the maximum con-nectedness."
"Finally, senses that are not covered by any sense covers are taken as singletons, and added to the nal sense partition."
"So the sense partition for \table&quot; becomes f (1 4) , (2 3 5) , (6) g ."
"Asbetweenyou canWord-see, our lexicon contains much less ambiguity: the ratio of monosemous words increased from 84% (88,650/105,461 .84) to 92% (96,964/105,461 .92), and the average number of senses for polysemous words decreased from 2.73 to 2.52 for nouns, and fromAs 3a.57noteto,2.our82 forlexiconverbsis. similar to CORELEX[REF_CITE](or CORELEX-II presented[REF_CITE]), in that both lexicons share the same motivation."
"However, our lexicon di ers from CORELEX in that CORELEX looks at all senses of a word and groups words that have the same sense distribution pattern, whereas our lexicon groups word senses that have the same systematic relation."
"Thus, our lexicon represents systematic polysemy at a ner level than CORELEX, by pinpointing related senses within each word."
"To test if the sense partitions in our lexicon con-stitute an appropriate (or useful) level of granular-ity, we applied it to the inter-annotator disagree-porament: observedWordNetinSemcortwo semantically(Landes et annotated al. , 1998) andcor-"
The agreement between those corpora is previously studied[REF_CITE].
"In our current work, we rst re-produced their agreement data, then used our sense partitions to see whether or not they yield a better agreement. tencesIn /thisinstancesexperimentfor 191, wewordsextracted(consisting28,772of sen-121 nouns and 70 verbs) tagged in the intersection of the two corpora."
This constitutes the base data set.
Table 5 shows the breakdown of the number of in-stances where tags agreed and disagreed. [Footnote_10]
"10 Note that the numbers reported[REF_CITE]are slightly more than the ones reported in this paper. For in-stance, the number of sentences in the intersected corpus re-ported[REF_CITE]is 30,315. We speculate the dis-crepancies are due to the di erent sentence alignment meth-"
"As you can see, the agreement is not very high: only around 48%. 11 sureThiscalledlow theagreementstatisticratio(Carlettais also re, 1996ected;"
"Brucein a mea-and[REF_CITE]). measure takes into account chance agreement, thus better representing the state of disagreement."
"A value is calculated for each word, on a confusion matrix where rows represent the senses assigned by judge 1 (DSO) and columns represent the senses assigned by judge 2 (Semcor)."
Table 6 shows an example matrix for the noun \table&quot;. useAthevaluenotationfor a andwordformulais calculatedused inas follows(Bruce. and[REF_CITE]).
"Let n ij denote the number of in-stances where the judge 1 assigned sense i and the judge 2 assigned sense j to the same instance, and n i + and n + i denote the marginal totals of rows and columns respectively."
The formula is:
P P ii ;
"P i P i + P + i k = i 1 ; P i P i P (9) + + i where P ii = nn (i.e., proportion of n ii , the number ii of instances where ++ both judges agreed on sense i , to the total instances),"
"P i + = nn i+ and P + i = nn . +i + agreement + is perfect ++ (i.e P ., valuesvalueinisthe1.0owhen-diagonalthe cells are all 0, that The is, i P ii = 1), or 0 when the agreement is purely ods 11 used(Ng et in al the . , 1999experiments) reports. a higher agreement of 57%."
"We speculate the discrepancy might be from the version of Word-Net senses used in DSO, which was slightly di erent from the standard delivery version (as noted[REF_CITE]). by chance (i.e., values in a row (or column) are uni- P ii ="
"P i + P + i for all 1formly distributed across rows i ( M or,columnswhere M ), thatis theis, number of rows/columns). also takes a negative valuetween whenthe twotherejudgesis a(esystematic.g., some valuesdisagreementin the diago-be-nal cells : 8 isareconsidered0, that is, P a ii good= 0 foragreementsome i )."
"Normally(Carletta,, 1996). 191Bywordsusingwasthe formula.264, asaboveshown, thein Tableaverage5. for the [Footnote_12] This means the agreement between Semcor and DSO is quite low. and[REF_CITE]wordsto reducefrom ourthelexiconsize of, the confusion matrices."
12[REF_CITE]&apos;s result is slightly higher: = : 317.
"For each word, we computed the for the reduced matrix, and compared it with the for a random sense grouping of the same parti-tion pattern. [Footnote_13]"
"13 For this comparison, we excluded 23 words whose sense partitions consisted of only 1 sense cover. This is re ected in the total number of instances in Table 8."
"For example, the partition pattern of f (1 4) , (2 3 5) , (6) g for \table&quot; mentioned earlier (where Table 7 shows ; its reduced matrix) is a multi-nomial combination 6 ."
"The value for a ran-dom grouping is obtained 2 3 1 by generating 5,000 ran-dom partitions which have the same pattern as the corresponding sense partition in our lexicon, then taking the mean of their &apos;s."
"Then we measured the possible increase in by our lexicon by taking the di erence between the paired values for all words (i.e., w by our sense partition - w by random par-tition, for a word w ), and performed a signi cance test, with a null hypothesis that there was no signif-icant increase."
"The result showed that the P-values were 4.17 and 2.65 for nouns and verbs respectively, which were both statistically signi cant."
"Therefore, the null hypothesis was rejected, and we concluded that there was a signi cant increase in by using our lexicon. ourAslexicona note,andthe theiraveragecorresponding&apos;s for the 191randomwordsparti-from tions were .260 and .233 respectively."
Those values are in fact lower than that for the original WordNet lexicon.
There are two major reasons for this.
"First, in general, combining any arbitrary senses does not always increase ."
"In the given P formula 9, actually decreases when the increase in i P ii (i.e., the diag-onal sum P ) in the reduced matrix is less than the in-crease in i P i + P + i (i.e., the marginal product sum) by some factor. 14"
"This situation typically happens when senses combined are well distinguished in the original matrix, in the sense that, for senses i and j , n ij and n ji are 0 or very small (relative to the total frequency)."
"Second, some systematic relations are in fact easily distinguishable."
"Senses in such relations often denote di erent objects in a context, for in-stance ANIMAL and MEAT senses of \chicken&quot;."
"Since our lexicon groups those senses together, the &apos;s for the reduce matrices decrease for the reason we men-tioned above."
Table 8 shows the breakdown of the average for our lexicon and random groupings. 14 This is because P P i + P + i is subtracted in both the nu-merator P and the denominator P i in the formula.
Note that both i P ii and i P i + P + i always increase when any ar-bitrary 1 ; P senses are combined.
The factor mentioned here is 1 ; P i P ii+ P ii P +i .
"As we reported in previous sections, our tree-cut extraction method discovered 89% of the Word-Net cousins."
"Although the precision was rela-tively low (50-60%), this is an encouraging re-sistently yielded bettersult."
"As for the lexicon, ourvaluessense thanpartitionsarbitrarycon-be quite promising."
We Ourconsiderdatatheseis availableresults atto sense groupings.[URL_CITE]. partitionsIt is signiderivedcant toinnotethisthatworkclusterare domainpairs andindepen-sense dent.
"Such information is useful in broad-domain applications, or as a background lexicon (Kilgarri , 1997) in domain speci c applications or text catego-rization and IR tasks."
"For those tasks, we anticipate that our extraction methods may be useful in deriv-ing characteristics of the domains or given corpus, as well as customizing the lexical resource."
"This is our next future research. automaticFor otherwayfutureof detectingwork, weandplan tolteringinvestigateunrelatedan relations."
We are also planning to compare our sense partitions with the systematic disagreement ob-tained[REF_CITE]&apos;s automatic classi er.
This paper presents a corpus-based approach to word sense disambiguation where a decision tree as-signs a sense to an ambiguous word based on the bigrams that occur nearby.
This approach is evalu-ated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.
"It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words."
"Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs."
"For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined."
"For example, suppose bill has the following set of possi-ble meanings: a piece of currency, pending legisla-tion, or a bird jaw."
"When used in the context of The Senate bill is under consideration , a human reader immediately understands that bill is being used in the legislative sense."
"However, a computer program attempting to perform the same task faces a diÆcult problem since it does not have the bene t of innate common{sense or linguistic knowledge."
"Rather than attempting to provide computer pro-grams with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods."
These approaches use techniques from statistics and machine learn-ing to induce models of language usage from large samples of text.
"These models are trained to per-form particular tasks, usually via supervised learn-ing."
This paper describes an approach where a deci-sion tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context.
"Prior to learning, the sense{tagged corpus must be converted into a more regular form suitable for auto-matic processing."
"Each sense{tagged occurrence of an ambiguous word is converted into a feature vec-tor, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process."
"Given the exibility and complexity of human language, there is poten-tially an in nite set of features that could be utilized."
"However, in corpus{based approaches features usu-ally consist of information that can be readily iden-ti ed in the text, without relying on extensive exter-nal knowledge sources."
"These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word."
"The approach in this paper relies upon a feature set made up of bigrams , two word sequences that occur in a text."
The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.
"We take this approach since surface lexical fea-tures like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation ac-curacy."
"It is not clear how much disambiguation ac-curacy is improved through the use of features that are identi ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora res-olution."
One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation us-ing feature sets that do not impose substantial pre{ processing requirements.
This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.
"Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are in-cluded for purposes of comparison."
"The experimen-tal data is discussed, and then the empirical results are presented."
We close with an analysis of our nd-ings and a discussion of related work.
"We have developed an approach to word sense dis-ambiguation that represents text entirely in terms of the occurrence of bigrams, which we de ne to be two consecutive words that occur in a text."
The distri-butional characteristics of bigrams are fairly consis-tent across corpora; a majority of them only occur one time.
"Given the sparse and skewed nature of this data, the statistical methods used to select in-teresting bigrams must be carefully chosen."
"We ex-plore two alternatives, the power divergence family of goodness of t statistics and the Dice CoeÆcient, an information theoretic measure related to point-wise Mutual Information."
Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 2 contingency table.
The value of n 11 shows how many times the bigram big cat occurs in the corpus.
The value of n 12 shows how often bigrams occur where big is the rst word and cat is not the second.
The counts in n +1 and n 1+ indicate how often words big and cat occur as the rst and second words of any bigram in the corpus.
The total number of bigrams in the corpus is represented by n ++ .
"A number of well known statistics belong to this family, includ-ing the likelihood ratio statistic G 2 and Pearson&apos;s X 2 statistic."
"These measure the divergence of the observed ( n ij ) and expected ( m ij ) bigram counts, where m ij is estimated based on the assumption that the com-ponent words in the bigram occur together strictly by chance: m ij = n i + n + j n ++ G 2 and X 2 are calculated as: Given this value X , G 2 = 2 n ij log n ij i;j m ij X 2 = X ( n ij m ij ) 2 i;j m ij[REF_CITE]argues in favor of G 2 over X 2 , es-pecially when dealing with very sparse and skewed data distributions."
"However,[REF_CITE]suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other."
"In light of this,[REF_CITE]presents Fisher&apos;s exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson&apos;s test and the likelihood ratio."
Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.
"We take the following approach, based on the obser-vation that all tests should assign approximately the same measure of statistical signi cance when the bi-gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of t statistics."
"We perform tests us-ing X 2 , G 2 , and Fisher&apos;s exact test for each bigram."
"If the resulting measures of statistical signi cance di er, then the distribution of the bigram counts is causing at least one of the tests to become unreli-able."
When this occurs we rely upon the value from Fisher&apos;s exact test since it makes fewer assumptions about the underlying distribution of data.
"For the experiments in this paper, we identi ed the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word."
"There were no cases where rankings produced by G 2 , X 2 , and Fisher&apos;s exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded."
"Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic."
The Dice CoeÆcient is a descriptive statistic that provides a measure of association among two words in a corpus.
"It is similar to pointwise Mutual Infor-mation, a widely used measure that was rst intro-duced for identifying lexical relationships[REF_CITE]."
Pointwise Mutual Information can be de ned as follows:
MI ( w 1 ; w 2 ) = log 2 n 11 n ++ n +1 n 1+ where w 1 and w 2 represent the two words that make up the bigram.
Pointwise Mutual Information quanti es how of-ten two words occur together in a bigram (the nu-merator) relative to how often they occur overall in the corpus (the denominator).
"However, there is a curious limitation to pointwise Mutual Information."
"A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases."
"Thus, the maximum pointwise Mutual"
"Information in a given corpus will be assigned to bi-grams that occur one time, and whose component words never occur outside that bigram."
"These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information."
"The Dice CoeÆcient overcomes this limitation, and can be de ned as follows:"
Dice ( w 1 ; w 2 ) = 2 n 11 n +1 + n 1+
When the value of n 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co-eÆcient are similar to those of Mutual Information.
The relationship between pointwise Mutual Infor-mation and the Dice CoeÆcient is also discussed[REF_CITE].
We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.
This software is written in Perl and is freely available[URL_CITE]
Decision trees are among the most widely used ma-chine learning algorithms.
"They perform a general to speci c search of a feature space, adding the most informative features to a tree structure as the search proceeds."
The objective is to select a minimal set of features that eÆciently partitions the feature space into classes of observations and assemble them into a tree.
"In our case, the observations are manually sense{tagged examples of an ambiguous word in con-text and the partitions correspond to the di erent possible senses."
Each feature selected during the search process is represented by a node in the learned decision tree.
Each node represents a choice point between a num-ber of di erent possible values for a feature.
Learn-ing continues until all the training examples are ac-counted for by the decision tree.
"In general, such a tree will be overly speci c to the training data and not generalize well to new examples."
Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances.
Test instances are disambiguated by nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed fea-tures.
"An instance of an ambiguous word is dis-ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context."
"We also include three benchmark learning algo-rithms in this study: the majority classi er, the de- cision stump, and the Naive Bayesian classi er."
The majority classi er assigns the most common sense in the training data to every instance in the test data.
A decision stump is a one node decision tree[REF_CITE]that is created by stopping the de-cision tree learner after the single most informative feature is added to the tree.
The Naive Bayesian classi er[REF_CITE]is based on certain blanket assumptions about the interactions among features in a corpus.
There is no search of the feature space performed to build a representative model as is the case with decision trees.
"Instead, all features are included in the classi-er and assumed to be relevant to the task at hand."
"There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word."
"It is most often used with a bag of words feature set, where every word in the training sample is represented by a bi-nary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word."
"We use the Weka[REF_CITE]imple-mentations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi er."
Weka is written in Java and is freely avail-able[URL_CITE]
Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.
Ten teams partic-ipated in the supervised learning portion of this event.
"Additional details about the exercise, in-cluding the data and results referred to in this paper, can be found at the SENSEVAL web site[URL_CITE]and[REF_CITE]."
We included all 36 tasks from SENSEVAL for which training and test data were provided.
Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense{tagged instances in the training data.
Some words were used in multiple tasks as di erent parts of speech.
"For example, there were two tasks associated with bet , one for its use as a noun and the other as a verb."
"Thus, there are 36 tasks involving the disambiguation of 29 di erent words."
The words and part of speech associated with each task are shown in Table 1 in column 1.
"Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided."
The number of test and training instances for each task are shown in columns 2 and 4.
Each instance consists of the sentence in which the ambiguous word occurs as well as one or two surrounding sentences.
In general the total context available for each ambiguous word is less than 100 surrounding words.
The number of distinct senses in the test data for each task is shown in column 3.
The following process is repeated for each task.
Cap-italization and punctuation are removed from the training and test data.
Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient.
The bigram must have oc-curred 5 or more times to be included as a feature.
This step lters out a large number of possible bi-grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.
The training and test data are converted to fea-ture vectors where each feature represents the occur-rence of one of the bigrams that belong in the feature set.
This representation of the training data is the actual input to the learning algorithms.
"Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature set identi ed by the Dice CoeÆcient."
The majority clas-si er simply determines the most frequent sense in the training data and assigns that to all instances in the test data.
The Naive Bayesian classi er is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature.
All of these learned models are used to disam-biguate the test data.
The test data is kept separate until this stage.
"We employ a ne grained scoring method, where a word is counted as correctly disam-biguated only when the assigned sense tag exactly matches the true sense tag."
No partial credit is as-signed for near misses.
The accuracy attained by each of the learning algo-rithms is shown in Table 1.
"Column 5 reports the accuracy of the majority classi er, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams."
"The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product."
"However, the best precision and recall may have come from di erent teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system."
The average accuracy in column 7 is the product of the average precision and recall reported for the par-ticipating SENSEVAL teams.
Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identi ed by a power di-vergence statistic.
Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice CoeÆcient respectively.
"Fi-nally, column 13 shows the accuracy of the Naive Bayesian classi er based on a bag of words feature set."
The most accurate method is the decision tree based on a feature set determined by the power di-vergence statistic.
The last line of Table 1 shows the win-tie-loss score of the decision tree/power di-vergence method relative to every other method.
"A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate."
"The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36 tasks when compared to the average reported accu-racy."
"The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks."
In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice CoeÆcient.
"The power divergence tests prove to be more reliable since they account for all possible events surround-ing two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither."
"The Dice CoeÆcient is based strictly on the event where w 1 Thereand are 6 tasks where the decision tree / power w 2 occur together in a bigram. divergence approach is less accurate than the SEN-SEVAL average; promise-n, scrap-n, shirt-n, amaze-v, bitter-p, and sanction-p."
"The most dramatic dif-ference occurred with amaze-v, where the SENSE-VAL average was 92.4% and the decision tree accu-racy was 58.6%."
"However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data."
The characteristics of the decision trees and deci-sion stumps learned for each word are shown in Table 2.
Column 1 shows the word and part of speech.
"Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice CoeÆ-cient."
Columns 2 and 5 show the node selected to serve as the decision stump.
Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.
Columns 4 and 7 show the number of bigram features selected to represent the training data.
This table shows that there is little di erence in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice CoeÆcient.
"This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those."
"However, there are di erences between the feature sets selected by the power divergence statistics and the Dice CoeÆcient."
These are re ected in the dif-ferent sized trees that are learned based on these feature sets.
The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.
The number of internal nodes is simply the di erence between the total nodes and the leaf nodes.
Each leaf node represents the end of a path through the decision tree that makes a sense distinction.
"Since a bigram feature can only appear once in the decision tree, the number of inter- nal nodes represents the number of bigram features selected by the decision tree learner."
One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features.
This was motivated by the success of decision stumps in performing disam-biguation based on a single bigram feature.
"In these experiments, there were no decision trees that used all of the bigram features identi ed by the ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features."
This can be seen by comparing the number of inter-nal nodes with the number of candidate features as shown in columns 4 or 7. [Footnote_1]
"1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi-grams that occurred more than 5 times then all such bigrams are included in the feature set."
It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice CoeÆcient.
"This is to be expected, since the selection of the bigrams from raw text is only mea- suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses."
"In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the over-all Mutual Information between the bigram and a particular word sense."
"Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods."
A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classi er.
A deci-sion tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.
One of our long-term objectives is to identify a core set of features that will be useful for disambiguat-ing a wide class of words using both supervised and unsupervised methodologies.
"We have presented an ensemble approach to word sense disambiguati[REF_CITE]where mul-tiple Naive Bayesian classi ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely stud-ied nouns interest and line ."
"While the accuracy of this approach was as good as any previously pub-lished results, the learned models were complex and diÆcult to interpret, in e ect acting as very accurate black boxes."
Our experience has been that variations in learn-ing algorithms are far less signi cant contributors to disambiguation accuracy than are variations in the feature set.
"In other words, an informative fea-ture set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features."
"Therefore, our focus is on developing and discover-ing feature sets that make distinctions among word senses."
"Our learning algorithms must not only pro-duce accurate models, but they should also shed new light on the relationships among features and allow us to continue re ning and understanding our fea-ture sets."
We believe that decision trees meet these criteria.
"A wide range of implementations are available, and they are known to be robust and accurate across a range of domains."
"Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation."
"Bigrams have been used as features for word sense disambiguation, particularly in the form of colloca-tions where the ambiguous word is one component of the bigram (e.g.,[REF_CITE],[REF_CITE],[REF_CITE])."
"While some of the bigrams we identify are collocations that include the word being disambiguated, there is no require-ment that this be the case."
"Decision trees have been used in supervised learn-ing approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g.,[REF_CITE],[REF_CITE])."
"In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the mor-phology of the ambiguous word."
We believe that the approach in this paper is the rst time that de-cision trees based strictly on bigram features have been employed.
"The decision list is a closely related approach that has also been applied to word sense disambigua-tion (e.g.,[REF_CITE],[REF_CITE],[REF_CITE])."
"Rather than building and traversing a tree to perform disambiguation, a list is employed."
In the general case a decision list may suf-fer from less fragmentation during learning than de-cision trees; as a practical matter this means that the decision list is less likely to be over{trained.
"How-ever, we believe that fragmentation also re ects on the feature set used for learning."
Ours consists of at most approximately 100 binary features.
This re-sults in a relatively small feature space that is not as likely to su er from fragmentation as are larger spaces.
There are a number of immediate extensions to this work.
The rst is to ease the requirement that bi-grams be made up of two consecutive words.
"Rather, we will search for bigrams where the component words may be separated by other words in the text."
The second is to eliminate the ltering step by which candidate bigrams are selected by a power diver-gence statistic.
"Instead, the decision tree learner would consider all possible bigrams."
"Despite increas-ing the danger of fragmentation, this is an interest-ing issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the ltering step."
"In particular, we will determine if the ltering process ever eliminates bi-grams that could be signi cant sources of disam-biguation information."
"In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene t of sense tagged text."
"We are optimistic that this is viable, since bigram features are easy to identify in raw text."
This paper shows that the combination of a simple feature set made up of bigrams and a standard deci-sion tree learning algorithm results in accurate word sense disambiguation.
The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more ac-curate than the best SENSEVAL results for 19 of 36 words.
"The Bigram Statistics Package has been imple-mented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholar-ship from the OÆce of the Vice President for Re-search and the Dean of the Graduate School of the University of Minnesota."
We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.
The comments of three anonymous reviewers were very helpful in preparing the nal version of this paper.
A preliminary version of this paper appears[REF_CITE].
"Æc·¸ºoÇ!´Â¾%ÈÉ½&gt;^Ê´Â»r·&gt;Æ9ÇT¾½ÌËÍÆÂ¹¢ÈÎ&gt;&gt;¹ ÏÐ¾ ·&gt; ÇT¾½ÌËÍÆÂ¹¢ÈÎ&gt;¹o½&gt;Ä9ËÍ³ ´Â»(³Ô¾½KÁ  &gt;¾ÔÕ%  ,¿ &gt;&gt;ÄÖ¶  *&gt;%Ù ·¸Ù%    $!Ç ¾½`ËÍÆÃ¹¢ÈÎ&gt;&gt;¹ ÏÛ·&gt;¾%ÈÜ»rÆÂ·&gt; &gt;¾   Ò$  ¶ &gt;%*·b¶% !»rÁ  ¶ &gt;&gt; $  %ÆÂ¹¼»rÆc·&gt; &gt;½ ºàÆÂ¹rá&gt;&gt;Ä9Î&gt;&gt;{³  &gt;Ò ×Éâ %% &gt;ÑbÙ%´ÃÎKÁ &gt; ^¶ È ´äã&amp; &gt;&gt; µr× æ ´cµÖ¶ç %Bè·¸¶;&gt;&amp;¹rº»^½K¾!»^;Ké8B  &gt;%Ù ÆÂ¹¢Ñë½&gt;Ä&amp;³   %&gt;Ù%·¸Ù%&gt; % &amp;¶  ¶&gt;· &gt;,%]%&gt;%Ù ·¸Ù%%&gt; ½Ìá&gt; &amp; % &gt;á ½KÆÃá&gt;!ÁÎ&gt;!Á &gt;Ä %&gt;% %·¸º(*È% &gt;ÙÅ¶ ÆÃ¹¢Ño×&lt; &gt;&amp;¶ &gt;]Æ ³ %¾¹* &gt;%´ÂÆÂ´Ã¿Ò &gt;%&gt;¾] ´Ã¹rº( Ò&gt;Ïv·&gt;¾ È"
I present a method of identifying cognates in the vo-cabularies of related languages.
"I show that a mea-sure of phonetic similarity based on multivalued fea-tures performs better than “orthographic” measures, such as the Longest Common Subsequence Ratio (LCSR) or Dice’s coefficient."
I introduce a proce-dure for estimating semantic similarity of glosses that employs keyword selection and WordNet.
Tests performed on vocabularies of four Algonquian lan-guages indicate that the method is capable of discov-ering on average nearly 75% percent of cognates at 50% precision.
"In the narrow sense used in historical linguistics, cognates are words in related languages that have developed from the same ancestor word."
"An ex-ample of a cognate pair is French lait and Span-ish leche, both of which come from Latin lacte."
"In other contexts, including this paper, the term is often used more loosely, denoting words in different lan-guages that are similar in form and meaning, without making a distinction between borrowed and genet-ically related words; for example, English sprint and the Japanese borrowing supurinto are consid-ered cognate, even though these two languages are unrelated."
"In historical linguistics, the identification of cog-nates is a component of two principal tasks of the field: establishing the relatedness of languages and reconstructing the histories of language families."
"In corpus linguistics, cognates have been used for bi-text alignment[REF_CITE], and for extracting lexicographically interesting word-pairs from multilingual corpora[REF_CITE]."
The task addressed in this paper can be formu- lated in two ways.
"On the word level, given two words (lexemes) from different languages, the goal is to compute a value that reflects the likelihood of the pair being cognate."
"I assume that each lexeme is given in a phonetic notation, and that it is accom-panied by one or more glosses that specify its mean-ing in a metalanguage for which a lexical resource is available (for example, English)."
"On the language level, given two vocabulary lists representing two languages, the goal is to single out all pairs that ap-pear to be cognate."
Tables 1 and 2 show sample en-tries from two typical vocabulary lists.
Such vocab-ulary lists are sometimes the only data available for lesser-studied languages.
"In general, deciding whether two words are ge-netically related requires expert knowledge of the history of the languages in question."
"With time, words in all languages change their form and mean-ing."
"After several millennia, cognates often acquire very different phonetic shapes."
"For example, En-glish hundred, French cent, and Polish sto are all descendants of Proto-Indo-European *kmtom."
"The semantic change can be no less dramatic; for ex-ample, English guest and Latin hostis ‘enemy’ are cognates even though their meanings are diametri-cally different."
"On the other hand, phonetic similar-ity of semantically equivalent words can be a matter of chance resemblance, as in English day and Latin die ‘day’."
"In the traditional approach to cognate identifica-tion, words with similar meanings are placed side by side."
Those pairs that exhibit some phonologi-cal similarity are analyzed in order to find systematic correspondences of sounds.
The correspondences in turn can be used to distinguish between genuine cog-nates and borrowings or chance resemblances.
"My approach to the identification of cognates is based on the assumption that, in spite of the in-evitable diachronic changes, cognates on average display higher semantic and phonetic similarity than words that are unrelated. 1"
"In this paper, I present COGIT, a cognate-identification system that com-bines ALINE[REF_CITE], a feature-based al-gorithm for measuring phonetic similarity, with a novel procedure for estimating semantic similari-ty that employs keyword selection and WordNet."
"When tested on data from four native American lan-guages, COGIT was able to discover, on average, nearly 75% percent of cognates at 50% precision, without resorting to a table of systematic sound cor-respondences."
The results show that a large percent-age of cognates can be detected automatically.
"To my knowledge, no previously proposed algo-rithmic method is able to identify cognates directly in vocabulary lists."
Guy’s (1994) program COG-NATE identifies probable letter correspondences be-tween words and estimates how likely it is that the words are related.
"The algorithm has no semantic component, as the words are assumed to have al-ready been matched by their meanings."
Such an approach by definition cannot detect cognates that have undergone a semantic shift.
Hewson (1974; 1993) employed a simple strategy of generating proto-projections to produce a dictionary of over 4000 Proto-Algonquian etyma from vocabularies of several contemporary Algonquian languages.
"The proto-projections, generated using long-established systematic sound correspondences, were then exam-ined individually in order to select true cognates."
The “Reconstruction Engine”[REF_CITE]uses a similar strategy of generating proto-projections to establish cognate sets.
Hewson’sand Lowe and Mazaudon’s approaches re-quire a complete table of systematic sound corre-spondences to be provided beforehand.
"Such ta-bles can be constructed for well-studied language families on the basis of previously identified cog-nate sets, but are not available for many African and native American languages, especially in the cases where the relationship between languages has not been adequately proven."
"In contrast, the method presented in this paper operates directly on the vo-cabulary lists."
The approaches to measuring word similarity can be divided into two groups.
"The “orthographic” ap-proaches disregard the fact that alphabetic symbols express actual sounds, employing a binary identity function on the level of character comparison."
A one-to-one encoding of symbols has no effect on the results.
"The “phonetic” approaches, on the other hand, attempt to take advantage of the phonetic char-acteristics of individual sounds in order to estimate their similarity."
This presupposes a transcription of the words into a phonetic or phonemic representa-tion.
The “orthographic” approaches are commonly used in corpus linguistics.
"For example, Dice’s coefficient is de-fined as"
DICE x y  2 bigrams x  bigrams y  bigrams x  bigrams y where bigrams(x) is a multi-set of character bi-grams in x.[REF_CITE]uses 4-grams at the level of character sequences.
"LCS x y  LCSR x y  max x y where LCS(x,y) is the longest common subsequence of x and y."
"ALINE[REF_CITE], is an example of the “phonetic” approach."
"ALINE was originally devel-oped for aligning phonetic sequences, but since it chooses the optimal alignment on the basis of a sim-ilarity score, it can also be used for computing sim-ilarity."
Each phoneme is represented as a vector of phonetically-based feature values.
The number of distinct values for each feature is not constrained. 2
The features have salience coefficients that express their relative importance.
ALINE uses dynamic pro-gramming to compute similarity scores.
"Because it uses similarity rather than distance, the score as-signed to two identical words is not a constant, but depends on the length and content of the words."
"Intuitively, a complex algorithm such as ALINE should be more accurate than simple, “ortho-graphic” coefficients."
"By applying various methods to a specific task, such as cognate identification, their relative performance can be objectively evaluated."
The meanings of the lexemes are represented by their glosses.
"Therefore, the simplest method to de-tect semantic similarity is to check if the lexemes have at least one gloss in common."
"For example, the cognates kottāčı̄win ‘terror, fear’ and kostāčı̄win ‘fear, alarm’ in Tables 1 and 2 are correctly associ-ated by this method."
"However, in many cases, the similarity of semantically related glosses is not rec-ognized."
The most common reasons are listed be-low. 1.
"Spelling errors or variants: ‘vermilion’ and ‘vermillion’, ‘sweet grass’ and ‘sweetgrass’, ‘plow’ and ‘plough’; [Footnote_2]."
"2 For a different “phonetic” approach, based on binary artic-ulatory features, see[REF_CITE]."
Morphological differences: ‘ash’ and ‘ashes’; [Footnote_3].
"3 The idea of using WordNet for the detection of semantic relationshipscomes[REF_CITE](footnote 13, page 406)."
"Determiners: ‘a mark’ and ‘mark’, ‘my finger’ and ‘finger’, ‘fish’ and ‘kind of fish’; 4."
Adjectival modifiers: ‘small stone’ and ‘stone’; 5.
Nominal modifiers: ‘goose’ and ‘snow goose’; 6.
"Complements and adjuncts: ‘stone’ and ‘stone of peach’, ‘island’ and ‘island in a river’; 7."
Synonymy: ‘grave’ and ‘tomb’; 8.
Small semantic changes: ‘fowl’ and ‘turkey’; 9.
Radical semantic changes: ‘broth’ and ‘grease’.
"Spelling errors, which may be especially fre-quent in data that have been acquired through op-tical character recognition, are easy to detect but have to be corrected manually."
Morphological dif-ferences (category 2) can be removed by lemmati-zation.
"Many of the cases belonging to categories 3 and 4 can be handled by adopting a stop list of de-terminers, possessive pronouns, and very common modifiers such as certain, kind of, his, big, female, etc."
"Categories 4, 5, and 6 illustrate a common phe-nomenon of minor semantic shifts that can be de-tected without resorting to a lexical resource."
"All that is needed is the determination of the heads of the phrases, or, more generally, keywords."
Pairs of glosses that contain matching keywords are usually semantically related.
"For the remaining categories, string matching is of no assistance, and some lexical resource is called for."
"In this paper, I use WordNet[REF_CITE], or rather, its noun hierarchy, which is the most devel-oped of the four WordNet hierarchies. 3 WordNet is well-suited not only for detecting synonyms but also for associating lexemes that have undergone small semantic changes."
Certain types of semantic change have direct par-allels among WordNet’s lexical relations.
"General-ization can be seen as moving up the IS-A hierar-chy along a hypernymy link, while specialization is moving in the opposite direction, along a hyponymy link."
Synecdoche can be interpreted as a movement along a meronymy/holonymy link.
"However, other types of semantic change, such as metonymy, melio-ration/pejoration, and metaphor, have no direct ana-logues in WordNet."
The use of WordNet for semantic similarity detec-tion is possible only if English is the glossing met-alanguage.
"If the available vocabularies are glossed in other languages, one possible solution is to trans-late the glosses into English, which, however, may increase their ambiguity."
"A better solution could be to use a multilingual lexical resource, such as Eu-roWordNet[REF_CITE], which is modeled on the original Princeton WordNet."
"Given two vocabulary lists representing distinct lan-guages, COGIT, the cognate identification system (Figure 1), produces a list of vocabulary-entry pairs, sorted according to the estimated likelihood of their cognateness."
"Each vocabulary entry consists of a lexeme l and its gloss g. COGIT is composed of a set of Perl scripts for preprocessing the vocabulary lists, and phonetic and semantic modules written in C++."
"Both modules return similarity scores in the range 0 1 , which are combined into an overall sim-ilarity score by the following formula:"
Sim overall l 1 g 1 l 2 g 2 ! 1 &quot; α $# Sim phon l 1 l 2  α # Sim sem g 1 g 2 % where α is a parameter reflecting the relative impor-tance of the semantic vs. phonetic score.
The algo-rithm is presented informally in Figure 2.
The preprocessing of the glosses involves stop word removal and keyword selection.
A simple heuristic is used for the latter: the preprocessing script marks as keywords all nouns apart from those that follow a wh-word or a preposition other than “of”.
"Nouns are identified by a part-of-speech tag-ger[REF_CITE], which is applied to glosses after prepending them with the string “It is a”."
Checking and correcting the spelling of glosses is assumed to have been done beforehand.
"The phonetic module calculates phonetic similar-ity using either ALINE or a straightforward method such as LCSR, DICE, or truncation."
The truncation coefficient is obtained by dividing the length of the common prefix by the average of the lengths of the two words being compared.
"The similarity score re-turned by ALINE is also normalized, so that it falls in the range 0 1 ."
The implementation of ALINE is described[REF_CITE].
"For the calculation of a WordNet-based seman-tic similarity score, I initially used the length of the shortest path between synsets, measured in the num- ber of IS-A links. 4"
"However, I found the effect of considering paths longer than one link to be negligi-ble."
"Moreover, the process of determining the link distances between all possible pairs of glosses, sep-arately for each pair, was too time-consuming."
"Currently, the semantic score is computed by a faster method that employs QueryData, a Perl Word-Net [Footnote_5] module[REF_CITE]."
5 The version of WordNet used is 1.6.
"A list of synonyms, hyponyms, and meronyms is generated for each gloss and keyword in the preprocessing phase."
"Dur-ing the execution of the program, regular string matching is performed directly on the listed senses."
Words are considered to be related if there is a rela-tionship link between any of their senses.
"The se-mantic score is determined according to a 9-point scale of semantic similarity, which is shown in Ta-ble 3."
"The levels of similarity are considered in or-der, starting with gloss identity."
The exact scores corresponding to each level were established empir-ically.
The coverage figures are discussed in Sec-tion 6.
The QueryData module also carries out the lemmatization process.
COGIT was evaluated on noun vocabularies of four Algonquian languages.
The source of the data was machine-readable vocabulary lists that had been used to produce a computer-generated Algo-nquian dictionary[REF_CITE].
"No grapheme-to-phoneme conversion was required, as the Algon-quian lexemes are given in a phonemic transcrip-tion."
"The lists can be characterized as noisy data; they contain many errors, inconsistencies, dupli-cates, and lacunae."
"As much as possible, the entries were cross-checked with the dictionary itself, which is much more consistent."
"The dictionary, which con-tains entries from the four languages grouped in cog-nate sets, also served as a reliable source of cognate-ness information."
Table [Footnote_4] specifies the number of lexemes available for each language.
4 A number of more sophisticated methods exist for measur-ing semantic similarity using WordNet[REF_CITE].
Only about a third of those nouns are actually in the dictionary; the rest occur only in the vocabulary lists.
Table 5 shows the number of cognate pairs for each language combination.
"To take the Menomini–Ojibwa pair as an example, the task of the system was to iden-tify 259 cognate-pairs from 1540 &amp; 1023 possible lexeme-pairs."
The average ratio of non-cognate to cognate pairs was about 6500.
Experimental results support the intuition that both the phonetic and the semantic similarity be-tween cognates is greater than between randomly selected lexemes.
"Table 6 contrasts phonetic simi-larity scores for cognate pairs and for randomly se-lected pairs, averaged over all six combinations of languages."
"The average value of the semantic simi-larity score, as defined in Table 3, was .713 for cog-nate pairs, and less than .003 for randomly selected pairs. various cognate indentification methods."
"Methods G, K, and W use ALINE combined with increasingly complex semantic similarity detection (α 0 &apos; 2)."
"The values of all parameters, including α, ALINE’s parameters [Footnote_6] , and the semantic similarity scale given in Table 3, were established during the development phase of the system, using only the Cree–Ojibwa data."
"6 ALINE’s parameters were set as follows: C skip = –1, C sub = 10, C exp = 15 and C vwl = 1. The salience settings were the same as[REF_CITE], except that the salience of feature “Long” was set to 5."
These two languages were chosen as the development set because they are represented by the most complete vocabularies and share the largest number of cognates.
"However, as it turned out later, they are also the most closely related among the four Algonquian languages, according to all measures of phonetic similarity."
It is quite possible that the overall performance of the system would have been better if a different language pair had been chosen as the development set.
"Table 7 compares the effectiveness of various cognate identification methods, using interpolated 3-point average precision."
"The first four meth-ods (Truncation, DICE, LCSR, and ALINE) are based solely on phonetic similarity."
"The remaining three methods combine ALINE with increasingly sophisticated semantic similarity detection: Method G considers gloss identity only, Method K adds keyword-matching, and Method W employs also WordNet relations."
The results for the development set (Cree–Ojibwa) are given in the first column.
The results for the remaining five sets are given jointly as their average and standard deviation.
The choice of 3-point average precision requires explanation.
The output of the system is a sorted list of suspected cognate pairs.
"Typically, true cognates are very frequent near the top of the list, and be- come less frequent towards the bottom."
"The thresh-old value that determines the cut-off depends on the intended application, the degree of relatedness be-tween languages, and the particular method used."
"Rather than reporting precision and recall values for an arbitrarily selected threshold, precision is com-puted for the levels 20%, 50%, and 80%, and then averaged to yield a single number."
"Figure 3 shows a more detailed comparison of the effectiveness of the methods on test sets, in the form of precision–recall curves."
"Among the phonetic methods, ALINE outperforms all “ortho-graphic” coefficients, including LCSR, The dom-inance of ALINE increases as more remote lan-guages are considered."
"Dice’s coefficient performs poorly as a cognate identification method, being only slightly better than a naive truncation method."
All three methods that use the semantic information provided by the glosses perform substantially bet-ter than the purely phonetic methods.
Impressive re-sults are reached even when only gloss identity is considered.
"Adding keyword-matching and Word-Net relations brings additional, albeit modest, im-provements. [Footnote_7] When, instead of ALINE, LCSR is used in conjunction with the semantic methods, the average precision numbers are lower by over 10 per-centage points."
"7 The curve for Method K, which would be slightly below the curve for Method W, is omitted for clarity."
Figure 4 illustrates the effect of varying the set-ting of the parameter α on the average precision of COGIT when ALINE is used in conjunction with full semantic analysis.
"The greater the value of α, the more weight is given to the semantic score, so α 0 implies that the semantic information is ig-nored."
The optimal value of α for both the devel-opment and the test sets is close to 0.2.
"With α ap-proaching 1, the role of the phonetic score is increas-ingly limited to ordering candidate pairs within se-mantic similarity levels."
Average precision plum-mets to 0.161 when α is set to 1 and hence no pho-netic score is available.
The rightmost column in Table 3 in Section 5 compares proportions of all cognate pairs in the data that are covered by individual semantic similarity levels.
The cases in which the existence of a WordNet relation influences the value of the similarity score account for less than 10% of the cognate pairs.
"In particular, instances of meronymy between cognates are very rare."
"Apart from the limited coverage of WordNet-related semantic similarity levels, there are other reasons for the relatively small contribution of WordNet to the overall performance of the system."
"First, even after preprocessing that includes check-ing the spelling, lemmatization, and stop word re-moval, many of the glosses are not in a form that can be recognized by WordNet."
"These include compounds written as a single word (e.g. ‘snow-shoe’), and rare words (e.g. ‘spawner’) that are not in WordNet."
"Second, when many words have sev-eral meanings that participate in different synsets, the senses detected to be related are not necessarily the senses used in the glosses."
"For example, ‘star’ and ‘lead’ share a synset (“an actor who plays a prin-cipal role”), but in the Algonquian vocabularies both words are always used in their most literal sense."
Only in the case of complete identity of glosses can the lexemes be assumed to be synonymous in all senses.
"Finally, since the data for all Algonquian languages originates from a single project, it is quite homogeneous."
"As a result, many glosses match per-fectly within cognate sets, which limits the need for application of WordNet lexical relations."
"The performance figures are adversely affected by the presence of the usual “noise”, which is unavoid- able in the case of authentic data."
Manual prepara-tion of the vocabulary lists would undoubtedly result in better performance.
"However, because of its size, only limited automatic validation of the data had been performed."
It should also be noted that exam-ination of apparent false positives sometimes leads to discovering true cognates that are not identified as such in Hewson’s dictionary.
"One interesting exam-ple is Cree pı̄sākanāpiy ‘rope, rawhide thong’, and Ojibwa pı̄ššākaniyāp ‘string’."
In this case COGIT detected the synonymy of the glosses by consulting WordNet.
The results show that it is possible to identify a large portion of cognates in related languages with-out explicit knowledge of systematic sound corre-spondences between them or phonological changes that they have undergone.
This is because cognates on average display higher phonetic and semantic similarity than words that are unrelated.
Many vo-cabulary entries can be classified as cognates solely on the basis of their phonetic similarity.
"ALINE, a sophisticated algorithm based on phonological fea-tures, is more successful at this task than simple “or-thographic” measures."
Analysis of semantic infor-mation extracted from glosses yields a dramatic in-crease in the number of identified cognates.
Most of the improvement comes from detecting entries that have matching glosses.
"On the other hand, the con-tribution of WordNet is small."
A system such as COGIT can be of assistance for comparative linguists dealing with large vocabulary data from languages with which they are unfamil-iar.
It can also serve as one of the principal mod-ules of a language reconstruction system.
"However, in spite of the fact that the main focus of this paper is diachronic phonology, the techniques and findings presented here may also be applicable in other con-texts where it is necessary to identify cognates, such as bitext alignment."
"Using finite-state automata for the text analysis component in a text-to-speech system is problem-atic in several respects: the rewrite rules from which the automata are compiled are difficult to write and maintain, and the resulting automata can become very large and therefore inefficient."
Converting the knowledge represented explicitly in rewrite rules into a more efficient format is difficult.
"We take an indirect route, learning an efficient decision tree rep-resentation from data and tapping information con-tained in existing rewrite rules, which increases per-formance compared to learning exclusively from a pronunciation lexicon."
"Text-to-speech (TTS) systems, like any other piece of sophisticated software, suffer from the shortcom-ings of the traditional software development pro-cess."
"Highly skilled developers are a costly re-source, the complexity and sheer size of the code involved are difficult to manage."
A paradigmatic example of this is the letter-to-sound component within the text analysis module of a mature large-scale text-to-speech system.
In the system described[REF_CITE]text analysis is performed using finite-state transducers compiled from rewrite rules[REF_CITE]and other high-level descriptions.
"While the exclu-sive use of finite-state technology has advantages, it is not without its shortcomings, both technical and stemming from the use of hand-crafted rule sets and how they are represented: 1."
"Extensive rule sets need to be constructed by human experts, which is labor-intensive and expensive[REF_CITE]. 2."
Realistic rule sets are difficult to maintain be-cause of complex interactions between serially composed rules. 3.
"Although rewrite rules can, in principle, be compiled into a huge monolithic transducer that is then very time-efficient, in practice this is not feasible because of the enormous sizes of the resulting machines (cf. the numbers given[REF_CITE]and ([REF_CITE]74)). 4."
"For reasons of space efficiency, certain com-putations are deferred until run-time[REF_CITE], with a significant impact on time efficiency."
"While there is a clear need for human expert knowledge ([REF_CITE]75ff.), those experts should not have to deal with the performance as-pects of the knowledge representation."
Ideally we would like to use a knowledge representation that is both time and space efficient and can be con-structed automatically from individually meaning-ful features supplied by human experts.
"For practi-cal reasons we have to be content with methods that address the efficiency issues and can make use of explicitly represented knowledge from legacy sys-tems, so that moving to a new way of building TTS systems does not entail starting over from scratch."
As a case study of how this transition might be achieved we took the letter-to-phoneme rules for French in the TTS system described[REF_CITE]and proceeded to 1. Construct a lexicon using the existing system. 2. Produce an alignment for that lexicon. 3. Convert the aligned lexicon into training in-stances for an automatically induced classifier. 4. Train and evaluate decision trees.
By running the existing system on a small news-paper corpus (ca. 1M words of newspaper text from Le Monde) and eliminating abbreviations we ob-tained a lexicon of about 18k words.
This means that the performance of the automatically trained system built from this lexicon is relative to the ex-isting system.
"The key steps, aligning the lexicon and building a training set, are described in detail in Sections 2 and 3 below."
"Our choice of decision trees was motivated by their following desirable properties: 1. Space and time efficiency, provided the feature functions can be represented and computed ef-ficiently, which they can be in our case. 2."
Symbolic representation that can easily be in-spected and converted.
"The first property addresses the efficiency re-quirements stated above: if every feature function can be computed in time O(f), where the function f does not involve the height of the decision tree h, then the classification function represented by the decision tree can be computed in time O(λn. h × f(n)) ="
"O(f) if feature values can be mapped to child nodes in constant time, e. g. through hashing; and similarly for space."
The other properties justify the use of decision trees as a knowledge representation format.
"In par-ticular, decision trees can be converted into im-plicational rules that an expert could inspect and can in principle be compiled back into finite-state machines[REF_CITE], although that would re-introduce the original efficiency problems."
"On the other hand, finite-state transducers have the advantage of being invertible, which can be ex-ploited e. g. for testing hand-crafted rule sets."
"We use a standard decision tree learner[REF_CITE], since we believe that it would be pre-mature to investigate the implications of different choices of machine learning algorithms while the fundamental question of what any such algorithm should use as training data is still open."
This topic is explored further in Section 5.
Related work is discussed in Section 6.
Learning a mapping between sets of strings is dif-ficult unless the task is suitably restricted or addi-tional supervision is provided.
Aligning the lexicon allows us to transform the learning task into a clas-sification task to which standard machine learning techniques can be applied.
Given a lexical entry we ideally would want to align each letter with zero or more phonemes in a way that minimizes the descriptions of the func-tion performing the mapping and of the exceptions.
"Since we do not know how to do this efficiently, we chose to be content with an alignment produced by the first phase of the algorithm described[REF_CITE]: we treat the strings to be aligned as bags of symbols, count all possible com-binations, and use this to estimate the parameters for a zeroth-order Markov model."
"Figure 1 shows two examples of an alignment, where the dot represents the empty string (for rea-sons of visual clarity), also referred to as ε."
"Align-ment (b), while not as intuitively plausible as align-ment (a), is possible as an extreme case."
"In gen-eral, when counting the combinations of ` letters with p phonemes, we want to include p empty let-ters and ` empty phonemes."
"For example, given the letters ‘texte’ and corresponding phonemes /tEkst/, we count C L (t, ε) = 10, C L (t, t) = 4, C L (t, k) = 2, etc."
By normalizing the counts we arrive at an em-pirical joint probability distribution P̃ L for the lexi-con.
The existing rewrite rules were another source of information.
A rewrite rule is of the form φ → ψ / λ ρ where φ is usually a string of letters and ψ a string of phonemes.
The contextual restrictions expressed by λ and ρ will be ignored.
"Typically φ and ψ are very short, rarely consisting of more than four symbols."
"We created a second lexicon consisting of around 200 pairs hφ, ψi mentioned in the rewrite rules, and applied the same procedure as before to obtain counts C R and from those a joint probability distribution P̃ R ."
The two empirical distributions were combined and smoothed by linear interpolation with a uniform distribution P U :
"P (x, y) = λ 1 P̃ R (x, y) + λ 2 P̃ L (x, y) + λ 3 P U (x, y) where each λ i ≥ 0 and λ 1 + λ 2 + λ 3 = 1."
The effects of using different coefficient vectors λ~ will be discussed in Section 4.
"Since we had available a library for manipulating weighted automata[REF_CITE], the align-ments were computed by using negative log proba-bilities as weights for a transducer with a single state (hence equivalent to a zeroth-order Markov model), composing on the left with the letter string and on the right with the phoneme string, and finding the best path[REF_CITE]."
"This amounts to inserting ε-symbols into both the string of letters and the string of phonemes in a way that minimizes the overall weight of the transduction, i. e. maximizes the probability of the alignment with respect to the model."
Now we bring in additional restrictions that allow us to express the task of finding a function that maps letter sequences to phoneme sequences as the sim-pler task of inducing a mapping from a single letter to a single phoneme.
"This is a standard classifica-tion task, and once we have a set of feature func-tions and training instances we can choose from a multitude of learning algorithms and target repre-sentations."
"However, investigating the implications of different choices is not our goal."
The first simplifying assumption is to pretend that translating an entire text amounts to translating each word in isolation (but see the discussion of liaison in Section 5 below).
"Secondly we make use of the fact that the pronunciation of a letter is in most cases fully determined by its local context, much more so in French[REF_CITE]than in English."
"Each letter is to be mapped to a phoneme, or the empty string ε, in the case of “silent” letters (dele-tions)."
"An additional mechanism is needed for those cases where a letter corresponds to more than one phoneme (insertions), e. g. the letter ‘x’ correspond-ing to the phonemes /ks/ in Figure 2a."
The problem is the non-uniform appearance of an explicit empty string symbol that allows for insertions.
We avoided having to build a separate classifier to predict these insertion points (see[REF_CITE]in the context of pronunciation modeling) by simply pretending that an explicit empty string is present before each letter and after the last letter.
This is illustrated in Fig-ure 2b.
Visual inspection of several aligned lexica revealed that at most one empty string symbol is needed between any two letters.
From these aligned and padded strings we derived training instances by considering local windows of a fixed size.
"A context of size one requires a win- dow of size three, which is centered on the letter aligned with the target phoneme."
Figure 3 shows the first few training instances derived from the ex-ample in Figure 2b above.
The beginning and end of the string are marked with a special symbol.
"Note that the empty string symbol only appears in the center of the window, never in the contextual part, where it would not convey any information."
"We delineated a 90%/10% split of the lexicon and performed the alignment using a probability distri-bution with coefficients λ 1 = 0, λ 2 = 0.9, and λ 3 = 0.1, i.e., no information from the rewrite rules was used and the empirical probabilities de-rived from the lexicon were smoothed slightly."
The value for λ 3 was determined empirically after sev-eral trial runs on a held-out portion.
"We then gener-ated training instances as described in the previous section, and set aside the 10% we had earmarked earlier for testing purposes."
"We ran C4.5 on the re-maining portion of the data, using the held out 10% for testing."
Table 1 summarizes the following as-pects of the performance of the induced decision tree classifiers on the test data relative to the size of context used for classification: classification accu-racy per symbol; micro-averaged precision (P) and recall (R) per symbol; size of the tree in number of nodes; and size of the saved tree data in kilobytes.
All trees were pruned and the subsetting option of C4.5 was used to further reduce the size of the trees.
Further increasing the context size did not result in better performance.
"We did see a performance in- crease, however, when we repeated the above proce-dure with different coefficients ~λ."
"This time we set λ 1 = 0.9, λ 2 = 0.09, and λ 3 = 0.01."
These partic-ular values were again determined empirically.
"The important thing to note is that the information from the rewrite rules is now dominant, as compared to before when it was completely absent."
The effect this had on performance is summarized in Table 2 for three letters of context.
"As before, classification accuracy is given on a per-symbol basis; average ac-curacy per word is around 85%."
Notice that the size of the tree decreases as a result of a better alignment.
These figures are all relative to our existing sys-tem.
"What is most important to us are the vast im-provements in efficiency: the decision trees take up less than 10% of the space of the original letter-to-phoneme component, which weighs in at 6.7 MB total with composition deferred until runtime, since off-line composition would have resulted in an im-practically large machine."
"The size of the origi-nal component could be reduced through the use of compression techniques[REF_CITE], which would lead to an additional run-time overhead."
"Classification speed of the decision trees is on the order of several thousand letters per second (de-pending on platform details), which is many times faster than the existing system."
"The exact details of a speed comparison depend heavily on platform is-sues and what one considers to be the average case, but a conservative estimate places the speedup at a factor of 20 or more."
The tremendous gains in efficiency will enable us to investigate the use of additional processing mod-ules that are not included in the existing system be-cause they would have pushed performance below an acceptable bound.
"For example no sophisticated part-of-speech (POS) disambiguation is done at the moment, but would be needed to distinguish, e. g., between different pronunciations of French words ending in -ent, which could be verbs, nouns, ad-verbs, etc."
"The need for POS disambiguation is even clearer for languages with “deep” orthogra-phies, such as English."
"In conjunction with shallow parsing, POS disambiguation would give us enough information to deal with most cases of liaison, an inter-word phenomenon that required special atten-tion in the existing system and that we have so far ignored in the new approach because of the exclu-sive focus on regularities at the level of isolated words."
"We have been using the existing automaton-based system as our baseline, which is unfair because that system makes mistakes which could very well obscure some regularities the inductive approach might otherwise have discovered."
"Future compar-isons should use an independent gold standard, such as a large dictionary, to evaluate and compare both approaches."
The advantage of using the existing system instead of a dictionary is that we could gen-erate large amounts of training data from corpora.
"But even with plenty of training data available, the paradigms of verbal inflections, for example, are quite extensive in French, inflected verb forms are typically not listed in a dictionary, and we can-not guarantee that sufficiently many forms appear in a corpus to guarantee full coverage."
In this case it would make sense to use a hybrid approach that reuses the explicit representations of verbal inflec-tions from the existing system.
"More importantly, having more training data available for use with our new approach would only help to a small extent."
"Though more and/or cleaner data would possibly result in better align-ments, we do not expect to find vast improvements unless the restriction imposed by the zeroth-order Markov assumption used for alignment is dropped, which could easily be done."
"However, it is not clear that using a bigram or trigram model for alignment would optimize the alignment in such a way that the decision tree classifier learned from the aligned data is as small and accurate as possible."
"This points to a fundamental shortcoming of the usual two-step procedure, which we followed here: the goodness of an alignment performed in the first step should be determined by the impact it has on producing an optimal classifier that is induced in the second step."
"However, there is no provision for feedback from the second step to the first step."
For this a different setup would be needed that would discover an optimal alignment and classifier at the same time.
"This, to us, is one of the key research questions yet to be addressed in learning letter-to-sound rules, since the quality of an alignment and hence the training data for a classifier learner is es-sential for ensuring satisfactory performance of the induced classifier."
The question of which classifier (learner) to use is secondary and not necessarily spe-cific to the task of learning letter–sound correspon-dences.
"The problem of letter-to-sound conversion is very similar to the problem of modeling pronuncia-tion variation, or phonetic/phonological model-ing[REF_CITE]."
"For pronunciation modeling where alternative pronunciations are generated from known forms one can use standard similarity met-rics for strings (Hamming distance, Levenshtein distance, etc.), which are not meaningful for map-pings between sequences over dissimilar alphabets, such as letter-to-phoneme mappings."
General techniques for letter-to-phoneme con-version need to go beyond dictionary lookups and should be able to handle all possible written word forms.
"Since the general problem of learning reg-ular mappings between regular languages is in-tractable because of the vast hypothesis space, all existing research on automatic methods has im-posed restrictions on the class of target functions."
"In almost all cases, this paper included, one only con-siders functions that are local in the sense that only a fixed amount of context is relevant for mapping a letter to a phoneme."
"One exception to this is[REF_CITE], where the target function space are the subse-quential transducers, for which a limit-identification algorithm exists[REF_CITE]."
"However, without additional guidance, that algorithm cannot be directly applied to the phonetic modeling task due to data sparseness and/or lack of sufficient bi[REF_CITE]."
We would argue that the lack of locality restrictions is at the root of the convergence problems for that approach.
"Our approach effectively restricts the hypothe-sis space even further to include only the k-local (or strictly k-testable) sequential transducers, where a classification decision is made deterministically and based on a fixed amount of context."
We con-sider this to be a good target since we would like the letter-to-sound mapping to be a function (every piece of text has exactly one contextually appropri-ate phonetic realization) and to be deterministically computable without involving any kind of search.
Locality gives us enough bias for efficiently learn-ing classifiers with good performance.
"Since we are dealing with a restricted subclass of finite-state transducers, our approach is, at a theoretical level, fully consistent with the claim[REF_CITE]that letter–phoneme correspondences can be expressed as regular relations."
"However, it must be stressed that just because something is finite-state does not mean it should be implemented directly as a finite-state automaton."
Other machine learning approaches employ es-sentially the same locality restrictions.
"Different learning algorithms can be used, including Artificial neural networks[REF_CITE], decision tree learners[REF_CITE], memory-based learners and hybrid symbolic approaches (Van den[REF_CITE]; Daelemans and van den[REF_CITE]), or Markov models."
"Out of these the approach[REF_CITE]is most similar to ours, but it presupposes that phoneme strings are never longer than the cor-responding letter strings, which is mostly true, but has systematic exceptions, e.g. ‘exact’ in English or French."
"English has many more exceptions that do not involve the letter ‘x’, such as ‘cubism’ (/kju-bIz@m/ according to cmudict.0.6) or ‘mutual-sim’."
The problem of finding a good alignment has not received its due attention in the literature.
Work on multiple alignments in computational biology cannot be adapted directly because the letter-to-sound mapping is between dissimilar alphabets.
"The alignment problem in statistical machine trans-lati[REF_CITE]is too general: long-distance displacement of large chunks of material may occur frequently when translating whole sen-tences, but are unlikely to play any role for the letter-to-sound mapping, though local reorderings do occur[REF_CITE]."
"Ad hoc figures of merit for alignments (Daelemans and van den[REF_CITE]) or hand-corrected alignments[REF_CITE]might give good results in practice, but do not get us any closer to a principled solution."
The present work is another step towards obtaining better align-ments by exploiting easily available knowledge in a systematic fashion.
"We presented a method for building efficient letter-to-sound rules from information extractable from, or with the help of, existing hand-crafted rewrite rules."
"Using decision trees as the new target rep-resentation, significant improvements in time and space efficiency could be achieved at the cost of a reduction in accuracy."
Our approach relies on finding an alignment between strings of letters and phonemes.
We identified a way to improve align-ments and argued that finding a good alignment is crucial for success and should receive more atten-tion.
"We present a simple architecture for parsing transcribed speech in which an edited-word de-tector first removes such words from the sen-tence string, and then a standard statistical parser trained on transcribed speech parses the remaining words."
"The edit detector achieves a misclassification rate on edited words of 2.2%. (The NULL -model, which marks everything as not edited, has an error rate of 5.9%.)"
"To evalu-ate our parsing results we introduce a new eval-uation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes."
By this metric the parser achieves 85.3% precision and 86.5% recall.
"While significant effort has been expended on the parsing of written text, parsing speech has received relatively little attention."
"The comparative neglect of speech (or transcribed speech) is understandable, since parsing tran-scribed speech presents several problems absent in regular text: “um”s and “ah”s (or more formally, filled pauses), frequent use of par-entheticals (e.g., “you know”), ungrammatical constructions, and speech repairs (e.g., “Why didn’t he, why didn’t she stay home?”)."
In this paper we present and evaluate a simple two-pass architecture for handling the problems of parsing transcribed speech.
"The first pass tries to identify which of the words in the string are edited (“why didn’t he,” in the above exam-ple)."
"These words are removed from the string given to the second pass, an already existing sta-tistical parser trained on a transcribed speech corpus. (In particular, all of the research in this paper was performed on the parsed “Switch-board” corpus as provided by the Linguistic Data Consortium.)"
This architecture is based upon a fundamen-tal assumption: that the semantic and prag-matic content of an utterance is based solely on the unedited words in the word sequence.
This assumption is not completely true.
"For example, Core and Schubert [8] point to coun-terexamples such as “have the engine take the oranges to Elmira, um, I mean, take them to Corning” where the antecedent of “them” is found in the EDITED words."
"However, we be-lieve that the assumption is so close to true that the number of errors introduced by this assump-tion is small compared to the total number of errors made by the system."
In order to evaluate the parser’s output we compare it with the gold-standard parse trees.
For this purpose a very simple third pass is added to the architecture: the hypothesized edited words are inserted into the parser output (see Section 3 for details).
"To the degree that our fundamental assumption holds, a “real” ap-plication would ignore this last step."
This architecture has several things to recom-mend it.
"First, it allows us to treat the editing problem as a pre-process, keeping the parser un-changed."
"Second, the major clues in detecting edited words in transcribed speech seem to be relatively shallow phenomena, such as repeated word and part-of-speech sequences."
"The kind of information that a parser would add, e.g., the node dominating the EDITED node, seems much less critical."
"Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion."
Our reasoning here is based upon what one might and might not expect from a second-pass statistical parser.
"For example, ungram-maticality in some sense is relative, so if the training corpus contains the same kind of un-grammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper."
"Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules."
"Similarly, parentheticals and filled pauses ex-ist in the newspaper text these parsers currently handle, albeit at a much lower rate."
Thus there is no particular reason to expect these construc-tions to have a major impact. 1
This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser.
It is for that rea-son that we have chosen to handle it separately.
The organization of this paper follows the ar-chitecture just described.
Section 2 describes the first pass.
We present therein a boosting model for learning to detect edited nodes (Sec-tions 2.1 – 2.2) and an evaluation of the model as a stand-alone edit detector (Section 2.3).
Section 3 describes the parser.
"Since the parser is that already reported in [3], this section sim-ply describes the parsing metrics used (Section 3.1), the details of the experimental setup (Sec-tion 3.[Footnote_2]), and the results (Section 3.3)."
"2 It turns out that many pairs of features are exten-sionally equivalent, i.e., take the same values on each"
The Switchboard corpus annotates disfluencies such as restarts and repairs using the terminol-ogy of Shriberg [15].
"The disfluencies include repetitions and substitutions, italicized in (1a) and (1b) respectively. (1) a."
"I really, I really like pizza. b. Why didn’t he, why didn’t she stay home?"
"Restarts and repairs are indicated by disfluency tags ‘[’, ‘+’ and ‘]’ in the disfluency POS-tagged Switchboard corpus, and by EDITED nodes in the tree-tagged corpus."
"This section describes a procedure for automatically identifying words corrected by a restart or repair, i.e., words that are dominated by an EDITED node in the tree-tagged corpus."
"This method treats the problem of identify-ing EDITED nodes as a word-token classification problem, where each word token is classified as either edited or not."
The classifier applies to words only; punctuation inherits the classifica-tion of the preceding word.
A linear classifier trained by a greedy boosting algorithm [16] is used to predict whether a word token is edited.
Our boosting classifier is directly based on the greedy boosting algorithm described by Collins [7].
This paper contains important implemen-tation details that are not repeated here.
We chose Collins’ algorithm because it offers good performance and scales to hundreds of thou-sands of possible feature combinations.
This section describes the kinds of linear clas-sifiers that the boosting algorithm infers.
"Ab-stractly, we regard each word token as an event characterized by a finite tuple of random vari-ables (Y, X 1 , . . . , X m )."
"Y is the the conditioned variable and ranges over {−1,+1}, with Y = +[Footnote_1] indicating that the word is not edited."
"1 Indeed, [17] suggests that filled pauses tend to indi-cate clause boundaries, and thus may be a help in pars-ing."
"X 1 , . .. , X m are the con-ditioning variables; each X j ranges over a finite set X j ."
"For example, X 1 is the orthographic form of the word and X 1 is the set of all words observed in the training section of the corpus."
Our classifiers use m = 18 conditioning vari-ables.
"The following subsection describes the conditioning variables in more detail; they in-clude variables indicating the POS tag of the preceding word, the tag of the following word, whether or not the word token appears in a “rough copy” as explained below, etc."
"The goal of the classifier is to predict the value of Y given values for X 1 , . . . , X m ."
The classifier makes its predictions based on the oc-curence of combinations of conditioning vari-able/value pairs called features.
"A feature F is a set of variable-value pairs hX j ,x j i, with x j ∈ X j ."
"Our classifier is defined in terms of a finite number n of features F 1 , . . . , F n , where n ≈ 10 6 in our classifiers. 2"
Each feature F i de- fines an associated random boolean variable
"Y F i = (X j =x j ), hX j ,x j i∈F i where (X=x) takes the value 1 if X = x and 0 otherwise."
"That is, F i = 1 iff X j = x j for all hX j , x j i ∈ F i ."
"Our classifier estimates a feature weight α i for each feature F i , that is used to define the pre-diction variable Z:"
X n Z = α i F i . i=1
The prediction made by the classifier is sign(Z) =
"Z/|Z|, i.e., −1 or +1 depending on the sign of Z."
"Intuitively, our goal is to adjust the vector of feature weights α~ = (α 1 , . . . , α n ) to minimize the expected misclassification rate E[(sign(Z) 6= Y)]."
"This function is difficult to minimize, so our boosting classifier minimizes the ex-pected Boost loss E[exp(−Y Z)]."
"As Singer and Schapire [16] point out, the misclassification rate is bounded above by the Boost loss, so a low value for the Boost loss implies a low mis-classification rate."
"Our classifier estimates the Boost loss as E b t [exp(−YZ)], where E b t [·] is the expectation on the empirical training corpus distribution."
The feature weights are adjusted iteratively; one weight is changed per iteration.
The fea-ture whose weight is to be changed is selected greedily to minimize the Boost loss using the algorithm described in [7].
"Training contin-ues for 25,000 iterations."
"After each iteration the misclassification rate on the development corpus E b d [(sign(Z) 6= Y )] is estimated, where E b d [·] is the expectation on empirical develop-ment corpus distribution."
"While each iteration lowers the Boost loss on the training corpus, a graph of the misclassification rate on the de-velopment corpus versus iteration number is a noisy U-shaped curve, rising at later iterations due to overlearning."
"The value of α~ returned by the estimator is the one that minimizes the misclassficiation rate on the development cor-pus; typically the minimum is obtained after about 12,000 iterations, and the feature weight vector ~α contains around 8000 nonzero feature weights (since some weights are adjusted more than once). [Footnote_3]"
"3 We used a smoothing parameter as described in [7], which we estimate by using a line-minimization rou-tine to minimize the classifier’s minimum misclassifica-tion rate on the development corpus."
This subsection describes the conditioning vari-ables used in the EDITED classifier.
Many of the variables are defined in terms of what we call a rough copy.
"Intuitively, a rough copy iden-tifies repeated sequences of words that might be restarts or repairs."
"Punctuation is ignored for the purposes of defining a rough copy, al-though conditioning variables indicate whether the rough copy includes punctuation."
"A rough copy in a tagged string of words is a substring of the form α 1 βγα 2 , where: 1. α 1 (the source) and α 2 (the copy) both be-gin with non-punctuation, 2. the strings of non-punctuation POS tags of α 1 and α 2 are identical, 3. β (the free final) consists of zero or more sequences of a free final word (see below) followed by optional punctuation, and 4. γ (the interregnum) consists of sequences of an interregnum string (see below) followed by optional punctuation."
"The set of free-final words includes all partial words (i.e., ending in a hyphen) and a small set of conjunctions, adverbs and miscellanea, such as and, or, actually, so, etc."
"The set of interreg-num strings consists of a small set of expressions such as uh, you know, I guess, I mean, etc."
"We search for rough copies in each sentence start-ing from left to right, searching for longer copies first."
"After we find a rough copy, we restart searching for additional rough copies following the free final string of the previous copy."
We say that a word token is in a rough copy iff it appears in either the source or the free final. [Footnote_4] (2) is an example of a rough copy.
"4 In fact, our definition of rough copy is more complex. For example, if a word token appears in an interregnum"
Table 1 lists the conditioning variables used in our classifier.
"In that table, subscript inte-gers refer to the relative position of word to-kens relative to the current word; e.g. T 1 is the POS tag of the following word."
The sub-script f refers to the tag of the first word of the free final match.
"If a variable is not defined for a particular word it is given the special value ‘ NULL ’; e.g., if a word is not in a rough copy then variables such as N m , N u , N i , N l , N r and T f all take the value NULL ."
"Flags are boolean-valued variables, while numeric-valued variables are bounded to a value between 0 and 4 (as well as NULL , if appropriate)."
"The three variables C t , C w and T i are intended to help the classifier capture very short restarts or repairs that may not involve a rough copy."
The flags C t and C i indicate whether the orthographic form and/or tag of the next word (ignoring punctuation) are the same as those of the current word.
T i has a non- NULL value only if the current word is followed by an interregnum string; in that case T i is the POS tag of the word following that interregnum.
"As described above, the classifier’s features are sets of variable-value pairs."
"Given a tuple of variables, we generate a feature for each tuple of values that the variable tuple assumes in the training data."
"In order to keep the feature set managable, the tuples of variables we consider are restricted in various ways."
"The most impor-tant of these are constraints of the form ‘if X j is included among feature’s variables, then so is X k ’."
"For example, we require that if a fea-ture contains P i+1 then it also contains P i for i ≥ 0, and we impose a similiar constraint on POS tags."
"For the purposes of this research the Switch-board corpus, as distributed by the Linguistic Data Consortium, was divided into four sections (or subcorpora)."
The training subcorpus con-sists of all files in the directories 2 and 3 of the parsed/merged Switchboard corpus.
Directory 4 is split into three approximately equal-size sec-tions. (Note that the files are not consecutively numbered.)
The first of these (files sw4004.mrg to sw4153.mrg) is the testing corpus.
All edit detection and parsing results reported herein are from this subcorpus.
The files sw4154.mrg to sw4483.mrg are reserved for future use.
The files sw4519.mrg to sw4936.mrg are the devel-opment corpus.
In the complete corpus three parse trees were sufficiently ill formed in that our tree-reader failed to read them.
"These trees received trivial modifications to allow them to be read, e.g., adding the missing extra set of parentheses around the complete tree."
"We trained our classifier on the parsed data files in the training and development sections, and evaluated the classifer on the test section."
Section 3 evaluates the parser’s output in con-junction with this classifier; this section focuses on the classifier’s performance at the individual word token level.
"In our complete application, the classifier uses a bitag tagger to assign each word a POS tag."
"Like all such taggers, our tag-ger has a nonnegligible error rate, and these tag-ging could conceivably affect the performance of the classifier."
"To determine if this is the case, we report classifier performance when trained both on “Gold Tags” (the tags assigned by the human annotators of the Switchboard corpus) and on “Machine Tags” (the tags assigned by our bitag tagger)."
"We compare these results to a baseline “null” classifier, which never identi-fies a word as EDITED ."
Our basic measure of performance is the word misclassification rate (see Section 2.1).
"However, we also report pre-cision and recall scores for EDITED words alone."
"All words are assigned one of the two possible labels, EDITED or not."
"However, in our evalua-tion we report the accuracy of only words other than punctuation and filled pauses."
"Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation [3,5, 6] on the grounds that its placement is entirely conventional."
The same can be said for filled pauses in the switchboard corpus.
Our results are given in Table 2.
"They show that our classifier makes only approximately 1/3 of the misclassification errors made by the null classifier (0.022 vs. 0.059), and that using the POS tags produced by the bitag tagger does not have much effect on the classifier’s perfor-mance (e.g., EDITED recall decreases from 0.678 to 0.668)."
"We now turn to the second pass of our two-pass architecture, using an “off-the-shelf” statistical parser to parse the transcribed speech after hav-ing removed the words identified as edited by the first pass."
We first define the evaluation metric we use and then describe the results of our experiments.
In this section we describe the metric we use to grade the parser output.
As a first desider-atum we want a metric that is a logical exten-sion of that used to grade previous statistical parsing work.
"We have taken as our starting point what we call the “relaxed labeled preci-sion/recall” metric from previous research (e.g. [3,5])."
This metric is characterized as follows.
For a particular test corpus let N be the total number of nonterminal (and non-preterminal) constituents in the gold standard parses.
"Let M be the number of such constituents returned by the parser, and let C be the number of these that are correct (as defined below)."
Then pre-cision = C/M and recall = C/N.
"A constituent c is correct if there exists a con-stituent d in the gold standard such that: 1. label(c) = label(d) 5 5 For some reason, starting with [12] the labels ADVP 2. begin(c) ≡ r begin(d) 3. end(c) ≡ r end(d)"
In 2 and 3 above we introduce an equivalence relation ≡ r between string positions.
We define ≡ r to be the smallest equivalence relation sat-isfying a ≡ r b for all pairs of string positions a and b separated solely by punctuation symbols.
"The parsing literature uses ≡ r rather than = because it is felt that two constituents should be considered equal if they disagree only in the placement of, say, a comma (or any other se-quence of punctuation), where one constituent includes the punctuation and the other excludes it."
"Our new metric, “relaxed edited labeled preci-sion/recall” is identical to relaxed labeled preci-sion/recall except for two modifications."
"First, in the gold standard all non-terminal subcon-stituents of an EDITED node are removed and the terminal constituents are made immediate children of a single EDITED node."
"Furthermore, two or more EDITED nodes with no separating non-edited material between them are merged into a single EDITED node."
We call this version a “simplified gold standard parse.”
All precision recall measurements are taken with respected to the simplified gold standard.
"Second, we replace ≡ r with a new equiva-lence relation ≡ e which we define as the smallest equivalence relation containing ≡ r and satisfy-ing begin(c) ≡ e end(c) for each EDITED node c in the gold standard parse. [Footnote_6] and PRT are considered to be identical as well."
6 We considered but ultimately rejected defining ≡ e using the EDITED nodes in the returned parse rather
We give a concrete example in Figure 1.
"The first row indicates string position (as usual in parsing work, position indicators are between words)."
The second row gives the words of the sentence.
Words that are edited out have an “E” above them.
The third row indicates the equivalence relation by labeling each string posi-tion with the smallest such position with which it is equivalent.
There are two basic ideas behind this defini-tion.
"First, we do not care where the EDITED nodes appear in the tree structure produced by the parser."
"Second, we are not interested in the fine structure of EDITED sections of the string, just the fact that they are EDITED ."
That we do care which words are EDITED comes into our figure of merit in two ways.
"First, (non-contiguous) EDITED nodes remain, even though their substructure does not, and thus they are counted in the precision and recall numbers."
"Secondly (and probably more importantly), fail-ure to decide on the correct positions of edited nodes can cause collateral damage to neighbor-ing constituents by causing them to start or stop in the wrong place."
"This is particularly rele-vant because according to our definition, while the positions at the beginning and ending of an edit node are equivalent, the interior positions are not (unless related by the punctuation rule). than the simplified gold standard."
We rejected this be-cause the ≡ e relation would then itself be dependent
See Figure 1.
The parser described in [3] was trained on the Switchboard training corpus as specified in sec-tion 2.1.
The input to the training algorithm was the gold standard parses minus all EDITED nodes and their children.
We tested on the Switchboard testing sub-corpus (again as specified in Section 2.1).
All parsing results reported herein are from all sen-tences of length less than or equal to 100 words and punctuation.
When parsing the test corpus we carried out the following operations: 1. create the simplified gold standard parse by removing non-terminal children of an EDITED node and merging consecutive EDITED nodes. 2. remove from the sentence to be fed to the parser all words marked as edited by an edit detector (see below). 3. parse the resulting sentence. 4. add to the resulting parse EDITED nodes containing the non-terminal symbols re-moved in step 2.
The nodes are added as high as possible (though the definition of equivalence from Section 3.1 should make the placement of this node largely irrele-vant). 5. evaluate the parse from step 4 against the simplified gold standard parse from step 1.
"We ran the parser in three experimental sit-uations, each using a different edit detector in step 2."
In the first of the experiments (labeled “Gold Edits”) the “edit detector” was simply the simplified gold standard itself.
This was to see how well the parser would do it if had perfect information about the edit locations.
"In the second experiment (labeled “Gold Tags”), the edit detector was the one described in Section 2 trained and tested on the part-of-speech tags as specified in the gold standard trees."
Note that the parser was not given the gold standard part-of-speech tags.
We were in-terested in contrasting the results of this experi-ment with that of the third experiment to gauge what improvement one could expect from using a more sophisticated tagger as input to the edit detector.
In the third experiment (“Machine Tags”) we used the edit detector based upon the machine generated tags.
The results of the experiments are given in Table 3.
The last line in the figure indicates the performance of this parser when trained and tested on Wall Street Journal text [3].
It is the “Machine Tags” results that we consider the “true” capability of the detector/parser combi-nation: 85.3% precision and 86.5% recall.
The general trends of Table 3 are much as one might expect.
Parsing the Switchboard data is much easier given the correct positions of the EDITED nodes than without this information.
"The difference between the Gold-tags and the Machine-tags parses is small, as would be ex-pected from the relatively small difference in the performance of the edit detector reported in Section 2."
This suggests that putting significant effort into a tagger for use by the edit detec-tor is unlikely to produce much improvement.
"Also, as one might expect, parsing conversa-tional speech is harder than Wall Street Jour-nal text, even given the gold-standard EDITED nodes."
Probably the only aspect of the above num-bers likely to raise any comment in the pars-ing community is the degree to which pre-cision numbers are lower than recall.
"With the exception of the single pair reported in [3] and repeated above, no precision values in the recent statistical-parsing literature [2,3,4,5,14] have ever been lower than recall values."
Even this one exception is by only 0.1% and not sta-tistically significant.
We attribute the dominance of recall over pre-cision primarily to the influence of edit-detector mistakes.
"First, note that when given the gold standard edits the difference is quite small (0.3%)."
When using the edit detector edits the difference increases to 1.2%.
"Our best guess is that because the edit detector has high preci-sion, and lower recall, many more words are left in the sentence to be parsed."
Thus one finds more nonterminal constituents in the machine parses than in the gold parses and the precision is lower than the recall.
"While there is a significant body of work on find-ing edit positions [1,9,10,13,17,18], it is difficult to make meaningful comparisons between the various research efforts as they differ in (a) the corpora used for training and testing, (b) the information available to the edit detector, and (c) the evaluation metrics used."
"For example, [13] uses a subsection of the ATIS corpus, takes as input the actual speech signal (and thus has access to silence duration but not to words), and uses as its evaluation metric the percentage of time the program identifies the start of the in-terregnum (see Section 2.2)."
"On the other hand, [9,10] use an internally developed corpus of sen-tences, work from a transcript enhanced with information from the speech signal (and thus use words), but do use a metric that seems to be similar to ours."
"Undoubtedly the work closest to ours is that of Stolcke et al. [18], which also uses the transcribed Switchboard corpus. (How-ever, they use information on pause length, etc., that goes beyond the transcript.)"
They cate-gorize the transitions between words into more categories than we do.
"At first glance there might be a mapping between their six categories and our two, with three of theirs corresponding to EDITED words and three to not edited."
"If one accepts this mapping they achieve an er-ror rate of 2.6%, down from their NULL rate of 4.5%, as contrasted with our error rate of 2.2% down from our NULL rate of 5.9%."
"The differ-ence in NULL rates, however, raises some doubts that the numbers are truly measuring the same thing."
"There is also a small body of work on parsing disfluent sentences [8,11]."
Hindle’s early work [11] does not give a formal evaluation of the parser’s accuracy.
"The recent work of Schubert and Core [8] does give such an evaluation, but on a different corpus (from Rochester Trains project)."
"Also, their parser is not statistical and returns parses on only 62% of the strings, and 32% of the strings that constitute sentences."
Our statistical parser naturally parses all of our corpus.
Thus it does not seem possible to make a meaningful comparison between the two sys-tems.
"We have presented a simple architecture for parsing transcribed speech in which an edited word detector is first used to remove such words from the sentence string, and then a statistical parser trained on edited speech (with the edited nodes removed) is used to parse the text."
The edit detector reduces the misclassification rate on edited words from the null-model (marking everything as not edited) rate of 5.9% to 2.2%.
"To evaluate our parsing results we have intro-duced a new evaluation metric, relaxed edited labeled precision/recall."
"The purpose of this metric is to make evaluation of a parse tree relatively indifferent to the exact tree posi-tion of EDITED nodes, in much the same way that the previous metric, relaxed labeled pre-cision/recall, make it indifferent to the attach-ment of punctuation."
By this metric the parser achieved 85.3% precision and 86.5% recall.
"There is, of course, great room for improve-ment, both in stand-alone edit detectors, and their combination with parsers."
"Also of interest are models that compute the joint probabilities of the edit detection and parsing decisions — that is, do both in a single integrated statistical process."
"In automatic speech recognition (ASR) enabled applications for medical dictations, corpora of literal transcriptions of speech are critical for training both speaker independent and speaker adapted acoustic models."
Obtaining these transcriptions is both costly and time consuming.
"Non-literal transcriptions, on the other hand, are easy to obtain because they are generated in the normal course of a medical transcription operation."
This paper presents a method of automatically generating texts that can take the place of literal transcriptions for training acoustic and language models.
ATRS [Footnote_1] is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor.
1 patent pending (Serial No.: 09/487398)
We will show that (i) adapted acoustic models trained on ATRS data perform as well as or better than adapted acoustic models trained on literal transcriptions (as measured by recognition accuracy) and (ii) language models trained on ATRS data have lower perplexity than language models trained on non-literal data .
Dictation applications of automatic speech recognition (ASR) require literal transcriptions of speech in order to train both speaker independent and speaker adapted acoustic models.
Literal transcriptions may also be used to train stochastic language models that need to perform well on spontaneous or disfluent speech.
"With the exception of personal desktop systems, however, obtaining these transcriptions is costly and time consuming since they must be produced manually by humans educated for the task."
The high cost makes literal transcription unworkable for ASR applications that require adapted acoustic models for thousands of talkers as well as accurate language models for idiosyncratic natural speech.
"Non-literal transcriptions, on the other hand, are easy to obtain because they are generated in the normal course of a medical transcription operation."
It has been previously shown[REF_CITE]that the non-literal transcriptions can be successfully used in acoustic adaptation.
"However, non-literal transcriptions are incomplete."
"They exclude many utterances that commonly occur in medical dictation—filled pauses, repetitions, repairs, ungrammatical phrases, pleasantries, asides to the transcriptionist, etc."
"Depending on the talker, such material may constitute a significant portion of the dictation."
We present a method of automatically generating texts that can take the place of literal transcriptions for training acoustic and language models.
ATRS is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor.
The following sections will describe ATRS and present experimental results from language and acoustic modeling.
We will show that (i) adapted acoustic models trained on ATRS data perform as well as or better than adapted acoustic models trained on literal transcriptions (as measured by recognition accuracy) and (ii) language models trained on ATRS data have lower perplexity than language models trained on non-literal data .
Data used in the experiments comes from medical dictations.
All of the dictations are telephone speech .
The application for our work is medical dictation over the telephone.
"Medical dictation differs from other telephony based ASR applications, e.g. airline reservation systems, because the talkers are repeat users and utterances are long."
Dictations usually consist of 1-30 minutes of speech.
The talkers call in 3-5 days per week and produce between 1 and 12 dictations each day they call.
Hence a medical dictation operation has access to hours of speech for each talker.
Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech.
A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. ([REF_CITE]Stolcke et al.
Medical over-the-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse ([REF_CITE]).
"Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &quot;disfluencies&quot; characteristic of dictated speech."
"An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as several paragraphs back."
"Most ASR dictation applications focus on desktop users; for example, Dragon, IBM, Philips and Lernout &amp; Hauspie all sell desktop dictation recognizers that work on high quality microphone speech."
"Typically, the desktop system builds an adapted acoustic model if the talker &quot;enrolls&quot;, i.e. reads a prepared script that serves as a literal transcription."
Forced alignment of the script and the speech provides the input to acoustic model adaptation.
Enrollment makes it relatively easy to obtain literal transcriptions for adaptation.
"However, enrollment is not feasible for dictation over the telephone primarily because most physicians will refuse to take the time to enroll."
"The alternative is to hire humans who will type literal transcriptions of dictation until enough have been accumulated to build an adapted model, an impractical solution for a large scale operation that processes speech from thousands of talkers."
ATRS is appealing because it can generate an approximation of literal transcription that can replace enrollment scripts and the need for manually generated literal transcriptions.
"In this paper, training texts for language and acoustic models fall into three categories:"
Non-Literal: Non-literal transcripts present the meaning of what was spoken in a written form appropriate for the domain.
"In a commercial medical transcription operation, the non-literal transcript will present the dictation in a format appropriate for a medical record."
"This typically involves (i.) ignoring filled pauses, pleasantries, and repeats; (ii.) acting on directions for repairs (&quot;delete the second paragraph and put this in instead...&quot;); (iii.) adding non-dictated punctuation; (iv.) correcting grammatical errors; and (v.) re-formatting certain phrases such as &quot;Lung are Clear&quot;, to a standard form such as &quot;Lungs - Clear&quot;."
Literal transcriptions are exact transcriptions of what was spoken.
"This includes any elements not found in the non-literal transcript, such as filled pauses (um&apos;s and ah&apos;s), pleasantries and body noises (&quot;thank you very much, just a moment, cough&quot;), repeats, fragments, repairs and directions for repairs, and asides (&quot;make that bold&quot;)."
"Literal transcriptions require significant human effort, and therefore are expensive to produce."
"Even though they are carefully prepared, some errors will be present in the result."
"In their study of how humans deal with transcribing spoken discourse, Lindsay and O&apos;[REF_CITE]have found that literal transcripts were &quot;far from verbatim.&quot; (p.111) They find that the transcribers in their study tended to have the most difficulty transcribing hesitation phenomena, followed by sentence fragments, adverbs and conjunctions and, finally, nouns, verbs, adjectives and prepositions."
Our informal observations made from the transcripts produced by highly trained medical transcriptionists suggest approximately 5% error margin and a gradation of errors similar to the one found by Lindsay and O&apos;Connell.
"Semi-Literal: Semi-literal transcripts are derived using non-literal transcripts, the recognizer output, a set of grammars, a dictionary, and an interpreter to integrate the recognized material into the non-literal transcription."
"Semi-literal transcripts will more closely resemble the literal transcripts, as many of the elements missing from the non-literal transcripts will be restored."
It is well known that ASR systems perform best when acoustic models are adapted to a particular talker’s speech.
This is why commercial desktop systems use enrollment.
"Although less widely applied, language model adaptation based on linear interpolation is an effective technique for tailoring stochastic grammars to particular domains of discourse and to particular speakers ([REF_CITE],[REF_CITE])."
"The training texts used in acoustic modeling come from recognizer-generated texts, literal transcriptions or non-literal transcriptions."
"Within the family of transformation and combined approaches to acoustic modeling ([REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE]) three basic adaptation methods can be identified: unsupervised, supervised, or semi-supervised."
Each adaptation method depends on a different type of training text.
What follows will briefly introduce the three methods.
Unsupervised adaptation relies on the recognizer’s output as the text guiding the adaptation.
Efficacy of unsupervised adaptation fully depends on the recognition accuracy.
"In laboratory conditions, the errors introduced by unsupervised adaptation can be averaged out by using more data[REF_CITE]; however, in a telephony operation with degraded input that is not feasible."
Supervised adaptation is dependent on literal transcription availability and is widely used in enrollment in most desktop ASR systems.
A speaker’s speech sample is transcribed verbatim and then the speech signal is aligned with pronunciations frame by frame for each individual word.
A speaker independent model is augmented to include the observations resulting from the alignment.
Semi-supervised adaptation rests on the idea that the speech signal can be partially aligned by using of the recognition output and the non-literal transcription.
A significant problem with semi-supervised adaptation is that only the speech that the recognizer already recognizes successfully ends up being used for adaptation.
This reinforces what is already well represented in the model.
They note that background noise and speech disfluency are detrimental to the unsupervised adaptation.
"In addition to the two problems with semi-supervised adaptation pointed out by Wightman and Harder, we find one more potential problem."
"As a result of matching the word labels produced by the recognizer and the non-literal transcription, some words may be skipped which may introduce unnatural phone transitions at word boundaries."
Language model adaptation is not an appropriate domain for acoustic adaptation methods.
"However, adapted language models can be loosely described as supervised or unsupervised, based on the types of training texts—literal or non-literal—that were used in building the model."
In the following sections we will describe the system of generating data that is well suited for acoustic and language adaptation and present results of experimental evaluation of this system.
ATRS is based on reconstruction of non-literal transcriptions to train utterance specific language models.
"First, a non-literal transcription is used to train an augmented probabilistic finite state model (APFSM) which is, in turn, used by the recognizer to re-recognize the exact same utterance that the non-literal transcription was generated from."
The APFSM is constructed by linear interpolation of a finite state model where all transitional probabilities are equal to 1 with two other stochastic models.
"One of the two models is a background model that accounts for expressions such as greetings, thanking, false starts and repairs."
A list of these out-of-transcription expressions is derived by comparing already existing literal transcriptions with their non-literal transcription counterparts.
The other model represents the same non-literal transcription populated with filled pauses (FP) (“um’s and ah’s”) using a stochastic FP model derived from a relatively large corpus of literal transcriptions ([REF_CITE]). pronunciations based on the existing dictionary spelling-pronunciation alignments.
The result of interpolating these two background models is that some of the transitional probabilities found in the finite state model are no longer 1.
The language model so derived can now be used to produce a transcription that is likely to be more true to what has actually been said than the non-literal transcription that we started to work with.
Further refinement of the new semi-literal transcription is carried out by using dynamic programming alignment on the recognizer’s hypothesis (HYP) and the non-literal transcription that is used as reference (REF).
"The alignment results in each HYP label being designated as a MATCH, a DELETION, a SUBSTITUTION or an INSERTION."
Those labels present in the HYP stream that do not align with anything in the REF stream are designated as insertions and are assumed to represent the out-of-transcription elements of the dictation.
Those labels that do align but do not match are designated as substitutions.
"Finally, the labels found in the REF stream that do not align with anything in the HYP stream are designated as deletions."
Interpolation weights are established empirically by calculating the resulting model’s perplexity against held out data.
Out-of-vocabulary (OOV) items are handled provisionally by generating on-the-fly
The final semi-literal transcription is constructed differently depending on the intended purpose of the transcription.
"If the transcription will be used for acoustic modeling, then the MATCHES, the REF portion of SUBSTITUTIONS and the HYP portion of only those INSERTIONS that represent punctuation and filled pauses make it into the final semi-literal transcription."
It is important to filter out everything else because acoustic modeling is very sensitive to misalignment errors.
"Language modeling, on the other hand, is less sensitive to alignment errors; therefore, INSERTIONS and DELETIONS can be introduced into the semi-literal transcription."
One method of ascertaining the quality of semi-literal reconstruction is to measure its alignment errors against literal data using a dynamic programming application.
"By measuring the correctness spread between ATRS and literal data, as well as the correctness spread between non-literal and literal data, the ATRS alignment correctness rate was observed to be 4.4% higher absolute over 774 dictation files tested."
Chart 1 summarizes the results.
The X axis represents the number of dictations in each bin displayed along the Y axis representing the % improvement over the non-literal counterparts.
The results showed nearly all ATRS files had better alignment correctness than their non-literal counterparts.
The majority of the reconstructed dictations resemble literal transcriptions between 1% and 8% better than their non-literal counterparts.
These results are statistically significant as evidenced by a t-test at 0.05 confidence level.
Much of the increase in alignment can be attributed to the introduction of filled pauses by ATRS.
"However, ignoring filled pauses, we have observed informally that the correctness still improves in ATRS files versus non-literal."
In the following sections we will address acoustic and language modeling and show that semi-literal training data is a good substitute for literal data.
The usefulness of semi-literal transcriptions was evaluated in two ways: acoustic adaptation and language modeling.
Three speaker adapted acoustic models were trained for each of the 5 talkers in this study using the three types of label files and evaluated on the talker’s testing data.
The data collected for each talker were split into testing and training.
"All talkers are native speakers of English, two males and three females."
"Non-literal transcriptions of this data were obtained in the course of normal transcription operation where trained medical transcriptionists record the dictations while filtering out disfluency, asides and ungrammatical utterances."
Literal transcriptions were obtained by having 5 medical transcriptionists specially trained not to filter out disfluency and asides transcribe all the dictations used in this study.
Semi-literal transcriptions were obtained with the system described in section 5 of this paper.
Testing Data Three dictations (0.5 – 2 min) each were pulled out of the Literal transcriptions training set and set aside for each talker for testing.
Recognition and evaluation software and formalism
"Software licensed from Entropic Laboratory was used for performing recognition, evaluating accuracy and acoustic adaptation.[REF_CITE]."
Adapted models were trained using MLLR technique[REF_CITE]available as part of the Entropic package.
Recognition accuracy and correctness reported in this study were calculated according to the following formulas: (1) Acc = hits – insertions / total words (2) Correctness = hits / total words
The following Acoustic Models were trained via adaptation with a general SI model for each talker using all available data (except for the testing data).
Each model’s name reflects the kind of label data that was used for training.
Each audio file was recognized using SI acoustic and language models.
The recognition output was aligned with the non-literal transcription using dynamic programming.
Only those portions of audio that corresponded to direct matches in the alignment were used to produce alignments for acoustic modeling.
This method was originally used for medical dictations[REF_CITE].
Each audio file has been processed to produce a semi-literal transcription that was then aligned with recognition output generated in the process of creating semi-literal transcriptions.
The portions of the audio corresponding to matching segments were used for acoustic adaptation training.
The SI model had been trained on all available at the time (12 hours) [Footnote_2] similar medical dictations to the ones used in this study.
"2[REF_CITE]-100 hours of data for SI modeling is the industry standard, the population we are dealing with is highly homogeneous and reasonable results can be obtained with lesser amount of data."
The data for the speakers in this study were not used in training the SI model.
using Literal transcriptions yields an overall 10.84% absolute gain in correctness and 11.49% in accuracy over the baseline.
Adaptation using Non-literal transcriptions yields an overall 6.36 % absolute gain in correctness and 5.23 % in accuracy over the baseline.
Adaptation with Semi-literal transcriptions yields an overall 11.39 % absolute gain in correctness and 11.05 % in accuracy over the baseline.
No statistical significance tests were performed on this data.
The results of this experiment provide additional support for using automatically generated semi-literal transcriptions as a viable (and possibly superior) substitute for literal data.
The fact that three SEMI-LITERAL adapted AM’s out of 5 performed better than their LITERAL counterparts seems to indicate that there may be undesirable noise either in the literal transcriptions or in the corresponding audio.
It may also be due to the relatively small amount of training data used for SI modeling thus providing a baseline that can be improved with little effort.
"However, the results still indicate that generating semi-literal transcriptions may help eliminate the undesirable noise and, at the same time, get the benefits of broader coverage that semi-literal transcripts can afford over NON-LITERAL transcriptions."
"For ASR applications where there are significant discrepancies between an utterance and its formal transcription, the inclusion of literal data in the language model can reduce language model perplexity and improve recognition accuracy."
"In medical transcription, the non-literal texts typically depart from what has actually been said."
"Hence if the talker says &quot;lungs are clear&quot; or &quot;lungs sound pretty clear&quot;, the typed transcription is likely to have &quot;Lungs - clear&quot;."
"In addition, as we noted earlier, the non-literal transcription will omit disfluencies and asides and will correct grammatical errors."
Literal and semi-literal texts can be added onto language model training data or interpolated into an existing language model.
"Below we will present results of a language modeling experiment that compares language models built from literal, semi-literal and non-literal versions of the same training set."
The results substantiate our claim that automatically generated semi-literal transcription can lead to a significant improvement in language model quality.
"In order to test the proposed method’s suitability for language modeling, we constructed three trigram language models and used perplexity as the measure of the models’ goodness."
"We have described ATRS, a system for reconstructing semi-literal transcriptions automatically."
"ATRS texts can be used as a substitute for literal transcriptions when the cost and time required for generating literal transcriptions are infeasible, e.g. in a telephony based transcription operation that processes thousands of acoustic and language models."
"Texts produced with ATRS were used in training speaker adapted acoustic models, speaker independent acoustic models and language models."
Experimental results show that models built from ATRS training data yield performance results that are equivalent to those obtained with models trained on literal transcriptions.
"In the future, we will address the issue of the amount of training data for the SI model."
"Also, current ATRS system does not take advantage of various confidence scores available in leading recognition engines."
We believe that using such confidence measures can improve the generation of semi-literal transcriptions considerably.
We would also like to investigate the point at which the size of the various kinds of data used for adaptation stops making improvements in recognition accuracy.
¦&apos;G¨?/¬ §©¬/ª=§±¨c²i¤£:ª³¯ow¬/´-??/ jc©s¢¬(µ¶¥c· ©s¦&apos;ª/§¡G¨ ~/°c\= §±¨c²z½¿¾ÀÁ?/jzj¬/Â¬=/ª ?¥¤¬=Ã ª /&apos;Ã³¬=ª«¥»ª/?±¬9£ÅG9ª/?s¬/$ª³¯oÆ¬/´?-?= Gc ©s¢¬9§±¨= ª/?§©¬T-¥¤zss½ÇºGª=?¥j¬=ª/§©¦ÆÄz¨?§¡ª/&apos;Ã³¬=ª«¥»ª=Â¢c°cs©¬T¥j= ¦&apos;§©s¨Gª/¡ÉL©s¥¤«¨?¥jc/j¢Ë°?¥¤ª/¥?Ì0&apos;ÍOs¦&apos;ª/?² ¥¤¨-°¿¥j=À¥¤¬«¬=c¦&apos;§±¥»ª/©¦s´c/§±¨c² ©/§¡²GjªÁ§©¨Gª=s²j«¥»ª/§¡G¨¿¤£T¦&apos;j¨cÃ ¾À-/¬ ª/«¥»§±¨jª«¬­£Ê=\¬=\ /=§©j´-¬­/? °Ó£ÅGI©s¥j/?¨ ±¥¤¨?²j´?/ª=?¥¤¬?//ª==§±¨c²z½=&apos;Ã ///¥¤§ ©¨c\°Á¥j´cª=G¢¥¤ª=¡©Éw£Ê=G¢Ô?¥»§±/¬I»£ ¬=G´?/ª¬/ª/¥»ª/?¤§±¦&apos;9¥¤¨-°-/\= §±¨c² ¥¤¨-°6ª/¥j=/=ª \/ ¥j¨?¦&apos;\=Tª«c§±¬(/Á°c&apos;Ã ® ¤s¡G#±¬£ÅG(Õ ¨c²¤©§±¬/Ã2Öj¥¤?¥j¨cs¬/iª«/¥j¨?=¬ ±¥»ª/§¡G¨#¥j¨?° ?/ s¬//? i-\2£ÅG/¢¥j¨? ¦&apos;»£Òª/? s¬/i¢°?/¥j¨?=¬
"Ã ±¥»ª=?=±¬=Ts®`¥»±´?¥»ª//? ?/«¥¤¨?¬/±¥»ª=~/?( ¦&apos;G¨Gª=&apos;¹cªI»£ ¥f¦s¥»©I/j´cª/§©¨c²#ª/¥¤¬«ØÙ»£$?´ ?¨ ?¬/ª/?=ª ª=\ 2Ã ¥¤¨-¦&apos;s¬s½ Û Ü?oc./ jc?+ ©s¢Ý»£P0F0MB&lt; ?,=&lt; . ? ( ª«/¥j¨?¬=±¥»ª/ s°"
This paper describes an approach to Machine Translation that places linguistic information at its foundation.
The difficulty of translation from English to Japanese is illustrated with data that shows the influence of various linguistic contextual factors.
"Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints."
"The method has been implemented, and the results of an evaluation are presented."
"High-quality automatic translation requires the disambiguation of common, highly ambiguous verbs, such as to have, to take, or to get."
"It also requires the correct handling of non-compositional, idiomatic expressions with varying degrees of “fixedness”."
We view Machine Translation in terms of linguistic information represented as typed feature structures.
"By integrating translation information represented as example pairs with other types of linguistic information represented as rules, our approach extends the capabilities of current machine translation methods, and solves a number of key problems."
"In translating different words, phrases, and expressions, different types and amounts of information from the context need to be considered. (Only the sentential context is considered here.)"
"So far, a systematic solution to this problem has not been found."
"This section illustrates the extent of this problem, and the remainder of this paper describes our approach."
We examined the problem of translating the English main verb to have into Japanese.
"The verb to have was selected because it is quite common in colloquial English, yet forms a large variety of senses, collocations, and idioms. 615 different expressions containing the English verb to have were extracted from a 7000-sentence corpus from the “international travel” domain."
Each English expression was manually translated into Japanese in the most general way possible.
The most general translation for the construction “X have Y” in this domain was found to be   (X-ni Y-ga aru):
The copy shop next door has a fax machine.    tonari-no kopiiya-ni fakkusu-ga arimasu. next-ATT copy shop-LOC fax-NOM exist
Other translations are often necessary when the target language imposes finer semantic distinction on the state or on the action that is described.
"For example, if the object noun phrase refers to one or more human beings, the Japanese verb aru is changed into iru."
"Similarly, the word pet or a pet animal as the object noun phrase triggers the translation of to have as katteiru, a Japanese verb for keeping an animal as a pet :"
We have two sons.  musuko-ga futari imasu. son-NOM two-CONTR exist
Do you have pets?
Other examples of finer target-language distinctions include a symptom/disease as the object of to have.
"While many physical symptoms and minor diagnoses (e.g. pain, cavity, fever, allergy) use the default translation (X-ga aru), a serious illness or diagnosis is translated into the copula construction."
Many other to have constructions with a symptom/disease object require verbs that are specific to the object noun phrase in Japanese:
I have diabetes. &quot; #$%&amp; watashi-wa toonyobyoo desu.
"I-TOP diabetes COP My wife had a stroke last year. ()*+,&amp;-. / tsuma-wa kyonen noosocchu-de taoremashita wife-NOM last year stroke-with fall-PST My husband had a heart attack. / / otto-ga shinzoohossa-wo okoshi-mashita husband-NOM heart attack-ACC cause-PST 1.3."
Adjuncts in the Source Language
"Some verbal adjuncts can affect the translation of the to have construction, not by altering the basic sense of ‘existing’, but by adding further information to specify the way in which something ‘exists’."
One example of such an adjunct is a prepositional phrase (PP) whose object noun phrase shares its referent with the SUBJ of have.
"For example, the utterance below expresses that the map is held or carried by the speaker, and the Japanese translation uses the verb motte-iru, literally meaning to be carrying/holding."
I have the map with me. &quot;:  watashi-wa sono chizu-wo motte-imasu.
TOP the map-ACC hold-ST
"If the subject noun phrase is inanimate, the Japanese translation uses the verb tsuite-iru, which literally means to be attached."
The main dish has a salad with it.    meindisshu-ni-wa sarada-ga tsuite-imasu. main dish-LOC-TOP salad-NOM attach-ST
"Similarly, a construction with an on-PP is translated into the Japanese construction notte-iru, which literally means to be written/placed on."
"A construction with an in-PP is translated into the Japanese construction haitte-iru, which literally means to be placed in:"
Does the map have subway lines on it.  8;&lt;= !
Sono chizu-ni chikatetsusen-ga notte-imasu-ka. the map-LOC subway line-NOM written-on-Q
The closet has extra hangers in it.    kurozetto-ni yobun-no hangaa-ga haitte-imasu. closet-LOC extra-ATT hanger-NOM placed-in-ST
Adjunct adjectival phrases and past participles also specify the way something exists.
"For example, available in the have construction generally changes the translation to aite-iru, to be open or available:"
We have one twin room available. & gt;?@  tsuin-no heya-ga hitotsu aite-imasu twin-ATT room-NOM one-CONTR open-ST 1.4.
Source Language Ambiguities
"In some cases, the to have construction in English carries more than one sense, and some linguistic contexts can bring out one of the senses as the preferred meaning."
"For example, the construction X has a Y taste is ambiguous between to be exercising Y (personal) taste and to taste X. This ambiguity is usually resolved by looking at the semantic properties of the subject noun phrase, as illustrated in the examples below:"
He has simple tastes. @/  kare-ga shinpuru-na shumi-wo shiteiru he-NOM simple taste-ACC do-ST This wine has a very clean taste. ? @ MNO! kono wain-wa totemo sawayaka-na aji-ga suru this wine-TOP very refreshing taste-NOM do
"When the object refers to a specific type of information, such as number or address, the construction is inherently ambiguous between to know (the number), to be carrying (the number), and (for the number) to exist."
"The construction usually carries the meaning of to know, but if the construction is negated, then the sense of to be carrying becomes more preferred, since the negative construction is more specific and only negates the proposition that the object is accessible:"
I don’t have his phone number. &quot;:  watashi-ga kare-no denwabangoo-wo motteinai I-NOM he-GEN phone number-ACC hold-ST-NEG
"On the other hand, if the object noun phrase is an indefinite noun phrase, it is more likely to mean to exist :"
Do you have an extension number?
T=! naisen bangou-ga arimasu-ka extension number-NOM exist-Q
"Another example of the ambiguities of to have concerns the two senses to have something available and to eat, when the object noun phrase refers to an edible entity."
Our corpus analysis shows that some of the linguistic contexts bring out one of the two senses as clearly preferred.
"For example, the past tense or the perfective aspect brings out the to eat sense, whereas the present tense without any aspect markers suppresses this sense:"
I had raw fish for dinner.   / sakana-no sashimi-wo yuushoku-ni tabemashita fish-ATT raw-ACC dinner-GOAL eat-PST
I don’t have any American beer on tap. [\]^ _ amerika-no namabiiru-wa arimasen.
America-ATT draft beer-TOP exist-NEG 1.5.
Support Verb Constructions and Idioms
"In some of the constructions, to have functions as a support verb."
In the support verb construction the object noun phrase constitutes a part of the verbal predicate rather than an argument of the verb.
"If the target language does not have an equivalent support verb construction, such an expression with a support verb construction has to be translated into the corresponding single verb construction."
"Idiomatic expressions in the source and target languages, and their varying degrees of “fixedness”, also play a role."
"For example, the  (kentoo), the Japanese translation of a clue in I don’t have a clue, requires the special verb (tsuku), to constitute an idiomatic expression  (kentoo-ga tsuku)."
"As another example, the English expression Have a good one does not allow a compositional translation into a Japanese construction with a main verb plus an object."
"From the data described above, it is clear that there are various factors that contribute to the different patterns of translation."
"In order to handle these different translations correctly, it is necessary to identify the linguistic features of the context that trigger different translations, and to determine how the different features and contexts interact."
"In the case of the English to have construction, the following surface linguistic features are identified that can be interpreted as ‘triggers’ for translations other than the default translation: • past tense • interrogative or imperative constructions • negative • modal auxiliaries • progressive and/or perfective aspect • adjectival modifiers for the object NP (noun phrase) • prepositional phrase modifiers for the object NP • adjectival modifiers for the VP (verb phrase) • prepositional phrase modifiers for the VP • adverbial modifiers for the VP • constructions that carry a pragmatic force (request, suggestion, etc.)"
We found that some of the factors have stronger influence on the translation than others.
"For example, consider the following expression:"
Can I have a look at the room? .! sono heya-wo mi-raremasu-ka. the room-ACC look-PTN-Q
The source-language expression contains more than one factor that can trigger a different translation.
"The first factor is the construction that usually carries the pragmatic force of “request”, Can I have X?, which usually triggers the X (X-wo o-negai dekimasu-ka) construction."
"At the same time, the object noun phrase a look means that the verb to have is used as a support verb."
"For this reason, the combination of the verb have and the object noun phrase a look has to be translated into Japanese as the verbal predicate  (miru)."
This shows that the translation preference that is triggered by the root string of the object noun phrase is stronger and should take preference over the translation preference that is triggered by the pragmatic force.
"We argue that the sorts of complex translation correspondences that were illustrated in the previous section are best represented as translation examples, but that the transfer procedure must use qualitative linguistic constraints in order to choose the correct examples."
"Given the types of linguistic features that influence translation, a highly expressive linguistic representation for both input and translation examples is required."
We employ typed feature structures throughout all stages of translation.
"Since there are complex interactions among different contextual factors, a single quantitative matching function that calculates a distance between the input and the examples is not sufficient."
"Multiple steps of matching are needed, each considering a small number of linguistic dimensions, with the steps executed in the appropriate order."
This is best achieved with a rule-based linguistic transfer procedure that controls the example matching procedure.
"The transfer component for information-based MT consists of two main procedures, the linguistic transfer procedure and the example matching procedure."
This is illustrated in Figure 1.
The input to this component is the source-language typed feature structure; this is created by an analysis component that is not described further here.
"Similarly, the output of the transfer component is a target-language typed feature structure, from which the target-language expression is generated by the generation component (also not described further)."
The linguistic transfer procedure is implemented as a rewrite-grammar using the special-purpose Grammar Programming Language (GPL) ([REF_CITE]).
"The general role of the transfer grammar is to operate on the input feature structure in a recursive manner, and to perform source-to-target transfer by invoking the example matching procedure, and by using the translation examples to construct a target-language feature structure."
The transfer grammar implements the principle of ”large to small” in covering the input feature structure.
"When the transfer procedure invokes the example matching procedure, it implements the principle of “specific to general”."
"Since the linguistic features interact with each other when they are combined, and since some of the features have more influence on the translation than others, it is necessary to specify a number of separate invocations of the example matching procedure, and to pay particular attention to their order."
"The invocations of the example matching procedure are arranged so that each call focuses on one or two features, making sure that both the input and the example contain the same feature(s)."
"Different invocations of the matching procedure are ordered so that the system checks the existence of the most important factors first, gradually progressing to the least important factors."
"The example matching procedure matches the input feature structure against the example feature structures, and it returns the most appropriate example."
The architecture of this module is shown in Figure 2.
"When the transfer procedure invokes the example matching procedure, it specifies a set of linguistic constraints on which examples may be considered."
This is used to narrow down the search space from all the examples to a much smaller set.
The examples that satisfy these constraints are matched in detail against the input feature structure.
"The detailed match is a recursive process operating on the two feature structures that is based on costs for inserting, deleting, or altering features, and on certain constraints for particular features."
Lexical similarity is calculated from the thesaurus on the basis of the information content of the thesaurus nodes.
"During example matching, the input feature structure is aligned with the example feature structure."
The alignment information is used by the transfer procedure to handle differences between the input and the example.
"For example, if the input contains grammatical features, modifiers, adjuncts, or sub-constituents that are not in the examples, then they are transferred to the target-language representation."
"Similarly, if the example feature structure contains information that is not present in the input, then the transfer procedure deletes the relevant information."
The example database contains a large set of translation examples represented as pairs of typed feature structures in the source and target languages.
"Using a Treebanking tool, the examples are disambiguated, and indices that show corresponding constituents are added."
"In addition to the type and complexity of the example feature structures, there are three methods for identifying the degree of linguistic specificity of an example: marked examples, example indices, and semantic constraints."
"This information is used by the transfer procedure and the matching procedure to select the best example, using the mechanism of linguistic matching constraints that was described above."
Some of the features that were shown in Section 2 to influence the translation have been traditionally described as “marked“.
"Examples include negation, interrogative, and also the presence of certain adjuncts."
"The transfer procedure regards these examples as more specific than unmarked examples, and (via the linguistic constraints passed to the matching procedure) only allows such examples when appropriate."
Examples can contain two types of indices linking a source-language sub-feature-structure with a target-language sub-feature-structure.
"A CORRESPOND-INDEX signals that the two constituents correspond to each other, while a REPLACE-INDEX signals that two constituents correspond to each other and can be replaced by similar constituents."
The absence of such indices in a major argument phrase (such as the subject or object) indicates that the example is more specific.
"A CORRESPOND-INDEX is more specific than a REPLACE-INDEX, since a CORRESPOND-INDEX indicates that although the head of the constituent allows modifiers, the constituent can not be substituted."
"For example, the object the bucket in the example for the idiom to kick the bucket does not contain any indices, since the idiom does not allow substitution or modification."
"On the other hand, a heart attack in to have a heart attack allows modifiers (e.g. a severe heart attack), so the example for the idiomatic translation carries a CORRESPOND-INDEX."
The example database also contains certain semantic constraints on source-language sub-feature-structures.
"When an input feature structure is matched with such an example, the matching procedure checks whether the input satisfies the semantic constraint."
"If it does, then that example is preferred over other examples, since it is more specific than other examples that do not carry a semantic constraint."
"On the other hand, if the input does not match the constraint, then the match is rejected."
Figure 3 shows the example pair for the expressions Can I have your name?    (o-namae-wo o-negai dekimasu-ka).
This example has a number of marked features.
"The mood of the sentence is yes-no question, the modal auxiliary can is present, and the subject does not contain an index."
These features are used by the transfer procedure to ensure that the example is only used to translate appropriate input.
A prototype implementation of this translation method has been created by the Sony USRL Speech Translation gro[REF_CITE].
"The prototype was developed for the “overseas travel domain”, which includes utterances and expressions useful for travel between e.g. Japan and the USA."
"The English-to-Japanese translation system includes an English dictionary with 6483 unique English root forms, and the English-to-Japanese example database contains 14,281 separate example pairs."
"These entries consist of constructions of various sizes, ranging from conjoined sentences to individual words."
"For some example pairs, the system automatically extracts corresponding parts from the source and target expressions, and creates a new example pair."
"As a result, the system has a total of 24,072 example database entries available."
"We developed, tested, and refined the system until all of the main predicates of the 615 development set sentences with to have were translated correctly."
"For this, the system used 129 distinct example pairs with the main verb to have."
Many example pairs encode a specific translation: 68 out of the 129 entries were used to translate only one expression from the development set.
"On the other hand, some entries are very general, and are used to translate a large number of expressions."
"The most frequently used entry is Do you have sushi?   (sushi-ga arimasu-ka), which is used to translate 113 out of the 615 development set expressions."
The transfer grammar contains 153 context-free rules.
"Each rule includes a rule-body with GPL statements, which can include calls to the example matching procedure, and calls to sub-transfer rules."
"To translate the 615 expression in the to have development set, the system performed an average of 3.4 match-and-transfer steps. (In many cases, more than one transfer path was pursued.)"
Examples of such expressions include Have a good one! and You can have it.
"At the other extreme, the maximum number of match-and-transfer steps required to translate a single input expression was 9."
One of the expressions that required 9 match-and-transfer steps was The double on the third floor has a really nice view of the ocean.
The system was evaluated using a new corpus of unseen expressions with the verb to have.
"The evaluation data was collected from three different travel phrase books published by Barron, Berlitz, and Lonely Planet."
The English expressions containing to have as a regular verb (and have got as a main predicate) were manually extracted from the phrase books.
"The evaluation corpus was translated by the translation system, and each of the output expressions was examined and manually categorized according to its translation quality."
The result is shown in the table below:.
"Wrong Translations 6 1.5%[REF_CITE]% The category “flawless translation” refers to translations without any obvious flaws or problems. “Incomplete translations due to OOV” refers to translations where the main predicate was correctly translated, but due to some out-of-vocabulary (OOV) nouns or modifiers, parts of the source-language input words were carried through to the target language expression."
"The category “wrong translation” refers to translations where the main predicate is incorrectly with or withouttranslated, out-of-vocabulary words."
"Some of the wrong translations are due to ambiguities in the object noun phrase, such as a fall in My child has had a fall, which the system translated as watashi-no kodomo-wa aki-ga arimashita (meaning My child had an autum)."
"There were also a number of expressions that should have been translated into different predicates in Japanese, but which were not covered in the example database."
Examples of these include the following :
Input: I’ve got a nosebleed.
Output: g g g hanaji-ga ari-masu nosebleed-NOM exist
Appropriate Translation:   hanaji-ga dete-imasu nosebleed-NOM come out-ST
"The evaluation shows that the information-based translation method works reliably for translating short, single-clause utterances."
"In support of the generality of this method, we found that translation accuracy could be improved by adding more examples, and that the features that mark specificity of example entries are applicable to expressions with other common verbs besides have."
One difficult problem remains in the treatment of support verb constructions.
"When the object has a modifier, the modifier has to be transferred as a verbal modifier in the target language if the target language requires a single verb construction."
"For example, to have a close look is translated as to look closely, and to have another look is translated as to look again."
"There are, however, not enough data in the development set to draw any conclusions about how general these modifiers can be treated across different support verb constructions."
One hypothesis is that there are different degrees of proximity between the support verb and the object noun phrase.
"In some cases, there might be only one fixed phrase to be interpreted as the support verb construction, while other cases may allow many different modifiers for the object noun phrase."
This is suggested by the case of to have a seat in the development set.
This phrase allows the interpretation of to sit only if the object noun phrase is exactly a seat.
"The expression to have another seat cannot be translated as to sit again, but more like for another seat to exist."
"Further analysis of support verb construction data, including instances with other verbs besides have, will be necessary to determine how these constructions can best be handled in the current framework."
"Another avenue for future work is the use of Machine Learning techniques to select linguistic features, and statistical methods (such as loglinear models) to model the effect of feature combinations."
"The approach described in this paper is based on the conviction that natural language transfer must be driven by qualitative, linguistic information."
The analysis of the problem of translating one construction from English to Japanese has shown that a significant amount of linguistic information is necessary for achieving high-quality translation of something as simple as single-clause input.
The transfer method that this paper described as one possible solution can integrate translation examples with linguistic rules and constraints in an effective manner.
The linguistic information used in this approach is general and domain-independent; domain-specific translation knowledge is confined to the example database.
"This modular system architecture presents significant advantages for developing, maintaining, and extending a practical machine translation system."
"In human sentence processing, cognitive load can be defined many ways."
This report considers a defini-tion of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word w i given its prefix w 0... i−1 on a phrase-structural lan-guage model.
These loads can be efficiently calcu-lated using a probabilistic Earley parser[REF_CITE]which is interpreted as generating predictions about reading time on a word-by-word basis.
"Un-der grammatical assumptions supported by corpus-frequency data, the operation of Stolcke’s probabilis-tic Earley parser correctly predicts processing phe-nomena associated with garden path structural am-biguity and with the subject/object relative asym-metry."
What is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure?
The answer to be proposed here observes three principles.
The relation between the parser and grammar is one of strong competence.
"Strong competence holds that the human sentence processing mechanism directly uses rules of gram-mar in its operation, and that a bare minimum of extragrammatical machinery is necessary."
"This hy-pothesis, originally proposed by Chomsky ([REF_CITE]page 9) has been pursued by many re-searchers[REF_CITE], and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism."
The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St.[REF_CITE])[REF_CITE]suggests a statistical theory of language performance.
The present work adopts a numerical view of competition in grammar that is grounded in probability.
Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situa-tions to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used.
The proposal is that a person’s difficulty per-ceiving syntactic structure be modeled by word-to-word surprisal ([REF_CITE]page 6) which can be directly computed from a probabilistic phrase-structure grammar.
The approach taken here uses a parsing algorithm developed by Stolcke.
"In the course of explaining the algorithm at a very high level I will indicate how the algorithm, interpreted as a psycholinguistic model, observes each principle."
"After that will come some simulation results, and then a conclusion."
Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition sys-tem.
"In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen."
"Given some lexicon of all possible words, a language model as-signs a probability to every string of words from the lexicon."
This defines a probabilistic language[REF_CITE].
A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far.
This is typically done using conditional probabilities of the form
"P (W n = w n |W 1 = w 1 , . . ."
"W n−1 = w n−1 ) the probability that the nth word will actually be w n given that the words leading up to the nth have been w 1 , w 2 , . . . w n−1 ."
"Given some finite lexicon, the probability of each possible outcome for W n can be estimated using that outcome’s relative frequency in a sample."
"Traditional language models used for speech are n-gram models, in which n − 1 words of history serve as the basis for predicting the nth word."
"Such mod-els do not have any notion of hierarchical syntactic structure, except as might be visible through an n-word window."
"Aware that the n-gram obscures many linguistically-significant distinctions ([REF_CITE]section 2.3), many speech researchers[REF_CITE]sought to incorporate hierar-chical phrase structure into language modeling (see[REF_CITE]) although it was not until the late 1990s that such models were able to significantly improve on 3-grams[REF_CITE]."
Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model.
"The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP . . . . . . see ([REF_CITE]chapter 5)"
Such a grammar defines a probabilistic language in terms of a stochastic process that rewrites strings of grammar symbols according to the probabilities on the rules.
Then each sentence in the language of the grammar has a probability equal to the product of the probabilities of all the rules used to generate it.
This multiplication embodies the assumption that rule choices are independent.
Sentences with more than one derivation accumulate the probability of all derivations that generate them.
"Through recursion, infinite languages can be specified; an important mathematical question in this context is whether or not such a grammar is consistent – whether it assigns some probability to infinite derivations, or whether all derivations are guaranteed to terminate."
"Even if a PCFG is consistent, it would appear to have another drawback: it only assigns probabili-ties to complete sentences of its language."
This is as inconvenient for speech recognition as it is for mod-eling reading times.
"Stolcke’s algorithm solves this problem by com-puting, at each word of an input string, the prefix probability."
This is the sum of the probabilities of all derivations whose yield is compatible with the string seen so far.
If the grammar is consistent (the proba-bilities of all derivations sum to 1.0) then subtracting the prefix probability from 1.0 gives the total proba-bility of all the analyses the parser has disconfirmed.
"If the human parser is eager, then the “work” done during sentence processing is exactly this disconfir-mation."
The computation of prefix probabilities takes advan-tage of the design of the Earley parser[REF_CITE]which by itself is not probabilistic.
In this section I provide a brief overview of Stolcke’s algorithm but the original paper should be consulted for full details[REF_CITE].
"Earley parsers work top-down, and propagate predictions confirmed by the input string back up through a set of states representing hypotheses the parser is entertaining about the structure of the sen-tence."
"The global state of the parser at any one time is completely defined by this collection of states, a chart, which defines a tree set."
A state is a record that specifies • the current input string position processed so far • a grammar rule • a “dot-position” in the rule representing how much of the rule has already been recognized • the leftmost edge of the substring this rule gen-erates
"An Earley parser has three main functions, pre-dict, scan and complete, each of which can enter new states into the chart."
"Starting from a dummy start state in which the dot is just to the left of the grammar’s start symbol, predict adds new states for rules which could expand the start symbol."
"In these new predicted states, the dot is at the far left-hand side of each rule."
"After prediction, scan checks the input string: if the symbol immediately following the dot matches the current word in the input, then the dot is moved rightward, across the symbol."
The parser has “scanned” this word.
"Finally, complete propagates this change throughout the chart."
"If, as a result of scanning, any states are now present in which the dot is at the end of a rule, then the left hand side of that rule has been recognized, and any other states having a dot immediately in front of the newly-recognized left hand side symbol can now have their dots moved as well."
This happens over and over until no new states are generated.
Parsing finishes when the dot in the dummy start state is moved across the grammar’s start symbol.
"Stolcke’s innovation, as regards prefix probabili-ties is to add two additional pieces of information to each state: α, the forward, or prefix probability, and γ the “inside” probability."
"He notes that path An (unconstrained) Earley path, or simply path, is a sequence of Earley states linked by prediction, scanning, or completion. constrained A path is said to be con-strained by, or generate a string x if the terminals immediately to the left of the dot in all scanned states, in se-quence, form the string x. . . ."
The significance of Earley paths is that they are in a one-to-one correspondence with left-most derivations.
"This will al-low us to talk about probabilities of deriva-tions, strings and prefixes in terms of the actions performed by Earley’s parser. ([REF_CITE]page 8)"
This correspondence between paths of parser op-erations and derivations enables the computation of the prefix probability – the sum of all derivations compatible with the prefix seen so far.
"By the cor-respondence between derivations and Earley paths, one would need only to compute the sum of all paths that are constrained by the observed prefix."
But this can be done in the course of parsing by storing the current prefix probability in each state.
"Then, when a new state is added by some parser opera-tion, the contribution from each antecedent state – each previous state linked by some parser operation – is summed in the new state."
Knowing the prefix probability at each state and then summing for all parser operations that result in the same new state efficiently counts all possible derivations.
Predicting a rule corresponds to multiplying by that rule’s probability.
Scanning does not alter any probabilities.
"Completion, though, requires knowing γ, the inside probability, which records how probable was the inner structure of some recognized phrasal node."
"When a state is completed, a bottom-up con-firmation is united with a top-down prediction, so the α value of the complete-ee is multiplied by the γ value of the complete-er."
Important technical problems involving left-recursive and unit productions are examined and overcome[REF_CITE].
"However, these com-plications do not add any further machinery to the parsing algorithm per se beyond the grammar rules and the dot-moving conventions: in particular, there are no heuristic parsing principles or intermediate structures that are later destroyed."
In this respect the algorithm observes strong competence – princi-ple [Footnote_1].
"1 This assumption is inevitable given principles 1 and 2. If there were separate processing costs distinct from the opti-mization costs postulated in the grammar, then strong com-petence is violated. Defining all grammatical structures as equally easy to disconfirm or perceive likewise voids the grad-edness of grammaticality of any content."
In virtue of being a probabilistic parser it observes principle 2.
"Finally, in the sense that pre-dict and complete each apply exhaustively at each new input word, the algorithm is eager, satisfying principle 3."
Psycholinguistic theories vary regarding the amount bandwidth they attribute to the human sentence processing mechanism.
Theories of initial parsing preferences[REF_CITE]suggest that the human parser is fundamentally serial: a func-tion from a tree and new word to a new tree.
These theories explain processing difficulty by appealing to “garden pathing” in which the current analysis is faced with words that cannot be reconciled with the structures built so far.
A middle ground is held by bounded-parallelism theories[REF_CITE].
"In these theories the human parser is modeled as a function from some subset of consistent trees and the new word, to a new tree subset."
"Garden paths arise in these theories when analyses fall out of the set of trees maintained from word to word, and have to be reanalyzed, as on strictly serial theories."
"Finally, there is the possibility of total parallelism, in which the entire set of trees compatible with the input is maintained somehow from word to word."
"On such a theory, garden-pathing cannot be explained by re-analysis."
"The probabilistic Earley parser computes all parses of its input, so as a psycholinguistic theory it is a total parallelism theory."
The explanation for garden-pathing will turn on the reduction in the probability of the new tree set compared with the previous tree set – reanalysis plays no role.
"Before illustrating this kind of explanation with a specific example, it will be important to first clarify the na-ture of the linking hypothesis between the operation of the probabilistic Earley parser and the measured effects of the human parser."
"The measure of cognitive effort mentioned earlier is defined over prefixes: for some observed prefix, the cognitive effort expended to parse that prefix is pro-portional to the total probability of all the struc-tural analyses which cannot be compatible with the observed prefix."
"This is consistent with eagerness since, if the parser were to fail to infer the incom-patibility of some incompatible analysis, it would be delaying a computation, and hence not be eager."
"This prefix-based linking hypothesis can be turned into one that generates predictions about word-by-word reading times by comparing the total effort expended before some word to the total effort af-ter: in particular, take the comparison to be a ratio."
"Making the further assumption that the probabili-ties on PCFG rules are statements about how diffi-cult it is to disconfirm each rule 1 , then the ratio of the α value for the previous word to the α value for the current word measures the combined difficulty of disconfirming all disconfirmable structures at a given word – the definition of cognitive load."
"Scal-ing this number by taking its log gives the surprisal, and defines a word-based measure of cognitive effort in terms of the prefix-based one."
"Of course, if the language model is sensitive to hierarchical structure, then the measure of cognitive effort so defined will be structure-sensitive as well."
The debate over the form grammar takes in the mind is clearly a fundamental one for cognitive science.
Much recent psycholinguistic work has generated a wealth of evidence that frequency of exposure to lin-guistic elements can affect our processing[REF_CITE].
"However, there is no clear consensus as to the size of the ele-ments over which exposure has clearest effect."
Gib-son and Pearlmutter identify it as an “outstanding question” whether or not phrase structure statistics are necessary to explain performance effects in sen-tence comprehension:
"Are phrase-level contingent frequency con-straints necessary to explain comprehen-sion performance, or are the remaining types of constraints sufficient."
"If phrase-level contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? ([REF_CITE]page 13)"
"Equally, formal work in linguistics has demon-strated the inadequacy of context-free grammars as an appropriate model for natural language in the general case[REF_CITE]."
"To address this criti-cism, the same prefix probabilities could be comput-ing using tree-adjoining grammars[REF_CITE]."
"With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics [Footnote_2] simplicity seems as good a guide as any in the selection of a grammar formalism."
2 Some important work in computational psycholinguistics[REF_CITE]assumes a Lexical-Functional Grammar where the c-structure rules are essentially context-free and have attached to them “strengths” which one might interpret as probabilities.
Probabilistic context-free grammar (1) will help il-lustrate the way a phrase-structured language model could account for garden path structural ambiguity.
Grammar (1) generates the celebrated garden path sentence “the horse raced past the barn fell”[REF_CITE].
"English speakers hearing these words one by one are inclined to take “the horse” as the subject of “raced,” expecting the sentence to end at the word “barn.”"
This is the main verb reading in figure 1.
"The human sentence processing mechanism is metaphorically led up the garden path by the main verb reading, when, upon hearing “fell” it is forced to accept the alternative reduced relative reading shown in figure 2."
"The confusion between the main verb and the re-duced relative readings, which is resolved upon hear-ing “fell” is the empirical phenomenon at issue."
"As the parse trees indicate, grammar (1) analyzes reduced relative clauses as a VP adjoined to an NP [Footnote_3] ."
3 See section 1.24 of the Treebank style guide
In one sample of parsed text [Footnote_4] such adjunctions are about 7 times less likely than simple NPs made up of a determiner followed by a noun.
"4 The sample, starts at sentence 93 of section 16 of the Treebank and goes for 500 sentences (12924 words) For information about the Penn Treebank project[URL_CITE]"
The probabilities of the other crucial rules are likewise estimated by their relative frequencies in the sample.
This simple grammar exhibits the essential character of the explanation: garden paths happen at points where the parser can disconfirm alternatives that to-gether comprise a great amount of probability.
Note the category ambiguity present with raced which can show up as both a past-tense verb (VBD) and a past participle (VBN).
Figure 3 shows the reading time predictions [Footnote_5] derived via the linking hypothesis that reading time at word n is proportional to the surprisal log αα n−1n .
5 Whether the quantitative values of the predicted read-ing times can be mapped onto a particular experiment in-volves taking some position on the oft-observed[REF_CITE]imperfect relationship between corpus fre-quency and psychological norms.
"At “fell,” the parser garden-paths: up until that point, both the main-verb and reduced-relative structures are consistent with the input."
"The prefix probability before “fell” is scanned is more than 10 times greater than after, suggesting that the proba-bility mass of the analyses disconfirmed at that point was indeed great."
"In fact, all of the probability as-signed to the main-verb structure is now lost, and only parses that involve the low-probability NP rule survive – a rule introduced [Footnote_5] words back."
5 Whether the quantitative values of the predicted read-ing times can be mapped onto a particular experiment in-volves taking some position on the oft-observed[REF_CITE]imperfect relationship between corpus fre-quency and psychological norms.
"If this garden path effect is truly a result of both the main verb and the reduced relative structures be-ing simultaneously available up until the final verb, then the effect should disappear when words inter-vene that cancel the reduced relative interpretation early on."
"To examine this possibility, consider now a differ-ent example sentence, this time from the language of grammar (2)."
0.425072046063 S → VP 1.0 SBAR → WHNP S 0.80412371161 NP → DT NN 0.082474226966 NP → NP SBAR 0.113402061424 NP → NP VP 0.11043 VP → VBD PP 0.141104 VP → VBD NP PP 0.214724 VP → AUX VP 0.484663 VP → VBN PP 0.0490798 VP → VBD (2) 1.0 PP → IN NP 1.0 WHNP → who 1.0 DT → the 0.33 NN → boss 0.33
NN → banker 0.33
NN → buy-back 0.5 IN → about 0.5 IN → by 1.0 AUX → was 0.74309393 VBD → told 0.25690607 VBD → resigned 1.0 VBN → told
The probabilities in grammar (2) are estimated from the same sample as before.
"It generates a sentence composed of words actually found in the sample, “the banker told about the buy-back resigned.”"
This sentence exhibits the same reduced relative clause structure as does “the horse raced past the barn fell.”
Grammar (2) also generates [Footnote_6] the subject relative “the banker who was told about the buy-back re-signed.”
"6 This grammar also generates active and simple passive sentences, rating passive sentences as more probable than the actives. This is presumably a fact about the writing style favored by the Wall Street Journal."
Now a comparison of two conditions is pos-sible.
MV and RC the banker told about the buy-back re-signed
"The words who was cancel the main verb reading, and should make that condition easier to process."
This asymmetry is borne out in graphs 4 and 5.
At “resigned” the probabilistic Earley parser predicts less reading time in the subject relative condition than in the reduced relative condition.
"This comparison verifies that the same sorts of phenomena treated in reanalysis and bounded paral-lelism parsing theories fall out as cases of the present, total parallelism theory."
"Although they used frequency estimates provided by corpus data, the previous two grammars were par-tially hand-built."
They used a subset of the rules found in the sample of parsed text.
A grammar in-cluding all rules observed in the entire sample sup-ports the same sort of reasoning.
"In this grammar, instead of just 2 NP rules there are 532, along with 120 S rules."
"Many of these generate analyses com-patible with prefixes of the reduced relative clause at various points during parsing, so the expectation is that the parser will be disconfirming many more hy-potheses at each word than in the simpler example."
Figure 6 shows the reading time predictions derived from this much richer grammar.
"Because the terminal vocabulary of this richer grammar is so much larger, a comparatively large amount of information is conveyed by the nouns “banker” and “buy-back” leading to high surprisal values at those words."
"However, the garden path effect is still observable at “resigned” where the pre-fix probability ratio is nearly 10 times greater than at either of the nouns."
"Amid the lexical effects, the probabilistic Earley parser is affected by the same structural ambiguity that affects English speakers."
The same kind of explanation supports an account of the subject-object relative asymmetry (cf. refer-ences[REF_CITE]) in the processing of unre-duced relative clauses.
"Since the Earley parser is designed to work with context-free grammars, the following example grammar adopts a GPSG-style analysis of relative clauses ([REF_CITE]page 155)."
"The estimates of the ratios for the two S[+R] rules are obtained by counting the proportion of sub-ject relatives among all relatives in the Treebank’s parsed Brown corpus 7 . 0.33 NP → SPECNP NBAR 0.33 NP → you 0.33 NP → me 1.0 SPECNP → DT 0.5 NBAR → NBAR S[+R] 0.5 NBAR → N 1.0 S → NP VP 0.86864638 S[+R] → NP[+R] VP (3) 0.13135362 S[+R] → NP[+R] S/NP 1.0 S/NP → NP VP/NP 1.0 VP/NP → V NP/NP 1.0 VP → V NP 1.0 V → saw 1.0 NP[+R] → who 1.0 DT → the 1.0 N → man 1.0 NP/NP → 7 In particular, relative clauses in the Treebank are ana-"
NP → NP SBAR (rule 1) lyzed as SBAR → WHNP S (rule 2) where the S con-
Grammar (3) generates both subject and object rela-tive clauses.
S[+R] → NP[+R]
VP is the rule that gen-erates subject relatives and S[+R] → NP[+R] S/NP generates object relatives.
One might expect there to be a greater processing load for object relatives as soon as enough lexical material is present to deter-mine that the sentence is in fact an object relative [Footnote_8] .
"8 The difference in probability between subject and object rules could be due to the work necessary to set up storage for the filler, effectively recapitulating the HOLD Hypothesis ([REF_CITE]page 119)"
The same probabilistic Earley parser (modified to handle null-productions) explains this asymmetry in the same way as it explains the garden path effect.
"Its predictions, under the same linking hypothesis as in the previous cases, are depicted in graphs 7 and 8."
The mean surprisal for the object relative is about 5.0 whereas the mean surprisal for the subject relative is about 2.1.
We propose a novel Co-Training method for statistical parsing.
"The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text."
The algo-rithm iteratively labels the entire data set with parse trees.
Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out-performs training only on the labeled data.
The current crop of statistical parsers share a similar training methodology.
"They train from the Penn Tree-bank[REF_CITE]; a collection of 40,000 sen-tences that are labeled with corrected parse trees (ap-proximately a million word tokens)."
"In this paper, we explore methods for statistical parsing that can be used to combine small amounts of labeled data with unlimited amounts of unlabeled data."
"In the experiment reported here, we use 9695 sentences of bracketed data (234467 word tokens)."
Such methods are attractive for the follow-ing reasons:
Bracketing sentences is an expensive process.
A parser that can be trained on a small amount of la-beled data will reduce this annotation cost.
Creating statistical parsers for novel domains and new languages will become easier.
Combining labeled data with unlabeled data allows exploration of unsupervised methods which can now be tested using evaluations compatible with su-pervised statistical parsing.
In this paper we introduce a new approach that com-bines unlabeled data with a small amount of labeled (bracketed) data to train a statistical parser.
"We use a Co-Training method[REF_CITE]that has been used pre-viously to train classifiers in applications like word-sense disambiguati[REF_CITE], document classifica-ti[REF_CITE]and named-entity recog-niti[REF_CITE]and apply this method to the more complex domain of statistical parsing."
"While machine learning techniques that exploit anno-tated data have been very successful in attacking prob-lems in NLP, there are still some aspects which are con-sidered to be open issues:"
"Adapting to new domains: training on one domain, testing (using) on another."
Higher performance when using limited amounts of annotated data.
Separating structural (robust) aspects of the prob-lem from lexical (sparse) ones to improve perfor-mance on unseen data.
In the particular domain of statistical parsing there has been limited success in moving towards unsupervised machine learning techniques (see Section 7 for more dis-cussion).
A more promising approach is that of combin-ing small amounts of seed labeled data with unlimited amounts of unlabeled data to bootstrap statistical parsers.
"In this paper, we use one such machine learning tech-nique: Co-Training, which has been used successfully in several classification tasks like web page classification, word sense disambiguation and named-entity recogni-tion."
Early work in combining labeled and unlabeled data for NLP tasks was done in the area of unsupervised part of speech (POS) tagging.[REF_CITE]reported very high results (96% on the Brown corpus) for un-supervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.
Tag dictionaries are predefined as-signments of all possible POS tags to words in the test data.
This impressive result triggered several follow-up studies in which the effect of hand tuning the tag dictio-nary was quantified as a combination of labeled and unla- beled data.
The experiments[REF_CITE]showed that only in very specific cases HMMs were effective in combining labeled and unlabeled data.
"However,[REF_CITE]showed that aggressively us-ing tag dictionaries extracted from labeled data could be used to bootstrap an unsupervised POS tagger with high accuracy (approx 95% on WSJ data)."
We exploit this ap-proach of using tag dictionaries in our method as well (see Section 3.2 for more details).
"It is important to point out that, before attacking the problem of parsing using similar machine learning techniques, we face a represen-tational problem which makes it difficult to define the notion of tag dictionary for a statistical parser."
The problem we face in parsing is more complex than assigning a small fixed set of labels to examples.
"If the parser is to be generally applicable, it has to produce a fairly complex “label” given an input sentence."
"For example, given the sentence Pierre Vinken will join the board as a non-executive director, the parser is expected to produce an output as shown in Figure 1."
"Since the entire parse cannot be reasonably considered as a monolithic label, the usual method in parsing is to decompose the structure assigned in the following way:"
VP(join) NP(Vinken) !
Pierre Vinken VP(join) ! will VP(join) VP(join) ! join NP(board) PP(as) : : :
"However, such a recursive decomposition of structure does not allow a simple notion of a tag dictionary."
We solve this problem by decomposing the structure in an approach that is different from that shown above which uses context-free rules.
The approach uses the notion of tree rewriting as defined in the Lexicalized Tree Adjoining Grammar (LTAG) formalism[REF_CITE]1 which re- tains the notion of lexicalization that is crucial in the suc-cess of a statistical parser while permitting a simple def-inition of tag dictionary.
"For example, the parse in Fig-ure 1 can be generated by assigning the structured labels shown in Figure 2 to each word in the sentence (for sim-plicity, we assume that the noun phrases are generated here as a single word)."
We use a tool described[REF_CITE]to convert the Penn Treebank into this rep-resentation.
Combining the trees together by rewriting nodes as trees (explained in Section 2.[Footnote_1]) gives us the parse tree in Figure 1.
1 This is a lexicalized version of Tree Adjoining Grammar[REF_CITE].
A history of the bi-lexical dependencies that define the probability model used to construct the parse is shown in Figure 3.
This history is called the derivation tree.
"In addition, as a byproduct of this kind of represen-tation we obtain more than the phrase structure of each sentence."
"We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilis-"
A stochastic LTAG derivation proceeds as follows[REF_CITE].
An initial tree is selected with probability P init and other trees selected by words in the sentence are combined using the operations of substitu-tion and adjoining.
These operations are explained below with examples.
Each of these operations is performed with probability P attach.
Many supervised methods of learning from a Treebank have been studied.
The question we want to pursue in this paper is whether unlabeled data can be used to im-prove the performance of a statistical parser and at the same time reduce the amount of labeled training data necessary for good performance.
We will assume the data that is input to our method will have the following characteristics: 1.
A small set of sentences labeled with corrected parse trees and large set of unlabeled data. 2.
A pair of probabilistic models that form parts of a statistical parser.
This pair of models must be able to mutually constrain each other. 3.
A tag dictionary (used within a backoff smoothing strategy) for labels are not covered in the labeled set.
The pair of probabilistic models can be exploited to bootstrap new information from unlabeled data.
"Since both of these steps ultimately have to agree with each other, we can utilize an iterative method called Co-Training that attempts to increase agreement between a pair of statistical models by exploiting mutual constraints between their output."
"Co-Training has been used before in applications like word-sense disambiguati[REF_CITE], web-page classificati[REF_CITE]and named-entity identificati[REF_CITE]."
"In all of these cases, using unlabeled data has resulted in per-formance that rivals training solely from labeled data."
"However, these previous approaches were on tasks that involved identifying the right label from a small set of labels (typically 2–3), and in a relatively small parame-ter space."
"Compared to these earlier models, a statistical parser has a very large parameter space and the labels that are expected as output are parse trees which have to be built up recursively."
We discuss previous work in combining labeled and unlabeled data in more detail in Section 7.
Co-training[REF_CITE]can be informally described in the following man-ner:
Pick two (or more) “views” of a classification prob-lem.
Build separate models for each of these “views” and train each model on a small set of labeled data.
Sample an unlabeled data set and to find examples that each model independently labels with high con-fidence.[REF_CITE]
Confidently labeled examples can be picked in var-ious ways.[REF_CITE]
Take these examples as being valuable as training examples and iterate this procedure until the unla-beled data is exhausted.
"Effectively, by picking confidently labeled data from each model to add to the training data, one model is la-beling data for the other model."
"In the representation we use, parsing using a lexicalized grammar is done in two steps: 1. Assigning a set of lexicalized structures to each word in the input sentence (as shown in Figure 2). 2. Finding the correct attachments between these structures to get the best parse (as shown in Fig-ure 1)."
Each of these two steps involves ambiguity which can be resolved using a statistical model.
"By explicitly rep-resenting these two steps independently, we can pursue independent statistical models for each step: 1."
Each word in the sentence can take many different lexicalized structures.
We can introduce a statistical model that disambiguates the lexicalized structure assigned to a word depending on the local context. 2.
"After each word is assigned a certain set of lexical-ized structures, finding the right parse tree involves computing the correct attachments between these lexicalized structures."
Disambiguating attachments correctly using an appropriate statistical model is essential to finding the right parse tree.
These two models have to agree with each other on the trees assigned to each word in the sentence.
"Not only do the right trees have to be assigned as predicted by the first model, but they also have to fit together to cover the entire sentence as predicted by the second model [Footnote_2] ."
2 See x 7 for a discussion of the relation of this approach to that of SuperTagging[REF_CITE]
This represents the mutual constraint that each model places on the other.
"For the words that appear in the (unlabeled) training data, we collect a list of part-of-speech labels and trees that each word is known to select in the training data."
This information is stored in a POS tag dictionary and a tree dictionary.
It is important to note that no frequency or any other distributional information is stored.
The only information stored in the dictionary is which tags or trees can be selected by each word in the training data.
We use a count cutoff for trees in the labeled data and combine observed counts into an unobserved tree count.
This is similar to the usual technique of assigning the token unknown to infrequent word tokens.
"In this way, trees unseen in the labeled data but in the tag dictionary are assigned a probability in the parser."
The problem of lexical coverage is a severe one for unsupervised approaches.
The use of tag dictionaries is a way around this problem.
Such an approach has al-ready been used for unsupervised part-of-speech tagging[REF_CITE]where seed data of which POS tags can be selected by each word is given as input to the unsu-pervised tagger.
"In future work, it would be interesting to extend mod-els for unknown-word handling or other machine learn-ing techniques in clustering or the learning of subcatego-rization frames to the creation of such tag dictionaries."
"As described before, we treat parsing as a two-step pro-cess."
The two models that we use are: 1.
H1: selects trees based on previous context (tagging probability model) 2.
H2: computes attachments between trees and re-turns best parse (parsing probability model)
We select the most likely trees for each word by examin-ing the local context.
The statistical model we use to de-cide this is the trigram model that was used by B. Srinivas in his SuperTagging model[REF_CITE].
"The model assigns an n -best lattice of tree assignments associated with the input sentence with each path corresponding to an assignment of an elementary tree for each word in the sentence. (for further details, see[REF_CITE])."
P ( T j W ) =
P ( T 0 : : : T n j W 0 : : : W n ) (1) 0 : : : W n j T 0 : : : T n ) =
"P ( T 0 : : : T n ) P ( WP ( W (2) 0 : : : W n ) P ( T i j T i , 2 T i , 1 ) P ( W i j T i ) (3) where T 0 : : : T n is a sequence of elementary trees as-signed to the sentence W 0 : : : W n ."
We get (2) by using Bayes theorem and we obtain (3) from (2) by ignore the denominator and by applying the usual Markov assumptions.
The output of this model is a probabilistic ranking of trees for the input sentence which is sensitive to a small local context window.
"Once the words in a sentence have selected a set of el-ementary trees, parsing is the process of attaching these trees together to give us a consistent bracketing of the sentences."
Notation: Let stand for an elementary tree which is lexicalized by a word: w and a part of speech tag: p .
Let P init (introduced earlier in 2.1) stand for the prob-ability of being root of a derivation tree defined as fol-
"X P lows: init ( ) = 1 including lexical information, this is written as:"
Pr( ; w; p j top = 1) =
Pr( j top = 1) (4) Pr( p j ; top = 1) (5) Pr( w j ; p; top = 1); (6) where the variable top indicates that is the tree that begins the current derivation.
There is a useful approxi-mation for P init:
Pr( ; w; p j top = 1) Pr( label j top = 1) where label is the label of the root node of .
Pr^ ( label j top = 1) =
Count ( top = 1 ; label ) + (7) Count ( top = 1) + N where N is the number of bracketing labels and is a constant used to smooth zero counts.
Let P attach (introduced earlier in 2.1) stand for the probability of attachment of 0 into another :
P attach ( ; !
"NA ) + X P attach ( ; ! 0 ) = 1 0 including lexical information, this is written as:"
Pr( 0 ; p 0 ; w 0 j Node; ; w; p ) (8) Pr( NA j Node; ; w; p ) (9)
We decompose (8) into the following components:
Pr( 0 ; p 0 ; w 0 j Node; ; w; p ) =
Pr( 0 j Node; ;w;p ) (10) Pr( p 0 j 0 ;Node; ;w;p ) (11) Pr( w 0 j p 0 ; 0 ;Node; ;w;p ); (12)
We do a similar decomposition for (9).
"For each of the equations above, we use a backoff model which is used to handle sparse data problems."
We compute a backoff model as follows:
Let e 1 stand for the original lexicalized model and e 2 be the backoff level which only uses part of speech infor-mation: e 1 : Node; ;w;p e 2 : Node; ;p
"For both P init and P attach, let c = Count ( e 1 ) ."
"Then the backoff model is computed as follows: ( c ) e 1 + (1 , ( c )) e 2 where ( c ) = ( c + cD ) and D is the diversity of e 1 (i.e. the number of distinct counts for e 1 )."
"For P attach we further smooth probabilities (10), (11) and (12)."
Pr^ ( 0 j Node; ;w;p ) = ( Count ( Node; ;w;p; 0 ) + ) (13) ( Count ( Node; ;w;p ) + k )
"Count X Count ( Node; ; w; p ) = ( Node; ; w; p; y ) (14) y 2T 0 where k is the diversity of adjunction, that is: the num-ber of different trees that can attach at that node."
T 0 is the set of all trees 0 that can possibly attach at Node in tree .
"For our experiments, the value of is set to 1001 ; 000 ."
"We are now in the position to describe the Co-Training algorithm, which combines the models described in Sec-tion 4.1 and in Section 4.2 in order to iteratively label a large pool of unlabeled data."
We use the following datasets in the algorithm: labeled a set of sentences bracketed with the correct parse trees. cache a small pool of sentences which is the focus of each iteration of the Co-Training algorithm. unlabeled a large set of unlabeled sentences.
The only information we collect from this set of sentences is a tree-dictionary: tree-dict and part-of-speech dic-tionary: pos-dict.
Construction of these dictionaries is covered in Section 3.2.
"In addition to the above datasets, we also use the usual development test set (termed dev in this paper), and a test set (called test) which is used to evaluate the bracketing accuracy of the parser."
The Co-Training algorithm consists of the following steps which are repeated iteratively until all the sentences in the set unlabeled are exhausted. 1.
Input: labeled and unlabeled 2.
Update cache Randomly select sentences from unlabeled and refill cache If cache is empty; exit 3.
Train models H1 and H2 using labeled 4. Apply H1 and H2 to cache. 5. Pick most probable n from H1 (run through H2) and add to labeled. 6. Pick most probable n from H2 and add to labeled 7. n = n + k ; Go to Step 2
"For the experiment reported here, n = 10 , and k was set to be n in each iteration."
We ran the algorithm for 12 iterations (covering 20480 of the sentences in unlabeled) and then added the best parses for all the remaining sen-tences.
The experiments we report were done on the Penn Tree-bank WSJ Corpus[REF_CITE].
The various settings for the Co-Training algorithm (from Section 5) are as follows: labeled was set[REF_CITE]-06 of the Penn Tree-bank WSJ (9625 sentences) unlabeled was 30137 sentences ([REF_CITE]-21 of the Treebank stripped of all annotations).
A tag dictionary of all lexicalized trees from labeled and unlabeled.
Novel trees were treated as unknown tree tokens.
The cache size was 3000 sentences.
"While it might seem expensive to run the parser over the cache multiple times, we use the pruning capabilities of the parser to good use here."
During the iterations we set the beam size to a value which is likely to prune out all derivations for a large portion of the cache except the most likely ones.
"This allows the parser to run faster, hence avoiding the usual problem with running an iter-ative algorithm over thousands of sentences."
In the ini-tial runs we also limit the length of the sentences entered into the cache because shorter sentences are more likely to beat out the longer sentences in any case.
The beam size is reset when running the parser on the test data to allow the parser a better chance at finding the most likely parse.
We scored the output of the parser[REF_CITE]of the Wall Street Journal Penn Treebank.
"The following are some aspects of the scoring that might be useful for com-parision with other results: No punctuations are scored, including sentence final punctuation."
Empty elements are not scored.
"We used EVALB (written by Satoshi Sekine and Michael Collins) which scores based on PAR - SEVAL[REF_CITE]; with the standard parame-ter file (as per standard practice, part of speech brackets were not part of the evaluation)."
"Also, we used Adwait Ratnaparkhi’s part-of-speech tagger[REF_CITE]to tag unknown words in the test data."
The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall.
These re-sults show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.
"It is important to note that unlike previous studies, our method of moving towards unsupervised parsing are di-rectly compared to the output of supervised parsers."
Certain differences in the applicability of the usual methods of smoothing to our parser cause the lower ac-curacy as compared to other state of the art statistical parsers.
"However, we have consistently seen increase in performance when using the Co-Training method over the baseline across several trials."
It should be empha-sised that this is a result based on less than 20% of data that is usually used by other parsers.
We are experiment-ing with the use of an even smaller set of labeled data to investigate the learning curve.
The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTag-ger[REF_CITE]which is a statistical model for tag-ging sentences with elementary lexicalized structures.
"This was particularly so in the Lightweight Dependency Analyzer (LDA), which used shortest attachment heuris-tics after an initial SuperTagging stage to find syntactic dependencies between words in a sentence."
"However, there was no statistical model for attachments and the notion of mutual constraints between these two steps was not exploited in this work."
Previous studies in unsupervised methods for parsing have concentrated on the use of inside-outside algorithm[REF_CITE].
"How-ever, there are several limitations of the inside-outside al-gorithm for unsupervised parsing, see[REF_CITE]for some experiments that draw out the mismatch be-tween minimizing error rate and iteratively increasing the likelihood of the corpus."
Other approaches have tried to move away from phrase structural representations into dependency style parsing[REF_CITE].
"However, there are still inherent computa-tional limitations due to the vast search space (see[REF_CITE]for discussion)."
None of these approaches can even be realistically compared to supervised parsers that are trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank.[REF_CITE]combine unlabeled and labeled data for parsing with a view towards language modeling applications.
The goal in their work is not to get the right bracketing or dependencies but to reduce the word error rate in a speech recognizer.
Our approach is closely related to previous Co-Training methods[REF_CITE].[REF_CITE]first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the con-straint that in a segment of discourse only one sense of a word is used.
This use of unlabeled data improved performance of the disambiguator above that of purely supervised methods.[REF_CITE]further embellish this approach and gave it the name of Co-Training.
Their definition of Co-Training includes the notion (exploited in this paper) that different models can constrain each other by exploiting different ‘views’ of the data.
They also prove some PAC results on learnability.
They also discuss an application of classifying web pages by using their method of mutually constrained models.[REF_CITE]further extend the use of clas-sifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Boosting).[REF_CITE]provide a variant of Co-Training which is suited to the learning of deci-sion trees where the data is split up into different equiv-alence classes for each of the models and they use hy-pothesis testing to determine the agreement between the models.
In future work we would like to experiment whether some of these ideas could be incorporated into our model.
In future work we would like to explore use of the en-tire 1M words of the WSJ Penn Treebank as our labeled data and to use a larger set of unbracketed WSJ data as input to the Co-Training algorithm.
"In addition, we plan to explore the following points that bear on understand-ing the nature of the Co-Training learning algorithm:"
The contribution of the dictionary of trees extracted from the unlabeled set is an issue that we would like to explore in future experiments.
"Ideally, we wish to design a co-training method where no such informa-tion is used from the unlabeled set."
The relationship between co-training and EM bears investigation.[REF_CITE]is a study which tries to separate two factors: (1) The gradi-ent descent aspect of EM vs. the iterative nature of co-training and (2) The generative model used in EM vs. the conditional independence between the features used by the two models that is exploited in co-training.
"Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see[REF_CITE])."
"In our experiments, un[REF_CITE]we do not balance the label priors when pick-ing new labeled examples for addition to the train-ing data."
One way to incorporate this into our algo-rithm would be to incorporate some form of sample selection (or active learning) into the selection of examples that are considered as labeled with high confidence[REF_CITE].
"In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data."
It uses a Co-Training method where a pair of mod-els attempt to increase their agreement on labeling the data.
"The algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexi-calized structures for each word in this training set (based on the LTAG formalism)."
The algorithm presented itera-tively labels the unlabeled data set with parse trees.
We then train a statistical parser on the combined set of la-beled and unlabeled data.
The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and re-call.
These results show that training a statistical parser using our Co-training method to combine labeled and un-labeled data strongly outperforms training only on the la-beled data.
"It is important to note that unlike previous studies, our method of moving towards unsupervised parsing can be directly compared to the output of supervised parsers."
Unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of represen-tations and the complexity of sentences that are found in the Penn Treebank.
"In addition, as a byproduct of our representation we obtain more than the phrase structure of each sentence."
"We also produce a more embellished parse in which phe-nomena such as predicate-argument structure, subcate-gorization and movement are given a probabilistic treat-ment."
We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input.
"Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English."
"Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed."
"Many NLP tasks, such as building machine-readable dictionaries, are dependent on the results of morphological analysis."
"While morphological analyzers have existed since the early 1960s, current algorithms require human labor to build rules for morphological structure."
"In an attempt to avoid this labor-intensive process, recent work has focused on machine-learning approaches to induce morphological structure using large corpora."
"In this paper, we propose a knowledge-free algorithm to automatically induce the morphology structures of a language."
Our algorithm takes as input a large corpus and produces as output a set of conflation sets indicating the various inflected and derived forms for each word in the language.
"As an example, the conflation set of the word “abuse” would contain “abuse”, “abused”, “abuses”, “abusive”, “abusively”, and so forth."
"Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms using a Latent Semantic Analysis approach to corpus-based semantics[REF_CITE], affix frequency, syntactic context, and transitive closure."
"Using the hand-labeled CELEX lexic[REF_CITE]as our gold standard, the current version of our algorithm achieves an F-score of 88.1% on the task of identifying conflation sets in English, outperforming earlier algorithms."
"Our algorithm is also applied to German and Dutch and evaluated on jurafsky@cs.colorado.edu its ability to find prefixes, suffixes, and circumfixes in these languages."
"To our knowledge, this serves as the first evaluation of complete regular morphological induction of German or Dutch (although researchers such[REF_CITE]have evaluated induction algorithms on morphological sub-problems in German)."
Previous morphology induction approaches have fallen into three categories.
These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis.
We here briefly describe work in each category.
Some researchers begin with some initial human-labeled source from which they induce other morphological components.
"In particular,[REF_CITE]use word context derived from a corpus to refine Porter stemmer output."
"Also,[REF_CITE]obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech."
"A second, knowledge-free category of research has focused on obtaining affix inventories."
He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following the stems as suffixes.
"Due to the existence of morphological ambiguity (such as with the word “caring” whose stem is “care” rather than “car”), finding affixes alone does not constitute a complete morphological analysis."
"Hence, the last category of research is also knowledge-free but attempts to induce, for each word of a corpus, a complete analysis."
"Since our approach falls into this category (expanding upon our earlier approach[REF_CITE]), we describe work in this area in more detail."
"He also clusters groups of words having the same kinds of word endings, which gives an added performance boost."
"He applies his algorithm to a French term list and scores based on sampled, by-hand evaluation."
Goldsmith (1997/2000) tries to automatically sever each word in exactly one place in order to establish a potential set of stems and suffixes.
He uses the expectation-maximization algorithm (EM) and MDL as well as some triage procedures to help eliminate inappropriate parses for every word in a corpus.
He collects the possible suffixes for each stem and calls these signatures which give clues about word classes.
"With the exceptions of capitalization removal and some word segmentation, Goldsmith&apos;s algorithm is otherwise knowledge-free."
"His algorithm, Linguistica, is freely available on the Internet."
Goldsmith applies his algorithm to various languages but evaluates in English and French.
"In our earlier work, we[REF_CITE]generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes."
We then applied Latent Semantic Analysis[REF_CITE]as a method of automatically determining semantic relatedness between word pairs.
"Using statistics from the semantic relations, we identified those word pairs that have strong semantic correlations as being morphological variants of each other."
"With the exception of word segmentation, we provided no human information to our system."
We applied our system to an English corpus and evaluated by comparing each word’s conflation set as produced by our algorithm to those derivable from CELEX.
Most of the existing algorithms described focus on suffixing in inflectional languages (though Jacquemin and DéJean describe work on prefixes).
"None of these algorithms consider the general conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages[REF_CITE]."
"Additionally, most approaches have centered around statistics of orthographic properties."
"We had noted previously[REF_CITE], however, that errors can arise from strictly orthographic systems."
"We had observed in other systems such errors as inappropriate removal of valid affixes (“ally” &lt; “all”), failure to resolve morphological ambiguities (“hated” &lt; “hat”), and pruning of semi-productive affixes (“dirty” h “dirt”)."
Yet we illustrated that induced semantics can help overcome some of these errors.
"However, we have since observed that induced semantics can give rise to different kinds of problems."
"For instance, morphological variants may be semantically opaque such that the meaning of one variant cannot be readily determined by the other (“reusability” h “use”)."
"Additionally, high-frequency function words may be conflated due to having weak semantic information (“as” &lt; “a”)."
"Coupling semantic and orthographic statistics, as well as introducing induced syntactic information and relational transitivity can help in overcoming these problems."
"Therefore, we begin with an approach similar to our previous algorithm."
"Yet we build upon this algorithm in several ways in that we: [1] consider circumfixes, [2] automatically identify capitalizations by treating them similar to prefixes [3] incorporate frequency information, [4] use distributional information to help identify syntactic properties, and [5] use transitive closure to help find variants that may not have been found to be semantically related but which are related to mutual variants."
"We then apply these strategies to English,"
"German, and Dutch."
We evaluate our algorithm against the human-labeled CELEX lexicon in all three languages and compare our results to those that the Goldsmith and Schone/Jurafsky algorithms would have obtained on our same data.
We show how each of our additions result in progressively better overall solutions.
"As in our earlier approach[REF_CITE], we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants."
"Our algorithm has changed somewhat, though, since we previously sought word pairs that vary only by a prefix or a suffix, yet we now wish to generalize to those with circumfixing differences."
We use “circumfix” to mean true circumfixes like the German ge-/-t as well as combinations of prefixes and suffixes.
It should be mentioned also that we assume the existence of languages having valid circumfixes that are not composed merely of a prefix and a suffix that appear independently elsewhere.
"To find potential morphological variants, our first goal is to find word endings which could serve as suffixes."
"We had shown in our earlier work how one might do this using a character tree, or trie (as in"
"Yet using this approach, there may be circumfixes whose endings will be overlooked in the search for suffixes unless we first remove all candidate prefixes."
"Therefore, we build a lexicon consisting of all words in our corpus and identify all word beginnings with frequencies in excess of some threshold (T 1 )."
We call these pseudo-prefixes.
We strip all pseudo-prefixes from each word in our lexicon and add the word residuals back into the lexicon as if they were also words.
"Using this final lexicon, we can now seek for suffixes in a manner equivalent to what we had done[REF_CITE]."
"To demonstrate how this is done, suppose our initial lexicon / contained the words “align,” “real,” “aligns,” “realign”, “realigned”, “react”, “reacts,” and “reacted.”"
Due to the high frequency occurrence of “re-” suppose it is identified as a pseudo-prefix.
"If we strip off “re-” from all words, and add all residuals to a trie, the branch of the trie of words beginning with “a” is depicted in Figure 2."
Figure 2: Inserting the residual lexicon into a trie
"In our earlier work, we showed that a majority of the regular suffixes in the corpus can be found by identifying trie branches that appear repetitively."
By “branch” we mean those places in the trie where some splitting occurs.
"In the case of Figure 2, for example, the branches NULL (empty circle), “-s” and “-ed” each appear twice."
We assemble a list of all trie branches that occur some minimum number of times (T 2 ) and refer to such as potential suffixes.
"Given this list, we can now find potential prefixes using a similar strategy."
"Using our original lexicon, we can now strip off all potential suffixes from each word and form a new augmented lexicon."
"Then, (as we had proposed before) if we reverse the ordering on the words and insert them into a trie, the branches that are formed will be potential prefixes (in reverse order)."
"Before describing the last steps of this procedure, it is beneficial to define a few terms (some of which appeared in our previous work): [a] potential circumfix: A pair B/E where B and E occur respectively in potential prefix and suffix lists [b] pseudo-stem: the residue of a word after its potential circumfix is removed [c] candidate circumfix: a potential circumfix which appears affixed to at least T 3 pseudo-stems that are shared by other potential circumfixes [d] rule: a pair of candidate circumfixes sharing at least T 4 pseudo-stems [e] pair of potential morphological variants (PPMV): two words sharing the same rule but distinct candidate circumfixes [f] ruleset: the set of all PPMVs for a common rule"
Our final goal in this first stage of induction is to find all of the possible rules and their corresponding rulesets.
We therefore re-evaluate each word in the original lexicon to identify all potential circumfixes that could have been valid for the word.
"For example, suppose that the lists of potential suffixes and prefixes contained “-ed” and “re-” respectively."
Note also that NULL exists by default in both lists as well.
"If we consider the word “realigned” from our lexicon / , we would find that its potential circumfixes would be NULL/ed, re/NULL, and re/ed and the corresponding pseudo-stems would be “realign,” “aligned,” and “align,” respectively,"
"From / , we also note that circumfixes re/ed and NULL/ing share the pseudo-stems “us,” “align,” and “view” so a rule could be created: re/ed &lt; NULL/ing."
This means that word pairs such as “reused/using” and “realigned/aligning” would be deemed PPMVs.
"Although the choices in T 1 through T 4 is somewhat arbitrary, we chose T 1 =T 2 =T 3 =10 and T 4 =3."
"In English, for example, this yielded 30535 possible rules."
Table 1 gives a sampling of these potential rules in each of the three languages in terms of frequency-sorted rank.
"Notice that several “rules” are quite valid, such as the indication of an English suffix -s."
There are also valid circumfixes like the ge-/-t circumfix of German.
"Capitalization also appears (as a ‘prefix’), such as C &lt; c in English, D &lt; d in German, and V &lt; v in Dutch."
"Likewise,there are also some rules that may only be true in certain circumstances, such as -d &lt; -r in English (such as worked/worker, but certainly not for steed/steer.)"
"However, there are some rules that are wrong: the potential ‘s-’ prefix of English is never valid although word combinations like stick/tick spark/park, and slap/lap happen frequently in English."
Incorporating semantics can help determine the validity of each rule.
"To do this, one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus."
"The SVD decomposes M into the product of three matrices, U, D, and V T such that U and V T are orthogonal matrices and D is a diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal k-dimensional subspace."
This methodology is well-described in the literature[REF_CITE].
"In order to obtain semantic representations of each word, we apply our previous strategy[REF_CITE]."
"Rather than using a term-document matrix, we had followed an approach akin to that[REF_CITE], who performed SVD on a Nx2N term-term matrix."
The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.
"The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N columns represent those words that follow by up to 50 words."
"Since SVDs are more designed to work with normally-distributed data ([REF_CITE]p. 565), we fill each entry with a normalized count (or Z-score) rather than straight frequency."
We then compute the SVD and keep the top 300 singular values to form semantic vectors for each word.
"Word w would be assigned the semantic vector W= U w D k , where U w represents the row of U corresponding to w and D k indicates that only the top k diagonal entries of D have been preserved."
"As a last comment, one would like to be able to obtain a separate semantic vector for every word (not just those in the top N)."
SVD computations can be expensive and impractical for large values of N.
"Yet due to the fact that U and V T are orthogonal matrices, we can start with a matrix of reasonable-sized N and “fold in” the remaining terms, which is the approach we have followed."
"For details about folding in terms, the reader is referred to Manning and Schütze (1999, p. 563)."
"To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated[REF_CITE]."
The normalized cosine score between two words w 1 and w 2 is determined by first computing cosine values between each word’s semantic vector and 200 other randomly selected semantic vectors.
This provides a mean (µ) and variance ( 1 2 ) of correlation for each word.
"The NCS is given to be min cos( w1 , w2 ) µ k (1) NCS(w 1 ,w 2 ) k (1,2) 1 k"
"We had previously illustrated NCS values on various PPMVs and showed that this type of score seems to be appropriately identifying semantic relationships. (For example, the PPMVs of car/cars and ally/allies had NCS values of 5.6 and 6.5 respectively, whereas car/cares and ally/all had scored only -0.14 and -1.3.)"
"Further, we showed that by performing this normalizing process, one can estimate the probability that an NCS is random or not."
"We expect that random NCSs will be approximately normally distributed according to N(0,1)."
We can also estimate the distribution
"N(µ T , 1 T2 ) of true correlations and number of terms in that distribution (n T )."
"If we define a function - NCS (µ,1) P NCS exp[ ((x µ)/1) 2 ]dx then, if there were n R items in the ruleset, the probability that a NCS is non-random is n T - NCS (µ T ,1 T ) Pr(NCS) . (n R n T )- NCS (0,1) n T - NCS (µ T ,1 T )"
"We define Pr sem (w 1 &lt; w 2 )=Pr(NCS(w 1 ,w 2 ))."
"We choose to accept as valid relationships only those PPMVs with Pr sem T 5 , where T 5 is an acceptance threshold."
We showed in our earlier work that T 5 =85% affords high overall precision while still identifying most valid morphological relationships.
The first major change to our previous algorithm is an attempt to overcome some of the weaknesses of purely semantic-based morphology induction by incorporating information about affix frequencies.
"As validated[REF_CITE], high frequency word endings and beginnings in inflectional languages are very likely to be legitimate affixes."
"In English, for example, the highest frequency rule is -s &lt;L ."
CELEX suggests that 99.7% of our PPMVs for this rule would be true.
"However, since the purely semantic-based approach tends to select only relationships with contextually similar meanings, only 92% of the PPMVs are retained."
This suggests that one might improve the analysis by supplementing semantic probabilities with orthographic-based probabilities (Pr orth ).
Our approach to obtaining Pr orth is motivated by an appeal to minimum edit distance (MED).
MED has been applied to the morphology induction problem by other researchers (such[REF_CITE]).
"MED determines the minimum-weighted set of insertions, substitutions, and deletions required to transform one word into another."
"For example, only a single deletion is required to transform “rates” into “rate” whereas two substitutions and an insertion are required to transform it into “rating.”"
"Effectively, if Cost( &amp; ) is transforming cost, Cost(rates &lt; rate) = Cost(s &lt;L ) whereas Cost(rates &lt; rating)=Cost(es &lt; ing)."
"More generally, suppose word X has circumfix C 1 =B 1 /E 1 and pseudo-stem -S-, and word Y has circumfix C 2 =B 2 /E 2 also with pseudo-stem -S-."
"Then, Cost(X &lt; Y)=Cost(B 1 SE 1 &lt; B 2 SE 2 )=Cost(C 1 &lt; C 2 )."
"Since we are free to choose whatever cost function we desire, we can equally choose one whose range lies in the interval of [0,1]."
"Hence, we can assign Pr orth (X &lt; Y) = 1-Cost(X &lt; Y)."
This calculation implies that the orthographic probability that X and Y are morphological variants is directly derivable from the cost of transforming C 1 into C 2 .
The only question remaining is how to determine Cost(C 1 &lt; C 2 ).
"This cost should depend on a number of factors: the frequency of the rule f(C 1 &lt; C 2 ), the reliability of the metric in comparison to that of semantics ( . , where . [0,1]), and the frequencies of other rules involving C 1 and C 2 ."
We define the orthographic probability of validity as 2 . f(C 1 &lt; C 2 ) Cost(C 1 &lt; C 2 ) 1 max f(C 1 &lt; Z) max f(W &lt; C 2 ) ~ Z ~ W
"We suppose that orthographic information is less reliable than semantic information, so we arbitrarily set . =0.5."
"Now since Pr orth (X &lt; Y)=1-Cost(C 1 &lt; C 2 ), we can readily combine it with Pr sem if we assume independence using the “noisy or” formulation:"
Pr s-o (valid) =
Pr sem +Pr orth - (Pr sem Pr orth ). (2)
"By using this formula, we obtain 3% (absolute) more of the correct PPMVs than semantics alone had provided for the -s &lt;L rule and, as will be shown later, gives reasonable improvements overall."
"Since a primary role of morphology — inflectional morphology in particular — is to convey syntactic information, there is no guarantee that two words that are morphological variants need to share similar semantic properties."
"This suggests that performance could improve if the induction process took advantage of local, syntactic contexts around words in addition to the more global, large-window contexts used in semantic processing."
Consider Table 2 which is a sample of PPMVs from the ruleset for “-s &lt;L ” along with their probabilities of validity.
A validity threshold (T 5 ) of 85% would mean that the four bottom PPMVs would be deemed invalid.
"Yet if we find that the local contexts of these low-scoring word pairs match the contexts of other PPMVs having high scores (i.e., those whose scores exceed T 5 ), then their probabilities of validity should increase."
"If we could compute a syntax-based probability for these words, namely Pr syntax , then assuming independence we would have:"
Pr (valid) =
Pr s-o +Pr syntax - (Pr s-o Pr syntax )
Figure 3 describes the pseudo-code for an algorithm to compute Pr syntax .
"Essentially, the algorithm has two major components."
"First, for left (L) and right-hand (R) sides of each valid PPMV of a given ruleset, try to find a collection of words from the corpus that are collocated with L and R but which occur statistically too many or too few times in these collocations."
Such word sets form signatures.
"Then, determine similar signatures for a randomly-chosen set of words from the corpus as well as for each of the PPMVs of the ruleset that are not yet validated."
"Lastly, compute the NCS and their corresponding probabilities (see equation 1) between the ruleset’s signatures and those of the to-be-validated PPMVs to see if they can be validated."
Table 3 gives an example of the kinds of contextual words one might expect for the “-s &lt;L ” rule.
"In fact, the syntactic signature for “-s &lt;L ” does indeed include such words as are, other, these, two, were, and have as indicators of words that occur on the left-hand side of the ruleset, and a, an, this, is, has, and A as indicators of the right-hand side."
These terms help distinguish plurals from singulars.
Table 3: Examples of “-s &lt;L ” contexts
"There is an added benefit from following this approach: it can also be used to find rules that, though different, seem to convey similar information ."
Table 4 illustrates a number of such agreements.
"We have yet to take advantage of this feature, but it clearly could be of use for part-of-speech induction."
"Despite the semantic, orthographic, and syntactic components of the algorithm, there are still valid PPMVs, (X &lt; Y), that may seem unrelated due to 



"
"However, note that there is a path that can be followed along solid edges from every correct word to every other correct variant."
"This suggests that taking into consideration link transitivity (i.e., if X &lt; Y 1 , Y 1 &lt; Y 2 , Y 2 &lt; Y 3 ,... and Y t &lt; Z, then X &lt; Z) may drastically reduce the number of deletions."
There are two caveats that need to be considered for transitivity to be properly pursued.
"The first caveat: if no rule exists that would transform X into Z, we will assume that despite the fact that there may be a probabilistic path between the two, we will disregard such a path."
"The second caveat is that we will say that paths can only consist of solid edges, namely each Pr(Y i &lt; Y i+1 ) on every path must exceed the specified threshold."
"Given these constraints, suppose now there is a transitive relation from X to Z by way of some intermediate path  i ={Y 1, Y 2,.."
Y t }.
"That is, assume there is a path X &lt; Y 1, Y 1 &lt; Y 2 ,...,Y t &lt; Z. Suppose also that the probabilities of these relationships are respectively p 0 , p 1 , p 2 ,...,p t ."
"If is a decay factor in the unit interval accounting for the number of link separations, then we will say that the Pr(X &lt; Z) N . p ."
We along path  i has probability Pr  i t t j 0 j combine the probabilities of all independent paths between X and Z according to Figure 5:
the algorithms we test against.
"Furthermore, since CELEX has limited coverage, many of these lower-frequency words could not be scored anyway."
This cut-off also helps each of the algorithms to obtain stronger statistical information on the words they do process which means that any observed failures cannot be attributed to weak statistics.
Morphological relationships can be represented as directed graphs.
"Figure 6, for instance, illustrates the directed graph, according to CELEX, of words associated with “conduct.”"
We will call the words of such a directed graph the conflation set for any of the words in the graph.
"Due to the difficulty in developing a scoring algorithm to compare directed graphs, we will follow our earlier approach and only compare induced conflation sets to those of CELEX."
"To evaluate, we compute the number of correct ( &amp; ), inserted ( , ), and deleted ( &apos; ) words each algorithm predicts for each hypothesized conflation set."
"If X w represents word w&apos;s conflation set according to an algorithm, and if Y w represents its CELEX-based conflation set, then, &amp; = ~ w (|X w  w |/|Y w |) , &apos; = ~ w (|Y w -(X w  w )|/|Y w |) , and , = ~ w (|X w -(X w  w )|/|Y w |) ,"
"In making these computations, we disregard any CELEX words absent from our data set and vice versa."
Most capital words are not in CELEX so this process also discards them.
"Hence, we also make an augmented CELEX to incorporate capitalized forms."
"Table 5 uses the above scoring mechanism to compare the F-Scores (product of precision and recall divided by average of the two ) of our system at a cutoff threshold of 85% to those of our earlier algorithm (“S/[REF_CITE]”) at the same threshold; Goldsmith; and a baseline system which performs no analysis (claiming that for any word, its conflation set only consists of itself)."
The “S” and “C” columns respectively indicate performance of systems when scoring for suffixing and circumfixing (using the unaugmented CELEX).
The “A” column shows circumfixing performance using the augmented CELEX.
"Space limitations required that we illustrate “A” scores for one language only, but performance in the other two language is similarly degraded."
Boxes are shaded out for algorithms not designed to produce circumfixes.
Note that each of our additions resulted in an overall improvement which held true across each of the three languages.
"Furthermore, using ten-fold cross validation on the English data, we find that F-score differences of the S column are each statistically significant at least at the 95% level."
We have illustrated three extensions to our earlier morphology induction work[REF_CITE].
"In addition to induced semantics, we incorporated induced orthographic, syntactic, and transitive information resulting in almost a 20% relative reduction in overall induction error."
"We have also extended the work by illustrating performance in German and Dutch where, to our knowledge, complete morphology induction performance measures have not previously been obtained."
"Lastly, we showed a mechanism whereby circumfixes as well as combinations of prefixing and suffixing can be induced in lieu of the suffix-only strategies prevailing in most previous research."
"For the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that[REF_CITE], who assume some information about regular morphology in order to induce irregular morphology."
"We also believe that some findings of this work can benefit other areas of linguistic induction, such as part of speech."
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
SVMs are known to achieve high generalization perfor-mance even with input data of high dimensional feature spaces.
"Furthermore, by the Kernel princi-ple, SVMs can carry out training with smaller com-putational overhead independent of their dimen-sionality."
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk repre-sentations.
Experimental results show that our ap-proach achieves higher accuracy than previous ap-proaches.
"Chunking is recognized as series of processes — first identifying proper chunks from a sequence of tokens (such as words), and second classifying these chunks into some grammatical classes."
Various NLP tasks can be seen as a chunking task.
"Exam-ples include English base noun phrase identification (base NP chunking), English base phrase identifica-tion (chunking), Japanese chunk (bunsetsu) identi-fication and named entity extraction."
"Tokenization and part-of-speech tagging can also be regarded as a chunking task, if we assume each character as a token."
"Machine learning techniques are often applied to chunking, since the task is formulated as estimating an identifying function from the information (fea-tures) available in the surrounding context."
Various machine learning approaches have been proposed for chunking ([REF_CITE]; Tjong[REF_CITE]; Tjong[REF_CITE]; Tjong[REF_CITE]; van[REF_CITE]).
"Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy."
They do not provide a method for automatic selec-tion of given feature sets.
"Usually, heuristics are used for selecting effective features and their com-binations."
New statistical learning techniques such as Sup-port Vector Machines (SVMs)[REF_CITE]and Boosting[REF_CITE]have been proposed.
These tech-niques take a strategy that maximizes the margin between critical samples and the separating hyper-plane.
"In particular, SVMs achieve high generaliza-tion even with training data of a very high dimen-sion."
"Furthermore, by introducing the Kernel func-tion, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature."
"In the field of natural language processing, SVMs are applied to text categorization and syntactic de-pendency structure analysis, and are reported to have achieved higher accuracy than previous ap-proaches.[REF_CITE]."
"In this paper, we apply Support Vector Machines to the chunking task."
"In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using dis-tinct chunk representations."
"For the weighted vot-ing systems, we introduce a new type of weighting strategy which are derived from the theoretical basis of the SVMs."
Let us define the training samples each of which belongs    either  to  positive  or negative ! #&quot;% class $ &amp; $# as ( : ples.
"In the basic SVMs framework, we try to sep-arate the positive and / negative (. 0&quot; samples by a hyper- : plane expressed as: ; ."
SVMs find an “optimal &lt;- =1 ” hyperplane (i.e. an optimal parameter set for ) which separates the training data into two classes.
What does “optimal” mean?
"In order to define it, we need to consider the margin between two classes."
Figure 1 illus-trates this idea.
"Solid lines show two possible hyper-planes, each of which correctly separates the train-ing data into two classes."
Two dashed lines paral-lel to the separating hyperplane indicate the bound-aries in which one can move the separating hyper-plane without any misclassification.
We call the dis-tance between those parallel dashed lines as mar-gin.
SVMs find the separating hyperplane which maximizes its margin.
"Precisely, two ?- dashed .@A1B3&quot; lines and C $ margin 36EDFHG ( &gt; -JI ) can be expressed as: & gt; . =I -KI To maximize this margin, we should minimize."
"In other words, this problem becomes equiva-lent to solving the following optimization problem:"
"LNMPOQMSRTMVU W9X Y [3 I -JI \] \ ^ _`aQb W c d8dfe9X . ))&quot; $ 3n$ ` * ,"
The training samples which lie on either of two dashed lines are called support vectors.
It is known that only the support vectors in given training data matter.
This implies that we can obtain the same de-cision function even if we remove all training sam-ples except for the extracted support vectors.
"In practice, even in the case where we cannot sep-arate training data linearly because of some noise in the training data, etc, we can build the sep-arating linear hyperplane by allowing some mis-classifications."
"Though we omit the details here, we can build an optimal hyperplane by introducing a soft margin parameter o , which trades off between the training error and the magnitude of the margin."
"Furthermore, SVMs have a potential to carry out the non-linear classification."
"Though we leave the details[REF_CITE], the optimization problem can be rewritten into a dual form, where all feature vectors appear in their dot products ) .  "
"By p simply sub-stituting every dot product of and  in dual form with a certain Kernel function q , SVMs can handle non-linear hypotheses."
"Among many kinds of s Kernel functions available ) , we  will ) focus . on &quot;z$ the f{ -th polynomial s kernel: q ."
Use of -th polynomial kernel functions allows us to build an optimal separating hyperplane which takes s into account all combinations of features up to .
Statistical Learning Theory[REF_CITE]states that training error (empirical risk) |:} and test error (risk) |~ hold the following theorem.
"Theorem 1 (Vapnik) If  N6, is the VC dimen-sion of the class functions implemented by some ma-chine learning algorithms, then for all functions $&amp; of that class, with a probability of at least , the risk is bounded by 6 &quot;  O \ 6&quot; $ [K&amp; O% |~ | } (1) , where  is a non-negative integer called the Vapnik Chervonenkis (VC) dimension, and is a measure of the complexity of the given decision function."
The r.h.s. term of (1) is called VC bound.
"In order to minimize the risk, we have to minimize the empir-ical risk as well as VC dimension."
It is known that the following theorem holds for VC dimension  and margin &gt;[REF_CITE].
"Theorem 2 (Vapnik) Suppose + as the dimension of given training samples &gt; as the margin, and  as the smallest diameter which encloses all train-ing sample, then VC dimension  of the SVMs are bounded by %MPOR \ F \ !&quot; $  (2)  &gt; +"
"In order to minimize the VC dimension  , we have to maximize the margin &gt; , which is exactly the strategy that SVMs take."
Vapnik gives an alternative bound for the risk.
"Theorem 3 (Vapnik) Suppose | is an error rate estimated by Leave-One-Out procedure, | is bounded as 1  # )% =# |  1 %#D D (3)    r0+ x* `+* , &lt; H % "
"Leave-One-Out procedure is a simple method to ex-amine the risk of the decision function — first by removing a single sample from the training data, we construct the decision function on the basis of the remaining training data, and then test the removed sample."
"In this fashion, we test all , samples of the training data using , different decision functions. (3) is a natural consequence bearing in mind that sup-port vectors are the only factors contributing to the final decision function."
"Namely, when the every re-moved support vector becomes error in Leave-One-Out procedure, | becomes the r.h.s. term of (3)."
"In practice, it is known that this bound is less predic-tive than the VC bound."
There are mainly two types of representations for proper chunks.
"One is Inside/Outside representa-tion, and the other is Start/End representation. 1."
"Inside/Outside This representation was first introduced[REF_CITE], and has been applied for base NP chunking."
This method uses the following set of three tags for repre-senting proper chunks.
Current token is inside of a chunk.
Current token is outside of any chunk.
B Current token is the beginning of a chunk which immediately follows another chunk.
"Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions — IOB2,IOE1 and IOE2 (Tjong[REF_CITE])."
IOB2 A B tag is given for every token which exists at the beginning of a chunk.
Other tokens are the same as IOB1.
IOE1 An E tag is used to mark the last to-ken of a chunk immediately preceding another chunk.
IOE2 An E tag is given for every token which exists at the end of a chunk. 2.
"Start/End This method has been used for the Japanese named entity extraction task, and requires the following five tags for representing proper chunks[REF_CITE][Footnote_1] ."
"1 Originally, Uchimoto uses C/E/U/O/S representation. However we rename them as B/I/O/E/S for our purpose, since"
B Current token is the start of a chunk con-sisting of more than one token.
E Current token is the end of a chunk consist-ing of more than one token.
I Current token is a middle of a chunk con-sisting of more than two tokens.
S Current token is a chunk consisting of only one token.
Current token is outside of any chunk.
Examples of these five representations are shown in Table 1.
"If we have to identify the grammatical class of each chunk, we represent them by a pair of an I/O/B/E/S label and a class label."
"For example, in IOB2 representation, B-VP label is given to a to-ken which represents the beginning of a verb base phrase (VP)."
"Basically, SVMs are binary classifiers, thus we must extend SVMs to multi-class classifiers in order to classify three (B,I,O) or more (B,I,O,E,S) classes."
There are two popular methods to extend a binary classification task to that of q classes.
One is one class vs. all others.
The idea is to build q classi-fiers so as to separate one class from all others.
The other is pairwise !&amp; $ fFDE classification.
"The idea is to build q¢¡ classifiers considering all pairs of q classes, and final decision is given by their weighted voting."
There are a number of other methods to ex-tend SVMs to multiclass classifiers.
"For example, Dietterich and Bakiri[REF_CITE]and Allwe[REF_CITE]introduce a uni-fying framework for solving the multiclass problem by reducing them into binary models."
"However, we employ the simple pairwise classifiers because of the following reasons: \ ¥¤ (1) In general, SVMs require £ + £ +H¦ training cost (where + is the size of training data)."
"Thus, if the size of training data for individual bi-nary classifiers is small, we can significantly reduce the training cost."
"Although pairwise classifiers tend to build a larger number of binary classifiers, the training cost required for pairwise method is much more tractable compared to the one vs. all others. (2) Some experiments[REF_CITE]report that a combination of pairwise classifiers performs bet-ter than the one vs. all others."
"For the feature sets for actual training and classi-fication of SVMs, we use all the information avail-able in the surrounding context, such as the words, their part-of-speech tags as well as the chunk labels."
"More precisely, we give = the following features to identify the chunk label for the * -th word:"
"Here, ¨ is the word appearing = at * -th position, is the POS tag of ¨ , and is the (extended) chunk label for * -th word."
"In addition, we can reverse the parsing direction (from right to left) by using two chunk tags  ) = which   appear to the r.h.s. of the current token ( \ )."
"In this paper, we call the method which parses from left to right as forward parsing, and the method which parses from right to left as backward parsing. = \ for"
"Since the preceding =  chunk = labels ( forward parsing , \ for backward parsing) are not given in the test data, they are decided dy-namically during the tagging of chunk labels."
"The technique can be regarded as a sort of Dynamic Pro-gramming (DP) matching, in which the best answer is searched by maximizing the total certainty score for the combination of tags."
"In using DP matching, we limit a number of ambiguities by applying beam search with width ."
"In this paper, however, we apply determin-istic method instead of applying beam search with keeping some ambiguities."
The reason we apply de-terministic method is that our further experiments and investigation for the selection of beam width shows that larger beam width dose not always give a significant improvement in the accuracy.
"Given our experiments, we conclude that satisfying accuracies can be obtained even with the deterministic parsing."
Another reason for selecting the simpler setting is that the major purpose of this paper is to compare weighted voting schemes and to show an effective weighting method with the help of empirical risk estimation frameworks.
"Tjong Kim Sang et al. report that they achieve higher accuracy by applying weighted voting of sys-tems which are trained using distinct chunk rep-resentations and different machine learning algo-rithms, such as MBL, ME and IGTree(Tjong[REF_CITE]; Tjong[REF_CITE])."
"It is well-known that weighted voting scheme has a potential to maximize the margin between critical samples and the separating hyperplane, and pro-duces a decision function with high generalization performance[REF_CITE]."
"The boosting technique is a type of weighted voting scheme, and has been applied to many NLP problems such as parsing, part-of-speech tagging and text categoriza-tion."
"In our experiments, in order to obtain higher ac-curacy, we also apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations."
"Before applying weighted voting method, first we need to decide the weights to be given to individual systems."
We can obtain the best weights if we could obtain the accuracy for the “true” test data.
"However, it is impossible to estimate them."
"In boosting technique, the voting weights are given by the accuracy of the training data during the iteration of changing the frequency (distribution) of training data."
"However, we can-not use the accuracy of the training data for vot-ing weights, since SVMs do not depend on the fre-quency (distribution) of training data, and can sepa-rate the training data without any mis-classification by selecting the appropriate kernel function and the soft margin parameter."
"In this paper, we introduce the following four weighting methods in our exper-iments: 1."
Uniform weights We give the same voting weight to all systems.
This method is taken as the baseline for other weighting methods. 2.
"Cross validation Dividing training data into &amp;[URL_CITE] portions $ , we em-ploy the training by using  portions, and then evaluate the remaining  portion."
"In this fashion, we will have individual accuracy."
Final voting weights are  given by the average of these accuracies.  3.
"VC-bound By applying (1) and (2), we estimate the lower bound of accuracy for each system, and use the accuracy as a voting weight 3«$ &amp; ."
The 1 voting  s weight is calculated as: ¨ + . o
"The value of  , which represents the smallest  diameter enclosing all of the training data, is approximated by the maximum distance from the origin. 4."
"Leave-One-Out bound By using (3), we estimate the lower bound of the accuracy of a 3i system $&amp; ."
The voting weight is calculated as: ¨ | .
The procedure of our experiments is summarized as follows: 1.
We convert the training data into 4 representa-tions (IOB1/IOB2/IOE1/IOE2). 2.
"We consider two parsing directions (For-ward E36­ /Backward) for each representation, i.e. ¬ ¡ systems for a single training data set."
"Then, we employ SVMs training using these independent chunk representations. 3."
"After training, we examine the VC bound and Leave-One-Out bound for each of 8 systems."
"As for cross validation, we employ the steps 1 and 2 for each divided training data, and obtain the weights. 4."
We test these 8 systems with a separated test data set.
"Before employing weighted voting, we have to convert them into a uniform repre-sentation, since the tag sets used in individual 8 systems are different."
"For this purpose, we re-convert each of the estimated results into 4 representations (IOB1/IOB2/IOE2/IOE1). 5."
We employ weighted voting of 8 systems with respect to the converted 4 uniform representa-tions and the 4 voting ¬ schemes respectively.
"Fi-nally, we have (types of uniform 3®$ representa-tions) ¡ 4 (types of weights) results for our experiments."
"Although we can use models with IOBES-F or IOBES-B representations for the committees for the weighted voting, we do not use them in our voting experiments."
The reason is that the num-ber of classes are different (3 vs. 5) and the esti-mated VC and LOO bound cannot straightforwardly be compared with other models that have three classes (IOB1/IOB2/IOE1/IOE2) under the same condition.
We conduct experiments with IOBES-F and IOBES-B representations only to investigate how far the difference of various chunk representa-tions would affect the actual chunking accuracies.
We use the following three annotated corpora for our experiments. ° Base NP standard data set (baseNP-S)
"This data set was first introduced[REF_CITE], and taken as the standard data set for baseNP identification task 2 ."
"This data set consists of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank for the training data, and one section (20) for the test data."
The data has part-of-speech (POS) tags annotated by the Brill tag-ger[REF_CITE]. ° Base NP large data set (baseNP-L)
"This data set consists of 20 sections (02-21) of the WSJ part of the Penn Treebank for the training data, and one section (00) for the test data."
POS tags in this data sets are also anno-tated by the Brill tagger.
We omit the experi-ments IOB1 and IOE1 representations for this training data since the data size is too large for our current SVMs learning program.
"In case of IOB1 and IOE1, the size of training data for one classifier which estimates the class I and O becomes much larger compared with IOB2 and IOE2 models."
"In addition, we also omit to estimate the voting weights using cross valida-tion method due to a large amount of training cost. °"
Chunking data set (chunking)
This data set was used[REF_CITE]shared task(Tjong[REF_CITE]).
"In this data set, the total of 10 base phrase classes (NP,VP,PP,ADJP,ADVP,CONJP,"
"INITJ,LST,PTR,SBAR) are annotated."
"This data set consists of 4 sections (15-18) of the WSJ part of the Penn Treebank for the training data, and one section (20) for the test data 3 ."
"All the experiments are carried out with our soft-ware package TinySVM [URL_CITE] , which is designed and op-timized to handle large sparse feature vectors and large number of training samples."
This package can estimate the VC bound and Leave-One-Out bound automatically.
"For the kernel function, we use the 2-nd polynomial function and set the soft margin parameter o to be 1."
"In the baseNP identification task, the perfor-mance of the systems is usually measured /EB. # with   three  . rates   : precision    ,  recall &quot; and  f²D³ * * + , ,j j, ."
"In this paper  , we re- * * + , fer to ± ²D³ as accuracy."
Table 2 shows results of our SVMs based chunk-ing with individual chunk representations.
"This ta-ble also lists the voting weights estimated by differ-ent approaches (B:Cross Validation, C:VC-bound, D:Leave-one-out)."
We also show the results of Start/End representation in Table 2.
"Table 3 shows the results of the weighted vot-ing of four different voting [URL_CITE]µ´ methods: A: Uniform, B: Cross Validation ( ), C: VC bound, D: Leave-One-Out Bound  ."
"Table 4 shows the precision, recall and ± ²D³ of the best result for each data set."
We obtain the best accuracy when we ap-ply IOE2-B representation for baseNP-S and chunking data set.
"In fact, we cannot find a significant difference in the performance be-tween Inside/Outside(IOB1/IOB2/IOE1/IOE2) and Start/End(IOBES) representations."
Sassano and Utsuro evaluate how the difference of the chunk representation would affect the perfor-mance of the systems based on different machine learning algorithms[REF_CITE].
"They report that Decision List system performs better with Start/End representation than with In-side/Outside, since Decision List considers the spe-cific combination of features."
"As for Maximum Entropy, they report that it performs better with Inside/Outside representation than with Start/End, since Maximum Entropy model regards all features as independent and tries to catch the more general feature sets."
"We believe that SVMs perform well regardless of the chunk representation, since SVMs have a high generalization performance and a potential to select the optimal features for the given task."
"By applying weighted voting, we achieve higher ac-curacy than any of single representation system re-gardless of the voting weights."
"Furthermore, we achieve higher accuracy by applying Cross valida-tion and VC-bound and Leave-One-Out methods than the baseline method."
"By using VC bound for each weight, we achieve nearly the same accuracy as that of Cross valida-tion."
This result suggests that the VC bound has a potential to predict the error rate for the “true” test data accurately.
"Focusing on the relationship be-tween the accuracy of the test data and the estimated weights, we find that VC bound can predict the ac-curacy for the test data precisely."
"Even if we have no room for applying the voting schemes because of some real-world constraints (limited computation and memory capacity), the use of VC bound may al-low to obtain the best accuracy."
"On the other hand, we find that the prediction ability of Leave-One-Out is worse than that of VC bound."
Cross validation is the standard method to esti-mate the voting weights for different systems.
"How-ever, Cross validation requires a larger amount of computational overhead as the training data is di-vided and is repeatedly used to obtain the voting weights."
"We believe that VC bound is more effec-tive than Cross validation, since it can obtain the comparable results to Cross validation without in-creasing computational overhead."
"Tjong Kim Sang et al. report that they achieve accu-racy of 93.86 for baseNP-S data set, and 94.90 for baseNP-L data set."
"They apply weighted voting of the systems which are trained using distinct chunk representations and different machine learning al-gorithms such as MBL, ME and IGTree(Tjong[REF_CITE]; Tjong[REF_CITE])."
"Our experiments achieve the accuracy of 93.76 - 94.11 for baseNP-S, and 95.29 - 95.34 for baseNP- L even with a single chunk representation."
"In addi-tion, by applying the weighted voting framework, we achieve accuracy of 94.22 for baseNP-S, and 95.77 for baseNP-L data set."
"As far as accuracies are concerned, our model outperforms Tjong Kim Sang’s model."
"In the[REF_CITE]shared task, we achieved the accuracy of 93.48 using IOB2-F representati[REF_CITE][Footnote_5] ."
"5 In our experiments, the accuracy of 93.46 is obtained with IOB2-F representation, which was the exactly the same repre-sentation we applied[REF_CITE]shared task. This slight difference of accuracy arises from the following two reason : (1) The difference of beam width for parsing (N=1 vs. N=5), (2) The difference of applied SVMs package (TinySVM vs. º`»½¼8¾À¿ÂÁxÃfÄ ."
"By combining weighted voting schemes, we achieve accuracy of 93.91."
"In addition, our method also outperforms other methods based on the weighted voting(van[REF_CITE]; Tjong[REF_CITE])."
"° Applying to other chunking tasks Our chunking method can be equally appli-cable to other chunking task, such as English POS tagging, Japanese chunk(bunsetsu) iden-tification and named entity extraction."
"For fu-ture, we will apply our method to those chunk-ing tasks and examine the performance of the method. ° Incorporating variable context length model In our experiments, we simply use the so-called fixed context length model."
We believe that we can achieve higher accuracy by select-ing appropriate context length which is actu-ally needed for identifying individual chunk tags.
Sassano and Utsuro[REF_CITE]introduce a variable context length model for Japanese named entity identification task and perform better results.
"We will incor-porate the variable context length model into our system. ° Considering more predictable bound In our experiments, we introduce new types of voting methods which stem from the theo-rems of SVMs — VC bound and Leave-One-Out bound."
"On the other hand, Chapelle and Vapnik introduce an alternative and more pre-dictable bound for the risk and report their proposed bound is quite useful for selecting the kernel function and soft margin parame-ter[REF_CITE]."
We believe that we can obtain higher accuracy using this more predictable bound for the voting weights in our experiments.
"In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs)."
Experimental results on WSJ corpus show that our method outperforms other conventional ma-chine learning frameworks such MBL and Max-imum Entropy Models.
The results are due to the good characteristics of generalization and non-overfitting of SVMs even with a high dimensional vector space.
"In addition, we achieve higher accu-racy by applying weighted voting of 8-SVM based systems which are trained using distinct chunk rep-resentations."
ÁÃ.ÇiÈÉ¼i·O¿Ê·Å¹±¼iÈÉ¼iË·* Í ·»BÈÀË¼i·
Í ½ºÁaÈ Í  *Æ  ½BÈÀÁ¼R»&lt;.*ÁËÐ ¼iÈÀ½BÈÀÁ¼Ñ·¹º¹ºÁ¹±»aÈÉ¼ÒÅÓÆ*Ák¹º¸I¾R»5Æ Á¿À¿Ê·Æ*½B·
Í ÂÔ¹BÁkÄÕÅÓ½º¹±ÅÃÈÉ¼hÈÀ¼KÐ  Í ÈÀÅ¿ÊÁË¾i·: ×3¶·  ½ºÇR·-¸R¹B· Í ÈÉÆ*?ÁÃÂ&lt;+ Å¿Ê¿ÀÎ:*Æ  ¸R¾i½±ÅÚi¿À·ÓÂÔ¹ºÁÄÛ½ºÇR·¸R¹ºÁ»ºÁ
Í +½ºÇi·3»º¸W· ·Æ.Ç *ÁË¼iÈÀ½BÈÀÁ¼ß¸R¹BÁiÆ  **Á¼ Í  Í ½ºÇR· Í ÈÀÅ¿ÊÁË¾i·²ÇiÈÉ»º½BÁ¹ºÎ×:à) ÂÏ·ÅÃ½º¾I¹B·» ¹º·
Í ¾RÆ ·ÝÆ*  äÃéI× êêkëU½ºÁ ìåi×çæÃäkë9× í î [g^iGRï+&gt;^RABïz[ ñ *·  Í ;¿ÀÅ¹BË· ¿ÀÎ6 Í ·¸I·¼
Í   ½aÅÃ¿} Í  ÈÊ½)ÈÉ» ÂÏÁk¹  Í  Á¹±¹B·Æ*½½ºÇi·Ä  ÄÅÃ½ºÈÀÆ)»º¸W· ·Æ.:Ç   ¾R¹±ÅÆ !½ºÇi·O¸R¹ºÈÊÐ
ÄÅ¹ºÎ »ºÁ¾I¹ºÆ ·-ÁÃÂ·¹º¹ºÁ¹+ ÁÄ¸IÁk¾R¼
Í  Çi·¼øÆ*Á¼iÂÔ¹BÁk¼½º· Í
ÚkÎÿ»±¾RÆ.Ç] ýk½±¾ Í  ¼ ?
Í ½ºÁ:»BÙ+ÈÀ½ºÆ.Ç ½ºÁ  Å¸R¹ºÁ»ºÁ
"Í  Î ÄÅ¹±ÖÃ·ÞR·×ÍËI× ! $&quot; #&amp;(% )+ _ *- ÎK¿À· , ./, /: ;$&lt; &apos; ; 0&lt;2;=:+ !ü) Í ·6· ?½ ÅÃ¿}×ÊÞ~ìêêä &gt; Kà6ØKÈÊÐ  ê ?@&gt;BA ;· ?é 6C&gt; ??Å¼"
Í ED   ¸WÁ»±»ºÈÀÚi¿ÀÎÿ G · F ~*½ºÈÊØ·ÒÈÉ¼EÇK¾RÄÅ¼KÐ_ÇK¾RÄÅ¼ *ÁkÄÄ5¾R¼iÈÉÆ Å½BÈÀØ·Z»º· .Ç *Æ  Æ Å¸I¸I·Å¹±»?
Í  ?_.ÇiÈÉ¼i·   ½BÈÀÁ¼R» ö A ;· é &gt; ýKÁ¿À½ºÅ¾aÅ¼ Í ¶3ÅÈÀÚW· }¿ ÞÃäÃèèèi÷±Þ !
Å¾R»º·6ÈÀ½ Í H È F ~ ½±Çi·a»º¸W· ·Æ.!*Ð ÁËk¼i
J È I  Í Á¼P
L × K ¼EÅ ÍRÍ  ¹B· Ð »±¸IÁk¼R»B·»aÈÀ¼ Í  Á¹±¹B·Æ*½OÚW· ¿ÀÈÊ· ÂÔ» Þ »º¾IÆ.Ç3Å»OÅ¼3ÈÉÄ5Ð ¸i¿ÀÈÀÆ *Ák¼k½ºÅÃÈÉ¼iÈÉ¼iË  öÔ·×ËW N × MPO@QGRSQT% ; $; WYX &quot;=@:
Z&lt; &lt;[;]\ ;_`RS^ $; ab&apos;+; 02&lt; ;=: ÝÙ Çi·¼ ½ºÇR·Ó¾R»B·¹²ÇRÅ»²»±ÅÃÈ Í ?Å¼k½±»½BÁ Í  c Ä C +ÅÃ¿À½BÈÊÐ  Å¾I»B·OÆ*Ák¼R»ºÈ Í ·¹ºÅÚi¿À·
Í e È d 9Æ +ÂÏÁ¹+*· Í Ù  · Í ½BÁ3Æ***Ák¼RÆ*·¸i½BÈÀÁ¼ÒÅ¼ Í Å¿À»ºÁ+
Å¼I»BÙ i ;!Á Í?Å¼i· g Ù  f  &amp; ö h !!· ½ ÅÃ¿} ·
Developing dialogue systems is a complex pro-cess.
"In particular, designing efficient dialogue management strategies is often difficult as there are no precise guidelines to develop them and no sure test to validate them."
Several suggestions have been made recently to use reinforcement learning to search for the optimal management strategy for specific dialogue situations.
"These approaches have produced interesting results, including applications involving real world dia-logue systems."
"However, reinforcement learning"
"As dialogue systems become ubiquitous, dia-logue management strategies are receiving more and more attention."
They define the system be-havior and mainly determine how well or badly it is perceived by users.
Generic methodolo-gies exist for developing and testing manage-ment strategies.
Many of these take a user-centric approach based on Wizard of Oz stud-ies and iterative design[REF_CITE].
However there are still no precise guidelines about when to use specific techniques such as mixed-initiative.
Reinforcement learning has been used in several recent approaches to search for the optimal dialogue management strategy for specific dialogue situations[REF_CITE].
"In these approaches, a dia-logue is seen as a walk through a series of states, from an initial state when the dialogue begins until a terminal state when the dialogue ends."
The actions of the dialogue manager as well as those of the user influence the transitions be-tween states.
"Each transition is associated with a reward, which expresses how good or bad it was to make that transition."
A dialogue strat-egy is then seen as a Markov Decision Process[REF_CITE].
"Reinforcement learning can be used in this framework to search for an op-timal strategy, i.e., a strategy that makes the expected sum of rewards maximal for all the training dialogues."
The main idea behind re-inforcement learning is to explore the space of possible dialogues and select the strategy which optimizes the expected rewards ([REF_CITE]ch. 13).
"Once the optimal strategy has been found, it can be implemented in the final sys-tem."
Reinforcement learning is state-based.
It finds out which action to take next given the current state.
This makes the explanation of the strategy relatively hard and limits its potential re-use to other dialogue situations.
It is quite difficult to find out whether generic lessons can be learned from the optimal strategy.
In this pa-per we use inductive logic programming (ILP) to learn sets of rules that generalize the optimal strategy.
We show that these can be simpler to interpret than the decision tables given by re-inforcement learning and can help modify and re-use the strategies.
This is important as hu-man dialogue designers are usually ultimately in charge of writing and changing the strategies.
The paper is organized as follows.
We first describe in section 2 a simple dialogue system which we use as an example throughout the pa-per.
"In section 3, we present our method and results on using ILP to generalize the optimal dialogue management strategy found by rein-forcement learning."
We also investigate the use of the rules learned during the search of the op-timal strategy.
"We show that, in some cases, the number of dialogues needed to obtain the optimal strategy can be dramatically reduced."
Section 4 presents our current results on this aspect.
We compare our approach to other cur-rent pieces of work in section 5 and conclude the paper in section 6.
In this section we present a simple dialogue sys-tem that we use in the rest of the paper to de-scribe and explain our results.
This system will be used with automated users in order to sim-ulate dialogues.
The aim of the system is to be simple enough so that its operation is easy to understand while being complex enough to allow the study of the phenomena we are in-terested in.
This will provide a simple way to explain our approach.
"We chose a system whose goal is to find values for three pieces of information, called, unorigi-nally, a, b and c. In a practical system such as an automated travel agent for example, these values could be departure and arrival cities and the time of a flight."
"We now describe the system in terms of states, transitions, actions and rewards, which are the basic notions of reinforcement learning."
"The system has four actions at its disposal: pre-pare to ask (prepAsk) a question about one of the pieces of information, prepare to recognize (prepRec) a user’s utterance about a piece of information, ask and recognize (ask&amp;recognize) which outputs all the prepared questions and tries to recognize all the expected utterances, and end (end) which terminates the dialogue."
"We chose these actions as they are common, in one form or another, in most speech dialogue systems."
"To get a specific piece of information, the system must prepare a question about it and expect a user utterance as an answer be-fore carrying out an ask&amp;recognize action."
The system can try to get more than one piece of information in a single ask&amp;recognize action by preparing more than one question and prepar-ing to recognize more than one answer.
Actions are associated with rewards or penal-ties.
"Every system action, except ending, has a penalty of -5 corresponding to some imagined processing cost."
Ending provides a reward of 100 times the number of pieces of information known when the dialogue ends.
We hope that these numbers simulate a realistic reward func-tion.
They could be tuned to reflect user satis-faction for a real dialogue manager.
The state of the system represents which pieces of information are known or unknown and what questions and recognitions have been prepared.
There is also a special end state.
"For this example, there are 513 different states."
Pieces of information become known when users answer the system’s questions.
"In our tu-torial example, we used automated users."
"These users always give one piece of information if properly asked as explained above, and answer potential further questions with a decreasing probability (0.5 for a second piece of informa-tion, and 0.25 for a third in our example)."
We could tune these probabilities to reflect real user behavior.
Using simulated users enables us to quickly train our system.
It could also allow us to test the usefulness of ILP under different conditions.
In this section we explain how we obtain and interpret rules expressing the optimal manage-ment strategy found by reinforcement learning for the system presented in section 2 as well as a more realistic one.
We first search for the optimal strategy of our example system by using reinforcement learn-ing.
We do this by having repetitive dialogues with the automated users and evaluating the average reward of the actions taken by the sys-tem.
"When deciding what to do in each state, we choose the up-to-now best action with prob-ability 0.8 and other actions with uniform prob-ability totaling 0.2."
This allows the system to explore the dialogue space while preferably fol-lowing the best strategy found.
The optimal strategy for the tutorial example is shown in table 1.
"This strategy is very simple: ask one piece of information at a time until all the pieces have been collected, and then end the dialogue."
"A typical dialogue following this strategy would simply go like this, using the travel agent exam-ple:"
S: Where do you want to leave from?
S: Where do you want to go to?
S: When do you want to travel?
"Then, in order to learn rules generalizing the optimal strategy, we use foidl. foidl is a pro-gram which learns first-order rules from exam-ples ([REF_CITE]ch. 10). foidl starts with rules without condi-tions and then adds further terms so that they cover the examples given but not others."
"In our case, rule conditions are about properties of states and rule actions are the best actions to take."
Some advantages of foidl are that it can learn from a relatively small set of posi-tive examples without the need for explicit neg-ative examples and that it uses intentional back-ground knowledge[REF_CITE]. foidl has two main learning modes.
"When the examples are functional, i.e., for each state there is only one best action, foidl learns a set of ordered rules, from the more generic to the more specific."
When applying the rules only the first most generic rule whose precondition holds needs to be taken into account.
"When the ex- amples are not functional, i.e., there is at least one state where two actions are equally good, foidl learns a bag of rules."
All rules whose pre-conditions hold are applied.
Ordered rules are usually easier to understand.
"In this paper, we use foidl in both modes: functional mode for the tutorial example and non-functional mode for the other example."
The rules learned by foidl from the optimal strategy are presented in table 2.
Precondi-tions on states express the required and suffi-cient conditions for the action to be taken for a state.
"Uppercase letters represent variables (à la Prolog) which can unify with a, b or c. Rules were learned in functional mode."
The more generic rules are at the bottom of the ta-ble and the more specific at the top.
"It can be quite clearly seen from these rules that the strat-egy is composed of two kinds of rules: ordering rules which indicate in what order the variables should be obtained, and generic rules, typeset in italic, which express the strategy of obtaining one piece of information at a time."
"The order-ing, which is a, then b, then c, is arbitrary."
It was imposed by the reinforcement learning algo-rithm.
"The general strategy consists in prepar-ing to ask whatever piece of information the or-dering rules have decided to recognize, and then asking and recognizing a piece of information as soon as we can."
By expressing the strategy in the form of rules it is apparent how it operates.
It would then be relatively easy for a dialogue engineer to implement a strategy that keeps the optimal one-at-a-time questioning strategy but does not necessarily impose the same order on the process.
"Although the tutorial example showed how rules could be obtained and interpreted, it does not say much about the practical use of our ap-proach for real-world dialogues."
"In order to study this, we applied foidl to the optimal strategy presented[REF_CITE], which “presents a large-scale application of RL [reinforcement learning] to the problem of op-timizing dialogue strategy selection [...]”."
This system is a more realistic application than our introductory example.
It has been used by hu-man users over the phone.
The dialogue is about activities in New Jersey.
"A user states his/her preferences for a type of activity (mu-seum visit, etc.) and availability, and the sys-tem retrieves potential activities."
The system can vary its dialogue strategy by allowing or not allowing users to give extra information when answering a question.
It can also decide to con-firm or not to confirm a piece of information it has received.
The state of the system represents what pieces of information have been obtained and some information on how the dialogue has evolved so far.
"This information is repre-sented by variables indicating, in column or-der in table 3, whether the system has greeted the user (0=no, 1=yes), which piece of infor-mation the system wants (1, 2 or 3), what was the confidence in the user’s last an-swer (0=low, 1=medium, 2=high, 3=accept, 4=deny), whether the system got a value for the piece of information (0=no, 1=yes), the number of times the system asked for that piece of in-formation, whether the last question was open-ended (0=open-ended, 1=restrictive), and how well the dialogue was going (0=bad, 1=good)."
"The ac-tions the system can take are: greeting users (greetu), asking questions to users (asku), re-asking questions to users with an open or re-strictive question (reaskm/reasks), asking for confirmation or not (expconf/noconf)."
The op-timal strategy is composed of 42 state-action pairs.
It can be reduced to 24 equivalent rules.
We present the rules in table 3.
Some of these rules are very specific to the state they apply to.
"The more generic ones, which are valid what-ever the exact piece of information being asked, are typeset in italic."
The number of states they generalize is indicated in brackets.
These rules can be divided into four cate-gories:
Asking The first rule simply states that asking (asku) is the best thing to do if we have never asked the value of a piece of informa-tion before.
"Re-asking The second rule states that the sys-tem should re-ask for a value with a re-stricted grammar (reasks), i.e., a gram-mar that does not allow mixed-initiative, if the previous attempt was made with an open-ended grammar and the user denied the value obtained."
The third rule states that re-asking with an open-ended question (reaskm) is fine when the user denied the value obtained but the dialogue was going well until now.
"Confirming The fourth and fifth rules state that the system should explicitly confirm (expconf) a value if the grammar to get it was open-ended, the confidence in the value obtained being medium or even high."
No confirmation (noconf) is needed when the confidence is high and the answer was obtained with a restricted grammar even when the dialogue is going badly.
Greeting The last rule indicates that the sys-tem should greet the user if it has not done so already.
"When preconditions hold for more than one rule, which can for example be the case for reasks and reaskm in some situations, all the actions allowed by the activated rules are pos-sible."
The generic rules are more explicit than the state-based decision table given by reinforce-ment learning.
"For example, the rules about asking and greeting are obvious and it is reas-suring that the approach suggests them."
The effects of open-ended or closed questions on the reasking and confirming policies also become much more apparent.
Restricting the potential inputs is the best thing to do when re-asking except if the dialogue was going well until that point.
In that case the system can risk having an open-ended grammar.
The rules on confir-mation show the preference to confirm if the value was obtained via an open-ended grammar and that no confirmation is required if the sys-tem has high confidence in a value asked via a closed grammar even if the dialogue is going badly.
"Because the rules enable us to better un-derstand what the optimal policy does, we may be able to re-use the strategy learned in this specific situation in other dialogue situations."
It should be noted that the generic rules gen-eralize only a part of the total strategy (18 states out of 42 in the example).
Therefore a lot remains to be explained about the less generic rules.
"For example, the second piece of infor-mation does not require confirmation even if we got it with a low confidence value if the gram-mar was restrictive and the dialogue going well."
Under the same conditions the first piece of in-formation would require a confirmation.
The underlying reasons for these differences are not clear.
"Some of the decisions made by the re-inforcement learning algorithm are also hard to explain, whether in the form of rules or not."
"For example, the optimal strategy states that the third piece of information does not require con-firmation if we got it with low confidence and the dialogue was going badly."
It is difficult to explain why this is the best action to take.
"In this section, we discuss the use of rules during learning."
"Since rules can generalize the optimal strategy as we saw in the previous section, it is interesting to see whether they can general-ize strategies obtained during training."
"If the rules can generalize the up-to-now best strat-egy, we may then be able to benefit from the rules to guide the search for the optimal strat-egy throughout the search space."
"In order to test this, we ran the same reinforcement learn-ing algorithm to find out the optimal policy in the same setting as in the example system of sec-tion 3."
We also ran the same algorithm but this time we stopped it every 5000 iterations.
An iteration corresponds to a transition between states in the dialogue.
We then searched for rules summarizing the best policy found until then.
"We took the generic rules found, i.e., not the ones that are specific to a particular state, and used these to direct the search."
"That is to say, when a rule applied we chose to take the action it suggested rather than the action sug-gested by the state’s values (this is still sub-jected to the 0.8 probability selection)."
"The idea behind this was that, if the rules gener-alize correctly the best strategy, following the rules would guide us more quickly to the best policy than a blind exploration."
"It should be noted that the underlying representation is still state-based, i.e., we do not generalize the state evaluation function."
Our method is therefore guaranteed to find the optimal policy even if the actions suggested by the rules are not the right ones.
Table 4 summarizes the value of the best pol-icy found after each step of 5000 iterations.
A star (*) indicates that the optimal strategy has been consistently found.
"As can be seen from this table, using rules during learning improved the value of the best strategy found so far and reduced the number of iterations needed to find the optimal strategy for this particular example."
The main effect of using rules seems to be the stabilization of the search on the optimal policy.
The search without rules finds the optimal pol-icy but then goes off track before coming back to it.
"This may not be always the case [Footnote_1] since the best strategy found at first may not be opti-mal at all (for example, a rather good strategy at first is to end the dialogue immediately since it avoids negative rewards), or the dialogue may not be regular enough for rules to be useful."
"1 We do not claim any statistical evidence since we ran only a limited set of experiments on the effects of rules and present just one here. Even if we ran enough experiments to get statistically significant results, they would be of little use as they would depend on a par-ticular type of dialogues. Much more work needs to be done to evaluate the influence of rules on reinforcement learning and, if possible, in which conditions they are useful."
In these cases using rules may well be detrimen-tal.
"Nevertheless it is important to see that rules can help reduce, in this case by a factor of 2, the number of iterations needed to find the optimal strategy."
"Computationally, using rules may not be much different than not using them since the benefits of fewer reinforcement learn-ing cycles are counter-balanced by the inductive learning costs."
"However, requiring fewer train-ing dialogues is still an important advantage of this method."
This is especially true for systems that train online with real users rather than sim-ulated ones.
"In this case, example dialogues are an expensive commodity and reducing the need for training dialogues is beneficial."
Recent work on reinforcement learning and di-alogue management has mainly focused on how to reduce the search space for the optimal strat-egy.
"Because reinforcement learning is state based and there may potentially be a large num-ber of states, problems may arise when few di-alogues are available and the data too sparse to select the best strategy."
States can usually be collapsed to make this problem less acute.
The main idea here is to express the state of the dialogue by a limited number of features while keeping enough and the right kind of informa-tion to be able to learn useful strategies[REF_CITE].
There has also been new research on how to model the dialogue with partially ob-servable Markov models[REF_CITE].
Some work has also been done on finding out rules to select dialogue management strategies.
"For example,[REF_CITE]use ma-chine learning to learn rules detecting when di-alogues go badly."
The dialogue manager uses a strategy predefined by a dialogue designer.
"If a rule detects a bad dialogue, the dialogue strategy is changed to a more restrictive, more system guided strategy."
Our approach is dif-ferent from that work since the strategy is not predefined but based on the optimal strategy found by reinforcement learning.
Our rules not only detect in principle when a dialogue is go-ing badly but also indicate which action to take.
The efficiency of the rules obviously depends on the way the optimal strategy search space has been modeled and other conditions influencing learning.
Some pieces of work have been concerned with natural language processing from an induc- tive logic programming point of view.
"Notably, work on morphology[REF_CITE]and parsing[REF_CITE]has been carried out."
"However, as far as we know, the application of inductive logic programming to dialogue management is new."
"In this paper, we presented an approach for finding and expressing optimal dialogue strate-gies."
We suggested using inductive logic pro-gramming to generalize the results given by re-inforcement learning methods.
The resulting rules are more explicit than the decision tables given by reinforcement learning alone.
This al-lows dialogue designers to better understand the effect of the optimal strategy and improves po-tential re-use of the strategies learned.
We also show that in some situations rules may have a beneficial effect when used during learning.
"By guiding the search based on the best strategy found so far, they can direct a reinforcement learning program towards the optimal strategy, thus reducing the amount of training dialogues needed."
"More work needs to be done to deter-mine, if possible, under which conditions such improvements can be obtained."
"Recent contributions to statistical language model-ing for speech recognition have shown that prob-abilistically parsing a partial word sequence aids the prediction of the next word, leading to “struc-tured” language models that have the potential to outperform n-grams."
Existing approaches to struc-tured language modeling construct nodes in the par-tial parse tree after all of the underlying words have been predicted.
"This paper presents a different ap-proach, based on probabilistic left-corner grammar (PLCG) parsing, that extends a partial parse both from the bottom up and from the top down, lead-ing to a more focused and more accurate, though somewhat less robust, search of the parse space."
At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming.
"Prelim-inary perplexity and word-accuracy results appear to be competitive with previous ones, while speed is increased."
"In its current incarnation, (unconstrained) speech recognition relies on a left-to-right language model L, which estimates the occurrence of a next word w j given a sequence of preceding words c j = w 0j−1 (the context): [Footnote_1]"
"1 As a shorthand, w ba denotes a sequence w a w a+1 ...w b if b ≥ a, else it is the empty sequence."
L(w j |c j ) = p̂(w j |c j ).
L is called a language model (LM).
"Obviously the context space is huge and even in very large training corpora most contexts never occur, which prohibits a reliable probability esti-mation."
"Therefore the context space needs to be mapped to a much smaller space, such that only the essential information is retained."
"In spite of its simplicity the trigram LM, that reduces c j to w j−2 , is hard to improve on and still the main language model component in state-of-the-art speech recog-nition systems."
"It is therefore commonly used as a baseline in the evaluation of other models, including the one described in this paper."
Structured language models (SLM) introduce parsing into language modeling by alternating be-tween predicting the next word using features of partial parses of the context and extending the par-tial parses to cover the next word.
"Following this approach,[REF_CITE]obtained a SLM that slightly improves on a trigram model both in perplexity and recognition performance."
"The Chelba-Jelinek SLM is, to our knowledge, the first left-to-right LM using parsing techniques that is successfully applied to large vocabulary speech recognition."
It is built on top of a lexicalized prob-abilistic shift-reduce parser that predicts the next word from the headwords (“exposed” heads) and categories of the last two predicted isolated con-stituents of the context.
Then the predicted word becomes the last isolated constituent and the last two constituents are repeatedly recombined until the parser decides to stop.
"A dynamic programming (DP) version of Chelba’s parser, inspired on the CYK chart parser, was proposed[REF_CITE]."
"Our implementation is roughly quadratic in the length of the sentence, but not significantly faster than Chelba’s non-DP parser."
"It scored somewhat lower in perplexity before reestimation (presumably by avoiding search errors), but remained roughly at the same level after full inside-outside reestimati[REF_CITE]."
An obvious weakness of the Chelba-Jelinek SLM is the bottom-up behavior of the parser: it creates isolated constituents and only afterwards is it able to check whether a constituent fits into a higher struc-ture.
"The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be con-ditioned on the underlying lexical information, thus producing a lot of wrong parses."
"The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing[REF_CITE], which follows a mixed bottom-up and top-down ap-proach."
"Its potential to enhance parsing efficiency has been recognized[REF_CITE], who simulated a left-corner parser with a top-down best-first parser applying a left-corner-transformed PCFG grammar."
"For the language model described in this paper, however, we implemented a DP ver-sion of a native left-corner parser using a left-corner treebank grammar (containing projection rules in-stead of production rules)."
The efficiency of our im-plementation further allowed to enrich the history annotation of the parser states and to apply a lexi-calized grammar.
The following section contains a brief review of Manning’s PLCG parser.
"Section 3 describes how it was adapted to our SLM framework: we introduce lexicalization and context-sensitivity, present a DP algorithm using a chart of parser states and finally we define a language model based on the adapted PLCG parser."
At the end of the same section we ex-plain how the initial language model can be trained on additional plain text through a variant of inside-outside reestimation.
In section 4 we evaluate a few PLCG-based SLMs obtained from the Penn Tree-bank and BLLIP WSJ Corpus.
We present test set perplexity measurements and word accuracy after n-best list rescoring to assess their viability for speech recognition.
The parameters of a PLCG are called projection probabilities.
"They are of the form p(Z → X α|X, G), to be read as “given a completed constituent X dom-inated by a goal category G, the probability that there is a Z that has X as its first daughter and α as its next daughters”."
"A PLCG contains essentially the same rules as a probabilistic context-free gram-mar (PCFG), but the latter conditions the rule prob- abilities on the mother category Z (production prob-abilities)."
In both cases the joint probability of the entire parse tree and the parsed sentence is the prod-uct of the production resp. projection probabilities of the local trees it consists of.
"While PCFG parsing proceeds from the top down or from the bottom up, PLCG naturally leads to a parsing scheme that is a mixture of both."
The ad-vantages of this are made clear in the subsections below.
"Formally, a PLCG parser has three elemen-tary operations: • SHIFT : given that an unexpanded constituent G starts from position i, shift the next word w i with probability p s (w i |G) (G is called the goal category); • PROJECT : given a complete constituent X, dominated by a goal category G, starting in po-sition i and ending in j, predict a mother con-stituent Z starting in position i and completed up till position j, and zero or more unexpanded sister constituents α starting in j with probabil-ity p p (Z → X α|X, G); • ATTACH : given a complete constituent X dom-inated by a goal category G, identify the first as the latter with probability p a (X, G)."
In this subsection we present the basic parsing al-gorithm and its data structures and operations.
"In the subsections that follow, we will introduce lexi-calization and context-sensitivity by extending this framework."
"The PLCG parsing process is interpreted as a search through a network of states, a compact re-presentation of the search space."
The network nodes correspond to states and the arcs to operations (an-notated with transition probabilities).
A (partial) parse corresponds to a (partial) path through the net-work.
"The joint probability of a partial parse and the covered part of the sentence is equal to the partial path probability, i.e. the product of the probabilities of the transitions in the path."
"We write a state q as q = (G; Z → i X ? j β; µ, ν) (1) where G is the goal category, Z is the category of a constituent from position i complete up till position j, X is the first daughter category, β denotes the re-maining unresolved daughters of Z, and µ and ν are forward and inner probabilities defined below."
"The wildcard ? symbolizes zero or more resolved daugh-ter categories: we make abstraction of the identities of resolved daughters (except the first one), because further parser moves do not depend on them."
"If β is empty, q is called a complete state, otherwise q is a goal state."
Given a state q as defined in (1).
"We define its for-ward probability µ = µ(q) as the sum of the prob-abilities of the paths ending in q, starting in the ini- j−1 tial state and generating w 0 ."
"As a consequence, µ(q) = p(w 0j−1 , q) (joint probability)."
"The inner probability ν = ν(q) is the sum of the j−1 probabilities of the paths generating w i , ending in q and starting with a SHIFT of w i ."
"As a conse-quence, ν(q) = p(w ij−1 , q)."
Note that the forward and inner probabilities of the final state should be identical and equal to p(S).
In this paragraph we reformulate the classic PLCG parser operations in terms of transitions between states.
We hereby specify update formulas for for-ward and inner probabilities.
"Shift The SHIFT operation starts from a goal state q = (G; Z → i X ? j Y β; µ, ν) (2) and shifts the next word w at position j of the input by updating q 0 or generating a new state q 0 where [Footnote_2] q 0 = (Y ; W → j w ? j+1 ; µ 0 += µp, ν 0 = p) (3) with transition probability p = p s (w|Y ). (4)"
"2 The C-like shorthand notation µ 0 += µp means that µ 0 is set to µp if there was no q 0 in the chart yet, otherwise µ 0 is incremented with µp."
"If q 0 already lives in the chart, only its forward prob-ability is updated."
The given update formula is jus-tified by the relation µ(q 0 ) = X µ(q)p(q ⇒ q 0 ) q⇒ s q 0 where the sum is over all SHIFT transitions from q to q 0 and p(q ⇒ q 0 ) denotes the transition probability from q to q 0 .
Computing ν(q 0 ) is a trivial case of the definition.
"Projection From a complete state, two transitions are possible: ATTACH to a goal state with a prob-ability p a or PROJECT with a probability 1 − p a ."
"PROJECT starts from a complete state q = (G; Z → i X ? j ; µ, ν) (5) and generates or updates a state q 0 = (G; T → i Z ? j α; µ 0 += µp, ν 0 += νp) (6) with transition probability p = p p (T, α|Z, G) · (1 − p a (Z, G)). (7)"
"Again, the forward probability is computed recur-sively as a sum of products."
"Now ν 0 needs to be accumulated, too: the constituent Z in general may be resolved with more than one different X, which each time adds to ν 0 ."
Note that a mother constituent inherits G from her first daughter (left-corner).
"Attachment Given a complete state q as in (5) where G = Z and some goal state q 00 in the par-tial path leading to q q 00 = (G 00 ; T → h U ? i Z β; µ 00 , ν 00 ) (8) then the ATTACH operation is a transition from q to q 0 with q 0 = (G 00 ; T → h U ? j β; µ 0 += µ 00 νp/ν 00 , ν 0 += νp) (9) and transition probability p = p a (Z, G) · ν 00 . (10)"
"Why can µ 0 not be updated from µ, similarly to (3) and (6)?"
The reason is that ATTACH makes use of non-local constraints: the transition from q to q 0 is only possible if a matching goal state q 00 occurred in a path leading to q.
"Therefore computing µ as in (3) and (6) would include all paths that generate q 0 , also those that do not contain q 00 ."
"Instead, the update of µ 0 in (9) combines all paths leading to q 00 with the paths starting from q 00 and ending in q."
The update of ν 0 follows an analogous reasoning.
The parser produces a set of states that can be conve-niently organized in a staircase-shaped chart similar to the one used by the CYK parser.
"In the chart cell with coordinates (i, j) we store all the states starting in i and completed up till position j."
"Following[REF_CITE], we represent a sentence by a sequence of word identities starting with a sentence-begin token h s i, that is used in the con-text but not predicted, followed by a sentence-end token h /s i, that is predicted by the model."
"We are collecting the sentence proper together with h /s i un-der a node labeled TOP 0 , and the TOP 0 node together with h s i under a TOP node."
The parser starts from the initial state q I = ( TOP ; TOP /h s i → −1 SB /h s i ? 0
"TOP 0 ; 1, 1). (11)"
"After processing the sentence S = w 0N−1 and pro-vided a full parse was found, the final state q F = ( TOP ; TOP /h s i → −1 SB /h s i ? N ; p(S), p(S)) (12) is found in cell (−1, N)."
Now we are ready to formulate the parsing algo-rithm.
"Note that we treat an ATTACH operation as a special PROJECT , as explained in Sec. 4.1."
"2 for i ← j − 1, j − 2 to −1 3 foreach complete state q in cell (i, j) 4 foreach proj in projections(q) 5 if goal(q) = cat(q) and proj = ‘attach’ 6 for h ← i − 1, i − 2 to −1 7 foreach goal state m in cell (h,i) matching q 8 q 0 ← ATTACH (q, m) 9 add q 0 to cell (h, j) 10 else 11 q 0 ← PROJECT (q) 12 add q 0 to cell (i, j) 13 if q 0 is complete, recursively add further projections/attachments 14 if j = N 15 break 16 for i ← −1, 0 to j − 1 17 foreach goal state q in cell (i, j) 18 q 0 ← SHIFT (q, w j )"
"Probably the most important shortcoming of PCFG’s is the assumption of context-free rule prob-abilities, i.e. the probability distribution over pos-sible righthand sides given a lefthand side is inde-pendent from the function or position of the left-hand side."
This assumption is quite wrong.
"For instance, in the Penn Treebank an NP in subject position produces a personal pronoun in 13.7% of the cases, while in object position it only does so in 2.1% of the cases[REF_CITE]."
"Furthermore, findings from corpus-based linguis-tic studies and developments in functional gram-mar indicate that the lexical realization of a con-text, besides its syntactic analysis, strongly influ-ences patterns of syntactic preference."
Today’s best automatic parsers are made substantially more ef-ficient and accurate by applying lexicalized gram-mar[REF_CITE].
"In our work we did not attempt to find semantic gen-eralizations (such as casting a verb form to its infini-tive form or finding semantic attributes); our simple (but probably suboptimal) approach, borrowed[REF_CITE], is to percolate words upward in the parse tree in the form in which they appear in the sentence."
"In our experiments, we opted to hardcode the head posi-tions as part of the projection rules. [Footnote_3]"
"3 Inserting a probabilistic head percolation model, as[REF_CITE], may be an alternative."
The nodes of the resulting partial parse trees thus are annotated with a category label (the CAT feature) and a lexical label (the WORD feature).
"The notation (1) of a state is now replaced with q = (G, L 1 , L 2 ; Z/z → i X/x ? j β; µ, ν) (13) where z is the WORD of the mother (possibly empty), x is the WORD of the first daughter (not empty), and the extended context contains • G = CAT of a goal state q g ; • L 1 = ( CAT , WORD ) of the state q 1 projecting q g ; • L 2 = ( CAT , WORD ) of the state q 2 projecting a goal state dominating q 1 ."
"If the grammar only contains unary and binary rules, L 1 and L 2 correspond with Chelba’s concept of exposed heads — which was in fact the idea be-hind the definition above."
The mixed bottom-up and top-down parsing order of PLCG allows to condi-tion q on a goal constituent G higher up in the par-tial tree containing q; this turns out to significantly improve efficiency with respect to Jelinek’s bottom-up chart parser.
"In this section, we extend the parser operations of Sec. 3.1.3 to handle context-sensitive and lexical-ized states."
The forward and inner probability up-date formulas remain formally the same and are not repeated here.
"The SHIFT operation q ⇒ s q 0 is a transition from q to q 0 with probability p where q = (G, L 1 , L 2 ; Z/z → i X/x ? j Y β; µ, ν) (2 0 ) q 0 = (Y, X/x, L 1 ; W /w → j W /w ? j+1 ; µ 0 , ν 0 ) (3 0 ) p = p s (w j |q). (4 0 )"
"The PROJECT operation q ⇒ p q 0 is a transition from q to q 0 with probability p where q = (G, L 1 , L 2 ; Z/z → i X/x ? j ; µ, ν) (5 0 ) q 0 = (G, L 1 , L 2 ; T/t → i Z/z ? j α; µ 0 , ν 0 ) ([Footnote_6] 0 ) p = p p (T, α|q) · (1 − p a (q)) (7 0 )"
6 Care has to be taken that an outer probability is complete before it propagates to other items. A topological sort could serve this purpose.
"If Z is in head position, t = z; otherwise t is left unspecified."
"The ATTACH operation q ⇒ a q 0 is a transition from q to q 0 given q 00 with a probability p where q 00 = (G, L 1 , L 2 ; Z/z → h X/x ? i Yβ; µ 00 , ν 00 ) (8 0 ) q = (Y, X/x, L 1 ; Y/y → i T/t ? j ; µ, ν) q 0 = (G, L 1 , L 2 ; Z/z 0 → h X/x ? j β; µ 0 , ν 0 ) (9 0 ) p = p a (q) · ν 00 (10 0 )"
"If Y is in head position, z 0 = y; otherwise, z 0 = z."
A language model (LM) is a word sequence pre-dictor (or an estimator of word sequence probabili-ties).
"Following common practice in language mod-eling for speech recognition, we predict words in a sentence from left to right [Footnote_4] with probabilities of the form p(w j |w 0j−1 )."
4 Since this allows the language model to be applied in early stages of the search.
Suppose the parser has worked its way through w 0j−1 and is about to make w j - SHIFT transitions.
Then we can write p(w j |q)p(q|w 0j−1 ). p(w j |w 0j−1 ) = X (14) q∈ j where j is the set of goal states in position j. The factor p(w j |q) is given by the transition probability associated with the SHIFT operation. 5
"On the other hand, note that"
X µ(q) = p(w 0j−1 ) µ(q) = X (15) q∈ j q∈ j where j is the set of states in position j that resulted from SHIFT operations.
"The first equa-tion holds because there are only PROJECT and AT - TACH transitions between the elements of j and j , since the sum of outgoing transitions from each state in that region equals 1 and therefore the total probability mass is preserved."
P P q∈ j p(w j |q)µ(q) . (16) q∈ j µ(q)
"The p p , p s and p a submodels can be rees-timated with iterative expectation-maximization, which needs the computation of frequency expec-tations."
"For this purpose we define the outer prob-ability of a state q, written as ξ(q), as the sum of probabilities of precisely that part of the paths that is not included in the inner probability of q."
The outer probability of a complete state is analogous to Baker’s (1979) definition of an outside probability.
"The outer probabilities are computed in the re-verse direction starting from q F , provided that a list of backward references were stored with each state (ξ(q 0 ) ≡ ξ 0 , ξ(q 00 ) ≡ ξ 00 ): 6 • ξ(q F ) = 1. • Reverse ATTACH (cfr. (8 0 , 9 0 , 10 0 )): ξ+= ξ 0 p and ξ 00 += ξ 0 νp/ν 00 ."
"These formulas are made clear in Fig. 1. • Reverse PROJECT (cfr. (5 0 , 6 0 , 7 0 )): ξ+= ξ 0 p. • A reverse SHIFT is not necessary, but could be used as a computational check. 5 Consequently the computation of LM probabilities re-quires almost no extra work."
A model p(w j |q) used in (14) different from p s (w j |q) used by the parser may be chosen how-ever.
"Now the expected frequency of a transition o ∈ {s, p, a} from q to q 0 in a full parse of S is"
E[Freq(q ⇒ o q 0 |S)] = XPr (path|S)Freq(q ⇒ o q 0 |path). (17) all paths
"Since all full parses terminate in q F , the final state, ν(q F ) = µ(q F ) = Pr(S)."
"E[Freq(q ⇒ o q 0 |S)] = ( 1 ν(q 0 )ξ(q 0 ) if o = s, ν(q F ) (18) ν(q F )1 ν(q)p(q ⇒ o q 0 )ξ(q 0 ) else."
The expected frequencies required for the reesti-mation of the conditional distributions are then ob-tained by summing (18) over the state attributes from which the required distribution is independent.
We have trained two sets of models.
The first set was trained on sections 0–20 of the Penn Treebank (PTB)[REF_CITE]using sections 21–22 for development decisions and tested on sections 23–24.
"The second set was trained on the BLLIP WSJ Corpus (BWC), which is a machine-parsed[REF_CITE]version of (a selection of) the ACL/DCI corpus, very similar to the selection made for the WSJ0/1 CSR corpus."
"As the training set, we used the BWC minus the WSJ0/1 “dfiles” and “efiles” intended for CSR development and evalua-tion testing."
"The PTB devset was used for fixing submodel pa-rameterizations and software debugging, while per-plexities are measured on the PTB testset."
The BWC trainset was used in rescoring N-best lists in order to assess the models’ potential in speech recognition.
Both the PTB and BWC underwent the following preprocessing steps: (a) A vocabu-lary was fixed as the 10k (PTB) resp. 30k (BWC) most frequent words; out-of-vocabulary words were replaced by hunki.
Numbers in Arabic digits were replaced by one token ‘N’. (b) Punctuation was re-moved. (c) All characters were converted to lower-case. (d) All parse trees were binarized in much the same way as detailed in ([REF_CITE]pp. 12–17) ; non-terminal unary productions were eliminated by collapsing two nodes connected by a unary branch to one node annotated with a combined label.
This step allowed a simple implementation and compar-ison of results with related publications.
"We dis-tinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-of-speech. (e) All constituents were annotated with a lexical head using deterministic rules[REF_CITE]."
"The training then proceded by decomposing all parse trees into sequences of SHIFT , PROJECT and ATTACH transitions."
The submodels were finally estimated from smoothed relative counts of transi-tions using standard language modeling techniques: Good-Turing back-[REF_CITE]and deleted in-terpolati[REF_CITE].
Table 1 lists test set perplexities (excluding OOVs and unparsed parts of sentences) of Good-Turing smoothed back-off models (GT) and deleted-interpolation smoothed (DI) models trained on the PTB trainset and tested on the PTB testset.
We ob-served similar results with both smoothing meth-ods.
"As a baseline, word trigram (a) was trained and tested on the same material."
"The PPL obtained with the PLCG-based LM (b), using parametriza-tions (19) and (20), is not much lower than the base-line PPL. [Footnote_7] Interpolation (c) with the baseline how-ever yields a relative PPL reduction of 14 to 16% with respect to the baseline."
"7 Using parametrizations p p (T,α|z, G, L 1 . CAT ) for projec-tion from W-items and p p (T, α|G, Z, X, z) for other projec-tions, we recently obtained a[REF_CITE](and 155 when inter-polated). This result is left out from the discussion in order to keep it clear and complete."
"Parse accuracy is around 79% for both labeled precision and recall on section 23 of PTB (exclud-ing unparsed sentences, about 4% of all sentences)."
"In comparison, with our own implementation of Chelba-Jelinek, we measured a labeled precision and recall of 57% and 75% on the same input."
"These results seem fairly low compared to other recent work on large-scale parsing, but may be partly due to the left-to-right restriction of our language mod-els, [Footnote_8] which for instance prohibits word-lookahead."
8 Not to be confused with left-to-right parsing.
"Moreover, while we measured accuracy against a binarized version of PTB, the original parses are rather flat, which may allow higher accuracies."
The main target application of our research into LM is speech recognition.
"We performed N-best list rescoring experiments on the DARPA WSJ Nov ’92 evaluation test set, non-verbalized punctuation."
"The N-best lists were obtained from the L&amp;H Voice Xpress v4 speech recognizer using the standard tri-gram model included in the test suite (20k open vo-cabulary, no punctuation)."
In Table 2 we report word-recognition error rates (WER) after rescoring using Chelba-Jelinek and PLCG-based models.
Both DI and GT smooth-ing methods yielded very comparable results.
"Due to technical limitations, all the models except the baseline trigram were trimmed by ignoring highest-order events that occurred only once."
The best PLCG-based SLM trained on the BWC train set (f) performs worse than the official word trigram (a).
"However, since the BWC does not com-pletely cover the complete WSJ0 LM train material and slightly differs in tokenization, it is more fair to compare with the performance of a word trigram trained on the BWC train set (b)."
Results (g) and (h) show that the PLCG-based SLM lowers WER with 4% relative when used in combination with the baseline models.
A comparable result was obtained with the Chelba-Jelinek SLM (results (d) and (e)).
The PLCG-based SLM exposes a slight loss of ro-bustness in the reduced recognition rate when it is used as a stand-alone rescoring LM.
"Combined with a word trigram LM however, perplexity and WER reductions with respect to a word 3-gram baseline seem similar to those obtained with the Chelba-Jelinek SLM and those previously reported[REF_CITE]."
"On the other hand, the PLCG-based SLM is significantly faster and obtains a higher parsing accuracy."
In the future we plan to evaluate full EM reesti-mation of the models on the trainset using the for-mulas given in this paper.
KQ&lt; =X %G =: `Ä`ekROKQTH: =: P T M &lt; R^%E Z    ECTHKÅã  TH:=PQPLÄS? &gt;EÁã¢&lt;&gt;KLR^E
M GåKQ4SFEÁã¢&lt; &gt;&lt; KLR^E-&lt; [M Z &gt;&lt; UHÚ ROFEN-Y =X G6:=`  `Ä:kG Â :=KQPQU-R M ` M SFECP=: &lt;@? M=Â ROFE&quot;T M &gt;&lt; ^U ^R
G6:=KQ&lt;@ROUá=: %U U Mk THKQ:kR^ECS 
KLROøSFEÁã¢&lt;&gt;KLR^EC&gt;&lt; ECUOUHàÌ{K&lt;&gt;:=PQPL=? \¿ROFEÕKDã¢&lt; &lt;&gt;KLROKQJ=E
X ECFEHG&lt; %=: PzS&gt;KU^ROKQ&lt;&gt;TÁRHÚ ROFE-X=G6=: `Ä`ÄkG: M &gt;&lt; :kX[=:
K±R&lt; ^%G EC:kROU=:=&gt;&lt; SéKQ`  EHG%:kROKLJ=E ÂÏM %G `ÄU =M Âbl ECS&gt;KQUOèJ=EHG%a4UekGOE: ;KQ&lt; ROFEC` K@&lt; R^EHG%T6&gt;=: FX&lt; =ECka4PQ: ?=à R¸? ¦ ECU&gt; E =M ÂÏMÂ-ZPQPR^MR^EHG%KQ:=FXûUOEC&lt; 4TÁEÌK&lt; @&lt; &gt;&lt; ^R
TECM&gt;&lt; GOGOECTÁROPL?ø:=THTÁE PK Â ^R?
X=G%:=`Ä`ekGCö: £-N$]  *U  KLROT6 Mk LÛROFE
DÛ&apos;ÎPQKLX[}U R6Ð6ÚKLROT6 Û Mk LÛ PQKLX[DÛ R   a ( Û e  Û &amp; \ 
"KQUåROFENPQKLXÛ }KQUÕPQKLX[ RåU[ R  NKLROT$]%&gt; ECS_ MâN9&lt; ;Ð6Ú PQKLR  ]$f Û ¦ ]$A  &quot; ,\   &quot;T Xe#e a Ü eQ(  Þ ( e*(  &apos;e*( d h Þ 1Î}!=: &lt; ? _ âN9;%:=S4KQ:kR M G  \  ]$f ¦ ]$A0K&lt;éDKLROT6FEC&lt;  N$]  ÷KQUÕROFEHG%Eõ=: @&lt; ?ÔG%:  ( & gt;KQkR:"
M GbKQBÎ&lt;}P M &quot;EHG DKLROT£-øPQKLX%FEC&lt; [Ð6Ú ROKQ&lt;FX Û kÛ
Ý Û &quot;T | a  N]$
M &gt;=: P Âä ]1 ewh \] a e d e  &apos;e5( d \m\ &gt;S KQ` RO&gt;!E PQKLX[ ROKQ&lt; M Ä: &gt;=: P Â Ð6à \ -&apos;e(  ¬FÈ® }ÒäÈ®
Ò WË ä¬&gt;È® ½é«-®l¯¾C¼¢Ç¿¬
I3JX D HE GF *F D FE&quot;{EHG%U ^M EC&gt;&lt; kG: :=%P T6 kR: _^ECPQPQKQREC&lt; ^R^EHGCEª-U\ §k%=UOKQU= ^ RO:=Ð^&lt; RÃÎ :ÕUO`Ä ªÚ´Î:=PP¶G}M a  M R ¦ |
T Z GOG%EC&lt;@ROPL;? azECKQ&lt;=ECP =M  ECS:   &quot;  U^ECkG: %T6 _
EC&lt; R^EHGC-SFECU\ %KLX[&lt;FECS ÂäM G Z UOE;KQ&lt;Ô`ÄKQTÁG M X=%G :CJ  KLRqÄ? =: &lt;&gt;SÄROkGOX: =EHR^ECS
ÂÏM GåS&gt;E  P M ?D`&quot;EC&lt; R M &lt;&lt; ^R EHG%&gt;&lt; :  M=ÂROK M &lt;&gt;RO&gt;:=EÃGP lM a :=MTÁERá:= &lt;&gt;ROS;:kROKROFEM .&lt; à  cG M ECTHa4PLEC`ÄU:
Z U^E =M =M Â Â `eKQTÁGM X=M%G JlKLRq:a4KQPQKLRq=? ? \ ]$&lt;&gt;[X PQKQUOU zM =EC&lt;WPQ=: FX&lt; Z kX: =E&quot;S&gt;KQ:=P M X Z EÃKU Â CJ: M G%ECSõ=:
U KLROU}Î : l? %G &gt;&lt; KQ`eEHG±EHRekGO: ?
ÃK=: @&lt; PàLR^EHG\ §kÂ =:=TÁEN`=  @: Ð M &gt;S =:=E UeTà ¦ FEM &gt;&lt; UOR^G Z  TÁR^ECSçRUOKQ` Z M PQ:kR:=MKQSG
KQ&lt;ýSFEHJ=ECP =M  `ÄEC@&lt; =: &gt;&lt; SøR^ECU^ROKQFX&lt; M=Â ROFEÌU zM =ECýS&lt; &gt;KQ:  P M X Z EáKQ&lt; ^R EHG Â :=TÁE=à ¦ FEáUOK` Z P:kR^ECS±G M a M :=ÌazEáKQ&lt; &lt;  ^U R^G Z TÁR^ECSÄR M ` M J=:kG M[Z &gt;&lt; SÄ!:
S4KQ:kX=%G =: ` =M Â   =: TÁE UO Z R^=: &lt;4SÄ`&quot;EC=:
U Z %G @&lt; JDKLG M 4`&lt; &quot;EC@&lt; RO=: P Â :=TÁR M %G UåU Z T6 =UáR^EC`  EHG%:kR Z GOEÄ:=&gt;&lt; SWTH:kGOa M WS&lt; &gt;K M ÀDKSFE=à ¦ &gt;EHGOEÄkGOE:: =PQU M S MlM G%U1K&lt;&quot;ROFENUOKQ` Z PQkROK: M &lt;&quot;RO4kRåTH: =: ±azE&lt; =M  EC&lt;FECS: =&lt;&gt;SÌTHP M ^U ECS¶D\ :=&lt;4S Â :=&lt;&gt;UåRO&gt;: :=&lt; Z %G FECS&lt; M &lt;B:=&lt;&gt;S:
Mk `&quot;EC&lt;JDKQ^R ECSªÃU: zM:=U¿:=EC±T&lt; Z KLR^ME `Ä`e=M Â ka: =: &lt;&gt;M[S&gt;Z UHàR §k¦ &gt;:kXE-U=EC^&lt;@DU?
ROU^R^{EC`G Z &lt;4KQUåKQ`&lt;&gt;KQ&lt;FX Z PLE&lt;    ECÕ-X&lt; =EC&lt; %&gt;KLR^ECTÁR Z };9 :kG  ROKQEHRá&lt; =: PúàL\ dC== ¨ ^Ð Ð6\8:=&gt;&lt; ;S G Z &gt;&lt; U M &lt; :Õ4KLX[  EC&gt;&lt; S  f- åM GOlUORO:kROK M &lt;.@Ú KLR{KQU8: kR: Z G%E  G M R M Rq?  =E \  4KQT%b&gt;=:
U azEHEC&lt; 4Z a¢PQKQTHPL?ÄSFEC` M &gt;&lt; ^U R^%G kR: ^ECS M &lt; &lt; Z `ÄEHG [M Z U M THTH: 
UOK M 4UHà&lt; ¦ &gt;EÕR^ECUOROUeSFECUOTÁG6KLazECSazECP M( : kG%G%KLECS
M Z R M &lt;B: ^%G =: &lt;4UOTÁG%KLazECSÕT M G 4Z U M=Â´©=§=©Dd!Z ^R ^R EHG%:=&lt;4TÁECUH\[ T M PQPLECTÁR^ECS=:
THT M %G S&gt;KFXûR&lt;
M : 
ECPQP  SFEÁã¢&lt;&gt;ECS  G M R M T M P SFECUOTÁG6KLazECSÕK&lt;õÎú[=: `&quot;:=PúàL\ §k== Ð6à
M &gt;&lt; lEHECT6çGOECTkX: [:=KQ&lt;  EHGM X[ÂÏM&gt;&lt; %G KQROK`&quot;ECSM çKQ&lt; Z UOKQèRO&lt; &gt;&lt; &gt;E UOKQ` ZZ PQ:=:kR&lt;&gt;^ECSTÁE ¦ MlM PLDKLRHKQU\
R^%G =: U^RÌR M  åMè GOEHJDK [M Z UÕU^DU? ^^R EC`ÄUH\-ROFER
M X=EHROFEHG  KLROW: _  I PQ=: &lt;&gt;X Z kX: =EÄ` M SFECPúà&quot;£&lt;WT M &lt; U PQ:=FX&lt;M ROKLJkZ :kX:kR=^Eo` =M ECSFECPlKQUFEHG&lt; %=: {P T ZM 4KÅã¢TH&lt;`  KQPQECSkROK: M Â &lt;G M =X`%G =: :`Ä`ÄFX&lt; :kGZ KQUOROKQTHÂÏM GN:=1] PQPL&lt; ?
X PQKQU%. \ Z UOK&lt;   
I EC`ÄKQ&lt; M `  KQPLEHG Î}9 MlM GOE=\[ dC== Ð6à ¦ &gt;E Z &gt;&lt; kROK: M &quot;&lt; X=G%=: `Ä`e:kG
ÂÏM %G `Ä:=PQKQU%`  G Mk JDKQSFECUW=: +&lt; EÁÀDR^GOEC`&quot;ECPQ?T M `  =: TÁRøSFECUOTÁG6K  ROK M &lt; M=Â : &gt;G M :=SøG%=:
FX&lt; =E =M Â PQK&lt;FX Z KQU^ROKTÃT M 4U&lt; ^R^G Z TÁROK M &lt;4UHÚ$KLRÃT Z G a a Z Rÿ: Â ^R EHGçT M `  KQPQ:kROK M &lt; EÁÀ  :=&gt;&lt; &gt;S UøROFECUPLEÁÀDKTH^:=.P ECM&lt;@R^G6KLECUHM =J EHG\ GOEC&lt; ROPL?BT M @&lt; RO:=K&lt;&gt;U ©k G Z :=&lt;4S =§ == !R _M ¢KLXI [G ZR-SFECTPQECUHà% åM_
DÚ J=EHG¦ %:kXZ =%G &lt; M&gt;&lt; &lt;;THP ZROFESFECU¶TÂ =: M;&lt; `Ä`Ä::=4S&lt; M &gt;G U kX= M T%= EHG%U @Ð6\  =: &lt;&gt;S:Î
KQU.RO&gt;E  GOECUOU Z GOE´kR:  KP M R U¶U^ECkR:   DÚ Z ECU^ROK M &gt;&lt; UÃÎ%:=S4KQVÔ4:kROK MkR: &lt; + n
PLEHJ=ECP´KQ&lt;4TÁGOEC:=UOKQFX&lt; @Ð6\  =: U^RNR^EC&lt;&gt;UOEÄ:=&gt;&lt; ;S GOE Â EHG%EC&lt;&gt;TÁE&quot;R M  =: ^U
R1ROKQ`&quot;ECUNÎ VÔ4: kR  =: UåROFE-TH:kGOa M ±S&lt; &gt;K M ÀFKQSFE-PLEHJ=ECP :kR±TÁG%E  &gt; kROT6é: kR±ã:
Â ^R EHEC&lt; M ûã4J=E @Ð6\ &lt; Z `bazEHGOECS M a ^ECTÁROUªÎ  ROkGOR-UOTÁEC: &gt;&lt; kG: %K M RO&gt;GOEHE @Ð64T\ M &lt; Z &lt;&gt;TÁROK M &lt; M=Â a M RO&apos; $:=&gt;&lt; STHPQ: Z UOECUÿÎ 9EC=: U Z G%E  GOECUOU Z G%EkR: TÁGOE  &gt; kROT6B: =: &lt;&gt;S ¢KLX[ RåS&gt;ECTO DÚ  R M TÁGOE  &gt;kROT6: =: &gt;&lt; SÃU  KLROT6 M ÃROFE&lt; Â =: &lt; @Ð6=\ ECPPQK   9 EC:=U Z GOE  GOECU  U ZG %GM E&lt; DàLàLà[M Z &lt;&gt; Mâ  :kaEC&lt;&quot;[M ROFE-TÁGOEZ ^EC`  EHG&gt;%:kR:kROT6Z GOEDàLàLà@Ð6\¶=: THP&lt;4SM ^U E-KLRZ U^E @M=Ð6àÂ #
KQ&gt;&lt; KÌT¦ &gt; ME ` _   KQPQI kROK:
PQM:=&lt;&gt;&lt; X ZKQUkX: =M&lt;&gt;PQ?
M SFECPZ U^ECS G M ÂÏMS GèGOECTZ TÁECSeaM [X? &gt;&lt; IKQROKEC`M .&lt; à # + :kG%UOKQ&lt;&gt;Xe=: UNU Z %T KQU  EHG ÂÏM G%&quot;` ECS M &lt; RO&gt;EªGOECT M [X &lt;&gt;KU^ECS ^U
R^%G KFXF&lt; \ Z UOKQFXROFE&lt;
I EC`ÄKQ&lt;&gt;K  :kG%UOEHGC\ :=&lt;&gt;S  G M S Z TÁECU eUOEC`Ä:=@&lt; ROKTáGOE  GOECUOEC@&lt; ROkROK: M &lt;;KQB&lt; eJ: =EHG%UOK M &lt; =M Â áZ =: UOK: µ¶M X[KTH=: PF M G%`üÎÏJk=: &quot;&lt; $] KOTOÃ=: &gt;&lt;  MlM GOE=\ dC== § Ð6à ¦ FE 8¡
Z &lt;&gt;KLã¢TH:kROK M WX&lt; =G%=: `e`Ä:kGC\ =: &lt;&gt;SKQU%U Z ECUáGOECP:kROKQ&lt; M KLROU ROT =:M KQP¶K`  õÎ&lt; KPQkROK:}C:M ?D&lt;R&lt; M_  :=I3ÂÏMPúàL\ §k%G=`B= .\ :kGOE&quot;SFECUOTÁG%;KQ&lt;;SFE  &gt;î ó 4ì¿í ¢ =¥4 -9¦4  £&lt; M G%&gt;S EHG$R M TH:kGOGO? [M Z   EHG%KQ`ÄEC@&lt; ROU1SFECUOTÁG%KLazECS M Â ROFE&apos;ROFGOEHE ^U ?
DU^^R %:ka M =J =E à=FEHGOE=\  &quot;E T M &lt;&gt;U^R^G Z TÁR^ECSW=:
PLR^EHG%&lt;&gt;kR: ^E&quot;=J EHG%U%K M &lt;&gt;U =M Â EC:=T6M G ROFEèÛ[ÜýÝlÞß â!âNY0:=&lt; ^?DU^R^EC`ÄUH\  :=&lt; ZD X=: ZPQPL?:kX=M &lt;&gt;U^^RM GSFECPQU`Z TÁR^ECSø&lt;&gt;M E&gt;S KLã4ECSJ=EHG%UOKR MM &gt;&lt; KQU&lt;&gt;THPM=ÂZ ROFE&gt;S
Eç:kX_=GOEHEC` I &quot;PQ=EC: &lt;&lt; R T M EHGOEÄU&gt;&lt; ^U R^G6=:Z KQa4UORO&lt;@ROUCà=: &lt;@¦ROKQFE=: !
PPL?&gt;&lt; E PQ:kGOX =J=EHGEHG%8\UOK=:M U!4U&lt; UO =M MÂ ROFE&lt;WKQ!&lt;X=G%¦=: `Ä`e:ka4PLE kG: %d UÚ ROFE åM G%U^R  G M \ M EHJ=EHGC\  :=UõRO&gt;:kRø`Ä=: &lt;@?
G Z PQECU Â G M ` ROFE M %G KLX[KQ&lt;4=:
P¶X=%G :=`Ä`ekG: ! &gt;=: ;S R M  ZD  PQKTHkR: ^ECSBKQ&lt;ÕR åMÄM  M GOE!UOPKLX[@ROPQ?eS&gt;K  EHG%KQ&lt;FX ÂäM G6`ÄUH\ X GOECkROPL: !?
T M `  PQKQTHkROKQ: &lt;&gt;$RO=: UO M=ÂFÂÏZ R Z =%G :=`e`Ä:kG =`Ä=: KQ&lt; R^EC&gt;&lt; =: &lt;&gt;TÁE=à£&lt; M @&lt; ^R G%=: UORH\ KLR  =: UEÁÀDR^GOEC`&quot;ECPQ?ªEC:=U^? `Ä:kGK&lt;  &gt;KQT6&apos;RO&gt;Eû:kX=GOEHEC`&quot; EC&lt; RTZ &lt;&gt;M&gt;&lt; U^^RkROK: G6:=MKQ&lt;@eX&lt; ROUø=G6&gt;:=`=: S  R M a Z KPQS±:=EHG6UOK M &lt; =M Â ROFE azEHEC&lt; M =J ECS.Ú¿RO4KQU M &gt;&lt; PL?K&lt;@J M PLJ=&lt;FX [M Z R!:  :=&gt;&lt; S ÂÏZ P =M ÂoÂ ECkR: Z GOEÕU  ECTHKÅã¢THkROK:
M &gt;&lt; UCàeVWE  EHGOEÕ&lt; M R&gt;  M `&quot;EC=: U Z &gt;KLG%ECTÁROPL?&lt; Z `ªazEHG M=Â T M &lt; ^R EÁÀDR   :
Â GOEHE  G M S Z TÁROK M &lt;&gt;U1KQeRO&lt; &gt; 1M J=EHG%%U K M &gt;&lt; U =M Â ROFE =X G%=: `Ä`ekGC: Fa\ Z RoROFE!UOK
HE =M Â ROFEáX=EC&lt;FEHG6kR: ^ECS  Z :=&gt;&lt; TÁE ~
X=%G :=`Ä`e:kGeSFECTÁGOEC=: ^U
ECS Â G M ` Dd g PQKQ&gt;&lt; ECU =M Â T M SFEBR M § g&quot;{\ :&gt;K  EHGOEC&gt;&lt; TÁEÄKQ&lt;UOK HEÄT M `  :kG6:ka4PLE  &gt;kR: M a4U^EHG%J=BROFE&lt; !]1FX&lt; [PQKQU%BâªâNY&apos;UO?lU^R^ =Â EC=: T%bX=%G =: `Ä`e:kG M &lt;bKLROU8: &gt; G =M  G%KQ:kR^E1T M G 4Z U Z UOKQ&lt;FXM â Z G!a4:=UOKQT  PQ=: &lt;  :=UªR M `&quot;EC:=U Z G%E  EHG ÂÏM %G `Ä=: &gt;&lt; TÁE  Z =: 4TÁE&lt; i[t¢v¡DpFrFv R MlM Pú\  &gt; KQT%eGOEHR Z G%4U&lt; $KQ&lt; ÂÏM G  `Ä:kROK M B&lt; ka: M[Z R åM G%SÌEHGOG
M %kR: ^]$ANÐ6&gt;\ U^EC&lt; R^EC&gt;&lt; TÁE EHGOG
M G8G%kR: ^  ]\=: &lt;4S!U^EC`Ä=: &lt; ROKQT´EHGOG M G8G%:kR^E-Î  EC`±Ð6à  M GåROFENâªâNYé:=&lt; U^lUOR? ^EC`ÄUH\  E Z U^^RO:=&lt;  ¦ ka4PLE: d ö  Z  M=Â &lt; M &lt;  PLEÁÀDKTH=:
P&quot;:=&lt;&gt;+S PLEÁÀFKQTH=: P _  I lU?G ^Z
R^PLECUEC`eUÂÏM  1M =J EHG%UOK M &lt;&gt;U =M Â
ROFENâ!:=&lt;&gt;S azECKQ:=S&gt;&lt;kG: %&lt;&quot;S 4T&lt;  =X M G%Z=: :=`Ä`Ä&gt;&lt;  G =M :kGÌGOEHRzM&gt;MEÁã¢RGOROK &gt;&lt;PLEHJMKLROK=Z &lt; G6ECP¶GOEM &gt;&lt;=M &lt; U±UOR&gt;Â =ZM  R^Â ^^RGOECU%G^U EHGEC`ÄKQ%^FX&lt; =:EC[&lt;&gt;&lt;:=UeGROTÁECU&lt; :kROK%:kROFEHGÌROM &lt;.&gt;à KT%M !G´G6KQ&gt;&lt;&gt;%G :=kR:TÁENROFEECTÁECKLJ&lt;ÔU^^:==E UE EHGOG`ÄS Z =: M&lt; GGROKQT±GOE%kR: ^?ÌROFEªUE ÂÏMGOECUG´KLR^EC&lt;Z M a4URO&lt;kROK: ^E M Z&gt;&lt; ECUCNµ&lt;\ R   EB`kG: %&quot;UOKQ EC&gt;&lt;%G =: ECUU ^Z ECGOECSÿU@&lt; kX:RO:=kROK=E ^\ MEC`ÄZ&gt;&lt; U%U =KQ: &lt;FXÄ&lt; GROKQTMk: ¡
GOECTU  ECTHKQM X[:=&gt;&lt; PQPQKQROKT? M &lt; M &lt;&gt;:UkG^R^%G:=Z &quot;` TÁR^EHRECSõR^EHG%U{MlMR M PúàÕVWE±U`ÄÀFKQ`ÄK: ^EHRHEåROFE!RO&gt;E± G Z=M =:zM&lt;&gt;TÁEG  ROK M &lt; M=Â.Z R^R^EHG6:=&gt;&lt; TÁECU  &gt; KQT%  G M S Z TÁECS&quot;áGOECT:
M X[&gt;&lt; KQROK M &lt;
GOECU Z PLRHö  KLROW&lt; M G%`e:=^EHR^ROKQ&lt;FX[UC\8U^EC&lt; ^R EC&lt;&gt;TÁECU [M Z ROUOKQSFE X=%G :=`Ä`ekG1T: M =J EHG6:kX=ENkG: %E-` M U^R Â %G E Z EC&lt; ROPLÃGOE? ^ECTÁR^ECS.à £&lt;&quot;RO&gt;KQU  :C?=\  E-`Ä:ÀFKQ`ÄK  Z G6&lt; =M Â KQ&lt; ÂÏM %G `Ä:  ROK M &lt; M õROFE&lt; [M Z R M =Â T M =J EHG%kX: =E zM GOROK M &lt; =M Â ROFE±T
M G  4Z UCà = X=%G =: `Ä`ÄkGÌU: ^EC&lt; R^EC&lt;&gt;TÁECUÌKQUeU^=EHGOçS? &gt;K  EHGOEC&lt; RH\M Âä K&lt;&gt;TÁE  EHG ÂÏM G%`Ä:=&lt;&gt;TÁE M &lt; KQ&lt;  X=G6:=`Ä`ÄkGÌ: =: &gt;&lt; S M[Z R  `Ä:k=ECUÕU^EC&lt;&gt;^U E R M T M &lt;&gt;UOKQS&gt;EHGÄROFEC` UOE  : kG%:kR^ECPL=? àÔVWE &gt;J: =EûR åM X=G6=: `Ä`ÄkG: %U ÂÏM GõEC=: T63T M G ¢Z UHÄ\ : ROKLX[ R X G%:=`Ä`ekGKQ`:  PLEC`&quot;EC&lt; ROKQFXè&lt; kX: =GOEHEC`&quot;EC&lt; R;T M &lt;4U^^R G%=: KQ&lt; ROU= =&gt;&lt; S:éP M M ^U
EøX=%G =: `Ä`ÄkG: Â :=KQPQKQ&lt;&gt;XèR M S M U M à ¦ &gt; KQU: &lt; :kR Z G6:=PQPL?U  PQKLROUÃEC:=%T éT
M G 4Z U&quot;KQ&lt;
R M ROFE ÂÏM PQP M KQFX&lt;&gt;
ÂÏM[Z G 
KLECTÁECUHö+@: Ð Z ^R R^EHG%=: &gt;&lt; TÁECUøKQ&lt;&gt;UOKQS&gt;EýT M J=EHG%kX: =E =M Â a MM J=ROªXEHG6=:kXG6=:=E `Ä`ÄM=Â ROkG: &gt;%UoÎE±P}MlM&gt;U^ÈÍEÄX{Ð6Ú=G%=:`e`ÄZ ^R ^:RkGCEHG6a\ =: &lt;&gt;ZTÁECUKQR!&lt; &lt;M&gt;RUOKQS&gt;M=ÂROFE ROKLXTUOKQS[ RªX=%G =: `e`Ä:kGÕÎ Ï¼.¼4¾® % äº¿Ð6Ú$TÐ Z R^R^EHG%:=&lt;&gt;TÁECUÃKQ&lt;  M Â &gt;ROFEWP MlMM U^J=E;EHG%X=kX: %G=:=E `Ä`Ä=M Â RO:kGõÎÏÈÒ&gt;E-ROKLX[ RåX=%¼¢Ç%G =: `Ä`Ääº¿Ð6ÚªS¢ÐkGC: \la ZZ Rå^R ^R&lt; EHGM R = =&gt;&lt; TÁECU-KQ&lt;&gt;UOKQS&gt; M J=EHG%kX: =E =M Â &lt;&gt;=G%:=`Ä`e: kGÃÎúÇ¿®lÒ ÈÍ¿®: ¯=Ð6à ¦ :ka4PLE § UO M UÃROFEÌGOECPLEHJk=: @&lt; &gt;GOECkDS: M &lt;   GF
ÂÏM =: %T  M=Â¿M[Z  M G  G6Dà: X Z £kX: &lt;=EBU^?DU^^R MEC`B&lt;@R^´ROFEÕKQ\EÁÀDR =M Â &lt; := èKQT&lt; M =J &lt;@EHGR^%EHG6kX: =:=E TÁROKLJ kGOR:= M=zMÂ =ROFEÌTECÿPQ&lt; :=M &lt;G  4Z   G6=: TÁROKQTH:=PQPLüK? &lt;@^R EHGOECUOROKQ&lt;FX M &lt;FE=à a4PLECU Ã GOECU^EC&lt; ROU  EHG ÂäM %G `e:=&gt;&lt;  Z GOECU ÂÏM  zM¦ :G  ROK M &lt; M=Â EC:=T6ûT M G 4Z &quot;U RO&gt;kReKQU:  KLRO&gt;KQÿT&lt; M J=EHG6:kX=E M=Â a M ROçX=%G =: `e`ÄkG: %UHàÔ&lt;ûR^EHG%`ÄU M=Âá1M G%SûEHGOG M G±G%:kR^=E \ = %G =: `Ä`ekG:  &gt;KQT63EC&lt; ÂäM G%TÁECUûkX: =GOEHEC`&quot;EC&lt; RçT M &lt;  ¦ ka4PLE:  öý$] GOG M GÕG%:kR^ECU ÂäM GÌa M ROÔJ=EHG%UOK M &lt;4U =M Â ROFE ! âNY(UO?lU^^R EC` M &lt; Z
EHG%:=&lt;4TÁECU  KLRO&gt;KQT&lt; M J=
EHG%kX: =E M=Ââ P MlM ^U EáX=G%:=`Ä`ekG: M &gt;&lt; PL?  ©   l ECS&gt;KQUO M &lt;FE=à1 M  M `  PQEHR^EC&lt;FECUOUH\ 
E= =PQU Mû
GOECU^EC&lt; &lt; ¦ :ka4PLE 
GOECU Z PLROU
ÂÏM G Z ^R R^EHG%=: 4TÁECU&lt;:
M Z ROUOKSFEeT M =J EHG6:kX=E M=Â ECKLROFEHGÃX=G%:=`Ä`Ä: kGÚ´ROFECU^EÌS&gt;KQU [  PQ:?ÃPKLR^&lt; R^EHGOECU^ROKQFXáJk&lt; kG: %KQkROK: M &lt;.{à KQ4&lt; :=PQPL?=\ ¦ :ka4PLE  X KLJ=ECUROFEèGOECU Z PQROU ÂäM GøROFE § KQ&gt;&lt; U%KQSFEBT M =J EHG%kX: =E M=Â RO&gt;EP MlM ^U E a !âZ
ReâNY&lt; M R±ROFEZ R^R^EHG%=:
ROKLX&lt;4TÁECU[ R [
X G%:=`Ä`ekGCà: f-&lt;&gt;U Z G  %G KUOKQ&lt;FX[PL?=\ M M UOEX=%G :=`e`ÄkG:=  EHG ÂÏM G% Z %T BazEHR^^R EHG M &lt;BRO&gt;^EHRHà ^U DU? ^¦ ^R EC`ÄU&gt;E-` =MM Â^U RoTÁG%KLROKTH:&gt;&lt;=P¢`SKQU-ROFE&quot;EC=: U Z &quot;GOEU^EC`Ä=M Â8:=&lt;EHGROKQTªEHGÂäM %G `e%G :=M &lt;&gt;GNGTÁE%kR: ÂÏM^E=G\  zM&gt;GOROKKT%M &lt; KQU=M Â M RO&lt;&gt;&gt;PQENT&apos;`? &quot;M G EC4Z:=&lt;&gt;K&lt;FX ÂÏZ P ÂÏM^RåUGõROFEéKQ%KLX[@ RH\DROFENã¢X&lt;  T M =J EHG6Z kX: GOECU=E KQ&lt;  G M ¦=J EC`ka4PLE: &quot;EC&lt; RåKQ:&lt;=PQUÂ MJ: :[M &gt;Z G ECM=:kGÄRÂ M UO[M R$=X õ`Ä:G6=: `Ä`Ä:kGOkGC:=ECSéKQ`\  KLRO
ÂÏM !G -Yb8\ :=&lt;&gt;S §k G M=zM GOROK M &gt;&lt; :=P´GOECS Z TÁROKÂÏM GM &gt;&lt; U  =ªàM .VWE  ÂÏMEHGOEeU!âáYªM `ÄE\  §k&gt; kR:
RO&gt;:=&lt;ROFE M ROFEHGbR åM ^U DU? ^^R
EC`ÄUC\{:=&lt;&gt;STH:kGOG%KLECS [M Z Rb: ±ï8ì&lt;  $E ò qñazEHX[kóï¿ì:=&lt; åM .GOì$ð M &lt;çRO^$ñ =&gt;î íE EÁÀ&gt;îWð $EHG6KQ`ó¸î&quot;4òlíEC&lt; [ROUÄSFEóï¿ì  ­ ® w¯ 2 2n0 ±°   2 UOTÁGEHROKQ%&lt;FX &gt;zMEHGOEUOKLROK=\ M &gt;&lt; E U&apos;TÂ ECPLRM!&gt;&lt; ROTÁEHG&gt;:kRbROFEHGOE%&lt;&gt;K&lt;FX RO&gt;E EHGOEÄRZ ECU1M^ROK M T&lt;
M ` M=Â  : =FEHRO&gt;EHG M GçM&lt; M RÿKQR  :=UçKQ`_ zMI-GORO:=&lt; RýR M K&gt;&lt; THPZ Z SFE `kX MGOEHEC`SFECPQUHà&quot;EC&lt; RÌTFG M `&gt;&lt; U^R^:çGOECUOECG%:=KQ&lt; ROUeK:kG%&lt; T% M  G6KLEC@&lt; a¢R^=:ECSROFE^U ECSéP:=M
FX&lt; G%EHROKQTH:kX==: EP ^U RO=: &lt;&gt;S  K@&lt; RH\  EWazECPQKLEHJ=ECSÔRO&gt;: åM[Z PQSÔazEWKQ`  zM
UOUOKLa¢PLEBR M KQX[&lt; M GOEýPQKQ: &gt;&lt; X Z KQU^ROKQT  &gt;EC&lt; M `&quot;EC&lt; M Ô&lt; =: U TÁEC&lt; ^R %G :=P-=: U±X=G%=: `Ä`e:kROKQTH=: :kX=GOEHEC`ÄEC&lt;@R  KLRO [M Z ReKQ&lt;  T Z GOG6KQ&lt;FXÌU M `&quot;EÄUOKLX[4KÅã¢TH&lt; =: &lt; R  EC&lt;&gt;:=PQR¸?=à %=: TÁROKTH=:    PLEC`ÄEC@&lt; RO:kROK M ÄEÁÀ&lt;  EHG%KLEC4TÁEo&lt; M EHJ=EHG zM KQ&lt; ^R ECS&quot;KQ&lt; ?
M &gt; zM
UOKLR^EýS&gt;KLGOECTÁROK M .&lt;  
M U^RWT M `Ä`&quot;EHG6THKQ:=^lUOR? ^EC`ÄU=
Â := M RO:k=EÕ:=THT M[Z @&lt; R =M Â :kX=GOEHEC`&quot;EC&lt; RH\$:=&lt;&gt;Sý=: T%4KLEHJ=E =: SFE Z :kR^E  EHG ÂäM G%`e=: &lt;&gt;TÁE Z U%KQ&lt;FXþ` M UOROPL?+U^EC`Ä=: &lt; ROKQT =&lt;&gt; M `Ä:=KQ&lt;BT M &gt;&lt; ^U
R^G6=: KQ@&lt; ROUCà: a M ROûUOKSFECUÄTH:=&lt;ûPZ PLROUoKQC: ? çU&lt; M Â&quot;` G ME;&lt; THPQR M=:=Â¿Z
UC\ M  azECKQEáRO&gt;FXWG&lt; KQ&gt;&lt; &quot;%KLXRO[&gt; :kRRHà KQ&lt; FEHG EHG%ÂäM G6`Ä&gt;&lt; :=SFEHECS&lt;&gt;TÁE  eJ: FEC=EHGO&lt;±?:kX=UOKQX%G [EHEC`&lt;&gt;&quot;KÅã¢THEC@&lt; =: &lt; M &lt;4U^R^G%G :=M KQJ=&lt; EC`&quot;kGOEEC: &lt; R ¦ :=S&gt;SFECSÿR M WX: =%G =: `Ä`Ä:kGàçY M EHJ=EHGC\&quot;KQ`  G M =J E  &quot;EC&lt; RåKU$`Ä:=&gt;&lt; K Â ECU^^R ECS±` M U^RoU^^R G M FX&lt; [PQÄ? : Z G Â =: TÁE` PLEHJ=ECPú\ =: $U `&quot;EC=: U Z  ? $] AÔ=: &lt;&gt;S  $] Aªà[-PQRO [M Z X[ ROFE&quot;&gt;S K 
EHGOEC&lt;&gt;^ROKQPQP¿U^^R G M FX&lt; [PQ? UOKLX[&gt;&lt;
PLEHJ=ECP =M Â UOEC`Ä:=@&lt;   %G ECU^EC@&lt; RO:kROK M .&lt; \RO&gt;$E :ka4U M P Z R^$E KQ&lt;  TÁGOEC:=U^&lt;ýUOEC`Ä:=@&lt; ROKTÕ:=THT Z %G :=TÁ?ýKU Â =: KLG%PQõUO`Ä? :=PQPúàõ  %G =: TÁROKTH=:
P¢U^DU? ^R^ Z KPQSFEHG$TH=: &lt;±GOEC=:
U M &gt;&lt; :ka¢PL? 1M &lt;4SFEHG  T M FEHRO&gt;&lt; UOKSFEHG&gt;EHG1ROFECUOE-X%:[^:=G%KQ:&lt;4Uå:kG% E-ECPLEC`&quot;&lt; M[ECZ&lt; ROX[kROK:eR MM &lt; ` MZ ROKLJkG6SFECkR: ^&lt;ENROFE : ^ECSÃa ?!: =S4S&gt;KQ&lt;:kX=G%EHEC`&quot;EC@&lt; RR M :-4:=&lt;&gt;S  T M SFECS _  IkR PQ=: &lt;FX Z :kX=Eª` M  Â RO&gt;E _  I ` M  M `  KQPLECS Â G M ` ±: &gt;KQX[  PLEHJ=ECP{SFECUOTÁG%K  ROK M .&lt; 4\ M EHJ=EHG\ PQKLR^=: &gt;S &gt;S KLROK M &lt;&gt;=: P åM
M  M &lt;FE=\D:=&gt;&lt; S±KQR1KQU M a JDK  [M Z UOPQ!?
SFECUOKLG%:ka¢PLE$R M KQ&gt;&lt; THP Z SFE1kX: =G%EHEC`&quot;EC&lt;@ M &lt;4U^^R %G =: KQ&lt; ROU KQ&lt;BRO&gt;EbX=G%=: `e`ÄkGCà: ¦ &gt;=: &lt; azEªK@&lt; R^EHG  GOEHR^ECS =: =: &lt; kGOX Z `ÄEC&lt;@&lt; Â J: [M Z G M= UOK&lt;FX &gt; KLX[  PLEHJ=ECP1PKQFX&lt; Z KQU : M SFECPQUHà%K  ROK M &gt;&lt; U1=: U =M &gt; zM ^U
ECSeR M &gt; :=4S&lt;  T M S&gt;ECS _  I` GOEC`&quot;µ8MEC`bazEHGªROM DKQ&lt;FX }Â &gt;Z : kRbX=%G :=:=`Ä`Ä&gt;EC=: :kROKTHS.@\ :=$P kX: =:=PUGOEHEC`M KQ`&quot;ECzM&lt; GORO=: &lt; M RR&lt;&gt;PQM? M &lt;FE M=Â éPQ: :kG%=X EG%=: FX&lt; =E =M Â PQKQ&lt;FX Z KQU^ROKQT  &gt; EC&lt; M &quot;` EC&gt;&lt; : T Z GOG%EC@&lt; ROPL?;KLX[&lt; M GOECSõa ?
R  G%:=TÁROKQTH:=  PLEC`&quot;EC&lt;  R M G%UHàèReKQU Z &gt;&lt; THPLEC:kG&quot;R MZ &quot;U  M PQ:kG%X= Z ` ZD PQ:kROKLJ=  :C?éazECT M &quot;` E  FEC&lt;=: KQPLECSÔ` M S  ECPQPQK&lt;FX M=Â `Ä=: &lt; ?U Z T6  FEC&lt; M `ÄEC&lt;&gt;:±KUá=: &gt;S SFECS;R M =: &lt; kROFE M GOEHROKTH:=@P T M :kG%^U E  X=G6:=KQFECSªP&lt; :=FX&lt; Z :kX=$E ` M SFECPú\ :=&lt;&gt;S:  ^U
EHEC`ÄU  PQ: Z UOKLa¢PLE RO&gt;:kR; [M Z  M 4UOKQS&lt;  EHG%ka4PQ: õX? =GOECkR: ^EHGÄRO&gt;=: &lt;ÿRO4:kR  G M S Z TÁECSøa øROFEBUOKQ? &lt;  [PLE Â EC :=FECUOKLa¢PLEÿR&lt; M `&quot;ECM&lt; M KQ&lt;&lt; =J  ECUEÕ4^ROKLX[CJ::kR=^EéROFECUEÌKQ&lt;@J=ECUOROKLX^E [:kRZ ^ECUECSÿFEHGOE^ROK M &lt;4UWEC`=à R X
µp¦L£8¦§¥^^§ »c¢¦^&gt;« °¥p¦c¨ ¦·¼^½^£®¢£@µ ¾¿{ ·¹c¬&gt;¸8}¢£À68¦^Á/¸z¹m¶&gt;p¨p¬/½&gt;£8¶L£z£?¬ µ¬ª¶^}£ ¨pº¤£8¦¥{³ ´ ¢¦p5¸8¸z®¢£}«^p¬©³ ?¢Ã8¢¦^° ª¶^£[¸8¨¤«^¢¸[¦^@§ ®Ä¦^°¤«Op°¤£7Â¨§L£8®¢¬&gt; ³Å®®¯p¦^°«&gt;p°£5«^¦^^¢¦^}¦?¢°¦^·&gt;¸}p¦cª®¹ ¢Â µ &gt;¦^§^®¢¢¦^°Æ¨p­&apos;¦^¨¤¦v©Ç¦&gt;¢¦ µ «L}¢¦a¨¤³ª§^£8³?ª¨ Âº¤£@)¶D´¶^¨p¾6£8¡£8³} &gt;¦ ¢¡¤£z¦^^¬ ¨p­ ª¶^£ @^&gt;£&apos;ºc¦^¬ µ  µ £8³{&gt;´ ¾6£&apos;¬ª¶^¨p¾ {²·O¸}pª}p¦Æ½&gt;;£ ^« ?¢§^£8¦v© ­|¹¦L¨¤¦c©¦&gt;pª¢¡¤£??¨­ËÊ7¦L°¤®¢$§^¡¤p¦v© Å°¤£;¨p­[¨«^@³ Â?£8ª¶^¨c§¥¢¬@&gt;¢/{´D¦^¨@¨¤¦ p¸z¨¤«^¢¸ ­s£}pª«^{^´ p¦^}[^£z¦ ª¶^£5p¸8¨«^¢¸ ¨«^?¢¬@¦^¨¤ {¡¤¢®Ä½^®¢£¤À&apos;Î;£ ^§ £8Â¨¤¦^¬ª³)ª¶&gt;p@½O¨¤ª¶ {£ p§Ï¦^§&lt;¬ µ ¨¤¦c) p^¦ £8¨^« ^« ³Å¦^¸8}¸ &lt;¦ ½O¸8®¯·&gt;£z§ °¶&apos;p¸8¸8«L³Åp¸z¹´¤¦^§&apos;ª¶&gt;·&gt;}¸ ^|^« ® ¬ µ £8£8)¸ ¶Æ£8¸z¨¤°¤¦LÃz£8³5¶c¹ µ ^¶ £z@¬ ¢¬&apos;Ð ÑÓ&apos;¸8¸8«^)³ p¶&gt;¦ ¸8®¯p¬ª·&gt;}¸ ¢¨¤;¦ ¨p­ µ |­ £8¸8Ô³Å¦^ µ ¢¨^¦ ¬}À7Î£[¨5)¸ &gt;¶ ³© p¸z £8³ªÃz£ µ p³ª©¨p­¯©¬ µ £8£8¸)¶¥¬ª£8»c^« £8^¦ ¸z£8¬@&gt; µ ®¯{¹?@¢¦ ^£8ª£8¸8ª¦L°? ¦^¨¦c©Ç¦Op¢¡£5¬ µ £8£8)¸ ¶DÀ§ Õ Ö ^ZªV f ¨p­D®¯¦^°¤«&gt;µ{£ &gt;}^¢®¢£[ª^§ ¢¨¤¦£z£ ^µ ¬p½O¨¤«L¢¦^^§ ¢¡c§L«^&gt;p®c}p¦L^¦L°Ë¦£z³6«^^¬ª£§
