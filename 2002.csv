sentence
Weighted finite-state transducers suffer from the lack of a train-ing algorithm.
"Training is even harder for transducers that have been assembled via finite-state operations such as composition, minimization, union, concatenation, and closure, as this yields tricky parameter tying."
"We formulate a “parameterized FST” paradigm and give training algorithms for it, including a gen-eral bookkeeping trick (“expectation semirings”) that cleanly and efficiently computes expectations and gradients."
Rational relations on strings have become wide-spread in language and speech engineering[REF_CITE].
"Despite bounded memory they are well-suited to describe many linguistic and tex-tual processes, either exactly or approximately."
"A relation is a set of (input, output) pairs."
Re-lations are more general than functions because they may pair a given input string with more or fewer than one output string.
The class of so-called rational relations admits a nice declarative programming paradigm.
Source code describing the relation (a regular expression) is compiled into efficient object code (in the form of a 2-tape automaton called a finite-state trans-ducer).
The object code can even be optimized for runtime and code size (via algorithms such as deter-minization and minimization of transducers).
"This programming paradigm supports efficient nondeterminism, including parallel processing over infinite sets of input strings, and even allows “re-verse” computation from output to input."
Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.
"It is common to define further useful operations (as macros), which modify existing rela-tions not by editing their source code but simply by operating on them “from outside.”"
"The entire paradigm has been generalized to weighted relations, which assign a weight to each (input, output) pair rather than simply including or excluding it."
"If these weights represent probabili-ties P(input,output) or P(output | input), the weighted relation is called a joint or conditional (probabilistic) relation and constitutes a statistical model."
"Such models can be efficiently restricted, manipulated or combined using rational operations as before."
An artificial example will appear in §2.
The availability of toolkits for this weighted case ([REF_CITE]; van[REF_CITE]) promises to unify much of statistical NLP.
"Such tools make it easy to run most current ap-proaches to statistical markup, chunking, normal-ization, segmentation, alignment, and noisy-channel decoding, [Footnote_1] including classic models for speech recogniti[REF_CITE]and machine translati[REF_CITE]."
"1 Given output, find input to maximize P(input, output)."
"More-over, once the models are expressed in the finite-state framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources."
"Unfortunately, there is a stumbling block: Where do the weights come from?"
"After all, statistical mod-els require supervised or unsupervised training."
"Cur-rently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs."
"Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and train-ing regimens."
"For example, the forward-backward algorithm[REF_CITE]trains only Hidden Markov Models, while[REF_CITE]trains only stochastic edit distance."
"In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run."
"This paper aims to provide a remedy through a new paradigm, which we call parameterized finite-state machines."
It lays out a fully general approach for training the weights of weighted rational rela-tions.
"First §2 considers how to parameterize such models, so that weights are defined in terms of un-derlying parameters to be learned. §3 asks what it means to learn these parameters from training data (what is to be optimized?), and notes the apparently formidable bookkeeping involved. §4 cuts through the difficulty with a surprisingly simple trick."
"Fi-nally, §5 removes inefficiencies from the basic algo-rithm, making it suitable for inclusion in an actual toolkit."
Such a toolkit could greatly shorten the de-velopment cycle in natural language engineering.
"Finite-state machines, including finite-state au-tomata (FSAs) and transducers (FSTs), are a kind of labeled directed multigraph."
"For ease and brevity, we explain them by example."
"Fig. 1a shows a proba-bilistic FST with input alphabet Σ = {a, b}, output alphabet ∆ = {x,z}, and all states final."
It may be regarded as a device for generating a string pair in Σ ∗ × ∆ ∗ by a random walk from 0 .
Two paths exist that generate both input aabb and output xz: a:x/.63 a: /.07 b: /.03 b:z/.4 0 −→ 0 −→ 1 −→ 2 −→ 2/.5 a:x/.63 a: /.07 b:z/.12 b: /.1 0 −→ 0 −→ 1 −→ 2 −→ 2/.5
"Each of the paths has probability .0002646, so the probability of somehow generating the pair (aabb, xz) is .0002646 + .0002646 = .0005292."
"Abstracting away from the idea of random walks, arc weights need not be probabilities."
"Still, define a path’s weight as the product of its arc weights and the stopping weight of its final state."
"Thus Fig. 1a defines a weighted relation f where f(aabb, xz) = .0005292."
This particular relation does happen to be probabilistic (see §1).
"It represents a joint distribu-tion (since P x,y f(x, y) = 1)."
"Meanwhile, Fig. 1c defines a conditional one (∀x P y f(x, y) = 1)."
This paper explains how to adjust probability dis-tributions like that of Fig. 1a so as to model training data better.
The algorithm improves an FST’s nu-meric weights while leaving its topology fixed.
How many parameters are there to adjust in Fig. 1a?
That is up to the user who built it!
"An FST model with few parameters is more constrained, making optimization easier."
"Some possibilities: • Most simply, the algorithm can be asked to tune the 17 numbers in Fig. 1a separately, subject to the constraint that the paths retain total probability 1."
"A more specific version of the constraint requires the FST to remain Markovian: each of the 4 states must present options with total probability 1 (at state 1 , 15+.7+.03.+.12=1)."
This preserves the random-walk interpretation and (we will show) entails no loss of generality.
"The 4 restrictions leave 13 free params. • But perhaps Fig. 1a was actually obtained as the composition of Fig. 1b–c, effectively defin-ing P (input, output) ="
"P mid P (input, mid) · P(output | mid)."
"If Fig. 1b–c are required to re-main Markovian, they have 5 and 1 degrees of free-dom respectively, so now Fig. 1a has only 6 param-eters total. 2"
"In general, composing machines mul-tiplies their arc counts but only adds their param-eter counts."
"We wish to optimize just the few un-derlying parameters, not independently optimize the many arc weights of the composed machine. • Perhaps Fig. 1b was itself obtained by the proba-bilistic regular expression (a : p)∗ λ (b : (p + µ q))∗ ν with the 3 parameters (λ, µ, ν) = (.7, .2, .5)."
"With ρ = .1 from footnote [Footnote_2], the composed machine (Fig. 1a) has now been described with a total of just 4 parameters! 3 Here, probabilistic union E + µ F def = µE + (1 − µ)F means “flip a µ-weighted coin and def generate E if heads, F if tails.”"
"2 Why does Fig. 1c have only 1 degree of freedom? The Markovian requirement means something different in Fig. 1c, which defines a conditional relation P(output | mid) rather than a joint one. A random walk on Fig. 1c chooses among arcs with a given input label. So the arcs from state 6 with input p must have total probability 1 (currently .9+.1). All other arc choices are forced by the input label and so have probability 1. The only tunable value is .1 (denote it by ρ), with .9 = 1 − ρ."
E∗ λ = (λE) ∗ (1−λ) means “repeatedly flip an λ-weighted coin and keep repeating E as long as it comes up heads.”
"These 4 parameters have global effects on Fig. 1a, b:p thanks to complex parameter tying: arcs 4 −→ 5 , b:q 5 −→ 5 in Fig. 1b get respective probabilities (1 − λ)µν and (1 − µ)ν, which covary with ν and vary oppositely with µ. Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a."
"We offer a theorem that highlights the broad applicability of these modeling techniques. 4 If f(input,output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a ∈ Σ ∪ { }, b ∈ ∆ ∪ { }) using concatenation, probabilistic union + p , and probabilistic closure ∗ p ."
"For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules[REF_CITE], ([Footnote_3]) by compilation of decision trees[REF_CITE], (4) as a relation that per-forms contextual left-to-right replacement of input substrings by a smaller conditional relation (Gerde-mann and van[REF_CITE]), 5 ([Footnote_5]) by conditionaliza-tion of a joint relation as discussed below."
"3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ)."
"5 In (4), the randomness is in the smaller relation’s choice of how to replace a match. One can also get randomness through the choice of matches, ignoring match possibilities by randomly deleting markers in Gerdemann and van Noord’s construction."
"A central technique is to define a joint relation as a noisy-channel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1[REF_CITE]."
The general form is illustrated by def
"P (v, z) = P w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. [Footnote_6],7"
"6 P(w, x) defines the source model, and is often an “identity FST” that requires w = x, really just an FSA."
There are also procedures for defining weighted FSTs that are not probabilistic[REF_CITE].
Arbitrary weights such as 2.[Footnote_7] may be assigned to arcs or sprinkled through a reg- : /2.7 exp (to be compiled into −→ arcs).
"7 We propose also using n-tape automata to generalize to “branching noisy channels” (a case of dendroid distributions). In P w,x P(v|w)P(v 0 |w)P(w,x)P(y|x), the true transcrip-tion w can be triply constrained by observing speech y and two errorful transcriptions v, v 0 , which independently depend on w."
"A more subtle example is weighted FSAs that approximate PCFGs[REF_CITE], or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation."
"These are parameterized by the PCFG’s parameters, but add or remove strings of the PCFG to leave an improper probability distribution."
"Fortunately for those techniques, an FST with positive arc weights can be normalized to make it jointly or conditionally probabilistic: • An easy approach is to normalize the options at each state to make the FST Markovian."
"Unfortu-nately, the result may differ for equivalent FSTs that express the same weighted relation."
Undesirable consequences of this fact have been termed “label bias”[REF_CITE].
"Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since “dead ends” leak probability mass). [Footnote_8] • A better-founded approach is global normal-ization, which simply divides each f(x,y) by P x 0 ,y 0 f(x 0 , y 0 ) (joint case) or by P y 0 f(x, y 0 ) (con-ditional case)."
"8 A corresponding problem exists in the joint case, but may be easily avoided there by first pruning non-coaccessible states."
"To implement the joint case, just di-vide stopping weights by the total weight of all paths (which §[Footnote_4] shows how to find), provided this is finite."
"4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger constructi[REF_CITE], taking care to write each regexp in the con-struction as a constant times a probabilistic regexp. A full proof is straightforward, as are proofs of (3)⇒(2), (2)⇒(1)."
"In the conditional case, let g be a copy of f with the output labels removed, so that g(x) finds the desired divisor; determinize g if possible (but this fails for some weighted FSAs), replace all weights with their reciprocals, and compose the result with f. [Footnote_9]"
"9 It suffices to make g unambiguous (one accepting path per string), a weaker condition than determinism. When this is not possible (as in the inverse of Fig. 1b, whose conditionaliza-"
Normalization is particularly important because it enables the use of log-linear (maximum-entropy) parameterizations.
"Here one defines each arc weight, coin weight, or regexp weight in terms of meaningful features associated by hand with that arc, coin, etc."
"Each feature has a strength ∈ R &gt;0 , and a weight is computed as the product of the strengths of its features. [Footnote_10]"
"10 Traditionally log(strength) values are called weights, but this paper uses “weight” to mean something else."
It is now the strengths that are the learnable parameters.
"This allows mean-ingful parameter tying: if certain arcs such as −→ u:i , −→ o:e , and −→ a:ae share a contextual “vowel-fronting” feature, then their weights rise and fall together with the strength of that feature."
"The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as de-sired."
Such approaches have been tried recently in restricted cases[REF_CITE].
"Normalization may be postponed and applied in-stead to the result of combining the FST with other FSTs by composition, union, concatenation, etc."
"A simple example is a probabilistic FSA defined by normalizing the intersection of other probabilistic FSAs f 1 , f 2 , . . .. (This is in fact a log-linear model in which the component FSAs define the features: string x has log f i (x) occurrences of feature i.)"
"In short, weighted finite-state operators provide a language for specifying a wide variety of parameter-ized statistical models."
Let us turn to their training.
"We are primarily concerned with the following train-ing paradigm, novel in its generality."
Let f θ : Σ ∗ ×∆ ∗ → R ≥0 be a joint probabilistic relation that is computed by a weighted FST.
The FST was built by some recipe that used the parameter vector θ.
"Changing θ may require us to rebuild the FST to get updated weights; this can involve composition, reg-exp compilation, multiplication of feature strengths, etc. (Lazy algorithms that compute arcs and states of f θ on demand[REF_CITE]can pay off here, since only part of f θ may be needed subsequently.)"
"As training data we are given a set of observed (input, output) pairs, (x i , y i )."
"These are assumed to be independent random samples from a joint dis-tribution of the form f θ̂ (x, y); the goal is to recover the true θ̂. Samples need not be fully observed (partly supervised training): thus x i ⊆ Σ ∗ , y i ⊆ ∆ ∗ may be given as regular sets in which input and out-put were observed to fall."
"For example, in ordinary HMM training, x i = Σ ∗ and represents a completely hidden state sequence (cf.[REF_CITE], who allows any regular set), while y i is a single string represent-ing a completely observed emission sequence. [Footnote_11]"
"11 To implement an HMM by an FST, compose a probabilistic FSA that generates a state sequence of the HMM with a condi-tional FST that transduces HMM states to emitted symbols."
What to optimize?
"Maximum-likelihood es-timation guesses θ̂ to be the θ maximizing Q i f θ (x i , y i )."
"Maximum-posterior estimation tries to maximize P (θ)·Q i f θ (x i , y i ) where P (θ) is a prior probability."
"In a log-linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting[REF_CITE]."
The EM algorithm[REF_CITE]can maximize these functions.
"Roughly, the E step guesses hidden information: if (x i ,y i ) was gener-ated from the current f θ , which FST paths stand a chance of having been the path used? (Guessing the path also guesses the exact input and output.)"
The M step updates θ to make those paths more likely.
EM alternates these steps and converges to a local optimum.
The M step’s form depends on the param-eterization and the E step serves the M step’s needs.
"Let f θ be Fig. 1a and suppose (x i , y i ) = (a(a + b) ∗ ,xxz)."
"During the E step, we restrict to paths compatible with this observation by computing x i ◦ f θ ◦ y i , shown in Fig. 2."
"To find each path’s pos-terior probability given the observation (x i , y i ), just conditionalize: divide its raw probability by the total probability (≈ 0.1003) of all paths in Fig. 2."
But that is not the full E step.
The M step uses not individual path probabilities (Fig. 2 has infinitely many) but expected counts derived from the paths.
"Crucially, §4 will show how the E step can accumu-late these counts effortlessly."
"We first explain their use by the M step, repeating the presentation of §2: • If the parameters are the 17 weights in Fig. 1a, the M step reestimates the probabilities of the arcs from each state to be proportional to the expected number of traversals of each arc (normalizing at each state to make the FST Markovian)."
So the E step must count traversals.
"This requires mapping Fig. 2 back onto Fig. 1a: to traverse either 8 −→ a:x 9 or 9 −→ a:x 10 in Fig. 2 is “really” to traverse 0 −→ a:x 0 in Fig. 1a. • If Fig. 1a was built by composition, the M step is similar but needs the expected traversals of the arcs in Fig. 1b–c. This requires further unwinding of Fig. 1a’s 0 −→ a:x 0 : to traverse that arc is “really” to a:p p:x traverse Fig. 1b’s 4 −→ 4 and Fig. 1c’s 6 −→ 6 . • If Fig. 1b was defined by the regexp given earlier, a:p traversing 4 −→ 4 is in turn “really” just evidence that the λ-coin came up heads."
"To learn the weights λ, ν, µ, ρ, count expected heads/tails for each coin. • If arc probabilities (or even λ, ν, µ, ρ) have log-linear parameterization, then the E step must com-pute c = P i ec f (x i ,y i ), where ec(x,y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x,y)."
"The M step then treats c as fixed, observed data and adjusts θ until the predicted vector of to-tal feature counts equals c, using Improved Itera-tive Scaling[REF_CITE]. 12 For globally normalized, joint models, the predicted vector is ec f (Σ ∗ , ∆ ∗ )."
"If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to de-scribe (though usually much easier to compute). [Footnote_13]"
"13 For per-state conditional normalization, let D j,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1. Besides computing c, the E step must count the expected number d j,a of traversals of arcs in each D j,a . Then the predicted vector given θ is P j,a d j,a ·(expected feature counts on a randomly chosen arc in D j,a ). Per-state joint normalization ([REF_CITE]§8.2) is similar but drops the dependence on a. The difficult case is global conditional nor-malization. It arises, for example, when training a joint model of the form f θ = · · · (g θ ◦ h θ ) · · ·, where h θ is a conditional"
"It is also possible to use this EM approach for dis-criminative training, where we wish to maximize Q i P (y i | x i ) and f θ (x, y) is a conditional FST that defines P (y | x)."
"The trick is to instead train a joint model g ◦ f θ , where g(x i ) defines P(x i ), thereby maximizing Q i P(x i ) ·"
"P(y i | x i ). (Of course, the method of this paper can train such composi-tions.)"
"If x 1 , . . . x n are fully observed, just define each g(x i ) = 1/n."
"But by choosing a more gen-eral model of g, we can also handle incompletely observed x i : training g ◦ f θ then forces g and f θ to cooperatively reconstruct a distribution over the possible inputs and do discriminative training of f θ given those inputs. (Any parameters of g may be ei-ther frozen before training or optimized along with the parameters of f θ .)"
"A final possibility is that each x i is defined by a probabilistic FSA that already sup-plies a distribution over the inputs; then we consider x i ◦ f θ ◦ y i directly, just as in the joint model."
"Finally, note that EM is not all-purpose."
"It only maximizes probabilistic objective functions, and even there it is not necessarily as fast as (say) conju-gate gradient."
"For this reason, we will also show be-low how to compute the gradient of f θ (x i , y i ) with respect to θ, for an arbitrary parameterized FST f θ ."
"We remark without elaboration that this can help optimize task-related objective functions, such as P i P y (P (x i , y) α / P y 0"
"P (x i , y 0 ) α ) · error(y, y i )."
"It remains to devise appropriate E steps, which looks rather daunting."
"Each path in Fig. 2 weaves together parameters from other machines, which we must un-tangle and tally."
"In the 4-coin parameterization, path a:x a:x a: a: b:z 8 −→ 9 −→ 10 −→ 10 −→ 10 −→ [Footnote_12] must yield up a vector hH λ , T λ , H µ , T µ , H ν , T ν , H ρ , T ρ i that counts observed heads and tails of the 4 coins."
"12 IIS is itself iterative; to avoid nested loops, run only one it-eration at each M step, giving a GEM algorithm[REF_CITE]. Alternatively, discard EM and use gradient-based optimization."
"This non-trivially works out to h4, 1, 0, 1, 1, 1, 1, 2i."
"For other parameterizations, the path must instead yield a vec-tor of arc traversal counts or feature counts."
"Computing a count vector for one path is hard enough, but it is the E step’s job to find the expected value of this vector—an average over the infinitely log-linear model of P(v | u) for u ∈ Σ 0∗ , v ∈ ∆ 0∗ ."
Then the predicted count vector contributed by h is P i
P u∈Σ 0∗
"P(u | x i , y i ) · ec h (u, ∆ 0∗ )."
"The term P i P(u | x i , y i ) computes the expected count of each u ∈ Σ 0∗ ."
"It may be found by a variant of §4 in which path values are regular expressions over Σ 0∗ . many paths π through Fig. 2 in proportion to their posterior probabilities P (π | x i , y i )."
"The results for all (x i , y i ) are summed and passed to the M step."
"Abstractly, let us say that each path π has not only a probability P(π) ∈ [0, 1] but also a value val(π) in a vector space V , which counts the arcs, features, or coin flips encountered along path π."
The value of a path is the sum of the values assigned to its arcs.
"The E step must return the expected value of the unknown path that generated (x i , y i )."
"For example, if every arc had value 1, then expected value would be expected path length."
"Letting Π denote the set of paths in x i ◦ f θ ◦ y i (Fig. 2), the expected value is 14"
"E[val(π) | x i , y i ] ="
P π∈Π P (π) val(π) (1) P π∈Π P (π)
The denominator of equation (1) is the total prob-ability of all accepting paths in x i ◦ f ◦ y i .
"But while computing this, we will also compute the numerator."
"The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability."
"We will enforce an invariant: the weight of any pathset Π must be (P π∈Π P (π), P π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute."
Multiplica-tion and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to com-bine the weights of alternative paths.
"To sum over infinite sets of cyclic paths we also need a closure operation ∗ , interpreted as k ∗ = L ∞i=0 k i ."
"The usual finite-state algorithms work if (K, ⊕, ⊗, ∗ ) has the structure of a closed semiring. [Footnote_15]"
"15 That is: (K,⊗) is a monoid (i.e., ⊗ : K × K → K is associative) with identity 1. (K, ⊕) is a commutative monoid with identity 0. ⊗ distributes over ⊕ from both sides, 0 ⊗ k = k ⊗ 0 = 0, and k ∗ = 1 ⊕ k ⊗ k ∗ = 1 ⊕ k ∗ ⊗ k. For finite-state composition, commutativity of ⊗ is needed as well."
"Ordinary probabilities fall in the semiring (R ≥0 , +, ×, ∗ ). [Footnote_16]"
"16 The closure operation is defined for p ∈ [0,1) as p ∗ = 1/(1 − p), so cycles with weights in [0, 1) are allowed."
Our novel weights fall in a novel 14 Formal derivation of (1): P π
"P(π | x i ,y i )val(π) = (P π P(π, x i , y i ) val(π))/P(x i , y i ) = (P π P(x i , y i | π)P(π)val(π))/P π P(x i ,y i | π)P(π); now observe that P(x i , y i | π) = 1 or 0 according to whether π ∈ Π."
"V -expectation semiring, (R ≥0 × V, ⊕, ⊗, ∗ ): def (p 1 , v 1 ) ⊗ (p 2 , v 2 ) = (p 1 p 2 , p 1 v 2 + v 1 p 2 ) (2) def (p 1 , v 1 ) ⊕ (p 2 , v 2 ) = (p 1 + p 2 , v 1 + v 2 ) (3) if p ∗ defined, (p, v) ∗ = def (p ∗ , p ∗ vp ∗ ) (4)"
"If an arc has probability p and value v, we give it the weight (p, pv), so that our invariant (see above) holds if Π consists of a single length-0 or length-1 path."
"The above definitions are designed to preserve our invariant as we build up larger paths and path-sets. ⊗ lets us concatenate (e.g.) simple paths π 1 , π 2 to get a longer path π with P(π) ="
P(π 1 )P(π 2 ) and val(π) = val(π 1 ) + val(π 2 ).
"The defini-tion of ⊗ guarantees that path π’s weight will be (P(π), P(π) · val(π)). ⊕ lets us take the union of two disjoint pathsets, and ∗ computes infinite unions."
"To compute (1) now, we only need the total weight t i of accepting paths in x i ◦ f ◦ y i (Fig. 2)."
"This can be computed with finite-state methods: the machine ( i )◦f ◦(y i ) is a version that replaces all input:output labels with : , so it maps ( , ) to the same total weight t i ."
Minimizing it yields a one-state FST from which t i can be read directly!
The other “magical” property of the expecta-tion semiring is that it automatically keeps track of the tangled parameter counts.
"For instance, recall that traversing 0 −→ a:x 0 should have the same ef- a:p fect as traversing both the underlying arcs 4 −→ 4 p:x and 6 −→ 6 ."
"And indeed, if the underlying arcs have values v 1 and v 2 , then the composed arc 0 −→ a:x 0 gets weight (p 1 ,p 1 v 1 ) ⊗ (p 2 ,p 2 v 2 ) = (p 1 p 2 , p 1 p 2 (v 1 + v 2 )), just as if it had value v 1 + v 2 ."
"Some concrete examples of values may be useful: • To count traversals of the arcs of Figs. 1b–c, num-ber these arcs and let arc ` have value e ` , the ` th basis vector."
"Then the ` th element of val(π) counts the ap-pearances of arc ` in path π, or underlying path π. • A regexp of form E+ µ F = µE+(1−µ)F should be weighted as (µ, µe k )E + (1 − µ, (1 − µ)e k+1 )F in the new semiring."
"Then elements k and k + 1 of val(π) count the heads and tails of the µ-coin. • For a global log-linear parameterization, an arc’s value is a vector specifying the arc’s features."
Then val(π) counts all the features encountered along π.
"Really we are manipulating weighted relations, not FSTs."
"We may combine FSTs, or determinize or minimize them, with any variant of the semiring-weighted algorithms. 17 As long as the resulting FST computes the right weighted relation, the arrange-ment of its states, arcs, and labels is unimportant."
The same semiring may be used to compute gradi-ents.
"We would like to find f θ (x i , y i ) and its gradient with respect to θ, where f θ is real-valued but need not be probabilistic."
"Whatever procedures are used to evaluate f θ (x i , y i ) exactly or approximately—for example, FST operations to compile f θ followed by minimization of ( × x i ) ◦ f θ ◦ (y i × )—can simply be applied over the expectation semiring, replacing each weight p by (p,∇p) and replacing the usual arithmetic operations with ⊕, ⊗, etc. [Footnote_18] (2)–(4) pre-serve the gradient ((2) is the derivative product rule), so this computation yields (f θ (x i , y i ), ∇f θ (x i , y i ))."
"18 Division and subtraction are also possible: −(p,v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ). Division is com-monly used in defining f θ (for normalization)."
Now for some important remarks on efficiency: • Computing t i is an instance of the well-known algebraic path problem[REF_CITE].
Let T i = x i ◦f◦y i .
Then t i is the total semir-ing weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and un-weighted).
"It is wasteful to compute t i as suggested earlier, by minimizing ( i )◦f ◦(y i ), since then the real work is done by an  step[REF_CITE]that implements the all-pairs version of alge-braic path, whereas all we need is the single-source version."
"If n and m are the number of states and edges, [Footnote_19] then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs[REF_CITE]."
19 Multiple edges from j to k are summed into a single edge.
"For a gen-eral graph T i ,[REF_CITE]shows how to partition into “hard” subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results."
"The overhead of partitioning and recombining is essentially only O(m). • For speeding up the O(n 3 ) problem on subgraphs, one can use an approximate relaxation technique[REF_CITE]."
"Efficient hardware implementation is also possible via chip-level parallelism[REF_CITE]. • In many cases of interest, T i is an acyclic graph. [Footnote_20]"
"20 If x i and y i are acyclic (e.g., fully observed strings), and f (or rather its FST) has no : cycles, then composition will “unroll” f into an acyclic machine. If only x i is acyclic, then the composition is still acyclic if domain(f) has no cycles."
"Then Tarjan’s method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."
"For HMMs (footnote 11), T i is the familiar trellis, and we would like this computation of t i to reduce to the forward-backward algorithm[REF_CITE]."
But notice that it has no backward pass.
"In place of pushing cumu-lative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) for-ward to the probabilities."
"This is slower because our ⊕ and ⊗ are vector operations, and the vec-tors rapidly lose sparsity as they are added together."
We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ).
This speedup also works for cyclic graphs and for any V .
"Write w jk as (p jk ,v jk ), and let w jk1 = (p jk1 ,v jk1 ) denote the weight of the edge from j to k. 19"
"Then it can be shown that w 0n = (p 0n ,P j,k p 0j v jk1 p kn )."
"The for-ward and backward probabilities, p 0j and p kn , can be computed using single-source algebraic path for the simpler semiring (R, +, ×, ∗ )—or equivalently, by solving a sparse linear system of equations over R, a much-studied problem at O(n) space, O(nm) time, and faster approximations[REF_CITE]. •"
"A Viterbi variant of the expectation semiring ex-ists: replace (3) with if(p 1 &gt; p 2 , (p 1 , v 1 ), (p 2 , v 2 ))."
"Here, the forward and backward probabilities can be computed in time only O(m + n log n)[REF_CITE]. k-best variants are also possible."
We have exhibited a training algorithm for param-eterized finite-state machines.
"Some specific conse-quences that we believe to be novel are (1) an EM al-gorithm for FSTs with cycles and epsilons; (2) train-ing algorithms for HMMs and weighted contextual edit distance that work on incomplete data; (3) end-to-end training of noisy channel cascades, so that it is not necessary to have separate training data for each machine in the cascade (cf."
We are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights.
"Many models of interest can be constructed in our paradigm, without having to write new code."
"Bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective func-tions, and finite-state algorithms to all of them."
"To avoid local maxima, one might try determinis-tic annealing[REF_CITE], or randomized methods, or place a prior on θ."
"Another extension is to adjust the machine topology, say by model merg-ing[REF_CITE]."
Such tech-niques build on our parameter estimation method.
The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.
"For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs."
We describe a speedup for training conditional maxi-mum entropy models.
"The algorithm is a simple vari-ation on Generalized Iterative Scaling, but converges roughly an order of magnitude faster, depending on the number of constraints, and the way speed is mea-sured."
"Rather than attempting to train all model pa-rameters simultaneously, the algorithm trains them sequentially."
"The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most maximum entropy problems."
"Conditional Maximum Entropy models have been used for a variety of natural language tasks, includ-ing Language Modeling[REF_CITE], part-of-speech tagging, prepositional phrase attachment, and parsing[REF_CITE], word selection for machine translati[REF_CITE], and find-ing sentence boundaries[REF_CITE]."
"Unfortunately, although maximum entropy (maxent) models can be applied very generally, the typical training algorithm for maxent, Generalized Iterative Scaling (GIS)[REF_CITE], can be extremely slow."
We have personally used up to a month of computer time to train a single model.
There have been several attempts to speed up max-ent training[REF_CITE].
"However, as we describe later, each of these has suffered from appli-cability to a limited number of applications."
"We show that this fast variation can also be used for conditional probabilities, and that it is useful for a larger range of problems than traditional speedup techniques."
"It achieves good speedups for all but the simplest models, and speedups of an order of magnitude or more for typical problems."
"It has only one disadvantage: when there are many possi-ble output values, the memory needed is prohibitive."
"By combining this technique with another speedup technique[REF_CITE], this disadvantage can be eliminated."
"Conditional maxent models are of the form exp i λ i f i (x, y) P (y|x) = (1) y exp i λ i f i (x, y ) where x is an input vector, y is an output, the f i are the so-called indicator functions or feature values that are true if a particular property of x, y is true, and λ i is a weight for the indicator f i ."
"For instance, if trying to do word sense disambiguation for the word “bank”, x would be the context around an oc-currence of the word; y would be a particular sense, e.g. financial or river; f i (x, y) could be 1 if the con-text includes the word “money” and y is the financial sense; and λ i would be a large positive number."
Maxent models have several valuable proper-ties.
The most important is constraint satisfaction.
"For a given f i , we can count how many times f i was observed in the training data, observed[i] = j f i (x j ,y j )."
"For a model P λ with parameters λ, we can see how many times the model pre-dicts that f i would be expected: expected[i] = j,y P λ (y|x j )f i (x j , y)."
Maxent models have the property that expected[i] = observed[i] for all i.
These equalities are called constraints.
"An addi-tional property is that, of models in the form of Equa-tion 1, the maxent model maximizes the probability of the training data."
"Yet another property is that max-ent models are as close as possible to the uniform distribution, subject to constraint satisfaction."
"Maximum entropy models are most commonly learned using GIS, which is actually a very simple algorithm."
"At each iteration, a step is taken in a di-rection that increases the likelihood of the training data."
The step size is guaranteed to be not too large and not too small: the likelihood of the training data increases at each iteration and eventually converges to the global optimum.
"Unfortunately, this guaran-tee comes at a price: GIS takes a step size inversely proportional to the maximum number of active con-straints."
"Maxent models are interesting precisely be-cause of their ability to combine many different kinds of information, so this weakness of GIS means that maxent models are slow to learn precisely when they are most useful."
We will describe a variation on GIS that works much faster.
"Rather than learning all parameters of the model simultaneously, we learn them sequen-tially: one, then the next, etc., and then back to the beginning."
The new algorithm converges to the same point as the original one.
"This sequential learning would not lead to much, if any, improvement, ex-cept that we also show how to cache subcomputa-tions."
The combination leads to improvements of an order of magnitude or more.
We begin by describing the classic GIS algorithm.
"Recall that GIS converges towards a model in which, for each f i , expected[i] = observed[i]."
"Whenever they are not equal, we can move them closer."
One simple idea is to just add logobserved[i]/expected[i] to λ i .
The problem with this is that it ignores the interaction with other λs.
"If updates to other λs made on the same iteration of GIS have a similar effect, we could easily go too far, and even make things worse."
"GIS introduces a slowing factor, f # , equal to the largest total value of f i : f # = max j,y i f i (x j , y)."
"Next, GIS computes an update: δ i = log observedf[ # i]/expected[i] (2)"
We then add δ i to λ i .
This update provably converges to the global optimum.
"GIS for joint models was given[REF_CITE]; the conditional version is due to Brown et al. (Unpublished), as described[REF_CITE]."
"In practice, we use the pseudocode of Figure 1. 1"
"We will write I for the number of training instances, ing the slack indicator, but fixing the corresponding λ to 0, and expected[0..F ] = 0 for each training instance j for each output y s[j, y] := 0 for each i such that f i (x j , y) = 0 s[j, y] += λ i × f i (x j , y) z := y e s[j,y] for each output y for each i such that f i (x j , y) = 0 expected[i] += f i (x j , y) × e s[j,y] /z for each i δ i = f1 # log observedexpected [[ii]] λ i += δ i"
Figure 1: One Iteration of Generalized Iterative Scal-ing (GIS) and F for number of indicator functions; we use Y for the number of output classes (values for y).
"We assume that we keep a data structure listing, for each training instance x j and each value y, the i such that f i (x j , y) = 0."
Now we can describe our variation on GIS.
"Basi-cally, instead of updating all λ’s simultaneously, we will loop over each indicator function, and compute an update for that indicator function, in turn."
"In par-ticular, the first change we make is that we exchange the outer loops over training instances and indicator functions."
"Notice that in order to do this efficiently, we also need to rearrange our data structures: while we previously assumed that the training data was stored as a sparse matrix of indicator functions with non-zero values for each instance, we now assume that the data is stored as a sparse matrix of instances with non-zero values for each indicator."
The size of the two matrices is obviously the same.
"The next change we make is to update each λ i near the inner loop, immediately after expected[i] is computed, rather than after expected values for all features have been computed."
"If we update the fea-tures one at a time, then the meaning of f # changes."
"In the original version of GIS, f # is the largest total of all features."
"However, f # only needs to be the largest total of all the features being updated, and in z[1..I] = Y s[1..I, [Footnote_1]..Y ] = 0 for each feature f i expected = 0 for each output y for each instance j such that f i (x j , y) = 0 expected += f i (x j , y) × e s[j,y] /z[j] δ i = max j,y 1f i (x j ,y) log observedexpected [[ii]] λ i += δ i for each output y for each instance j such that f i (x j , y) = 0 z[j] −= e s[j,y] s[j, y] += δ i z[j] += e s[j,y] this case, there is only one such feature."
"1 Many published versions of the GIS algorithm require in-clusion of a “slack” indicator function so that the same number of constraints always applies. In practice it is only necessary that the total of the indicator functions be bounded by f # , not necessarily equal to it. Alternatively, one can see this as includ-"
"Thus, in-stead of f # , we use max j,y f i (x j , y)."
"In many max-ent applications, the f i take on only the values 0 or 1, and thus, typically, max j,y f i (x j , y) = 1."
"There-fore, instead of slowing by a factor of f # , there may be no slowing at all!"
We make one last change in order to get a speedup.
"Rather than recompute for each instance j and each output y, s[j, y] = i λ i × f i (x j , y), and the corre-sponding normalizing factors z = y e s[j,y] we in-stead keep these arrays computed as invariants, and incrementally update them whenever a λ i changes."
"With this important change, we now get a substantial speedup."
The code for this transformed algorithm is given in Figure 2.
"The space of models in the form of Equation 1 is convex, with a single global optimum."
"Thus, GIS and SCGIS are guaranteed to converge towards the same point."
"For convergence proofs, see[REF_CITE], who prove convergence of the algorithm for joint models."
"In this section, we analyze the time and space re-quirements for SCGIS compared to GIS."
"The space results depend on Y, the number of output classes."
"When Y is small, SCGIS requires only a small amount more space than GIS."
"Note that in Section 3, we describe a technique that, when there are many output classes, uses clustering to get both a speedup and to reduce the number of outputs, thus alleviating the space issues."
"Typically for GIS, the training data is stored as a sparse matrix of size T of all non-zero indicator functions for each instance j and output y."
The trans-posed matrix used by SCGIS is the same size T .
"In order to make the relationship between GIS and SCGIS clearer, the algorithms in Figures 1 and 2 are given with some wasted space."
"For instance, the matrix s[j,y] of sums of λs only needs to be a simple array s[y] for GIS, but we wrote it as a matrix so that it would have the same meaning in both algorithms."
"In the space and time analyses, we will assume that such space-wasting techniques are optimized out before coding."
Now we can analyze the space and time for GIS.
"GIS requires the training matrix, of size T , the λs, of size F , as well as the expected and observed arrays, which are also size F. Thus, GIS requires space O(T + F)."
"Since T must be at least as large as F (we can eliminate any indicator functions that don’t appear in the training data), this is O(T )."
SCGIS is potentially somewhat larger.
"SCGIS also needs to store the training data, albeit in a differ-ent form, but one that is also of size T ."
"In particular, the matrix is interchanged so that its outermost index is over indicator functions, instead of training data."
"SCGIS also needs the observed and λ arrays, both of size F , and the array z[j] of size I, and, more im-portantly, the full array s[j, y], which is of size IY ."
"In many problems, Y is small – often 2 – and IY is negligible, but in problems like language modeling, Y can be very large (60,000 or more)."
"The overall space for SCGIS, O(T +IY ), is essentially the same as for GIS when Y is small, but much larger when Y is large – but see the optimization described in Section 3."
"Now, consider the time for each algorithm to ex-ecute one iteration."
"Assume that for every instance and output there is at least one non-zero indicator function, which is true in practice."
"Notice that for GIS, the top loops end up iterating over all non-zero indicator functions, for each output, for each training instance."
"In other words, they examine every entry in the training matrix T once, and thus require time T ."
"The bottom loops simply require time F , which is smaller than T ."
"Thus, GIS requires time O(T )."
"For SCGIS, the top loops are also over each non-zero entry in the training data, which takes time O(T)."
The bottom loops also require time O(T).
"Thus, one iteration of SCGIS takes about as long as one iteration of GIS, and in practice in our im- plementation, each SCGIS iteration takes about 1.3 times as long as each GIS iteration."
"The speedup in SCGIS comes from the step size: the update in GIS is slowed by f # , while the update in SCGIS is not."
"Thus, we expect SCGIS to converge by up to a factor of f # faster."
"For many applications, f # can be large."
"The speedup from the larger step size is difficult to analyze rigorously, and it may not be obvious whether the speedup we in fact observe is actually due to the f # improvement or to the caching."
"Note that without the caching, each iteration of SCGIS would be O(f#) times slower than an iteration of GIS; the caching is certainly a key component."
"But with the caching, each iteration of SCGIS is still marginally slower than GIS (by a small constant fac-tor)."
"In Section 4, we in fact empirically observe that fewer iterations are required to achieve a given level of convergence, and this reduction is very roughly proportional to f#."
"Thus, the speedup does appear to be because of the larger step size."
"However, the exact speedup from the step size depends on many factors, including how correlated features are, and the order in which they are trained."
"Although we are not aware of any problems where maxent training data does not fit in main memory, and yet the model can be learned in reasonable time, it is comforting that SCGIS, like GIS, requires se-quential, not random, access to the training data."
"So, if one wanted to train a model using a large amount of data on disk or tape, this could still be done with reasonable efficiency, as long as the s and z arrays, for which we need random access, fit in main mem-ory."
All of these analyses have assumed that the train-ing data is stored as a precomputed sparse matrix of the non-zero values for f i for each training instance for each output.
"In some applications, such as lan-guage modeling, this is not the case; instead, the f i are computed on the fly."
"However, with a bit of thought, those data structures also can be rearranged."
"Maximum entropy models are natu-rally maximally smooth, in the sense that they are as close as possible to uniform, subject to satisfy-ing the constraints."
"However, in practice, there may be enough constraints that the models are not nearly smooth enough – they overfit the training data."
Chen and Rosenfeld describe a technique whereby a Gaus-sian prior on the parameters is assumed.
"The models no longer satisfy the constraints exactly, but work much better on test data."
"In particular, instead of attempting to maximize the probability of the train-ing data, they maximize a slightly different objective function, the probability of the training data times the prior probability of the model:"
J arg max P λ (y j |x j )P (λ) (3) λ j=1 λ2i where P(λ) =
I √1 e − 2σ2 .
"In other words, i=1 [Footnote_2]πσ the probability of the λs is a simple normal distribu-tion with 0 mean, and a standard deviation of σ."
"2[REF_CITE]use an algorithm that might appear sequential, butanexaminationofthedefinitionoff # andrelated work shows that it is not."
"Chen and Rosenfeld describe a modified update rule in which to find the updates, one solves for δ i in observed[i] = expected[i] × e δ i f # + λ i σ+ 2 δ i"
"SCGIS can be modified in a similar way to use an update rule in which one solves for δ i in observed[i] = expected[i]×e δ i max j,y f i (x j ,y) +λ i σ+ 2 δ i"
"Although sequential updating was described for joint probabilities in the original paper on GIS[REF_CITE], GIS with sequential updating for conditional models appears previously unknown."
"Note that in the NLP community, almost all max-ent models have used conditional models (which are typically far more efficient to learn), and none to our knowledge has used this speedup. 2"
There appear to be two main reasons this speedup has not been used before for conditional models.
"One issue is that for joint models, it turns out to be more natural to compute the sums s[x], while for con-ditional models, it is more natural to compute the λs and not store the sums s."
Storing s is essential for our speedup.
"Also, one of the first and best known uses of conditional maxent models is for language mod-eling[REF_CITE], where the number of output classes is the vocabulary size, typically 5,000-60,000 words."
"For such applications, the array s[j, y] would be of a size at least 5000 times the number of train-ing instances: clearly impractical (but see below for a recently discovered trick)."
"Thus, it is unsurprising that this speedup was forgotten."
There have been several previous attempts to speed up maxent modeling.
"Best known is the work[REF_CITE], the Improved Iterative Scaling (IIS) algorithm."
"Instead of treating f # as a constant, we can treat it as a function of x j and y."
"In particular, let f # (x, y) = i f i (x, y) Then, solve numerically for δ i in the equation observed[i] = (4) P λ (y|x j ) × f i (x j , y) × exp(δ i f # (x j , y)) j,y"
"Notice that in the special case where f # (x,y) is a constant f # , Equation 4 reduces to Equation 2."
"However, for training instances where f # (x j , y) &lt; f # , the IIS update can take a proportionately larger step."
"Thus, IIS can lead to speedups when f # (x j , y) is substantially less than f # ."
"It is, however, hard to think of applications where this difference is typi-cally large."
We only know of one limited experiment comparing IIS to GIS[REF_CITE].
That experi-ment showed roughly a factor of 2 speedup.
"It should be noted that compared to GIS, IIS is much harder to implement efficiently."
"When solving Equation 4, one uses an algorithm such as Newton’s method that repeatedly evaluates the function."
"Either one must repeatedly cycle through the training data to compute the right hand side of this equation, or one must use tricks such as bucketing by the values of f # (x j , y)."
The first option is inefficient and the second adds considerably to the complexity of the algorithm.
"Note that IIS and SCGIS can be combined by us-ing an update rule where one solves for observed[i] = (5) P λ (x j , y) × f i (x j , y) × exp(δ i f i (x j , y)) j,y"
"For many model types, the f i take only the values 1 or 0."
"In this case, Equation 5 reduces to the normal SCGIS update."
"For binary-valued features, without the caching trick, SCGIS is the same as the algorithm described by Jelinek."
"The advantage of SCGIS over IS is the caching – without which there is no speedup – and because it is a variation on GIS, it can be applied to non-binary valued features."
"Also, with SCGIS, it is clear how to apply other improvements such as the smoothing technique[REF_CITE]."
"Several techniques have been developed specif-ically for speeding up conditional maxent models, especially when Y is large, such as language mod-els, and space precludes a full discussion here."
"These techniques include unigram caching, cluster expan-si[REF_CITE], and word clustering[REF_CITE]."
"Of these, the best appears to be word clustering, which leads to up to a factor of 35 speedup, and which has an additional advantage: it allows the SCGIS speedup to be used when there are a large number of outputs."
"The word clustering speedup (which can be ap-plied to almost any problem with many outputs, not just words) works as follows."
"Notice that in both GIS and in SCGIS, there are key loops over all outputs, y. Even with certain optimizations that can be applied, the length of these loops will still be bounded by, and often be proportional to, the number of outputs."
"We therefore change from a model of the form P(y|x) to modeling P (cluster(y)|x) × P (y|x, cluster(y))."
"Consider a language model in which y is a word, x represents the words preceding y, and the vocabulary size is 10,000 words."
"Then for a model P (y|x), there are 10,000 outputs."
"On the other hand, if we create 100 word clusters, each with 100 words per clus-ter, then for a model P (cluster(y)|x), there are 100 outputs, and for a model P (y|x, cluster(y)) there are also 100 outputs."
"Thus, instead of training one model with a time proportional to 10,000, we train two mod-els, each with time proportional to 100."
"Thus, in this example, there is a 50 times speedup."
"In practice, the speedups are not quite so large, but we do achieve speedups of up to a factor of 35."
"Although the model form learned is not exactly the same as the original model, the perplexity of the form using two models is actually marginally lower (better) than the perplex-ity of the form using a single model, so there does not seem to be any disadvantage to using it."
The word clustering technique can be extended to use multiple levels.
"For instance, by putting words into superclusters, such as their part of speech, and clusters, such as semantically similar words of a given part of speech, one could use a three level model."
"In fact, the technique can be extended to up to log 2 Y levels with two outputs per level, mean-ing that the space requirements are proportional to 2 instead of to the original Y ."
"Since SCGIS works by increasing the step size, and the cluster-based speedup works by increasing the speed of the in-ner loop (whchi SCGIS shares), we expect that the two techniques would complement each other well, and that the speedups would be nearly multiplica-tive."
Very preliminary language modeling experi-ments are consistent with this analysis.
There has been interesting recent unpublished work[REF_CITE].
"While this work is very preliminary, and the experimental setting somewhat unrealistic (dense features artificially generated), es-pecially for many natural language tasks, the results are dramatic enough to be worth noting."
"In particu-lar, Minka found that a version of conjugate gradient descent worked extremely well – much faster than GIS."
"If the problem domain resembles Minka’s, then conjugate gradient descent and related techniques are well worth trying, and it would be interesting to try these techniques for more realistic tasks."
SCGIS turns out to be related to boosting.
"As shown[REF_CITE], boosting is in some ways a sequential version of maxent."
"The single largest difference between our algorithm and Collins’is that we update each feature in order, while Collins’ algorithms select a (possibly new) feature to update."
That algorithm also require more storage than our algorithm when data is sparse: fast imple-mentations require storage of both the training data matrix (to compute which feature to update) and the transpose of the training data matrix (to perform the update efficiently.)
"In this section, we give experimental results, show-ing that SCGIS converges up to an order of magni-tude faster than GIS, or more, depending on the num-ber of non-zero indicator functions, and the method of measuring performance."
There are at least three ways in which one could measure performance of a maxent model: the ob-jective function optimized by GIS/SCGIS; the en-tropy on test data; and the percent correct on test data.
The objective function for both SCGIS and GIS when smoothing is Equation 3: the probabil-ity of the training data times the probability of the model.
"The most interesting measure, the percent correct on test data, tends to be noisy."
"For a test corpus, we chose to use exactly the same training, test, problems, and feature sets used[REF_CITE]."
"These problems consisted of trying to guess which of two confusable words, e.g. “their” or “there”, a user intended."
"Banko and Brill chose this data to be representative of typical ma-chine learning problems, and, by trying it across data sizes and different pairs of words, it exhibits a good deal of different behaviors."
"Banko and Brill used a standard set of features, including words within a window of 2, part-of-speech tags within a window of 2, pairs of word or tag features, and whether or not a given word occurred within a window of 9."
"Alto-gether, they had 55 feature types."
"That is, there were many thousands of features in the model (depending on the exact model), but at most 55 could be “true” for a given training or test instance."
We examine the performance of SCGIS versus GIS across three different axes.
The most important variable is the number of features.
"In addition to try-ing Banko and Brill’s 55 feature types, we tried using feature sets with 5 feature types (words within a win-dow of 2, plus the “unigram” feature) and 15 feature types (words within a window of 2, tags within a window of 2, the unigram, and pairs of words within a window of 2)."
"We also tried not using smoothing, and we tried varying the training data size."
"In Table 1, we present a “typical” configuration, using 55 feature types, and 10 million words of train-ing, and smoothing with a Gaussian prior."
The first two columns show the different confusable words.
Each column shows the ratio of how much longer (in terms of elapsed time) it takes GIS to achieve the same results as 10 iterations of SCGIS.
An “XXX” denotes a case in which GIS did not achieve the performance level[REF_CITE]iterations. (XXXs were not included in averages.) [Footnote_3] The “ob-jec” column shows the ratio of time to achieve the same value of the objective function (Equation 3); the “ent” column show the ratio of time to achieve the same test entropy; and the “cor” column shows the ratio of time to achieve the same test error rate.
"3 On a 1.7 GHz Pentium[REF_CITE]000,000 words train-ing, and 5 feature types it took between .006 and .24 seconds per iteration of SCGIS, and between .004 and .18 seconds for GIS.[REF_CITE]feature types, it took between .05 and 1.7 sec-onds for SCGIS and between .03 and 1.2 seconds for GIS. Note that many experiments use much larger datasets or many more feature types; run time scales linearly with training data size."
"For all three measurements, the ratio can be up to a factor of 30, though the average is somewhat lower, and in two cases, GIS converged faster."
"In Table 2 we repeat the experiment, but with-out smoothing."
On the objective function – which with no smoothing is just the training entropy – the increase from SCGIS is even larger.
"On the other criteria – test entropy and percentage correct – the increase from SCGIS is smaller than it was with smoothing, but still consistently large."
"In Tables 3 and 4, we show results with small and medium feature sets."
"As can be seen, the speedups with smaller features sets (5 feature types) are less than the speedups with the medium sized feature set (15 feature types), which are smaller than the base-line speedup with 55 features."
"Notice that across all experiments, there were no cases where GIS converged faster than SCGIS on the objective function; two cases where it coverged faster on test data entropy; and 5 cases where it con-verged faster on test data correctness."
"The objective function measure is less noisy than test data entropy, and test data entropy is less noisy than test data er-ror rate: the noisier the data, the more chance of an unexpected result."
"Thus, one possibility is that these cases are simply due to noise."
"Similarly, the four cases in which GIS never reached the test data entropy of SCGIS and the four cases in which GIS never reached the test data error rate of SCGIS might also be attributable to noise."
There is an alternative explanation that might be worth exploring.
"On a dif-ferent data set, 20 newsgroups, we found that early stopping techniques were helpful, and that GIS and SCGIS benefited differently depending on the ex-act settings."
It is possible that effects similar to the smoothing effect of early stopping played a role in both the XXX cases (in which SCGIS presumably benefited more from the effects) and in the cases where GIS beat SCGIS (in which cases GIS pre-sumably benefited more.)
"Additional research would be required to determine which explanation – early stopping or noise – is correct, although we suspect both explanations apply in some cases."
"We also ran experiments that were the same as the baseline experiment, except changing the training data size to 50 million words and to 1 million words."
"We found that the individual speedups were often different at the different sizes, but did not appear to be overall higher or lower or qualitatively different."
There are many reasons that maxent speedups are useful.
"First, in applications with active learning or parameter optimization or feature set selection, it may be necessary to run many rounds of maxent, making speed essential."
"There are other fast algo-rithms, such as Winnow, available, but in our ex-perience, there are some problems where smoothed maxent models are better classifiers than Winnow."
"Furthermore, many other fast classification algo-rithms, including Winnow, do not output probabil-ities, which are useful for precision/recall curves, or when there is a non-equal tradeoff between false positives and false negatives, or when the output of the classifier is used as input to other models."
"Fi-nally, there are many applications of maxent where huge amounts of data are available, such as for lan-guage modeling."
"Unfortunately, it has previously been very difficult to use maxent models for these types of experiments."
"For instance, in one language modeling experiment we performed, it took a month to learn a single model."
"Clearly, for models of this type, any speedup will be very helpful."
"Overall, we expect this technique to be widely used."
It leads to very significant speedups – up to an order of magnitude or more.
"It is very easy to imple-ment – other than the need to transpose the training data matrix, and store an extra array, it is no more complex than standard GIS."
"It can be easily applied to any model type, although it leads to the largest speedups on models with more feature types."
"Since models with many interacting features are the type for which maxent models are most interesting, this is typical."
"It requires very few additional resources: unless there are a large number of output classes, it uses about as much space as standard GIS, and when there are a large number of output classes, it can be combined with our clustering speedup technique[REF_CITE]to get both additional speedups, and to reduce the space requirements."
"Thus, there appear to be no real impediments to its use, and it leads to large, broadly applicable gains."
Natural-Language Generation from flat semantics is an NP-complete problem.
This makes it necessary to develop al-gorithms that run with reasonable effi-ciency in practice despite the high worst-case complexity.
"We show how to con-vert TAG generation problems into de-pendency parsing problems, which is use-ful because optimizations in recent de-pendency parsers based on constraint pro-gramming tackle exactly the combina-torics that make generation hard."
"Indeed, initial experiments display promising run-times."
Existing algorithms for realization from a flat input semantics all have runtimes which are exponential in the worst case.
Several different approaches to im-proving the runtime in practice have been suggested in the literature – e.g. heuristics[REF_CITE]and factorizations into smaller exponential subproblems[REF_CITE].
"While these solu-tions achieve some measure of success in making re-alization efficient, the contrast in efficiency to pars-ing is striking both in theory and in practice."
"The problematic runtimes of generation algo-rithms are explained by the fact that realization is an NP-complete problem even using just context-free grammars,[REF_CITE]showed in the context of shake-and-bake generation."
"The first contribution of our paper is a proof of a stronger NP-completeness result: If we allow semantic indices in the grammar, realization is NP-complete even if we fix a single grammar."
Our alternative proof shows clearly that the combinatorics in generation come from essen-tially the same sources as in parsing for free word order languages.
"It has been noted in the literature that this problem, too, becomes NP-complete very easily[REF_CITE]."
The main point of this paper is to show how to encode generation with a variant of tree-adjoining grammars (TAG) as a parsing problem with depen-dency grammars (DG).
"The particular variant of DG we use, Topological Dependency Grammar (TDG)[REF_CITE], was developed specifically with efficient parsing for free word order languages in mind."
"The mere exis-tence of this encoding proves TDG’s parsing prob-lem NP-complete as well, a result which has been conjectured but never formally shown so far."
But it turns out that the complexities that arise in gener-ation problems in practice seem to be precisely of the sort that the TDG parser can handle well.
"Initial experiments with generating from the XTAG gram-mar (XTAG[REF_CITE]) suggest that our generation system is competitive with state-of-the-art chart generators, and indeed seems to run in poly-nomial time in practice."
"Next to the attractive runtime behaviour, our ap-proach to realization is interesting because it may provide us with a different angle from which to look for tractable fragments of the general realiza-tion problem."
"As we will show, the computation that takes place in our system is very different from that in a chart generator, and may be more efficient in some cases by taking into account global informa-tion to guide local choices."
Plan of the Paper.
"We will define the problem we want to tackle in Section 2, and then show that it is NP-complete (Section 3)."
"In Section 4, we sketch the dependency grammar formalism we use."
"Sec-tion 5 is the heart of the paper: We show how to encode TAG generation as TDG parsing, and dis-cuss some examples and runtimes."
"We compare our approach to some others in Section 6, and conclude and discuss future research in Section 7."
"In this paper, we deal with the subtask of natural language generation known as surface realization: given a grammar and a semantic representation, the problem is to find a sentence which is grammatical according to the grammar and expresses the content of the semantic representation."
"We represent the semantic input as a multiset (bag) of ground atoms of predicate logic, such as {buy(e,a,b), name(a,mary) car(b)}."
"To en-code syntactic information, we use a tree-adjoining grammar without feature structures[REF_CITE]."
"We assume that the root node, all substitution nodes, and all nodes that admit ad-junction carry such index variables."
"We also assign a semantics to every elementary tree, so that lexi-cal entries are pairs of the form (ϕ, T), where ϕ is a multiset of semantic atoms, and T is an initial or auxiliary tree, e.g. S:x"
"NP:y VP:x ( {buy(x,y,z)}, ) V:x NP:z buys"
"When the lexicon is accessed, x,y,z get bound to terms occurring in the semantic input, e.g. e, a, b in our example."
"Since we furthermore assume that every index variable that appears in T also appears in ϕ, this means that all indices occurring in T get bound at this stage."
The semantics of a complex tree is the multiset union of the semantics of the elementary trees in-volved.
"Now we say that the realization problem of a grammar G is to decide for a given input semantics S and an index i whether there is a derivation tree which is grammatical according to G, is assigned the semantics S, and has a root node with index i."
"This definition is the simplest conceivable formal-ization of problems occurring in surface realization as a decision problem: It does not even require us to compute a single actual realization, just to check whether one exists."
Every practical generation sys-tem generating from flat semantics will have to ad-dress this problem in one form or another.
Now we show that this problem is NP-complete.
"A similar result was proved in the context of shake-and-bake generation[REF_CITE], but he needed to use the grammar in his encoding, which leaves the possibility open that for every single grammar G, there might be a realization algorithm tailored specifically to G which still runs in polynomial time."
Our result is stronger in that we define a single grammar G ham whose realization problem is NP-complete in the above sense.
"Furthermore, we find that our proof brings out the sources of the complex-ity more clearly."
"G ham does not permit adjunction, hence the result also holds for context-free gram-mars with indices."
"It is clear that the problem is in 1 ¯ ¯ 2 NP: We can simply guess the ele-mentary trees we need and how to combine them, and then check in ¯ 3 polynomial time whether they verbalize the seman-tics."
The NP-hardness proof is by reducing the well-known HAMILTONIAN - PATH problem to the realiza-tion problem.
"HAMILTONIAN - PATH is the problem of deciding whether a directed graph has a cycle that visits each node exactly once, e.g. (1,3,2,1) in the graph shown above."
"We will now construct an LTAG grammar G ham such that every graph G = (V,E) can be encoded as a semantic input S for the realization problem of G ham , which can be verbalized if and only if G has a Hamiltonian cycle."
S is defined as follows:
"S = {node(i) | i ∈ V } ∪ {edge(i, k) | (i, k) ∈ E} ∪ {start-eating, end-eating}."
"The grammar G ham is given in Fig. 1; the start symbol is B, and we want the root to have index 1."
The tree α 1 models an edge transition from node i to the node k by consuming the semantic encodings of this edge and (by way of a substitution of α 3 ) of the node i.
"The second substitution node of α 1 can be filled either by another α 1 , in which way a path through the graph is modelled, or by an α 4 , in which case we switch to an “edge eating mode”."
"In this mode, we can arbitrarily consume edges using α 2 , and close the tree with α 5 when we’re done."
"This is illustrated in Fig. 2, the tree corresponding to the cycle in the example graph above."
"The Hamiltonian cycle of the graph, if one exists, is represented in the indices of the B nodes."
"The list of these indices is a path in the graph, as the α 1 trees model edge transitions; it is a cycle because it starts in 1 and ends in 1; and it visits each node exactly once, for we use exactly one α 1 tree for each node literal."
The edges which weren’t used in the cycle can be consumed in the edge eating mode.
The main source for the combinatorics of the re-alization problem is thus the interaction of lexical ambiguity and the completely free order in the flat semantics.
"Once we have chosen between α 1 and α 2 in the realization of each edge literal, we have deter-mined which edges should be part of the prospective Hamiltonian cycle, and checking whether it really is one can be done in linear time."
"If, on the other hand, the order of the input placed restrictions on the structure of the derivation tree, we would again have information that told us when to switch into the edge eating mode, i.e. which edges should be part of the cycle."
A third source of combinatorics which does not become so clear in this encoding is the con-figuration of the elementary trees.
"Even when we have committed to the lexical entries, it is conceiv-able that only one particular way of plugging them into each other is grammatical."
"These factors are exactly the same that make depen-dency parsing for free word order languages diffi-cult, and it seems worthwhile to see whether op-timized parsers for dependency grammars can also contribute to making generation efficient."
We now sketch a dependency formalism which has an effi-cient parser and then discuss some of the important properties of this parser.
"In the next section, we will see how to employ the parser for generation."
"The parse trees of topological dependency grammar (TDG)[REF_CITE]are trees whose nodes correspond one-to-one to the words of the sentence, and whose edges are la-belled, e.g. with syntactic relations (see Fig. 3)."
"The trees are unordered, i.e. there is no intrinsic order among the children of a node."
"Word order in TDG is initially completely free, but there is a separate mechanism to specify constraints on linear prece-dence."
"Since completely free order is what we want for the realization problem, we do not need these mechanisms and do not go into them here."
"The lexicon assigns to each word a set of lexical entries; in a parse tree, one of these lexical entries has to be picked for each node."
The lexical entry specifies what labels are allowed on the incoming edge (the node’s labels) and the outgoing edges (the node’s valency).
"Here are some examples: word labels valency likes ∅ {subj, obj, adv∗} Peter {subj, obj} ∅"
"Mary {subj, obj} ∅"
"The lexical entry for “likes” specifies that the corre-sponding node does not accept any incoming edges (and hence must be the root), must have precisely one subject and one object edge going out, and can have arbitrarily many outgoing edges with label adv (indicated by ∗)."
The nodes for “Peter” and “Mary” both require their incoming edge to be labelled with either subj or obj and neither require nor allow any outgoing edges.
"A well-formed dependency tree for an input sen-tence is simply a tree with the appropriate nodes, whose edges obey the labels and valency restric-tions specified by the lexical entries."
"So, the tree in Fig. 3 is well-formed according to our lexicon."
"The parsing problem of TDG can be seen as a search problem: For each node, we must choose a lexi-cal entry and the correct mother-daughter relations it participates in."
One strength of the TDG approach is that it is amenable to strong syntactic inferences that tackle specifically the three sources of complexity mentioned above.
"The parsing algorithm[REF_CITE]is stated in the framework of constraint programming[REF_CITE], a general approach to coping with combinatorial problems."
"Before it explores all choices that are possible in a certain state of the search tree (distribution), it first tries to eliminate some of the choices which definitely cannot lead to a solution by simple inferences (propagations). “Sim-ple” means that propagations take only polynomial time; the combinatorics is in the distribution steps alone."
"That is, it can still happen that a search tree of exponential size has to be explored, but the time spent on propagation in each of its node is only poly-nomial."
"Strong propagation can reduce the size of the search tree, and it may even make the whole al-gorithm run in polynomial time in practice."
"The TDG parser translates the parsing prob-lem into constraints over (variables denoting) fi-nite sets of integers, as implemented efficiently in the Mozart programming system (Oz[REF_CITE])."
This translation is complete: Solutions of the set constraint can be translated back to cor-rect dependency trees.
"But for efficiency, the parser uses additional propagators tailored to the specific inferences of the dependency problem."
"For instance, in the “Peter likes Mary” example above, one such propagator could contribute the information that nei-ther the “Peter” nor the “Mary” node can be an adv child of “likes”, because neither can accept an adv edge."
"Once the choice has been made that “Peter” is the subj child of “likes”, a propagator can contribute that “Mary” must be its obj child, as it is the only possible candidate for the (obligatory) obj child."
"Finally, lexical ambiguity is handled by selection constraints."
These constraints restrict which lexical entry should be picked for a node.
"When all pos-sible lexical entries have some information in com-mon (e.g., that there must be an outgoing subj edge), this information is automatically lifted to the node and can be used by the other propagators."
Thus it is sometimes even possible to finish parsing without committing to single lexical entries for some nodes.
"We will now show how TDG parsing can be used to enumerate all sentences expressing a given input se-mantics, thereby solving the realization problem in-troduced in Section 2."
We first define the encoding.
Then we give an example and discuss some runtime results.
"Finally, we consider a particular restriction of our encoding and ways of overcoming it."
"Let G be a grammar as described in Section 2; i.e. lexical entries are of the form (ϕ,T), where ϕ is a flat semantics and T is a TAG elementary tree whose nodes are decorated with semantic in-dices."
We make the following simplifying assump-tions.
"First, we assume that the nodes of the elemen-tary trees of G are not labelled with feature struc-tures."
"Next, we assume that whenever we can adjoin an auxiliary tree at a node, we can adjoin arbitrarily many trees at this node."
"The idea of multiple adjunc-tion is not new[REF_CITE], but it is simplified here because we disregard complex ad-junction constraints."
We will discuss these two re-strictions in the conclusion.
"Finally, we assume that every lexical semantics ϕ has precisely one member; this restriction will be lifted in Section 5.4."
"Now let’s say we want to find the realizations of the input semantics S = {ϕ 1 ,... ,ϕ n }, using the grammar G."
"The input “sentence” of the parsing problem we construct is the sequence {start} ∪ S, where start is a special start symbol."
"The parse tree will correspond very closely to a TAG deriva-tion tree, its nodes standing for the instantiated ele-mentary trees that are used in the derivation."
"To this end, we use two types of edge labels – substitution and adjunction labels."
"An edge with a substitution label subst A,i,p from the node α to the node β (both of which stand for elementary trees) indicates that β should be plugged into the p-th sub-stitution node in α that has label A and index i. We write subst(A) for the maximum number of occur-rences of A as the label of substitution nodes in any elementary tree of G; this is the maximum value that p can take."
"An edge with an adjunction label adj A,i from α to β specifies that β is adjoined at some node within α carrying label A and index i and admitting adjunc-tion."
It does not matter for our purposes to which node in α β is adjoined exactly; the choice cannot af-fect grammaticality because there is no feature uni-fication involved.
The dependency grammar encodes how an ele-mentary tree can be used in a TAG derivation by restricting the labels of the incoming and outgoing edges via labels and valency requirements in the lex-icon.
"Let’s say that T is an elementary tree of G which has been matched with the input atom ϕ r , in-stantiating its index variables."
Let A be the label and i the index of the root of T .
"If T is an auxiliary tree, it accepts incoming adjunction edges for A and i, i.e. it gets the labels value {adj A,i }."
"If T is an initial tree, it will accept arbitrary incoming substi-tution edges for A and i, i.e. its labels value is {subst A,i,p | 1 ≤ p ≤ subst(A)}"
"In either case, T will require precisely one out-going substitution edge for each of its substitution nodes, and it will allow arbitrary numbers of outgo- ing adjunction edges for each node where we can adjoin."
"That is, the valency value is as follows: {subst A,i,p | ex. substitution node N in T s.t."
"A is label, i is index of N, and N is pth substitution node for A:i in T } ∪ {adj A,i ∗ | ex. node with label A, index i in T which admits adjunction}"
We obtain the set of all lexicon entries for the atom ϕ r by encoding all TAG lexicon entries which match ϕ r as just specified.
"The start symbol, start, gets a special lexicon entry: Its labels entry is the empty set (i.e. it must be the root of the tree), and its valency entry is the set {subst S,k,1 }, where k is the semantic index with which generation should start."
Now let us go through an example to make these def-initions a bit clearer.
"Let’s say we want to verbalize the semantics {name(m, mary), buy(e, m, c), car(c), indef(c), red(c)}"
"The LTAG grammar we use contains the elemen-tary trees which are used in the tree in Fig. 5, along with the obvious semantics; we want to generate a sentence starting with the main event e."
"The encod-ing produces the following dependency grammar; the entries in the “atom” column are to be read as abbreviations of the actual atoms in the input seman-tics. atom labels valency start ∅ {subst S,e,1 } buy {subst S,e,1 } {subst NP,c,1 , subst NP,m,1 , adj V P,e ∗, adj V,e ∗} mary {subst NP,m,1 , {adj NP,1 ∗, adj PN,m ∗} subst NP,m,2 } indef {subst NP,c,1 , {adj NP,c ∗} subst NP,c,2 } car {subst N,c,1 } {adj N,c ∗} red {adj N,c } ∅"
"If we parse the “sentence” start mary buy car indef red with this grammar, leaving the word order com-pletely open, we obtain precisely one parse tree, shown in Fig. 4."
"Reading this parse as a TAG derivation tree, we can reconstruct the derived tree in Fig. 5, which indeed produces the string “Mary buys a red car”."
"The overall realization algorithm we propose en-codes the input problem as a DG parsing problem and then runs the parser described in Section 4.2, which is freely available over the Web, as a black box."
"Because the information lifted to the nodes by the selection constraints may be strong enough to compute the parse tree without ever committing to unique lexical entries, the complete parse may still contain some lexical ambiguity."
"This is no problem, however, because the absence of features guarantees that every combination of choices will be grammat-ical."
"Similarly, a node can have multiple children over adjunction edges with the same label, and there may be more than one node in the upper elemen-tary tree to which the lower tree could be adjoined."
"Again, all remaining combinations are guaranteed to be grammatical."
"In order to get an idea of the performance of our realization algorithm in comparison to the state of the art, we have tried generating the following sentences, which are examples[REF_CITE]: (1) The manager in that office interviewed a new consultant from Germany. (2) Our manager organized an unusual additional weekly departmental conference."
"We have converted the XTAG grammar (XTAG[REF_CITE]) into our grammar format, automatically adding indices to the nodes of the el-ementary trees, removing features, simplifying ad-junction constraints, and adding artificial lexical se-mantics that consists of the words at the lexical an-chors and the indices used in the respective trees."
"XTAG typically assigns quite a few elementary trees to one lemma, and the same lexical semantics can of-ten be verbalized by more than hundred elementary trees in the converted grammar."
"It turns out that the dependency parser scales very nicely to this degree of lexical ambiguity: The sentence (1) is generated in 470 milliseconds (as opposed to Carroll et al.’s 1.8 seconds), whereas we generate (2) in about 170 mil-liseconds (as opposed to 4.3 seconds). [Footnote_1]"
"1 A newer version of Carroll et al.’s system generates (1) in 420 milliseconds (Copestake, p.c.). Our times were measured on a 700 MHz Pentium-III PC."
"Although these numbers are by no means a serious evaluation of our system’s performance, they do present a first proof of concept for our approach."
"The most encouraging aspect of these results is that despite the increased lexical ambiguity, the parser gets by without ever making any wrong choices, which means that it runs in polynomial time, on all examples we have tried."
"This is possible because on the one hand, the selection constraint au-tomatically compresses the many different elemen-tary trees that XTAG assigns to one lemma into very few classes."
"On the other hand, the propagation that rules out impossible edges is so strong that the free input order does not make the configuration prob-lem much harder in practice."
"Finally, our treatment of modification allows us to multiply out the possi-ble permutations in a postprocessing step, after the parser has done the hard work."
"A particularly strik-ing example is (2), where the parser gives us a single solution, which multiplies out to 312 = 13 · 4! dif-ferent realizations. ([REF_CITE]basic realizations corre-spond to different syntactic frames for the main verb in the XTAG grammar, e.g. for topicalized or pas-sive constructions.)"
"So far, we have only considered TAG grammars in which each elementary tree is assigned a semantics that contains precisely one atom."
"However, there are cases where an elementary tree either has an empty semantics, or a semantics that contains mul-tiple atoms."
"The first case can be avoided by ex-ploiting TAG’s extended domain of locality, see e.g.[REF_CITE]."
The simplest possible way for dealing with the second case is to preprocess the input into several different parsing problems.
"In a first step, we collect all possible instantiations of LTAG lexical entries matching subsets of the semantics."
"Then we con-struct all partitions of the input semantics in which each block in the partition is covered by a lexical en-try, and build a parsing problem in which each block is one symbol in the input to the parser."
"This seems to work quite well in practice, as there are usually not many possible partitions."
"In the worst case, however, this approach produces an exponen-tial number of parsing problems."
"Indeed, using a variant of the grammar from Section 3, it is easy to show that the problem of deciding whether there is a partition whose parsing problem can be solved is NP-complete as well."
An alternative approach is to push the partitioning process into the parser as well.
"We expect this will not hurt the runtime all that much, but the exact effect remains to be seen."
The perspective on realization that our system takes is quite different from previous approaches.
"In this section, we relate it to chart generati[REF_CITE]and to another constraint-based approach[REF_CITE]."
"In chart based approaches to realization, the main idea is to minimize the necessary computation by reusing partial results that have been computed be-fore."
"In the setting of fixed word order parsing, this brings an immense increase in efficiency."
"In genera-tion, however, the NP-completeness manifests itself in charts of worst-case exponential size."
"In addition, it can happen that substructures are built which are not used in the final realization, especially when pro-cessing modifications."
"By contrast, our system configures nodes into a dependency tree."
"It solves a search problem, made up by choices for mother-daughter relations in the tree."
"Propagation, which runs in polynomial time, has access to global information (illustrated in Sec-tion 4.2) and can thus rule out impossible mother-daughter relations efficiently; every propagation step that takes place actually contributes to zooming in on the possible realizations."
Our system can show exponential runtimes when the distributions span a search tree of exponential size.
"However, the performance of their system decreases rapidly as the input gets larger even when when working with a toy grammar."
"The main difference between their approach and ours seems to be that their algorithm tries to construct a derived tree, while ours builds a derivation tree."
"Our parser only has to deal with information that is essential to solve the combinatorial problem, and not e.g. with the internal structure of the elementary trees."
"The reconstruction of the derived tree, which is cheap once the derivation tree has been computed, is delegated to a post-processing step."
"Working with derived trees,[REF_CITE]cannot ig-nore any information and have to keep track of the relationships between nodes at points where they are not relevant."
Generation from flat semantics is an NP-complete problem.
"In this paper, we have first given an al-ternative proof for this fact, which works even for a fixed grammar and makes the connection to the complexity of free word order parsing clearly visi-ble."
"Then we have shown how to translate the re-alization problem of TAG into parsing problems of topological dependency grammar, and argued how the optimizations in the dependency parser – which were originally developed for free word order pars-ing – help reduce the runtime for the generation sys-tem."
"This reduction shows in passing that the pars-ing problem for TDG is NP-complete as well, which has been conjectured, but never proved."
The NP-completeness result for the realization problem explains immediately why all existing com-plete generation algorithms have exponential run-times in the worst case.
"As our proof shows, the main sources of the combinatorics are the interac-tion of lexical ambiguity and tree configuration with the completely unordered nature of the input."
"Mod-ification is important and deserves careful treatment (and indeed, our system deals very gracefully with it), but it is not as intrinsically important as some of the literature suggests; our proof gets by without modification."
"If we allow the grammar to be part of the input, we can even modify the proof to show NP-hardness of the case where semantic atoms can be verbalized more often than they appear in the in-put, and of the case where they can be verbalized less often."
The case where every atom can be used arbitrarily often remains open.
"By using techniques from constraint program-ming, the dependency parser seems to cope rather well with the combinatorics of generation."
"Propaga-tors can rule out impossible local structures on the grounds of global information, and selection con-straints greatly alleviate the proliferation of lexical ambiguity in large TAG grammars by making shared information available without having to commit to specific lexical entries."
"Initial experiments with the XTAG grammar indicate that we can generate prac-tical examples in polynomial time, and may be com-petitive with state-of-the-art realization systems in terms of raw runtime."
"In the future, it will first of all be necessary to lift the restrictions we have placed on the TAG gram-mar:"
"So far, the nodes of the elementary trees are only equipped with nonterminal labels and indices, not with general feature structures, and we allow only a restricted form of adjunction constraints."
"It should be possible to either encode these construc-tions directly in the dependency grammar (which al-lows user-defined features too), or filter out wrong realizations in a post-processing step."
The effect of such extensions on the runtime remains to be seen.
"Finally, we expect that despite the general NP-completeness, there are restricted generation prob-lems which can be solved in polynomial time, but still contain all problems that actually arise for nat-ural language."
"The results of this paper open up a new perspective from which such restrictions can be sought, especially considering that all the natural-language examples we tried are indeed processed in polynomial time."
"Such a polynomial realiza-tion algorithm would be the ideal starting point for algorithms that compute not just any, but the best possible realization – a problem which e.g.[REF_CITE]approximate using stochastic methods."
"We are grateful to Tilman Becker, Chris Brew, Ann Copestake, Ralph Debus-mann, Gerald Penn, Stefan Thater, and our reviewers for helpful comments and discussions."
We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy.
"We cast the problem of learning the contexts for the linguistic operations as classification tasks, and apply straightforward machine learning techniques, such as decision tree learning."
The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system.
The target features are extracted from links to surface syntax trees.
"Our evidence consists of four examples from the German sentence realization system code-named Amalgam: case assignment, assignment of verb position features, extraposition, and syntactic aggregation"
"The last stage of natural language generation, sentence realization, creates the surface string from an abstract (typically semantic) representation."
"This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output."
"Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques."
"Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure."
Nitrogen ([REF_CITE]1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness).
"This set of candidate surface strings, represented as a word lattice, is then rescored by a word-bigram language model, to produce the best-ranked output sentence."
"FERGUS[REF_CITE], on the other hand, employs a model of syntactic structure during sentence realization."
"In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system."
This tree-based model chooses a best-ranked XTAG representation for a given dependency structure.
"Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen."
"In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module)[REF_CITE]employs a series of linguistic operations which map a semantic representation to a surface syntactic tree via intermediate syntactic representations."
The contexts for most of these operations in Amalgam are machine learned.
The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read.
The goal of this paper is to show that it is possible to learn accurately the contexts for linguistically complex operations in sentence realization.
We propose that learning the contexts for the application of these linguistic operations can be viewed as per-operation classification problems.
This approach combines advantages of a linguistically informed approach to sentence realization with the advantages of a machine learning approach.
"The linguistically informed approach allows us to deal with complex linguistic phenomena, while machine learning automates the discovery of contexts that are linguistically relevant and relevant for the domain of the data."
The machine learning approach also facilitates adaptation of the system to a new domain or language.
"Furthermore, the quantitative nature of the machine learned models permits finer distinctions and ranking among possible solutions."
"To substantiate our claim, we provide four examples from Amalgam: assignment of case, assignment of verb position features, extraposition, and syntactic aggregation."
Amalgam takes as its input a sentence-level semantic graph representation with fixed lexical choices for content words (the logical form graph of the NLPWin system – see[REF_CITE]).
"This representation is first degraphed into a tree, and then gradually augmented by the insertion of function words, assignment of case and verb position features, syntactic labels, etc., and transformed into a syntactic surface tree."
"A generative statistical language model establishes linear order in the surface tree (Ringger et al., in preparation), and a surface string is generated from the leaf nodes."
Amalgam consists of eight stages.
We label these ML (machine-learned context) or RB (rule-based).
Stage 1 Pre-processing (RB): ¾ degraphing of the semantic representation ¾ retrieval of lexical information
Stage 2 Flesh-out (ML): ¾ assignment of syntactic labels ¾ insertion of function words ¾ assignment of case and verb position features
"Stage 3 Conversion to syntactic tree (RB): ¾ introduction of syntactic representation for coordination ¾ splitting of separable prefix verbs based on both lexical information and previously assigned verb position features ¾ reversal of heads (e.g., in quantitative expressions) (ML)"
"Stage 4 Movement: ¾ extraposition (ML) ¾ raising, wh movement (RB)"
Stage 5 Ordering (ML): ¾ ordering of constituents and leaf nodes in the tree
Stage 6 Surface cleanup (ML): ¾ lexical choice of determiners and relative pronouns ¾ syntactic aggregation
Stage 7 Punctuation (ML)
Stage 8 Inflectional generation (RB)
"All machine learned components, with the exception of the generative language model for ordering of constituents (stage 5), are decision tree classifiers built with the WinMine toolkit ([REF_CITE]; Chickering, nd.)."
There are a total of eighteen decision tree classifiers in the system.
The complexity of the decision trees varies with the complexity of the modeled task.
The number of branching nodes in the decision tree models in Amalgam ranges from 3 to 447.
"The data for all of the models were drawn from a set of 100,000 sentences from technical software manuals and help files."
"The sentences are analyzed by the NLPWin system, which provides a syntactic and logical form analysis."
"Nodes in the logical form representation are linked to the corresponding syntactic nodes, allowing us to learn contexts for the mapping from the semantic representation to a surface syntax tree."
The data is split 70/30 for training versus model parameter tuning.
For each set of data we built decision trees at several different levels of granularity (by manipulating the prior probability of tree structures to favor simpler structures) and selected the model with the maximal accuracy as determined on the parameter tuning set.
"All models are then tested on data extracted from a separate blind set of 10,000 sentences from the same domain."
"For both training and test, we only extract features from sentences that have received a complete, spanning parse: 85.14% of the sentences in the training and parameter tuning set, and 84.59% in the blind test set fall into that category."
Most sentences yield more than one training case.
We attempt to standardize as much as possible the set of features to be extracted.
"We exploit the full set of features and attributes available in the analysis, instead of pre-determining a small set of potentially relevant features[REF_CITE]."
This allows us to share the majority of code between the individual feature extraction tasks.
"More importantly, it enables us to discover new linguistically interesting and/or domain-specific generalizations from the data."
"Typically, we extract the full set of available analysis features of the node under investigation, its parent and its grandparent, with the only restriction being that these features need to be available at the stage where the model is consulted at generation run-time."
This provides us with a sufficiently large structural context for the operations.
"In addition, for some of the models we add a small set of features that we believe to be important for the task at hand, and that cannot easily be expressed as a combination of analysis features/attributes on constituents."
"Most features, such as lexical subcategorization features and semantic features such as [Definite] are binary."
"Other features, such as syntactic label or semantic relation, have as many as 25 values."
Training time on a standard 500MHz PC ranges from one hour to six hours.
"In German sentence realization, proper assignment of morphological case is essential for fluent and comprehensible output."
"German is a language with fairly free constituent order, and the identification of functional roles, such as subject versus object, is not determined by position in the sentence, as in English, but by morphological marking of one of the four cases: nominative, accusative, genitive or dative."
"In Amalgam, case assignment is one of the last steps in the Flesh-out stage (stage 2)."
"Morphological realization of case can be ambiguous in German (for example, a feminine singular NP is ambiguous between accusative and nominative case)."
"Since the morphological realization of case depends on the gender, number and morphological paradigm of a given NP, we chose to only consider NP nodes with unambiguous case as training data for the model [Footnote_1] ."
"1 Ideally, we should train the case assignment model on a corpus that is hand-disambiguated for case. In the absence of such a corpus, though, we believe that our approach is linguistically justified. The case of an NP depends solely on the syntactic context it appears in."
"As the target feature for this model is morphological case, it has four possible values for the four cases in German."
"For each data point, a total of 712 features was extracted."
"The selected features fall into the following categories: • syntactic label of the node, its parent and grandparent • lemma (i.e., citation form) of the parent, and lemma of the governing preposition • subcategorization information, including case governing properties of governing preposition and parent • semantic relation of the node itself to its parent, of the parent to its grandparent, and of the grandparent to its great- grandparent • number information on the parent and grandparent • tense and mood on the parent and grandparent • definiteness on the node, its parent and grandparent • the presence of various semantic dependents such as subject, direct and indirect objects, operators, attributive adjuncts and unspecified modifiers on the node and its parent and grandparent • quantification, negation, coordination on the node, the parent and grandparent • part of speech of the node, the parent and the grandparent • miscellaneous semantic features on the node itself and the parent"
"The decision tree model for case assignment has 226 branching nodes, making it one of the most complex models in Amalgam."
"For each nominal node in the 10,000 sentence test set, we compared the prediction of the model to the morphological case compatible with that node."
"The previously mentioned example of a singular feminine NP, for example, would yield a “correct“ if the model had predicted nominative or accusative (because thecase NP is morphologically ambiguous between accusative and nominative), and it would yield an “incorrect“ if the model had predicted genitive or dative."
This particular evaluation setup was a necessary compromise because of the absence of a hand-annotated corpus with disambiguated case in our domain.
"The caveat here is that downstream models in the Amalgam pipeline that pick up on case as one of their features rely on the absolute accuracy of the assigned case, not the relative accuracy with respect to morphological ambiguity."
Accuracy numbers for each of the four case assignments are given in Table 1.
"Note that it is impossible to give precision/recall numbers, without a hand-disambiguated test set."
The baseline for this task is 0.7049 (accuracy if the most frequent case (nominative) had been assigned to all NPs).
One of the most striking properties of German is the distributional pattern of verbs in main and subordinate clauses.
Most descriptive accounts of German syntax are based on a topology of the German sentence that treats the position of the verb as the fixed frame around which other syntactic constituents are organized in relatively free order (cf.
"The position of the verb in German is non-negotiable; errors in the positioning of the verb result in gibberish, whereas most permutations of other constituents only result in less fluent output."
"Depending on the position of the finite verb, German sentences and verb phrases are classified as being “verb-initial”, “verb-second” or “verb- final”."
"In verb-initial clauses (e.g., in imperatives), the finite verb is in initial position."
"Verb-second sentences contain one constituent preceding the finite verb, in the so-called “pre-field”."
"The finite verb is followed by any number of constituents in the “middle-field”, and any non-finite verbs are positioned at the right periphery of the clause, possibly followed by extraposed material or complement clauses (the “post-field”)."
"Verb-final clauses contain no verbal element in the verb-second position: all verbs are clustered at the right periphery, preceded by any number of constituents and followed only by complement clauses and extraposed material."
"During the Flesh-out stage in Amalgam, a decision tree classifier is consulted to make a classification decision among the four verb positions: “verb-initial”, “verb-second”, “verb-final”, and “undefined”."
"The value “undefined” for the target feature of verb position is extracted for those verbal constituents where the local syntactic context is too limited to make a clear distinction between initial, second, or final position of the verb."
"The number of “undefined” verb positions is small compared to the number of clearly established verb positions: in the test set, there were only 690 observed cases of “undefined” verb position out of a total of 15,492 data points."
"At runtime in Amalgam, verb position features are assigned based on the classification provided by the decision tree model."
"For each data point, 713 features were extracted."
"Of those features, 41 were selected by the decision tree algorithm."
"The selected features fall into the following categories: • syntactic label of the node and the parent • subcategorization features • semantic relations of the node to its parent and of the parent node to its parent • tense and mood features • presence of empty, uncontrolled subject • semantic features on the node and the parent"
The decision tree model for verb position has 115 branching nodes.
"Precision, recall and F- measure for the model are given in Table 2."
"As a point of reference for the verb position classifier, assigning the most frequent value (second) of the target feature yields a baseline score of 0.4240."
In both German and English it is possible to extrapose clausal material to the right periphery of the sentence (extraposed clauses underlined in the examples below):
Relative clause extraposition:
English: A man just left who had come to ask a question.
"German: Der Mann ist gerade weggegangen, der gekommen war, um eine Frage zu stellen."
Infinitival clause extraposition:
English: A decision was made to leave the country.
"German: Eine Entscheidung wurde getroffen, das Land zu verlassen."
Complement clause extraposition: English: A rumour has been circulating that he is ill.
"German: Ein Gerücht ging um, dass er krank ist."
Extraposition is not obligatory like other types of movement (such as Wh-movement).
"Both extraposed and non-extraposed versions of a sentence are acceptable, with varying degrees of fluency."
The interesting difference between English and German is the frequency of this phenomenon.
"While it can easily be argued that English sentence realization may ignore extraposition and still result in very fluent output, the fluency of sentence realization for German will suffer much more from the lack of a good extraposition mechanism."
We profiled data from various domains[REF_CITE]to substantiate this linguistic claim (see[REF_CITE]for similar results).
"In the technical domain, more than one third of German relative clauses are extraposed, as compared to a meagre 0.22% of English relative clauses."
"In encyclopaedia text (Microsoft Encarta), approximately every fifth German relative clause is extraposed, compared to only 0.3% of English relative clauses."
"For complement clauses and infinitival clauses, the differences are not as striking, but still significant: in the technical and encyclopaedia domains, extraposition of infinitival and complement clauses in German ranges from 1.5% to 3.2%, whereas English only shows a range from 0% to 0.53%."
We chose to model extraposition as an iterative movement process from the original attachment site to the next higher node in the tree (for an alternative one-step solution and a comparison of the two approaches see[REF_CITE]).
The target feature of the model is the answer to the yes/no question “Should the clause move from node X to the parent of node X?”.
"The tendency of a clause to be extraposed depends on properties of both the clause itself (e.g., some notion of “heaviness”) and the current attachment site."
Very coarse linguistic generalizations are that a relative clause tends to be extraposed if it is sufficiently “heavy” and if it is followed by verbal material in the same clause.
"Feature extraction for this model reflects that fact by taking into consideration features on the extraposition candidate, the current attachment site, and potential next higher landing site."
This results in a total of 1168 features.
"Each node in the parent chain of an extraposable clause, up to the actual attachment node, constitutes a single data point"
"During the decision tree building process, 60 features were selected as predictive."
They can be classified as follows:
General feature: • overall sentence length
"Features on the extraposable clause: • presence of verb-final and verb-second ancestor nodes • “heaviness” both in number of characters and number of tokens • various linguistic features in the local context (parent node and grandparent node): number and person, definiteness, voice, mood, transitivity, presence of logical subject and object, presence of certain semantic attributes, coordination, prepositional relations • syntactic label • presence of modal verbs • prepositional relations • transitivity"
"Features on the attachment site • presence of logical subject • status of the parent and grandparent as a separable prefix verb • voice and presence of modal verbs on the parent and grandparent • presence of arguments and transitivity features on the parent and grandparent • number, person and definiteness; the same on parent and grandparent • syntactic label; the same on the parent and grandparent • verb position; the same on the parent • prepositional relation on parent and grandparent • semantic relation that parent and grandparent have to their respective parent node"
"During testing of the extraposition model, the model was consulted for each extraposable clause to find the highest node to which that clause could be extraposed."
"In other words, the target node for extraposition is the highest node in the parent chain for which the answer to the classification task “Should the clause move from node X to the parent of node X?” is “yes” with no interceding “no” answer."
The prediction of the model was compared with the actual observed attachment site of the extraposable clause to yield the accuracy figures shown in Table 3.
The model has 116 branching nodes.
The baseline for this task is calculated by applying the most frequent value for the target feature (“don&apos;t move”) to all nodes.
The baseline for extraposition of infinitival and complement clauses is very high.
"The number of extraposed clauses of both types in the test set (fifteen extraposed infinitival clauses and twelve extraposed complement clauses) is very small, so it comes as no surprise that the model accuracy ranges around the baseline for these two types of extraposed clauses."
Any sentence realization component that generates from an abstract semantic representation and strives to produce fluent output beyond simple templates will have to deal with coordination and the problem of duplicated material in coordination.
This is generally viewed as a sub-area of aggregation in the generation literature[REF_CITE].
"In Amalgam, the approach we take is strictly intra-sentential, along the lines of what has been called conjunction reduction in the linguistic literature[REF_CITE]."
"While this may seem a fairly straightforward task compared to inter-sentential, semantic and lexical aggregation, it should be noted that the cross-linguistic complexity of the phenomenon makes it much less trivial than a first glance at English would suggest."
"In German, for example, position of the verb in the coordinated VPs plays an important role in determining which duplicated constituent can be omitted."
The target feature for the classification task is formulated as follows: “In which coordinated constituent is the duplicated constituent to be realized?”.
"There are three values for the target feature: “first”, “last”, and “middle”."
"The third value (“middle”) is a default value for cases where neither the first, nor the last coordinated constituent can be identified as the location for the realization of duplicated constituents."
"At generation runtime, multiple realizations of a constituent in coordination are collected and the aggregation model is consulted to decide on the optimal position in which to realize that constituent."
"The constituent in that position is retained, while all other duplicates are removed from the tree."
A total of 714 features were extracted for the syntactic aggregation model.
Each instance of coordination which exhibits duplicated material at the semantic level without corresponding duplication at the syntactic level constitutes a data point.
"Of these features, 15 were selected as predictive in the process of building the decision tree model: • syntactic label and syntactic label of the parent node • semantic relation to the parent of the duplicated node, its parent and grandparent • part of speech of the duplicated node • verb position across the coordinated node • position of the duplicated node in premodifiers or postmodifiers of the parent • coordination of the duplicated node and the grandparent of the duplicated node • status of parent and grandparent as a proposition • number feature on the parent • transitivity and presence of a direct object on the parent"
The syntactic aggregation model has 21 branching nodes.
"Precision, recall and F-measure for the model are given in Table 4."
"As was to be expected on the basis of linguistic intuition, the value “middle” for the target feature did not play any role."
In the test set there were only 2 observed instances of that value.
The baseline for this task is 0.8566 (assuming “first” as the default value).
We have demonstrated on the basis of four examples that it is possible to learn the contexts for complex linguistic operations in sentence realization with high accuracy.
"We proposed to standardize most of the feature extraction for the machine learning tasks to all available linguistic features on the node, and its parent and grandparent node."
This generalized set of features allows us to rapidly train on new sets of data and to experiment with new machine learning tasks.
"Furthermore, it prevents us from focusing on a small set of hand-selected features for a given phenomenon; hence, it allows us to learn new (and unexpected) generalizations from new data."
"We have found decision trees to be useful for our classification problems, but other classifiers are certainly applicable."
Decision trees provided an easily accessible inventory of the selected features and some indication of their relative importance in predicting the target features in question.
"Although our exposition has focused on the preferred value (the mode) predicted by the models, decision trees built by WinMine predict a probability distribution over all possible target values."
"For a system such as Amalgam, built as a pipeline of stages, this point is critical, since finding the best final hypothesis requires the consideration of multiple hypotheses and the concomitant combination of probabilities assigned by the various models in the pipeline to all possible target values."
"For example, our extraposition model presented above depends upon the value of the verb-position feature, which is predicted upstream in the pipeline."
"Currently, we greedily pursue the best hypothesis, which includes only the mode of the verb-position model’s prediction."
"However, work in progress involves a search that constructs multiple hypotheses incorporating each of the predictions of the verb-position model and their scores, and likewise for all other models."
We have found the combination of knowledge-engineered linguistic operations with machine-learned contexts to be advantageous.
"The knowledge-engineered choice of linguistic operations, allows us to deal with complex linguistic phenomena."
"Machine learning, on the other hand, automates the discovery of general and domain-specific contexts."
This facilitates adaptation of the system to a new domain or even to a new language.
It should also be noted that none of the learned models can be easily replaced by a rule.
"While case assignment, for example, depends to a high degree on the lexical properties of the governing preposition or governing verb, other factors such as semantic relations, etc., play a significant role, so that any rule approaching the accuracy of the model would have to be quite complex."
"We are currently adapting Amalgam to the task of French sentence realization, as a test of the linguistic generality of the system."
Initial results are encouraging.
It appears that much of the feature extraction and many of the linguistic operations are reusable.
 ? 76&apos; ?  $$% ? £$¦¢?§¨¤ ©$³T$ ) $© $£ £¡x¤­ª«®¤­£$))§£{$?$)ªa») ?$$~« ª)) £ ?  %µ6ºx£ ª«]~« )Ém7¢?¥v£$%¢?$¡¤$µ½ªm¥?°6§¨7®¢V½¡) ©  $£ ªa¢ª«ÀÊ§¨ª´aº»?$ ?¥Ë¿ÈÌaÃ7£$? 9ºx£{ª«M¹)¨ªa°?{ $$)  ?º§°?$ ªa¢»®[$£ ?$) ~« ª)??¥ $?$£ ½ª« ¢6£Â¤ ° ) §? ?  $$$&gt;$?)  £ ? £¡ª1ªa§3°?~ª ¢?¡³ Í9¢?¥?$? Î Ï
In this paper we explore the power of surface text patterns for open-domain question answering systems.
"In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically."
A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.
Patterns are then automatically extracted from the returned documents and standardized.
"We calculate the precision of each pattern, and the average precision for each question type."
These patterns are then applied to find answers to new questions.
"Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web."
Most of the recent open domain question-answering systems use external knowledge and tools for answer pinpointing.
"These may include named entity taggers, WordNet, parsers, hand-tagged corpora, and ontology lists[REF_CITE]."
"However, at the recent TREC-10 QA evaluati[REF_CITE], the winning system used just one resource: a fairly extensive list of surface patterns[REF_CITE]."
The apparent power of such patterns surprised many.
We therefore decided to investigate their potential by acquiring patterns automatically and to measure their accuracy.
It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases[REF_CITE].
"For example, for BIRTHDATEs (with questions like “When was X born?”), typical answers are “Mozart was born in 1756.” “Gandhi (1869–1948)…”"
"These examples suggest that phrases like “&lt;NAME&gt; was born in &lt;BIRTHDATE&gt;” “&lt;NAME&gt; (&lt;BIRTHDATE&gt;–” when formulated as regular expressions, can be used to locate the correct answer."
"In this paper we present an approach for automatically learning such regular expressions (along with determining their precision) from the web, for given types of questions."
Our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of QA pairs.
Similar techniques have been investigated extensively in the field of information extracti[REF_CITE].
"These techniques are greatly aided by the fact that there is no need to hand-tag a corpus, while the abundance of data on the web makes it easier to determine reliable statistical estimates."
Our system assumes each sentence to be a simple sequence of words and searches for repeated word orderings as evidence for useful answer phrases.
We use suffix trees for extracting substrings of optimal length.
We borrow the idea of suffix trees from computational biology[REF_CITE]where it is primarily used for detecting DNA sequences.
"Suffix trees can be processed in time linear on the size of the corpus and, more importantly, they do not restrict the length of substrings."
We then test the patterns learned by our system on new unseen questions from the TREC-10 set and evaluate their results to determine the precision of the patterns.
We describe the pattern-learning algorithm with an example.
A table of patterns is constructed for each individual question type by the following procedure (Algorithm 1). 1. Select an example for a given question type.
Thus for BIRTHYEAR questions we select “[REF_CITE]” (we refer to “Mozart” as the question term and “1756” as the answer term). 2. Submit the question and the answer term as queries to a search engine.
"Thus, we give the query +“Mozart” +“1756” to AltaVista[URL_CITE]3. Download the top 1000 web documents provided by the search engine. 4. Apply a sentence breaker to the documents. 5. Retain only those sentences that contain both the question and the answer term."
"Tokenize the input text, smooth variations in white space characters, and remove html and other extraneous tags, to allow simple regular expression matching tools such as egrep to be used. 6."
Pass each retained sentence through a suffix tree constructor.
"This finds all substrings, of all lengths, along with their counts."
"For example consider the sentences “The great composer Mozart (1756–1791) achieved fame at a young age” “Mozart (1756–1791) was a genius”, and “The whole world would always be indebted to the great music of Mozart (1756–1791)”."
"The longest matching substring for all 3 sentences is “Mozart (1756–1791)”, which the suffix tree would extract as one of the outputs along with the score of 3. 7. Pass each phrase in the suffix tree through a filter to retain only those phrases that contain both the question and the answer term."
"For the example, we extract only those phrases from the suffix tree that contain the words “Mozart” and “1756”. 8. Replace the word for the question term by the tag “&lt;NAME&gt;” and the word for the answer term by the term “&lt;ANSWER&gt;”."
This procedure is repeated for different examples of the same question type.
"For BIRTHDATE we also use “[REF_CITE]”, “[REF_CITE]”, etc."
"For BIRTHDATE, the above steps produce the following output: a. born in &lt;ANSWER&gt; , &lt;NAME&gt; b. &lt;NAME&gt; was born on &lt;ANSWER&gt; , c. &lt;NAME&gt; ( &lt;ANSWER&gt; - d. &lt;NAME&gt; ( &lt;ANSWER - ) ..."
These are some of the most common substrings of the extracted sentences that contain both &lt;NAME&gt; and &lt;ANSWER&gt;.
"Since the suffix tree records all substrings, partly overlapping strings such as c and d are separately saved, which allows us to obtain separate ofcounts their occurrence frequencies."
"As will be seen later, this allows us to differentiate patterns such as d (which records a still living person, and is quite precise) from its more general substring c (which is less precise)."
Algorithm 2: Calculating the precision of each pattern. 1.
"Query the search engine by using only the question term (in the example, only “Mozart”). 2. Download the top 1000 web documents provided by the search engine. 3."
"As before, segment these documents into individual sentences. 4. Retain only those sentences that contain the question term. 5."
"For each pattern obtained from Algorithm 1, check the presence of each pattern in the sentence obtained from above for two instances: i)"
Presence of the pattern with &lt;ANSWER&gt; tag matched by any word. ii) Presence of the pattern in the sentence with &lt;ANSWER&gt; tag matched by the correct answer term.
"In our example, for the pattern “&lt;NAME&gt; was born in &lt;ANSWER&gt;” we check the presence of the following strings in the answer sentence i)"
Mozart was born in &lt;ANY_WORD&gt; ii) Mozart was born in 1756
Calculate the precision of each pattern by the formula P = C a / C o where
C a = total number of patterns with the answer term present
C o = total number of patterns present with answer term replaced by any word 6. Retain only the patterns matching a sufficient number of examples (we choose the number of examples &gt; 5).
"We obtain a table of regular expression patterns for a given question type, along with the precision of each pattern."
This precision is the probability of each pattern containing the answer and follows directly from the principle of maximum likelihood estimation.
"For BIRTHDATE the following table is obtained: 1.0 &lt;NAME&gt;( &lt;ANSWER&gt; - ) 0.85 &lt;NAME&gt; was born on &lt;ANSWER&gt;, 0.6 &lt;NAME&gt; was born in &lt;ANSWER&gt; 0.59 &lt;NAME&gt; was born &lt;ANSWER&gt; 0.53 &lt;ANSWER&gt; &lt;NAME&gt; was born 0.50 – &lt;NAME&gt; ( &lt;ANSWER&gt; 0.36 &lt;NAME&gt; ( &lt;ANSWER&gt; -"
For a given question type a good range of patterns was obtained by giving the system as few as 10 examples.
The rather long list of patterns obtained would have been very difficult for any human to come up with manually.
The question term could appear in the documents obtained from the web in various ways.
"Thus “Mozart” could be written as “Wolfgang Amadeus Mozart”, “Mozart, Wolfgang Amadeus”, “Amadeus Mozart” or “Mozart”."
"To learn from such variations, in step 1 of Algorithm 1 we specify the various ways in which the question term could be specified in the text."
The presence of any of these names would cause it to be tagged as the original question term “Mozart”.
The same arrangement is also done for the answer term so that presence of any variant of the answer term would cause it to be treated exactly like the original answer term.
"While easy to do for BIRTHDATE, this step can be problematic for question types such as DEFINITION, which may contain various acceptable answers."
"In general the input example terms have to be carefully selected so that the questions they represent do not have a long list of possible answers, as this would affect the confidence of the precision scores for each pattern."
"All the answers need to be enlisted to ensure a high confidence in the precision score of each pattern, in the present framework."
The precision of the patterns obtained from one QA-pair example in algorithm 1 is calculated from the documents obtained in algorithm 2 for other examples of the same question type.
"In other words, the precision scores are calculated by cross-checking the patterns across various examples of the same type."
"This step proves to be very significant as it helps to eliminate dubious patterns, which may appear because the contents of two or more websites may be the same, or the same web document reappears in the search engine output for algorithms 1 and 2."
Algorithm 1 does not explicitly specify any particular question type.
Judicious choice of the QA example pair therefore allows it to be used for many question types without change.
Using the patterns to answer a new question we employ the following algorithm: 1. Determine the question type of the new question.
We use our existing QA system ([REF_CITE]; 2001) to do so. 2.
"The question term in the question is identified, also using our existing system. 3. Create a query from the question term and perform IR (by using a given answer document corpus such as the TREC-10 collection or web search otherwise). 4."
"Segment the documents obtained into sentences and smooth out white space variations and html and other tags, as before. 5. Replace the question term in each sentence by the question tag (“&lt;NAME&gt;”, in the case of BIRTHYEAR). 6."
"Using the pattern table developed for that particular question type, search for the presence of each pattern."
Select words matching the tag “&lt;ANSWER&gt;” as the answer. 7. Sort these answers by their pattern’s precision scores.
Discard duplicates (by elementary string comparisons).
Return the top 5 answers.
"From our Webclopedia QA Typology[REF_CITE]we selected 6 different question types: BIRTHDATE, LOCATION, INVENTOR, DISCOVERER, DEFINITION, WHY-FAMOUS."
The pattern table for each of these question types was constructed using Algorithm 1.
Some of the patterns obtained along with their precision are as follows
"BIRTHYEAR 1.0 &lt;NAME&gt; ( &lt;ANSWER&gt; - ) 0.85 &lt;NAME&gt; was born on &lt;ANSWER&gt; , 0.6 &lt;NAME&gt; was born in &lt;ANSWER&gt; 0.59 &lt;NAME&gt; was born &lt;ANSWER&gt; 0.53 &lt;ANSWER&gt; &lt;NAME&gt; was born 0.5 - &lt;NAME&gt; ( &lt;ANSWER&gt; 0.36 &lt;NAME&gt; ( &lt;ANSWER&gt; - 0.32 &lt;NAME&gt; ( &lt;ANSWER&gt; ) , 0.28 born in &lt;ANSWER&gt; , &lt;NAME&gt; 0.2 of &lt;NAME&gt; ( &lt;ANSWER&gt;"
"WHY-FAMOUS 1.0 &lt;ANSWER&gt; &lt;NAME&gt; called 1.0 laureate &lt;ANSWER&gt; &lt;NAME&gt; 1.0 by the &lt;ANSWER&gt; , &lt;NAME&gt; , 1.0 &lt;NAME&gt; - the &lt;ANSWER&gt; of 1.0 &lt;NAME&gt; was the &lt;ANSWER&gt; of 0.84 by the &lt;ANSWER&gt; &lt;NAME&gt; , 0.8 the famous &lt;ANSWER&gt; &lt;NAME&gt; , 0.73 the famous &lt;ANSWER&gt; &lt;NAME&gt; 0.72 &lt;ANSWER&gt; &gt; &lt;NAME&gt; 0.71 &lt;NAME&gt; is the &lt;ANSWER&gt; of"
"LOCATION 1.0 &lt;ANSWER&gt; &apos; s &lt;NAME&gt; . 1.0 regional : &lt;ANSWER&gt; : &lt;NAME&gt; 1.0 to &lt;ANSWER&gt; &apos; s &lt;NAME&gt; , 1.0 &lt;ANSWER&gt; &apos; s &lt;NAME&gt; in 1.0 in &lt;ANSWER&gt; &apos; s &lt;NAME&gt; ,"
"For each question type, we extracted the corresponding questions from the TREC-10 set."
These questions were run through the testing phase of the algorithm.
Two sets of experiments were performed.
"In the first case, the TREC corpus was used as the input source and IR was performed by the IR component of our QA system[REF_CITE]."
"In the second case, the web was the input source and the IR was performed by the AltaVista search engine."
"Results of the experiments, measured by Mean Reciprocal Rank (MRR) score[REF_CITE], are:"
The results indicate that the system performs better on the Web data than on the TREC corpus.
The abundance of data on the web makes it easier for the system to locate answers with high precision scores (the system finds many examples of correct answers among the top 20 when using the Web as the input source).
A similar result for QA was obtained[REF_CITE].
The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.
The WHY-FAMOUS question type is an exception and may be due to the fact that the system was tested on a small number of questions.
No external knowledge has been added to these patterns.
"We frequently observe the need for matching part of speech and/or semantic types, however."
"For example, the question: “Where are the Rocky Mountains located?” is answered by “Denver’s new airport, topped with white fiberglass cones in imitation of the Rocky Mountains in the background, continues to lie empty”, because the system picked the answer “the background” using the pattern “the &lt;NAME&gt; in &lt;ANSWER&gt;,”."
Using a named entity tagger and/or an ontology would enable the system to use the knowledge that “background” is not a location.
DEFINITION questions pose a related problem.
"Frequently the system’s patterns match a term that is too general, though correct technically."
"For “what is nepotism?” the pattern “&lt;ANSWER&gt;, &lt;NAME&gt;” matches “…in the form of widespread bureaucratic abuses: graft, nepotism…”; for “what is sonar?” the pattern “&lt;NAME&gt; and related &lt;ANSWER&gt;s” matches “…while its sonar and related underseas systems are built…”."
The patterns cannot handle long-distance dependencies.
"For example, for “Where is London?” the system cannot locate the answer in “London, which has one of the most busiest airports in the world, lies on the banks of the river Thames” due to the explosive danger of unrestricted wildcard matching, as would be required in the pattern “&lt;QUESTION&gt;, (&lt;any_word&gt;)*, lies on &lt;ANSWER&gt;”."
This is one of the reasons why the system performs very well on certain types of questions from the web but performs poorly with documents obtained from the TREC corpus.
The abundance and variation of data on the Internet allows the system to find an instance of its patterns without losing answers to long-term dependencies.
"The TREC corpus, on the other hand, typically contains fewer candidate answers for a given question and many of the answers present may match only long-term dependency patterns."
More information needs to be added to the text patterns regarding the length of the answer phrase to be expected.
The system searches in the range of 50 bytes of the answer phrase to capture the pattern.
It fails to perform under certain conditions as exemplified by the question “When was Lyndon B. Johnson born?”.
"The system selects the sentence “Tower gained national attention in 1960 when he lost to democratic Sen. Lyndon B. Johnson, who ran for both re-election and the vice presidency” using the pattern “&lt;NAME&gt; &lt;ANSWER&gt; –“."
The system lacks the information that the &lt;ANSWER&gt; tag should be replaced exactly by one word.
"Simple extensions could be made to the system so that instead of searching in the range of 50 bytes for the answer phrase it could search for the answer in the range of 1–2 chunks (basic phrases in English such as simple NP, VP, PP, etc.)."
A more serious limitation is that the present framework can handle only one anchor point (the question term) in the candidate answer sentence.
"It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other."
"For example, in “Which county does the city of Long Beach lie?”, the answer “Long Beach is situated in Los Angeles County” requires the pattern. “&lt;QUESTION_TERM_1&gt; situated in &lt;ANSWER&gt; &lt;QUESTION_TERM_2&gt;”, where &lt;QUESTION_TERM_1&gt; and &lt;QUESTION_TERM_2&gt; represent the terms “Long Beach” and “county” respectively."
"The performance of the system depends significantly on there being only one anchor word, which allows a single word match between the question and the candidate answer sentence."
The presence of multiple anchor words would help to eliminate many of the candidate answers by simply using the condition that all the anchor words from the question must be present in the candidate answer sentence.
The system does not classify or make any distinction between upper and lower case letters.
"For example, “What is micron?” is answered by “In Boise, Idaho, a spokesman for Micron, a maker of semiconductors, said Simms are ‘ a very high volume product for us …’ ”."
The answer returned by the system would have been perfect if the word “micron” had been capitalized in the question.
Canonicalization of words is also an issue.
"While giving examples in the bootstrapping procedure, say, for BIRTHDATE questions, the answer term could be written in many ways (for example, Gandhi’s birth date can be written as “1869”, “Oct. 2, 1869”, “2nd[REF_CITE]”, “[REF_CITE]”, and so on)."
Instead of enlisting all the possibilities a date tagger could be used to cluster all the variations and tag them with the same term.
"The same idea could also be extended for smoothing out the variations in the question term for names of persons (Gandhi could be written as “Mahatma Gandhi”, “Mohandas Karamchand Gandhi”, etc.)."
The web results easily outperform the TREC results.
This suggests that there is a need to integrate the outputs of the Web and the TREC corpus.
"Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers."
This would work well for question types like BIRTHDATE or LOCATION but is not clear for question types like DEFINITION.
The simplicity of this method makes it perfect for multilingual QA.
"Many tools required by sophisticated QA systems (named entity taggers, parsers, ontologies, etc.) are language specific and require significant effort to adapt to a new language."
"Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched."
"In Optimality-Theoretic Syntax, optimiza-tion with unrestricted expressive power on the side of the OT constraints is unde-cidable."
This paper provides a proof for the decidability of optimization based on constraints expressed with reference to lo-cal subtrees (which is in the spirit of OT theory).
The proof builds on Kaplan and Wedekind’s (2000) construction showing that LFG generation produces context-free languages.
"Optimality-Theoretic (OT) grammar systems are an interesting alternative to classical formal grammars, as they construe the task of learning from data in a meaning-based way: a form is defined as gram-matical if it is optimal (most harmonic) within a set of generation alternatives for an underlying logical form."
"The harmony of a candidate analysis depends on a language-specific ranking ( ) of violable con-straints, thus the learning task amounts to adjusting the ranking over a given set of constraints. (1) Candidate  is more harmonic than  iff it incurs fewer violations of the highest-ranking constraint in which  and  differ."
"The comparison-based setup of OT learning is closely related to discriminative learning approaches in probabilistic parsing[REF_CITE], [Footnote_1] however the comparison of generation alternatives – rather than parsing alternatives – adds the possibility of system-atically learning the basic language-specific gram-matical principles (which in probabilistic parsing are typically fixed a priori, using either a treebank-derived or a manually written grammar for the given language)."
1 This is for instance pointed out[REF_CITE].
The “base grammar” assumed as given can be highly unrestricted in the OT setup.
"Using a linguistically motivated set of constraints, learning proceeds with a bias for unmarked linguistic struc-tures (cf. e.g.,[REF_CITE])."
"For computational OT syntax, an interleaving of candidate generation and constraint checking has been proposed[REF_CITE]."
"But the decidability of the optimization task in OT syntax, i.e., the iden-tification of the optimal candidate(s) in a potentially infinite candidate set, has not been proven yet. [Footnote_2]"
"2 Most computational OT work so far focuses on candidates and constraints expressible as regular languages/rational rela-tions, based[REF_CITE](e.g., ([REF_CITE]; Gerdemann and van[REF_CITE]))."
"Assume that the candidate set is characterized by a context-free grammar (cfg)  , plus one addi-tional candidate ‘yes’."
There are two constraints ( ): is violated if the candidate is neither ‘yes’ nor a structure generated by a cfg ; is vi-olated only by ‘yes’.
"Now, ‘yes’ is in the language defined by this system iff there are no structures in  that are also in ."
"But the emptiness problem for the intersection of two context-free languages is known to be undecidable, so the optimization task for unrestricted OT is undecidable too. [Footnote_3]"
"3 Cf. also[REF_CITE]for the sketch of an undecidability argument and ([REF_CITE]4.2, 6.3) for further constructions."
"However, it is not in the spirit of OT to have extremely powerful individual constraints; the ex-planatory power should rather arise from interaction of simple constraints."
"Following[REF_CITE], we define a restricted OT system based on Lexical-Functional Grammar (LFG) represen-tations: c(ategory) structure/f(unctional) structure pairs like (4),(5) ."
Each c-structure tree node is mapped to a node in the f-structure graph by the function .
"The mapping is specified by f-annotations in the grammar rules (below category symbols, cf. (2)) and lexicon entries (3). [Footnote_4]"
"4 abbreviates /1. - , i.e., the present category’s - image; abbreviates - , i.e., the f-structure corresponding to the present node’s mother category."
The correct f-structure for a sentence is the min-imal model satisfying all properly instantiated f-annotations.
"In OT-LFG, the universe of possible candidates is defined by an[REF_CITE]:&lt;9 ; (encoding inviolable principles, like an X-bar scheme)."
"A particular can-didate set is the set Gen ?= &gt; @&gt;[REF_CITE]HG – i.e., the c-/f-structure pairs in I2+476[REF_CITE]&lt;; , which have the input +2 4 as their f-structure."
Constraints are expressed as lo-cal configurations in the c-/f-structure pairs.
They have one of the following implicational forms: [Footnote_5] (6) JKML JK where  are descriptions of nonterminals of Q &gt; @FARSBDC&gt; ; K K N are standard LFG f-annotations of constraining equations with as the only f-structure metavariable. (7) L J
5 Note that with GPSG-style category-level feature percola-tion it is possible to refer to (finitely many) nonlocal configura-tions at the local tree level.
"W U K where  U N U are descriptions of nonterminals of Q &gt; @ &gt; BDC ; JONPJ refer to the mother in a local subtree configuration, U N U refer to the same daughter cate-gory; K T NKT N W N  are regular expressions over nontermi-nals; N are standard f-annotations as in (6)."
"Any of the descriptions can be maximally unspe-cific; (6) can for example be instantiated by the O P S PEC constraint ( Y OP )=+ Z ( DF Y ) (an operator must be the value of a discourse function,[REF_CITE]) with the category information unspecified."
"An OT-LFG system [ is thus characterized by a base grammar and a set of constraints, with a language-specific ranking relation ]\ : ^  &gt ; @&gt; BDC &lt;N _NX`bacP ."
"The evaluation function Eval +d egf h aHi picks the most harmonic from a set of candidates, based on the con-straints and ranking."
The language (set of analyses) [Footnote_6] generated by an OT system is defined as j ^ lk _or  &gt;FAR@ &gt;SBDC sPt q &gt;vu@   o Nwq o
6 The string language is obtained by taking the terminal string of the c-structure part of the analyses.
Eval xzy7{| } ~ Gen H:: q &gt;@
"Our decidability proof for generation-based op-timization builds on the result[REF_CITE](K&amp;W00) that LFG generation produces context-free languages. (8) Given an arbitrary LFG grammar Q and a cycle-free f-structure q , a cfg Q can be constructed that generates"
I will refer to the resulting cfg exactly the strings to which Q assigns the  f-structure q . as G .
"E K&amp;W00 present a constructive proof, folding all f-structural contributions of lexical entries and LFG rules into the c-structural rewrite rules (which is possible since we know in advance the range of f-structural objects that can instantiate the f-structure meta-variables in the rules)."
I illustrate the special-ization steps with grammar (2) and lexicon (3) and for generation from f-structure (5).
"Initially, the generalized format of right-hand sides in LFG rules is converted to the standard context-free notation (resolving regular expressions by explicit disjunction or recursive rules)."
"F-structure (5) contains five substructures: the root f-structure, plus the embedded f-structures under the paths SUBJ , COMP , COMP SUBJ , and COMP OBJ ."
"Any relevant metavariable ( Y , ) in the grammar must end up instantiated to one of these."
"So for each path from the root f-structure, a distinct variable is introduced: , subscripted with the (abbreviated and possibly empty) feature path:."
Rule augmentation step 1 adds to each category name a concrete f-structure to which the category corresponds.
"So for FP, we get FP: , FP: , FP:  , FP: , and FP:  ."
"The rules are multiplied out to cover all combinations of augmented categories obeying the original f-annotations. [Footnote_7] Step 2 adds a set of instantiated f-annotation schemes to each sym-bol, based on the instantiation of metavariables from step 1."
7 VP: of V in the VP rule (2) enforces that - VP - V : .
One instance of the lexicon entry Mary look as follows:
NP:  : PRED )=‘Mary’ (9) Mary NUM )=
"The rules are again multiplied out to cover all combinations for which the set of f-constraints on the mother is the union of all daughters’ f-constraints, plus the appropriately instantiated rule-specific annotations."
"So, for   the VP rule based on the categories NP:  :"
PRED )=‘Mary’ and NUM )= SG V :  :  PRED )=‘laugh’ TNS )=
"PAST , we get the rule"
"VP:  NP:  NPV:  :  V :is excludedis allowed, since, whilethe = annotation"
With this bottom-up construction it is ensured that each new category ROOT: : . . . (corresponding to the original root symbol) contains a complete pos-sible collection of instantiated f-constraints.
To ex-clude analyses whose f-structure is not (for which we are generating strings) a new start symbol is in-troduced “above” the original root symbol.
"Only for the sets of f-constraints that have ! as their minimalmodel, rules of the form ROOT ROOT: : ... are introduced (this also excludes inconsistent f-constraint sets)."
"With the cfg G , standard techniques for E cfg’s can be applied, e.g., if there are infinitely many possible analyses for a given f-structure, the small-est one(s) can be produced, based on the pumping lemma for context-free languages."
Grammar (2) does indeed produce infinitely many analyses for the input f-structure (5).
"It overgenerates in several re-spects: The functional projection FP can be stacked due to recursions like the following (with the aug-mented % &quot;&amp; FP #% &quot;% #$&quot; # reoccuring in the F rules % &quot;$%# &quot;% &amp;&quot;# # &apos; *( &quot; #-+ is one of  the augmented  categories we get for that in (3), so ((2),(5)) generates an arbitrary number of thats on top of any FP."
"A similar repeti-tion effect will arise for the auxiliary had. [Footnote_8] Other choices in generation arise from the freedom of gen-erating the subject in the specifier of VP or FP and from the possibility of (unbounded) topicalization of the object (the first disjunction of the FP rule in (2) contains a functional-uncertainty equation): (10) a. John thought that Titanic, Mary had seen. b. Titanic, John thought that Mary had seen."
"8 The F 4 entries do not contribute any PRED value, which would exclude doubling due to the instantiated symbol charac-ter of PRED values (cf. K&amp;W00, fn. 2)."
"While grammar (2) would be considered defective as a classical LFG grammar, it constitutes a rea-sonable example of a candidate generation grammar ( 2547682:9&lt;; ) in OT."
"Here, it is the OT constraints that enforce language-specific restrictions, so b2547682:9&lt;; has to ensure that all candidates are generated in the first place."
"For instance, expletive elements as do in Who do you know will arise by passing a recursion in the cfg constructed during generation."
A candi-date containing such a vacuous cycle can still be-come the winner of the OT competition if the Faith-fulness constraint punishing expletives is outranked by some constraint favoring an aspect of the recur-sive structure.
So the harmony is increased by going through the recursion a certain number of times.
"It is for this very reason, that Who do you know is pre-dicted to be grammatical in English."
"So  , in OT-LFG it is not sufficient to apply just construction  ; I use an additional step: prior the to application of , the[REF_CITE]682:9&lt;; is converted to a different form e E 2+476[REF_CITE]&lt;; G (depend-ing on the constraint set ), which is still an LFG grammar but has category symbols which reflect lo-cal constraint violations."
"When the construc-tion is applied to e E 2547682:9&lt;;  G , all “pumping” struc-  tures generated by the cfg 2+4 GE e E +2 476[REF_CITE]&lt;; G can indeed be ignored since all OT-relevant candi-dates are already contained in the finite set of non-recursive structures."
"So, finally the ranking of the constraints is taken into consideration in order to de-termine the harmony of the candidates in this finite subset."
"Preprocessing Like K&amp;W00, I assume an initial conversion of the c-structure part of rules into stan-dard context-free form, i.e., the right-hand side is a category string rather than a regular expression."
"This ensures that for a given local subtree, each constraint (of form (6) or (7)) can be applied only a finite num-ber of times: if is the arity of the longest right-hand side of a rule, the maximal number of local viola-tions is (since some constraints of type (7) can be instantiated to all daughters)."
"Grammar conversion With the number of local vi-olations bounded, we can encode all candidate dis-tinctions with respect to constraint violations at the local-subtree level with finite means: The set of categories in the newly constructed LFG grammar e E +2 476[REF_CITE]; G is the finite set (11)    D5 :  : the set of categories in y Q &gt; @&gt; DB C k"
J :  N   N s
"J a nonterminal symbol of Q &gt; @&gt; BDC , ! the &quot; size &quot;% of $ the constraint set , # $ , the arity of the longest rhs in rules of Q &gt; @&gt; BDC "
The rules in e E 2+476[REF_CITE]; G are constructed in such a way that for each rule
"X 4 X ...X &amp; m m &amp; in 32+476[REF_CITE]&lt;; and each sequence (&apos; ) *&apos; ,) +-+-+ /&apos; .) , 021 &apos;43) 1 , all rules of the form"
"X 4 :  4  N &quot;% 4 $  4 P X :  * ...X &amp; :  &amp;  &amp; , m &amp; ! #&quot; m o &apos; )3 (the number of violations are included such 3 that of constraint incurred local to the rule) and the f-annotations . .. 6 are specified as follows:"
JK of form (6) 7 JK L  : (12) for ! mpo ( 9 &quot;;:&lt;&gt;&quot; = ) a. 4 ; m o if X 4 does ! not match the K condition J ; &quot;;D: &quot;%= b. 4 ; m m @?BA ; m o m o ( C ) ifX 4 matches !
J ; K ? mpo ( C ;&quot; D: &quot;&gt;= )
"K c. 4 ; m m ? ; m o ifX 4 matches 9 m o ( C &quot;;:D&quot;&gt;= ) both J K and J ; d. 4 ; m m @? ; m o if X 4 matches J but K ?EA not KJ ; ; m o mpo ( C F&quot; : &quot;&lt;= ) e. 4 9 ; m m ? if X 4 matches #! both J and  ; ) J L J (13) for of form (7) K W T UK W , : ! mpo ( 9 &quot;;:&lt;&quot;&gt;= ) a. 4 ; m o if X 4 does G &amp; not match the condition J ; ( 9 M&quot; :D&gt;&quot; = ), LK K K b. 4 o ; m o mpo N N o IH  where"
K K K K K m o ? if X 4 9 v. o ?BA ; m o N N J U matches both J and  ; X o matches both U and ; X ...X o match T and T ; X o ...X &amp; match W and W .
Note that the constraint profile of the daughter categories does not play any role in the determi-nation of constraint violations local to the &apos; 3 subtree under consideration (only the sequences ) are re-stricted by the conditions (12) and (13)).
"So for each new rule type, all combinations of constraint profiles on the daughters are constructed (creating a large but finite number of rules). 9"
This ensures that no sen-tence that can be parsed (or generated) by b2+476[REF_CITE]&lt;; is excluded from e E 2+476[REF_CITE]; G (as stated by fact (14)): [Footnote_10] (14) Coverage preseveration All strings generated by an LFG grammar Q are also gen-erated by y  .
"10 Providing all possible combinations of augmented category symbols on the right-hand rule sides in y ensures that the newly constructed rules can be reached from the root symbol in a derivation. It is also guaranteed that whenever a rule in Q contributes to an analysis, at least one of the rules constructed from will contribute to the corresponding analysis in y . This is ensured since the subclauses in (12) and (13) cover the full space of logical possibilities."
The original analysis can be recovered from an e E G analysis by applying a projection function Cat to all c-structure categories:
Cat J :   N   J for every category in    : :  (11)
We can overload the function name Cat with a func-tion applying to the set of analyses produced by an
LFG grammar by defining k s
"Cat  ?N8q  Nwq Q , m is derived from m by applying Cat to all category symbols  ."
Coverage preservation of the e construction holds also for the projected c-category skeleton (cf. the ar-gumentation in fn. 10): (15) C-structure level coverage preservation For an LFG grammar Q : Cat y  Q
Each category in e E G encodes the number of local violations for all constraints.
"Since all con-straints are locally evaluable by assumption, all con-straints violated by a candidate analysis have to be incurred local to some subtree."
"Hence the total number of constraint violations incurred by a can-didate can be computed by simply summing over all category-encoded local violation profiles: (16) Total number of constraint violations Let Nodes m be the multiset of categories occurring in the c-structure tree m , then the total number of viola-tions of constraint incurred by an analysis  NXq y Q &gt; 8@ A &gt; DB C G is     m x ~ B   Define"
"Total y m m  m   m  7 Applying  on e  2+476[REF_CITE]; can apply theSince e E 32+476[REF_CITE]&lt;; G is a standard LFG grammar, weconstruction to it to get a cfg for a given f-structure (&apos; +2 4 +- . +- The + *&apos; . category: : , withsymbolsand then have the form  X:arising from the construction."
We can over-load the projection function Cat again such that Cat &quot; E ! : : # : $ &amp; G % ! for all augmented category sym-bol of the new format; likewise Cat E G for a cfg.
"Since the e construction (strongly) preserves also after the application ofthe language generated, coverage  preservation holdsto e E +2 476[REF_CITE]; G and 2547682:[Footnote_9]&lt;; , respectively: (*&apos; ) (17) Cat (*&apos; )  y Q &gt;F@ ARSBDC&gt; 8NXq &gt;@"
"9 For one rule/constraint combination several new rules can antecedent ( J ) and the consequent ( J ) category descriptionresult; e.g., if the right-hand side of a rule (X 4 ) matches both the of a constraint of form (6), three clauses apply: (12b), (12c), and (12d). So, we get two new rules with the count of 0 local violations of the constraint and two rules with count 1, with a difference in the f-annotations."
"Cat Q &gt;FAR@ SBDC&gt; wN q &gt;@ P constraint violations, Cat E"
But since the symbols in  e E 2+476[REF_CITE]&lt;; G reflect  local 254 G&lt;GE e E +2 476[REF_CITE]&lt;; G has the property that all instances of recursion in the resulting cfg create candidates that are at most as harmonic as their non-recursive counterparts.
"As-suming a projection function CatCount &quot; E ! : : # : $ G % ! : , we can state more formally: (18) If m and m *&apos; are ) CatCount projections of trees produced by the cfg y Q &gt; @FARS&gt; BDC  &gt; @ , using exactly the same rules, and m contains a superset of the nodes that m  contains &quot;  , for, thenall  N  9  from  *  * and     "
"Total y m , Total y m &lt; ."
"This fact follows from definition[REF_CITE]: the violation counts in the additional nodes in will add to the total of constraint violations (and if none of the additional nodes contains any local constraint violation at all, the total will be the same as in )."
"Intuitively, the effect of the augmentation of the cat-egory  format is that certain recursions in the pureconstruction (which one may think of as a loop) are unfolded, leading to a longer loop."
The new loop is sufficiently large to make all relevant distinctions.
This result can be directly exploited in processing: if all non-recursive analyses are generated (of which there are only finitely many) it is guaranteed that a subset of the optimal candidates is among them.
"If the grammar does not contain any violation-free re-cursion, we even know that we have generated all optimal candidates. (19) A recursion with the derivation path L  L is called violation-free iff all categories dominated by the upper occurrence of , but not dominated by the lower occurrence ! of 9  have the form J u   N with N"
"Note that if there is an applicable violation-free re-cursion, the set of optimal candidates is infinite; so if the constraint set is set up properly in a linguis-tic analysis, one would assume that violation-free recursion should not arise.[REF_CITE]excludes the application of such recursions by a similar con-dition as offline parsability (which excludes vacu-ous  recursions over a string in parsing), but with theconstruction, this condition is not necessary for decidability of the generation-based optimization task."
"The cfg produced by can be transformed further to only generate the optimal candidates ac-cording to the constraint ranking \ of the OT sys-tem [ % \ &lt; , eliminating all but the 2547682:9&lt;; violation-free recursions in the grammar: (20) Creating a cfg that produces all optimal candidates a. Define ]k &apos; )  *  m y Q &gt; @ &gt; BDC  &gt; @ s m contains no recursion   .   is finite and can be easily computed, by keeping track of the rules already used in an analysis. b. Redefine Eval xzy7{ | g} ~ to apply on a set of context-free [Footnote_11] for all categories in y Q &gt; !"
11 The projection function Cat is again overloaded to also re-move the index on the categories.
"F@ AR&gt;SDB C  &gt; @ 9 of.theIntroduceform X:   N : : , where for a new unique in- 9 for ! each node of the &quot; form &quot; X:   N r : : , dex for some  9 where occurring in the analyses Eval xzy7{ | }g~   (i.e., different occurrences of the same category are distinguished). d. Construct the cfg"
Q    J    N m   
"N S    N    , where J   N m   are the indexed symbols of step c.; S    is a new *&apos; ) start symbol; the rules    are (i) those rules from y Q &gt; @FAR&gt;SBDC Nwq &gt; @ which were used in the analyses in Eval xzy7{| } ~  – with the original symbols *&apos; replaced )  by the indexed symbols –, (ii) the rules in y Q &gt; @ &gt; BDC  &gt; @ , in which the mother category and all daughter ! categories 9 are(withof thetheformnew X:  N ! * : : , for index added), and (iii) one rule S   "
S o : for each *)&apos; of the indexed versions S o : of the start symbols of y Q &gt; @FAR&gt;SBDC  &gt; @ .
"With the index introduced in step (20c), the origi-nal recursion in the cfg is eliminated in all but the violation-free cases."
"The grammar Cat E  &gt; @ G pro-duces (the c-structure of) the set of optimal candi-dates for the input +2 4 : [Footnote_12] (21) Cat k s Q   m ?N8q &gt;@ Eval xzy7{| g} ~ Gen  D:: q &gt;@  , i.e., the set of c-structures for the optimal candidates ^ for input f-structure q &gt; @ according to the OT system  &gt;FAR@ &gt;SBDC N _Nw` a P ."
"12 Like K&amp;W00, I make the assumption that the input f-structure in generation is fully specified (i.e., all the candidates have the form ?Nwq &gt; @ ), but the result can be extended to allow for the addition of a finite amount of f-structure information in generation. Then, the specified routine is computed separately for each possible f-structural extension and the results are com-pared in the end."
To prove fact (21) we will show that the c-structure of an arbitrary candidate analysis generated from 254 with 2547682:9&lt;; is contained in Cat E  &gt;@ G iff all other candidates are equally or less harmonic.
Take an arbitrary candidate c-structure gen-erated from 2 4 with 2547682:9&lt;; such that + Cat E  &gt;@ G .
We have to show that all other candi-dates generated from 254 are equally or less har-monic than .
Assume there were a that is more harmonic 3 than .
"Then there must be 3 some con-straint , such 3 that violates fewer times than does, and is ranked higher than any other constraint in which and differ."
"Constraints have to be incurred within some local subtree; so must contain a local violation configuration that does not contain, and by the construction (12)/([Footnote_13]) the e -augmented analysis of – call it e E G – must make use of some violation-marked rule not used in e E G ."
"13 The non-cyclicity condition is inherited from K&amp;W00; in linguistically motivated applications of the LFG formalism, cru-"
Now there are three possibilities: (i) Both e E G and e E G are free of recursion.
Then the fact that e E G avoids the highest-ranking constraint violation excludes from Cat E  &gt; @ G (by construction step (20b)).
This gives us a contradic-tion with our assumption. (ii) e E G contains a recursion and e E G is free of recursion.
"If the recursion in e E G is violation-free, then there is an equally harmonic recursion-free candidate ."
"But this is also less har-monic than e E G , such that it would have been ex-cluded from Cat E  &gt; @ G too."
This again means that e E G would also be excluded (for lack of the rel-evant rules in the non-recursive part).
"On the other hand, if it were the recursion in e E G that incurred the additional violation (as compared to e E G ), then there would be a more harmonic recursion-free candidate ."
"However, this would exclude the presence of e E G in  &gt; @ by construction step (20c,d) (only violation-free recursion is possible)."
So we get another contradiction to the assumption that Cat E  &gt; @ G . (iii) e E G contains a recursion.
"If this recursion is violation-free, we can pick the equally harmonic candidate avoiding the recursion to be our e E G , and we are back to case (i) and (ii)."
"Likewise, if the recursion in e E G does incur some violation, not using the recursion leads to an even more harmonic candidate, for which again cases (i) and (ii) will ap-ply."
"All possible cases lead to a contradiction with the assumptions, so no candidate is more harmonic than our Cat E  &gt; @ G ."
"We still have to prove that if the c-structure of a candidate analysis generated from +2 4 with 32+476[REF_CITE]&lt;; is equally or more harmonic than all other candi-dates, then it is contained in Cat E  &gt;@ G ."
"We can construct an augmented version of , such that Cat E G % and then show that there is a homo-morphism mapping  &gt; @ to some analysis with Cat E G % ."
We can use the constraint marking construction e and the construction to construct the tree with augmented category symbols of the analysis .
The result of K&amp;[REF_CITE]guarantee that Cat E G % .
"Now, there has to be a homo-morphism from the categories in to the cate-ongories  of some analysis  in  &gt; @ .  &gt; @ is also based +4 G (with an additional index 2E 2547682:&lt;9 ;  on each category  and some categories and rules of"
E +2 476[REF_CITE]&lt;; 254 G having no counterpart in  &gt; @ ).
"Since we know that is equally or more harmonic than any other candidate generated from +2 4 , we know that the augmented tree either contains no recursion or only violation-free recursion."
"If it does contain such violation-free recursions we map all categories 0 on the recursion paths to the indexed form : , and furthermore consider the variant of avoiding the recursion(s)."
"For our (non-recursive) tree, there is guaranteed to be a counterpart in the finite set of non-recursive trees in  &gt; @ with all cat-egories pairwise identical apart from the index in  &gt; @ ."
We pick this tree and map each of the cate-gories in to the -indexed counterpart.
The exis-tence of this homomorphism guarantees that an anal-ysis  &gt; @ exists with Cat E G % Cat E G %.
"We showed that for OT-LFG systems in which all constraints can be expressed relative to a local sub-tree in c-structure, the generation task from (non-cyclic [Footnote_13] ) f-structures is solvable."
"13 The non-cyclicity condition is inherited from K&amp;W00; in linguistically motivated applications of the LFG formalism, cru-"
The infinity of the conceptually underlying candidate set does not preclude a computational approach.
"It is obvious that the construction proposed here has the purpose of bringing out the principled computability, rather than suggesting a particular algorithm for imple-mentation."
"However on this basis, an implementa-tion can be easily devised."
"The locality condition on constraint-checking seems unproblematic for linguistically relevant con-straints, since a GPSG-style slash mechanism per-mits reference to (finitely many) nonlocal configu-rations from any given category (cf. fn. 5). [Footnote_14]"
"14 A hypothetical constraint that is excluded would be a paral-lelism constraint comparing two subtree structures of arbitrary depth. Such a constraint seems unnatural in a model of gram-maticality. Parallelism of conjuncts does play a role in models of human parsing preferences; however, here it seems reason-able to assume an upper bound on the depth of parallel struc-tures to be compared (due to memory restrictions)."
"Decidability of generation-based optimization (from a given input f-structure) alone does not im-ply that the recognition and parsing tasks for an OT grammar system defined as in sec. 3 are decidable: for these tasks, a string is given and it has to be shown that the string is optimal for some underlying input f-structure (cf.[REF_CITE])."
"However, a similar construction as the one presented here can be devised for parsing-based optimization (even for an LFG-style grammar that does not obey the offline parsability condition)."
"So, if the language generated by an OT system is defined based on (strong) bidi-rectional optimality ([REF_CITE]ch. 5), decidabil-ity of both the general parsing and generation prob-lem follows. [Footnote_15]"
"15 Parsing: for a given string, parsing-based optimization is used to determine the optimal underlying f-structure; then generation-based optimization is used to check whether the original string comes out optimal in this direction too. Gen-eration is symmetrical, starting with an f-structure."
"For the unidirectionally defined OT language (as in sec. 3), decidability of parsing can be guaranteed under the assumption of a contextual recoverability condition in parsing (Kuhn, in prepa-ration)."
This paper ties up some loose ends in finite-state Optimality Theory.
"First, it discusses how to perform comprehension un-der Optimality Theory grammars consisting of finite-state con-straints."
"Comprehension has not been much studied in OT; we show that unlike production, it does not always yield a regular set, making finite-state methods inapplicable."
"However, after giving a suitably flexible presentation of OT, we show care-fully how to treat comprehension under recent variants of OT in which grammars can be compiled into finite-state transduc-ers."
"We then unify these variants, showing that compilation is possible if all components of the grammar are regular relations, including the harmony ordering on scored candidates."
A side benefit of our construction is a far simpler implementation of directional OT[REF_CITE].
To produce language is to convert utterances from their underlying (“deep”) form to a surface form.
Optimality Theory or OT[REF_CITE]proposes to describe phonological production as an optimization process.
"For an underlying x, a speaker purportedly chooses the surface form z so as to maximize the harmony of the pair (x, z)."
"Broadly speaking, (x, z) is harmonic if z is “easy” to pronounce and “similar” to x."
"But the precise har-mony measure depends on the language; according to OT, it can be specified by a grammar of ranked desiderata known as constraints."
"According to OT, then, production maps each un-derlying form to its best possible surface pronuncia-tion."
"It is akin to the function that maps each child x to his or her most flattering outfit z. Different chil-dren look best in different clothes, and for an oddly shaped child x, even the best conceivable outfit z may be an awkward compromise between style and fit—that is, between ease of pronunciation and sim-ilarity to x."
Language comprehension is production in re-verse.
"In OT, it maps each outfit z to the set of chil- dren x for whom that outfit is optimal, i.e., is at least as flattering as any other outfit z 0 :"
"PRODUCE (x) = {z : (@z 0 ) (x, z 0 ) &gt; (x, z)} COMPREHEND (z) = {x : z ∈ PRODUCE (x)} = {x : (@z 0 ) (x, z 0 ) &gt; (x, z)}"
In general z and z 0 may range over infinitely many possible pronunciations.
"While the formulas above are almost identical, comprehension is in a sense more complex because it varies both the underlying and surface forms."
"While PRODUCE (x) considers all pairs (x, z 0 ), COMPREHEND (z) must for each x consider all pairs (x, z 0 )."
"Of course, this nested def-inition does not preclude computational shortcuts."
This paper has three modest goals: 1.
To show that OT comprehension does in fact present a computational problem that production does not.
"Even when the OT grammar is required to be finite-state, so that production can be performed with finite-state techniques, comprehension cannot in general be performed with finite-state techniques. 2. To consider recent constructions that cut through this problem ([REF_CITE]; Gerdemann and van[REF_CITE])."
"By altering or approximating the OT formalism—that is, by hook or by crook—these con-structions manage to compile OT grammars into finite-state transducers."
Transducers may readily be inverted to do comprehension as easily as produc-tion.
"We carefully lay out how to use them for com-prehension in realistic circumstances (in the pres-ence of correspondence theory, lexical constraints, hearer uncertainty, and phonetic postprocessing). 3. To give a unified treatment in the extended finite-state calculus of the constructions referenced above."
This clarifies their meaning and makes them easy to implement.
"For example, we obtain a transparent al-gebraic version of Eisner’s (2000) unbearably tech-nical automaton construction for his proposed for-malism of “directional OT.”"
"The treatment shows that all the constructions emerge directly from a generalized presentation of OT, in which the crucial fact is that the harmony or-dering on scored candidates is a regular relation."
Work focusing on OT comprehension—or even mentioning it—has been surprisingly sparse.
"While the recent constructions mentioned in §[Footnote_1] can easily be applied to the comprehension problem, as we will explain, they were motivated primarily by a desire to pare back OT’s generative power to that of previous rewrite-rule formalisms[REF_CITE]."
"1 Hale &amp; Reiss’s criticism may be specific to phonology and syntax. For some phenomena in semantics, pragmatics, and even morphology,[REF_CITE]argues for a one-to-one form-meaning mapping in which marked forms express marked meanings. He deliberately uses bidirectional optimization to rule out many-to-one cases: roughly speaking, an (x, z) pair is grammatical for him only if z is optimal given x and vice-versa."
"COMPREHEND (z) = ? {x : (@x 0 ) (x 0 , z) &gt; (x, z)}"
"The correctness of Smolensky’s proposal (i.e., whether it really computes COMPREHEND ) depends on the particular harmony measure."
"It can be made to work, multiple optima and all, if the harmony measure is constructed with both production and comprehension in mind."
"Indeed, for any phonology, it is trivial to design a harmony measure that both production and comprehension optimize. (Just de-fine the harmony of (x,z) to be 1 or 0 according to whether the mapping x 7→ z is in the language!)"
But we are really only interested in harmony mea-sures that are defined by OT-style grammars (rank-ings of “simple” constraints).
In this case Smolen-sky’s proposal can be unworkable.
"In particular, §4 will show that a finite-state production grammar in classical OT need not be invertible by any finite-state comprehension grammar."
"This section (graphically summarized in Fig. 1) lays out a generalized version of OT’s theory of produc-tion, introducing some notational and representa-tional conventions that may be useful to others and will be important below."
"In particular, all objects are represented as strings, or as functions that map strings to strings."
This will enable us to use finite-state techniques later.
The underlying form x and surface form z are represented as strings.
We often refer to these strings as input and output.
"The notation (x, z) that we have been using so far for candidates is actually misleading, since in fact the candidates y that are compared encode more than just x and z. They also encode a particular alignment or correspondence between x and z."
"For example, if x = abdip and z = a[di][bu], then a typical candidate would be encoded y = aab0[ddii][pb0u] which specifies that a corresponds to a, b was deleted (has no surface correspondent), voiceless p surfaces as voiced b, etc."
The harmony of y might depend on this alignment as well as on x and z (just as an outfit might fit worse when worn backwards).
"Because we are distinguishing underlying and surface material by using disjoint alphabets Σ = {a, b, . . .} and ∆ = {[, ], a, b, . . . }, [Footnote_2] it is easy to extract the underlying and surface forms (x and z) from y."
2 An alternative would be to distinguish them by odd and even positions in the string.
"Although the above example assumes that x and z are simple strings of phonemes and brackets, noth-ing herein depends on that assumption."
Autoseg-mental representations too can be encoded as strings[REF_CITE].
"In general, an OT grammar consists of 4 com-ponents: a constraint ranking, a harmony ordering, and generating and pronouncing functions."
The con-straint ranking is the language-specific part of the grammar; the other components are often supposed to be universal across languages.
The generating function G EN maps any x ∈ Σ ∗ to the (nonempty) set of candidates y whose under-lying form is x.
"In other words, G EN just inserts arbitrary substrings from ∆ ∗ amongst the charac-ters of x, subject to any restrictions on what consti-tutes a legitimate candidate y. 3 (Legitimacy might for instance demand that y’s surface material z have matched, non-nested left and right brackets, or even that z be similar to x in terms of edit distance.)"
"A constraint ranking is simply a sequence C 1 ,C 2 ,...C n of constraints."
Let us take each C i to be a function that scores candidates y by annotating them with violation marks ?.
"For ex-ample, a N O D ELETE constraint would map y = aab0c0[ddii][pb0u] to ȳ =N O D ELETE (y) = aab?0c?0[ddii][pb0u], inserting a ? after each underlying phoneme that does not correspond to any surface phoneme."
This unconventional formulation is needed for new approaches that care about the ex-act location of the ?’s.
"In traditional OT only the number of ?’s is important, although the locations are sometimes shown for readability."
"Finally, OT requires a harmony ordering on scored candidates ȳ ∈ (Σ ∪ ∆ ∪ {?}) ∗ ."
"In traditional OT, ȳ is most harmonic when it con-tains the fewest ?’s."
"For example, among candi-dates scored by N O D ELETE , the most harmonic ones are the ones with the fewest deletions; many candidates may tie for this honor. §6 considers other harmony orderings, a possibility recognized[REF_CITE]( corresponds to their H -E VAL )."
"In general may be a partial or-der: two competing candidates may be equally har-monic or incomparable (in which case both can survive), and candidates with different underlying forms never compete at all."
"Production under such a grammar is a matter of successive filtering by the constraints C 1 , . . ."
C n .
"Given an underlying form x, let"
Y 0 (x) = G EN (x) (1)
Y i (x) = {y ∈ Y i−1 (x) : (2) (@y 0 ∈ Y i−1 (x))
C i (y 0 )
C i (y)}
The set of optimal candidates is now Y n (x).
Ex-tracting z from each y ∈ Y n (x) gives the set Z(x) or P RODUCE (x) of acceptable surface forms:
Z(x) = {P RON (y) : y ∈ Y n (x)} ⊆ ∆ ∗ ([Footnote_3])
"3 It is never really necessary for G EN to enforce such restric-tions, since they can equally well be enforced by the top-ranked constraint C 1 (see below)."
P RON denotes the simple pronunciation function that extracts z from y.
"It is the counterpart to G EN : just as G EN fleshes out x ∈ Σ ∗ into y by inserting symbols of ∆, P RON slims y down to z ∈ ∆ ∗ by removing symbols of Σ."
Notice that Y n ⊆ Y n−1 ⊆ ... ⊆ Y 0 .
The only candidates y ∈ Y i−1 that survive filtering by C i are the ones that C i considers most harmonic.
"The above notation is general enough to handle some of the important variations of OT, such as Paradigm Uniformity and Sympathy Theory."
"In par-ticular, one can define G EN so that each candidate y encodes not just an alignment between x and z, but an alignment among x, z, and some other strings that are neither underlying nor surface."
"These other strings may represent the surface forms for other members of the same morphological paradigm, or intermediate throwaway candidates to which z is sympathetic."
"Production still optimizes y, which means that it simultaneously optimizes z and the other strings."
"This section assumes OT’s traditional harmony or-dering, in which the candidates that survive filtering by C i are the ones into which C i inserts fewest ?’s."
"Much computational work on OT has been con-ducted within a finite-state framework[REF_CITE], in keeping with a tradition of finite-state phonology[REF_CITE]. [Footnote_4]"
4 The tradition already included (inviolable) phonological
Finite-state OT is a restriction of the formal-ism discussed above.
"It specifically assumes that G EN , C 1 , . . ."
"C n , and P RON are all regular relations, meaning that they can be described by finite-state transducers."
G EN is a nondeterministic transducer that maps each x to multiple candidates y.
The other transducers map each y to a single ȳ or z.
These finite-state assumptions were proposed (in a different and slightly weaker form)[REF_CITE].
Their empirical adequacy has been defended[REF_CITE].
"In addition to having the right kind of power lin-guistically, regular relations are closed under vari-ous relevant operations and allow (efficient) parallel processing of regular sets of strings."
"Given x and a finite-state OT grammar, he used finite-state operations to con-struct the set Y n (x) of optimal candidates, repre-sented as a finite-state automaton."
Ellison’s construction demonstrates that Y n is al-ways a regular set.
"Since P RON is regular, it follows that PRODUCE (x) = Z(x) is also a regular set."
"We now show that COMPREHEND (z), in con-strast, need not be a regular set."
"Let Σ = {a,b}, ∆ = {[, ], a, b, . . .} and suppose that G EN allows candidates like the ones in §3, in which parts of the string may be bracketed between [ and ]."
The cru-cial grammar consists of two finite-state constraints.
C 2 penalizes a’s that fall between brackets (by in-serting ? next to each one) and also penalizes b’s that fall outside of brackets.
"It is dominated by C 1 , which penalizes brackets that do not fall at either edge of the string."
Note that this grammar is com-pletely permissive as to the number and location of surface characters other than brackets.
"If x contains more a’s than b’s, then PRODUCE (x) is the set ∆ˆ ∗ of all unbracketed surface forms, where ∆ˆ is ∆ minus the bracket symbols."
"If x contains fewer a’s than b’s, then PRODUCE (x) = [∆ˆ ∗ ]."
"And if a’s and b’s appear equally often in x, then PRODUCE (x) is the union of the two sets."
"Thus, while the x-to-z mapping is not a regular relation under this grammar, at least PRODUCE (x) is a regular set for each x—just as finite-state OT guarantees."
"But for any unbracketed z ∈ ∆ˆ ∗ , such as z = abc, COMPREHEND (z) is not regular: it is the set of underlying strings with # of a’s ≥ # of b’s."
This result seems to eliminate any hope of han-dling OT comprehension in a finite-state frame-work.
It is interesting to note that both OT and current speech recognition systems construct finite-state models of production and define comprehen-sion as the inverse of production.
Speech recog-nizers do correctly implement comprehension via finite-state optimizati[REF_CITE].
"But this is impossible in OT because OT has a more complicated production model. (In speech recog-nizers, the most probable phonetic or phonological surface form is not presumed to have suppressed its competitors.)"
One might try to salvage the situation by barring constraints like C 1 or C 2 from the theory as linguis-tically implausible.
Unfortunately this is unlikely to succeed.
"Primitive OT[REF_CITE]already re-stricts OT to something like a bare minimum of con-straints, allowing just two simple constraint families that are widely used by practitioners of OT."
Yet even these primitive constraints retain enough power to simulate any finite-state constraint.
"In any case, C 1 and C 2 themselves are fairly similar to “domain” constraints used to describe tone systems[REF_CITE]."
"While C 2 is somewhat odd in that it penalizes two distinct configurations at once, one would obtain the same effect by combining three separately plausible constraints: C 2 requires a’s be-tween brackets (i.e., in a tone domain) to receive sur-face high tones, C 3 requires b’s outside brackets to receive surface high tones, and C 4 penalizes all sur-face high tones. [Footnote_5]"
"5 Since the surface tones indicate the total number of a’s and b’s in the underlying form, COMPREHEND (z) is actually a finite set in this version, hence regular. But the non-regularity argu-ment does go through if the tonal information in z is not avail-able to the comprehension system (as when reading text with-out diacritics); we cover this case in §5. (One can assume that some lower-ranked constraints require a special suffix before ], so that the bracket information need not be directly available to the comprehension system either.)"
"Another obvious if unsatisfying hack would im-pose heuristic limits on the length of x, for exam-ple by allowing the comprehension system to return the approximation COMPREHEND (z) ∩ {x : |x| ≤ 2 · |z|}."
"This set is finite and hence regular, so per- haps it can be produced by some finite-state method, although the automaton to describe the set might be large in some cases."
Recent efforts to force OT into a fully finite-state mold are more promising.
"As we will see, they iden-tify the problem as the harmony ordering , rather than the space of constraints or the potential infini-tude of the answer set."
"Since COMPREHEND (z) need not be a regular set in traditional OT, a corollary is that COMPREHEND and its inverse PRODUCE are not regular relations."
"That much was previously shown by Markus Hiller and Paul Smolensky[REF_CITE], using similar examples."
"However, at least some OT grammars ought to de-scribe regular relations."
"It has long been hypothe-sized that all human phonologies are regular rela-tions, at least if one omits reduplication, and this is necessarily true of phonologies that were success-fully described with pre-OT formalisms[REF_CITE]."
Regular relations are important for us because they are computationally tractable.
"Any regular rela-tion can be implemented as a finite-state transducer T, which can be inverted and used for comprehen-sion as well as production."
"PRODUCE (x) = T (x) = range (x ◦ T), and COMPREHEND (z) = T −1 (z) = domain (T ◦ z)."
We are therefore interested in compiling OT grammars into finite-state transducers—by hook or by crook. §6 discusses how; but first let us see how such compilation is useful in realistic situations.
Any practical comprehension strategy must rec-ognize that the hearer does not really perceive the entire surface form.
"After all, the surface form con-tains phonetically invisible material (e.g., syllable and foot boundaries) and makes phonetically imper-ceptible distinctions (e.g., two copies of a tone ver-sus one doubly linked copy)."
How to comprehend in this case?
The solution is to modify P RON to “go all the way”—to delete not only underlying material but also phonetically invisible material.
"Indeed, P RON can also be made to perform any purely phonetic processing."
Each output z of PRODUCE is now not a phonological surface form but a string of phonemes or spectrogram segments.
"So long as P RON is a reg-ular relation (perhaps a nondeterministic or prob-abilistic one that takes phonetic variation into ac-count), we will still be able to construct T and use it for production and comprehension as above. [Footnote_6]"
"6[REF_CITE]build a speech recognizer by com-posing a probabilistic finite-state language model, a finite-state pronouncing dictionary, and a probabilistic finite-state acoustic model. These three components correspond precisely to the in-put to G EN , the traditional OT grammar, and P RON , so we are simply suggesting the same thing in different terminology."
How about the lexicon?
"When the phonology can be represented as a transducer, COMPREHEND (z) is a regular set."
It contains all inputs x that could have produced output z.
"In practice, many of these in-puts are not in the lexicon, nor are they possible novel words."
One should restrict to inputs that ap-pear in the lexicon (also a regular set) by intersecting COMPREHEND (z) with the lexicon.
"For novel words this intersection will be empty; but one can find the possible underlying forms of the novel word, for learning’s sake, by intersecting COMPREHEND (z) with a larger (infinite) regular set representing all forms satisfying the language’s lexical constraints."
There is an alternative treatment of the lexicon.
G EN can be extended “backwards” to incorporate morphology just as P RON was extended “forwards” to incorporate phonetics.
"On this view, the input x is a sequence of abstract morphemes, and G EN performs morphological preprocessing to turn x into possible candidates y. G EN looks up each abstract morpheme’s phonological string ∈ Σ ∗ from the lex-icon, [Footnote_7] then combines these phonological strings by concatenation or template merger, then nondeter-ministically inserts surface material from ∆ ∗ ."
"7 Nondeterministically in the case of phonologically condi-tioned allomorphs: INDEFINITE APPLE 7→ { Λ æpl, ænæpl} ⊆ Σ ∗ . This yields competing candidates that differ even in their underlying phonological material."
Such a G EN can plausibly be built up (by composition) as a regular relation from abstract morpheme se-quences to phonological candidates.
"This regularity, as for P RON , is all that is required."
Representing a phonology as a transducer T has additional virtues.
"T can be applied efficiently to any input string x, where[REF_CITE]or[REF_CITE]requires a fresh automaton construc-tion for each x. A nice trick is to build T without"
"P RON and apply it to all conceivable x’s in paral-lel, yielding the complete set of all optimal candi-dates Y n (Σ ∗ ) ="
S x∈Σ ∗ Y n (x).
"If Y and Y 0 denote the sets of optimal candidates under two grammars, then (Y ∩ ¬Y 0 ) ∪ (Y 0 ∩ ¬Y ) yields the candidates that are optimal under only one grammar."
"Applying G EN −1 or P RON to this set finds the regular set of underlying or surface forms that the two grammars would treat differently; one can then look for empir-ical cases in this set, in order to distinguish between the two grammars."
Why are OT phonologies not always regular re-lations?
"The trouble is that inputs may be arbi-trarily long, and so may accrue arbitrarily large numbers of violations."
Traditional OT (§4) is supposed to distinguish all such numbers.
"Con-sider syllabification in English, which prefers to syllabify the long input bibambam...bam | k copies {z } as [bi][bam][bam] . . . [bam] (with k codas) rather than [bib][am][bam]... [bam] (with k + 1 codas)."
N O C ODA must therefore distinguish annotated candidates ȳ with k ?’s (which are opti-mal) from those with k + 1 ?’s (which are not).
It requires a (≥ k + 2)-state automaton to make this distinction by looking only at the ?’s in ȳ.
"And if k can be arbitrarily large, then no finite-state automa-ton will handle all cases."
"Thus, constraints like N O C ODA do not allow an upper bound on k for all x ∈ Σ ∗ ."
"Of course, the min-imal number of violations k of a constraint is fixed given the underlying form x, which is useful in pro-duction. [Footnote_8]"
"8[REF_CITE]was able to construct P RODUCE (x) from x. One can even build a transducer for P RODUCE that is correct on all inputs that can achieve ≤ K violations and returns ∅ on other inputs (signalling that the transducer needs to be recompiled with increased K). Simply use the construction[REF_CITE], composed with a hard constraint that the answer must have ≤ K violations."
But comprehension is less fortunate: we cannot bound k given only the surface form z.
"In the grammar of §4, COMPREHEND (abc) included underlying forms whose optimal candidates had ar-bitrarily large numbers of violations k."
"Now, in most cases, the effect of an OT gram-mar can be achieved without actually counting any-thing. (This is to be expected since rewrite-rule grammars were previously written for the same phonologies, and they did not use counting!)"
"This is possible despite the above arguments because for some grammars, the distinction between opti-mal and suboptimal ȳ can be made by looking at the non-? symbols in ȳ rather than trying to count the ?’s."
"In our N O C ODA example, a surface sub-string such as . . . ib?][a. . . might signal that ȳ is suboptimal because it contains an “unnecessary” coda."
"Of course, the validity of this conclusion depends on the grammar and specifically the con-straints C 1 , . . ."
"C i−1 ranked above N O C ODA , since whether that coda is really unnecessary depends on whether Ȳ i−1 also contains the competing candidate . . . i][ba . . . with fewer codas."
"But as we have seen, some OT grammars do have effects that overstep the finite-state boundary (§4)."
Recent efforts to treat OT with transducers have therefore tried to remove counting from the formal-ism.
"We now unify such efforts by showing that they all modify the harmony ordering . §4 described finite-state OT grammars as ones where G EN , P RON , and the constraints are regular relations."
"We claim that if the harmony ordering is also a regular relation on strings of (Σ∪∆∪{?}) ∗ , then the entire grammar ( PRODUCE ) is also regular."
"We require harmony orderings to be compatible with G EN : an ordering must treat ȳ 0 , ȳ as incompa-rable (neither is the other) if they were produced from different underlying forms. [Footnote_9]"
"9 For example, the harmony ordering of traditional OT is {(ȳ 0 ,ȳ) : ȳ 0 has the same underlying form as, but contains fewer ?’s than, ȳ}. If we were allowed to drop the same-underlying-form condition then the ordering would become reg-ular, and then our claim would falsely imply that all traditional finite-state OT grammars were regular relations."
"To make the notation readable let us denote the relation by the letter H. Thus, a transducer for H accepts the pair (ȳ 0 , ȳ) if ȳ 0 ȳ."
The construction is inductive.
Y 0 =
G EN is reg-ular by assumption.
"If Y i−1 is regular, then so is Y i since (as we will show)"
Y i = (Ȳ i ◦ ¬ range (Ȳ i ◦ H)) ◦
D (4) def where Ȳ i = Y i−1 ◦
C i and maps x to the set of starred candidates that C i will prune; ¬ denotes the complement of a regular language; and D is a trans-ducer that removes all ?’s.
Therefore PRODUCE = Y n ◦
P RON is regular as claimed.
It remains to derive (4).
Equation (2) implies
"C i (Y i (x)) = {ȳ ∈ Ȳ i (x) : (@ȳ 0 ∈ Ȳ i (x)) ȳ 0 ȳ} (5) = Ȳ i (x) − {ȳ : (∃ȳ 0 ∈ Ȳ i (x)) ȳ 0 ȳ} (6) = Ȳ i (x) − H(Ȳ i (x)) (7) One can read H(Ȳ i (x)) as “starred candidates that are worse than other starred candidates,” i.e., subop-timal."
The set difference (7) leaves only the optimal candidates.
"We now see (x, ȳ) ∈ Y i ◦"
C i ⇔ ȳ ∈
"C i (Y i (x)) (8) ⇔ ȳ ∈ Ȳ i (x), ȳ 6∈ H(Ȳ i (x)) [by (7)] (9) ⇔ ȳ ∈ Ȳ i (x), (@z)ȳ ∈ H(Ȳ i (z)) [see below](10) ⇔ (x, ȳ) ∈ Ȳ i , ȳ 6∈ range (Ȳ i ◦ H) (11) ⇔ (x, ȳ) ∈ Ȳ i ◦ ¬ range (Ȳ i ◦"
H) (12) therefore Y i ◦
C i = Ȳ i ◦ ¬ range (Ȳ i ◦
H) (13) and composing both sides with D yields (4).
To jus-tify (9) ⇔ (10) we must show when ȳ ∈
Ȳ i (x) that ȳ ∈ H(Ȳ i (x)) ⇔ (∃z)ȳ ∈ H(Ȳ i (z)).
"For the ⇒ direction, just take z = x. For ⇐ , ȳ ∈ H(Ȳ i (z)) means that (∃ȳ 0 ∈ Ȳ i (z))ȳ 0 ȳ; but then x = z (giving ȳ ∈ H(Ȳ i (x))), since if not, our compatibil-ity requirement on H would have made ȳ 0 ∈"
Ȳ i (z) incomparable with ȳ ∈ Ȳ i (x).
"Extending the pretty notation[REF_CITE], we may use (4) to define a left-associative generalized optimality operator oo H : def"
Y oo H C = (Y ◦C◦¬ range (Y ◦C◦H))◦D (14)
"Then for any regular OT grammar, PRODUCE ="
G EN oo H C 1 oo H C 2 · · · oo H C n ◦
P RON and can be inverted to get COMPREHEND .
"More generally, different constraints can usefully be ap-plied with different H’s[REF_CITE]."
The algebraic construction above is inspired by a version that Gerdemann and van[REF_CITE]give for a particular variant of OT.
"Their regular expres-sions can be used to implement it, simply replacing their add_violation by our H."
"Typically, H ignores surface characters when comparing starred candidates."
"So H can be written as elim(∆)◦G◦elim(∆) −1 where elim(∆) is a transducer that removes all characters of ∆. To sat-isfy the compatibility requirement on H, G should be a subset of the relation (Σ| ? |( : ?) |(? : )) ∗ . [Footnote_10]"
"10 This transducer regexp says to map any symbol in Σ ∪ {?} to itself, or insert or delete ?—and then repeat."
"We now summarize the main proposals from the literature (see §1), propose operator names, and cast them in the general framework. • Y o C: Inviolable constraint[REF_CITE], implemented by composition. • Y o+ C: Counting constraint[REF_CITE]: more violations is more dishar-monic."
No finite-state implementation possible. • Y oo C: Binary approximati[REF_CITE].
All candidates with any violations are equally disharmonic.
Imple-mented by G = (Σ ∗ ( : ?)
"Σ ∗ ) + , which relates un-derlying forms without violations to the same forms with violations. • Y oo 3 C: 3-bounded approximati[REF_CITE]."
"Like o+ , but all candidates with ≥ 3 violations are equally dishar-monic."
"G is most easily described with a transducer that keeps count of the input and output ?’s so far, on a scale of 0, 1, 2, ≥ 3."
Final states are those whose output count exceeds their input count on this scale. • Y o⊂ C: Matching or subset approximation (Gerdemann and van[REF_CITE]).
A candidate is more disharmonic than another if it has stars in all the same locations and some more besides. [Footnote_11]
"11 Many candidates are incomparable under this ordering, so Gerdemann and van Noord also showed how to weaken the no-tation of “same location” in order to approximate o+ better."
Here G = ((Σ|?) ∗ ( : ?)(Σ|?) ∗ ) + . • Y o&gt; C: Left-to-right directional evaluati[REF_CITE].
"A candidate is more disharmonic than another if in the leftmost position where they differ (ignoring surface characters), it has a ?."
This revises OT’s “do only when necessary” mantra to “do only when necessary and then as late as possible” (even if delaying ?’s means suffering more of them later).
Here G = (Σ|?) ∗ (( : ?)|((Σ : ?)(Σ|?) ∗ )).
"Unlike the other proposals, here two forms can both be op-timal only if they have exactly the same pattern of violations with respect to their underlying material. • Y &lt;o C: Right-to-left directional evaluation. “Do only when necessary and then as early as possi-ble.”"
Here G is the reverse of the G used in o&gt; .
The novelty of the matching and directional pro-posals is their attention to where the violations fall.
"Eisner’s directional proposal (o&gt;, &lt;o) is the only one defended on linguistic as well as computational grounds."
"He argues that violation counting (o+) is a bug in OT rather than a feature worth approximat-ing, since it predicts unattested phenomena such as “majority assimilation”[REF_CITE]."
"Conversely, he argues that comparing viola-tions directionally is not a hack but a desirable fea-ture, since it naturally predicts “iterative phenom-ena” whose description in traditional OT (via Gener-alized Alignment) is awkward from both a linguistic and a computational point of view."
Fig. 2 contrasts the traditional and directional harmony orderings.
"The new algebraic construc-tion is simple and can be implemented with a few regular expressions, as for any other H."
See the itemized points in §1 for a detailed summary.
"In general, this paper has laid out a clear, general framework for finite-state OT systems, and used it to obtain positive and negative results about the under-studied problem of comprehension."
Perhaps these results will have some bearing on the development of realistic learning algorithms.
The paper has also established sufficient condi-tions for a finite-state OT grammar to compile into a finite-state transducer.
It should be easy to imagine new variants of OT that meet these conditions.
"This paper presents a new formalization of a unification- or join-preserving encoding of partially ordered sets that more essen-tially captures what it means for an en-coding to preserve joins, generalizing the standard definition in AI research."
It then shows that every statically typable ontol-ogy in the logic of typed feature struc-tures can be encoded in a data structure of fixed size without the need for resizing or additional union-find operations.
"This is important for any grammar implemen-tation or development system based on typed feature structures, as it significantly reduces the overhead of memory manage-ment and reference-pointer-chasing dur-ing unification."
"The logic of typed feature structures[REF_CITE]has been widely used as a means of formaliz-ing and developing natural language grammars that support computationally efficient parsing, genera-tion and SLD resolution, notably grammars within the Head-driven Phrase Structure Grammar (HPSG) framework, as evidenced by the recent successful development of the LinGO reference grammar for English[REF_CITE]."
"These grammars are for-mulated over a finite vocabulary of features and par-tially ordered types, in respect of constraints called appropriateness conditions."
"Appropriateness speci-fies, for each type, all and only the features that take values in feature structures of that type, along with the types of values (value restrictions) those feature values must have."
"In Figure 1, [Footnote_1] for example, all head-typed TFSs must have bool-typed values for the features MOD and PRD , and no values for any other feature."
"1 In this paper, Carpenter&apos;s (1992) convention of using as the most general type, and depicting subtypes above their su-pertypes is used."
"Relative to data structures like arrays or logical terms, typed feature structures (TFSs) can be re-garded as an expressive refinement in two different ways."
"First, they are typed, and the type system al-lows for subtyping chains of unbounded depth."
Fig-ure 1 has a chain of length from to noun.
"Point-ers to arrays and logical terms can only monoton-ically “refine” their (syntactic) type from unbound (for logical terms, variables) to bound."
"Second, al-though all the TFSs of a given type have the same features because of appropriateness, a TFS may ac-quire more features when it promotes to a subtype."
"If a head-typed TFS promotes to noun in the type sys-tem above, for example, it acquires one extra case-valued feature, CASE ."
"When a subtype has two or more incomparable supertypes, a TFS can also mul-tiply inherit features from other supertypes when it promotes."
"The overwhelmingly most prevalent operation when working with TFS-based grammars is unifica-tion, which corresponds mathematically to finding a least upper bound or join."
The most common in-stance of unification is the special case in which a TFS is unified with the most general TFS that satis-fies a description stated in the grammar.
"This special case can be decomposed at compile-time into more atomic operations that (1) promote a type to a sub-type, (2) bind a variable, or (3) traverse a feature path, according to the structure of the description."
"TFSs actually possess most of the properties of fixed-arity terms when it comes to unification, due to appropriateness."
"Nevertheless, unbounded sub-typing chains and acquiring new features conspire to force most internal representations of TFSs to per-form extra work when promoting a type to a subtype to earn the expressive power they confer."
"Upon be-ing repeatedly promoted to new subtypes, they must be repeatedly resized or repeatedly referenced with a pointer to newly allocated representations, both of which compromise locality of reference in memory and/or involve pointer-chasing."
These costs are sig-nificant.
"Because appropriateness involves value restric-tions, simply padding a representation with some ex-tra space for future features at the outset must guar-antee a proper means of filling that extra space with the right value when it is used."
"Internal representa-tions that lazily fill in structure must also be wary of the common practice in description languages of binding a variable to a feature value with a scope larger than a single TFS — for example, in sharing structure between a daughter category and a mother category in a phrase structure rule."
"In this case, the representation of a feature&apos;s value must also be in-terpretable independent of its context, because two separate TFSs may refer to that variable."
"These problems are artifacts of not using a rep-resentation which possesses what in knowledge rep-resentation is known as a join-preserving encoding of a grammar&apos;s TFSs — in other words, a repre-sentation with an operation that naturally behaves like TFS-unification."
The next section presents the standard definition of join-preserving encodings and provides a generalization that more essentially cap-tures what it means for an encoding to preserve joins.
Section 3 formalizes some of the defining characteristics of TFSs as they are used in com-putational linguistics.
Section 4 shows that these characteristics quite fortuitously agree with what is required to guarantee the existence of a join-preserving encoding of TFSs that needs no resizing or extra referencing during type promotion.
Sec-tion 5 then shows that a generalized encoding exists in which variable-binding scope can be larger than a single TFS — a property no classical encoding has.
"Earlier work on graph unification has focussed on labelled graphs with no appropriateness, so the cen-tral concern was simply to minimize structure copy-ing."
"While this is clearly germane to TFSs, appro-priateness creates a tradeoff among copying, the po-tential for more compact representations, and other memory management issues such as locality of ref-erence that can only be optimized empirically and relative to a given grammar and corpus (a recent ex-ample of which can be found[REF_CITE])."
"While the present work is a more theoretical consid-eration of how unification in one domain can sim-ulate unification in another, the data structure de-scribed here is very much motivated by the encod-ing of TFSs as Prolog terms allocated on a contigu-ous WAM-style heap."
"In that context, the emphasis on fixed arity is really an attempt to avoid copying, and lazily filling in structure is an attempt to make encodings compact, but only to the extent that join preservation is not disturbed."
"While this compro-mise solution must eventually be tested on larger and more diverse grammars, it has been shown to reduce the total parsing time of a large corpus on the ALE HPSG benchmark grammar of English[REF_CITE]by a factor of about 4[REF_CITE]."
We may begin with a familiar definition from dis-crete mathematics:
"Definition 1 Given two partial orders  and  , a function  is an order-  embedding iff, for every  ,  iff ! #&quot;$%&amp; !&quot; ."
"An order-embedding preserves the behavior of the order relation (for TFS type systems, subtyping; for TFSs themselves, subsumption) in the encoding codomain."
"As shown in Figure 2, however, order embeddings do not always preserve operations such as least upper bounds."
The reason is that the im-age of may not be closed under those operations in the codomain.
"In fact, the codomain could pro-vide joins where none were supposed to exist, or, as in Figure 2, no joins where one was supposed to ex-ist."
"Mellish (1991; 1992) was the first to formulate join-preserving encodings correctly, by explicitly re-quiring this preservation."
Let us write   for the join of and in partial order .
Definition[Footnote_2] A partial order % is bounded complete (BCPO) iff every set of elements with a common upper bound has a least upper bound.
"2 We use the notation &quot;$! &apos;%# )&amp; ( is undefined, and"
Bounded completeness ensures that unification or joins are well-defined among consistent types.
There are only a few common-sense restrictions we need to place on our type systems:
"Definition 5 A TFS type system consists of a finite BCPO of types, % , a finite set of features Feat, and a partial function,      such that, for every F  : (Feature Introduction) there is a    type such that: F &quot; F  # F &quot;  &quot; , and for all , if   F   &quot; , then  F &quot; , and (Upward Closure /"
"Right Monotonicity) if   F !  &quot; and ! &quot; , then   F   &quot; and   F ! &quot; #  F  &quot; ."
The function Approp maps a feature and type to the value restriction on that feature when it is appropri-ate to that type.
"If it is not appropriate, then Ap-prop is undefined at that pair."
Feature introduction ensures that every feature has a least type to which it is appropriate.
This makes description compila-tion more efficient.
"Upward closure ensures that subtypes inherit their supertypes&apos; features, and with consistent value restrictions."
"The combination of these two properties allows us to annotate a BCPO of types with features and value restrictions only where the feature is introduced or the value restriction is re-fined, as in Figure 1."
A very useful property for type systems to have is static typability.
"This means that if two TFSs that are well-formed according to appropriateness are unifiable, then their unification is automatically well-formed as well — no additional work is neces-sary."
"An appropriateness specification is statically typable iff, for all types !   such that  ! $% , and all F &amp; :"
"F  !  &quot; ))()) ,  F ! &quot; if   F !  &quot; and * ))))+ ,,   !"
"F  &quot; F   &quot; F &quot; if only ,  F !  &quot; F  &quot; if only ,  F   &quot; unrestricted otherwise"
"Not all type systems are statically typable, but a type system can be transformed into an equivalent stati-cally typable type system plus a set of universal con-straints, the proof of which is omitted here."
"In lin-guistic applications, we normally have a set of uni-versal constraints anyway for encoding principles of grammar, so it is easy and computationally inexpen-sive to conduct this transformation."
"As mentioned in Section 1, what we want is an en-coding of TFSs with a notion of unification that nat-urally corresponds to TFS-unification."
"As discussed in Section 3, static typability is something we can reasonably guarantee in our type systems, and is therefore something we expect to be reflected in our encodings — no extra work should be done apart from combining the types and recursing on feature values."
"If we can ensure this, then we have avoided the extra work that comes with resizing or unneces-sary referencing and pointer-chasing."
"As mentioned above, what would be best from the standpoint of memory management is simply a fixed array of memory cells, padded with extra space to accommodate features that might later be added."
We will call these frames.
Figure 4 depicts a frame for the head-typed TFS in Figure 5.
"In a frame, the rep-resentation of the type can either be (1) a bit vec-tor encoding the type, [Footnote_3] or (2) a reference pointer to another frame."
"3 Instead of a bit vector, we could also use an index into a table if least upper bounds are computed by table look-up."
"If backtracking is supported in search, changes to the type representation must be trailed."
"For each appropriate feature, there is also a pointer to a frame for that feature&apos;s value."
"There are also additional pointers for future features (for head, CASE ) that are grounded to some distinguished value indicating that they are unused — usually a circu-lar reference to the referring array position."
"Cyclic TFSs, if they are supported, would be represented with cyclic (but not 1-cyclic) chains of pointers."
"Frames can be implemented either directly as ar-rays, or as Prolog terms."
"In Prolog, the type rep-resentation could either be a term-encoding of the type, which is guaranteed to exist for any finite BCPO[REF_CITE], or in ex-tended Prologs, another trailable representation such as a mutable term[REF_CITE]or an attributed value[REF_CITE]."
Padding the representation with extra space means using a Pro-log term with extra arity.
A distinguished value for unused arguments must then be a unique unbound variable. [Footnote_4]
"4 Prolog terms require one additional unbound variable per TFS (sub)term in order to preserve the intensionality of the logic — unlike Prolog terms, structurally identical TFS substructures are not identical unless explicitly structure-shared."
"At first blush, the prospect of adding as many extra slots to a frame as there could be extra features in a TFS sounds hopelessly unscalable to large gram-mars."
"While recent experience[REF_CITE]suggests a trend towards modest increases in num-bers of features compared to massive increases in numbers of types as grammars grow large, this is nevertheless an important issue to address."
There are two discrete methods that can be used in combi-nation to reduce the required number of extra slots:
"Definition 6 Given a finite BCPO,  , the set  of modules of % is the finest partition of   , , such that (1) each is upward-closed (with respect to subtyping), and (2) if two types have a least upper bound, then they belong to the same module."
"Trivially, if a feature is introduced at a type in one module, then it is not appropriate to any type in any other module."
"As a result, a frame for a TFS only needs to allow for the features appropriate to the module of its type."
Even this number can normally be reduced:
"Definition 7 The feature graph, &quot; , of module is an undirected graph, whose vertices corre-spond to the features introduced in , and in which there is an edge,    &quot; , iff and are appropriate to a common maximally specific type in ."
Proposition 1 The least number of feature slots re-quired for a frame of any type in is the least for which &quot; is -colorable.
"There are type systems, of course, for which mod-ularization and graph-coloring will not help."
"Fig-ure 6, for example, has one module, three features, and a three-clique for a feature graph."
"There are statistical refinements that one could additionally make, such as determining the empirical probability that a particular feature will be acquired and electing to pay the cost of resizing or referencing for improb-able features in exchange for smaller frames."
"With the exception of extra slots for unused feature values, frames are clearly isomorphic in their struc-ture to the TFSs they represent."
"The implementation of unification that we prefer to avoid resizing and referencing is to (1) find the least upper bound of the types of the frames being unified, (2) update one frame&apos;s type to the least upper bound, and point the other&apos;s type representation to it, and (3) recurse on respective pairs of feature values."
"The frame does not need to be resized, only the types need to be ref-erenced, and in the special case of promoting the type of a single TFS to a subtype, the type only needs to be trailed."
"If cyclic TFSs are not supported, then acyclicity must also be enforced with an occurs-check."
The correctness of frames as a join-preserving en-coding of TFSs thus depends on being able to make sense of the values in these unused positions.
"The problem is that features may be introduced at join-reducible types, as in Figure 7."
"There is only one module, so the frames for a and b must have a slot available for the feature F ."
"When an a-typed TFS unifies with a b-typed TFS, the result will be of type c, so leaving the slot marked unused after recursion would be incorrect — we would need to look in a table to see what value to assign it."
An alternative would be to place that value in the frames for a and b from the beginning.
"But since the value itself must be of type a in the case of Figure 7, this strategy would not yield a finite representation."
The answer to this conundrum is to use a distin-guished circular reference in a slot iff the slot is ei-ther unused or the value it contains is (1) the most general satisfier of the value restriction of the fea-ture it represents and (2) not structure-shared with any other feature in the TFS. [Footnote_5]
"5 The sole exception is a TFS of type , which by definition belongs to no module and has no features. Its representation is a distinguished circular reference, unless two or more feature values share a single -typed TFS value, in which case one is a circular reference and the rest point to it. The circular one can be chosen canonically to ensure that the encoding is still classical."
"During unification, if one TFS is a circular reference, and the other is not, the circular reference is referenced to the other."
"If both values are circular references, then one is ref-erenced to the other, which remains circular."
"The feature structure in Figure 8, for example, has the frame representation shown in Figure 9."
"The PRD value is a TFS of type bool, and this value is not shared with any other structure in the TFS."
"If the values of MOD and PRD are both bool-typed, then if lar references[REF_CITE], and if they are not shared[REF_CITE], both of them use a different circular ref-erence[REF_CITE]."
"With this convention for circular references, frames are a classical join-preserving encoding of the TFSs of any statically typable type system."
"Al-though space does not permit a complete proof here, the intuition is that (1) most general satisfiers of value restrictions necessarily subsume every other value that a totally well-typed TFS could take at that feature, and (2) when features are introduced, their initial values are not structure-shared with any other substructure."
"Static typability ensures that value re-strictions unify to yield value restrictions, except in the final case of Theorem 1."
The following lemma deals with this case: Lemma 1
"If Approp is statically typable,  ! % , and for some F ,   F ! &quot; and   F  &quot; , then either   F ! or &quot;   F  ! $ &quot; /  F  F &quot; &quot; ."
Suppose   F !   
Proof: &quot; .
"Then   F &quot; , so ! $  F &quot; F ! &quot; . and ! and  F 7 &quot;"
F &quot; .
So there are three cases to consider: Intro F &quot; : then the result trivially holds.
"Intro F &quot; but Intro F &quot; (or by symmetry, the opposite): then we have the situation[REF_CITE]."
"It must be that  F  &quot;  ! , so by static typability, the lemma holds."
Intro F &quot; and Intro F &quot; : ! and F &quot; ! are con-sistent.
"By bounded completeness, !  F  &quot; and  !  "
F &quot;  ! .
"By upward closure,   F  # F  &quot; !!    F  # F &quot; &quot; and by static typability, ! &quot;    ! &quot; F  F &quot; &quot; ."
"Furthermore,  F &quot; ; thus by static typability the lemma holds."
This lemma is very significant in its own right — it says that we know more than Carpenter&apos;s Theo-rem 1.
An introduced feature&apos;s value restriction can always be predicted in a statically typable type sys-tem.
"The lemma implicitly relies on feature intro- duction, but in fact, the result holds if we allow for multiple introducing types, provided that all of them agree on what the value restriction for the feature should be."
"Would-be type systems that multiply in-troduce a feature at join-reducible elements (thus re-quiring some kind of distinguished-value encoding), disagree on the value restriction, and still remain statically typable are rather difficult to come by, but they do exist, and for them, a frame encoding will not work."
"In this signature, the unification: s t F d F b does not exist, but the unification of their frame en-codings must succeed because the -typed"
TFS&apos;s F value must be encoded as a circular reference.
"To the best of the author&apos;s knowledge, there is no fixed-size encoding[REF_CITE]."
"In practice, this classical encoding is not good for much."
"Description languages typically need to bind variables to various substructures of a TFS, , and then pass those variables outside the substructures of where they can be used to instantiate the value of another feature structure&apos;s feature, or as arguments to some function call or procedural goal."
"If a value in a single frame is a circular reference, we can prop-erly understand what that reference encodes with the above convention by looking at its context, i.e., the type."
"Outside the scope of that frame, we have no way of knowing which feature&apos;s value restriction it is supposed to encode."
A generalized term encoding provides an elegant solution to this problem.
"When a variable is bound to a substructure that is a circular reference, it can be filled in with a frame for the most general satis-fier that it represents and then passed out of context."
"Having more than one representative for the original TFS is consistent, because the set of representatives is closed under this filling operation."
A schematic overview of the generalized encod-ing is[REF_CITE].
"Every set of frames that encode a particular TFS has a least element, in which circular references are always opted for as introduced fea-ture values."
This is the same element as the classical encoding.
"It also has a greatest element, in which every unused slot still has a circular reference, but all unshared most general satisfiers are filled in with frames."
"Whenever we bind a variable to a substruc-ture of a TFS, filling pushes the TFS&apos;s encoding up within the same set to some other encoding."
"As a result, at any given point in time during a computa-tion, we do not exactly know which encoding we are using to represent a given TFS."
"Furthermore, when two TFSs are unified successfully, we do not know exactly what the result will be, but we do know that it falls inside the correct set of representatives because there is at least one frame with circular references for the values of every newly introduced feature."
"Simple frames with extra slots and a convention for filling in feature values provide a join-preserving en-coding of any statically typable type system, with no resizing and no referencing beyond that of type representations."
A frame thus remains stationary in memory once it is allocated.
"A generalized en-coding, moreover, is robust to side-effects such as extra-logical variable-sharing."
"Frames have many potential implementations, including Prolog terms, WAM-style heap frames, or fixed-sized records."
The paper presents an approach to ellipsis resolution in a framework of scope under-specification (Underspecified Discourse Representation Theory).
It is argued that the approach improves on previous pro-posals to integrate ellipsis resolution and scope underspecificati[REF_CITE]in that application pro-cesses like anaphora resolution do not re-quire full disambiguation but can work directly on the underspecified representa-tion.
Furthermore it is shown that the ap-proach presented can cope with the exam-ples discussed[REF_CITE]as well as a problem noted recently[REF_CITE].
Explicit computation of all scope configurations is apt to slow down an NLP system considerably.
"Therefore, underspecification of scope ambiguities is an important prerequisite for efficient processing."
"Many tasks, like ellipsis resolution or anaphora res-olution, are arguably best performed on a represen-tation with fixed scope order."
An underspecification formalism should support execution of these tasks.
"This paper aims to upgrade an existing underspec-ification formalism for scope ambiguities, Under-specified Discourse Representation Theory (UDRT)[REF_CITE], so that both ellipsis and anaphora res-olution can work on the underspecified structures."
"Several proposals have been made in the lit-erature on how to integrate scope underspecifica-tion and ellipsis resolution in a single formalism, e.g. Quasi-Logical Forms (QLF)[REF_CITE]and the Constraint Language for Lambda Structures (CLLS)[REF_CITE]."
That work has primar-ily aimed at devising methods to untangle quanti-fier scoping and ellipsis resolution which often in-teract closely (see Section 6).
"To this end, descrip-tion languages have been modelled in which the dis-ambiguation steps of a derivation need not be exe-cuted but rather can be explicitly recorded as con-straints on the final structure."
Constraints are only evaluated when the underspecified representation is finally interpreted.
"In contrast, UDRT aims at pro-viding a representation formalism that supports in-terpretation processes such as theorem proving and anaphora resolution."
"Understood in this sense, un-derspecification often obviates the need for com-plete disambiguation."
"Another consequence is, how-ever, that the strategy of postponing disambigua-tion steps is in some cases insufficient."
"A case in point is the phenomenon dubbed Missing An-tecedents[REF_CITE], illustrated in sentence (1): One of the pronoun’s antecedents is overt, the other is supplied by ellipsis resolution. (1) Harry sank a destroyer and so did Bill and they  both went down with all hands. ([REF_CITE]279)"
"Most approaches to ellipsis and anaphora resolution, e.g.[REF_CITE], can readily derive the reading."
But consider: (2) Harry sometimes reads a book about a sea-battle and so does Bill.
They borrow those books from the library.
"Example (2) still retains five readings (Are there two or even more books? are there one, two, or more than two sea-battles?)."
"An underspecified represen-tation should not be committed to any of these read-ings, but it should specify that “ a book ” has narrow scope with respect to the conjunction."
"Furthermore, an approach to underspecification and ellipsis reso-lution should make clear why this representation is to be constructed for the discourse (2)."
"While QLF fails the first requirement (a single representation), CLLS fails the second (triggers for construction). (3) *"
A destroyer went down in some battle and a cruiser did too.
Harry sank both destroyers  .
The discourse in (3) is not well-formed.
But none of the approaches mentioned can ascertain this fact without complete scope resolution (or ad-hoc re-strictions).
The paper is organized as follows.
Section 2 gives a short introduction to UDRT.
Section 3 formulates the general setup of ellipsis resolution assumed in the rest of the paper.
Section 4 presents a proposal to deal with scope parallelism in an underspecified representation.
Section 5 shows how ellipsis can be treated if it is contained in its antecedent.
Section 6 describes a way to model the interaction of ellipsis resolution and scope resolution in an underspecified structure.
In section 7 strict and sloppy identity is discussed.
Section 8 concludes.
The under-specified representations are called Underspeci-fied Discourse Representation Structures (UDRSs).
Completely specified UDRSs correspond to the Discourse Representation Structures (DRSs)[REF_CITE].
"A UDRS is a triple con-sisting of a top label, a set of labelled conditions or discourse referents, and a set of subordination constraints."
A UDRS is (partially) disambiguated by adding subordination constraints.
"A UDRS must, however, always comply with the following well-formedness conditions: (1) It does not contain cy-cles (subordination is a partial order). (2) No label is subordinated to two labels which are siblings, i.e. part of the same complex condition (subordination is a tree order)."
Figure 1 shows the UDRS for sentence 4 in formal and graph representation.
"For pronouns and definite descriptions another type of constraint is introduced, accessibility con-straints. ! #&quot; is accessible from ! %$ ( !&amp;$ acc ! #&quot; ) iff !&apos;)$ ( ! &quot; or *+!#,./- &amp;! 0$ (1! , and #! , is a right sibling of 2! &quot; in a condition expressing material implication or a generalized quantifier[REF_CITE]."
An accessibility constraint !3$ acc ! #&quot; indicates that #! &quot; is an anaphoric element or a presupposition; it thus can be used as a trigger for anaphora resolution and presupposition binding (van der[REF_CITE]).
"To bind an anaphor 2! &quot; to some antecedent expression &amp;! , , a binding constraint ( ! &quot;546 ! , ) and an equality constraint between two discourse referents are intro-duced."
Binding constraints are interpreted as equal-ity in the subordination order.
"Any unbound presup-positions remaining after anaphora resolution (cor-responding to accessibility constraints without bind-ing constraints) are accommodated, i.e. they end up in an accessible scope position which is as near to the top as possible."
Figure 2 shows the UDRS for sentence (5).
"Accessibility constraints are marked by broken lines, binding constraints are shown as squiggles. (5) John revised his paper."
The paper does not have much to say about task 1.
"Rather, some “parallelism” module is assumed to take care of task 1."
This module determines the UDRS representations of the source clause and of the source and target parallel elements.
It also pro-vides a bijective function associating the parallel labels and discourse referents in source and target.
For task 2 we adopt the substitutional approach advocated[REF_CITE]: The semantic rep-resentation of the target is a copy of the source where target parallel elements have  been substituted    for source  ).
Inparallelcontrastelementsto Higher-Order( $ $
Unification (HOU)[REF_CITE]sub-stitution is deterministic: Ambiguities somehow cropping up in the interpretation process (i.e. the strict/sloppy distinction) require a separate explana-tion.
It has frequently been observed that structural ambi-guity does not multiply in contexts involving ellip-sis: A scope ambiguity associated with the source must be resolved in the same way in source and tar-get.
"Sentence (6) e.g. has no reading where all pro-fessors found the same solution but the students who found a solution each found a different one. (6) Every professor found a solution, and most stu-dents did, too."
Scope parallelism seems to be somewhat at odds with the idea of resolving ellipses on scopally under-specified representations.
"If the decisions on scope order have not yet been taken, how can they be guar-anteed to be the same in source and target?"
The QLF approach[REF_CITE]gives an interesting answer to this question: It uses re-entrancy to prop-agate scope decisions among parallel structures.
"In sentence (6), we see that a scope decision can resolve more than one ambiguity."
"In UDRT, scope decisions are modelled as subordination constraints."
"Consequently, sentence (6) shows that subordina-tion constraints may affect more than one pair of labels."
Remember that in each process of ellipsis resolution the parallelism module returns a bijec-tive function which expresses the parallelism be-tween labels and discourse referents in source and target.
"As sentence (6) shows, a subordination con-straint that links two source labels ! $ and ! &quot; also links the labels corresponding to !3$ and ! &quot; in a parallel structure , i.e. &quot;! &amp;! $ # and $! ! #&quot; # for all ."
Thus the subordination constraint does not distinguish be-tween source label and parallel labels.
"Formally, we define two labels ! $ and ! &quot; to be equivalent ( ! &amp; $ % ! &quot; ) iff &apos;! $ # (&apos; ) * - ! &amp;! $ &quot;! ! &quot; #*&apos; ! &quot; $! &apos;! $ +# #*&apos; * ! ,/-! &quot; ! ! &amp; $ % ! - , , ! . , % ! &quot; # ."
Now we can model the par-allelism effects by stipulating that a subordination constraint connects two equivalence classes ! $ / and ! &quot; / rather than two individual labels ! $ and !#&quot; .
But every label in one class should not be linked to every label in the other class.
"If !3$ and ! &quot; are the source labels, it does not make sense, and actu-ally will often lead to a structure violating the well-formedness conditions, to connect e.g. the source label ! $ with some target label ! ! &quot; # ."
Thus we still need a proviso that only such labels can be linked that were determined to be parallel to the source la-bel in the same sequence of ellipsis resolutions.
"We talk about a sequence here, because, as sentence (7) shows, ellipses may be nested. (7) John arrived before the teacher did (1 arrive), and Bill did too (2 arrive before the teacher did (1 arrive))."
"For the implementation of classes, we take our cues from Prolog[REF_CITE]."
"In Pro-log, class membership is most efficiently tested via unification."
"For unification to work, the class mem-bers must be represented as instances of the repre-sentation of the class."
"If class members are mutually exclusive, their representations must have different constants at some argument position."
"In this vein, we can think of a label as a Prolog term whose func-tor denotes the equivalence class and whose argu-ment describes the sequence of ellipsis resolutions that generated the label."
Such a sequence can be modelled as a list of numbers which denote reso-lutions of particular ellipses.
An empty list indi-cates that the label was generated directly by se-mantic construction.
We will call the list of reso-lution numbers associated with a label the label’s context.
For reasons that will become clear only in section 7 discourse referents also have contexts.
"Although subordination constraints connect classes of labels, they are always evaluated in a particular context."
"Thus !%$ ( ! (or, more explicitly, - !&amp;$ ) can be spelled out as !%$ ( ! or ( !    ! $ , but never ! $ ( ! because in this ( ! case context changes."
"While scope resolution is subject to parallelism and binding is too (see Section 7), examples like (9) suggest that accommodation sites need not be par-allel [Footnote_1] . (“ The jeweler ” is accommodated with wide (8) A nurse saw every patient."
1[REF_CITE]use parallelism between subordination and accommodation to explain the “wide-scope puzzle” ob-served[REF_CITE]. Sentence (8) has only one reading: A specific nurse saw all patients.
"Dr. Smith did too. scope, but “ his wife ” is not.) (9) If Peter is married, his wife is lucky and the jeweler is too."
Ellipsis resolution works as follows.
"In semantic construction, all occurrences of labels and discourse referents (except those in subordination constraints) are assigned the empty context ( )."
"Whenever an occurrence of ellipsis is recognized, a counter is in-cremented."
Let be the counter’s new value.
All parallel labels ! and discourse referents in the tar-get are replaced by their counterparts in the source ( &quot;! ! #  and $!  # ).  
"After), thesubstitutionnew resolutionpropernumber( $ $ is added to the context of every label and discourse ments ( $ referent in ."
"Finally  ), if,anythe, arenon-paralleladded to thetargetseman-ele-tic representation of the target."
Figure 3 shows the UDRS for sentence (6) after ellipsis resolution.
"The example is problematic for all approaches which assume source and target scope order to be identical (HOU, QLF, CLLS). (10) John went to the station, and every student did too, on a bike."
In the approach proposed here no special adjustments are needed: The indefinite NP is designated by labels that do not have counterparts in the source.
The subordination order is still the same in source and target.
"The elliptical clause can also be contained in the source, cf. example (11)."
In this case the quantifier embedding the elliptical clause necessarily takes scope over the source.
"The treatment of this phenomenon in QLF and CLLS, which consists in checking for cyclic formulae af-ter scope resolution, cannot be transferred to UDRT, since it presupposes resolved scope."
Rather we make a distinction between proposed source and ac-tual source.
"If the target is not contained in the (proposed) source, the actual source is the proposed source."
"Otherwise, the actual source is defined to be that part of the proposed source which is potentially subordinated [Footnote_2] by the nuclear scope of the quantifier whose restriction contains the target."
2 is potentially subordinated to in a UDRS iff the subor-dination constraint could be added to the UDRS without violating well-formedness conditions.
Sentence (6) has a third reading in which the in-definite NP “ a solution ” is raised out of the source clause and gets wide scope over the conjunction.
"In this case, the quantifier itself is not copied, only the bound variables which remain in the source."
"Gen-erally, a quantifier that may or may not be raised out of the source is only copied if it gets scope in-side the source."
Thus the exact determination of the semantic material to be copied (i.e. of the source) is dependent on scope decisions.
"Consequently, in an approach working on fully specified representa-tions[REF_CITE]scope resolution can-not simply precede ellipsis resolution but rather is interleaved with it."
"In his approach, underspecified formulae are copied in ellipsis resolution."
"In such formulae, quantifiers are not expressed directly but rather stored in “q-terms”."
Q-terms are interpreted as bound variables.
Quan-tifiers are introduced into interpreted structure only when their scope is resolved.
"Since scope resolution is seen as constraining the structure rather than as an operation of its own, the QLF approach manages to untangle scope resolution and ellipsis resolution."
In CLLS[REF_CITE]no copy is made in the un-derspecified representation.
"In both approaches, the quantifier is not copied until scope resolution."
But the Missing Antecedents phenomenon (1) shows that a copy of the quantifier must be avail-able even before scope resolution so that it can serve as antecedent.
But this copy may evaporate later on when more is known about the scope configura-tion.
We will call conditions that possibly evaporate phantom conditions.
For their implementation we make use of the fact that a UDRS collects labelled conditions and subordination constraints in sets.
"In sets, identical elements collapse."
"Thus, a condition that is completely identical to another condition will vanish in a UDRS."
Phantom conditions only arise by parallelism; hence they are identical to their orig-inals but for the context of their labels and discourse referents.
"To capture the effect of possible evapora-tion, it suffices to make the update of context in a phantom condition dependent on the relevant scope decision."
"To implement phantom conditions in a Prolog-style environment, we insert a meta-variable in place of the context and control its instantiation by a special constraint expressing the dependence on the pertinent subordination constraint (a condi-tional constraint)."
"Conditional constraints have the form ! ! $ ( !  &quot;  K - K # where is the con-text variable, is a resolution number, and K is some context."
Figure 4 illustrates a UDRS with a phantom con-dition (again representing sentence (6)).
A graphical representation of this UDRS can be seen in the first conjunct of Figure 5.
"Contexts are marked by dotted boxes, conditional constraints by a dotted subordi-nation link with an equation."
"If the subsequent discourse contains a plural anaphoric NP such as “ both solutions ”, two or more discourse referents designating solutions are looked for."
"Two such discourse referents are found ( and ! # ), but they will collapse unless is set to ."
"After consultation of the conditional constraint, the subordination constraint  ! / ( ! is added."
"If the sub-sequent discourse contains a singular anaphoric NP “ the solution ”, anaphora resolution introduces the converse subordination constraint  ! / ( ! ."
"Examples involving nested ellipsis (cf. sen-tence (12)) require copying of context variables and conditional constraints. (12) Every professor found a solution before most students did, and every assistant did too."
"To copy a context variable , it is replaced by a new variable ."
The conditional constraint evaluating ( ! ! ( ! # ) is copied to a conditional con-straint evaluating .
"In this constraint is condi-tionally bound back to : ! ! ( &apos;! $ $  - # , where is the new resolution number and ! $ $ is the top label of the source."
"Consider the UDRS for sentence (12) in Figure 5 with three conditional con-straints: ! ! ( !  - # , ! ! ( &apos;! $ $  - # , and ! ! ( &amp;! $ $  # ."
"The ex- - istential NP “ a solution ” is copied three times (if ! ( ! ), once (if ! ! and ! ( !&apos;$ $ ), or not at all (if ! &apos;$ $ ). !"
"In the treatment of strict/sloppy ambiguity, we fol-low the approach[REF_CITE]which predicts five readings for the notorious example (13)[REF_CITE]. (13) John revised his paper before the teacher did, and Bill did too."
"In Kehler’s (1995) approach, strict/sloppy am-biguity results from a bifurcation in the process of ellipsis resolution: There are two ways to copy the binding constraint linking an anaphor with its antecedent if the antecedent is in the source [Footnote_3] ."
"3 If the antecedent of a pronoun is outside the source, the copied pronoun is bound to the source pronoun (strict interpretation), not directly to the antecedent, cf. the reading missing in sentence (14) in which Bill will say that Mary helped Bill before Susan helped John."
Let !&amp;$ !
K # - !
K # !
"J # , &apos;! $ !"
K # 46 !#&quot; !
J # be a binding constraint as introduced by anaphora resolution.
"The sloppy way to copy the constraint is the usual one, i.e. updating the contexts with the new resolu-tion number."
The strict way is to bind the variable of the copied pronoun to the variable of the source pro-noun.
Figure 6 shows the UDRS for a particular reading of sentence (13): John and Bill revised their own papers before the teacher revised John’s paper.
"The   ! # ), pronoun is first  copied  strict ( then  sloppy  ( ! ), and finally strict again ( # )."
We have tacitly assumed that source pronouns are resolved before ellipsis resolution.
No mechanism has been provided to propagate binding constraints in parallel structures.
But note that the order of op-erations in anaphora resolution is also constrained by structure: Anaphors embedded in other anaphors need to be resolved first (van der[REF_CITE]).
El-lipsis resolution may be considered on a par with anaphora resolution in this respect.
"Anaphors can occur in phantom conditions as well (cf. sentence (15)). (15) John revised a paper of his before the teacher did, and Bill did too."
An extension of the copy rules for binding con-straints along the lines of Section 6 is straightfor-ward (see below).
"If the embedding quantifier gets wide scope ( ! ! ), source and target constraints collapse (sloppy), or the target constraint asserts self-binding (strict). sloppy !&apos;$ ! # - ! # ! #  , &apos;! $ ! # 46 ! &quot; !  # , strict &apos;! $ ! # - ! # !"
"K # , &amp;! $ ! # 46 &amp;! $ !"
"K # ! ! ( !  K - K # , ! ! ( !  J - J #"
"There are, however, some problems with this exten-sion."
See Figure 7 for the strict-sloppy-strict read-ing of sentence (15).
"If the indefinite NP gets in-termediate scope between “ before ” and “ and ”,  the context variable will be  set to , and to  ."
"A clash follows, since !2, is bound both to ! %$ and !#, ."
"To remedy this defect, we stipulate that resolving the strict/sloppy ambiguity may partially disambiguate the scope structure: If in the course of resolving a particular ellipsis several anaphors are copied with different choices in the strict/sloppy bi-furcation, the conditional constraints are evaluated so that the anaphors cannot turn out to be the same."
This condition ensures that in the strict-sloppy-strict reading illustrated in Figure 7 the indefinite NP gets narrow scope under “ before ”.
The paper has presented a new approach to inte-grate ellipsis resolution with scope underspecifica-tion.
In contrast to previous work[REF_CITE]the proposed underspecified rep-resentation facilitates the resolution of anaphora by providing explicit representations of potential an-tecedents.
"To this end, a method to encode “phan-tom conditions” has been presented, i.e. subformu-lae whose presence depends on the scope configu-ration."
"Furthermore, a method to deal with scope parallelism in scopally underspecified structures has been proposed."
The proposed method has no trou-ble accounting for cases where the scope order in antecedent clause and elliptical clause is not entirely identical[REF_CITE].
"Finally, it has been shown that the approach can cope with a wide vari-ety of test examples discussed in the literature."
   ))-LC|^. (Lª«c¬ £{{-®M¯°SC±LL-hC¡m-- ^¡o:^§¡o¦+{¡pC ^ ¡o(- ¦ (^C¢{¢©³(mk´pµp¶:¬ ¡o·-­.(:¡¨¹I´o¶º^  { C¤:-LC|mC3»a©L¡oCaL-« ¼ ½ 2  &gt;= 0  &gt;À 0?2
Ä&gt;Í  &gt;Ä ÍÄ&gt;ÂÍ6Ô &gt;ÜÐÂÃ ÎØÄ&gt;   &gt;ÜÐÂMÃ ÎÐÄ&gt;Í ÌÆÑTò.
ÉMÈ B$   Äc×Ä  ÈmÒZÈmÔ  Ñ BC   21 SÃ(&gt;(ÌMÈWËmÆÍÉÎ^× â ý Èmòò&lt;. &gt;îoï&gt;ÎÐËIÈJÒZÄÔÖÆàÛ ÎØÄ&gt;Í$&gt;ÎÐÑ ÄÔ Û&gt;ÆÍ&apos;Î FE  ÆcÔ &gt;Ä ÂÍ&apos;É_Ã ÌMÄ&gt;Ñ¸ÈOÃªá¨ÄÖÃ   ?
ÎÐÄ&gt;é&gt;   ÂÆcÃ ÎØÄ&gt;
KMLONQP SR(TUR hé
Së í SR P N SR é  (VQN?W-XYN?[Z X RN
ÈmÃ ËcïhÎûÑ  ËIÄ&gt; Èú&gt;Ã  Ñ¸ÈmÈ \&quot;
"ËIÄ&gt;ÍÑ¸Ã¸ÔoÂËIÃ ÑtÆ_ ÆÜ  Ì&apos;ÆcÃñÎûÍMÒ°ÄÔoÅÇÆcÃ ÎØÄ&gt;  ÈmÃ1ÄÒâ &lt; ÎØÄ&gt;   îoú:ÆÍÉ,"
"Ã   ^]`Y_ )2/k`]\d, l_ )) *m f + _--+ _ ) -i ÿ  Ñoîoï ÿ  ÔoÎÐÍMÛ_ÒZÂ&apos;ÍËIÃ ÎØÄ&gt;Í1Ô   ÌMÈÖÉÎÐÑ-× ËIÄ&gt; ÈÂ&apos;ÍÎØÃ_ÎÐÍ6Æ,Õ&apos;ÆcÔ Ã ÎÐËmÂÜûÆcÔ&lt; 3ÄÙÎÐÍhÃ¸ÈmÔ E ÕÔ ÈmÃ Ã &gt;ÍMÄ&gt;ÂÍ[REF_CITE]ÎÐÄ&gt;ÍÑäÆcÔ È  ºÄ"
C &gt;ÍMÈóÃ ÌÆcÃ_Ã _Ã ÌMÈTÕ&apos;Ô Ä  SÝÙÒZÄÔ_Á ËIÔ  ÈmÒZÈmÔ &gt;ÍÈÖÒZÄÔ Ã ÆÍÉï %$ ËIÄ&gt;ÍÉÎÐÃ ÎØÄ&gt;ÍÑ(ÎÐÍ_ÆÎÐÜÐÑRÃ
ÌMÈtÑ¸ÝSÍ·Ã ÆËIÃÈmÒZÈmÔ Ñ¥ÆcÔ È ÎÐË^×  ÌMÈmÔ â  ÈmÒZÈmÔ   î?&gt;ÎûÍ·Ã &gt;ÂMÃüÃ ý  ý ÌÎØÃ¸Ã ÆcôÈmÔ â ÄÑ ÌÆcÃJÄ&gt;ÎÐÜ?ÆÍÑ¸á:ÈmÔ ÎØÄ&gt;Í   Ñ¸Ä&gt;&gt; ÆcÃ ÎÐÄ&gt;ÍÑmú Õ&apos;ÆcÔ Ã ÎûËmÂÜÐÆcÔõÆÑ¸Õ. Ñ ÄÒ(Ã ÌMÈ á .Ã &gt;Ñ &gt;ÜÐÂMÃ ÎØÄ&gt;ÍüÆÉ&apos;ÉMÔ   ÌMÈOÄÚÈmÔoÆÜÐÜ:Ñ¸Ä&gt;ÜÐÂÃ ÎØÄ&gt;ÍWÃ  .ÄÔ
Ã ÆÍhÃ) ÛÄ&gt;ÎØÄ&gt;ÈäÑ  ÈJÔ ÈmÒZÈmÔ ÌMÈmÝJá ÑüÅ:ÈmÔÂÑÈÇÍMÄÃñÈÃÓò.ÈóÆÚLÆÎûÜÐÆcò&apos;[REF_CITE]
"SÕ&apos;ÜÐÎÐËmÎÐÃ ÜØÝ -¡oC !&amp; &apos; :-Lª­C )  !( C *,  -+ /+ .10 cMpL©« C{¢|L{3»aCªõC-¡oC(¨LL©hC­¬) §¡m-L?? m¡o¥ 32 L«-¡ppcCCa )  3C¡m- L-(¦ C¢¡ ( ¡pCL­¸¸O¦(C¢F¡oC-¡oC­L ¡:C¡pCªo¢)   ¡pC{«L«-¡o¡oSC{? {L¡p·®  ÕÔ ÈmÃ  &gt;Ä ÍMÄ&gt;ÂÍ&apos;Ñmï ý  Õ  "
ÑRÄÒ3Ã   Ñ¸Ã .
"ÆËIÃ ÎØÄ&gt;Í &lt;Ä&gt;Í Ã ÌMÈ1Ã :ÆÝï  Ñ &lt;ÆÑ¸Õ. ÑÖÄÒõÃ ÌMÈóÑ Ä&gt;ÜÐÂMÃ ÎØÄ&gt; Ô  ÂÑ ÎûÍMÛÇÔ ÈmÒZÈmÔ  ÎØÄ&gt;     ÈmÒZÈmÔ  Ñmï 4 MÄ[REF_CITE]ÌMÈ Õ&apos;ÆÍhÃ ÑtÅ-ö þ &gt;_Ã ÌÈOÉÎÐÑ ËIÄ&gt;Ã ÎÐËmÎ^×   á¥ÎÐÜûÜkÃ ÆcôÈúkÃ ÌMÈOÒZÄ&gt;ÜÐÜØÄCá¥ÎÐÍMÛWÈ SËoÌÆÍMÛÈWÎÐÑ(ÃÝFÕ&apos;ÎÐËmÆÜ 21 C ,5,56 þ _&gt;ÎÐÍÈ_ Ã¸Ä ÿ  1"
ÌÄ&gt;ï 87  ÜÐÜ.ò.
ÈOÑ Î 1 ÖÕ?ïÅüï
ÉÂMÔoÆcÃ i ÿ ÎØÄÑ&gt;?ÍÇÄÒ?&gt;ÌMÈ&gt;ÍMÛc×ÉÎûÑ¸Ã ÆÍËIÈ  Ò°ÄÔ3Ã ÌMÈ & gt;ÍMÄ&gt;&gt;â ÷«ÆÑÎØÃ Ñ &gt;Ä ÈüÄ&gt;Í&apos;ÜØÝÞÆÚLÆÎûÜÐÆcò&apos;ÜØÈÒZÄ&gt;^Ü × \ îoï ? ë :9 híê ì 21 FÃ  Ì&apos;ÅÇÑÓÄÕ.ë
X PSW &lt;; R  ë &gt; ì
Fí W = !R
ÍÄÈÎØÃ&gt;ÂÍßá¥ÎØÃÌÙÃ ÌMÈTÍMÄ&gt;ÈüÃ ÌÆcÃÎÐÑ j   )î
ÌMÈmÔ:Ã ÎÐÅÇÆcÃÂÑ ÎÐÍÛOÑ¸ÝFÍhÃÎØÄ&gt;ÆËIÃ
ËIÄ&gt;ÂMÔoÑ¸È: â ¥Äò&apos;ò&apos;Ñmúâ 1 &quot; e ? &quot; ? â :Ô   ù RÆÜÐÉMá¥ÎÐÍaú 3 ÈmÔ    ù ?ÈmÃ¸Ô     ÌMÄ&gt;ÂÛ&gt;ÌJÉÎ¸Â&apos;ÉMÛ.&gt;ÎÐÍMÛóÑ  â È ÉÎ .ÎûËmÆÜ 3 ÈmÔ¸× Ô &gt;Ä  ÎØÄ&gt; ÂMÔ Èú·ËIÄ&gt; ÂMÔ Èú &quot; ÈmÃ Ëcï|   Ì&apos;ÆcÃõÆcÕM× .
ÑtÎÐÍÃ &gt;Ñ¸ÃOÑ ï A@ Õ    ÌMÈõÑ ÎØÃ Â&apos;ÆcÃ ÎØÄ&gt;._òhÝ Ã    Èú¥ÕÔ Äc× ÍMÄ&gt;ÂÍ&apos;ÑOÃ  ÈmÒZÈmÔ Ã¸ÄóÃ _
"ÑWÑ  È Ñ  ,ÎÐÍeÃ  &gt;Å ò. .ÈtÔ &gt;  ÜØÝï   Ã  &gt;Ä&gt;ÅÅäÈ(ÕÔ &gt;Ä ÍMÄ&gt;Â&apos;ÍÑ&gt;ÎÐÍ&apos;ÆÜÐÜØÝ6ÅäÄÃ Î^È× Ã ÌÆÍ,Ä&gt;_Ô ÈmÒZÈmÔ    ÎÐËÓÕÔ ÄÕ.ÈmÔ Ã ?ÝÈmÃ  &apos;ÍÉõÃ MúMÕÛMï ò&apos;ÎØÛ&gt;ÂÄ&gt;ÂÑmï Í"
È SÆÅÇÕ&apos;ÜØÈRðºÒZÔ Ä&gt;Åþ ÎÐÉ&apos;ÍMÈmÔ â  D &gt;îoúÃ  Ä&gt;ÍMÄ&gt;ÂÍ&lt;ÎÐÍ_ $ [ 21 ð
Ã ËmÆÍºÔ ÈmÒZÈmÔ &gt; ÌMÈmÔÃ ÌMÈÖÉMÄÛTÄÔõÃ_ÅäÄÔÌMÈÖò&apos;ÂÜÐÜúaò&apos;ÂMÃ
ÆÑ¨Ãþ ÎÐÉ&apos;ÍMÈmÔ«ÈÌMÈ¥Ô ÈmÒZÈmÔ  ÕÔ Ä&gt;ÍÄ&gt;  Ñ:
B .C«¸p{?aª^-£Iªo¸¥¡ C ¢·Ð ( Fo¢-L--oªL¬°L¢©¸oµ / ¸ -. t 1.   * 0 ¢¤p^¡o«©C¦(C-®¬ á¥ÎØÃIË &gt;Ä ÍhÃ¸È?    21  ÌMÈOÕÔ Ä&gt;ÍÄ&gt; ý   ÎÐÃ   æ ÷«ÆÅ&gt;Ñ .
ÎÐË e Ã¸Ô  ÜÐÝÖÆÜØÄ&gt;ÍMÛMï  &quot;   % \h hÄ h &gt;ÍMÈ  æ ( &gt;Í Ñ &gt;Â&apos;ÎÐÉMÈóÕÔ
"ÕÔÍMÄ&gt;ÂÍ&apos;ÑmúÈmÃ ÆcÃ ÎÐÄ,&gt;á¥ÌÎÐËoÌ ÉÎ .ÈmÔ"
"Å .Õ  &gt;&gt;Ä ÍÑ ÎØÚÈWÕÔ&gt;ÍMÄ&gt;ÂÍÑÄc× ÃÆÜØÄ&gt; Ô  ÎØÄ&gt;Ñ:Í&apos;ÑmïÄÔ:,)ÆÍS× 3  Ñ Ã ÌÆcÃ«á:ÈmÔ È:ÍÄÃ«ÎûÍõÑ ÂMò Q  &quot; ).Õ &gt;Ä Ñ ÎØÃ ÎØÄ&gt;Í â ÷«ÆÑ-×"
"ÅÇÆÍhÃ ÎÐËmÆÜÐÜØÝ&lt;ËIÄ&gt;&gt;ÅäÕ&apos;ÜØÈîoï þ &gt;  Ñ¸Ä&gt; 21 _(á¥ÌÎÐËoÌ&lt;Ã ÌMÈOÑ¸Õ. :ËmÆÍÍMÄÃ &gt;ÅÇÕ&apos;ÂMÃ¸È(ÆcÛÔ   ÂÔ â )Ã ÌÆcÃ áÍMÄ&gt;&gt;Ä ÂÜÐÉñò.È Ô  ? (Æ Õ.&gt;Í&apos;ÆÜcÕÔ Ä&gt;ÍMÄ&gt;ÂÍ (ð ÌÆÍS×  îoï¨ö¥ÄÃ ÎÐËIÈ(Ã  ÜØÈRÉÎ . 3 ÈmÔ :ò.ÈmÃá:  òÓÔÆºÆÍÉ Ã¸ÄõÃ ÌMÈ¥ËIÄ&gt;ÅäÕ&apos;ÜÐÈ WÄò È ñÍÆcÕ&apos;ôFÎÐÍaï  òaï &gt;ÍÑ  &gt;ÍMÄ&gt;ÂÍ,ÎÐÍ -ÑõÃ  ÎÐÚÈOÕÔ Ä&gt;ÍMÄ&gt;(Ã ÌMÈOÑ¸Õ.&gt;ÍS× \  &quot; Ä[REF_CITE]ÎÐÍMÛ \ &gt; &quot; ÂÜûÆcÔ  ÅäÄCÚÈ  .Ã¸ÄÇÃ(ÌMÈõÑÃ ÌMÈOÆcÕÕ&apos;ÜØÈtÄÎûÉMÈï &gt; ÌMÈõÍÆcÕôSÎÐÍ_ÆÍÉüÃ  N 5_-+ ÷¨ÂMÃ(Ã ÌMÈõÆcÕ&apos;Õ&apos;ÜØÈñÄ&gt;ÍüÃ ÌMÈõÍ&apos;ÆcÕôFÎûÍ&lt;ÆÍ&apos;ÉüÃ  ÅäÄCÚÈ  &amp;+ % aÃ¸ÄÓÃ + ÌMÈOÑ ÎûÉMÈï"
Z ÒZÈmÔßÃ¸Ä&amp;ÆÍ  ÎØÃÝ&gt;ÍÑ
ÍMÄÃÙÎÐÍÒZÄFËmÂ&apos;Ñ&gt;ÍMÄ&gt;â ÎÐÍÉÈú&amp;Ã¸Ä Ô ÈI×  îoï þ
ÕÔ   îÓÒZÄ&gt;ÂÍÉ8Ãÿ ÆÇÆÍÉÿ òJÉÎÂMò .ÈmÔ
Ñ_ÎÐÍhÃ¸ÈmÔ¸×ÜØÝ  ÎÐÃÝï &gt;^× Î  ÎÐÍ8Ã ÌMÈ  Ñ . aÑ¸Ã.ÆcÃÈmÃ¥ÆÜïâ .&gt;ÍÆÜÕ&apos;ÔÎûÍMÛ&gt;Ä&gt;ÂÎÐÑÍMÄ&gt;ÌMÈ &gt;Ä Å
ÃÌMÈ  ÎØÃ    .ÈI× % / -+  _
S% + e tÑ¸Ã ÆcÃ Â&apos;ÑmúMÅÇÆcÔ &lt;&gt;ÍÑ¸Ã¸ÔoÆcÃ  Bc/ ) g
ËmÆÂÑ¸È_Ã _ ÎØÄ&gt; ÌMÈüÉÎûÑ ËIÄ&gt; òÆcÔ  Ô  ÜØÝ_  ÎÐÄ&gt;.
ÌMÈmÝ6ÆcÔ È1ÎÐÍÊÃ &gt;Í·Ã¸È 21 SÃmútò&apos;ÂMÃ :&gt; ÎØÚÈÄÒ Ã &gt;ÍMÄ&gt;É&apos;Î ÃÝFÕ&apos;ÎÐËmÆÜûÜØÝò. 3
È Á ÍMÄÃÒZÔ &gt;ÄÆcò&apos;ÜÐÈÅ &amp;ÈÕÔ Ä&gt;ÍÄ&gt;ÂÍ 21 MËIÈmÕÃ ÎØÄ&gt;Í ÔÎûÑÃ&gt;ÌMÈÜÐÂMÃ ÎÐÄ&gt;Í ÅÇÆcÃÑ ÃÂÉS× 21   þÕ&apos;ÔÄ&gt;ÍMÄ.&gt;Èâ
Ì ÿ  îoú
"ÆcÔâ ÕÔ,È &gt;Ä ÍÄÜÐÆcò&gt;.É"
Ã Î Ñ % &quot; ?c JÕÔ Ä&gt;ÍÄ&gt;ÂÍÑoîoï  ÌÅ
F$  ÌMÈÓÕ&apos;Ô  ÎØÄ&gt;ÍàËIÄ&gt;Í·Ã¸È SÃ ÄÒ(Ã ÌÈÇÕÔ &gt;Ä ÍMÄ&gt;Â&apos;Í1Ã¸Ä e
"ÜÐÆcòâ .&lt;ÎØÃÞÆÑ,ÎÐÍ&apos;ËIÄ&gt;ÅäÕ&apos;ÆcÃ â"
ÍMÄ&gt;Ì  ÌMÈmÔßÆcò&apos;ÑÈmÒZÈmÔ[REF_CITE]á:MÄÔ8ÈÄ&gt;ÂÜÐÉ ò.È ÜÐÆcò.ÌMÈÕ&apos;Ô &gt;Ä ÍMÄÎÐÍÉÎØÚSÎÐÉÂ&apos;ÆÜ&gt;ÂÍ ÎÐÍ ^×ÎÐÍËIÄ&gt;ÅäÕkÆcÃ&gt;ÎØò&apos;ÜÌhÃ^Èï 21 .
ÕÔ &gt;ÄÍÄ+&gt;Ã ÌMÈeÕ&apos;Ô &gt;ÎØÄ&gt;Í+&lt;ÕÔ
ÈmÒZÈmÔ_&gt;ÍÑ _+Ô ÈmÒZÈmÔ¸×Ã ÌMÈ ý  Ñ¥ÆÍÉüÕ. &gt;Ä ÍÆÜ.ÕÔ &gt;Ä ÍMÄ&gt;Â&apos;ÍÑ:ÕÔ
È ÎØÄ&gt;Í ÄÒÄ&gt;ÍMÄ&gt;Ã ÌMÈ þ ×ÜÐÎÐÑ¸Ã&gt;ÜÐÂMÃ ÎÐÄ&gt;â þ ÎûÑèÆÍ.Èú
Ñ    ÆÜÐÜØÝÿ )Ñ &lt;&gt; ÎØÄÎØÚÈ&gt;Í 21  ÂMÔ ÈúÆÍÉ ÎûÍMÛ ÉÎûÑ ËIÄ&gt; ÂMÔ Èï : &quot;  i ÌÈõÑ  áRÆÑ) ÎØÛ&gt; Ò°ÄÔ Ñ¸Õ.  )
"ÆÍS×  ( ÌÈ¥ÉÎÐÑ ËIÄ&gt;(ËIÄ&gt;ÅÇÅäÄ&gt;&gt;ÂÍÉ ò  ÌÈmÝÊÆcÔ ,&gt;ÜÐÂS×. Ã ÎØÄ&gt; (ÆÑ Ñ  Ì&apos;ÆcÃÇÔ ÈmÒZÈmÔ  Ã ÎØÄ&gt;ÍÑ(  Ñ ÎÐÍhÃ¸ÄJÃ ÌMÈ_  Ñmú«ò&apos;ÂÃ  ÍÄÃ Ñ¸Õ.  ÌMÈ¥ÉMÈmÃ  "
ÌMÄ&gt;Ñ È¥Ò­ÂÍËIÃ ÎØÄ&gt;) ÌMÄ&gt;ÂMÛ&gt;ÌÖÍMÄÃ  Ã _: ÌÆÍÉM×Ñ ÎÐÅWÂÜÐÆcÃ ÎØÄ&gt;ÍtÄ&gt;ÍWÆ¥Ñ
"ÈmÃ3ÄÒ þ á¥ÎØÃ .&gt;Ä &gt;Ñmï ¡o 5 .¸ F Co.¡p«¢-C I ^C¢¡m?| H -Cp¦(L-L® {ªC§:¬û¦(^¡m¡p-­C -® , ) J  G (: ¦(¸L©.Rª:-£o-{o¢ o¢LL..»¡põo&apos;{C-?¡o¦({¤oc¸õ¸ -¦(C¤p- K Lm--LL¢¥o¡ C¢-?-m©Z³C- M &gt;L L o(-coC-²I-m® {R°³--ML:.© - N  ,2 ^:o « J  PO -IL oc¦(C¢-C)¡¥ L«C¸§¸{ -M-C¡p¡o(¦  C-¢£p-¡p (¦ ²I¨-m¦(--Cm|{¸¡omLL¸-tC-mp©C£p--® 6 L{ o 2 ¢¬  3 C¢ 2 {ª{£p- )"
Ðoa ( R  QS)T { U ª® 0
ÎÐÍMÛtÕ&apos;Ô  WYVQNQP Ä  R ÒZÄÔ¨ÜÐÎûÍMÛFë &gt;ÂÎÐÑ¸Ã ÎûË¨ËIÄ&gt;ÍÑ
Ã ÎØÃ  ÿÑmúRËIÄÌMÈmÔ¨Ã&gt;Í·Ã ÆÎûÍS×ÌÆÍ × Oî
J ] b` Z ] .ò
ÎÐÍMÛ&lt;ËIÄ&gt;ÍÑ¸Ã¸ËIÃ¸ËmÜÐÆÂÑÉ âÈTÃÿ ÑñÆ.ÍÉ
ÌMÈ1Ñ¸Äÿ þ&gt;ÜûÂMÃÑoîoÎØÄ&gt;ÆÍ  ÿ `
ÕÔ Ä FÝ
Ã ÎØÄ&gt; 01 ÍÑ¨ÆcÔ i ÈñÉMÈÿ ÎÐÆÉÉMÉÓÒ°ÄÔ¨Ã ÌMÕ&apos;ÜûÆï ¥ÆÑ¸ôh×ÔÈmÒ°ËIÈ¥Ò­&lt;ÂÍËp× [
"Ã ÎØÃ  ÂËoÌ, B$ ÆÑ Ã ÌÈ&lt;ËIÄ&gt;ÅäÕkÜØÈmÃ ÎØÄ&gt;ÍeÃ ÎÐÅäÈ&lt; ÌMÈ&lt; MÄÔJÎûÍÑ¸Ã ÆÍËIÈútÎØÒõÆßÑ  ._  8 &gt;ÍMÄ&gt;ÂÍäÔ?Ä&gt;ÜÐÂMÃ ÎØÄ&gt;híê ì  ÌMÈ êkéFéÔ ÈmÒZÈmÔ¸× #H 8  N é"
SR X PSW R  ÌÆcÃñÑ ÆcÃ ÎÐÑ   ÎûËõÃªÝhÕ.&gt; ÑñÆÍÉ $
ÆcÛÔ  $ ÂMÔ  ÌMÈWÕÔ Ä&gt;ÍMÄ&gt; ÈmÒZÈmÔ  Ñ ÆcÃ ÎÐÑ  $ .&gt; .È   Ñ )
ÌÔ Ä&gt;ÂMÛ&gt;Ì j  Ã ÎØÄ&gt;ÎØÃ. :ÄÔ &gt;ï &quot;! ýÎÐÃ ÌMÈ ËËmÜÐÆÂÑmÐÜ Æ ÂÑ&gt;Èú ï þ ÎØÄ&gt;ÍÑ3ÃÌÆcÃ ÕÔ ÄSÉÂËIÈÎÐÃ  j È ÍhÃ ÎØ&gt;Ä $# MÍ È ÞÉ È hÍ
Ã ØÎ Ã ÎØÈ Ñ cÆ
Ô È_¸Ñ È cÆ
Ô Ë Ì È ÉÞÎûÍeÜØmÈ
ÒZÃ-× Ò­ÂÍËIÃÆÍÇÆËmËIÈmÕÃÈmÒZÈmÔ .È(ÎØÄÄÒ&gt;.Ô
"ÄFÉ&apos;ÂËIÈ¥ÆÔ ÿ :Õ&apos;ÔÄ ÌMÈ ò&apos;ÜØÈàÑ ÌMÄCá¥ÑÇÃë ÎØÄ&gt;ÌMÈ &gt;ÍÑ  8  = N eç : #H X R  ¥Á,ÆcÃ?:á ÄÔ  [ )ÆL× i ÿ ÑÇÒ°ÄÔ Ý $ ïßÁ &amp; ñö Ã ÌÆcÃÖÕÔú ÿ ö Äc×ú ÚSÎÐÉMÈÆÍÉ &lt;&gt; ÎØò&apos;ÜØÈÖÔÿ ÈmÒZÈmÔþ ÿ ÆcÔÆÍÉÈ&lt;Äò   &gt;Ä  ÌMÈ&lt;Ô ÈmÒZÈmÔ  "
Ã :   j \&quot;  i ÆcÔ ÈõÆËIÃ ÎØÄ&gt;Í&apos;Ñmï
"Ñ ÎØÄ&gt;_Í ÎÐÑ(ÄÒ«ÃÎûÍ·Ã¸ÈmÔÌMÈOÜÐÄÛÕ&apos;Ô &gt;_ÆÑ:Ã¸Äâ Ã ÌÎÐÑ(ÎÐÑRÆÍüÆcòò&apos;Ô&gt;ÍÖÃ¸ÄWÛÈmÃ:Ã_ÚÈmÔ¸×  ÿ ÍMÛ&gt;ÎÐÍÈ bC   ! #&quot;$&amp;! %)( ! *#,+ ))! . *0/ ! 8&quot;$:! 9&lt;; ; ; =! % &gt;! ? ! @- * )/&amp;%,&lt;D ; ;"
E! + &gt;! ? ! -@*):/ +&gt;$; ; !&amp;&gt;! ? ! -@*)/ ); ; þ Ä WYP  
"Á  &lt; ÿ ËIÄ&gt;  ÍÑ !8$&quot;  E! % ()! # ÑøÃ * ,  +&quot;@ Ã¸Ä !"
EQ$ .ò ; È Æ   )ÿÁ ö ÿ ú×   ÿ  ð OúóÑ¸Äÿ þ &gt;ï Ã ÌMÈ    j   `_ I+ $ &apos;Í&apos;ÉÑ  ÿ ö ÷   á:_ÃÄ&gt;ÌMÈ_Í ?&lt; Ñ¸Äâ &gt;ÜØÚSÎÐÍMÛÈmÅÇÆÍ1ÉÎûÆÜØÄÛÆÍÉ&gt;Üû&gt;Ä úÅ  ¥Á  ö þ Ã ÌÎØÔoÉS×Õ.Èm&gt;Ä ÍWÕÔ &gt;Ä ÍM&gt;Ä Â&apos;ÍÑ
ÎÐÍä&gt;ÃîoÌMï Ñ È¥ÌÉÈWÎÐÆÅäÜØÄÛ&gt;ÆÍ&apos;ÎÐÌÍMÆÛÓÉÇÄ.ò Ò:ÈmÆÜÐÍ Ü  D   îoï `_  
"ÈmÚSÎØÄ&gt;ÂÑ(Ñ¸Ã Â&apos;ÉMÝ â :ÝFÔ Ä&gt;  ÂÜØÃÎÐÍÛ,É&apos;ÆcÃ&gt;ÍMÄ&gt;ÆÑ¸ÈmÃTËIÄÂÍÑmï &gt;ÍhÃ ÆÎÐÍÑ ÆÍÉ÷     `a  ÎÐË ÆÍÆÜØÝSÑ  "
ÎÐÑÎÐÍ Ã ÌMÈ  ÷ þ Ñ ÄôÉ&apos;ÎÐÆÜØÄÛ Ñ¸ÝSÑ¸Åüï  ÷ þ ÛÆÅÇÅÇÆcÔOÆÍÉ1ÜÐÈ
SÎÐËI&gt;Ä Í1ËIÄCÚÈmÛÈÓáRÆÑtÈ F× È Ã ÌMÈñÉÎÐÆÜÐÄÛ&gt;Ñmï -ÍOÆÉÉ&apos;ÎØÃ&gt;ÎØÄ&gt; 21 21
ÎÐÍhÃ¸ÈmÔ ) ¥ÁÙÑ  ÎÐÍMÛ(ÄÒkÍMÄÆÎÐÜÐÑ&gt;ÄÒ ÎÐÍÆÜûÑmúÃ
"ÌMÈ¨È  ÄÒ&apos;Ô (È SÕÔ  &gt;Í[REF_CITE]21 oC­». bdc cC¤oL¤RR²I³W.» oL3(^£I¸äm³W°» - WÖMC-¦((­L³mkLcoC  e , .¡õoCL¡o ¡o({C¤W¡o   L) f - J ¡o-ª3§-m¸® $&gt; hio#$rspthio# ? ?{ L-)tC{o¢¡o¤p-®"
ÚÈmÔÅÇÆÍhÃòàËIÄ&gt;ÅÓÅÇÆÍÉÎÐÍÛ&gt;&lt;_Æ  Ä&gt;ÍMÄ&gt; &gt;ËcïÑ â¥ÌMÈäÑ¸ÈI×Îï©Èï  ÑWÄ&gt;ÌMÈÍÞÕÔ_ÌÎûÑ
ÈmÚLÆÜûÂÆcÃÎÐË&lt;&lt;ÃÝFÕÆcÔ Û&gt;.ÎØÄ&lt;&gt;È ÿ .&gt;Ä
Ñ ÎØÃ &gt;ÎØÄ&gt;ÍMÄ&gt;.ÂÍäáRÆÑÈ_:ËIÄ&gt;ÈmÔÍS×È ÔËIÄ&gt;ÍÑ ÎûÉMÈmÔ  Ô  ÜØÝ ÔÌMÈüËIÄÔ&gt;Ô .ÈmÒZÈmÔÄÃ
Ì+Ã ÌMÈßËIÄÔ¸× ÃÆÍÉËmÆÜÐÜØÝÎØÄ&gt;Í &lt;á¨ÈmÔ ËIÄ&gt;&gt; ÎØÄ&gt;Ä&gt;.ÍMÄÈmÔ&gt;ÂÍÎûÍ·Ã¸ÈmÔ Õ&apos;Ô ÈmÃ&gt;ÅÓÆcÃÆcÃ ÎØÄ&gt;^ÎÍ ×  òFÝÖÃ
"ÌMÈO÷  ¥ÁøÑ¸ÄÒZÃáRÆcÔ Èï í ÌMÈmÔ&lt;ÕÔ &gt;Ä ÍMÄ&gt;ÂÍ,&gt;ÜÐÂMÃ ÎØÄ&gt;: 9 Fíì  XlP&amp;R Ã &gt;&gt; Ä&gt;ÍMÄ&gt;ÂÍ&apos;Ñ3Ã¸Ä ÂÑ¸ÈRÒ°ÄÔ¨ËIÄ&gt; &gt;Í,ÎÐÍÞÃ ÌÎÐÑWÈmÚLÆÜûÂMÃ ÎØÄ&gt; ÿ Ë ôÈmÔ ÃÖÆÍÉ þ .È Ñ ÃÂMÔ &gt;:ÍÖÌÆÍ&apos;ÉS×ÆÍÍMÄÃ_&gt;ÌÎÐÑ_ËIÄÔÕkÂÑmï á &gt;Í :ð"
ÕÔ Ä(&gt;ÍÄ&gt;ÂÍÑ â ?
ÈmÃ¸Ô .ÈmÔîoúñáRÆÑ  Èú3Ã ÌMÈ :
ÑmúMÆÍÉÇÃ¸Ô+ :Ñ &gt;&gt;ÍÜÐÝ
Ã  .&gt;ÍÆÜ:ÕÔ &gt;Ä ÍÄ&gt;ÂÍÑmï ¥Äá¥ÑÇÁ Ã ÌMÔ Ä&gt;ÂÛ&gt;Ì  aÄÒ 3Æcò&apos;ÜÐÈ % tÑ ÌMÄCáeÃ ÌÈRËIÄ&gt;&gt; ÎØÄ&gt;Í ÄÒÃ ÌMÈ ÈmÚcÆÜÐÂÆcÃâ :&gt;ÎØÄ&gt;Í ËIÄÔ ÕkÂÑüÂÑ ÎÐÍÛeÃ     &gt;Ä ÍMÄ&gt;&gt;ÜÐÂMÃ ÎØÄ&gt;Í_
Ã ÆcÔ Ã ( Ñ(Ã ÄÒ3÷ÌMÈ:Ô ÄáÙÍFÂÅ ÆcÔú&gt;ÆÍÉ
Ã ÌMÈ(ÉÎ &lt;.
Ã¸Ä á¥ÎØÃ Ì 3 ÈmÔ¸×
"Ã    & gt;ÍMÄ&gt;ÂÍÑRÎÐÍ_Ã ÌMÈõÈmÚLÆÜÐÂ&apos;ÆcÃ ËIÄÔÎØÄ&gt;Ô  ÜØÝüÔï %$ &apos;&amp;  Ä&gt; ¡oLk ) 3 &amp;  ( LL) * CC¢©³ J « ! . / T¢¦ (C©.¡oL^¡o LhL &lt; )Ö¡{to¥C{3C{-­. L  ® h¡o &lt; Ó-¢©³m¡p&gt; ¢»a£I-L  , / I¶ +  e  . ·C¡pC¡pCL&apos;{RCk. &lt; « J  ! 3I¡ c ¥Äá Ñ ÌMÄCá¥Ñ6ÃÎÐË ÃÝFÕ.È ÅÓÆcÃ  &gt; òhÝ ÕÔ .ÄÃ ÌèÕ. &gt;Ä &gt;  Äc× ÍMÄ&gt;ÂÍ&apos;Ñmï  _Ã ÌÈóÈ ."
"ÆÉ&apos;ÉÎÐÍMÛ,ÆËp× Ã  ÎÐÃ &lt;Ò°ÄÔüÍMÄ&gt; ËIÄ&gt;ÍÑ¸Ã ÎÐÃ   "
Ñ &gt;Í &gt;  21 SÕ.
Ã¸Ä ÎÐÅäÕ&apos;Ô ÆÑJáÄÚÈú:ò&apos;ÂÃ ÍMÄÃ)ÎûÍÆÜÐÜØÝúOÃÎÐËIÈ&lt;Ã ÌÈeÒ­ÂÜÐÜO÷.&gt;ÍÆÜûÑõÎÐÅäÕ&apos;Ô¥Á ÄÚÈ&lt;òhÝ ÎÐÑTÑ  ÎÐÍ  ¥Äá *
Mï    . 3 ÈmÔ   Ñ¸&gt;Ä ØÜÚ È Ñ ú¨MÍ È&gt;cÆÍÑÔ ØÜ ºÝMÉ Ä&gt;ÎØÚÈüÕÔÂ &apos;ò ÐÜ ÐÎ MÍ &gt;ÄüÛ ÍÄÃ&gt;ÌÂÍÑWËIÄÔÈ (ð&amp;Ô ò&apos;ÆÑ ÜØÝÞÔÈI×  ÌÈ&lt; _Ã¸Ä1Ã ÌMÈ Ñ ÎÐÅÇÈmÃ¸Õ&apos;ÜÐÎÐÃ Ñ¸ÎÐÃÍMÎûËóÛtÜûÅÎÐÍMò.
Ñ ÉÄÎÐÑÔ ËIÂMÄ&gt;Ã ÂMÎÐÜÐÔoÎ Ñ¸ÈàÑ¸Ã¸ËIÃ ÂMÔ Èï j
E ÎÐÍMÕ&gt;Ñ¸ÄFÉÝ  i ( ÂMÔoÆÜkÑ Ì&apos;ÎØÒ°Ã Ñ â  .ÈmÔ
"Û ÆÍ&apos;É&lt;÷¨ÎØÈmÔ¸× Ô    îoú Ñ ÌMÄ&gt;ÂÜûÉ1ÎÐÅäÕÔ ÄCÚÈÇÕ.ÈmÔ   ÄÕ.Ä&gt; :ÄÔ  &gt;Ä ñÍÈRÜûÎÐÅÇÎØÃÎÐË_ÆcÃÔ ÎØÄ&gt;Ä&gt;ÌÎÐÑ,Á ÉÄ&gt;ÅÇÆÎÐÍS×ÎÐÍ&apos;ÉMÈmÕÎÐÃ Ñ ÉMÈmÕ.. ÈmÚcÆÜÐÂÆcÃ ÎØÄ&gt;&gt;+ÜÐÆcò.  Ã ÎÐËRËIÄ&gt;ÍÑ¸Ã¸ÔoÆÎÐÍ·Ã Ñ &gt;.   â Èï©ÛMï Ã ÌMÈRÃ   -Ã ÌÆcÃ Ñ ÔoÎØÛ&gt;Ì·Ã  ÄÕ.Äc× ÛÄJÃ¸ÄTÁ¥ÚÄ&gt;Í TÆÑõÆ j Ñ ÎØÃ"
"ÎÐÄ&gt;&gt;)Á ÿÌÈTÃ× ÿ ð -Ã ÌMÈmÝ  ÎÐÑtÑ¸Õ.ÈI× ËmÎ F &apos;ËTÃ¸Ä $  ¥Á  -ö þ &gt;îoï ñÍÊÃ  ÎØÄ&gt;Í  ÌMÈJÒZÂÜûÜ:÷ÉÆcÃ Æ,á¥ÎÐÃ Ì8Ä&gt;&gt;ÅÇÆÎûÍS×ÎÐÍÉMÈmÕ¥Á .&lt;&gt; ÎÐËmÑ ÄÒäÃ(&gt;Ä ÍMÄ&gt;&gt;ÂÍÑTËIÄÔÅÇÆÎÐÍßÉÎûÑÔ ËIÄ&gt;ÎÐÍMÛºÆeÑ¸Ã ÆÍS×ÌMÈ   c {ª-:ª¸® * + +*+  , p r  . l Ltm¡o¬ sg sj^r n c &gt;® 3¡pLC-® ,* - + +*   ,  ? :&amp;9 -¡o¢ SG {  e&apos; ^ h ¹ /+   m¹   pµm® Ll e g"
SG sj g hi@  Mh Sp ^ m l &lt;p
"E r  j g SG 3® Ch  ¡ocL  ¡ppC® /O LM  ,* p + # (  / ´ ) +   ®{ /  l-"
L G ¡p \ C¡mLo gqj ®M¡o¯Z   :   So¢ / -¦¨m³  ¡pü£I¡och 8h Óp¸ _ oCC ©¡oñ3» J ¸ ` L¡ Wk pk¡o²® j- -t # _ h 5 ® ¢3¸oC-¦)C
J C¡pC-(® o ¤C 9 ® M--® * I + / ´ + ®R.C:C¡pC¬ûCL{mL-C-3.-m{¨¡o¬ &gt;g SG Pgqj  )  7 ^&apos; $o rsr # 8 S o j ¹ û ( - µ 0 !
"I H µ ED * *  H ® ¸Moo¢-¡o·LcCt¸oC·¦(® ¡p * +- L + ­ * o®   5 £I¡p(¦--«¯°^o p  &apos;  p E r   ¡p l  s 2 ) g &gt; - G s  j  r 7 n ª-  &gt; 80 { L 9L C 3  T ;  &gt; 2 l ©¬ ¡oMko¡pp , C¹L E ´¸ D ¡oh® . h® ¯° *,+/  + ® ® . ª-«{Cc¤WñcÇ® &apos; ²I---CªCL¸¤OCL¢Ðc¥¸C-©-m¡o- 8 µ ` o  , ® mqm . l  &gt; : G   j | g 4 ¤p   s 2 h  P L p £I    q m  SG  ©Z³¥  M   - l # 0 - p ® E r  j g SG oLco¤o- * ´ +   µp 5 L¡p¸L &apos;{ C *,+ - -*+ C®k * ? §coCc {C¢{¬û¡m--{ C¤¥-£I-m&apos;oLL®3F(-o¬C¬ ] ¢&apos; P ! ?-h 3® 5 ¬  5 ¬ -* * ¬ *  2 L£I©°³O¡oSLL­³¢£o 3® LL 5 ©{|L·¯oL¸® J oL *, C¡ + - po * ¹® 3 ®3L G ¯°¡m L® &quot; Lk¤(oñ³OLc-ñ¡o¦(C® -C-»aL¡o²ñ&gt; ¸¡oCk©C¡oª±L-¬ ® --mL¤®S¯° 8 7r 5 Cp® #"
"Sh _  *, r +/ @ + + sm &apos; ® * sr p ^ E r  )"
"J E2 r £I   sg N jtr SG ( sj ¢¡m¡p²ÇLo  n r $ n gqj  r  ² s; ! ªcLo¤o- V ?¸µ * ¹ +, p¹ ED ¹ /+ C® H e * D ¸µ *  e c L¤OoLWC¡o . L - ¡o .1 L * W-¡o¢C¡p·® b8 7r #"
"Ph _   $ m 9  --m¬ &gt;G Pgqj   ) g  µo´ ( H 0 ! e I . ´ ]D S p e µ . ® ® C  &apos; {¡poh¢¡o®&apos;{¤p¯°L t  Ó !&apos; 3_ [Footnote_5] ® &apos; £I-­¤I ¡p *,/+  -+ ¡Ó . {¡p¸õC 2 Cõ-¤o¦{ ({£pR &lt;p E r  l 2"
5 --C-p 5 &gt; ¬ +/. ¬ - e 2* C){£p©Z³¥¡o-c (¦ -SLL­³¢£ooLm.¡o p¦(L{ac¯°C§¡o¡o
Mg SG sj r [ n 809uT&gt;; v LLo¤p)´ .  
D -  * ® ®Cp &apos;  7 ® 8 7r  ® * # -+ + +
"Ph , _  ® h¦ r - (  ©¸ J9 m  ( &gt;  G  1  µp 1 µ û ( µ -0 ! µ ese ED µ , ® H ?­L¬ 3® &apos;   -C¸® # So8rsp   ,* p +  ´ / ¢ + |® w%  {  CL¤C® m &quot;# # 0p  r  h  r tgqj r  3® , &apos; -¤p¦L¸®( ,* - +  t *+ ® * &apos; &lt;p ) r E  {¡pC   l sg SG sj . r C- [ n   2 ]   ! ^ &quot;V pLLo¤o- V  8*/ ¸ * ¹ ED *  3® C &apos; -L¸® ¡pÓ¡o * -+ ) /+ C . ®- 5 ¡pCC¨oLO¡o­-L¡oWõtC A2 -®ÓF-LC¸o?-C¨m¬¡o 5 ¬"
Previous approaches to pronominalization have largely been theoretical rather than applied in nature.
"Frequently, such meth-ods are based on Centering Theory, which deals with the resolution of anaphoric pro-nouns."
"But it is not clear that complex the-oretical mechanisms, while having satis-fying explanatory power, are necessary for the actual generation of pronouns."
"We first illustrate examples of pronouns from vari-ous domains, describe a simple method for generating pronouns in an implemented multi-page generation system, and present an evaluation of its performance."
Pronominalization is an important element in the au-tomatic creation of multi-paragraph and multi-page texts using natural language generation (NLG).
"Au-thors routinely use pronouns in texts spanning all types of genres, such as newspaper copy, science fiction and even academic papers."
"Indeed, without pronouns, texts quickly become confusing as readers begin to pay more attention to the writing style than to the content that makes the text informative or en-joyable[REF_CITE]."
"Even worse, incorrect pronouns can lead readers to misinterpret the text or draw unsound inferences."
"Furthermore, current pronominalization strategies are ill-equipped to deal with the wide variety of reasons that pronouns are used in naturally occur-ring texts."
"Almost without exception, they focus on anaphoric pronouns as described in Focus/Centering Theory[REF_CITE], ignoring the multitude of other possible types."
"However, it is certainly true that authors make use of pronouns which are not mo-tivated by anaphoric reference."
"In addition, because such approaches are oriented towards anaphora resolution during parsing, they ig-nore structures such as the discourse plan which are present during generation but not parsing."
"A typi-cal discourse plan can include vital information for pronominalization such as time and clause bound-aries, ordering of propositions, and semantic de-tails verbal arguments."
Current approaches based on Centering algorithms thus attempt to recreate a text coherence structure that duplicates work already done by the discourse planner.
"Finally, there are significant obstacles to verifying the correctness of existing pronominalization algo-rithms for any pronominalization theory[REF_CITE]: the lack of natural language generation systems that can produce large enough texts to bring discourse-level processes into play."
"Because of this, researchers are forced to simulate by hand how their algorithms will work on a given text."
It is also not sufficient to use template generation systems to perform this task be-cause they lack the low-level discourse representa-tion needed to provide the information upon which most algorithms base their decisions.
In this paper we first summarize related work in both anaphora resolution and anaphora genera-tion.
We next describe the range of pronoun types that we found in a wide variety of texts.
We pro-ceed to describe an algorithm for determining ap-propriate pronominalizations that uses existing NLG structures and simple numeric techniques.
We also briefly describe an implemented generation system that contains enough low-level discourse informa-tion to motivate pronominalization decisions using this method.
"Finally, we quantitatively demonstrate the performance of this simple numerical approach in both a newspaper and fictional narrative domain."
"Because most NLG systems have focused on lin-guistic phenomena at the paragraph level and be-low, there has been intensive investigation into the core areas of generation that are required to pro-duce them: discourse planning, sentence planning and surface realization."
"Since pronouns are more likely to be a multiparagraph, discourse-level phe-nomenon, it has been possible to ignore their inclu-sion into working NLG systems which are not called upon to generate lengthy passages."
"Indeed, most work on pronouns in computational linguistics has come under the heading of anaphora resolution as an element of parsing rather than the heading of pronominalization as an element of gen-eration."
"Since discourse anaphora resolution was first studied theoretically[REF_CITE], it has come to be dominated by Centering Theory[REF_CITE]which proposes rules for the determination of focus and salience within a given segment of discourse."
Rel-atively little work has been done on alternate ap-proaches to pronoun resoluti[REF_CITE].
"While many NLG researchers have attempted to transfer the ideas of Centering Theory to genera-ti[REF_CITE], there has yet been no substan-tial return contribution to the field of anaphora res-olution."
There are two principal reasons for this.
"First, it is extremely difficult to create an NLG sys-tem that generates the large quantity of texts needed to exhibit discourse-level phenomena while consis-tently employing the deep linguistic representations needed to determine appropriate pronominal forms."
"Second, Centering Theory is still vague on the ex-act definition of terms such as “segment”[REF_CITE], making it difficult to create a mutually agreeable implementation."
"An additional area of NLG research that deals with pronouns is that of referring expression gen-erati[REF_CITE], which attempts to find the optimal noun phrase (whether full description, definite description, deixis, pronoun, or reduced noun phrase) to enable a reader to mentally select the intended referent from the set of possible referents[REF_CITE]."
"Comparatively, referring expression generation is a process for local disam-biguation and is not generally concerned with single phenomena spanning multiple paragraphs."
"Because of this, and because the domains and genres we have studied typically do not involve sets of very simi-lar referents, we concentrate on discourse-motivated sources of pronominalization."
"Pronominalization is the appropriate determination, marking and grammatical agreement of pronouns (he, she, their, herself, it, mine, those, each other, one, etc.) as a short-hand reference to an entity or event mentioned in the discourse."
"As with anaphora resolution, the task of a pronominalization algorithm is to correctly predict which pronoun a person would prefer in the same situation."
"The range of possibili-ties includes leaving the noun phrase as it is, reduc-ing it by removing some of its modifiers, or replac-ing it with a pronoun construction."
Our corpora analyses have identified a number of motivations for converting nouns into pronouns: 1.
"Anaphoric pronouns: These are the most-studied cases of pronoun occurrences, which sequentially follow a specific entity known as the referent."
"Anaphors are divided into two classes, short-distance (within the same sen-tence) and long-distance (previous sentences)."
"But John i had never been to New Orleans, and he i couldn’t remember if anyone in his i family had either. 2."
"According[REF_CITE], cataphors are those pronouns which occur before their referents in the linear flow of text within the same sentence, where the pro-noun is either at a lower structural level or is part of a fronted circumstantial clause or prepo-sitional phrase which could have appeared after the reference."
"Additionally, this category could include clefting pronouns."
"Before he i joined the navy, Gerald i made peace with his family. 3. Pronouns Lacking Textual Antecedents: This category includes document deixis (via a demonstrative pronoun), authorial or reader reference, and situational pronouns."
This is the first document to show . . .
We discuss these strategies in the next section.
The group had never seen anything like it. 4.
Reflexive and Reciprocal Pronouns: Most verbs use special pronouns when the subject and object corefer.
A discourse history algo-rithm can employ that knowledge to mark re-flexive and reciprocal pronouns appropriately.
Kittens i often watch themselves i in mirrors.
Baby lions j tackle each other j when playing. 5.
Partitive pronouns: It is important to know con-ceptually what it is that the pronoun is trying to replace.
"Otherwise, it becomes impossible to achieve the types of pronominalizations that authors are routinely capable of creating."
This requires accurate information in the knowledge base or linguistic structure from which the sen-tences are derived.
"As the horses ran by, she roped one. *"
"As the horses ran by, she roped it. *"
"As the horses ran by, she roped them."
"In addition to these motivations, we identified several factors that prevent pronouns from occurring where they otherwise might: 6. Pronouns across boundaries: After a chapter, section or other obvious boundary, such as a change in time, place, or both, as[REF_CITE], authors will typically “re-set” pronominalization just as if it were the beginning of the entire text."
"Antecedent ref-erences that break these boundaries are some-times marked by the authors in academic texts: As we saw in the previous section, . . . 7."
"Restrictions from modifiers: Because pronouns cannot have modifiers like nouns, adding an ad-jective, relative clause, or some other modifier prevents a noun from being replaced by a pro-noun."
The mayor had already read the full proposal. * The mayor had already read the full it. 8.
"Focused nouns: Especially after a vocally stressed discourse marker[REF_CITE]or some other marked shift in topic, a word that normally would be pronominalized is often not, as in this example: . .. and you frequently find that mice occupy an important part of the modern medical labo-ratory."
"In other words, mice are especially nec-essary for diagnosing human cancers . . . 9."
"Semantic and syntactic considerations: A small number of semantic relations and syntac-tic constructions prohibit pronominalization: * The stranger was just called him. (Bob) * Roberta was no longer a her. (child) * The father, a tyrant of a him, . . . (man) 10."
Optional pronominalization: Often there are borderline cases where some authors will use pronouns while others won’t.
"A single algo-rithm may be tuned to match a particular au-thor’s style, but parameterization will be nec-essary to match a variety of styles."
Thus it is extremely difficult to exactly match any partic-ular text without having the ability to adjust the pronominalization algorithm.
"Pronominalization occurs equally as often in ex-position as in dialogue, but dialogue can have slightly different pronominalizations depending on the relationship between the utterer and the hearer: 11."
"Speaker self-reference: “John thinks John will go find John’s shoes,” John said. changes to first person singular pronouns: “I think I will go find my shoes,” John said. 12."
"Speaker references hearer(s): “Mary should go find Mary’s shoes,” John said. changes to second person pronouns: “You should go find your shoes,” John said. 13."
"Reference to speaker and hearer (or to speaker and a third party): “John and Mary should go find John and Mary’s shoes,” John said. changes to first person plural pronouns: “We should go find our shoes,” John said. 14."
"Reference to a third party: “Bob and Mary went to eat Bob and Mary’s breakfast,” John said. changes to third person plural pronouns: “They went to eat their breakfast,” John said. 15."
"Finally, the treatment of pronouns differs de-pending if they are inside or outside of the di-rect quotation."
"For example: “Oh man, I forgot to eat my breakfast!”"
John muttered to himself while grabbing his shoes.
"Although this enumeration is surely incomplete, it provides a basic description of the types of phe-nomena that must be handled by a generation system in order to produce text with the types of pronouns found in routine human-produced prose."
"In order to correctly account for these phenomena during generation, it is necessary to have detailed information about the underlying discourse struc-ture."
"Although a template generation system could be augmented to record this information, in practice only deep structure, full-scale NLG systems have the requisite flexibility."
"Because a pronominalization al-gorithm typically follows the discourse planner, it frequently has access to the full discourse plan."
"A typical discourse plan is a tree structure, where internal nodes represent structuring relations while leaf nodes represent individual sentential elements that are organized semantically."
"In addition, the ele-ments of the discourse tree are typically rooted in the semantic knowledge base which the discourse plan-ner drew from when constructing the discourse plan."
The discourse plan supplies the following informa-tion that is useful for pronominalization:
The sequencing information stored in the discourse tree can be used to mo-tivate anaphoric and cataphoric pronouns as shown in items 1 &amp; 2 of Section 3.
"Semantic Structure: The original subgraphs (or semantic subnetworks) derived from the knowledge base can motivate content vs. sit-uational knowledge (item 3) reflexive and re-ciprocal pronouns via argument lists (item 4), partitive pronouns (item 5), and the existence of NP modifiers (item 7), and can identify se-mantic types in relations (item 9)."
"Discourse Structure: The rhetorical relations that hold between different sentences typically imply where section boundaries are located (item 6), indicate what types of discourse mark-ers are employed (item 8), and in the case of dialogue, know which actors are speaking, lis-tening, or not present (items 11-15)."
"This detailed knowledge of the discourse is avail-able to an implemented pronominalization compo-nent utilizing any theory, including Centering the-ory."
We thus now turn our attention to what role this information plays in a pronominalization algorithm.
"At an abstract level, the pronominalization algo-rithms derived from Centering theory are easily ex-pressed: if Centering theory predicts a pronoun would be used in anaphora resolution in a given seg-ment of text, then generate the appropriate pronoun."
"While this works for many cases of anaphoric pro-nouns [84.7%[REF_CITE], 87- 90%[REF_CITE]], we have seen that these form only a subset of the potential reasons for pronominalization."
"Furthermore, this approach as-sumes that the discourse tree was constructed with Centering theory in mind. while LNE 6 = do NE ( first ( LNE ) if[REF_CITE]SEEN then reset - counters ( NE ) , else updateSEEN - counters ( SEEN ( NE ) NE D ( updateDialogueState () RS ( updateLocalRhetoricalStructure () if ( topic - shift _ time - shift ) 2 RS then SC ( SC + 10 else if modifiers ( NE; RS ) = ^ ( special - relation _ appositive ) 62 RS if D == QuotedDialogue then mark ( quoted - pronoun ( NE; RS )) else if subject - matches - object ( NE; RS ) then mark ( ReflexivePronoun ) else if sent - distance ( NE; SC ) = 0 then mark ( MultipleInSentencePronoun ) else if 3 &lt; = sent - distance ( NE; SC ) &lt; 1 and nominal - distance ( NE ) &lt; 3 then mark ( LongDistancePronoun ) , else if recency ( NE ) &gt; 3 then mark ( ShortDistancePronoun ) , LNE ( remove - first ( LNE ) ; SC ( SC + 1"
"However, it is not clear that Centering theory itself is necessary in generation, let alone its accompany-ing algorithms and data structures."
"Because Cen-tering theory is typically applied to parsing (which starts with no discourse tree), it may not be the most efficient technique to use in generation (which has a complete discourse tree available for inference)."
"Instead, we attempted to determine if the informa-tion already present in the discourse tree was enough to motivate a simpler algorithm based on the follow-ing available data:"
"Ordered sequence of nominal elements: Be-cause the discourse tree is linearized and in-dividual leaves of the tree annotate which ele-ments have certain semantic roles, a very good guess can be made as to which nominal ele-ments precede others at the clause level."
Known paragraph and sentence boundaries: Analysis of the rhetorical structure of the dis- course tree allows for the determination of boundaries and thus the concept of metric dis-tance between elements.
Rhetorical relations: The rhetorical relations can tell us which nominal elements follow dis-course markers and which are used reflexively or reciprocally.
"Dialogue: By recording the participants in dia-logue, the discourse tree allows for the appro-priate assignment of pronouns both inside and outside of the direct quote itself."
"The algorithm we developed considers the cur-rent discourse leaf node and the rhetorical structure above it, and also makes use of the following data:"
Nominal element distance: How many total (non-distinct) nominal elements ago a particu-lar element was last used.
Recency: How many distinct nominal elements have been seen since its last use.
Sentential distance: How many sentences (pro-totypical clauses) have appeared since the last usage of this nominal element.
"The algorithm itself (Figure 1) is best character-ized as a counting method, that is, it loops once through the linearized list of nominal elements and makes pronominalization decisions based on the lo-cal information described above, and then updates those numerical counters."
"Numerical parameters (e.g., recency ( NE ) &gt; 3 ) are derived from empir-ical experimentation in generating multi-page prose in a narrative domain."
"While it lacks the explanatory power of a rela-tively mature linguistic theory, it also lacks the ac-companying complexity and is immediately appli-cable to real-world deep generation systems."
"The al-gorithm is traced in Figure 2, although due to space limitations some phenomena such as dialogue, long distance and reflexive pronouns are not shown."
"S TORY B OOK ([REF_CITE]; Call-away and Lester, in press) is an implemented nar-rative generation system that converts a pre-existing narrative (discourse) plan into a multi-page fic-tional narrative in the fairy tale domain."
"Using a pipelined generation architecture, S TORY B OOK per-forms pronominalization before sentence planning, and includes a revision component that is sensitive to pronominalization choices during clause aggre-gation."
A previous large-scale evaluation of S TORY - B OOK[REF_CITE]which included both a full version and a version with the pronomi-nalization component ablated showed that including such a component significantly increases the quality of the resulting prose.
"However, there are significant practical obstacles to comparing the performance of different pronomi-nalization algorithms using corpus matching criteria instead of “quality” as evaluated by human judges."
"Because systems that can handle a large quantity of text are very recent and because it can require years to create and organize the necessary knowledge to produce even one multi-paragraph text, much re-search on anaphora generation has instead relied on one of two techniques:"
"Checking algorithms by hand: One verification method is to manually examine a text, identify-ing candidates for pronominalization and simu-lating the rules of a particular theory."
"However, this method is prone to human error."
"Checking algorithms semiautomatically: Other researchers opt instead to annotate a corpus for pronominalization and their antecedents as well as the pronoun forms that should occur, and then simulate a pronominalization algo-rithm on the marked-up text[REF_CITE]."
"Similarly, this approach can suffer from interannotator agreement errors[REF_CITE]."
"To verify our pronominalization algorithm more rigorously, we instead used the S TORY B OOK deep generation system to recreate pre-existing multi-page texts with automatically selected pronouns."
"Without a full-scale implementation, it is impossible to determine whether an algorithm performs imper-fectly due to human error, a lack of available corpus data for making decisions, or if it is a fault with the algorithm itself."
"Using the algorithm described in Figure 1, we modified S TORY B OOK to substitute the types of pronouns described in Section 3."
We then created the discourse plan and lexicon necessary to generate the same three articles from the New York Times[REF_CITE].
"The results for both the newspaper texts and the Little Red Riding Hood nar-rative described in (Callaway and Lester, in press) are shown in Table 1."
"With the same three texts from the New York Times, S TORY B OOK performed better than the pre-vious reported results of 85-90% described[REF_CITE]on both animate and all anaphora using a corpus matching technique."
"Furthermore, this was obtained solely by adjusting the recency parameter to 4 (it was 3 in our narrative domain), and without considering other en-hancements such as gender/number constraints or domain-specific alterations. [Footnote_1]"
"1 It is important to note, however, that our counts of pronouns and antecedents do not match theirs. This may stem from a vari-ety of factors, such as including single instances of nominal de-scriptions, whether dialogue pronouns were considered, and if borderline quantifiers and words like “everyone” were counted. The generation community to-date has not settled on standard, marked corpora for comparison purposes as has the rest of the computational linguistics community."
Pronominalization is an important element in the au-tomatic creation of multi-paragraph and multi-page texts.
"Previous approaches, based largely on theo-retical approaches such as Centering Theory, deal exclusively with anaphoric pronouns and have com-plex processing and definitional requirements."
"Given the full rhetorical structure available to an implemented generation system, we devised a sim-pler method of determining appropriate pronom-inalizations which was more accurate than exist-ing methods simulated by hand or performed semi-automatically."
"This shows that approaches designed for use with anaphora resolution, which must build up discourse knowledge from scratch, may not be the most desirable method for use in NLG, where discourse knowledge already exists."
"The positive re-sults from our simple counting algorithm, after only minor changes in parameters from a narrative do-main to that of newspaper text, indicates that future high-quality prose generation systems are very near."
We would like to thank Michael Young and Renate Henschel for their helpful comments; Kathy McCoy very quickly provided the original 3 NYT articles upon request; the anonymous reviewers whose com-ments greatly improved this paper.
Support for this work was provided by ITC-irst and the IntelliMedia Initiative of North Carolina State University.
The incremental algorithm introduced[REF_CITE]for producing dis-tinguishing descriptions does not always generate a minimal description.
"In this paper, I show that when generalised to sets of individuals and disjunctive proper-ties, this approach might generate unnec-essarily long and ambiguous and/or epis-temically redundant descriptions."
"I then present an alternative, constraint-based al-gorithm and show that it builds on existing related algorithms in that (i) it produces minimal descriptions for sets of individu-als using positive, negative and disjunctive properties, (ii) it straightforwardly gener-alises to n-ary relations and (iii) it is inte-grated with surface realisation."
"In English and in many other languages, a possible function of definite descriptions is to identify a set of referents [Footnote_1] : by uttering an expression of the form The N, the speaker gives sufficient information to the hearer so that s/he can identify the set of the objects the speaker is referring to."
1 The other well-known function of a definite is to inform the hearer of some specific attributes the referent of the NP has.
"From the generation perspective, this means that, starting from the set of objects to be described and from the properties known to hold of these objects by both the speaker and the hearer, a definite de-scription must be constructed which allows the user to unambiguously identify the objects being talked about."
"While the task of constructing singular definite descriptions on the basis of positive properties has received much attention in the generation literature[REF_CITE], for a long time, a more general statement of the task at hand re-mained outstanding."
"Recently however, several pa-pers made a step in that direction. (van[REF_CITE]) showed how to extend the basic Dale and Re-iter Algorithm[REF_CITE]to generate plural definite descriptions using not just conjunc-tions of positive properties but also negative and disjunctive properties;[REF_CITE]integrates the D&amp;R algorithm into the surface realisation process and[REF_CITE]extends it to deal with collective and distributive plural NPs."
"Notably, in all three cases, the incremental struc-ture of the D&amp;R’s algorithm is preserved: the al-gorithm increments a set of properties till this set uniquely identifies the target set i.e., the set of ob-jects to be described."
"As[REF_CITE]shows, such an incremental algorithm while be-ing polynomial (and this, together with certain psy-cholinguistic observations, was one of the primary motivation for privileging this incremental strategy) is not guaranteed to find the minimal solution i.e., the description which uniquely identifies the target set using the smallest number of atomic properties."
"In this paper, I argue that this characteristic of the incremental algorithm while reasonably innocuous when generating singular definite descriptions using only conjunctions of positive properties, renders it cognitively inappropriate when generalised to sets of individuals and disjunctive properties."
I present an alternative approach which always produce the min-imal description thereby avoiding the shortcomings of the incremental algorithm.
I conclude by com-paring the proposed approach with related proposals and giving pointers for further research.
Dale and Reiter’s incremental algorithm (cf.
"Fig-ure 1) iterates through the properties of the target entity (the entity to be described) selecting a prop-erty, adding it to the description being built and com-puting the distractor set i.e., the set of elements for which the conjunction of properties selected so far holds."
The algorithm succeeds (and returns the se-lected properties) when the distractor set is the sin-gleton set containing the target entity.
It fails if all properties of the target entity have been selected and the distractor set contains more than the target entity (i.e. there is no distinguishing description for the target).
"This basic algorithm can be refined by ordering properties according to some fixed preferences and thereby selecting first e.g., some base level category in a taxonomy, second a size attribute third, a colour attribute etc.   : the domain; , the set of properties of ;"
"To generate the UID , do: 1."
"Initialise: := , := . (van[REF_CITE]) generalises the D&amp;R algo-rithm first, to plural definite descriptions and second, to disjunctive and negative properties as indicated in Figure 2."
"That is, the algorithm starts with a dis-tractor set + which initially is equal to the set of individuals present in the context."
"It then incremen-tally selects a property , that is true of the target set ( -/.1020 4323, ) but not of all elements in the distrac-tor set ( +1.56020 ,7323 )."
Each selected property is thus used to simultaneously increment the description be-ing built and to eliminate some distractors.
Success occurs when the distractor set equals the target set.
"The result is a distinguishing description ( DD , a de-scription that is true only of the target set) which is the conjunction of properties selected to reach that state."
"&lt;; , the set to be described; 8 , the properties true of the set ( ?= ;&gt; @CA B ; = A&gt; with = A&gt; the set of properties that are true of ); ;"
"To generate the distinguishing description , do: ; 1."
"Initialise: := , := . 



"
"Phase 1: Perform the extended D&amp;R algorithm using all liter-als i.e., properties in &gt;MLON ; if this is successful then stop,otherwise go to phase 2."
"Phase 2: Perform the extended  D&amp;R algorithm  using all prop-with % successful then stop, otherwise go to phase MLON&gt; 3. ; if this is erties of the form"
"To generalise this algorithm to disjunctive and negative properties, van Deemter adds one more level of incrementality, an incrementality over the length of the properties being used (cf."
"First, literals are used i.e., atomic properties and their negation."
"If this fails, disjunctive properties of length two (i.e. with two literals) are used; then of length three etc."
We now show that this generalised algorithm might generate (i) epistemically redundant descriptions and (ii) unnecessarily long and ambiguous descrip-tions.
Epistemically redundant descriptions.
Suppose the context is as illustrated in Figure 4 and the target set is SUTWVUXYT[]Z \ .
"To build a distinguishing description for the tar-get set SUTWVUXYT[Ze\ , the incremental algorithm will first look for a property , in the set of literals such that (i) SUTWVUXYT[Ze\ is in the extension of P and (ii) , is not true of all elements in the distractor set + (which at this stage is the whole universe i.e., SUT V XYT Z XYT[f]XYT[#g XYT[[]i \ )."
Two literals satisfy these criteria: the property of being a board mem-ber and that of not being the treasurer [Footnote_2] Suppose the incremental algorithm first selects the board-member property thereby reducing the distractor set to SUT V XYT Z #XYTjh]\ .
2 Note that selecting properties in order of specificity will not help in this case as neither president nor treasurer meet the selection criterion (their extension does not include the target set).
Then l treasurer is selected which restricts the distractor set to  g XYT h \ .
"There is no other literal which could be used to fur-ther reduce the distractor set hence properties of the form ,/no7p, are used."
"At this stage, the algo-rithm might select the property q[rtsunIv]wUxCy whose intersection with the distractor set yields the target set SUT V XYT Z \ ."
"Thus, the description produced is in this case: board-member {z l treasurer }z |~q[r ]wUxCyt which can be phrased as the president and the sec-retary who are board members and not treasurers – whereas the minimal DD the president and the sec-retary would be a much better output."
"One problem thus is that, although perfectly well formed minimal DD s might be available, the incre-mental algorithm may produce “epistemically re-dundant descriptions” i.e. descriptions which in-clude information already entailed (through what we know) by some information present elsewhere in the description."
Unnecessarily long and ambiguous descriptions.
Another aspect of the same problem is that the al-gorithm may yield unnecessarily long and ambigu-ous descriptions.
Here is an example.
Suppose the context is as given in Figure 5 and the target set is SUT h XYT i XYT[]#\ .
W D C B S M Pi Po H J ^ _ ` _ _ #ab _ _ _ _ _ #cd _ _ _ _ _ _ _ _ # _ _ _ _ _ _ # _ _ _ _
"The most natural and probably shortest descrip-tion in this case is a description involving a disjunc-tion with four disjuncts namely , , which can be verbalised as the Pitbul, the Pooddle, the Holstein and the Jersey."
"This is not however, the description that will be returned by the incremental algorithm."
"Recall that at each step in the loop going over the proper-ties of various (disjunctive) lengths, the incremen-tal algorithm adds to the description being built any property that is true of the target set and such that the current distractor set is not included in the set of objects having that property."
"Thus in the first loop over properties of length one, the algorithm will select the property  , add it to the descrip-tion and update the distractor set to +020E323  f XYT g XYT h XYT i XYT&apos;]XYT[] XYT[]\ ."
"Since the new distractor set is not equal to the target set and since no other property of length one satisfies the selection criteria, the algorithm proceeds with properties of length two."
"Figure 6 lists the prop-erties , of length two meeting the selection cri-teria at that stage ( SUT h XYT i XYT[] ] \020,4323 and SUT V XYT Z XYT[]f XYTkg#[  XYT  XYT "
"XYT  \ 5. 020,4323 ."
The incremental algorithm selects any of these properties to increment  n¢+ .theThecurrent
DD is DD then.
Sup-up-pose it selects  n+¤ and the distractor set todated to  z£| SUT f XYT g
XYT h XYT i XYT&apos;]XYT[]\ .
"Except for ¢n¥+ and lR6n which would not eliminate any dis-tractor, each of the other property in the table can be used to further reduce the distractor set."
Thus the algorithm  ¨&apos;z©+ will |$ eventually {{- &apos;z©| build the descriptionthereby re- ¦z§| ducing the distractor set to [  XYT  XYT  \ .
At this point success still has not been reached (the distractor set is not equal to the target set).
"It will eventually be reached (at the latest when incrementing the description with the disjunction , nR¬n  )."
"However, already at this stage , of processing, it is clear that the resulting descrip-from the description built so far (  tion will be awkward to phrase."
"A direct translation  n®+¤{zz­| $| {-{¯z°| lR£ ) would yield e.g., (1) The white things that are big or a cow, a Hol-stein or not small, and a Jersey or not medium size"
"Another problem then, is that when generalised to disjunctive and negative properties, the incremen-tal strategy might yield descriptions that are unnec-essarily ambiguous (because of the high number of logical connectives they contain) and in the extreme cases, incomprehensible."
One possible solution to the problems raised by the incremental algorithm is to generate only minimal descriptions i.e. descriptions which use the smallest number of literals to uniquely identify the target set.
"By definition, these will never be redundant nor will they be unnecessarily long and ambiguous."
"As[REF_CITE]shows, the problem of finding minimal distinguishing descriptions can be formulated as a set cover problem and is there-fore known to be NP hard."
"However, given an effi-cient implementation this might not be a hindrance in practice."
"The alternative algorithm I propose is therefore based on the use of constraint program-ming (CP), a paradigm aimed at efficiently solving NP hard combinatoric problems such as scheduling and optimization."
"Instead of following a generate-and-test strategy which might result in an intractable search space, CP minimises the search space by following a propagate-and-distribute strategy where propagation draws inferences on the basis of effi-cient, deterministic inference rules and distribution performs a case distinction for a variable value."
The basic version.
Consider the definition of a distinguishing description given[REF_CITE].
"Let y be the intended referent, and + be the distractor set; then, a set ± of attribute-value pairs will represent a distinguishing description if the following two conditions hold:"
"C1: Every attribute-value pair in ± ap-plies to y : that is, every element of ± specifies an attribute value that y possesses."
"For every member x of + , there is at least one element ² of ± that does not apply to x : that is, there is an ± in ± that specifies an attribute-value that x does not possess. ² is said to rule out x ."
The constraints (cf. Figure 7) used in the pro-posed algorithm directly mirror this definition.
"A description for the target set - is represented by a pair of set variables constrained to be a subset of the set of positive(i.e., properties that are true of all elements in - ) and of negative (i.e., properties that are true of none of the elements in - ) properties ³ ´¨µ : the universe; :¶· ´ ¶ : the ´¸ set [´¨µ of ¶ :propertiesthe set of properties T has; T does not have; ´ ¹µ  º ¹ ´ ¶µ : the set of properties true of all ele-ments ´ ¹·  of ´¸¬»- ;   ¹ ´¨µ¶ : the set of properties false of all elements of µ - ; · ¹ ½¼$, ¹ , ¹:¾ is a basic distinguishing descrip-tion for S iff: 1. , ¹µ . ´ ¹µ , 2. , ¹· . ´ ¹· and 3. ¿&apos;x©+ ¹ XeÀÁ$| , ¹µ ¸Â´¨µÃ  » $| , ¹·  ¨´ Ãµ KÀ(Ä­Å of - respectively."
The third constraint ensures that the conjunction of properties thus built eliminates all distractors i.e. each element of the universe which is not in - .
"More specifically, it states that for each distractor x there is at least one property , such that either , is true of (all elements in) - but not of x or , is false of (all elements in) - and true of x ."
The constraints thus specify what it is to be a DD for a given target set.
"Additionally, a distribution strategy needs to be made precise which specifies how to search for solutions i.e., for assignments of values to variables such that all constraints are si-multaneously verified."
"To ensure that solutions are searched for in increasing order of size, we distribute the output description À , ¹ (i.e. make case distinctions µ ) » over , ¹· À thestartingcardinalitywith theof lowest possible value."
"That is, µ first · the algorithm will try to find a description $¼ , ¹ , ¹ ¾ with cardi-nality one, then with cardinality two etc."
The algo-rithm stops as soon as it finds a solution.
"In this way, the description output by the algorithm is guaranteed to always be the shortest possible description."
Extending the algorithm with disjunctive prop-erties.
"To take into account disjunctive properties, the constraints used can be modified as indicated in Figure 8. that their union -ÆV That is, the algorithm »ÇKÇKÇ] looks » -jÈ foris thea tupletargetofsetsets - suchand such that for each set -jÉ in that tuple there is a basic ¹  ¹ ^ n ÇKÇKÇ n ¹eÊ is a distinguishing descrip-tion for a set of individuals - iff:"
DD ¹ .
The resulting description is the disjunctive description ¹ ^ n ÇKÇKÇ n© ]¹ Ê where each ¹ is a conjunctive description.
"As before solutions are searched for in increasing order of size (i.e., number of literals occurring in the description) by distributing over the cardinality of the resulting description."
"Integration with surface realisation As[REF_CITE]clearly shows, the two-step strat-egy which consists in first computing a DD and sec-ond, generating a definite NP realising that DD , does not do language justice."
"This is because, as the fol-lowing example[REF_CITE]il-lustrates, the information used to uniquely identify some object need not be localised to a definite de-scription. (2) Remove the rabbit from the hat."
"In a context where there are several rabbits and several hats but only one rabbit in a hat (and only one hat containing a rabbit), the sentence in (2) is sufficient to identify the rabbit that is in the hat."
"In this case thus, it is the presupposition of the verb “re-move” which ensures this: since x remove y from z presupposes that Ô was in Õ before the action, we can infer from (2) that the rabbit talked about is indeed the rabbit that is in the hat."
The solution proposed[REF_CITE]and implemented in the SPUD (Sentence Plan-ning Using Descriptions) generator is to integrate surface realisation and DD computation.
"As a prop-erty true of the target set is selected, the correspond-ing lexical entry is integrated in the phrase structure tree being built to satisfy the given communicative goals."
Generation ends when the resulting tree (i) satisfies all communicative goals and (ii) is syntac-tically complete.
"In particular, the goal of describ-ing some discourse old entity using a definite de-scription is satisfied as soon as the given informa-tion (i.e. information shared by speaker and hearer) associated by the grammar with the tree suffices to uniquely identify this object."
"Similarly, the constraint-based algorithm for generating DD presented here has been inte-grated with surface realisation within the generator I N D I G EN[URL_CITE]cl/projects/indigen.html) as follows."
"As in SPUD , the generation process is driven by the communicative goals and in particular, by in-forming and describing goals."
"In practice, these goals contribute to updating a “goal semantics” which the generator seeks to realise by building a phrase structure tree that (i) realises that goal seman-tics, (ii) is syntactically complete and (iii) is prag-matically appropriate."
"Specifically, if an entity must be described which is discourse old, a DD will be computed for that en-tity and added to the current goal semantics thereby driving further generation."
"Like SPUD , this modified version of the SPUD al-gorithm can account for the fact that a DD need not be wholy realised within the corresponding NP – as a DD is added to the goal semantics, it guides the lex-ical lookup process (only items in the lexicon whose semantics subsumes part of the goal semantics are selected) but there is no restriction on how the given semantic information is realised."
"Unlike SPUD however, the I N D I G EN generator does not follow an incremental greedy search strat-egy mirroring the incremental D&amp;R algorithm (at each step in the generation process, SPUD compares all possible continuations and only pursues the best one; There is no backtracking)."
It follows a chart based strategy instead[REF_CITE]producing all possible paraphrases.
The drawback is of course a loss in efficiency.
The advantages on the other hand are twofold.
"First, I N D I G EN only generates definite descrip-tions that realize minimal DD ."
"Thus unlike SPUD , it will not run into the problems mentioned in section 2 once generalised to negative and disjunctive prop- erties."
"Second, if there is no DD for a given entity, this will be immediately noticed in the present approach thus allowing for a non definite NP or a quantifier to be constructed instead."
"In contrast, SPUD will, if unconstrained, keep adding material to the tree until all properties of the object to be described have been realised."
"Once all properties have been realised and since there is no backtracking, generation will fail."
The set variables used in our con-straints solver are variables ranging over sets of in-tegers.
"This, in effect, means that prior to applying constraints, the algorithm will perform an encoding of the objects being constrained – individuals and properties – into (pairwise distinct) integers."
It fol-lows that the algorithm easily generalises to n-ary relations.
"Just like the proposition red(  ) using the unary-relation “red” can be encoded by an integer, so can the proposition on( w V X w Z ) using the binary-relation “on” be encoded by two integers (one for on( X wUZ ) and one for on( w#V¡X )."
Thus the present algorithm improves on (van[REF_CITE]) which is restricted to unary rela-tions.
"It also differs[REF_CITE], who use graphs and graph algorithms for computing DD s – while graphs provides a transparent encoding of unary and binary relations, they lose much of their intuitive appeal when applied to relations of higher arity."
"It is also worth noting that the infinite regress problem observed[REF_CITE]to hold for the D&amp;R algorithm (and similarly for its van Deemter’s generalisation) when extended to deal with binary relations, does not hold in the present approach."
"In the D&amp;R algorithm, the problem stems from the fact that DD are generated recursively: if when generating a DD for some entity  , a relation y is selected which relates  to e.g., wUZ , the D&amp;R al-gorithm will recursively go on to produce a DD for wUZ ."
"Without additional restriction, the algorithm can thus loop forever, first describing #w V in terms of w¡Z , then wUZ in terms of  , then w#V in terms of wUZ etc."
The solution adopted[REF_CITE]is to stipulate that facts from the knowledge base can only be used once within a given call to the algorithm.
"In contrast, the solution follows, in the present al-gorithm (as in SPUD ), from its integration with sur-face realisation."
"Suppose for instance, that the initial goal is to describe the discourse old entity  ."
"The initially empty goal semantics will be updated with its DD say, SeÖC]×²Y| ]Ø?| \ ."
"This information is then used to select appropri-ate lexical entries i.e., the noun entry for “bowl” and the preposition entry for “on”."
The resulting tree (with leaves “the bowl on”) is syntactically incom-plete hence generation continues attempting to pro-vide a description for s .
"If s is discourse old, the lexical entry for the will be selected and a DD com-puted say, $wMäsY| ]Ø?| \ ."
This then is added to the current goal semantics yielding the goal se-mantics $wM|äsY X ÖC]×²Y| ]?
"Ø | \ which is com-pared with the semantics of the tree built so far i..e., SeÖC]×²Y| ?| \ ."
"Since goal and tree semantics are different, gener-ation continue selecting the lexical entry for “table” and integrating it in the tree being built."
D N the N PP bowl P NP on N å D the table
Goal Semantics =  ÆÞ ß !% Ü á Û $% $% $â  !
Tree Semantics = YÛÝÜ
Þàß % á Û %! %*$â  â !
"At this stage, the semantics of that tree is SUsOã(| X |&amp; ]Ø?| \ which is equivalent to the goal semantics."
"Since furthermore the tree is syntactically and pragmatically complete, genera-tion stops yielding the NP the bowl on the table."
"In sum, infinite regress is avoided by using the computed DD s to control the addition of new mate-rial to the tree being built."
Minimality and overspecified descriptions.
"It has often been observed that human beings produce overspecified i.e., non-minimal descriptions."
One might therefore wonder whether generating minimal descriptions is in fact appropriate.
Two points speak for it.
"First, it is unclear whether redundant information is present because of a cognitive artifact (e.g., incre-mental processing) or because it helps fulfill some other communicative goal besides identification."
"So for instance,[REF_CITE]shows that in a specific task context, redundant attributes are used to indi-cate the violation of a task constraint (for instance, when violating a colour constraint, a task participant will use the description “the red table” rather than “the table” to indicate that s/he violates a constraint to the effect that red object may not be used at that stage of the task)."
"More generally, it seems unlikely that no rule at all governs the presence of redundant information in definite descriptions."
"If redundant descriptions are to be produced, they should therefore be produced in relation to some general principle (i.e., because the algorithm goes through a fixed order of attribute classes or because the redundant information fulfills a particular communicative goal) not randomly, as is done in the generalised incremental algorithm."
"Second, the psycholinguistic literature bearing on the presence of redundant information in definite descriptions has mainly been concerned with unary atomic relations."
"Again once binary, ternary and dis-junctive relations are considered, it is unclear that the phenomenon generalises."
"As[REF_CITE]observed, “it is unlikely that someone would describe an object as “the dog next to the tree in front of the garage” in a situation where “the dog next to the tree” would suffice."
The ideas presented in this pa-per have been implemented within the genera-tor I N D I G EN using the concurrent constraint pro-gramming language Oz (Programming Systems[REF_CITE]) which supports set variables ranging over finite sets of integers and provides an efficient implementation of the associated constraint theory.
The proof-of-concept implementation in-cludes the constraint solver described in section 4 and its integration in a chart-based generator inte-grating surface realisation and inference.
"For the ex-amples discussed in this paper, the constraint solver returns the minimal solution (i.e., The cat and the dog and The poodle, the Jersey, the pitbul and the Holstein) in 80 ms and 1.4 seconds respectively."
The integration of the constraint solver within the gener-ator permits realising definite NPs including nega-tive information (the cat that is not white) and sim-ple conjunctions (The cat and the dog).
One area that deserves further investigation is the relation to surface realisation.
"Once disjunctive and negative relations are used, interesting questions arise as to how these should be realised."
"How should conjunctions, disjunctions and negations be realised within the sentence?"
How are they realised in prac-tice? and how can we impose the appropriate con-straints so as to predict linguistically and cognitively acceptable structures?
"More generally, there is the question of which communicative goals refer to sets rather than just individuals and of the relationship to what in the generation literature has been bap-tised “aggregation” roughly, the grouping together of facts exhibiting various degrees and forms of sim-ilarity."
"We present a noun phrase coreference sys-tem that extends the work[REF_CITE]and, to our knowledge, pro-duces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, re-spectively."
Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale ex-pansion of the feature set to include more sophisticated linguistic knowledge.
Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a doc-ument.
"Machine learning approaches to this prob-lem have been reasonably successful, operating pri-marily by recasting the problem as a classification task (e.g.[REF_CITE],[REF_CITE])."
"Specifically, a pair of NPs is clas-sified as co-referring or not based on constraints that are learned from an annotated corpus."
A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs.
"Perhaps surprisingly, this was accomplished in a decidedly knowledge-lean manner — the learn-ing algorithm has access to just 12 surface-level fea-tures."
This paper presents an NP coreference system that investigates two types of extensions to the Soon et al. corpus-based approach.
"First, we propose and evaluate three extra-linguistic modifications to the machine learning framework, which together pro-vide substantial and statistically significant gains in coreference resolution precision."
"Second, in an attempt to understand whether incorporating addi-tional knowledge can improve the performance of a corpus-based coreference resolution system, we expand the Soon et al. feature set from 12 features to an arguably deeper set of 53."
"We propose addi-tional lexical, semantic, and knowledge-based fea-tures; most notably, however, we propose 26 addi-tional grammatical features that include a variety of linguistic constraints and preferences."
"Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.[REF_CITE]) and NP coreference resolution (e.g.[REF_CITE],[REF_CITE]), most previous work treats linguistic constraints as broadly and un-conditionally applicable hard constraints."
"Because sources of linguistic information in a learning-based system are represented as features, we can, in con-trast, incorporate them selectively rather than as uni-versal hard constraints."
Our results using an expanded feature set are mixed.
"First, we find that performance drops signifi-cantly when using the full feature set, even though the learning algorithms investigated have built-in feature selection mechanisms."
"We demonstrate em- pirically that the degradation in performance can be attributed, at least in part, to poor performance on common noun resolution."
"A manually selected sub-set of 22–26 features, however, is shown to pro-vide significant gains in performance when chosen specifically to improve precision on common noun resolution."
"Overall, the learning framework and lin-guistic knowledge source modifications boost per-formance of Soon’s learning-based coreference res-olution approach from an F-measure of 62.6 to 70.4, and from 60.4 to 63.4 for the MUC-6 and MUC-7 data sets, respectively."
"To our knowledge, these are the best results reported to date on these data sets for the full NP coreference problem. 1"
The rest of the paper is organized as follows.
"In sections 2 and 3, we present the baseline corefer-ence system and explore extra-linguistic modifica-tions to the machine learning framework."
Section 4 describes and evaluates the expanded feature set.
We conclude with related and future work in Section 5.
Our baseline coreference system attempts to dupli-cate both the approach and the knowledge sources employed[REF_CITE].
"More specifically, it employs the standard combination of classification and clustering described above."
Building an NP coreference classifier.
"We use the C4.5 decision tree induction system[REF_CITE]to train a classifier that, given a description of two NPs in a document, NP and NP , decides whether or not they are coreferent."
"Each training instance represents the two NPs under consideration and consists of the 12 Soon et al. features, which are described in Table 1."
"Linguistically, the features can be divided into four groups: lexical, grammati-cal, semantic, and positional. 2 The classification as-sociated with a training instance is one of COREF - ERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated training text."
"We follow the procedure employed in Soon et al. to cre- ate the training data: we rely on coreference chains from the MUC answer keys to create ([Footnote_1]) a positive instance for each anaphoric noun phrase, NP , and its closest preceding antecedent, NP ; and ([Footnote_2]) a negative instance for NP paired with each of the intervening NPs, NP  , NP  , , NP  ."
"1 Results presented[REF_CITE]are higher than those reported here, but assume that all and only the noun phrases involved in coreference relationships are provided for analysis by the coreference resolution system. We presume no preprocessing of the training and test documents."
"2 In all of the work presented here, NPs are identified, and features values computed entirely automatically."
"This method of neg-ative instance selection is further described[REF_CITE]; it is designed to operate in conjunction with their method for creating coreference chains, which is explained next."
Applying the classifier to create coreference chains.
"After training, the decision tree is used by a clustering algorithm to impose a partitioning on all NPs in the test texts, creating one cluster for each set of coreferent NPs."
"As in Soon et al., texts are pro-cessed from left to right."
"Each NP encountered, NP , is compared in turn to each preceding NP, NP , from right to left."
"For each pair, a test instance is created as during training and is presented to the corefer-ence classifier, which returns a number between 0 and 1 that indicates the likelihood that the two NPs are coreferent. [Footnote_3] NP pairs with class values above 0.5 are considered COREFERENT ; otherwise the pair is considered NOT COREFERENT ."
"3 We convert the binary class value using the smoothed ratio , where p is the number of positive instances and t is the total number of instances contained in the corresponding leaf node."
The process termi-nates as soon as an antecedent is found for NP or the beginning of the text is reached.
"We evaluate the Duplicated Soon Baseline sys-tem using the standard[REF_CITE]and[REF_CITE]coreference corpora, training the corefer-ence classifier on the 30 “dry run” texts, and ap-plying the coreference resolution algorithm on the 20–30 “formal evaluation” texts."
The MUC-6 cor-pus produces a training set of 26455 instances (5.4% positive) from 4381 NPs and a test set of 28443 instances (5.2% positive) from 4565 NPs.
"For the MUC-7 corpus, we obtain a training set of 35895 in-stances (4.4% positive) from 5270 NPs and a test set of 22699 instances (3.9% positive) from 3558 NPs."
"Results are shown in Table 2 (Duplicated Soon Baseline) where performance is reported in terms of recall, precision, and F-measure using the model-theoretic MUC scoring program[REF_CITE]."
"The system achieves an F-measure of 66.3 and 61.2 on the MUC-6 and MUC-7 data sets, respec-tively."
"Similar, but slightly worse performance was obtained using RIPPER[REF_CITE], an information-gain-based rule learning system."
"Both sets of results are at least as strong as the original Soon results (row one of Table 2), indicating indi-rectly that our Baseline system is a reasonable du-plication of that system. [Footnote_4]"
"4 In all of the experiments described in this paper, default settings for all C4.5 parameters are used. Similarly, all RIPPER parameters are set to their default value except that classification rules are induced for both the positive and negative instances."
"In addition, the trees pro-duced by Soon and by our Duplicated Soon Baseline are essentially the same, differing only in two places where the Baseline system imposes additional con-ditions on coreference."
"The primary reason for improvements over the original Soon system for the MUC-6 data set ap-pears to be our higher upper bound on recall (93.8% vs. 89.9%), due to better identification of NPs."
"For MUC-7, our improvement stems from increases in precision, presumably due to more accurate feature value computation."
This section studies the effect of three changes to the general machine learning framework employed by Soon et al. with the goal of improving precision in the resulting coreference resolution systems.
"Rather than a right-to-left search from each anaphoric NP for the first coref-erent NP, we hypothesized that a right-to-left search for a highly likely antecedent might offer more pre-cise, if not generally better coreference chains."
"As a result, we modify the coreference clustering algo-rithm to select as the antecedent of NP the NP with the highest coreference likelihood value from among preceding NPs with coreference class values above 0.5."
Training set creation.
"For the proposed best-first clustering to be successful, however, a different method for training instance selection would be needed: rather than generate a positive training ex-ample for each anaphoric NP and its closest an-tecedent, we instead generate a positive training ex-amples for its most confident antecedent."
"More specifically, for a non-pronominal NP, we assume that the most confident antecedent is the closest non- pronominal preceding antecedent."
"For pronouns, we assume that the most confident antecedent is sim-ply its closest preceding antecedent."
Negative exam-ples are generated as in the Baseline system. [Footnote_5] String match feature.
"5 This new method of training set creation slightly alters the class value distribution in the training data: for the MUC-6 cor-pus, there are now 27654 training instances of which 5.2% are positive; for the MUC-7 corpus, there are now 37870 training instances of which 4.2% are positive."
Soon’s string match feature ( SOON STR ) tests whether the two NPs under con-sideration are the same string after removing deter-miners from each.
"We hypothesized, however, that splitting this feature into several primitive features, depending on the type of NP, might give the learn-ing algorithm additional flexibility in creating coref-erence rules."
"Exact string match is likely to be a better coreference predictor for proper names than it is for pronouns, for example."
"Specifically, we replace the SOON STR feature with three features — PRO STR , PN STR , and WORDS STR — which restrict the application of string matching to pro-nouns, proper names, and non-pronominal NPs, re-spectively. (See the first entries in Table 3.)"
"Al-though similar feature splits might have been con-sidered for other features (e.g. GENDER and NUM - BER ), only the string match feature was tested here."
Results and discussion.
Results on the learning framework modifications are shown in Table 2 (third block of results).
"When used in combination, the modifications consistently provide statistically sig-nificant gains in precision over the Baseline system without any loss in recall. [Footnote_6] As a result, we observe reasonable increases in F-measure for both classi-fiers and both data sets."
"6 Chi-square statistical significance tests are applied to changes in recall and precision throughout the paper. Unless otherwise noted, reported differences are at the 0.05 level or higher. The chi-square test is not applicable to F-measure."
"When using RIPPER, for example, performance increases from 64.3 to 67.2 for the MUC-6 data set and from 60.8 to 63.2 for MUC-7."
"Similar, but weaker, effects occur when ap-plying each of the learning framework modifications to the Baseline system in isolation. (See the indented Learning Framework results in Table 2.)"
Our results provide direct evidence for the claim[REF_CITE]that the extra-linguistic strategies employed to combine the available linguistic knowl-edge sources play an important role in computa-tional approaches to coreference resolution.
"In par-ticular, our results suggest that additional perfor-mance gains might be obtained by further investi-gating the interaction between training instance se-lection, feature selection, and the coreference clus-tering algorithm."
"This section describes the second major extension to the Soon approach investigated here: we explore the effect of including 41 additional, potentially use-ful knowledge sources for the coreference resolu-tion classifier (Table 3)."
"The features were not de-rived empirically from the corpus, but were based on common-sense knowledge and linguistic intuitions regarding coreference."
"Specifically, we increase the number of lexical features to nine to allow more complex NP string matching operations."
"In addi-tion, we include four new semantic features to al-low finer-grained semantic compatibility tests."
"We test for ancestor-descendent relationships in Word-Net ( SUBCLASS ), for example, and also measure the WordNet graph-traversal distance ( WNDIST ) be-tween NP and NP ."
"Furthermore, we add a new posi-tional feature that measures the distance in terms of the number of paragraphs ( PARANUM ) between the two NPs."
"The most substantial changes to the feature set, however, occur for grammatical features: we add 26 new features to allow the acquisition of more sophis-ticated syntactic coreference resolution rules."
"Four features simply determine NP type, e.g. are both NPs definite, or pronouns, or part of a quoted string?"
These features allow other tests to be conditioned on the types of NPs being compared.
"Similarly, three new features determine the grammatical role of one or both of the NPs."
"Currently, only tests for clausal subjects are made."
"Next, eight features encode tra-ditional linguistic (hard) constraints on coreference."
"For example, coreferent NPs must agree both in gen-der and number ( AGREEMENT ); cannot SPAN one another (e.g. “government” and “government offi-cials”); and cannot violate the BINDING constraints."
Still other grammatical features encode general lin-guistic preferences either for or against coreference.
"For example, an indefinite NP (that is not in appo-sition to an anaphoric NP) is not likely to be coref-erent with any NP that precedes it ( ARTICLE )."
"The last subset of grammatical features encodes slightly more complex, but generally non-linguistic heuris-tics."
"For instance, the CONTAINS PN feature ef-fectively disallows coreference between NPs that contain distinct proper names but are not them-selves proper names (e.g. “IBM executives” and “Microsoft executives”)."
"Two final features make use of an in-house naive pronoun resolution algorithm ( PRO RESOLVE ) and a rule-based coreference resolution system ( RULE RESOLVE ), each of which relies on the origi-nal and expanded feature sets described above."
Results and discussion.
Results using the ex-panded feature set are shown in the All Features block of Table 2.
These and all subsequent results also incorporate the learning framework changes from Section 3.
"In comparison, we see statistically significant increases in recall, but much larger de-creases in precision."
"As a result, F-measure drops precipitously for both learning algorithms and both data sets."
A closer examination of the results indi-cates very poor precision on common nouns in com-parison to that of pronouns and proper nouns. (See the indented All Features results in Table 2. [Footnote_7] )
"7 For each of the NP-type-specific runs, we measure overall coreference performance, but restrict NP to be of the specified type. As a result, recall and F-measure for these runs are not particularly informative."
"In particular, the classifiers acquire a number of low-precision rules for common noun resolution, pre-sumably because the current feature set is insuffi-cient."
"For instance, a rule induced by RIPPER clas-sifies two NPs as coreferent if the first NP is a proper name, the second NP is a definite NP in the subject position, and the two NPs have the same seman-tic class and are at most one sentence apart from each other."
"This rule covers 38 examples, but has 18 exceptions."
"In comparison, the Baseline sys-tem obtains much better precision on common nouns (i.e. 53.3 for MUC-6/[REF_CITE].0 for MUC-[Footnote_7]/RIPPER with lower recall in both cases) where the primary mechanism employed by the classifiers for common noun resolution is its high-precision string matching facility."
"7 For each of the NP-type-specific runs, we measure overall coreference performance, but restrict NP to be of the specified type. As a result, recall and F-measure for these runs are not particularly informative."
Our results also suggest that data fragmentation is likely to have contributed to the drop in performance (i.e. we increased the number of features without increasing the size of the training set).
"For example, the decision tree induced from the MUC-6 data set using the Soon feature set (Learn-ing Framework results) has 16 leaves, each of which contains 1728 instances on average; the tree induced from the same data set using all of the 53 features, on the other hand, has 86 leaves with an average of 322 instances per leaf."
Hand-selected feature sets.
"As a result, we next evaluate a version of the system that employs man-ual feature selection: for each classifier/data set combination, we discard features used primarily to induce low-precision rules for common noun res-olution and re-train the coreference classifier using the reduced feature set."
"Here, feature selection does not depend on a separate development corpus and is guided solely by inspection of the features associ-ated with low-precision rules induced from the train-ing data."
"In current work, we are automating this feature selection process, which currently employs a fair amount of user discretion, e.g. to determine a precision cut-off."
Features in the hand-selected set for at least one of the tested system variations are *’d in Tables 1 and 3.
"In general, we hypothesized that the hand-selected features would reclaim precision, hopefully without losing recall."
"For the most part, the ex-perimental results support this hypothesis. (See the Hand-selected Features block in Table 2.)"
"In com-parison to the All Features version, we see statisti-cally significant gains in precision and statistically significant, but much smaller, drops in recall, pro-ducing systems with better F-measure scores."
"In addition, precision on common nouns rises substan-tially, as expected."
"Unfortunately, the hand-selected features precipitate a large drop in precision for pro-noun resolution for the MUC-7/C4.5 data set."
Ad-ditional analysis is required to determine the reason for this.
"Moreover, the Hand-selected Features produce the highest scores posted to date for both the MUC-6 and MUC-7 data sets: F-measure increases w.r.t. the Baseline system from 64.3 to 70.4 for MUC-6/RIPPER, and from 61.2 to 63.4 for MUC-7/C4.5."
"In one variation (MUC-7/RIPPER), however, the Hand-selected Features slightly underperforms the Learning Framework modifications (F-measure of 63.1 vs. 63.2) although changes in recall and pre-cision are not statistically significant."
"Overall, our results indicate that pronoun and especially com-mon noun resolution remain important challenges for coreference resolution systems."
"Somewhat dis-appointingly, only four of the new grammatical features corresponding to linguistic constraints and preferences are selected by the symbolic learning algorithms investigated: AGREEMENT , ANIMACY , BINDING , and MAXIMALNP ."
"In an attempt to gain additional in-sight into the difference in performance between our system and the original Soon system, we compare the decision tree induced by each for the MUC-6 data set. [Footnote_8]"
8[REF_CITE]present only the tree learned for the MUC-6 data set.
"For our system, we use the tree induced on the hand-selected features (Figure 1)."
The two trees are fairly different.
"In particular, our tree makes use of many of the features that are not present in the original Soon feature set."
"The root feature for Soon, for example, is the general string match fea-ture ( SOON STR ); splitting the SOON STR feature into three primitive features promotes the ALIAS fea-ture to the root of our tree, on the other hand."
"In addition, given two non-pronominal, matching NPs ( SOON STR NONPRO =C), our tree requires an addi-tional test on ANIMACY before considering the two NPs coreferent; the Soon tree instead determines two NPs to be coreferent as long as they are the same string."
"Pronoun resolution is also performed quite differently by the two trees, although both consider two pronouns coreferent when their strings match."
"Finally, intersentential and intrasentential pronomi-nal references are possible in our system while inter-sentential pronominal references are largely prohib-ited by the Soon system."
We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.
"First, we pro-pose three extra-linguistic modifications to the ma-chine learning framework, which together consis-tently produce statistically significant gains in pre-cision and corresponding increases in F-measure."
"Our results indicate that coreference resolution sys-tems can improve by effectively exploiting the in-teraction between the classification algorithm, train-ing instance selection, and the clustering algorithm."
"We plan to continue investigations along these lines, developing, for example, a true best-first clustering coreference framework and exploring a “supervised clustering” approach to the problem."
"In addition, we provide the learning algorithms with many addi-tional linguistic knowledge sources for coreference resolution."
"Unfortunately, we find that performance drops significantly when using the full feature set; we attribute this, at least in part, to the system’s poor performance on common noun resolution and to data fragmentation problems that arise with the larger feature set."
"Manual feature selection, with an eye toward eliminating low-precision rules for common noun resolution, is shown to reliably improve per-formance over the full feature set and produces the best results to date on the MUC-6 and MUC-7 coref-erence data sets — F-measures of 70.4 and 63.4, re-spectively."
"Nevertheless, there is substantial room for improvement."
"As noted above, for example, it is important to automate the precision-oriented feature selection procedure as well as to investigate other methods for feature selection."
"We also plan to in-vestigate previous work on common noun phrase interpretation (e.g.[REF_CITE],[REF_CITE]) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems."
"We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages."
"In natural language process-ing, this problem arises in several areas of application, including natural language generation, speech recognition and ma-chine translation."
"We present two tabu-lar algorithms for parsing of non-recursive context-free grammars, and show that they perform well in practical settings, despite the fact that this problem is PSPACE-complete."
Several applications in natural language processing require “parsing” of a large but finite set of candidate strings.
"Here parsing means some computation that selects those strings out of the finite set that are well-formed according to some grammar, or that are most likely according to some language model."
"In these applications, the finite set is typically encoded in a compact way as a context-free grammar (CFG) that is non-recursive."
"This is motivated by the fact that non-recursive CFGs allow very compact represen-tations for finite languages, since the strings deriv-able from single nonterminals may be substrings of many different strings in the language."
"Unfolding such a grammar and parsing the generated strings one by one then leads to an unnecessary duplica-tion of subcomputations, since each occurrence of a repeated substring has to be independently parsed."
"As this approach may be prohibitively expensive, it is preferable to find a parsing algorithm that shares subcomputations among different strings by work-ing directly on the nonterminals and the rules of the non-recursive CFG."
"In this way, “parsing” a nonter-minal of the grammar amounts to shared parsing of all the substrings encoded by that nonterminal."
"To give a few examples, in some natural lan-guage generation systems[REF_CITE]non-recursive CFGs are used to encode very large sets of candidate sentences realizing some input con-ceptual representation (Langkilde calls such gram-mars forests)."
"Each CFG is later “parsed” using a language model, in order to rank the sentences in the set according to their likelyhood."
"Similarly, in some approaches to automatic speech understand-ing[REF_CITE]the -best sen-tences obtained from the speech recognition module are “compressed” into a non-recursive CFG gram-mar, which is later provided as input to a parser."
"Fi-nally, in some machine translation applications re-lated techniques are exploited to obtain sentences that simultaneously realize two different conceptual representations[REF_CITE]."
This is done in order to produce translations that preserve syntactic or semantic ambiguity in cases where the ambiguity could not be resolved when processing the source sentence.
"To be able to describe the above applications in an abstract way, let us first fix some terminology."
The term “recognition” refers to the process of deciding
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 112-119. whether an input string is in the language described by a grammar, the parsing grammar  ."
"We will generalize this notion in a natural way to input rep-resenting a set of strings, and here the goal of recog-nition is to decide whether at least one of the strings in the set is in the language described by  ."
"If the input is itself given in the form of a grammar, the input grammar  , then recognition amounts to de-termining whether the intersection of the languages described by  and is non-empty."
"In this paper we use the term parsing as synonymous to recog-nition, since the recognition algorithms we present can be easily extended to yield parse trees (with as-sociated probabilities if either or or both are probabilistic)."
In what follows we consider the case where both and  are CFGs.
General CFGs have un-favourable computational properties with respect to intersection.
"In particular, the problem of deciding whether the intersection of two CFGs is non-empty is undecidable[REF_CITE]."
"Following the ter-minology adopted above, this means that parsing a context-free input grammar on the basis of a context-free parsing grammar  is not possible in general."
One way to make the parsing problem decidable is to place some additional restrictions on or .
"This direction is taken[REF_CITE], where is a non-recursive CFG and repre-sents a regular language, more precisely an -gram model."
"In this way the problem can be solved us-ing a stochastic variant of an algorithm presented[REF_CITE], where it is shown that the intersection of a general context-free language and a regular language is still context-free."
"In the present paper we leave the theoretical framework[REF_CITE], and consider parsing grammars  that are unrestricted CFGs, and input grammars  that are non-recursive context-free grammars."
In this case the parsing (in-tersection) problem becomes PSPACE-complete. [Footnote_1]
"1 The PSPACE-hardness result has been shown by Harry B. Hunt III and Dan Rosenkrantz (Harry B. Hunt III, p.c.). Mem-bership in PSPACE is shown[REF_CITE]."
"Despite of this unfavourable theoretical result, algo-rithms for the problem at hand have been proposed in the literature and are currently used in practical applications."
In[REF_CITE]is unfolded into a lattice (acyclic finite automaton) and later parsed with  using an algorithm close to the one[REF_CITE].
"The algorithm pro-posed[REF_CITE]involves copy-ing of charts, and this makes it very similar in be-haviour to the former approach."
"Thus in both al-gorithms parts of the input grammar are copied where a nonterminal occurs more than once, which destroys the compactness of the representation."
In this paper we propose two alternative tabular algo-rithms that exploit the compactness of as much as possible.
"Although a limited amount of copying is also done by our algorithms, this never happens in cases where the resulting structure is ungrammatical with respect to the parsing grammar ."
The structure of this paper is as follows.
"In Sec-tion 2 we introduce some preliminary definitions, followed in Section 3 by a first algorithm based on CKY parsing."
"A more sophisticated algorithm, sat-isfying the equivalent of the correct-prefix property and based on Earley’s algorithm, is presented in Sec-tion 4."
Section 5 presents our experimental results and Section 6 closes with some discussion.
In this section we briefly recall some standard no-tions from formal language theory.
For more details we refer the reader to textbooks such[REF_CITE].
", where is a finite set of terminals, called the alphabet, is a finite set of nonterminals, including the start symbol , and is a finite set of rules hav-ing the form  with ! and &quot;#$&amp;% ."
"Throughout the paper we assume the following con-ventions: , ()+*,+* * denote nonterminals, - , /. +,* *,* de-note terminals, 0 , 1 , are strings in $[Footnote_2]%! and 3  are strings in ."
"2 Strictly speaking, the assumption about the absence of ep-silon rules is not without loss of generality, since without ep-silon rules the language cannot contain the empty string. How-ever, this has no practical consequence."
"We also assume that each CFG is reduced, i.e., no CFG contains nonterminals that do not occur in any derivation of a string in the language."
"Furthermore, we assume that the input grammars do not contain epsilon rules and that there is only one rule  defining the start symbol . 2 Finally, in Section 3 we will consider parsing gram- mars in Chomsky normal form (CNF), i.e., gram-mars with rules of the form (:9 or  ."
"Instead of working with non-recursive CFGs, it will be more convenient in the specification of our algorithms to encode  as a push-down automaton (PDA) with stack size bounded by some constant."
"Unlike many text-books, we assume PDAs do not have states; this is without loss of generality, since states can be encoded in the symbols that occur top-most on the stack."
"Thus, a PDA is a 5-tuple &lt;;= &gt; A? +@ CB? &gt;ED @/F5G &lt;H , where is the alphabet as above, ; is a finite set &gt; of ?A@+I? B stack symbols including the ED&gt; initial @/F5G stack symbol and the final stack symbol , and H is the set of transitions &gt; ML&gt; , having one of the fol-lowing three forms:   &gt; (a &gt; push PJ transition L ), O (a pop transition), or (a scan per we use the following conventions: Q&quot; transition, scanning symbol - )."
Throughout &gt; this L pa-  denote stack symbols and  are strings in ; representing stacks.
"We remark that in our notation stacks grow from left to right, i.e., the top-most stack symbol will be found at the right end."
"Configurations of the PDA have the form  , where ; is a stack and [4 \ is the remain-ing input."
"We let the binary relation ] be defined by: ^_W  3 4 tion in H `]  if and J only if there 3edgf is a ,transi-or of of the PJ T form, where S 3hd - , where the form S ."
The relation ] denotes the reflexive and transitive closure of ] .
An input )&gt; ?i string @+CB?  4 ] is recognized ED&gt; @/F5G f . by the PDA if and onlyif
"In this section we present our first parsing algorithm, based on the so-called CKY algorithm[REF_CITE]and exploiting a decomposition of computa-tions of PDAs cast in a specific form."
We start with a construction that translates the non-recursive input CFG  into a PDA accepting  the same language.
Let  d with  is specified $) .
"The PDA associated as ) ;=&amp;// where ; consists of symbols of the form j #m 1tn for  , and H contains the following transitions: m For H each pair of rules (x1 and ( , j  x1tn(  contains: j  x1tnujo( ( j  (x1tn{jo(|m}n  (rm 1tn . and m For j  PJ ~-m0~-11tn each rule , H contains: ."
"Observe that for all PDAs constructed as above, no push transition can be immediately followed &gt; by a pop transition, i.e. &gt; , there &gt;ML are noandstack &gt;&gt; ?"
"A@+?IB O L symbols , and O such that ."
"As a |&gt; consequence D @/F5G f of theof thisPDA, a computationcan always and uniquely  ] be decomposed into consecutive subcomputations, which we call segments, each starting with zero or more push transitions, followed by a single scan transition and by zero or more pop transitions."
"In what follows, we will formalize this basic idea and exploit it within P our parsing algorithm."
"We write S {d  T to indicate that there is a com-putation  ]  f of the PDA such that all of the following three conditions hold: (i) either  d2 or  dr ; (ii) the computation starts with zero or more push transitions, followed by one scan transition reading - and by zero or more pop transitions; (iii) if   then the top-most symbol of S must be in the right-hand side of a pop or scan tran-sition (i.e., top-most in the stack at the end of a previous segment) and if t  , then the top-most symbol of T must be the left-hand side of a push or scan transition (i.e., top-most in the stack at the beginning of a following segment)."
Let  L  $ &gt; ^  ~drj &gt; &gt; &gt;ML?APJ +@ CBU?
"L % n   O &gt; &apos;L j &gt; d8 PJ &gt;EDL @/F5Gn  %% &gt; formal definition of relation  above is provided n   L j&gt; n  % ,  and &gt;  L {j&gt; ."
Figure 1 by means of a deduction system.
"We assign a procedural interpretation to such a system follow-ing[REF_CITE], resulting in an algorithm for the computation of the relation."
We now turn to an important &gt; i? +@ ?
"CB property {+, X- of seg- ] ments &gt;|D @/F5G . f Any,  computation  , can be computed by combining 



  segments represented by  d{Pq¯ T_ , |°  , with S  d &gt; A? +@ ?"
"CB , T  d &gt;ED /@"
"F5G , and for )   , T _ is a suffix of  ¦  or  ¦  is a suffix of _T ."
"This which defines the relationis done by the deduction system d{ ¦ .givenThe secondin Figureside-2, ment S condition &gt; of d{P  inference T L mayruleborder(5) checkson otherwhethersegmentsa seg-, or may be the first or last segment in a computation."
Figure 3 illustrates a computation of a PDA rec-ognizing a string -  -³q-´R-tµ .
"A horizontal line seg-ment in the curve represents a scan transition, an up-ward line segment represents a push transition, and a downward line segment a pop Pq¯ transitionAs.anTheexampleshaded, areas represent segments S d~&gt;?A@+IB?T . d~+P ¶ &gt; A? +@ ?IB·&gt;  &gt; ³ the area labelled I represents , for certain stack symbols  and &gt; ³ , &gt; & gt; where ?"
A@+?IB the left edge of the shaded area &gt; i? @+ represents CB? &gt;  &gt; ³ .
Note thatandseg-the right edge Pq¯ represents ments  {d   abstract away from the stack sym-bols that are pushed and then popped again.
"Fur-thermore, in the context of the whole computation, segments abstract away from stack symbols that are not accessed during a subcomputation."
"As an exam-ple, the PR¸ shaded O , forareacertainlabelledstackIIIsymbolsrepresents L  ,segment L ³ and L  L ³ {d  O , and this abstracts away L from the stack symbols that may occur below  and O ."
Figure 4 illustrates how two adjacent segments are combined.
"The dashed box in the left-hand side of the picture represents stack symbols from the right edge of segment II that need not be explicitly repre-sented by segment III, as discussed above."
"We may assume that these symbols exist, so that II and III can be combined into the larger computation in the right-hand side of the picture."
"Note that if a com-putation S d{« :¦ T is obtained as the combination of two segments as in Figure 4, then some internal details of these segments are abstracted away, i.e., stack elements that were pushed and again popped in the combined computation are no longer recorded."
"This abstraction is a key feature of the parsing al-gorithm to be presented next, in that it considerably reduces the time complexity as compared with that of an algorithm that investigates all computations of the PDA in isolation."
"We are now ready to present our parsing algo-rithm, which is the main result of this section."
"The algorithm combines the deduction system in Fig-ure 2, as applied to the PDA encoding the input grammar , with the CKY algorithm as applied to the parsing grammar  . (We assume that is in CNF.)"
The parsing algorithm may rule out many combinations of segments from Figure 2 that are in-consistent with the language generated by  .
Also ruled out are structural compositions of segments that are inconsistent with the structure that as-signs to the corresponding substrings.
"The parsing algorithm is again specified as a de-duction system, presented in Figure 5."
"The algo-rithm manipulates items of the form j  , where is a nonterminal of and S , T are stacks of the PDA encoding ."
"Such an item indicates that there is some terminal string 4 that ]  f is derivable from in &gt; A? , @+IB? and &gt;|D such /@"
F5G n thatcan  .
"If the item  be derived by the algorithm, then the intersection of the language generated by and the language accepted by the PDA (gener-ated by  ) is non-empty."
The CKY algorithm from Figure 5 can be seen to filter out a selection of the computations that may be derived by the deduction system from Figure 2.
One may however be even more selective in determining which computations of the PDA to consider.
The ba-sis for the algorithm in this section is Earley’s algo-rithm[REF_CITE].
This algorithm differs from the CKY algorithm in that it satisfies the correct-prefix property[REF_CITE].
The new algorithm is presented by Figure 6.
There are now two types of item involved.
"The first item has the form j 0ÄmÅ1 has the 0Âm:1ZWÂÃ\ , where  same role as the dot-ted rules in Earley’s original algorithm."
"The sec-ond and third components are stacks of the PDA as before, but these stacks now contain a distin-guished position, indicated by Ã ."
"The existence of an item j] ^0 f m¥1#3 implies that ^_W  3 , where is now a string deriv-able from 0 ."
"This is quite similar to the meaning we assigned to the items of the CKY algorithm, but here not all stack symbols in WS and  are involved in this computation: only the symbols in S and T are now accessed, while all symbols in W remain unaf-fected."
"The portion of the stack represented by W is needed to ensure the correct-prefix property in sub-sequent computations following from this item, in case all of the symbols in T are popped."
The correct-prefix property is ensured in the fol-lowing sense.
"The existence of an item j  1Ç+ implies that (i) there is a string 4 3 that is both a prefix of a string accepted by the PDA and of a string generated by the CFG such that after processing 4 , is expanded in a left-most deriva-tion and some stack can be obtained of which WS represent 3 the top-most elements 3 , and (ii) 0 is rewrit-ten to and while processing the PDA replaces the stack elements S by T . 3"
The second type of item has the form j  1`_uQ|Ê+n .
"The first three compo-nents are the same as before, and Q indicates that we wish to know whether a stack with top-most symbols  may arise after reading a prefix of a string that may also lead to expansion of nonterminal in a left-most derivation."
Such an item results if it is de-tected that the existence of Q below W_S needs to be ensured in order to continue the computation under the constraint of the correct-prefix property.
"Our algorithm also makes use of segments, as computed by the algorithm from Figure 1."
"Con-sistently &gt; d~PZ¦ with &gt; T L:$&lt;a)§ L  S rule (5) from Figure 2, &gt; we d{P write T L to represent a segment S such that ."
The use of seg-ments that were computed bottom-up is a departure from pure left-to-right processing in the spirit of Ear-ley’s original algorithm.
The motivation is that we have found empirically that the use of rule (2) was essential for avoiding a large part of the exponen-tial behaviour; note that that rule considers at most a number of stacks that is quadratic in the size of the PDA.
"The first inference rule (11) can be easily justified: we want to investigate strings that are both generated by the grammar and recognized by the PDA, so we begin by combining the start symbol and a match-ing right-hand side from the grammar with the initial stack for the PDA."
Segments are incorporated into the left-to-right computation by rules (12) and (13).
These two rules are the equivalents of (9) and (10) from Figure 5.
Note that in the case of (13) we require the presence of W below the marker in the antecedent.
This indi-cates that a stack with top-most symbols _W S and a dotted rule  can be obtained by simulta-neously processing a string from left to right by the grammar and the PDA.
"Thereby, we may continue the derivation with the item in the consequent with-out violating the correct-prefix property."
"We do this one symbol at a time, starting with the symbol Q just beneath the part of the stack that is already available."
This will be discussed more care-fully below.
"The predictor step of Earley’s algorithm is repre-sented by (15), and the completer step by rules (16) and (17)."
These latter two are very similar to (12) and (13) in that they incorporate a smaller derivation in a larger derivation.
"In the antecedent of rule (18) the position of the marker is irrelevant, and is not indicated explic-itly."
"Similarly, for rule (19) we assume the position of the marker is copied unaltered from the first an-tecedent to the consequent."
"If we find the required stack symbol Q , we prop-agate the information forward that this symbol may indeed occur at the specified position in the stack."
This is implemented by rules (20) and (21).
We have implemented the two algorithms and tested them on non-recursive input CFGs and a parsing CFG.
We have had access to six input CFGs of the form described[REF_CITE].
As parsing CFG we have taken a small hand-written grammar of about 100 rules.
"While this small size is not at all typical of practical grammars, it suffices to demon-strate the applicability of our algorithms."
The results of the experiments are reported in Fig-ure 1.
"We have ordered the input grammars by size, according to the number of nonterminals (or the number of nodes in the forest, following the ter-minology[REF_CITE])."
"The second column presents the number of strings generated by the input CFG, or more accurately, the number of derivations, as the grammars contain some ambiguity."
The high numbers show that with-out a doubt the naive solution of processing the input grammars by enumerating individual strings (deriva-tions) is not a viable option.
"The third column shows the size, expressed as number of states, of a lattice (acyclic finite au-tomaton) that would result by unfolding the gram-mar[REF_CITE]."
"Although this approach could be of more practical interest than the naive approach of enumerating all strings, it still leads to large intermediate results."
"In fact, practical context-free parsing algorithms for finite automata have cubic time complexity in the number of states, and derive a number of items that is quadratic in the number of states."
S The d{P  next T .
Thesecolumnapplypresentsto boththe numberalgorithmof. segmentsWe only compute segments S d{P  T for terminals - that also occur in the parsing grammar. (Further obvious op-timizations in the case of Earley’s algorithm were found to lead to no more than a slight reduction of produced segments.)
"The last two columns present the number of items specific to the two algorithms in Figures 5 and 6, respectively."
"Although our two algorithms are exponential in the number of stack symbols in the worst case, just as approaches that enumerate all strings or that unfold into a lattice, we see that the numbers of items are relatively mod-erate if we compare them to the number of strings generated by the input grammars."
Earley’s algorithm generally produces more items than the CKY algorithm.
An exception is the last in-put CFG; it seems that the number of items that Ear-ley’s algorithm needs to consider in order to main-tain the correct-prefix property is very sensitive to qualities of the particular input CFG.
The present implementations use a trie to store stacks; the arcs in the trie closest to the root rep-resent stack symbols closest P to the top of the stacks.
"For example, for storing S d~ T , the algorithm rep-trie, and it indexes S d~P  resents S and T by their corresponding T nodes in the twice, once through each associated node."
"Since the trie is doubly linked (i.e. we may traverse the trie upwards as well as downwards), we can always reconstruct the stacks from the corresponding nodes."
"This structure is also convenient for finding pairs of matching stacks, one of which may be deeper than the other, as required by the inference rules from e.g. Figure 5, since given the first stack in such a pair, the second can be found by traversing the trie either upwards or downwards."
It is straightforward to give an algorithm for parsing a finite language: we may trivially parse each string in the language in isolation.
"However, this is not a practical solution when the number of strings in the language exceeds all reasonable bounds."
Some algorithms have been described in the exist-ing literature that parse sets of strings of exponential size in the length of the input description.
"These so-lutions have not considered context-free parsing of finite languages encoded by non-recursive CFGs, in a way that takes full advantage of the compactness of the representation."
"Our algorithms make this pos-sible, relying on the compactness of the input gram-mars for efficiency in practical cases, and on the ab-sence of recursion for guaranteeing termination."
Our experiments also show that these algorithms are of practical interest.
It is necessary to have a (large) annotated cor-pus to build a statistical parser.
Acquisition of such a corpus is costly and time-consuming.
"This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus."
Sample selection for annotation is based upon “representativeness” and “usefulness”.
A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees.
"Based on this distance, the active learning process analyzes the sample dis-tribution by clustering and calculates the den-sity of each sample to quantify its representa-tiveness."
"Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores."
Experiments are carried out in the shallow se-mantic parser of an air travel dialog system.
"Our result shows that for about the same pars-ing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method."
A prerequisite for building statistical parsers[REF_CITE]is the availability of a (large) corpus of parsed sen-tences.
Acquiring such a corpus is expensive and time-consuming and is often the bottleneck to build a parser for a new application or domain.
The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning.
"Active learning has been studied in the context of many natural language processing (NLP) applications such as information extracti[REF_CITE], text clas-sificati[REF_CITE]and natural lan-guage parsing[REF_CITE], to name a few."
"The basic idea is to couple tightly knowl-edge acquisition, e.g., annotating sentences for parsing, with model-training, as opposed to treating them sepa-rately."
"In our setup, we assume that a small amount of annotated sentences is initially available, which is used to build a statistical parser."
We also assume that there is a large corpus of unannotated sentences at our disposal – this corpus is called active training set.
"A batch of sam-ples [Footnote_1] is selected using algorithms developed here, and are annotated by human beings and are then added to training data to rebuild the model."
1 A sample means a sentence in this paper.
The procedure is iterated until the model reaches a certain accuracy level.
"Our efforts are devoted to two aspects: first, we be-lieve that the selected samples should reflect the underly-ing distribution of the training corpus."
"In other words, the selected samples need to be representative."
"To this end, a model-based structural distance is defined to quantify how “far” two sentences are apart, and with the help of this distance, the active training set is clustered so that we can define and compute the “density” of a sample; second, we propose and test several entropy-based mea-sures to quantify the uncertainty of a sample in the active training set using an existing model, as it makes sense to ask human beings to annotate the portion of data for which the existing model is not doing well."
Samples are selected from the clusters based on uncertainty scores.
The rest of the paper is organized as follows.
"In Sec-tion 2, a structural distance is first defined based on the se-quential representation of a parse tree."
It is then straight-forward to employ a k-means algorithm to cluster sen-tences in the active training set.
"Section 3 is devoted to confidence measures, where three uncertainty measures are proposed."
Active learning results on the shallow se-mantic parser of an air travel dialog system are presented
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 120-127. in Section 4."
A summary of related work is given in Section 5.
The paper closes with conclusions and future work.
"To characterize the “representativeness” of a sentence, we need to know how far two sentences are apart so that we can measure roughly how many similar sentences there are in the active training set."
"For our purpose, the dis-tance ought to have the property that two sentences with similar structures have a small distance, even if they are lexically different."
"This leads us to define the distance be-tween two sentences based on their parse trees, which are obtained by applying an existing model to the active train-ing set."
"However, computing the distance of two parse trees requires a digression of how they are represented in our parser."
"A statistical parser computes  , the probability of a parse given a sentence ."
"Since the space of the entire parses is too large and cannot be modeled directly, a parse tree  is decomposed  ."
"In theasparsera series(Jelinekof individualet al., 1994actions) we used in this study, this is accomplished through a bottom-up-left-most (BULM) derivation."
"In the BULM deriva-tion, there are three types of parse actions: tag, label and extension."
"There is a corresponding vocabulary for tag or label, and there are four extension directions: RIGHT, LEFT, UP and UNIQUE."
"If a child node is the only node under a label, the child node is said to extend UNIQUE to its parent node; if there are multiple children under a parent node, the left-most child is said to extend RIGHT to the parent node, the right-most child node is said to extend LEFT to the parent node, while all the other in-termediate children are said to extend UP to their parent node."
The BULM derivation can be best explained by an example in Figure 1.
"Figure 1: Serial decomposition of a parse tree as 17 parsing actions: tags (1,3,5,7,11,13) – blue boxes, labels (9,15,17)–green underlines, extensions (2,4,6,8,10,12,14,16)– red parentheses."
Numbers indi-cate the order of actions.
The input sentence is fly from new york to boston.
"Numbers on its semantic parse tree indicate the order of parse actions while colors indicate types of actions: tags are numbered in blue boxes, extensions in red parentheses and labels in green underlines."
"For this example, the first action is tagging the first word fly given the sentence; the second action is extending the tag wd RIGHT, as the tag wd is the left-most child of the con-stituent S; and the third action is tagging the second word from given the sentence and the two proceeding actions, and so on and so forth."
We define an event as a parse action together with its context.
"It is clear that the BULM derivation converts a parse tree into a unique sequence of parse events, and a valid event sequence corresponds to a unique parse tree."
Therefore a parse tree can be equivalently represented by a sequence of events.
"Let  be the set of tagging ac-tions,  be the labeling actions and !&quot; be the ex-tending actions of , and let $# tions ahead of the action , then % be  thecansequencebe rewrittenof ac-as: % &apos;&amp; )(+* ) -, )%. 0/ &amp; ( +, 4 /  $ 5 6 ( ,+4 /  #$5 # ( ;&lt; ,=4 / &gt; $# &gt; ;@? (1) :"
Note that   ! &amp;JILK .
The three models (1) can be trained using decision trees[REF_CITE].
Note that raw context space MONP #$ ;QSR is too huge to store and manipulate efficiently.
"In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions."
Answers of each question are represented as bitstrings.
"To support questions like “what is the previous word (or tag, label, extension)?”, word, tag, label and extension vocabularies are all en-coded as bitstrings."
"Words are encoded through an au-tomatic clustering algorithm[REF_CITE]while tags, labels and extensions are normally encoded using diagonal bits."
An example can be found[REF_CITE].
"In summary, a parse tree can be represented uniquely by a sequence of events, while each event can in turn be represented as a bitstring."
"With this in mind, we are now ready to define a structural distance for two sentences given an existing model."
"Recall T trainedthat itwithis assumeda small amountthat thereofisannotateda statisticaldataparser. infer structures of two sentences and , we use T To and  ."
"Theanddistanceand getbetweentheir mostandlikely,parsegiventrees T , to decode is defined UFV as the distance U between  and , or:"
"To emphasize UV  the  dependency on T , A we  ? &amp;   (2) denote the dis-tance as ."
Note that we assume here that have similar “true” parses T if they have similar and structures under the current model .
"We have shown in Section 2.1 that a parse tree can be represented by a sequence of events, each of which ing questions."
Let ! )
"X&amp; W ,) can in turn be represented as bitstrings 5/ W ,) /  through W ),  / answer-be the sequence / representation / for /  ) ) ( [\&amp;^] _ ), / where ) &amp; # ,) `  ), ` , and # ,) ` is the context and ), ` is the parsing action of the  event of the parse tree ) ."
We can define UFV the distance between Ufe two sentences as   &amp; U     5g &amp; ! ! (3)
The distance between two sequences ! and ! is com-puted as the editing distance using dynamic program-ming[REF_CITE].
We now describe the
"We take advantage of the fact that contexts # ), ` / R can distance between two individual events. be encoded as bitstrings, and define the distance between two contexts as the Hamming distance between their bit-string representations."
"We further define the distance be-tween two parsing actions as follows: it is either h or a constant &gt; if two parse actions are of the same type (re-call there are three types of parsing actions: tag, label and extension), and infinity if different / types."
"We choose &gt; to be the number of bits in # ,) ` to emphasize the importance i of parsing actions in distance computation."
"Formally, let be U the / type / of &amp; # , ` / # ,+j / $B U n, ` / n+, j / action , then"
"W , ` W +, j (4) where # , ` / # +, j / pq is the Hamming / distance / , and U if , ` &amp; / ,+j U -, ` / ;n+, j / o&amp; r qsqq &gt;h if Y / ( , ` ) = / Y( ,+j / ) I n,ot` &amp; n/ +, j (5) if Y( , ` &amp;t Y( ,+j / ). u"
Computing the editing distance (3) requires dynamic programming and it is computationally extensive.
"To speed up computation, we can choose to ignore the dif-ference U in contexts / / , v&amp; U# , ` / # +, j / LB or in other words, ( U 4) n becomes , ` / ;n+, j /W , ` W +, j -, ` / ;n+, j / @? x (6)"
The distance UFV + makes it possible to characterize how ?+ dense ? +?
"Y|{ a R ,sentencethe densityis.ofGivensamplea set ) ofis definedsentencesas: yz&amp; }  ) &amp;  * ) ~UFV  ] ` ) ? (7)"
"That is, the sample density is defined as the inverse of its average distance to other samples."
We also define the centroid 2 4 of S as 4 &amp; argmax 4 Z }  ) ;@? (8)
"With the model-based distance measure defined above, we can use the K-means algorithm to cluster sentences."
Let y&amp; MS A sketch of the algorithm +? ? ?
YR{ (be[REF_CITE]f sentences) is as followsto be. clustered. 1.
Partition M   ?+?? { R into k ini-tial clusters `  ( &amp; ] ?+? ? ).
"Let h&amp; . [Footnote_2]. Find the centroid  c` for each collection UFV  ` c , that is: A )   c` &amp; argmin  | 4 ZP  3. Re-partition  ` c% +b&amp;] UFVY  ? ?+? { R into  clusters , where UFV  `%c  &amp;zMS f)  )  `c   )  cd # m&amp;t ? 4. Let &amp;\] ."
2 We constrain the centroid to be an element of the set as it is not clear how to “average” sentences.
"Repeat Step 2 and Step 3 untill the al-gorithm converges (e.g., relative change of the total distortion is smaller than a threshold)."
"For each iteration we need to compute:  c the, distance between samples ) and cluster centers  the ` pair-wise distances within each cluster."
"The basic operation here is to compute the distance be-tween two sentences, which involves a dynamic program-ming process and is time-consuming."
"The complexity of this algorithm is, if we assume the N samples are uni-formly  {H B distributed ~ x  £¢_ , or  between { j when  the x k ~¡ clusters  ,. approximatelyIn our experi-ments j dynamic ~ programming ] androutine  ] 0]h , wetimesneed eachto callitera-the tion!"
"To speed up, dynamic programming is constrained so that only the band surrounding the diagonal line[REF_CITE]is allowed, and repeated sentences are stored as a unique copy with its count so that computation for the same sentence pair is never repeated."
The latter is a quite effective for dialog systems as a sentence is often seen more than once in the training corpus.
"Intuitively, we would like to select samples that the cur-rent model is not doing well."
"The current model’s un-certainty about a sentence could be because similar sen-tences are under-represented in the (annotated) training set, or similar sentences are intrinsically difficult."
We take advantage of the availability of parsing scores from the existing statistical parser and propose three entropy-based uncertainty scores.
"After decision trees are grown, we can compute the en-tropy of each leaf node ¦ as: &amp;   ©¨) [0 ¨ 0[ (10) where [ sums over either tag f , { label ­ , ) / or extension vocab-ulary, and ¨ %0[ is simply ®  { ­ , ` / , where ~ %[0 is the count of [ in leaf node ¦ ."
The model entropy k is the weighted sum of k¯§ : ka&amp;  § ~ § k § (11) bility of training events ~ .where ~ § &amp;  ) § %0[ .
Note that  k is the log proba-
"After seeing an unlabeled sentence , we can decode it using the existing model and get its most probable parse ."
"The tree can then be represented by a sequence of events, which can be “poured” down the grown trees, and the count ~ 0§ %[0 can be updated accordingly – denote the updated count as ~±§° 0[ ."
"A new model entropy k ° can be computed based on ±°~ § %0[ , and the absolute difference, after it is normalized by the number of events I$K in , is the change of entropy we are after: k¯²³&amp; k -°I "
It is worth pointing out that k ² is a “local” quantity in we only have to visit leaf nodes where counts change.
"Inthat the vast majority of ~ §° % [0 is equal to ~ [0 , and thus other k² wordscharacterizes,  canhowbe computeda sentenceefficiently“surprises. ” the ex-isting model: if the addition of events due to changes a lot of M § @R , and consequently, k , the sentence is proba-bly not ¨ well represented in the initial training set and k ² will be large."
We would like to annotate these sentences.
Now let us consider another measurement which seeks to address the intrinsic difficulty of a sentence.
"Intuitively, we can consider a sentence more difficult if there are po-tentially more parses."
"We calculate the entropy of the dis-tribution over all candidate parses as the sentence entropy Given a sentence , the existing model T could gener-to measure the intrinsic ambiguity. ate the top ´ most likely parses M µ)  [ &amp;D] _ ´wR , each ) having a probability ¶ ) :"
T   ) ¶ f¼) +*)½ (13) where ) is the [ possible parse and ¶ ) is its associated score.
"Without confusion, we drop ¶ ) ’s dependency on T and define the sentence entropy as: k 4 &amp; )½*  ¨ ) ª  ¨ ) (14) ¨ ) &amp;  ½` ¶* ) ¶ ` ? where: (15)"
"As we can imagine, a long sentence tends to have more possible parsing results not because it is difficult but sim-ply because it is long."
"To counter this effect, we can nor-malize the sentence entropy by the length of sentence to calculate per word entropy of a sentence: k¯¾¿&amp; k 44 (16) where 4 is the number of words in ."
Figure 2 illustrates the distribution of the three differ-ent uncertainty scores versus sentence lengths. k ² favors longer sentences more.
This can be explained as follows: longer sentences tend to have more complex structures ( extension and labeling ) than shorter sentences.
And the models for these complex structures are relatively less trained as compared with models for tagging.
"As a result, longer sentences would have higher change of entropy, in other words, larger impact on models."
"As explained above, longer sentences also have larger sentence entropy."
"After normalizing, this trend is re-versed in word entropy."
All experiments are done with a shallow semantic parser (a.k.a. classer[REF_CITE]) of the natural language understanding part in DARPA Communica-tor (DARPA[REF_CITE]).
We built an initial model using 1000 sentences.
An independent test set consists of 4254 sentences.
A fixed batch size ÀÁ&amp;Â] is used through out our experi-ments.
"Exact match is used to compute the accuracy, i.e., the accuracy is the number of sentences whose decod-ing trees are exactly the same as human annotation di-vided by the number of sentences in the test set."
"The ef-fectiveness of active learning is measured by comparing learning curves (i.e., test accuracy vs. number of training sentences ) of active learning and random selection."
We experimented two basic sample selection algorithms.
"The first one is selecting samples based solely on uncer-tainty scores, while the second one clusters sentences, and then selects the most uncertain ones from each clus-ter. "
"Uncertainty Only: at each active learning iteration, the most uncertain À sentences are selected."
The drawback of this selection method is that it risks selecting outliers because outliers are likely to get high uncertainty scores under the existing models.
"Figure 3 shows the test accuracy of this selection method against the number of samples selected from the active training set. 



"
In the sample selection process we calculated the density of each sample.
"For those samples selected, we also have the knowledge of their correct annotations, which can be used to evalutate the model’s performance on them."
We exploit this knowledge and experiment two weight-ing schemes.
"The effect of weighting samples is highlighted in Ta-ble 1, where results are obtained after 1000 samples are selected using the same uncertainty score &quot;k ¾ , but with different weighting schemes."
Weighting samples by den-sity leads to the best performance.
"Since weighting sam-ples by density is a way to tweak sample distribution of training set toward the distribution of the entire sample space, including unannotated sentences, it indicates that it is important to ensure the distribution of training set matches that of the sample space."
"Therefore, we believe that clustering is a necessary and useful step."
"Figure 7 compares the best learning curve using only un-certainty score(i.e., sentence entropy in Figure 3) to select samples with the best learning curve resulted from clus-tering and the word entropy k&quot;¾ ."
It is clear that clustering results in a better learning curve.
Figure 8 shows the best active learning result compared with that of random selection.
The learning curve for ac-tive learning is obtained using k ¾ as uncertainty measure and selected samples are weighted by density.
"Both ac-tive learning and random selection are run 40 times, each time selecting 100 samples."
The horizontal line on the graph is the performance if all 20K sentences are used.
It is remarkable to notice that active learning can use far less samples ( usually less than one third ) to achieve the same level of performance of random selection.
"And after only about 2800 sentences are selected, the active learning re-sult becomes very close to the best possible accuracy."
"While active learning has been studied extensively in the context of machine learning[REF_CITE], and has been applied to text classifica-ti[REF_CITE]and part-of-speech tagging[REF_CITE], there are only a handful studies on natural language parsing[REF_CITE]and[REF_CITE].[REF_CITE]uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words."
"It is more natural to de-fine uncertainty scores in our study because of the avail-bility of parse scores.[REF_CITE]is related closely to our work in that both use entropy-based un-certainty scores, but Hwa does not characterize the dis-tribution of sample space."
"Knowing the distribution of sample space is important since uncertainty measure, if used alone for sample selection, will be likely to select outliers.[REF_CITE]used an entropy-based criterion to reduce the size of backoff n-gram language models."
The major contribution of this paper is that a model-based distance measure is proposed and used in active learning.
The distance measures structural difference of two sentences relative to an existing model.
Similar idea is also exploited[REF_CITE]where authors use the divergence between the unigram word distributions of two documents to measure their differ-ence.
"This distance enables us to cluster the active train-ing set and a sample is then selected and weighted based on both its uncertainty score and its density.[REF_CITE]applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training."
This is a different venue for reduc-ing annotation work in that the current model output is directly used and no human annotation is assumed.[REF_CITE]also aimed to making use of unla-beled data to improve statistical parsers by transforming model parameters.
We have examined three entropy-based uncertainty scores to measure the “usefulness” of a sample to im-proving a statistical model.
We also define a distance for sentences of natural languages.
"Based on this distance, we are able to quantify concepts such as sentence density and homogeneity of a corpus."
Sentence clustering algo-rithms are also developed with the help of these concepts.
"Armed with uncertainty scores and sentence clusters, we have developed sample selection algorithms which has achieved significant savings in terms of labeling cost: we have shown that we can use one-third of training data of random selection and reach the same level of parsing ac-curacy."
"While we have shown the importance of both con-fidence score and modeling the distribution of sample space, it is not clear whether or not it is the best way to combine or reconcile the two."
It would be nice to have a single number to rank candidate sentences.
"We also want to test the algorithms developed here on other domains (e.g., Wall Street Journal corpus)."
Improving speed of sentence clustering is also worthwhile.
We thank Kishore Papineni and Todd Ward for many use-ful discussions.
The anonymous reviewer’s suggestions to improve the paper is greatly appreciated.
This work is partially supported by DARPA under SPAWAR contract number[REF_CITE]-99-2-8916.
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and con-texts.
"Parameter search with EM produces higher quality analyses than previously exhibited by un-supervised systems, giving the best published un-supervised parsing results on the ATIS corpus."
Ex-periments on Penn treebank sentences of compara-ble length show an even higher F 1 of 71% on non-trivial brackets.
"We compare distributionally in-duced and actual part-of-speech tags as input data, and examine extensions to the basic model."
"We dis-cuss errors made by the system, compare the sys-tem to previous models, and discuss upper bounds, lower bounds, and stability for this task."
The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attenti[REF_CITE].
"Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus[REF_CITE], to use in-duction systems as a first stage in constructing large treebanks (van[REF_CITE]), or to build better lan-guage models[REF_CITE]."
"In previous work, we presented a conditional model over trees which gave the best published re-sults for unsupervised parsing of the ATIS corpus[REF_CITE]."
"However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction."
"Here, we improve on that model in several ways."
"First, we construct a generative model which utilizes the same features."
"Then, we extend the model to allow mul-tiple constituent types and multiple prior distribu- tions over trees."
"The new model gives a 13% reduc-tion in parsing error on WSJ sentence experiments, including a positive qualitative shift in error types."
"Additionally, it produces much more stable results, does not require heavy smoothing, and exhibits a re-liable correspondence between the maximized ob-jective and parsing accuracy."
"It is also much faster, not requiring a fitting phase for each iteration."
"We fol-lowed this for most experiments, but in section 4.3, we use distributionally induced tags as input."
"Perfor-mance with induced tags is somewhat reduced, but still gives better performance than previous models."
"Early work on grammar induction emphasized heu-ristic structure search, where the primary induction is done by incrementally adding new productions to an initially empty grammar[REF_CITE]."
"In the early 1990s, attempts were made to do grammar induction by parameter search, where the broad structure of the grammar is fixed in advance and only parameters are induced[REF_CITE]. [Footnote_1]"
"1 On this approach, the question of which rules are included or excluded becomes the question of which parameters are zero."
"However, this appeared unpromising and most recent work has re-turned to using structure search."
Note that both ap-proaches are local.
"Structure search requires ways of deciding locally which merges will produce a co-herent, globally good grammar."
"To the extent that such approaches work, they work because good lo-cal heuristics have been engineered[REF_CITE]. (b) (c) and (c) the yields and contexts for each constituent span in that bracketing."
"Distituent yields and contexts are not shown, but are modeled."
Parameter search is also local; parameters which are locally optimal may be globally poor.
A con-crete example is the experiments[REF_CITE].
"They restricted the space of gram-mars to those isomorphic to a dependency grammar over the POS symbols in the Penn treebank, and then searched for parameters with the inside-outside algorithm[REF_CITE]starting with 300 random production weight vectors."
"Each seed converged to a different locally optimal grammar, none of them nearly as good as the treebank grammar, measured either by parsing performance or data-likelihood."
"However, parameter search methods have a poten-tial advantage."
"By aggregating over only valid, com-plete parses of each sentence, they naturally incor-porate the constraint that constituents cannot cross – the bracketing decisions made by the grammar must be coherent."
The Carroll and Charniak exper-iments had two primary causes for failure.
"First, random initialization is not always good, or neces-sary."
"The parameter space is riddled with local like-lihood maxima, and starting with a very specific, but random, grammar should not be expected to work well."
"We duplicated their experiments, but used a uniform parameter initialization where all produc-tions were equally likely."
"This allowed the interac-tion between the grammar and data to break the ini-tial symmetry, and resulted in an induced grammar of higher quality than Carroll and Charniak reported."
"This grammar, which we refer to as DEP - PCFG will be evaluated in more detail in section 4."
"The sec-ond way in which their experiment was guaranteed to be somewhat unencouraging is that a delexical-ized dependency grammar is a very poor model of language, even in a supervised setting."
"By the F 1 measure used in the experiments in section 4, an in-duced dependency[REF_CITE].2, compared to a score of 82.1 for a supervised PCFG read from local trees of the treebank."
"However, a supervised dependency PCFG scores only 53.5, not much bet-ter than the unsupervised version, and worse than a right-branching baseline (of 60.0)."
"As an example of the inherent shortcomings of the dependency gram-mar, it is structurally unable to distinguish whether the subject or object should be attached to the verb first."
"Since both parses involve the same set of pro-ductions, both will have equal likelihood."
"To exploit the benefits of parameter search, we used a novel model which is designed specifically to en-able a more felicitous search space."
The funda-mental assumption is a much weakened version of classic linguistic constituency tests[REF_CITE]: constituents appear in constituent contexts.
"A par-ticular linguistic phenomenon that the system ex-ploits is that long constituents often have short, com-mon equivalents, or proforms, which appear in sim-ilar contexts and whose constituency is easily dis-covered (or guaranteed)."
"Our model is designed to transfer the constituency of a sequence directly to its containing context, which is intended to then pressure new sequences that occur in that context into being parsed as constituents in the next round."
"The model is also designed to exploit the successes of distributional clustering, and can equally well be viewed as doing distributional clustering in the pres-ence of no-overlap constraints."
"Unlike a PCFG, our model describes all contigu-ous subsequences of a sentence (spans), including empty spans, whether they are constituents or non-constituents (distituents)."
"A span encloses a se-quence of terminals, or yield, α, such as DT JJ NN ."
"A span occurs in a context x, such as  VBZ , where x is the ordered pair of preceding and following ter- minals ( denotes a sentence boundary)."
"A bracket-ing of a sentence is a boolean matrix B, which in-dicates which spans are constituents and which are not."
"Figure 1 shows a parse of a short sentence, the bracketing corresponding to that parse, and the la-bels, yields, and contexts of its constituent spans."
Figure 2 shows several bracketings of the sen-tence in figure 1.
"A bracketing B of a sentence is non-crossing if, whenever two spans cross, at most one is a constituent in B. A non-crossing bracket-ing is tree-equivalent if the size-one terminal spans and the full-sentence span are constituents, and all size-zero spans are distituents."
Figure 2(a) and (b) are tree-equivalent.
Tree-equivalent bracketings B correspond to (unlabeled) trees in the obvious way.
A bracketing is binary if it corresponds to a binary tree.
Figure 2(b) is binary.
We will induce trees by inducing tree-equivalent bracketings.
Our generative model over sentences S has two phases.
"First, we choose a bracketing B according to some distribution P(B) and then generate the sen-tence given that bracketing:"
"P(S, B) = P(B)P(S|B)"
"Given B, we fill in each span independently."
"The context and yield of each span are independent of each other, and generated conditionally on the con-stituency B ij of that span."
"P(S|B) = Y P(α ij , x ij |B ij ) hi,ji∈spans(S) ="
"Y P(α ij |B ij )P(x ij |B ij ) hi,ji"
The distribution P(α ij |B ij ) is a pair of multinomial distributions over the set of all possible yields: one for constituents (B ij = c) and one for distituents (B ij = d).
Similarly for P(x ij |B ij ) and contexts.
The marginal probability assigned to the sentence S is given by summing over all possible bracketings of S: P(S) = P P(B)P(S|B). 2 B
"To induce structure, we run EM over this model, treating the sentences S as observed and the brack-etings B as unobserved."
The parameters 2 of the model are the constituency-conditional yield and context distributions P(α|b) and P(x|b).
"If P(B) is uniform over all (possibly crossing) brack-etings, then this procedure will be equivalent to soft-clustering with two equal-prior classes."
"There is reason to believe that such soft cluster-ings alone will not produce valuable distinctions, even with a significantly larger number of classes."
"The distituents must necessarily outnumber the con-stituents, and so such distributional clustering will result in mostly distituent classes."
"To underscore the difference between the bracketing and labeling tasks, consider figure 3."
"In both plots, each point is a frequent tag sequence, assigned to the (normalized) vector of its context frequencies."
Each plot has been projected onto the first two prin-cipal components of its respective data set.
The left plot shows the most frequent sequences of three con-stituent types.
"Even in just two dimensions, the clus-ters seem coherent, and it is easy to believe that they would be found by a clustering algorithm in the full space."
"On the right, sequences have been labeled according to whether their occurrences are constituents more or less of the time than a cutoff (of 0.[Footnote_2])."
"2 Viewed as a model generating sentences, this model is defi-cient, placing mass on yield and context choices which will not tile into a valid sentence, either because specifications for posi-tions conflict or because yields of incorrect lengths are chosen. However, we can renormalize by dividing by the mass placed on proper sentences and zeroing the probability of improper brack-etings. The rest of the paper, and results, would be unchanged except for notation to track the renormalization constant."
The distinction between constituent and distituent seems much less easily discernible.
We can turn what at first seems to be distributional clustering into tree induction by confining P(B) to put mass only on tree-equivalent bracketings.
"In par-ticular, consider P bin (B) which is uniform over bi-nary bracketings and zero elsewhere."
"If we take this bracketing distribution, then when we sum over data completions, we will only involve bracketings which correspond to valid binary trees."
This restriction is the basis for our algorithm.
We now essentially have our induction algorithm.
"We take P(B) to be P bin (B), so that all binary trees are equally likely."
We then apply the EM algorithm:
"E-Step: Find the conditional completion likeli-hoods P(B|S, 2) according to the current 2."
"Fix P(B|S, 2) and find the 2 0 which max-imizes P P(B|S, 2) log P(S, B|2 ). 0 B"
"The completions (bracketings) cannot be efficiently enumerated, and so a cubic dynamic program simi-lar to the inside-outside algorithm is used to calcu-late the expected counts of each yield and context, both as constituents and distituents."
Relative fre-quency estimates (which are the ML estimates for this model) are used to set 2 0 .
"To begin the process, we did not begin at the E-step with an initial guess at 2."
"Rather, we began at the M-step, using an initial distribution over com-pletions."
The initial distribution was not the uniform distribution over binary trees P bin (B).
"That was un-desirable as an initial point because, combinatorily, almost all trees are relatively balanced."
"On the other hand, in language, we want to allow unbalanced structures to have a reasonable chance to be discov-ered."
"Therefore, consider the following uniform-splitting process of generating binary trees over k terminals: choose a split point at random, then recur-sively build trees by this process on each side of the split."
"This process gives a distribution P split which puts relatively more weight on unbalanced trees, but only in a very general, non language-specific way."
"This distribution was not used in the model itself, however."
"It seemed to bias too strongly against bal-anced structures, and led to entirely linear-branching structures."
The smoothing used was straightforward.
"For each yield α or context x, we added 10 counts of that item as a constituent and 50 as a distituent."
This re-flected the relative skew of random spans being more likely to be distituents.
"This contrasts with our previ-ous work, which was sensitive to smoothing method, and required a massive amount of it."
We performed most experiments on the 7422 sen-tences in the Penn treebank Wall Street Journal sec-tion which contained no more than 10 words af-ter the removal of punctuation and null elements (WSJ-10).
"Evaluation was done by measuring un-labeled precision, recall, and their harmonic mean F 1 against the treebank parses."
"Constituents which could not be gotten wrong (single words and en-tire sentences) were discarded. [Footnote_3] The basic experi-ments, as described above, do not label constituents."
"3 Since reproducible evaluation is important, a few more notes: this is different from the original (unlabeled) bracket-ing measures proposed in the PARSEVAL standard, which did not count single words as constituents, but did give points for putting a bracket over the entire sentence. Secondly, bracket la-bels and multiplicity are just ignored. Below, we also present results using the EVALB program for comparability, but we note that while one can get results from it that ignore bracket labels, it never ignores bracket multiplicity. Both these alternatives seem less satisfactory to us as measures for evaluating unsu-pervised constituency decisions."
An advantage to having only a single constituent class is that it encourages constituents of one type to be found even when they occur in a context which canonically holds another type.
"For example, NP s and PP s both occur between a verb and the end of the sentence, and they can transfer constituency to each other through that context."
Figure 4 shows the F 1 score for various meth-ods of parsing.
RANDOM chooses a tree uniformly at random from the set of binary trees. 4
This is the unsupervised baseline.
"DEP - PCFG is the re-sult of duplicating the experiments[REF_CITE], using EM to train a dependency-structured PCFG."
"LBRANCH and RBRANCH choose the left- and right-branching structures, respectively."
"RBRANCH is a frequently used baseline for super-vised parsing, but it should be stressed that it en-codes a significant fact about English structure, and an induction system need not beat it to claim a degree of success."
"CCM is our system, as de-scribed above."
"SUP - PCFG is a supervised PCFG parser trained on a 90-10 split of this data, using the treebank grammar, with the Viterbi parse right-binarized. [Footnote_5] UBOUND is the upper bound of how well a binary system can do against the treebank sen-tences, which are generally flatter than binary, limit-ing the maximum precision."
"5 Without post-binarization, the F 1 score was 88.9."
"CCM is doing quite well at 71.1%, substantially better than right-branching structure."
One common issue with grammar induction systems is a tendency to chunk in a bottom-up fashion.
"Especially since the CCM does not model recursive structure explic-itly, one might be concerned that the high overall accuracy is due to a high accuracy on short-span constituents."
Figure 5 shows that this is not true.
"Recall drops slightly for mid-size constituents, but longer constituents are as reliably proposed as short ones."
"Another effect illustrated in this graph is that, for span 2, constituents have low precision for their recall."
"This contrast is primarily due to the single largest difference between the system’s induced structures and those in the treebank: the treebank does not parse into NP s such as DT JJ NN , while our system does, and generally does so correctly, identifying N units like JJ NN ."
This overproposal drops span-2 precision.
"In contrast, figure 5 also shows the F 1 for DEP - PCFG , which does exhibit a drop in F 1 over larger spans."
"The top row of figure 8 shows the recall of non-trivial brackets, split according the brackets’ labels in the treebank."
"Unsurprisingly, NP recall is high-est, but other categories are also high."
"Because we ignore trivial constituents, the comparatively low S represents only embedded sentences, which are somewhat harder even for supervised systems."
"To facilitate comparison to other recent work, fig-ure 6 shows the accuracy of our system when trained on the same WSJ data, but tested on the ATIS cor-pus, and evaluated according to the EVALB pro-gram. [Footnote_6] The F 1 numbers are lower for this corpus and evaluation method. [Footnote_7] Still, CCM beats not only RBRANCH (by 8.3%), but also the previous condi-tional COND - CCM and the next closest unsupervised system (which does not beat RBRANCH in F 1 )."
"6 EMILE and ABL are lexical systems described in (van[REF_CITE]). CDC -40,[REF_CITE], reflects training on much more data (12M words)."
7 The primary cause of the lower F 1 is that the ATIS corpus is replete with span-one NP s; adding an extra bracket around all single words raises our EVALB recall to 71.9; removing all unaries from the ATIS gold standard gives an F 1 of 63.3%.
Parsing figures can only be a component of evaluat-ing an unsupervised induction system.
"Low scores may indicate systematic alternate analyses rather than true confusion, and the Penn treebank is a sometimes arbitrary or even inconsistent gold stan-dard."
"To give a better sense of the kinds of errors the system is or is not making, we can look at which se-quences are most often over-proposed, or most often under-proposed, compared to the treebank parses."
Figure 7 shows the 10 most frequently over- and under-proposed sequences.
The system’s main error trends can be seen directly from these two lists.
"It forms MD VB verb groups systematically, and it at-taches the possessive particle to the right, like a de-terminer, rather than to the left. [Footnote_8]"
"8 Linguists have at times argued for both analyses:[REF_CITE]and[REF_CITE], respectively."
"It provides binary-branching analyses within NP s, normally resulting in correct extra N constituents, like JJ NN , which are not bracketed in the treebank."
"More seriously, it tends to attach post-verbal prepositions to the verb and gets confused by long sequences of nouns."
"A significant improvement over earlier systems is the absence of subject-verb groups, which disappeared when we switched to P split (B) for initial comple-tions; the more balanced subject-verb analysis had a substantial combinatorial advantage with P bin (B)."
"We also ran the system with multiple constituent classes, using a slightly more complex generative model in which the bracketing generates a labeling which then generates the constituents and contexts."
The set of labels for constituent spans and distituent spans are forced to be disjoint.
"Intuitively, it seems that more classes should help, by allowing the system to distinguish different types of constituents and constituent contexts."
"However, it seemed to slightly hurt parsing accuracy overall."
"Figure 8 compares the performance for 2 versus 12 classes; in both cases, only one of the classes was allocated for distituents."
"Overall F 1 dropped very slightly with 12 classes, but the category recall num-bers indicate that the errors shifted around substan-tially."
"PP accuracy is lower, which is not surprising considering that PP s tend to appear rather option-ally and in contexts in which other, easier categories also frequently appear."
"On the other hand, embed-ded sentence recall is substantially higher, possibly because of more effective use of the top-level sen-tences which occur in the signature context ."
"The classes found, as might be expected, range from clearly identifiable to nonsense."
"Note that sim-ply directly clustering all sequences into 12 cate-gories produced almost entirely the latter, with clus-ters representing various distituent types."
Figure 9 shows several of the 12 classes.
Class 0 is the model’s distituent class.
"Its most frequent mem-bers are a mix of obvious distituents ( IN DT , DT JJ , IN DT , NN VBZ ) and seemingly good sequences like NNP NNP ."
"However, there are many sequences of 3 or more NNP tags in a row, and not all adjacent pairs can possibly be constituents at the same time."
"Class 1 is mainly common NP sequences, class 2 is proper NP s, class 3 is NP s which involve numbers, and class 6 is N sequences, which tend to be lin-guistically right but unmarked in the treebank."
"Class 4 is a mix of seemingly good NP s, often from posi-tions like VBZ – NN where they were not constituents, and other sequences that share such contexts with otherwise good NP sequences."
"This is a danger of not jointly modeling yield and context, and of not modeling any kind of recursive structure."
Class 5 is mainly composed of verb phrases and verb groups.
No class corresponded neatly to PP s: perhaps be-cause they have no signature contexts.
The 2-class model is effective at identifying them only because they share contexts with a range of other constituent types (such as NP s and VP s).
"A reasonable criticism of the experiments presented so far, and some other earlier work, is that we as-sume treebank part-of-speech tags as input."
This criticism could be two-fold.
"First, state-of-the-art supervised PCFGs do not perform nearly so well with their input delexicalized."
"We may be reduc-ing data sparsity and making it easier to see a broad picture of the grammar, but we are also limiting how well we can possibly do."
It is certainly worth explor-ing methods which supplement or replace tagged in-put with lexical input.
"However, we address here the more serious criticism: that our results stem from clues latent in the treebank tagging informa-tion which are conceptually posterior to knowledge of structure."
"For instance, some treebank tag dis-tinctions, such as particle ( RP ) vs. preposition ( IN ) or predeterminer ( PDT ) vs. determiner ( DT ) or ad-jective ( JJ ), could be said to import into the tagset distinctions that can only be made syntactically."
"To show results from a complete grammar induc-tion system, we also did experiments starting with a clustering of the words in the treebank."
We used basically the baseline method of word type cluster-ing[REF_CITE](which is close to the meth-ods[REF_CITE]).
"For (all-lowercased) word types in the Penn treebank, a 1000 element vector was made by counting how often each co-occurred with each of the 500 most common words imme-diately to the left or right in Treebank text and ad-ditional 1994–96 WSJ newswire."
"These vectors were length-normalized, and then rank-reduced by an SVD, keeping the 50 largest singular vectors."
"The resulting vectors were clustered into 200 word classes by a weighted k-means algorithm, and then grammar induction operated over these classes."
"We do not believe that the quality of our tags matches that of the better methods[REF_CITE], much less the recent results[REF_CITE]."
"Nevertheless, using these tags as input still gave induced structure substantially above right-branching."
Figure 8 shows the performance with induced tags compared to cor-rect tags.
"Overall F 1 has dropped, but, interestingly, VP and S recall are higher."
"This seems to be due to a marked difference between the induced tags and the treebank tags: nouns are scattered among a dispro-portionally large number of induced tags, increasing the number of common NP sequences, but decreas-ing the frequency of each."
Another issue with previous systems is their sensi-tivity to initial choices.
"The conditional model[REF_CITE]had the drawback that the variance of final F 1 , and qualitative grammars found, was fairly high, depending on small differ-ences in first-round random parses."
"The model pre-sented here does not suffer from this: while it is clearly sensitive to the quality of the input tagging, it is robust with respect to smoothing parameters and data splits."
Varying the smoothing counts a factor of ten in either direction did not change the overall F 1 by more than 1%.
"Training on random subsets of the training data brought lower performance, but constantly lower over equal-size splits."
"Moreover, there are no first-round random decisions to be sen-sitive to; the soft EM procedure is deterministic."
"9 The data likelihood is not shown exactly, but rather we show the linear transformation of it calculated by the system."
"NP recall exhibits the more typical pattern of a sharp rise followed by a slow fall, but the other categories, after some initial drops, all increase until convergence."
These graphs stop at 40 iterations.
"The system actually converged in both likelihood and F 1 by iteration 38, to within a tolerance of 10 −10 ."
"The time to convergence varied according to smooth-ing amount, number of classes, and tags used, but the system almost always converged within 80 iter-ations, usually within 40."
We have presented a simple generative model for the unsupervised distributional induction of hierar-chical linguistic structure.
The system achieves the best published unsupervised parsing scores on the WSJ-[Footnote_10] and ATIS data sets.
10[REF_CITE]find otherwise for PCFGs.
The induction algo-rithm combines the benefits of EM-based parame-ter search and distributional clustering methods.
"We have shown that this method acquires a substan-tial amount of correct structure, to the point that the most frequent discrepancies between the induced trees and the treebank gold standard are systematic alternate analyses, many of which are linguistically plausible."
"We have shown that the system is not re-liant on supervised POS tag input, and demonstrated increased accuracy, speed, simplicity, and stability compared to previous systems."
This paper describes a simple pattern-matching algorithm for recovering empty nodes and identifying their co-indexed an-tecedents in phrase structure trees that do not contain this information.
The pat-terns are minimal connected tree frag-ments containing an empty node and all other nodes co-indexed with it.
"This pa-per also proposes an evaluation proce-dure for empty node recovery procedures which is independent of most of the de-tails of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a gold-standard corpus."
Evaluating the algorithm on the output of Charniak’s parser[REF_CITE]and the Penn treebank[REF_CITE]shows that the pattern-matching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
One of the main motivations for research on pars-ing is that syntactic structure provides important in-formation for semantic interpretation; hence syntac-tic parsing is an important first step in a variety of useful tasks.
"Broad coverage syntactic parsers with good performance have recently become available[REF_CITE], but these typically produce as output a parse tree that only encodes lo-cal syntactic information, i.e., a tree that does not include any “empty nodes”. ([REF_CITE]dis-cusses the recovery of one kind of empty node, viz., WH-traces)."
This paper describes a simple pattern-matching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees.
"Empty nodes encode additional information about non-local dependencies between words and phrases which is important for the interpretation of construc-tions such as WH-questions, relative clauses, etc. [Footnote_1]"
"1 There are other ways to represent this information that do not require empty nodes; however, information about non-local dependencies must be represented somehow in order to interpret these constructions."
"For example, in the noun phrase the man Sam likes the fact the man is interpreted as the direct object of the verb likes is indicated in Penn treebank notation by empty nodes and coindexation as shown in Fig-ure 1 (see the next section for an explanation of why likes is tagged VBZ t rather than the standard VBZ)."
The broad-coverage statistical parsers just men-tioned produce a simpler tree structure for such a rel-ative clause that contains neither of the empty nodes just indicated.
"Rather, they produce trees of the kind shown in Figure 2."
"Unlike the tree depicted in Fig-ure 1, this type of tree does not explicitly represent the relationship between likes and the man."
This paper presents an algorithm that takes as its input a tree without empty nodes of the kind shown in Figure 2 and modifies it by inserting empty nodes and coindexation to produce a the tree shown in Fig-ure 1.
The algorithm is described in detail in sec-tion 2.
"The standard Parseval precision and recall measures for evaluating parse accuracy do not mea-sure the accuracy of empty node and antecedent re-covery, but there is a fairly straightforward extension of them that can evaluate empty node and antecedent recovery, as described in section 3."
"The rest of this section provides a brief introduction to empty nodes, especially as they are used in the Penn Treebank."
"Non-local dependencies and displacement phe-nomena, such as Passive and WH-movement, have been a central topic of generative linguistics since its inception half a century ago."
"However, current linguistic research focuses on explaining the pos-sible non-local dependencies, and has little to say about how likely different kinds of dependencies are."
"Many current linguistic theories of non-local dependencies are extremely complex, and would be difficult to apply with the kind of broad coverage de-scribed here."
"Psycholinguists have also investigated certain kinds of non-local dependencies, and their theories of parsing preferences might serve as the basis for specialized algorithms for recovering cer-tain kinds of non-local dependencies, such as WH dependencies."
All of these approaches require con-siderably more specialized linguitic knowledge than the pattern-matching algorithm described here.
"This algorithm is both simple and general, and can serve as a benchmark against which more complex ap-proaches can be evaluated."
"The pattern-matching approach is not tied to any particular linguistic theory, but it does require a tree-bank training corpus from which the algorithm ex-tracts its patterns."
"We used sections 2–21 of the Penn Treebank as the training corpus; section 24 was used as the development corpus for experimen-tation and tuning, while the test corpus (section 23) was used exactly once (to obtain the results in sec-tion 3)."
Chapter 4 of the Penn Treebank tagging guidelines[REF_CITE]contains an extensive description of the kinds of empty nodes and the use of co-indexation in the Penn Treebank.
Table 1 contains summary statistics on the distribution of empty nodes in the Penn Treebank.
The entry with POS SBAR and no label refers to a “compound” type of empty structure labelled SBAR consisting of an empty complementizer and an empty (moved) S (thus SBAR is really a nonterminal label rather than a part of speech); a typical example is shown in Figure 3.
"As might be expected the distribution is highly skewed, with most of the empty node tokens belonging to just a few types."
"Because of this, a sys-tem can provide good average performance on all empty nodes if it performs well on the most frequent types of empty nodes, and conversely, a system will perform poorly on average if it does not perform at least moderately well on the most common types of empty nodes, irrespective of how well it performs on more esoteric constructions."
This section describes the pattern-matching algo-rithm in detail.
"In broad outline the algorithm can be regarded as an instance of the Memory-Based Learning approach, where both the pattern extrac-tion and pattern matching involve recursively visit-ing all of the subtrees of the tree concerned."
"It can also be regarded as a kind of tree transformation, so the overall system architecture (including the parser) is an instance of the “transform-detransform” ap-proach advocated[REF_CITE]."
The algorithm has two phases.
The first phase of the algorithm extracts the patterns from the trees in the training corpus.
The second phase of the algorithm uses these extracted patterns to insert empty nodes and index their antecedents in trees that do not contain empty nodes.
"Before the trees are used in the train-ing and insertion phases they are passed through a common preproccessing step, which relabels preter-minal nodes dominating auxiliary verbs and transi-tive verbs."
The preprocessing step relabels auxiliary verbs and transitive verbs in all trees seen by the algorithm.
"This relabelling is deterministic and depends only on the terminal (i.e., the word) and its preterminal label."
Auxiliary verbs such as is and being are relabelled as either a AUX or AUXG respectively.
The relabelling of auxiliary verbs was performed primarily because Charniak’s parser (which produced one of the test corpora) produces trees with such labels; experi-ments (on the development section) show that aux-iliary relabelling has little effect on the algorithm’s performance.
The transitive verb relabelling suffixes the preter-minal labels of transitive verbs with “ t”.
"For ex-ample, in Figure 1 the verb likes is relabelled VBZ t in this step."
A verb is deemed transitive if its stem is followed by an NP without any grammatical func-tion annotation at least 50% of the time in the train-ing corpus; all such verbs are relabelled whether or not any particular instance is followed by an NP.
"Intuitively, transitivity would seem to be a power-ful cue that there is an empty node following a verb."
Experiments on the development corpus showed that transitivity annotation provides a small but useful improvement to the algorithm’s performance.
The accuracy of transitivity labelling was not systemati-cally evaluated here.
"Informally, patterns are minimal connected tree fragments containing an empty node and all nodes co-indexed with it."
The intuition is that the path from the empty node to its antecedents specifies im-portant aspects of the context in which the empty node can appear.
"There are many different possible ways of realiz-ing this intuition, but all of the ones tried gave ap-proximately similar results so we present the sim-plest one here."
"The results given below were gener-ated where the pattern for an empty node is the min-imal tree fragment (i.e., connected set of local trees) required to connect the empty node with all of the nodes coindexed with it."
Any indices occuring on nodes in the pattern are systematically renumbered beginning with 1.
"If an empty node does not bear an index, its pattern is just the local tree containing it."
Figure 4 displays the single pattern that would be extracted corresponding to the two empty nodes in the tree depicted in Figure 1.
For this kind of pattern we define pattern match-ing informally as follows.
"If p is a pattern and t is a tree, then p matches t iff t is an extension of p ig-noring empty nodes in p."
"For example, the pattern displayed in Figure 4 matches the subtree rooted un-der SBAR depicted in Figure 2."
"If a pattern p matches a tree t, then it is possible to substitute p for the fragment of t that it matches."
"For example, the result of substituting the pattern shown in Figure 4 for the subtree rooted under SBAR depicted in Figure 2 is the tree shown in Figure 1."
Note that the substitution process must “standardize apart” or renumber indices appropriately in order to avoid accidentally labelling empty nodes inserted by two independent patterns with the same index.
"Pattern matching and substitution can be defined more rigorously using tree automata[REF_CITE], but for reasons of space these def-initions are not given here."
"In fact, the actual implementation of pattern matching and substitution used here is considerably more complex than just described."
It goes to some lengths to handle complex cases such as adjunction and where two or more empty nodes’ paths cross (in these cases the pattern extracted consists of the union of the local trees that constitute the patterns for each of the empty nodes).
"However, given the low frequency of these constructions, there is prob-ably only one case where this extra complexity is justified: viz., the empty compound SBAR subtree shown in Figure 3."
Suppose we have a rank-ordered list of patterns (the next subsection describes how to obtain such a list).
The procedure that uses these to insert empty nodes into a tree t not containing empty nodes is as fol-lows.
"We perform a pre-order traversal of the sub-trees of t (i.e., visit parents before their children), and at each subtree we find the set of patterns that match the subtree."
"If this set is non-empty we sub-stitute the highest ranked pattern in the set into the subtree, inserting an empty node and (if required) co-indexing it with its antecedents."
"Note that the use of a pre-order traversal effec-tively biases the procedure toward “deeper”, more embedded patterns."
"Since empty nodes are typi-cally located in the most embedded local trees of patterns (i.e., movement is usually “upward” in a tree), if two different patterns (corresponding to dif-ferent non-local dependencies) could potentially in-sert empty nodes into the same tree fragment in t, the deeper pattern will match at a higher node in t, and hence will be substituted."
"Since the substitu-tion of one pattern typically destroys the context for a match of another pattern, the shallower patterns no longer match."
"On the other hand, since shal- lower patterns contain less structure they are likely to match a greater variety of trees than the deeper patterns, they still have ample opportunity to apply."
"Finally, the pattern matching process can be speeded considerably by indexing patterns appropri-ately, since the number of patterns involved is quite large (approximately 11,000)."
"For patterns of the kind described here, patterns can be indexed on their topmost local tree (i.e., the pattern’s root node label and the sequence of node labels of its children)."
"After relabelling preterminals as described above, patterns are extracted during a traversal of each of the trees in the training corpus."
Table 2 lists the most frequent patterns extracted from the Penn Tree-bank training corpus.
The algorithm also records how often each pattern was seen; this is shown in the “count” column of Table 2.
The next step of the algorithm determines approx-imately how many times each pattern can match some subtree of a version of the training corpus from which all empty nodes have been removed (regard-less of whether or not the corresponding substitu-tions would insert empty nodes correctly).
"This in-formation is shown under the “match” column in Ta-ble 2, and is used to filter patterns which would most often be incorrect to apply even though they match."
"If c is the count value for a pattern and m is its match value, then the algorithm discards that pattern when the lower bound of a 67% confidence interval for its success probability (given c successes out of m tri-als) is less than 1/2."
"This is a standard technique for “discounting” success probabilities from small sample size data[REF_CITE]. (As ex-plained immediately below, the estimates of c and m given in Table 2 are inaccurate, so whenever the es-timate of m is less than c we replace m by c in this calculation)."
"This pruning removes approximately 2,000 patterns, leaving 9,000 patterns."
The match value is obtained by making a second pre-order traversal through a version of the train-ing data from which empty nodes are removed.
It turns out that subtle differences in how the match value is obtained make a large difference to the algo-rithm’s performance.
Initially we defined the match value of a pattern to be the number of subtrees that match that pattern in the training corpus.
"But as ex- plained above, the earlier substitution of a deeper pattern may prevent smaller patterns from applying, so this simple definition of match value undoubt-edly over-estimates the number of times shallow pat-terns might apply."
"To avoid this over-estimation, af-ter we have matched all patterns against a node of a training corpus tree we determine the correct pat-tern (if any) to apply in order to recover the empty nodes that were originally present, and reinsert the relevant empty nodes."
"This blocks the matching of shallower patterns, reducing their match values and hence raising their success probability. (Undoubt-edly the “count” values are also over-estimated in the same way; however, experiments showed that es-timating count values in a similar manner to the way in which match values are estimated reduces the al-gorithm’s performance)."
"Finally, we rank all of the remaining patterns."
"We experimented with several different ranking crite-ria, including pattern depth, success probability (i.e., c/m) and discounted success probability."
"Perhaps surprisingly, all produced similiar results on the de-velopment corpus."
We used pattern depth as the ranking criterion to produce the results reported be-low because it ensures that “deep” patterns receive a chance to apply.
"For example, this ensures that the pattern inserting an empty NP * and WHNP can apply before the pattern inserting an empty comple-mentizer 0."
The previous section described an algorithm for restoring empty nodes and co-indexing their an-tecedents.
This section describes two evaluation procedures for such algorithms.
"The first, which measures the accuracy of empty node recovery but not co-indexation, is just the standard Parseval eval-uation applied to empty nodes only, viz., precision and recall and scores derived from these."
"In this evaluation, each node is represented by a triple con-sisting of its category and its left and right string po-sitions. (Note that because empty nodes dominate the empty string, their left and right string positions of empty nodes are always identical)."
Let G be the set of such empty node represen-tations derived from the “gold standard” evaluation corpus and T the set of empty node representations derived from the corpus to be evaluated.
"Then as is standard, the precision P , recall R and f-score f are calculated as follows: test corpora: (i) a version of section 23 of the Penn Treebank from which empty nodes, indices and unary branching chains consisting of nodes of the same category were removed, and (ii) the trees produced by Charniak’s parser on the strings of sec-tion 23[REF_CITE]."
"To evaluate co-indexation of empty nodes and their antecedents, we augment the representation of empty nodes as follows."
"The augmented represen-tation for empty nodes consists of the triple of cat-egory plus string positions as above, together with the set of triples of all of the non-empty nodes the empty node is co-indexed with. (Usually this set of antecedents is either empty or contains a single node)."
"Precision, recall and f-score are defined for these augmented representations as before."
"Note that this is a particularly stringent evalua-tion measure for a system including a parser, since it is necessary for the parser to produce a non-empty node of the correct category in the correct location to serve as an antecedent for the empty node."
Table 4 provides these measures for the same two corpora described earlier.
In an attempt to devise an evaluation measure for empty node co-indexation that depends less on syn-tactic structure we experimented with a modified augmented empty node representation in which each antecedent is represented by its head’s category and location. (The intuition behind this is that we do not want to penalize the empty node antecedent-finding algorithm if the parser misattaches modi-fiers to the antecedent).
In fact this head-based an-tecedent representation yields scores very similiar to those obtained using the phrase-based represen-tation.
"It seems that in the cases where the parser does not construct a phrase in the appropriate loca-tion to serve as the antecedent for an empty node, the syntactic structure is typically so distorted that either the pattern-matcher fails or the head-finding algorithm does not return the “correct” head either."
"This paper described a simple pattern-matching al-gorithm for restoring empty nodes in parse trees that do not contain them, and appropriately index-ing these nodes with their antecedents."
The pattern-matching algorithm combines both simplicity and reasonable performance over the frequently occur-ing types of empty nodes.
"Performance drops considerably when using trees produced by the parser, even though this parser’s precision and recall is around 0.9."
Presumably this is because the pattern matching technique requires that the parser correctly identify large tree fragments that encode long-range dependencies not captured by the parser.
"If the parser makes a single parsing error anywhere in the tree fragment matched by a pattern, the pattern will no longer match."
This is not unlikely since the statistical model used by the parser does not model these larger tree fragments.
"It suggests that one might improve performance by integrating parsing, empty node recovery and an-tecedent finding in a single system, in which case the current algorithm might serve as a useful baseline."
"Alternatively, one might try to design a “sloppy” pat-tern matching algorithm which in effect recognizes and corrects common parser errors in these construc-tions."
"Also, it is undoubtedly possible to build pro-grams that can do better than this algorithm on special cases."
"For example, we constructed a Boosting classifier which does recover *U* and empty complementizers 0 more accurately than the pattern-matcher described here (although the pattern-matching algorithm does quite well on these constructions), but this classifier’s performance av-eraged over all empty node types was approximately the same as the pattern-matching algorithm."
"As a comparison of tables 3 and 4 shows, the pattern-matching algorithm’s biggest weakness is its inability to correctly distinguish co-indexed NP * (i.e., NP PRO) from free (i.e., unindexed) NP *."
"This seems to be a hard problem, and lexical infor-mation (especially the class of the governing verb) seems relevant."
"We experimented with specialized classifiers for determining if an NP * is co-indexed, but they did not perform much better than the algo-rithm presented here. (Also, while we did not sys- tematically investigate this, there seems to be a num-ber of errors in the annotation of free vs. co-indexed NP * in the treebank)."
There are modications and variations on this al-gorithm that are worth exploring in future work.
"We experimented with lexicalizing patterns, but the simple method we tried did not improve re-sults."
"Inspired by results suggesting that the pattern-matching algorithm suffers from over-learning (e.g., testing on the training corpus), we experimented with more abstract “skeletal” patterns, which im-proved performance on some types of empty nodes but hurt performance on others, leaving overall per-formance approximately unchanged."
Possibly there is a way to use both skeletal and the original kind of patterns in a single system.
This paper presents a method for incor-porating word pronunciation information in a noisy channel model for spelling cor-rection.
The proposed method builds an explicit error model for word pronuncia-tions.
By modeling pronunciation simi-larities between words we achieve a sub-stantial performance improvement over the previous best performing models for spelling correction.
Spelling errors are generally grouped into two classes[REF_CITE]— typographic and cogni-tive.
Cognitive errors occur when the writer does not know how to spell a word.
In these cases the misspelling often has the same pronunciation as the correct word ( for example writing latex as latecks).
"Typographic errors are mostly errors related to the keyboard; e.g., substitution or transposition of two letters because their keys are close on the keyboard."
Many of the early algorithms for spelling correction are based on the assumption that the cor-rect word differs from the misspelling by exactly one of these operations (M. D.[REF_CITE]; Mayes and F.[REF_CITE]).
"By estimating probabilities or weights for the different edit operations and conditioning on the left and right context for insertions and deletions and allowing multiple edit operations, high spelling correction accuracy has been achieved."
This model reduced the error rate of the best previous model by nearly 50%.
"It proved advantageous to model substitutions of up to 5-letter sequences (e.g ent being mistyped as ant, ph as f, al as le, etc.)"
This model deals with phonetic errors significantly better than previous models since it allows a much larger context size.
"However this model makes residual errors, many of which have to do with word pronunciation."
"For example, the following are triples of misspelling, correct word and (incorrect) guess that the Brill and Moore model made: edelvise edelweiss advise bouncie bouncy bounce latecks latex lacks"
In this work we take the approach of modeling phonetic errors explicitly by building a separate er-ror model for phonetic errors.
"More specifically, we build two different error models using the Brill and Moore learning algorithm."
One of them is a letter-based model which is exactly the Brill and Moore model trained on a similar dataset.
"The other is a phone-sequence-to-phone-sequence error model trained on the same data as the first model, but using the pronunciations of the correct words and the es-timated pronunciations of the misspellings to learn phone-sequence-to-phone-sequence edits and esti-mate their probabilities."
"At classification time, N -best list predictions of the two models are combined using a log linear model."
A requirement for our model is the availability of a letter-to-phone model that can generate pronunci-ations for misspellings.
We build a letter-to-phone model automatically from a dictionary.
The rest of the paper is structured as follows: Section 2 describes the Brill and Moore model and briefly describes how we use it to build our er-ror models.
"Section 3 presents our letter-to-phone model, which is the result of a series of improve-ments on a previously proposed N-gram letter-to-phone model[REF_CITE]."
Section 4 describes the training and test phases of our algorithm in more de-tail and reports on experiments comparing the new model to the Brill and Moore model.
Section 6 con-tains conclusions and ideas for future work.
Many statistical spelling correction methods can be viewed as instances of the noisy channel model.
The misspelling of a word is viewed as the result of cor-ruption of the intended word as it passes through a noisy communications channel.
"The task of spelling correction is a task of finding, for a misspelling w , a correct word r 2 D , where D is a given dictionary and r is the most probable word to have been garbled into w ."
"Equivalently, the problem is to find a word r for which"
P ( r j w ) =
P ( rP ) P ( w ( w ) j r ) is maximized.
"Since the denominator is constant, this is the same as maximizing P ( r ) P ( w j r ) ."
"In the terminology of noisy channel modeling, the distribu-tion P ( r ) is referred to as the source model, and the distribution P ( w j r ) is the error or channel model."
"Typically, spelling correction models are not used for identifying misspelled words, only for propos-ing corrections for words that are not found in a dictionary."
"Notice, however, that the noisy chan-nel model offers the possibility of correcting mis-spellings without a dictionary, as long as sufficient data is available to estimate the source model fac-tors."
"For example, if r = Osama bin Laden and w = Ossama bin Laden, the model will predict that the correct spelling r is more likely than the incor-rect spelling w , provided that"
P ( r ) &lt; PP (( ww jj wr ))
P ( w ) where P ( w j r ) =P ( w j w ) would be approximately the odds of doubling the s in Osama.
"We do not pursue this, here, however."
The model has a set of pa-rameters P ( ! ) for letter sequences of lengths up to 5 .
An extension they presented has refined pa-rameters P ( ! j PSN ) which also depend on the position of the substitution in the source word.
"According to this model, the misspelling is gener-ated by the correct word as follows: First, a person picks a partition of the correct word and then types each partition independently, possibly making some errors."
The probability for the generation of the mis-spelling will then be the product of the substitution probabilities for each of the parts in the partition.
"For example, if a person chooses to type the word bouncy and picks the partition boun cy, the proba-bility that she mistypes this word as boun cie will be P ( boun ! boun )"
P ( cie ! cy ) .
The probability P ( w j r ) is estimated as the maximum over all parti-tions of r of the probability that w is generated from r given that partition.
We use this method to build an error model for letter strings and a separate error model for phone sequences.
"Two models are learned; one model LTR (standing for “letter”) has a set of substitution prob-abilities P ( ! ) where and are character strings, and another model PH (for “phone”) has a set of substitution probabilities P ( ! ) where and are phone sequences."
We learn these two models on the same data set of misspellings and correct words.
"For LTR, we use the training data as is and run the Brill and Moore training algorithm over it to learn the parameters of LTR."
"For PH, we convert the misspelling/correct-word pairs into pairs of pronunciations of the mis-spelling and the correct word, and run the Brill and Moore training algorithm over that."
"For PH, we need word pronunciations for the cor-rect words and the misspellings."
As the misspellings are certainly not in the dictionary we need a letter-to-phone converter that generates possible pronun-ciations for them.
The next section describes our letter-to-phone model.
There has been a lot of research on machine learn-ing methods for letter-to-phone conversion.
"High accuracy is achieved, for example, by using neural networks[REF_CITE], deci-sion trees[REF_CITE], and N -grams[REF_CITE]."
"We use a modified version of the method pro-posed by Fisher, incorporating several extensions re-sulting in substantial gains in performance."
"In this section we first describe how we do alignment at the phone level, then describe Fisher’s model, and fi-nally present our extensions and the resulting letter-to-phone conversion accuracy."
"The machine learning algorithms for converting text to phones usually start off with training data in the form of a set of examples, consisting of let-ters in context and their corresponding phones (clas-sifications)."
"Pronunciation dictionaries are the ma-jor source of training data for these algorithms, but they do not contain information for correspondences between letters and phones directly; they have cor-respondences between sequences of letters and se-quences of phones."
"A first step before running a machine learning algorithm on a dictionary is, therefore, alignment between individual letters and phones."
The align-ment algorithm is dependent on the phone set used.
"We experimented with two dictionaries, the NETtalk dataset and the Microsoft Speech dictionary."
Statis-tics about them and how we split them into training and test sets are shown in Table 1.
The NETtalk dataset contains information for phone level align-ment and we used it to test our algorithm for auto-matic alignment.
The Microsoft Speech dictionary is not aligned at the phone level but it is much big-ger and is the dictionary we used for learning our final letter-to-phone model.
"The NETtalk dictionary has been designed so that each letter correspond to at most one phone, so a word is always longer, or of the same length as, its pronunciation."
"The alignment algorithm has to de-cide which of the letters correspond to phones and which ones correspond to nothing (i.e., are silent)."
"For example, the entry in NETtalk (when we remove the empties, which contain information for phone level alignment) for the word able is ABLE e b L ."
"The correct alignment is A/e B/b L/L E/– , where – de-notes the empty phone."
"In the Microsoft Speech dic-tionary, on the other hand, each letter can naturally correspond to 0 , 1 , or 2 phones."
"For example, the en-try in that dictionary for able is ABLE ey b ax l ."
The correct alignment is A/ey B/b L/ax&amp;l E/– .
"If we also allowed two letters as a group to correspond to two phones as a group, the correct alignment might be A/ey B/b LE/ax&amp;l , but that would make it harder for the machine learning algorithm."
"Our alignment algorithm is an implementa-tion of hard EM (Viterbi training) that starts off with heuristically estimated initial parameters for P ( phones j letter ) and, at each iteration, finds the most likely alignment for each word given the pa-rameters and then re-estimates the parameters col-lecting counts from the obtained alignments."
"Here phones ranges over sequences of 0 (empty), 1 , and 2 phones for the Microsoft Speech dictionary and 0 or 1 phones for NETtalk."
The parameters P ( phones j letter ) were initialized by a method sim-ilar to the one proposed in (Daelemans and van den[REF_CITE]).
Word frequencies were not taken into consideration here as the dictionary contains no fre-quency information.
The method we started with was the N-gram model[REF_CITE].
"From training data, it learns rules that predict the pronunciation of a letter based on m letters of left and n letters of right context."
The rules are of the following form: [ Lm:T:
Rn ! ph 1 p 1 ph 2 p 2 : : : ]
Here Lm stands for a sequence of m letters to the left of T and Rn is a sequence of n letters to the right.
The number of letters in the context to the left and right varies.
We used from 0 to 4 letters on each side.
"For example, two rules learned for the letter B were: [ AB:B:OT ! 1 : 0] and [ B ! b : 96 : 04] , meaning that in the first context the letter B is silent with probability 1 : 0 , and in the second it is pro-nounced as b with probability : 96 and is silent with probability : 04 ."
Training this model consists of collecting counts for the contexts that appear in the data with the se-lected window size to the left and right.
We col-lected counts for all configurations
"Rn for m 2 f 0 ; 1 ; 2 ; 3 ; 4 g , n 2 f 0 ; 1 ; 2 ; 3 ; 4 g that occurred in the data."
The model is applied by choosing for each letter T the most probable translation as pre-dicted by the most specific rule for the context of occurrence of the letter.
"For example, if we want to find how to pronounce the second b in abbot we would chose the empty phone because the first rule mentioned above is more specific than the second."
We implemented five extensions to the initial model which together decreased the error rate of the letter-to-phone model by around 20% .
These are :
Combination of the predictions of several ap-plicable rules by linear interpolation
Rescoring of N -best proposed pronunciations for a word using a trigram phone sequence lan-guage model
Explicit distinction between middle of word versus start or end Rescoring of N -best proposed pronunciations for a word using a fourgram vowel sequence language model
"The performance figures reported[REF_CITE]are significantly higher than our figures us-ing the basic model, which is probably due to the cleaner data used in their experiments and the dif-ferences in phoneset size."
The extensions we implemented are inspired largely by the work on letter-to-phone conversion using decision trees[REF_CITE].
"The last extension, rescoring based on vowel fourgams, has not been proposed previously."
"We tested the algo-rithms on the NETtalk and Microsoft Speech dic-tionaries, by splitting them into training and test sets in proportion 80%/20% training-set to test-set size."
We trained the letter-to-phone models using the training splits and tested on the test splits.
"We are reporting accuracy figures only on the NETtalk dataset since this dataset has been used extensively in building letter-to-phone models, and because phone accuracy is hard to determine for the non-phonetically-aligned Microsoft Speech dictionary."
"For our spelling correction algorithm we use a letter-to-phone model learned from the Microsoft Speech dictionary, however."
The results for phone accuracy and word accuracy of the initial model and extensions are shown in Ta-ble 2.
The phone accuracy is the percentage cor-rect of all phones proposed (excluding the empties) and the word accuracy is the percentage of words for which pronunciations were guessed without any error.
For our data we noticed that the most specific rule that matches is often not a sufficiently good predictor.
By linearly interpolating the probabili-ties given by the five most specific matching rules we decreased the word error rate by 14.3%.
The weights for the individual rules in the top five were set to be equal.
It seems reasonable to combine the predictions from several rules especially because the choice of which rule is more specific of two is arbi-trary when neither is a substring of the other.
"For example, of the two rules with contexts A:B: and :B:B , where the first has 0 right context and the second has 0 left letter context, one heuristic is to choose the latter as more specific since right context seems more valuable than left[REF_CITE]."
How-ever this choice may not always be the best and it proves useful to combine predictions from several rules.
In Table 2 the row labeled “Interpolation of contexts” refers to this extension of the basic model.
Adding a symbol for interior of word produced a gain in accuracy.
"Prior to adding this feature, we had features for beginning and end of word."
Explic-itly modeling interior proved helpful and further de-creased our error rate by 4.3%.
The results after this improvement are shown in the third row of Table 2.
After linearly combining the predictions from the top matching rules we have a probability distribu-tion over phones for each letter.
It has been shown that modeling the probability of sequences of phones can greatly reduce the error[REF_CITE].
We learned a trigram phone sequence model and used it to re-score the N -best predictions from the basic model.
"We computed the score for a sequence of phones given a sequence of letters, as follows:"
Score (log p 1 ; p Y ; : :P: ; ( pp jj 2 n l 1 ; l 2 : : : l n ) = i l 1 ; l 2 : : : l n ) + log Y P ( p j i =1 :::n i p i 1 ;p i 2 ) (1) i =1 :::n
Here the probabilities P ( p j l ;l ::: l ) n are the i 1 2 distributions over phones that we obtain for each let-ter from combination of the matching rules.
The weight for the phone sequence model was esti-mated from a held-out set by a linear search.
This model further improved our performance and the re-sults it achieves are in the fourth row of Table 2.
The final improvement is adding a term from a vowel fourgram language model to equation 1 with a weight .
The term is the log probability of the sequence of vowels in the word according to a four-gram model over vowel sequences learned from the data.
The final accuracy we achieve is shown in the fifth row of the same table.
"As a comparison, the best accuracy achieved[REF_CITE]on NETalk using a similar proportion of training and test set sizes was 65 : 8% ."
"Their system uses more sources of information, such as phones in the left context as features in the decision tree."
They also achieve a large performance gain by combining multiple decision trees trained on separate portions of the training data.
The accuracy of our letter-to-phone model is comparable to state of the art sys-tems.
Further improvements in this component may lead to higher spelling correction accuracy.
Our combined error model gives the probability P CMB ( w j r ) where w is the misspelling and r is a word in the dictionary.
The spelling correction algo-rithm selects for a misspelling w the word r in the dictionary for which the product P ( r ) P ( w j r ) CMB is maximized.
In our experiments we used a uniform source language model over the words in the dictio-nary.
Therefore our spelling correction algorithm se-lects the word r that maximizes P CMB ( w j r ) .
They also showed that the addition of a language model does not obviate the need for a good error model and that improvements in the error model lead to significant improvements in the full noisy channel model.
"We build two separate error models, LTR and PH (standing for “letter” model and “phone” model)."
"The letter-based model estimates a prob-ability distribution P LTR ( w j r ) over words, and the phone-based model estimates a distribution P PH ( pron w j pron r ) over pronunciations."
"Using the PH model and the letter-to-phone model, we de-rive a distribution P PHL ( w j r ) in a way to be made precise shortly."
We combine the two models to esti-mate scores as follows:
S CMB ( w j r ) = log P ( w j r ) +
LTR log P ( w j r ) PHL
The r that maximizes this score will also maxi-mize the probability P CMB ( w j r ) .
The probabilities P PHL ( w j r ) are computed as follows:
P PHL ( w j r ) X P ( pron r; w j r )= = X P ( pron r j r ) pron r pron r P ( w j pron r;r )
This equation is approximated by the expression for P PHL shown in Figure 1 after several simplify-ing assumptions.
The probabilities P ( pron r j r ) are taken to be equal for all possible pronunciations of r in the dictionary.
Next we assume independence of the misspelling from the right word given the pro-nunciation of the right word i.e. P ( w j r; pron r ) =
P ( w j pron r ) .
By inversion of the conditional prob-ability this is equal to P ( pron r j w ) multiplied by P ( w ) =P ( pron r ) .
"Since we do not model these marginal probabilities, we drop the latter factor."
Next the probability P ( pron r j w ) is expressed as X P ( pron w; pron r j w ) pron w which is approximated by the maximum term in the sum.
After the following decomposition:
P ( pron w; pron r j w ) =
"P ( pron w j w ) P ( pron r j w; pron w ) P ( pron w j w ) P ( pron r j pron w ) where the second part represents a final indepen-dence assumption, we get the expression in Figure 1."
The probabilities P ( pron w j w ) are given by the letter-to-phone model.
"In the following subsections, we first describe how we train and apply the individ-ual error models, and then we show performance re-sults for the combined model compared to the letter-based error model."
The error model LTR was trained exactly as de-scribed originally[REF_CITE].
Given a training set of pairs f w ;r g the algorithm ! ) es-timates a set of rewrite probabilities p ( i i which are the basis for computing probabilities P LTR ( w j r ) .
The parameters of the modelPH P PH ( pron w j pron r ) are obtained by training a phone-sequence-to-phone-sequence error model starting from the same training set of pairs f w ; r g i i of misspelling and correct word as for the LTR model.
We convert this set to a set of pronunciations of misspellings and pronunciations of correct words in the following way: For each training sample f w ;r g we generate m training samples i i of corresponding pronunciations where m is the number of pronunciations of the correct word r i in our dictionary.
Each of those m samples is the most probable pronunciation of w i according to our letter-to-phone model paired with one of the possible pronunciations of r i .
"Using this training set, we run the algorithm of Brill and Moore to es-timate a set of substitution probabilities ! for sequences of phones to sequences of phones."
"The probability P PH ( pron w j pron r ) is then computed as a product of the substitution probabilities in the most probable alignment, as Brill and Moore did."
"We tested our system and compared it to the Brill and Moore model on a dataset of around 10 ; 000 pairs of misspellings and corresponding correct words, split into training and test sets."
The ex-act data sizes are 7 ; 385 word pairs in the training set and 1 ; 812 word pairs in the test set.
This set is slightly different from the dataset used in Brill and Moore’s experiments because we removed from the original dataset the pairs for which we did not have the correct word in the pronunciation dictio-nary.
Both models LTR and PH were trained on the same training set.
The interpolation weight that the combined model CMB uses is also set on the train-ing set to maximize the classification accuracy.
At test time we do not search through all possible words r in the dictionary to find the one maximizing Score CMB ( w j r ) .
"Rather, we compute the combi-nation score only for candidate words r that are in the top N according to the P LTR ( w j r ) or are in the top N according to P PH ( pron w j pron r ) for any of the pronunciations of r from the dictionary and any of the pronunciations for w that were proposed by the letter-to-phone model."
The letter-to-phone model returned for each w the 3 most probable pro-nunciations only.
Our performance was better when we considered the top 3 pronunciations of w rather than a single most likely hypothesis.
That is prob-ably due to the fact that the 3 -best accuracy of the letter-to-phone model is significantly higher than its 1 -best accuracy.
"Table 3 shows the spelling correction accuracy when using the model LTR, PH, or both in com-bination."
The table shows N -best accuracy results.
The N -best accuracy figures represent the percent test cases for which the correct word was in the top N words proposed by the model.
We chose the con-text size of 3 for the LTR model as this context size maximized test set accuracy.
Larger context sizes neither helped nor hurt accuracy.
"As we can see from the table, the phone-based model alone produces respectable accuracy results considering that it is only dealing with word pronun-ciations."
"The error reduction of the combined model compared to the letters-only model is substantial: for 1-Best, the error reduction is over 23% ; for 2- Best, 3-Best, and 4-Best it is even higher, reaching over 46% for 4-Best."
"As an example of the influence of pronuncia-tion modeling, in Table 4 we list some misspelling-correct word pairs where the LTR model made an incorrect guess and the combined model CMB guessed accurately."
We have presented a method for using word pro-nunciation information to improve spelling correc-tion accuracy.
The proposed method substantially reduces the error rate of the previous best spelling correction model.
A subject of future research is looking for a bet-ter way to combine the two error models or building a single model that can recognize whether there is a phonetic or typographic error.
"Another interest-ing task is exploring the potential of our model in different settings such as the Web, e-mail, or as a specialized model for non-native English speakers of particular origin."
"In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues pertaining to text reuse and derivation, especially in the context of newspapers using newswire sources."
"Although the reuse of text by journalists has been studied in linguistics, we are not aware of any investigation using existing com-putational methods for this particular task."
"We investigate the classi cation of newspaper articles according to their degree of dependence upon, or deriva-tion from, a newswire source using a simple 3-level scheme designed by jour-nalists."
"Three approaches to measur-ing text similarity are considered: n-gram overlap, Greedy String Tiling, and sentence alignment."
"Measured against a manually annotated corpus of source and derived news text, we show that a combined classi er with fea-tures automatically selected performs best overall for the ternary classi ca-tion achieving an average F 1 -measure score of 0.664 across all three cate-gories."
A topic of considerable theoretical and practical interest is that of text reuse : the reuse of existing written sources in the creation of a new text.
"Of course, reusing language is as old as the retelling of stories, but current technologies for creating, copying and disseminating electronic text, make it easier than ever before to take some or all of any number of existing text sources and reuse them verbatim or with varying degrees of mod-i cation."
"One form of unacceptable text reuse, plagia-rism , has received considerable attention and software for automatic plagiarism detection is now available (see, e.g.[REF_CITE]for a re-cent review)."
But in this paper we present a benign and acceptable form of text reuse that is encountered virtually every day: the reuse of news agency text (called copy ) in the produc-tion of daily newspapers.
"The question is not just whether agency copy has been reused, but to what extent and subject to what transforma-tions."
"Using existing approaches from computa-tional text analysis, we investigate their ability to classify newspapers articles into categories in-dicating their dependency on agency copy."
"The process of gathering, editing and publish-ing newspaper stories is a complex and spe-cialised task often operating within speci c pub-lishing constraints such as: 1) short deadlines; 2) prescriptive writing practice (see, e.g.[REF_CITE]); 3) limits of physical size; 4) readability and audience comprehension, e.g. a tabloid&apos;s vocabulary limitations; 5) journalistic bias, e.g. political and 6) a newspaper&apos;s house style."
"Of-ten newsworkers, such as the reporter and edi-tor, will rely upon news agency copy as the basis of a news story or to verify facts and assess the"
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 152-159. importance of a story in the context of all those appearing on the newswire."
"Because of the na-ture of journalistic text reuse, di erences will arise between reused news agency copy and the original text."
For example consider the follow-ing:
Original (news agency)
A drink-driver who ran into the Queen Mother&apos;s oÆcial Daim-ler was ned $700 and banned from driving for two years.
Rewrite (tabloid) A DRUNK driver who ploughed into the Queen Mother&apos;s limo was ned $700 and banned for two years yes-terday.
This simple example illustrates the types of rewrite that can occur even in a very short sentence.
"The rewrite makes use of slang and exaggeration to capture its readers&apos; attention (e.g. DRUNK, limo, ploughed )."
Deletion (e.g. from driving ) has also been used and the addi-tion of yesterday indicates when the event oc-curred.
"Many of the transformations we ob-served between moving from news agency copy to the newspaper version have also been re-ported by the summarisation community (see, e.g.,[REF_CITE])."
"Given the value of the information news agen-cies supply, the ease with which text can be reused and commercial pressures, it would be bene cial to be able to identify those news sto-ries appearing in the newspapers that have relied upon agency copy in their production."
Potential uses include: 1) monitoring take-up of agency copy; 2) identifying the most reused stories ; 3) determining customer dependency upon agency copy and 4) new methods for charging customers based upon the amount of copy reused.
"Given the large volume of news agency copy output each day, it would be infeasible to identify and quantify reuse manually; therefore an automatic method is required."
"To begin to get a handle on measuring text reuse, we have developed a document-level clas-si cation scheme, indicating the level at which a newspaper story as a whole is derived from agency copy, and a lexical-level classi cation scheme, indicating the level at which individ-ual word sequences within a newspaper story are derived from agency copy."
"This framework rests upon the intuitions of trained journalists to judge text reuse, and not on an explicit lex-ical/syntactic de nition of reuse (which would presuppose what we are setting out to discover)."
"At the document level, newspaper stories are assigned to one of three possible categories coarsely re ecting the amount of text reused from the news agency and the dependency of the newspaper story upon news agency copy for the provision of \facts&quot;."
The categories in-dicate whether a trained journalist can iden-tify text rewritten from the news agency in a candidate derived newspaper article.
"They are: 1) wholly-derived (WD) : all text in the newspaper article is rewritten only from news agency copy; 2) partially-derived (PD) : some text is derived from the news agency, but other sources have also been used; and 3) non-derived (ND) : news agency has not been used as the source of the article; although words may still co-occur between the newspaper article and news agency copy on the same topic, the jour-nalist is con dent the news agency has not been used."
"At the lexical or word sequence level, individ-ual words and phrases within a newspaper story are classi ed as to whether they are used to ex-press the same information as words in news agency copy (i.e. paraphrases) and or used to express information not found in agency copy."
"Once again, three categories are used, based on the judgement of a trained journalist: 1) verba-tim : text appearing word-for-word to express the same information; 2) rewrite : text para-phrased to create a di erent surface appearance, but express the same information and 3) new : text used to express information not appearing in agency copy (can include verbatim/rewritten text, but being used in a di erent context)."
"Based on this conceptual framework, we have constructed a small annotated corpus of news texts using the UK Press Association (PA) as the news agency source and nine British daily newspapers 1 who subscribe to the PA as candi-date reusers."
"The METER corpus[REF_CITE]is a collection of 1716 texts (over 500,000 words) carefully selected from a 12 month period from the areas of law and court reporting (769 stories) and showbusiness (175 stories). 772 of these texts are PA copy and 944 from the nine newspapers."
These texts cover 265 di erent stories[REF_CITE]and all newspaper stories have been manually classi-ed at the document-level.
Many problems in computational text analy-sis involve the measurement of similarity .
"For example, the retrieval of documents to ful l a user information need, clustering documents ac-cording to some criterion, multi-document sum-marisation, aligning sentences from one lan-guage with those in another, detecting exact and near duplicates of documents, plagiarism detec-tion, routing documents according to their style and identifying authorship attribution."
"Meth-ods typically vary depending upon the match-ing method, e.g. exact or partial, the degree to which natural language processing techniques are used and the type of problem, e.g. search-ing, clustering, aligning etc."
"We have not had time to investigate all of these techniques, nor is there space here to review them."
"We have concentrated on just three: ngram overlap mea-sures, Greedy String Tiling, and sentence align-ment."
The rst was investigated because it of-fers perhaps the simplest approach to the prob-lem.
"The second was investigated because it has been successfully used in plagiarism detection, a problem which at least super cially is quite close to the text reuse issues we are investigating."
"Fi-nally, alignment (treating the derived text as a \translation&quot; of the rst) seemed an intriguing idea, and contrasts, certainly with the ngram ap-proach, by focusing more on local, as opposed to global measures of similarity."
"An initial, straightforward approach to assessing the reuse between two texts is to measure the number of shared word ngrams."
This method underlies many of the approaches used in copy detection including the approach taken[REF_CITE].
They measure similarity using the set-theoretic measures of containment and resem-blance of shared trigrams to separate texts writ-ten independently and those with suÆcient sim-ilarity to indicate some form of copying.
We treat each document as a set of overlap-ping n-word sequences (initially considering only n-word types) and compute a similarity score from this.
"Given two sets of ngrams, we use the set-theoretic containment score to measure similarity between the documents for ngrams of length [Footnote_1] to 10 words."
"1 The newspapers include ve popular papers (e.g. The Sun, The Daily Mail, Daily Star, Daily Mirror) and four quality papers (e.g. Daily Telegraph, The Guardian, The Independent and The Times)."
"For a source text A and a possibly derived text B represented by sets of ngrams S n ( A ) and S n ( B ) respectively, the pro-portion of ngrams in B also in A , the ngram con-tainment C n ( A; B ), is given by:"
C n ( A; B ) = j S n ( j AS ) n \ ( BS ) n j ( B ) j (1)
"Informally containment measures the number of matches between the elements of ngram sets S n ( A ) and S n ( B ), scaled by the size of S n ( B )."
In other words we measure the proportion of unique n-grams in B that are found in A .
"The score ranges from 0 to 1, indicating none to all newspaper copy shared with PA respectively."
"We also compare texts by counting only those ngrams with low frequency, in particular those occurring once."
"For 1-grams, this is the same as comparing the hapax legomena which has been shown to discriminate plagiarised texts from those written independently even when lexical overlap between the texts is already high (e.g. 70%)[REF_CITE]."
"Unlike Finlay&apos;s work, we nd that repetition in PA copy 2 drastically re-duces the number of shared hapax legomena thereby inhibiting classi cation of derived and non-derived texts."
"Therefore we compute the containment of hapax legomena (hapax contain-ment) by comparing words occurring once in the newspaper, i.e. those 1-grams in S 1 ( B ) that oc-cur once with all 1-grams in PA copy, S 1 ( A )."
This containment score represents the number of newspaper hapax legomena also appearing at least once in PA copy.
"Greedy String-Tiling (GST) is a substring matching algorithm which computes the degree of similarity between two strings, for exam-ple software code, free text or biological subse-quences[REF_CITE]."
"Compared with previous algorithms for computing string similarity, such as the Longest Common Subsequence or"
"Levenshtein distance, GST is able to deal with transposition of tokens (in earlier approaches transposition is seen as a number of single inser-tions/deletions rather than a single block move)."
The GST algorithm performs a 1:1 matching of tokens between two strings so that as much of one token stream is covered with maximal length substrings from the other (called tiles ).
"In our problem, we consider how much newspaper text can be maximally covered by words from PA copy."
A minimum match length (MML) can be used to avoid spurious matches (e.g. of 1 or 2 tokens) and the resulting similarity between the strings can be expressed as a quantitative similarity match or a qualitative list of common substrings.
Figure 1 shows the result of GST for the example in Section 2.
"Given PA copy A , a newspaper text B and a set of maximal matches, tiles , of a given length between A and B, the similarity, gstsim(A,B) , is expressed as:"
P i 2 tiles length i gstsim ( A; B ) = ([Footnote_2]) j B j
"2 As stories unfold, PA release copy with new, as well as previous versions of the story"
"In the past decade, various alignment algorithms have been suggested for aligning multilingual parallel corpora[REF_CITE]."
These algorithms have been used to map translation equivalents across di erent languages.
"In this speci c case, we investigate whether alignment can map de-rived texts (or parts of them) to their source texts."
"PA copy may be subject to various changes during text reuse, e.g. a single sen-tence may derive from parts of several source sentences."
"Therefore, strong correlations of sen-tence length between the derived and source sentences cannot be guaranteed."
"As a result, sentence-length based statistical alignment al-gorithms[REF_CITE]are not appropriate for this case."
"On the other hand, cognate-based algorithms[REF_CITE]are more eÆcient for coping with change of text format."
"There-fore, a cognate-based approach is adopted for the METER task."
"Here cognates are de ned as pairs of terms that are identical, share the same stems, or are substitutable in the given context."
The algorithm consists of two principal com-ponents: a comparison strategy and a scoring function.
"In brief, the comparison works as fol-lows (more details may be found[REF_CITE])."
For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to nd the best match.
A DT sentence is allowed to match up to three possibly non-consecutive ST sentences.
The candidate pair with the highest score (see be-low) above a threshold is accepted as a true alignment.
"If no such candidate is found, the DT sentence is assumed to be independent of the ST ."
"Based on individual DT sentence align-ments, the overall possibility of derivation for the DT is estimated with a score ranging be- tween 0 and 1."
This score re ects the propor-tion of aligned sentences in the newspaper text.
"Note that not only may multiple sentences in the ST be aligned with a single sentence in the DT , but also multiple sentences in the DT may be aligned with one sentence in the ST ."
"Given a candidate derived sentence DS and a proposed (set of) source sentence(s) SS , the scoring function works as follows."
Three basic measures are computed for each pair of candi-date DS and SS : SNG is the sum of lengths of the maximum length non-overlapping shared n-grams with n 2; SWD is the number of matched words sharing stems not in an n-gram guring in SNG ; and SUB is the number of substitutable terms (mainly synonyms) not g-uring in SNG or SWD .
Let L 1 be the length of the candidate DS and L 2 the length of candidate SS .
"Then, three scores P D , P S (Dice score) and PV S are calculated as follows:"
P SD = SW D + SNG + SUB L 1 PS = 2( SW D + SNG + SUB )
L 1 + L 2 P SNG = SW D + SNSNGG +
These three scores re ect di erent aspects of relations between the candidate DS and SS : 1.
The proportion of the DS which is shared material. 2. PS:
The proportion of shared terms in DS and SS .
"This measure prefers SS &apos;s which not only contain many terms in the DS , but also do not contain many additional terms. 3."
The proportion of matching n-grams amongst the shared terms.
"This measure captures the intuition that sen-tences sharing not only words, but word se-quences are more likely to be related."
"These three scores are weighted and combined together to provide an alignment metric WS (weighted score), which is calculated as follows:"
WS = Æ 1 PSD + Æ 2 PS + Æ 3 PSNG where Æ 1 + Æ 2 + Æ 3 = 1.
The three weighting vari-ables Æ i ( i = 1 ; 2 ; 3) have been determined empir-ically and are currently set to:
Æ 1 = 0 : 85 ;Æ 2 = 0 : 05 ; Æ 3 = 0 : 1.
"To evaluate the previous approaches for measur-ing text reuse at the document-level , we cast the problem into one of a supervised learning task."
We used similarity scores as attributes for a ma-chine learning algorithm and used the Weka 3.2 software[REF_CITE].
"Because of the small number of examples, we used tenfold cross-validation repeated 10 times (i.e. 10 runs ) and combined this with strati cation to ensure approximately the same proportion of samples from each class were used in each fold of the cross-validation."
"For each newspaper text, we compared PA source texts from the same story to create results in the form: newspaper; class; score ."
These results were ordered according to each set to create the same 10 datasets for each approach thereby en-abling comparison.
Using this data we rst trained ve single-feature Naive Bayes classi ers to do the ternary classi cation task.
"The feature in each case was a variant of one of the three similarity measures described in Section 4, computed between the two texts in the training set."
The target classi -cation value was the reuse classi cation category from the corpus.
"A Naive Bayes classi er was used because of its success in previous classi -cation tasks, however we are aware of its naive assumptions that attributes are assumed inde-pendent and data to be normally distributed."
We evaluated results using the F 1 -measure (harmonic mean of precision and recall given equal weighting).
"For each run, we calculated the average F 1 score across the classes."
The overall average F 1 -measure scores were com-puted from the 10 runs for each class (a single accuracy measure would suÆce but the Weka package outputs F 1 -measures).
Statistical di erences between results were identi ed using Bonferroni analy-sis 3 .
"After examining the results of these single fea-ture classi ers, we also trained a \combined&quot; classi er using a correlation-based lter ap-proach[REF_CITE]to select the com-bination of features giving the highest classi ca-tion score ( correlation-based ltering evaluates all possible combinations of features)."
Feature selection was carried for each fold during cross-validation and features used in all 10 folds were chosen as candidates.
Those which occurred in at least 5 of the 10 runs formed the nal selec-tion.
"We also tried splitting the training data into various binary partitions (e.g. WD/PD vs. ND) and training binary classi ers, using feature se-lection, to see how well binary classi cation could be performed."
We then com-puted how well such a cascaded classi er should perform using the best binary classi er results.
Table 1 shows the results of the single ternary classi ers.
The baseline F 1 measure is based upon the prior probability of a document falling into one of the classes.
The gures in parenthe-sis are the standard deviations for the F 1 scores across the ten evaluation runs.
The nal row shows the results for combining features selected using the correlation-based lter.
Table 2 shows the result of training binary classi ers using feature selection to select the most discriminating features for various binary splits of the training data.
"For both ternary and binary classi ers feature selection produced better results than using all possible features, with the one exception of the binary classi cation between PD and ND."
"From Table 1, we nd that all classi er results are signi cantly higher than the baseline (at p &lt; 0 : 01) and all di erences are signi cant ex-cept between hapax containment and alignment."
"The highest F-measure for the [Footnote_3]-class problem is 0.664 for the \combined&quot; classi er, which is signi cantly greater than 0.651 obtained with-out."
3 Using[REF_CITE].0 for Windows.
"We notice that highest WD classi cation is with alignment at 0.774, highest PD classi-cation is 0.654 with hapax containment and highest ND classi cation is 0.629 with combined features."
Using hapax containment gives higher results than 1-gram containment alone and in fact provides results as good as or better than the more complex sentence alignment and GST approaches.
"Previous research[REF_CITE]and[REF_CITE]had shown derived texts could be distinguished using trigram overlap and tiling with a match length of 3 or more, respectively."
"However, our results run counter to this be-cause the highest classi cation scores are ob-tained with 1-grams and an MML of 1, i.e. as n or MML length increases, the F 1 scores de-crease."
We believe this results from two factors which are characteristic of reuse in journalism.
"First, since even ND texts are thematically sim-ilar (same events being described) there is high likelihood of coincidental overlap of ngrams of length 3 or more (e.g. quoted speech)."
"Secondly, when journalists rewrite it is rare for them not to vary the source."
For the intended application { helping the PA to monitor text reuse { the cost of di erent mis-classi cations is not equal.
"If the classi er makes a mistake, it is better that WD and ND texts are mis-classi ed as PD, and PD as WD."
"Given the di erence in distribution of documents across classes where PD contains the most documents, the classi er will be biased towards this class anyway as required."
Table 3 shows the confu-sion matrix for the combined ternary classi er.
"Although the overall F 1 -measure score is low (0.664), mis-classi cation of both WD as ND and ND as WD is also very low, as most mis- classi cations are as PD."
"Note the high mis-classi cation of PD as both WD and ND, re-ecting the diÆculty of separating this class."
"From Table 2, we nd alignment is a selected feature for each binary partition of the data."
"The highest binary classi cation is achieved be-tween the WD and ND classes using alignment only, and the highest three scores show WD is the easiest class to separate from the others."
"The PD class is the hardest to isolate, re ect-ing the mis-classi cations seen in Table 3."
To predict how well a cascaded binary classi-er will perform we can reason as follows.
From the preceding discussion we see that WD can be separated most accurately; hence we choose WD versus PD/ND as the rst binary classi er.
This forces the second classi er to be PD versus ND.
From the results in Table 2 and the follow-ing equation to compute the F 1 measure for a two-stage binary classi er
"W D + ( P D=ND )( PD +2 ND ) 2 we obtain an overall F 1 measure for ternary clas-sication of 0.703, which is signi cantly higher than the best single stage ternary classi er."
"In this paper we have investigated text reuse in the context of the reuse of news agency copy, an area of theoretical and practical interest."
We present a conceptual framework in which we measure reuse and based on which the METER corpus has been constructed.
"We have presented the results of using similarity scores, computed using n-gram containment, Greedy String Tiling and an alignment algorithm, as attributes for a supervised learning algorithm faced with the task of learning how to classify newspaper sto-ries as to whether they are wholly, partially or non-derived from a news agency source."
We show that the best single feature ternary clas-si er uses either alignment or simple hapax con-tainment measures and that a cascaded binary classi er using a combination of features can outperform this.
"The results are lower than one might like, and re ect the problems of measuring journalis- tic reuse, stemming from complex editing trans-formations and the high amount of verbatim text overlapping as a result of thematic simi-larity and \expected&quot; similarity due to, e.g., di-rect/indirect quotes."
"Given the relative close-ness of results obtained by all approaches we have considered, we speculate that any compar-ison method based upon lexical similarity will probably not improve classi cation results by much."
"Perhaps improved performance at this task may possible by using more advanced nat-ural language processing techniques, e.g. better modeling of the lexical variation and syntactic transformation that goes on in journalistic reuse."
Nevertheless the results we have obtained are strong enough in some cases (e.g. wholly derived texts can be identi ed with &gt; 80% accuracy) to begin to be exploited.
"In summary measuring text reuse is an excit-ing new area that will have a number of appli-cations, in particular, but not limited to, mon-itoring and controlling the copy produced by a newswire."
We are adapting the GST algorithm to deal with simple rewrites (e.g. synonym substitution) and to observe the e ects of rewriting upon nding longest common substrings.
We are also experi-menting using the more detailed METER corpus lexical-level annotations to investigate how well the GST and ngrams approaches can identify reuse at this level.
"A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [Footnote_4] and will continue to be enhanced."
"Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes, radiology reports and discharge summaries."
"In the medical domain, a significant part of the general problem of text normalization is abbreviation and acronym disambiguation."
Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document.
In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization.
"I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~89% accuracy."
"Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes, radiology reports and discharge summaries, to name a few."
"In the medical domain, a significant part of the general problem of text normalization is abbreviation and acronym [Footnote_1] disambiguation."
"1 To save space and for ease of presentation, I will use the word “abbreviation” to mean both “abbreviation” and “acronym” since the two could be used interchangeably for the purposes described in this paper."
Numerous abbreviations are used routinely throughout such texts and identifying their meaning is critical to understanding of the document.
The problem is that abbreviations are highly ambiguous with respect to their meaning.
"For example, according to UMLS  [Footnote_2] (2001), RA may stand for “rheumatoid arthritis”, “renal artery”, “right atrium”, “right atrial”, “refractory anemia”, “radioactive”, “right arm”, “rheumatic arthritis,” etc."
"2 Unified Medical Language System  , a database containing biomedical information and a tools repository developed at the National Library of Medicine to help helath professionals as well as medical informatics researchers."
"In addition to problems with text interpretation,[REF_CITE]also point out that abbreviations constitute a major source of errors in a system that automatically generates lexicons for medical NLP applications."
"Ideally, when looking for documents containing “rheumatoid arthritis”, we want to retrieve everything that has a mention of RA in the sense of “rheumatoid arthritis” but not those documents where RA means “right atrial.”"
"In a way, abbreviation normalization problem is a special case of the word sense disambiguation (WSD) problem."
"Modern approaches to WSD include supervised machine learning techniques, where some amount of training data is marked up by hand and is used to train a classifier."
One such technique involves using a decision tree classifier[REF_CITE].
"On the other side of the spectrum, the fully unsupervised learning methods such as clustering have also been successfully used[REF_CITE]."
A hybrid class of machine learning techniques for WSD relies on a small set of hand labeled data used to bootstrap a larger corpus of training data ([REF_CITE]).
"Regardless of the technique that is used for WSD, the most important part of the process is the context in which the word appears[REF_CITE]."
This is also true for abbreviation normalization.
"For the problem at hand, one way to take context into account is to encode the type of discourse in which the abbreviation occurs, where discourse is defined narrowly as the type of the medical document and the medical specialty, into a set of explicit rules."
"If we see RA in a cardiology report, then it can be normalized to “right atrial”; otherwise, if it occurs in the context of a rheumatology note, it is likely to mean “rheumatoid arthritis” or “rheumatic arthritis.”"
This method of explicitely using global context to resolve the abbreviation ambiguity in suffers from at least three major drawbacks from the standpoint of automation.
"First of all, it requires a database of abbreviations and their expansions linked with possible contexts in which particular expansions can be used, which is an error-prone labor intensive task."
"Second, it requires a rule-based system for assigning correct expansions to their abbreviations, which is likely to become fairly large and difficult to maintain."
"Third, the distinctions made between various meanings are bound to be very coarse."
"We may be able to distinguish correctly between “rheumatoid arthritis” and “right atrial” since the two are likely to occur in clearly separable contexts; however, distinguishing between “rheumatoid arthritis” and “right arm” becomes more of a challenge and may require introducing additional rules to further complicate the system."
"The approach I am investigating falls into the hybrid category of bootstrapping or semi-supervised approaches to training classifiers; however, it uses a different notion of bootstrapping from that[REF_CITE]and[REF_CITE]."
The bootstrapping portion of this approach consists of using a hand crafted table of abbreviations and their expansions pertinent to the medical domain.
This should not be confused with dictionary or semantic network approaches.
The table of abbreviations and their expansions is just a simple list representing a one-to-many relationship between abbreviations and their possible “meanings” that is used to automatically label the training data.
To disambiguate the “meaning” of abbreviations I am using a Maximum Entropy (ME) classifier.
"Maximum Entropy modeling has been used successfully in the recent years for various NLP tasks such as sentence boundary detection, part-of-speech tagging, punctuation normalization, etc. ([REF_CITE])."
In this paper I will demonstrate using Maximum Entropy for a mostly data driven process of abbreviation normalization in the medical domain.
"In the following sections, I will briefly describe Maximum Entropy as a statistical technique."
I will also describe the process of automatically generating training data for ME modeling and present examples of training and testing data obtained from a medical sub-domain of rheumatology.
"Finally, I will discuss the training and testing process and present the results of testing the ME models trained on two different data sets."
One set contains one abbreviation per training/testing corpus and the other -- multiple abbreviations per corpus.
Both sets show around 89% accuracy results when tested on the held-out data.
"The data that was used for this study consists of a corpus of ~10,000 clinical notes (medical dictations) extracted at random from a larger corpus of 171,000 notes (~400,000 words) and encompasses one of many medical specialties at the Mayo Clinic – rheumatology."
"In the Mayo Clinic’s setting, each clinical note is a document recording information pertinent to treatment of a patient that consists of a number of subsections such as Chief Complaint (CC), History of Present Illness (HPI), Impresssion/Report/Plan (IP), Final Diagnoses (DX) [Footnote_3] , to name a few."
"3 This format is specific to the Mayo Clinic. Probably the most commonly used format outside of Mayo is the so-called SOAP format that stands for Subjective, Objective, Assessment, Plan. The idea is the same, but the granularity is lower."
"In clinical settings other than the Mayo Clinic, the notes may have different segmentation and section headings; however, most clinical notes in most clinical settings do have some sort of segmentation and contain some sort of discourse markers, such as CC, HPI, etc., that can be useful clues to tasks such as the one discussed in this paper."
"Theoretically, it is possible that an abbreviation such as PA may stand for “paternal aunt” in the context of Family History (FH), and “polyarthritis” in the Final Diagnoses context."
ME technique lends itself to modeling information that comes from a number of heterogeneous sources such as various levels of local and discourse context.
One of the challenging tasks in text normalization discussed in the literature is the detection of abbreviations in unrestricted text.
"Various techniques, including ME, have proven useful for detecting abbreviations with varying degrees of success. ([REF_CITE]Park and"
"It is important to mention that the methods described in this paper are different from abbreviation detection; however, they are meant to operate in tandem with abbreviation detection methods."
Two types of methods will be discussed in this section.
"First, I will briefly introduce the Maximum Entropy modeling technique and then the method I used for generating the training data for ME modeling."
This section presents a brief description of ME.
"A more detailed and informative description can be found[REF_CITE][Footnote_4] ,[REF_CITE],[REF_CITE]to name just a few."
4 This paper presents an Improved Iterative Scaling but covers the Generalized Iterative Scaling as well.
"Maximum Entropy is a relatively new statistical technique to Natural Language Processing, although the notion of maximum entropy has been around for a long time."
One of the useful aspects of this technique is that it allows to predefine the characteristics of the objects being modeled.
The modeling involves a set of predefined features or constraints on the training data and uniformly distributes the probability space between the candidates that do not conform to the constraints.
"Since the entropy of a uniform distribution is at its maximum, hence the name of the modeling technique."
Features are represented by indicator functions of the following kind [Footnote_5] :
5 Borrowed from Ratnaparkhi implementation of POS tagger.
"F(o,c) = 1, if o = x and c = y (1) 0, otherwise"
Where “o” stands for outcome and “c” stands for context.
This function maps contexts and outcomes to a binary set.
"For example, to take a simplified part-of-speech tagging example, if y = “the” and x=”noun”, then F(o,c) = 1, where y is the word immediately preceding x."
This means that in the context of “the” the next word is classified as a noun.
"To find the maximum entropy distribution the Generalized Iterative Scaling (GIS) algorithm is used, which is a procedure for finding the maximum entropy distribution that conforms to the constraints imposed by the empirical distribution of the modeled properties in the training data [Footnote_6] ."
6 A consice step-by-step description and an explanation of the algorithm itself can be found[REF_CITE].
"For the study presented in this paper, I used an implementation of ME that is similar to that of Ratnaparkhi’s and has been developed as part of the open source Maxent 1.2.4 package [Footnote_7] . (Jason Baldridge, Tom Morton, and Gann Bierner,[URL_CITE]"
7 The ContextGenerator class of the maxent package was modified to allow for the features discussed in this paper.
"In the Maxent implementation, features are reduced to contextual predicates, represented by the variable y in (1)."
"Just as an example, one of such contextual predicates could be the type of discourse that the outcome “o” occurs in: PA paternal aunt | y = FH; PA polyarthritis | y = DX."
"Of course, using discourse markers as the only contextual predicate may not be sufficient."
Other features such as the words surrounding the abbreviation in question may have to be considered as well.
For this study two kinds of models were trained for each data set: local context models (LCM) and combo (CM) models.
"The former were built by training on the sentence-level context only defined as two preceding (w i-2 ,w i-1 ) and two following (w i+1 ,w i+2 ) words surrounding an abbreviation expansion."
The latter kind is a model trained on a combination of sentence and section level contexts defined simply as the heading of the section in which an abbreviation expansion was found.
"In order to generate the training data, first, I identify potential candidates for an abbreviation by taking the list of expansions from a UMLS database and applying it to the raw corpus of text data in the following manner."
The expansions for each abbreviation found in the UMLS’s LRABR table are loaded into a hash indexed by the abbreviation.
The raw text of clinical notes is input and filtered through a dynamic sliding- window buffer whose maximum window size is set to the maximum length of any abbreviation expansion in the UMLS.
"When a match to an expansion is found, the expansion and it’s context are recorded in a training file as if the expansion were an actual abbreviation."
The file is fed to the ME modeling software.
"In this particular implementation, the context of 7 words to the left and 7 words to the right of the found expansion as well as the section label in which the expansion occurs are recorded; however, not all of this context ended up being used in this study."
"This methodology makes a reasonable assumption that given an abbreviation and one of it’s expansions, the two are likely to have similar distribution."
"For example, if we encounter a phrase like “rheumatoid arthritis”, it is likely that the context surrounding the use of an expanded phrase “rheumatoid arthritis” is similar to the context surrounding the use of the abbreviation “RA” when it is used to refer to rheumatoid arthritis."
The following subsection provides additional motivation for using expansions to simulate abbreviations.
"Just to get an idea of how similar are the contexts in which abbreviations and their expansions occur, I conducted the following limited experiment."
"I processed a corpus of all available rheumatology notes (171,000) and recorded immediate contexts composed of words in positions {w i-1, w i-2 ,w i+1, w i+2 } for one unambiguous abbreviation – DJD (degenerative joint disease)."
Here w i is either the abbreviation DJD or its multiword expansion “degenerative joint disease.”
"Since this abbreviation has only one possible expansion, we can rely entirely on finding the strings “DJD” and “degenerative joint disease” in the corpus without having to disambiguate the abbreviation by hand in each instance."
"For each instance of the strings “DJD” and “degenerative joint disease” , I recorded the frequency with which words (tokens) in positions w i-1, w i-2 , w i+1 and w i+2 occur with that string as well as the number of unique strings (types) in these positions."
"It turns out that “DJD” occurs 2906 times , “degenerative joint disease” occurs 2517 times."
The overlap between DJD and its expansion is 115 types in w i-1 position and 66 types in w i+1 position.
"Table 2 summarizes the results for all four {w i-1, w i-2 ,w i+1, w i+2 } positions."
"On average, the overlap between the contexts in which DJD and “degenerative joint disease” occur is around 50%, which is a considerable number because this overlap covers on average 91% of all occurrences in w i-1 and w i+1 as well as w i-2 and w i+2 positions."
One of the questions that arose during implementation is whether it would be better to build a large set of small ME models trained on sub-corpora containing context for each abbreviation of interest separately or if it would be more beneficial to train one model on a single corpus with contexts for multiple abbreviations.
This was motivated by the idea that ME models trained on corpora focused on a single abbreviation may perform more accurately; even though such approach may be computationally expensive.
"For this study, I generated two sets of data."
"The first set (Set A) is composed of training and testing data for 6 abbreviations (NR, PA, PN, BD, INF, RA), where each training/testing subset contains only one abbreviation per corpus. resulting in six subsets."
Table 1 shows the potential expansions for these abbreviations that were actually found in the training corpora.
Not all of the possible expansions found in the UMLS for a given abbreviations will be found in the text of the clinical notes.
Table 3 shows the number of expansions actually found in the rheumatology training data for each of the 6 abbreviations listed in Table 1 as well as the expansions found for a given abbreviation in the UMLS database.
The UMLS database has on average 3 times more variability in possible expansions that were actually found in the given set of training data.
"This is not surprising because the training data was derived from a relatively small subset of 10,000 notes."
"The other set (Set B) is similar to the first corpus of training events; however, it is not limited to just one abbreviation sample per corpus."
"Instead, it is compiled of training samples containing expansions from 69 abbreviations."
The abbreviations to include in the training/testing were selected based on the following criteria: a. has at least two expansions b. has 100-1000 training data samples
The data compiled for each set and subset was split at random in the 80/20 fashion into training and testing data.
The two types of ME models (LCM and CM) were trained for each subset on 100 iterations through the data with no cutoff (all training samples used in training).
"To summarize the goals of this study, one of the main questions in this study is whether local sentence-level context can be used successfully to disambiguate abbreviation expansion."
"Another question that naturally arose from the structure of the data used for this study is whether more global section-level context indicated by section headings such as “chief complaint”, “history of present illness” , etc., would have an effect on the accuracy of predicting the abbreviation expansion."
"Finally, the third question is whether it is more beneficial to construct multiple ME models limited to a single abbreviation."
"To answer these questions, 4 sets of tests were conducted: 1."
Local Context Model and Set A 2.
Combo Model and Set A 3.
Local Context Model and Set B 4.
Combo Model and Set B
"The results in Table 3 show that, on average, after a ten-fold cross-validation test, the expansions for the given 6 abbreviations have been predicted correctly 89.14%."
"Table 3 as well as table 4 display the accuracy, the number of training and testing events/samples, the number of outcomes (possible expansions for a given abbreviation) and the number of contextual predicates averaged across 10 iterations of the cross-validation test."
Table 4 presents the results of the Combo approach with the data also from Set A.
The results of the combined discourse + local context approach are only slightly better that those of the sentence-level only approach.
Table 5 displays the results for the set of tests performed on data containing multiple abbreviations – Set B but contrasts the Local Context Model with the Combo Model.
The first row shows that the LCM model performs with 89.17% accuracy.
CM’s result is very close: 89.01%.
"Just as with Tables 3 and 4, the statistics reported in Table 5 are averaged across 10 iterations of cross-validation."
The results of this study suggest that using Maximum Entropy modeling for abbreviation disambiguation is a promising avenue of research as well as technical implementation for text normalization tasks involving abbreviations.
Several observations can be made about the results of this study.
"First of all, the accuracy results on the small pilot sample of 6 abbreviations as well as the larger sample with 69 abbreviations are quite encouraging in light of the fact that the training of the ME models is largely unsupervised [Footnote_8] ."
"8 With the exception of having to have a database of acronym/abbreviations and their expansions which has to be compiled by hand. However, once such list is compiled, any amount of data can be used for training with no manual annotation."
Another observation is that it appears that using section-level context is not really beneficial to abbreviation expansion disambiguation in this case.
"The results, however, are not by any means conclusive."
It is entirely possible that using section headings as indicators of discourse context will prove to be beneficial on a larger corpus of data with more than 69 abbreviations.
The abbreviation/acronym database in the UMLS tends to be more comprehensive than most practical applications would require.
"For example, the Mayo Clinic regards the proliferation of abbreviations and acronyms with multiple meanings as a serious patient safety concern and makes efforts to ensure that only the “approved” abbreviations (these tend to have lower ambiguity) are used in clinical practice, which would also make the task of their normalization easier and more accurate."
It may still be necessary to use a combination of the UMLS’s and a particular clinic’s abbreviation lists in order to avoid missing occasional abbreviations that occur in the text but have not made it to the approved clinic’s list.
This issue also remains to be investigated.
"In the future, I am planning to test the assumption that abbreviations and their expansions occur in similar contexts by testing on hand-labeled data."
I also plan to vary the size of the window used for determining the local context from two words on each side of the expression in question as well as the cutoff used during ME training.
It will also be necessary to extend this approach to other medical and possibly non-medical domains with larger data sets.
"Finally, I will experiment with combining the UMLS abbreviations table with the Mayo Clinic specific abbreviations."
*po3¡C:p£*¢ *¢ T¨§.©&amp;ª «U¬­¡ ® £8¡k¯ ¢¤¥9°k£*±²¡kp³²´k£8¡C3pµ¤¡k¶c³Q¢¤·k¢ ¶°k3¯ ¢ ` ¢ `·r¸£*_° p¯ ¢  p¡Cº3¶¢ \.³p¢½¼ ·k¢ ¶°k­¡kp³¾³Q¢¤3¶¸°À¿e¶¡kp´_»p¡C´k¢.¢ Q´_Q¢¤¢¤£8ÁQ´ µ½°_¯v_p¢ T*7¡k3³i£*¢ _»Q£¹µ½¢ 7Â¡¾£* ® ¡k*p°_. ©&amp; µ½*»p£*¢Äp¡k ¢ 3¡Cº3¶¸¢ ³¾»pMQ°kM°_ ¶°kl¡  º ¢¤£ ° ® *»pµ¤µ½¢ 8 ® »3¶ ¡Cp3¶Áµ¤¡C*¸°_p ® °k£e·Æ¡C£*¼: °_»pi¶¡kQ´_»p¡C´k¢op£*° rµ½¢ ***¡k*± *»pµ8 ¡k0É ® °k£8¯v¡C*°_ *¸°_xË¹¬ º3»Qi¡k¶*° **:  °_pÏ´k¢   Q¢ ® £8¡k¯ ¢¤¥9°k£**µ¤¡C£8£*¿Î°_»QÏ¢¤·Æ¡k¶»3¡C*¸°_pe°_Ð*Q¢Â¡Cp3¶Áµ¤¡Æ¼ : |³Q¢¤·k¢ ¶¸°kÅ¡Cpx¶µ¤¡C*¸°_ º*°_»Q£8µ½¢ *¸_»p¡C´k¢   ³Ñ°_ *M*Q°k£*°_»Q´_lÓ!pÁµ½° ³Q¢Ô*»pp:°k£*¤Ã y ³p»3µ¤Q´¨£ÕÀG\kR] _[&amp;S)*)E^ ]_A? [ _¯ :°_Q¢ `*3£*° µ½¢ 8= »r¼ £ ¢ Ù »p¸£*¢  `*¸°_* Q´_Q¢¤¢¤£¹Q´e¡k* µ½*.° ®*¯v¡ko¶¡kQ´_»p¡C´k¢l¡ke3¡C£*Å° ® ¡C33¶µ¤¡C*¸°_3   ® Y¥9¡C£8¢ *Q¢ ¸£.µ½°_*¸°_ pÁU3¡C:¢¤£&apos;£*¢¤:°k£** &apos;¥H°k£8±­°_ §.&amp;© ª « ®ÚA¥¡C£¬p¡kl*® £8¡k*¶¸°k3¯v¢*»Q£*T²¢ ® °k£U¶¡kQ´*p¡CÂµ½°__»p¡C´k¢Ñp£*°rµ½¢²°*_¼  ® £8°_T**pA¿ :¢7° ® p£*¢ ³pÁµ½*¡Cº3¶¸Y¿xÛ
Reducing language model (LM) size is a critical issue when applying a LM to realistic applications which have memory constraints.
"In this paper, three measures are studied for the purpose of LM pruning."
"They are probability, rank, and entropy."
We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER).
"We first present an empirical comparison, showing that rank performs the best in most cases."
We also show that the high-performance of rank lies in its strong correlation with error rate.
We then present a novel method of combining two criteria in model pruning.
"Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same CER."
Backoff n-gram models for applications such as large vocabulary speech recognition are typically trained on very large text corpora.
An uncompressed LM is usually too large for practical use since all realistic applications have memory constraints.
"Therefore, LM pruning techniques are used to produce the smallest model while keeping the performance loss as small as possible."
"Research on backoff n-gram model pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model."
The traditional count cutoff method[REF_CITE]used a pruning criterion based on absolute frequency while recent research has shown that better pruning criteria can be developed based on more sophisticated measures such as perplexity.
"In this paper, we study three measures for pruning backoff n-gram models."
"They are probability, rank and entropy."
We evaluated the performance of the three pruning criteria in a real application of Chinese text input[REF_CITE]through CER.
"We first present an empirical comparison, showing that rank performs the best in most cases."
We also show that the high-performance of rank lies in its strong correlation with error rate.
We then present a novel method of combining two pruning criteria in model pruning.
Our results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately.
"In particular, the combination of rank and entropy achieves the smallest models at a given CER."
The rest of the paper is structured as follows: Section 2 discusses briefly the related work on backoff n-gram pruning.
Section 3 describes in detail several pruning criteria.
Section 4 presents an empirical comparison of pruning criteria using a Chinese text input system.
Section 5 proposes our method of combining two criteria in model pruning.
Section 6 presents conclusions and our future work.
N-gram models predict the next word given the previous n-1 words by estimating the conditional probability P(w n |w [Footnote_1] …w n-1 ).
1 This work was done while Zhang was working at Microsoft Research Asia as a visiting student.
"In practice, n is usually set to 2 (bigram), or 3 (trigram)."
"For simplicity, we restrict our discussion to bigrams P(w n | w n-1 ), but our approaches can be extended to any n-gram."
The bigram probabilities are estimated from the training data by maximum likelihood estimation (MLE).
"However, the intrinsic problem of MLE is that of data sparseness: MLE leads to zero-value probabilities for unseen bigrams."
"To deal with this problem,[REF_CITE]proposed a backoff scheme."
He estimates the probability of an unseen bigram by utilizing unigram estimates as follows
"P(w i | w i−1 ) = P d (w i | w i−1 ) c(w i−1 ,w i ) &gt; 0 , (1)  α (w i−1 )"
"P(w i ) otherwise where c(w i-1 w i ) is the frequency of word pair (w i-1 w i ) in the training data, P d represents the Good-Turing discounted estimate for seen word pairs, and α (w i-1 ) is a normalization factor."
"Due to the memory limitation in realistic applications, only a finite set of word pairs have conditional"
P(w i |w i-1 ) explicitlyprobability represented in the model.
The remaining word pairs are assigned a probability by backoff (i.e. unigram estimates).
The goal of bigram pruning is to remove uncommon explicit bigram estimates P(w i |w i-1 ) from the model to reduce the number of parameters while minimizing the performance loss.
"The research on backoff n-gram model pruning can be formulated as the definition of the pruning criterion, which is used to estimate the performance loss of the pruned model."
"Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows:"
The algorithm in Figure 1 together with several pruning criteria has been studied previously ([REF_CITE]; etc).
A comparative study of these techniques is presented[REF_CITE].
"In this paper, three pruning criteria will be studied: probability, rank, and entropy."
Probability serves as the baseline pruning criterion.
It is derived from perplexity which has been widely used as a LM evaluation measure.
Rank and entropy have been previously used as a metric for LM evaluation[REF_CITE].
"In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning."
"In the next section, we will describe how pruning criteria are developed using these two measures."
"In this section, we describe the three pruning criteria we evaluated."
"They are derived from LM evaluation measures including perplexity, rank, and entropy."
The goal of the pruning criterion is to estimate the performance loss due to pruning each bigram individually.
"Therefore, we represent the pruning criterion as a loss function, denoted by LF below."
The probability pruning criterion is derived from perplexity.
The perplexity is defined as
N − 1 ∑ logP(w i |w i−1 ) (2) PP = 2 N i=1 where N is the size of the test data.
The perplexity can be roughly interpreted as the expected branching factor of the test document when presented to the LM.
It is expected that lower perplexities are correlated with lower error rates.
The method of pruning bigram models using probability can be described as follows: all bigrams that change perplexity by less than a threshold are removed from the model.
"In this study, we assume that the change in model perplexity of the LM can be expressed in terms of a weighted difference of the log probability estimate before and after pruning a bigram."
"The loss function of probability LF probability , is then defined as −P(w i−1 w i ) [logP&apos;(w i | w i−1 ) − logP(w i | w i−1 )] , (3) where P(.|.) denotes the conditional probabilities assigned by the original model, P’(.|.) denotes the probabilities in the pruned model, and P(w i-1 w i ) is a smoothed probability estimate in the original model."
"We notice that LF probability of Equation (3) is very similar to that proposed[REF_CITE], where the loss function is −N(w i−1 w i ) [logP&apos;(w i | w i−1 ) − logP(w i | w i−1 )] ."
Here N(w i-1 w i ) is the discounted frequency that bigram w i-1 w i was observed in training.
N(w i-1 w i ) is conceptually identical to P(w i-1 w i ) in Equation (3).
"From Equations (2) and (3), we can see that lower LF probability is strongly correlated with lower perplexity."
"However, we found that LF probability is suboptimal as a pruning criterion, evaluated on CER in our experiments."
We assume that it is largely due to the deficiency of perplexity as a LM performance measure.
"Although perplexity is widely used due to its simplicity and efficiency, recent researches show that its correlation with error rate is not as strong as once thought."
"Therefore, they used other measures such as rank and entropy for LM evaluation."
These measures are based on the probability distribution over the whole vocabulary.
"That is, if the test text is w 1n , then perplexity is based on the values of P(w i |w i-1 ), and the new measures will be based on the values of P(w|w i-1 ) for all w in the vocabulary."
"Since these measures take into account the probability distribution over all competing words (including the target word) within the decoder, they are, hopefully, better correlated with error rate, and expected to evaluate LMs more precisely than perplexity."
"The rank of the target word w is defined as the word’s position in an ordered list of the bigram probabilities P(w|w i-1 ) where w ∈ V, and V is the vocabulary."
"Thus the most likely word (within the decoder at a certain time point) has the rank of one, and the least likely has rank |V|, where |V| is the vocabulary size."
We propose to use rank for pruning as follows: all bigrams that change rank by less than a threshold after pruning are removed from the model.
The corresponding loss function LF rank is defined as ∑ p(w i−1 w i ){log[R′(w i | w i−1 ) + k]− logR(w i | w i−1 )} (4) w i w i−1 where R(.|.) denotes the rank of the observed bigram
"P(w i |w i-1 ) in the list of bigram probabilities P(w|w i-1 ) where w ∈ V, before pruning, R’(.|.) is the new rank of it after pruning, and the summation is over all word pairs (w i-1 w i ). k is a constant to assure that log[R′(w i | w i−1 ) + k]− logR(w i | w i−1 ) ≠ 0 . k is set to 0.1 in our experiments."
"Given a bigram model, the entropy H of the probability distribution over the vocabulary V is generally given by"
H(w i ) = −∑ Vj=1 P(w j | w i )log P(w j | w i ) .
We propose to use entropy for pruning as follows: all bigrams that change entropy by less than a threshold after pruning are removed from the model.
"The corresponding loss function LF entropy is defined as − 1 ∑ N (H′(w i−1 ) − H(w i−1 )) (5) N i=1 where H is the entropy before pruning given history w i-1 , H’ is the new entropy after pruning, and N is the size of the test data."
The entropy-based pruning is conceptually similar to the pruning method proposed[REF_CITE].
Stolcke used the Kullback-Leibler divergence between the pruned and un-pruned model probability distribution in a given context over the entire vocabulary.
"In particular, the increase in relative entropy from pruning a bigram is computed by − ∑ P(w i−1 w i ) [logP&apos;(w i | w i−1 ) −log"
"P(w i | w i−1 )] , w i−1 w i"
"We evaluated the pruning criteria introduced in the previous section on a realistic application, Chinese text input."
"In this application, a string of Pinyin (phonetic alphabet) is converted into Chinese characters, which is the standard way of inputting text on Chinese computers."
This is a similar problem to speech recognition except that it does not include acoustic ambiguity.
"We measure performance in terms of character error rate (CER), which is the number of characters wrongly converted from the Pinyin string divided by the number of characters in the correct transcript."
"The role of the language model is, for all possible word strings that match the typed Pinyin string, to select the word string with the highest language model probability."
"The training data we used is a balanced corpus of approximately 26 million characters from various domains of text such as newspapers, novels, manuals, etc."
"The test data consists of half a million characters that have been proofread and balanced among domain, style and time."
The back-off bigram models we generated in this study are character-based models.
"That is, the training and test corpora are not word-segmented."
"As a result, the lexicon we used contains 7871 single Chinese characters only."
"While word-based n-gram models are widely applied, we used character-based models for two reasons."
"First, pilot experiments show that the results of word-based and character-based models are qualitatively very similar."
"More importantly, because we need to build a very large number of models in our experiments as shown below, character-based models are much more efficient, both for training and for decoding."
We used the absolute discount smoothing method for model training.
None of the pruning techniques we consider are loss-less.
"Therefore, whenever we compare pruning criteria, we do so by comparing the size reduction of the pruning criteria at the same CER."
Figure 2 shows how the CER varies with the bigram numbers in the models.
"For comparison, we also include in Figure 2 the results using count cutoff pruning."
We can see that CER decreases as we keep more and more bigrams in the model.
A steeper curve indicates a better pruning criterion.
"The main result to notice here is that the rank-based pruning achieves consistently the best performance among all of them over a wide range of CER values, producing models that are at 55-85% of the size of the probability-based pruned models with the same CER."
"An example of the detailed comparison results is shown in Table 1, where the[REF_CITE].8% and the value of cutoff is 1."
The last column of Table 1 shows the relative model sizes with respect to the probability-based pruned model with the[REF_CITE].8%.
"Another interesting result is the good performance of count cutoff, which is almost overlapping with probability-based pruning at larger model sizes [Footnote_2] ."
"2 The result is consistent with that reported[REF_CITE], where an explanation was offered."
"The entropy-based pruning unfortunately, achieved the worst performance."
We assume that the superior performance of rank-based pruning lies in the fact that rank (acting as a LM evaluation measure) has better correlation with CER.
"The related part of their results to our study are shown in Table 2, where r is the Pearson product-moment correlation coefficient, r s is the Spearman rank-order correlation coefficient, and T is the Kendall rank-order correlation coefficient."
We now investigate methods of combining pruning criteria described above.
We begin by examining the overlap of the bigrams pruned by two different criteria to investigate which might usefully be combined.
Then the thresholding pruning algorithm described in Figure 1 is modified so as to make use of two pruning criteria simultaneously.
The problem here is how to find the optimal settings of the pruning threshold pair (each for one pruning criterion) for different model sizes.
We show how an optimal function which defines the optimal settings of the threshold pairs is efficiently established using our techniques.
"From the abovementioned three pruning criteria, we investigated the overlap of the bigrams pruned by a pair of criteria."
There are three criteria pairs.
The overlap results are shown in Figure 3.
"We can see that the percentage of the number of bigrams pruned by both criteria seems to increase as the model size decreases, but all criterion-pairs have overlaps much lower than 100%."
"In particular, we find that the average overlap between probability and entropy is approximately 71%, which is the biggest among the three pairs."
The pruning method based on the criteria of rank and entropy has the smallest average overlap of 63.6%.
"The results suggest that we might be able to obtain improvements by combining these two criteria for bigram pruning since the information provided by these criteria is, in some sense, complementary."
"In order to prune a bigram model based on two criteria simultaneously, we modified the thresholding pruning algorithm described in Figure 1."
"Let lf i be the value of the performance loss estimated by the loss function LF i , θ i be the threshold defined by the pruning criterion C i ."
The modified thresholding pruning algorithm can be described as follows:
"Now, the remaining problem is how to find the optimal settings of the pruning threshold pair for different model sizes."
"This seems to be a very tedious task since for each model size, a large number of settings ( θ 1 θ 2 ) have to be tried for finding the optimal ones."
"Therefore, we convert the problem to the following one: How to find an optimal function θ 2 =f( θ 1 ) by which the optimal threshold θ 2 is defined for each threshold θ 1 ."
The function can be learned by pilot experiments described below.
"Given two thresholds θ 1 and θ 2 of pruning criteria C 1 and C 2 , we try a large number of values of θ 1 , θ 2 , and build a large number of models pruned using the algorithm described in Figure 4."
"For each model size, we find an optimal setting of the threshold setting ( θ 1 θ 2 ) which results in a pruned model with the lowest CER."
"Finally, all these optimal threshold settings serve as the sample data, from which the optimal function can be learned."
"We found that in pilot experiments, a relatively small set of sample settings is enough to generate the function which is close enough to the optimal one."
This allows us to relatively quickly search through what would otherwise be an overwhelmingly large search space.
We used the same training data described in Section 4 for bigram model training.
We divided the test set described in Section 4 into two non-overlapped subsets.
We performed testing on one subset containing 80% of the test set.
We performed optimal function learning using the remaining 20% of the test set (referred to as held-out data below).
Take the combination of rank and entropy as an example.
An uncompressed bigram model was first built using all training data.
"We then built a very large number of pruned bigram models using different threshold setting ( θ rank θ entropy ), where the values θ rank , θ entropy ∈ [3E-12, 3E-6]."
"By evaluating pruned models on the held-out data, optimal settings can be found."
Some sample settings are shown in Table 3.
"In experiments, we found that a linear regression model of Equation (6) is powerful enough to learn a function which is close enough to the optimal one. log θ ( entropy ) = α 1 ×log( θ rank )+ α 2 (6)"
Here α 1 and α 2 are coefficients estimated from the sample settings.
Optimal functions of the other two threshold-pair settings ( θ rank θ probability ) and ( θ probability θ entropy ) are obtained similarly.
They are shown in Table 4.
"In Figure 5, we present the results using models pruned with all three threshold-pairs defined by the functions in Table 4."
"As we expected, in all three cases, using a combination of two pruning criteria achieves consistently better performance than using either of the criteria separately."
"In particular, using the combination of rank and entropy, we obtained the best models over a wide large of CER values."
It corresponds to a significant size reduction of 15-54% over the probability-based LM pruning at the same CER.
An example of the detailed comparison results is shown in Table 5.
There are two reasons for the superior performance of the combination of rank and entropy.
"First, the rank-based pruning achieves very good performance as described in Section 4."
"Second, as shown in Section 5.1, there is a relatively small overlap between the bigrams chosen by these two pruning criteria, thus big improvement can be achieved through the combination."
"The research on backoff n-gram pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model."
This paper explores several pruning criteria for backoff n-gram model size reduction.
"Besides the widely used probability, two new pruning criteria have been developed based on rank and entropy."
We have performed an empirical comparison of these pruning criteria.
"We also presented a thresholding algorithm for model pruning, in which two pruning criteria can be used simultaneously."
"Finally, we described our techniques of finding the optimal setting of the threshold pair given a specific model size."
We have shown several interesting results.
They include the confirmation of the estimation that the measures which are better correlated with CER for LM evaluation leads to better pruning criteria.
"Our experiments show that rank, which has the best correlation with CER, achieves the best performance when there is only one criterion used in bigram model pruning."
We then show empirically that the overlap of the bigrams pruned by different criteria is relatively low.
This indicates that we might obtain improvements through a combination of two criteria for bigram pruning since the information provided by these criteria is complementary.
This hypothesis is confirmed by our experiments.
Results show that using two pruning criteria simultaneously achieves better bigram models than using either of the criteria separately.
"In particular, the combination of rank and entropy achieves the smallest bigram models at the same CER."
"For our future work, more experiments will be performed on other language models such as word-based bigram and trigram for Chinese and English."
More pruning criteria and their combinations will be investigated as well.
"The n-gram model is a stochastic model, which predicts the next word (predicted word) given the previous words (conditional words) in a word sequence."
The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster.
It has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words.
This is the basis of the asymmetric cluster model (ACM) discussed in our study.
"In this paper, we first present a formal definition of the ACM."
We then describe in detail the methodology of constructing the ACM.
"The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion."
Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size.
Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model.
"The n-gram model has been widely applied in many applications such as speech recognition, machine translation, and Asian language text input [[REF_CITE]]."
"It is a stochastic model, which predicts the next word (predicted word) given the previous n-[Footnote_1] words (conditional words) in a word sequence."
1 This work was done while Cao was visiting Microsoft Research Asia.
"Microsoft Research, Redmond"
The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster.
This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications.
"Recent research [[REF_CITE]] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [[REF_CITE]]."
"This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper."
"Although similar models have been used in previous studies [[REF_CITE]], several issues have not been completely investigated."
"These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative study of the ACM with classical cluster models and word models when they are applied to a realistic application, and (3) an analysis of the reason why the ACM is superior."
The goal of this study is to address the above three issues.
We first present a formal definition of the ACM; then we describe in detail the methodology of constructing the ACM including (1) an asymmetric clustering algorithm in which different metrics are used for clustering the predicted and conditional words respectively; and (2) a method for model parameter optimization in which the optimal cluster numbers are found for different clusters.
"We evaluate the ACM on a real application, Japanese Kana-Kanji conversion, which converts phonetic Kana strings into proper Japanese orthography."
The performance is measured in terms of character error rate (CER).
Our results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size.
"Our analysis shows that the high-performance of the ACM comes from better structure and better smoothing, both of which lie in the asymmetry of the model."
"This paper is organized as follows: Section 1 introduces our research topic, and then Section 2 reviews related work."
Section 3 defines the ACM and describes in detail the method of model construction.
Section 4 first introduces the Japanese Kana-Kanji conversion task; it then presents our main experiments and a discussion of our findings.
"Finally, conclusions are presented in Section 5."
A large amount of previous research on clustering has been focused on how to find the best clusters [[REF_CITE]].
"Only small differences have been observed, however, in the performance of the different techniques for constructing clusters."
"In this study, we focused our research on novel techniques for using clusters – the ACM, in which different clusters are used for predicted and conditional words respectively."
The discussion of the ACM in this paper is an extension of several studies below.
The first similar cluster model was presented by Goodman and Gao [2000] in which the clustering techniques were combined with Stolcke’s [1998] pruning to reduce the language model (LM) size effectively.
"Goodman [2001] and Gao et al, [2001] give detailed descriptions of the asymmetric clustering algorithm."
"However, the impact of the asymmetric clustering on the performance of the resulting cluster model was not empirically studied there."
"Gao et al., [2001] presented a fairly thorough empirical study of clustering techniques for Asian language modeling."
"Unfortunately, all of the above work studied the ACM without applying it to an application; thus only perplexity results were presented."
The first real application of the ACM was a simplified bigram ACM used in a Chinese text input system [[REF_CITE]].
"However, quite a few techniques (including clustering) were integrated to construct a Chinese language modeling system, and the contribution of using the ACM alone was by no means completely investigated."
"Finally, there is one more point worth mentioning."
Most language modeling improvements reported previously required significantly more space than word trigram models [[REF_CITE]].
Their practical value is questionable since all realistic applications have memory constraints.
"In this paper, our goal is to achieve a better tradeoff between LM performance (perplexity and CER) and model size."
"Thus, whenever we compare the performance of different models (i.e. ACM vs. word trigram model), Stolcke’s pruning is employed to bring the models compared to similar sizes."
The LM predicts the next word w i given its history h by estimating the conditional probability
P(w i |h).
"Using the trigram approximation, we have"
"P(w i |h)≈P(w i |w i-2 w i-1 ), assuming that the next word depends only on the two preceding words."
"In the ACM, we will use different clusters for words in different positions."
"For the predicted word, w i , we will denote the cluster of the word by PW i , and we will refer to this as the predictive cluster. ."
"For the words w i-2 and w i-1 that we are conditioning on, we will denote their clusters by CW i-2 and CW i-1 which we call conditional clusters."
"When we which to refer to a cluster of a word w in general we will use the notation W. The ACM estimates the probability of w i given the two preceeding words w i-2 and w i-1 as the product of the following two probabilities: (1) The probability of the predicted cluster PW i given the preceding conditional clusters CW i-2 and CW i-1 , P(PW i |CW i-2"
"CW i-1 ), and (2) The probability of the word given its cluster PW i and the preceding conditional clusters CW i-2 and CW i-1 ,"
P(w i |CW i-2
CW i-1 PW i ).
"Thus, the ACM can be parameterized by"
"P(w i |h) ≈ P(PW i |CW i−2 CW i−1 ) ×P(w i |CW i−2 CW i−1 PW i ) (1) The ACM consists of two sub-models: (1) the cluster sub-model P(PW i |CW i-2 CW i-1 ), and (2) the word sub-model P(w i |CW i-2"
CW i-1 PW i ).
"To deal with the data sparseness problem, we used a backoff scheme[REF_CITE]for the parameter estimation of each sub-model."
The backoff scheme recursively estimates the probability of an unseen n-gram by utilizing (n-1)-gram estimates.
The basic idea underlying the ACM is the use of different clusters for predicted and conditional words respectively.
Classical cluster models are symmetric in that the same clusters are employed for both predicted and conditional words.
"However, the symmetric cluster model is suboptimal in practice."
"For example, consider a pair of words like “a” and “an”."
"In general, “a” and “an” can follow the same words, and thus, as predicted words, belong in the same cluster."
"But, there are very few words that can follow both “a” and “an”."
"So as conditional words, they belong in different clusters."
"In generating clusters, two factors need to be considered: (1) clustering metrics, and (2) cluster numbers."
"In what follows, we will investigate the impact of each of the factors."
The basic criterion for statistical clustering is to maximize the resulting probability (or minimize the resulting perplexity) of the training data.
"Many traditional clustering techniques [[REF_CITE]] attempt to maximize the average mutual information of adjacent clusters (2) I(W 1 ,W 2 ) = ∑ P(W 1 W 2 ) log P(WP(W 2 |W 1 ) , W 1 ,W 2 2 ) where the same clusters are used for both predicted and conditional words."
"We will call these clustering techniques symmetric clustering, and the resulting clusters both clusters."
"In constructing the ACM, we used asymmetric clustering, in which different clusters are used for predicted and conditional words."
"In particular, for clustering conditional words, we try to minimize the perplexity of training data for a bigram of the form"
"P(w i |W i-1 ), which is equivalent to maximizing"
P(w i |W i−1 ) . (3) i=1 where N is the total number of words in the training data.
We will call the resulting clusters conditional clusters denoted by CW.
"For clustering predicted words, we try to minimize the perplexity of training data of P(W i |w i-1 ) ×P(w i |W i )."
We will call the resulting clusters predicted clusters denoted by PW.
We have 2
N P(w i−1 W i ) × P(W i w i ) N ∏
P(W i | w i−1 ) ×
P(w i |W i ) = ∏ i=1 i=1
P(w i−1 )
P(W i ) = ∏ N P(W i w i ) ×
P(w i−1 W i ) i=1
P(w i−1 )
P(W i ) = ∏ N P(w i ) ×P(w |W ) i−1 i . i=1
P(w i−1 )
"Now, P(w i ) is independent of the clustering used."
P(w i−1 )
"Therefore, for the selection of the best clusters, it is sufficient to try to maximize"
P(w i−1 |W i ) . (4) i=1
This is very convenient since it is exactly the op-posite of what was done for conditional clustering.
"It means that we can use the same clustering tool for both, and simply switch the order used by the program used to get the raw counts for clustering."
The clustering technique we used creates a binary branching tree with words at the leaves.
"The ACM in this study is a hard cluster model, meaning that each word belongs to only one cluster."
"So in the clustering tree, each word occurs in a single leaf."
"In the ACM, we actually use two different clustering trees."
"One is optimized for predicted words, and the other for conditional words."
"The basic approach to clustering we used is a top-down, splitting clustering algorithm."
"In each iteration, a cluster is split into two clusters in the way that the splitting achieves the maximal entropy decrease (estimated by Equations (3) or (4))."
"Finally, we can also perform iterations of swapping all words between all clusters until convergence i.e. no more entropy decrease can be found [Footnote_3] ."
"3 Notice that for experiments reported in this paper, we used the basic top-down algorithm without swapping. Although the resulting clusters without swapping are not even locally optimal, our experiments show that the quality of clusters (in terms of the perplexity of the resulting ACM) is not inferior to that of clusters with swapping."
We find that our algorithm is much more efficient than agglomerative clustering algorithms – those which merge words bottom up.
Asymmetric clustering results in two binary clustering trees.
"By cutting the trees at a certain level, it is possible to achieve a wide variety of different numbers of clusters."
"For instance, if the tree is cut after the 8 th level, there will be roughly 2 8 =256 clusters."
"Since the tree is not balanced, the actual number of clusters may be somewhat smaller."
We use W l to represent the cluster of a word w using a tree cut at level l.
"In particular, if we set l to the value “all”, it means that the tree is cut at infinite depth, i.e. each cluster contains a single word."
The ACM model of Equation (1) can be rewritten as
P(PW il |CW i-[Footnote_2]j CW i-1j ) ×P(w i |PW i-[Footnote_2]k CW i-1k CW il ). (5)
2 Thanks to Lillian Lee for suggesting this justification of predictive clusters.
2 Thanks to Lillian Lee for suggesting this justification of predictive clusters.
"To optimally apply the ACM to realistic applications with memory constraints, we are always seeking the correct balance between model size and performance."
We used Stolcke’s pruning method to produce many ACMs with different model sizes.
"In our experiments, whenever we compare techniques, we do so by comparing the performance (perplexity and CER) of the LM techniques at the same model sizes."
"Stolcke’s pruning is an entropy-based cutoff method, which can be described as follows: all n-grams that change perplexity by less than a threshold are removed from the model."
"For pruning the ACM, we have two thresholds: one for the cluster sub-model P(PW il |CW i-2j CW i-1j ) and one for the word sub-model"
P(w i |CW i-2k
"CW i-1k PW il ) respectively, denoted by t c and t w below."
"In this way, we have 5 different parameters that need to be simultaneously optimized: l, j, k, t c , and t w , where j, k, and l are the numbers of clusters, and t c and t w are the pruning thresholds."
A brute-force approach to optimizing such a large number of parameters is prohibitively expensive.
"Rather than trying a large number of combinations of all 5 parameters, we give an alternative technique that is significantly more efficient."
Simple math shows that the perplexity of the overall model P(PW il |CW i-2j CW i-1j )×
P(w i |CW i-2k
CW i-1k PW il ) is equal to the perplexity of the cluster sub-model P(PW il |CW i-2j CW i-1j ) times the perplexity of the word sub-model
P(w i |CW i-2k
CW i-1k PW il ).
The size of the overall model is clearly the sum of the sizes of the two sub-models.
"Thus, we try a large number of values of j, l, and a pruning threshold t c for P(PW il |CW i-2j CW i-1j ), computing sizes and perplexities of each, and a similarly large number of values of l, k, and a separate threshold t w for P(w i |CW i-2k"
CW i-1k PW il ).
We can then look at all compatible pairs of these models (those with the same value of l) and quickly compute the perplexity and size of the overall models.
This allows us to relatively quickly search through what would otherwise be an overwhelmingly large search space.
Japanese Kana-Kanji conversion is the standard method of inputting Japanese text by converting a syllabary-based Kana string into the appropriate combination of ideographic Kanji and Kana.
"This is a similar problem to speech recognition, except that it does not include acoustic ambiguity."
"The performance is generally measured in terms of character error rate (CER), which is the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript."
"The role of the language model is, for all possible word strings that match the typed phonetic symbol string, to select the word string with the highest language model probability."
Current products make about 5-10% errors in con-version of real data in a wide variety of domains.
"In the experiments, we used two Japanese newspaper corpora: the Nikkei Newspaper corpus, and the Yomiuri Newspaper corpus."
"Both text corpora have been word-segmented using a lexicon containing 167,107 entries."
"We performed two sets of experiments: (1) pilot experiments, in which model performance is measured in terms of perplexity and (2) Japanese Kana-Kanji conversion experiments, in which the performance of which is measured in terms of CER."
"In the pilot experiments, we used a subset of the Nikkei newspaper corpus: ten million words of the Nikkei corpus for language model training, 10,000 words for held-out data, and 20,000 words for testing data."
None of the three data sets overlapped.
"In the Japanese Kana-Kanji conversion experiments, we built language models on a subset of the Nikkei Newspaper corpus, which contains 36 million words."
"We performed parameter optimization on a subset of held-out data from the Yomiuri Newspaper corpus, which contains 100,000 words."
"We performed testing on another subset of the Yomiuri Newspaper corpus, which contains 100,000 words."
"In both sets of experiments, word clusters were derived from bigram counts generated from the training corpora."
Out-of-vocabulary words were not included in perplexity and error rate computations.
"As described in Section 3.2, depending on the clustering metrics we chose for generating clusters, we obtained three types of clusters: both clusters (the metric of Equation (2)), conditional clusters (the metric of Equation (3)), and predicted clusters (the metric of Equation (4))."
We then performed a series of experiments to investigate the impact of different types of clusters on the ACM.
We used three variants of the trigram ACM: (1) the predictive cluster model P(w i |w i-2 w i-1 W i )×
"P(W i |w i-2 w i-1 ) where only predicted words are clustered, (2) the conditional cluster model P(w i |W i-2 W i-1 ) where only conditional words are clustered, and (3) the IBM model"
P(w i |W i )×
"P(W i |W i-2 W i-1 ) which can be treated as a special case of the ACM of Equation (5) by using the same type of cluster for both predicted and conditional words, and setting k = 0, and l = j."
"For each cluster trigram model, we compared their perplexities and CER results on Japanese Kana-Kanji conversion using different types of clusters."
"For each cluster type, the number of clusters were fixed to the same value 2^6 just for comparison."
The results are shown in Table 1.
It turns out that the benefit of using different clusters in different positions is obvious.
"For each cluster trigram model, the best results were achieved by using the “matched” clusters, e.g. the predictive cluster model P(w i |w i-2 w i-1 W i ) ×"
P(W i |w i-2 w i-1 ) has the best performance when the cluster
W i is the predictive cluster PW i generated by using the metric of Equation (4).
"In particular, the IBM model achieved the best results when predicted and conditional clusters were used for predicted and conditional words respectively."
"That is, the IBM model is of the form"
P(w i |PW i )× P(PW i |CW i-2
CW i-1 ).
"In this section, we first present our pilot experiments of finding the optimal parameter set of the ACM (l, j, k, t c , t w ) described in Section 2.3."
"Then, we compare the ACM to the IBM model, showing that the superiority of the ACM results from its better structure."
"In this section, the performance of LMs was measured in terms of perplexity, and the size was measured as the total number of parameters of the LM: one parameter for each bigram and trigram, one parameter for each normalization parameter α that was needed, and one parameter for each unigram."
We first used the conditional cluster model of the form
P(w i |CW i-2j CW i-1j ).
"Some sample settings of parameters (j, t w ) are shown in Figure 1."
"The performance was consistently improved by increasing the number of clusters j, except at the smallest sizes."
"The word trigram model was consistently the best model, except at the smallest sizes, and even then was only marginally worse than the conditional cluster models."
This is not surprising because the conditional cluster model always discards information for predicting words.
"We then used the predictive cluster model of the form P(PW il |w i-2 w i-1 ) ×P(w i |w i-2 w i-1 PW il ), where only predicted words are clustered."
"Some sample settings of the parameters (l, t c , t w ) are shown in Figure 2."
"For simplicity, we assumed t c =t w , meaning that the same pruning threshold values were used for both sub-models."
It turns out that predictive cluster models achieve the best perplexity results at about 2^6 or 2^8 clusters.
The models consistently outperform the baseline word trigram models.
"We finally returned to the ACM of Equation (5), where both conditional words and the predicted word are clustered (with different numbers of clusters), and which is referred to as the combined cluster model below."
"In addition, we allow different values of the threshold for different sub-models."
"Therefore, we need to optimize the model parameter set l, j, k, t c , t w ."
"Based on the pilot experiment results using conditional and predictive cluster models, we tried combined cluster models for values l ∈ [4, 10], j, k ∈ [8, 16]."
"We also allow j, k=all."
"Rather than plot all points of all models together, we show only the outer envelope of the points."
"That is, if for a given model type and a given point there is some other point of the same type with both lower perplexity and smaller size than the first point, then we do not plot the first, worse point."
"The results are shown in Figure 3, where the cluster number of IBM models is 2^14 which achieves the best performance for IBM models in our experiments."
"It turns out that when l ∈ [6, 8] and j, k&gt;12, combined cluster models yield the best results."
We also found that the predictive cluster models give as good performance as the best combined ones while combined models outperformed very slightly only when model sizes are small.
This is not difficult to explain.
"Recall that the predictive cluster model is a special case of the combined model where words are used in conditional positions, i.e. j=k=all."
"Our experiments show that combined models achieved good performance when large numbers of clusters are used for conditional words, i.e. large j, k&gt;12, which are similar to words."
The most interesting analysis is to look at some sample settings of the parameters of the combined cluster models in Figure 3.
"In Table 2, we show the best parameter settings at several levels of model size."
"Notice that in larger model sizes, predictive cluster models (i.e. j=k=all) perform the best in some cases."
The ‘prune’ columns (i.e. columns 6 and 7) indicate the Stolcke pruning parameter we used.
"First, notice that the two pruning parameters (in columns 6 and 7) tend to be very similar."
This is desirable since applying the theory of relative entropy pruning predicts that the two pruning parameters should actually have the same value.
"Next, let us compare the ACM P(PW il |CW i-2j CW i-1j )×P(w i |CW i-2k CW i-1k PW il ) to traditional IBM clustering of the form P(W il |W i-2l W i-1l )×P(w i |W il ), which is equal to P(W il |W i-2l W i-1l )×P(w i |W i-20 W i-10 W il ) (assuming the model same type of cluster is used for both predictive and conditional words)."
Our results in Figure 3 show that the performance of IBM models is roughly an order of magnitude worse than that of ACMs.
"This is because in addition to the use of the symmetric cluster model, the traditional IBM model makes two more assumptions that we consider suboptimal."
"First, it assumes that j=l. We see that the best results come from unequal settings of j and l. Second, more importantly, IBM clustering assumes that k=0."
"We see that not only is the optimal setting for k not 0, but also typically the exact opposite is the optimal: k=all in P(w i |CW i-2k"
CW i-1k PW il )=which case
"P(w i |w i-2 w i-1 PW il ), or k=14, 16, which is very similar."
"That is, we see that words depend on the previous words and that an independence assumption is a poor one."
"Of course, many of these word dependencies are pruned away – but when a word does depend on something, the previous words are better predictors than the previous clusters."
"Another important finding here is that for most of these settings, the unpruned model is actually larger than a normal trigram model – whenever k=all or 14, 16, the unpruned model P(PW il |CW i-2j CW i-1j ) ×"
P(w i |CW i-2k
CW i-1k PW il ) is actually larger than an unpruned model
P(w i |w i-2 w i-1 ).
"This analysis of the data is very interesting – it implies that the gains from clustering are not from compression, but rather from capturing structure."
"Factoring the model into two models, in which the cluster is predicted first, and then the word is predicted given the cluster, allows the structure and regularities of the model to be found."
"This larger, better structured model can be pruned more effectively, and it achieved better performance than a word trigram model at the same model size."
"Before we present CER results of the Japanese Kana-Kanji conversion system, we briefly describe our method for storing the ACM in practice."
"One of the most common methods for storing backoff n-gram models is to store n-gram probabilities (and backoff weights) in a tree structure, which begins with a hypothetical root node that branches out into unigram nodes at the first level of the tree, and each of those unigram nodes in turn branches out into bigram nodes at the second level and so on."
"To save storage, n-gram probabilities such as P(w i |w i-1 ) and backoff weights such as α(w i-2 w i-1 ) are stored in a single (bigram) node array[REF_CITE]."
Applying the above tree structure to storing the ACM is a bit complicated – there are some representation issues.
"For example, consider the cluster sub-model P(PW il |CW i-2j CW i-1j )."
"N-gram probabilities such as P(PW il |CW i-1j ) and backoff weights such as α(CW i-2j CW i-1j ) cannot be stored in a single (bigram) node array, because l ≠ j and"
"Therefore, we used two separate trees to store probabilities and backoff weights, respectively."
"As a result, we used four tree structures to store ACMs in practice: two for the cluster sub-model P(PW il |CW i-2j CW i-1j ), and two for the word sub-model P(w i |CW i-2k"
CW i-1k PW il ).
We found that the effect of the storage structure cannot be ignored in a real application.
"In addition, we used several techniques to compress model parameters (i.e. word id, n-gram probability, and backoff weight, etc.) and reduce the storage space of models significantly."
"For example, rather than store [Footnote_4]-byte floating point values for all n-gram probabilities and backoff weights, the values are quantized to a small number of quantization levels."
"4 The backoff rates are estimated using the baseline trigram model, so the choice could be biased against the word trigram model."
"Quantization is performed separately on each of the n-gram probability and backoff weight lists, and separate quantization level look-up tables are generated for each of these sets of parameters."
"We used 8-bit quantization, which shows no performance decline in our experiments."
Our goal is to achieve the best tradeoff between performance and model size.
"Therefore, we would like to compare the ACM with the word trigram model at the same model size."
"Unfortunately, the ACM contains four sub-models and this makes it difficult to be pruned to a specific size."
"Thus for comparison, we always choose the ACM with smaller size than its competing word trigram model to guarantee that our evaluation is under-estimated."
Experiments show that the ACMs achieve statistically significant improvements over word trigram models at even smaller model sizes (p-value =8.0E-9).
Some results are shown in Table 3.
Now we discuss why the ACM is superior to simple word trigrams.
"In addition to the better structure as shown in Section 3.3, we assume here that the benefit of our model also comes from its better smoothing."
Consider a probability such as P(Tuesday| party on).
"If we put the word “Tuesday” into the cluster WEEKDAY, we decompose the probability"
P(Tuesday | party on) =
P(WEEKDAY | party on)× P(Tuesday | party on WEEKDAY).
"When each word belongs to one class, simple math shows that this decomposition is a strict equality."
"However, when smoothing is taken into consideration, using the clustered probability will be more accurate than using the non-clustered probability."
"For instance, even if we have never seen an example of “party on Tuesday”, perhaps we have seen examples of other phrases, such as “party on Wednesday”; thus, the probability P(WEEKDAY | party on) will be relatively high."
"Furthermore, although we may never have seen an example of “party on WEEKDAY Tuesday”, after we backoff or interpolate with a lower order model, we may able to accurately estimate P(Tuesday | on WEEKDAY)."
"Thus, our smoothed clustered estimate may be a good one."
Our assumption can be tested empirically by following experiments.
We first constructed several test sets with different backoff rates 4 .
"The backoff rate of a test set, when presented to a trigram model, is defined as the number of words whose trigram probabilities are estimated by backoff bigram probabilities divided by the number of words in the test set."
"Then for each test set, we obtained a pair of CER results using the ACM and the word trigram model respectively."
"As shown in Figure 4, in both cases, CER increases as the backoff rate increases from 28% to 40%."
But the curve of the word trigram model has a steeper upward trend.
"The difference of the upward trends of the two curves can be shown more clearly by plotting the CER difference between them, as shown in Figure 5."
"The results indicate that because of its better smoothing, when the backoff rate increases, the CER using the ACM does not increase as fast as that using the word trigram model."
"Therefore, we are reasonably confident that some portion of the benefit of the ACM comes from its better smoothing."
There are three main contributions of this paper.
"First, after presenting a formal definition of the ACM, we described in detail the methodology of constructing the ACM effectively."
We showed empirically that both the asymmetric clustering and the parameter optimization (i.e. optimal cluster numbers) have positive impacts on the performance of the resulting ACM.
The finding demonstrates partially the effectiveness of our research focus: techniques for using clusters (i.e. the ACM) rather than techniques for finding clusters (i.e. clustering algorithms).
"Second, we explored the actual representation of the ACM and evaluate it on a realistic application – Japanese Kana-Kanji conversion."
"Results show approximately 6-10% CER reduction of the ACMs in comparison with the word trigram models, even when the ACMs are slightly smaller."
"Third, the reasons underlying the superiority of the ACM are analyzed."
"For instance, our analysis suggests the benefit of the ACM comes partially from its better structure and its better smoothing."
"All cluster models discussed in this paper are based on hard clustering, meaning that each word belongs to only one cluster."
"One area we have not explored is the use of soft clustering, where a word w can be assigned to multiple clusters W with a probability P(W|w) [[REF_CITE]]."
Saul and Pereira [1997] demonstrated the utility of soft clustering and concluded that any method that assigns each word to a single cluster would lose information.
It is an interesting question whether our techniques for hard clustering can be extended to soft clustering.
"On the other hand, soft clustering models tend to be larger than hard clustering models because a given word can belong to multiple clusters, and thus a training instance"
P(w i |w i-2 w i-1 ) can lead to multiple counts instead of just 1.
"We study the impact of richer syntac-tic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accu-racy (LP/LR), perplexity (PPL) and word-error-rate (WER, N-best re-scoring)."
"We show that our models achieve an im-provement in LP/LR, PPL and/or WER over the reported baseline results us-ing the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively."
Analysis of parsing per-formance shows correlation between the quality of the parser (as measured by pre-cision/recall) and the language model per-formance (PPL and WER).
A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms[REF_CITE]% when used in isolation as a second pass (N-best re-scoring) language model.
The structured language model uses hidden parse trees to assign conditional word-level language model probabilities.
"As explained[REF_CITE], Section 4.4.1, if the final best parse is used to be the only parse, the reduction in PPL —relative to a 3-gram baseline— using the SLM’s headword parametrization for word prediction is about 40%."
"The key to achieving this reduction is a good guess of the final best parse for a given sen-tence as it is being traversed left-to-right, which is much harder than finding the final best parse for the entire sentence, as it is sought by a regular statistical parser."
"Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sen-tence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure."
The statistical parsing community has used var-ious ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree[REF_CITE].
"Recently, such models[REF_CITE]have been shown to out-perform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively."
"In[REF_CITE], a simple way of enriching the probabilistic dependencies in the CONSTRUC-TOR component of the SLM also showed better PPL and WER performance; the simple modifica-tion to the training procedure brought the WER per-formance of the SLM to the same level with the best as reported[REF_CITE]."
"In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work[REF_CITE]."
The results show that an improved parser (as mea-sured by LP/LR) is indeed helpful in reducing the PPL and WER.
Another remarkable fact is that for the first time a language model exploiting elemen- tary syntactic dependencies obviates the need for interpolation with a 3-gram model in N-best re-scoring.
An extensive presentation of the SLM can be found probabilityin (Chelba and[REF_CITE]).
The model assigns ato every sentence and ev-ery possible binary parse .
"The terminals of are the words of with POS tags, and the nodes of are annotated with phrase headwords and non-terminal labels."
Let be a sentence of length words to which we have prepended the sentence be-ginning marker &lt;s&gt; and appended the sentence end s&gt; so that  &lt;s&gt; and  &lt;/s&gt;. marker  &lt;/   Let be the word -prefix of the sentence up to the current position — andsentence — the words from the beginning of  the X the word-parse -prefix.
"Figure 1 shows a word-parse -prefix; h_0, .., h_{-m} are the ex-posed heads, each head being a pair (headword, non-terminal label), or (word, POS tag) in the case of a root-only tree."
The exposed heads at a given po-sition in the input sentence are a function of the word-parse -prefix.
B0AC4 #!3 2 065&quot; ! #287&quot; D $2C 7 2:$ 9 2E90 &apos; 02:9&apos; :20 (&lt;9 ; #!0 &amp; 5&quot;&gt;= ?22 &amp;7= 2$ &amp;D:2 209:FGFGF0 &apos;D 2C:2 99 00 IH(&amp; 5 2 @( ; ! $ &amp; &apos;)+( * (1) where JJ K  : is the word-parse PORQ -prefix
JTS is the word predicted by WORD-PREDICTOR JVU is OWQ the tag assigned to by the TAGGER is the number of operations the CON-STRUCTOR executes at sentence position before
"U passing control to the WORD-PREDICTOR (the -th U operation at position k is the null transi-tion JYX Z ); is a function of denotes the [ -th CONSTRUCTOR operation carried out at position k in the word string; the oper-ations performed by the CONSTRUCTOR are illus-trated in Figures 2-3 and they ensure that all possi-ble binary branching parses, with all possible head-word and non-terminal label assignments for the \X ] 2 word sequence, can be generated."
The sequence of CONSTRUCTOR operations at position grows the word-parse  -prefix into a word-parse -prefix.
"The SLM is based on three probabilities, each es-timated using deleted interpolation and parameter-ized (approximated) as follows: ! &quot;&gt;#! &quot;65 2/7&amp;$$ :2 9 0 &apos;&apos; 2:9 0 (( ! #&quot;65 ^ _ &amp;^ 9 0 (&lt;&amp; (2) # = 2 7 5 2 #! &quot; D 2:2C 9 7 $0 2:2 &apos;9 20 ( ! &quot;6= 2 7 5 2 &amp;^ _ &amp;^ 9 0 (@&amp; # (3) # 2 ! &quot;D C ^ _ &amp;^ 9 0 ( F (4)"
"It is worth noting that if the binary branching struc-ture developed by the parser were always right-branching and we mapped the POS tag and non-terminal label vocabularies to a single type, then our model would be equivalent to a trigram lan-guage model."
"Since  the number of parses for a given `ba  `fehg word prefix @i grows exponentially with , , the state space of our model is huge even for relatively short sentences, so we have to use a search strategy that prunes it."
One choice is a synchronous multi-stack search algorithm which is very similar to a beam search.
"The language model probability assignment for Q in the input sentence is the word at position made using:  &quot;65 2 &quot;I.1$ 0 27 $&amp;&apos; 22 (( ** ! # 2 &apos; 2 2 ( 7!#&quot;&gt;5 2 .2 0 7 2$ #! 2 &apos;&quot;I$2 (&lt;2; &apos; I&quot;2$ (&lt;&amp;2 &amp;&apos; 2 (&lt;&amp; ! &quot;I$ 2 (5) over stringswhich ensures  a proper probability normalization, where is the set of all parses present in our stacks at the current stage ."
"Each model component —WORD-PREDICTOR, TAGGER, CONSTRUCTOR— is initialized from a set of parsed sentences after undergoing headword percolation and binarization, see Section 2.2."
An N-best EM[REF_CITE]variant is then employed to jointly reestimate the model parameters such that the PPL on training data is decreased — the likelihood of the training data under our model is increased.
The reduction in PPL is shown experi-mentally to carry over to the test data.
"As explained in the previous section, the SLM is ini-tialized on parse trees that have been binarized and the non-terminal (NT) tags at each node have been enriched with headwords."
We will briefly review the headword percolation and binarization procedures; they are explained in detail[REF_CITE].
"The position of the headword within a constituent — equivalent to a context-free  production  are NToflabelsthe typeor   , where POS tags (only for Z ) — is specified using a rule-based approach."
"Assuming that the index of the headword on the right-hand side of the rule is , we binarize the con-stituent as follows: depending on the identity we apply one of the two binarization schemes in Fig-binarization schemes receive the NT labelure 4."
The intermediate nodes created by the  above [Footnote_1] .
1 Any resemblance to X-bar theory is purely coincidental.
The choice among the two schemes is made according to a list of rules based on the identity of the label on the left-hand-side of a CF rewrite rule.
"The SLM is a strict left-to-right, bottom-up parser, therefore in Eq.( 2, 3, 4) the probabilities are con- ditioned on the left contextual information."
"There are two main reasons we prefer strict left-to-right parsers for the purpose of language modeling (Roark J , 2001): when looking for the most likely word string given the acoustic signal (as required in a speech recognizer), the search space is orga-nized as a prefix tree."
A language model whose aim is to guide the search must thus operate J left-to-right. previous results[REF_CITE]show that a grammar-based language model benefits from interpolation with a 3-gram model.
Strict left-to-right parsing makes it easy to combine with a standard 3-gram at the word level[REF_CITE]rather than at sentence level[REF_CITE].
"For these reasons, we prefer enriching the syntactic dependencies by information from the left context."
"However, as mentioned[REF_CITE], one way of conditioning the probabilities is by annotat-ing the extra conditioning information onto the node labels in the parse tree."
We can annotate the training corpus with richer information and with the same SLM training procedure we can estimate the prob-abilities under the richer syntactic tags.
"Since the treebank parses allow us to annotate parent informa-tion onto the constituents, as Johnson did[REF_CITE], this richer predictive annotation can ex-tend information slightly beyond the left context."
"Under the equivalence classification in Eq.( 2, 3, 4), the conditional information avail-able to the SLM model components is made up of the two most-recent exposed heads consisting of two NT tags and two headwords."
"In an attempt to extend the syntactic dependencies beyond this level, we enrich the non-terminal tag of a node in the binarized parse tree with the NT tag of the parent node, or the NT tag of the child node from which the headword is not being percolated (same as[REF_CITE]), or we add the NT tag of the third most-recent exposed head to the history of the CONSTRUCTOR component."
"The three ways are briefly described as: 1. opposite (OP): we use the non-terminal tag of the child node from which the headword is not being percolated [Footnote_2]. parent (PA): we use the non-terminal tag of the parent node to enrich the current node 3. h-2: we enrich the conditioning information of the CONSTRUCTOR with the non-terminal tag of the third most-recent exposed head, but not the headword itself."
2 The NP+* has not been enriched yet because we have not specified the NT tag of the parent of the NP group
"Consequently, Eq. 4 becomes ! &quot; D C 7 2 2# 2 $ &apos; ( * !#&quot; D 2C ^ _ &amp;^ 9 0 &amp;^ 9 F=  8("
We take the example[REF_CITE]to illustrate our enrichment approaches.
"Assume that after binarization and headword percolation, we have a noun phrase constituent: (NN group)))), which, after enriching the non-terminal tags using the opposite and parent scheme, respectively, be-comes (NP+*_group (DT+NP the) (NP’+NP_group (NNP+NP’ dutch) (NP’+NP’_group (VBG+NP’ publishing) (NN+NP’ group))))."
A given binarized tree is traversed recursively in depth-first order and each constituent is enriched in the parent or opposite manner or both.
"Then from the resulting parse trees, all three components of the SLM are initialized and N-best EM training can be started."
"Notice that both parent and opposite affect all three components of the SLM since they change the NT/POS vocabularies, but h-2 only affects the CON-STRUCTOR component."
"So we believe that if h-2 helps in reducing PPL and WER, it’s because we have thereby obtained a better parser."
We should also notice the difference between parent and op-posite in the bottom-up parser.
"In opposite scheme, POS (part of speech) tags are not enriched."
"As we parse the sentence, two most-recent exposed heads will be adjoined together under some enriched NT label (Figure 2, 3), the NT label has to match the NT tag of the child node from which the headword is not being percolated."
"Since the NT tags of the chil-dren are already known at the moment, the opposite scheme actually restricts the possible NT labels."
"In the parent scheme, POS tags are also enriched with the NT tag of the parent node."
"When a POS tag is predicted from the TAGGER, actually both the POS tag and the NT tag of the parent node are hypoth-esized."
"Then when two most recent exposed heads are adjoined together under some enriched NT label, the NT label has to match the parent NT informa-tion carried in both of the exposed heads."
"In other words, if the two exposed heads bear different in-formation about their parents, they can never be ad-joined."
"Since this restriction of adjoin movement is very tight, pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing perfor-mance."
"Since the SLM parses sentences bottom-up while the parsers used[REF_CITE],[REF_CITE]and[REF_CITE]are top-down, it’s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above."
"However, it is their “pick-and-choose” strategy that inspired our study of richer syntactic dependencies for the SLM."
"With the three enrichment schemes described in Sec-tion 3 and their combinations, we evaluated the PPL performance of the resulting seven models on the UPenn Treebank and the WER performance on the WSJ setup, respectively."
"In order to see the corre-spondence between parsing accuracy and PPL/WER performance, we also evaluated the labeled preci-sion and recall statistics (LP/LR, the standard pars-ing accuracy measures) on the UPenn Treebank cor-pus."
"For every model component in our experi-ments, deleted-interpolation was used for smooth-ing."
The interpolation weights were estimated from separate held-out data.
"For example, in the UPenn Treebank setup, we used section 00-20 as training data, section 21-22 as held-out data, and section 23- 24 as test data."
"We have evaluated the perplexity of the seven dif-ferent models, resulting from applying parent, op-posite, h-2 and their combinations."
For each way of initializing the SLM we have performed 3 iterations of N-best EM training.
"The SLM is interpolated with a 3-gram model, built from exactly the same training data and word vocabulary, using a fixed in-terpolation weight."
"As we mentioned in Section 3, the NT/POS vocabularies for the seven models are different because of the enrichment of NT/POS tags."
"Table 1 shows the actual vocabulary size we used for each model (for parser, the vocabulary is a list of all possible parser operations)."
The baseline model is the standard SLM as described[REF_CITE].
The PPL results are summarized in Table 2.
The SLM is interpolated with a 3-gram model as shown in the equation:  6 6  L   6 QTO
We should note that the PPL result of the 3-gram model is 166.6.
"As we can see from the table, without interpolating with the 3-gram, the oppo-site scheme performed the best, reducing the PPL of the baseline SLM by almost 5% relative."
"When the SLM is interpolated with the 3-gram, the h- 2+opposite+parent scheme performed the best, re-ducing the PPL of the baseline SLM by 3.3%."
"How-ever, the parent and opposite+parent schemes are both worse than the baseline, especially before the EM training and with =0.0."
We will discuss the results further in Section 4.4.
Table 3 shows the labeled precision/recall accuracy results.
The labeled precision/recall results of our model are much worse than those reported[REF_CITE]and[REF_CITE].
"One of the rea-sons is that the SLM was not aimed at being a parser, but rather a language model."
"Therefore, in the search algorithm, the end-of-sentence symbol can be predicted before the parse of the sentence is ready for completion 3 , thus completing the parse with a series of special CONSTRUCTOR moves (see[REF_CITE]for details)."
The SLM allows right-branching parses which are not seen in the UPenn Treebank corpus and thus the evaluation against the UPenn Treebank is inherently biased.
"It can also be seen that both the LP and the LR dropped after [Footnote_3] training iterations: the N-best EM variant used for SLM training algorithm increases the likelihood of the training data, but it cannot guar-antee an increase in LP/LR, since the re-estimation algorithm does not explicitly use parsing accuracy as a criterion."
"3 A parse is ready for completion when at the end of the sentence there are exactly two exposed headwords, the first of which if the start-of-sentence symbol and the second is an or-dinary word. See[REF_CITE]for details about special rules."
"To test our enrichment schemes in the context of speech recognition, we evaluated the seven models in the WSJ DARPA’93 HUB1 test setup."
"The same setup was also used[REF_CITE],[REF_CITE]and[REF_CITE]."
"The size of the test set is 213 utterances, 3446 words."
The lat-tices and N-best lists were generated using the stan-dard 3-gram model trained on 45M words of WSJ.
"The N-best size was at most 50 for each utterance, and the average size was about 23."
"The SLM was trained on 20M words of WSJ text automatically parsed using the parser[REF_CITE], bi-narized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Sec-tion 3."
"Because SLM training on the 20M words of WSJ text is very expensive, especially after enrich-ing the NT/POS tags, we only evaluated the WER performance of the seven models with initial statis-tics from binarized and enriched parse trees."
The results are shown in Table 4.
"The table shows not only the results according to different interpolation weights , but also the results corresponding to , a virtual interpolation weight."
"We split the test data into two parts, and ."
"The best interpolation weight, estimated from part , was used to decode part , and vice versa."
We finally put the decod-ing results of the two parts together to get the final decoding output.
The interpolation weight is vir-tual because the best interpolation weights for the two parts might be different.
"Ideally, should be estimated from separate held-out data and then ap-plied to the test data."
"However, since we have a small number of N-best lists, our approach should be a good estimate of the WER under the ideal inter-polation weight."
"As can be seen, the h-2+opposite scheme achieved the best WER result, with a 0.5% abso-lute reduction over the performance of the opposite scheme."
"Overall, the enriched[REF_CITE]% relative reduction in WER over the 3-gram model   )."
"By enriching the syntactic dependencies, we expect the resulting models to be more accurate and thus give better PPL results."
"However, in Table 2, we can see that this is not always the case."
"For ex-ample, the parent and opposite+parent schemes are worse than baseline in the first iteration when =0.0, the h-2+parent and h-2+opposite+parent schemes are also worse than h-2 scheme in the first iteration when =0.0."
Why wouldn’t more information help?
There are two possible reasons that come to mind:
"In order to validate the first hypothesis, we eval-uated the training data PPL for each model scheme."
"As can be seen from Table 5, over-parameterization is indeed a problem."
"From scheme h-2 to h- 2+opposite+parent, as we add more information to the conditioning context, the training data PPL de-creases."
"The test data PPL in Table 2 does not follow this trend, which is a clear sign of over-parameterization."
"Over-parameterization might also occur for par-ent and opposite+parent, but it alone can not explain the high PPL of training data for both schemes."
The LP/LR results in Table 3 show that bad parsing ac- curacy also plays a role in these situations.
The la-beled recall results of parent and opposite+parent are much worse than those of baseline and other schemes.
The end-of-sentence parse completion strategy employed by the SLM is responsible for the high precision/low recall operation of the parent and opposite+parent models.
"Adding h-2 remedies the parsing performance of the SLM in this situation, but not sufficiently."
It is very interesting to note that labeled recall and language model performance (WER/PPL) are well correlated.
"Figure 5 compares PPL, WER ( =0.0 at training iteration 0) and labeled precision/recall error(100-LP/LR) for all models."
"Overall, the la-beled recall is well correlated with the WER and PPL values."
Our results show that improvement in the parser accuracy is expected to lead to improve-ment in WER.
"Finally, in comparison with the language model[REF_CITE]which is based on a probabilistic top-down parser, and with the Bihead/Trihead lan-guage models[REF_CITE]which are based on immediate head parsing, our enriched models are less effective in reducing the test data PPL: the best PPL result[REF_CITE]on the same experimen-tal setup is 137.3, and the best PPL result[REF_CITE]is 126.1."
We believe that examining the differences between the SLM and these models could help in understanding the degradation: 1.
The parser[REF_CITE]uses a “pick-and-choose” strategy for the conditioning informa-tion used in the probability models.
This al-lows the parser to choose information depend-ing on the constituent that is being expanded.
"The SLM, on the other hand, always uses the same dependency structure that is decided be-forehand. 2."
The parser[REF_CITE]is not a strict left-to-right parser.
"Since it is top-down, it is able to use the immediate head of a constituent before it occurs, while this immediate head is not available for conditioning by a strict left-to-right parser such as the SLM."
"Consequently, the interpolation with the 3-gram model is done at the sentence level, which is weaker than in-terpolating at the word level."
"Since the WER results[REF_CITE]are based on less training data (2.2M words total), we do not have a fair comparison between our best model and Roark’s model."
We have presented a study on enriching the syn-tactic dependency structures in the SLM.
We have built and evaluated the performance of seven dif-ferent models.
All of our models improve on the baseline SLM in either PPL or WER or both.
We have shown that adding the NT tag of the third most-recent exposed head in the parser model improves the parsing performance significantly.
"The improve-ment in parsing accuracy carries over to enhanc-ing language model performance, as evaluated by both WER and PPL."
"Furthermore, our best result shows that an uninterpolated grammar-based lan-guage model can outperform a 3-gram model."
The best model achieved an overall WER improvement of 10% relative to the 3-gram baseline.
"Although conditioning on more contextual infor-mation helps, we should note that some of our mod-els suffer from over-parameterization."
"One solu-tion would be to apply the maximum entropy esti-mation technique (MaxEnt[REF_CITE]) to all of the three components of the SLM, or at least to the CONSTRUCTOR."
That would also allow for fine-tuning of the particular syntactic dependencies used in the model rather than the template based method we have used.
"Along these lines, the Max-Ent model has already shown promising improve-ments by combining syntactic dependencies in the WORD-PREDICTOR of the SLM[REF_CITE]."
We present a constancy rate princi-ple governing language generation.
We show that this principle implies that lo-cal measures of entropy (ignoring con-text) should increase with the sentence number.
We demonstrate that this is indeed the case by measuring entropy in three different ways.
We also show that this effect has both lexical (which words are used) and non-lexical (how the words are used) causes.
It is well-known from Information Theory that the most efficient way to send information through noisy channels is at a constant rate.
"If humans try to communicate in the most efficient way, then they must obey this principle."
"The communication medium we examine in this pa-per is text, and we present some evidence that this principle holds here."
Entropy is a measure of information first pro-posed[REF_CITE].
"Informally, entropy of a random variable is proportional to the diffi-culty of correctly guessing the value of this vari-able (when the distribution is known)."
"Entropy is the highest when all values are equally prob-able, and is lowest (equal to 0) when one of the choices has probability of 1, i.e. deterministi-cally known in advance."
"In this paper we are concerned with entropy of English as exhibited through written text, though these results can easily be extended to speech as well."
"The random variable we deal with is therefore a unit of text (a word, for our purposes [Footnote_1] ) that a random person who has pro-duced all the previous words in the text stream is likely to produce next."
"1 It may seem like an arbitrary choice, but a word is a natural unit of length, after all when one is asked to give the length of an essay one typically chooses the number of words as a measure."
We have as many ran-dom variables as we have words in a text.
The distributions of these variables are obviously dif-ferent and depend on all previous words pro-duced.
"We claim, however, that the entropy of these random variables is on average the same [Footnote_2] ."
"2 Strictly speaking, we want the cross-entropy between all words in the sentences number n and the true model of English to be the same for all n."
There has been work in the speech community inspired by this constancy rate principle.
"In speech, distortion of the audio signal is an extra source of uncertainty, and this principle can by applied in the following way:"
"A given word in one speech context might be common, while in another context it might be rare."
"To keep the entropy rate constant over time, it would be necessary to take more time (i.e., pronounce more carefully) in less common situations."
It has also been suggested that the principle of constant entropy rate agrees with biological evidence of how human language processing has evolved[REF_CITE].
"Let {X i },i = 1... n be a sequence of random variables, with X i corresponding to word w i in the corpus."
Let us consider i to be fixed.
"The random variable we are interested in is Y i , a ran-dom variable that has the same distribution as X i |X 1 = w 1 ,...,X i−1 = w i−1 for some fixed words w 1 ... w i−1 ."
"For each word w i there will be some word w j , (j ≤ i) which is the start-ing word of the sentence w i belongs to."
We will combine random variables X 1 . . .
X i−1 into two sets.
"The first, which we call C i (for context), contains X 1 through X j−1 , i.e. all the words from the preceding sentences."
"The remaining set, which we call L i (for local), will contain words X j through X i−1 ."
Both L i and C i could be empty sets.
"We can now write our variable Y i as X i |C i , L i ."
"Our claim is that the entropy of Y i , H(Y i ) stays constant for all i. By the definition of rel-ative mutual information between X i and C i ,"
H(Y i ) =
"H(X i |C i , L i ) ="
"H(X i |L i ) − I(X i |C i , L i ) where the last term is the mutual information between the word and context given the sen-tence."
"As i increases, so does the set C i ."
"L i , on the other hand, increases until we reach the end of the sentence, and then becomes small again."
"Intuitively, we expect the mutual information at, say, word k of each sentence (where L i has the same size for all i) to increase as the sen-tence number is increasing."
By our hypothesis we then expect H(X i |L i ) to increase with the sentence number as well.
"Current techniques are not very good at es-timating H(Y i ), because we do not have a very good model of context, since this model must be mostly semantic in nature."
"We have shown, however, that if we can instead estimate H(X i |L i ) and show that it increases with the sentence number, we will provide evidence to support the constancy rate principle."
"The latter expression is much easier to esti-mate, because it involves only words from the beginning of the sentence whose relationship is largely local and can be successfully cap-tured through something as simple as an n-gram model."
"We are only interested in the mean value of the H(X j |L j ) for w j ∈ S i , where S i is the ith sentence."
"This number is equal to |S1 i | H(S i ), which reduces the problem to the one of esti-mating the entropy of a sentence."
"We use three different ways to estimate the entropy: • Estimate H(S i ) using an n-gram probabilis-tic model • Estimate H(S i ) using a probabilistic model induced by a statistical parser • Estimate H(X i ) directly, using a non-para-metric estimator."
We estimate the entropy for the beginning of each sentence.
"This approach estimates H(X i ), not H(X i |L i ), i.e. ignores not only the context, but also the local syntactic information."
N-gram models make the simplifying assump-tion that the current word depends on a con-stant number of the preceding words (we use three).
The probability model for sentence S thus looks as follows:
P (S) = P (w 1 )P (w 2 |w 1 )P (w 3 |w 2 w 1 ) Y n × P (w n |w n−1 w n−2 w n−3 ) i=4
"To estimate the entropy of the sentence S, we compute log P(S)."
This is in fact an estimate of cross entropy between our model and true distri-bution.
"Thus we are overestimating the entropy, but if we assume that the overestimation error is more or less uniform, we should still see our esti-mate increase as the sentence number increases."
"Penn Treebank corpus[REF_CITE]sections 0-20 were used for training, sections 21- 24 for testing."
"Each article was treated as a sep-arate text, results for each sentence number were grouped together, and the mean value reported on Figure 1 (dashed line)."
"Since most articles are short, there are fewer sentences available for larger sentence numbers, thus results for large sentence numbers are less reliable."
"The trend is fairly obvious, especially for small sentence numbers: sentences (with no con-text used) get harder as sentence number in-creases, i.e. the probability of the sentence given the model decreases."
We also computed the log-likelihood of the sen-tence using a statistical parser described[REF_CITE]3 .
The probability model for sentence S with parse tree T is (roughly):
Y P (S) = P (x|parents(x)) x∈T where parents(x) are words which are parents of node x in the the tree T. This model takes into account syntactic information present in the sentence which the previous model does not.
The entropy estimate is again log P (S).
"Overall, these estimates are lower (closer to the true en-tropy) in this model because the model is closer to the true probability distribution."
"The same corpus, training and testing sets were used."
The results are reported on Figure 1 (solid line).
"The estimates are lower (better), but follow the same trend as the n-gram estimates."
Finally we compute the entropy using the esti-mator described[REF_CITE].
The estimation is done as follows.
Let T be our training corpus.
Let S = {w 1 . . . w n } be the test sentence.
"We find the largest k ≤ n, such that sequence of words w 1 . . . w k occurs in T. Then log S k is an estimate of the entropy at the word w 1 ."
"We compute such estimates for many first sentences, second sentences, etc., and take the average."
For this experiment we used [Footnote_3] million words of the Wall Street Journal (year 1988) as the train-ing set and 23 million words (full year 1987) as the testing set [Footnote_4] .
"3 This parser does not proceed in a strictly left-to-right fashion, but this is not very important since we estimate entropy for the whole sentence, rather than individual words"
"4 This is not the same training set as the one used in two previous experiments. For this experiment we needed a larger, but similar data set"
The results are shown on Fig-ure 2.
"They demonstrate the expected behavior, except for the strong abnormality on the second sentence."
This abnormality is probably corpus-specific.
"For example, 1.5% of the second sen-tences in this corpus start with words “the terms were not disclosed”, which makes such sentences easy to predict and decreases entropy."
We have shown that the entropy of a sentence (taken without context) tends to increase with the sentence number.
We now examine the causes of this effect.
These causes may be split into two categories: lexical (which words are used) and non-lexical (how the words are used).
"If the effects are en-tirely lexical, we would expect the per-word en-tropy of the closed-class words not to increase with sentence number, since presumably the same set of words gets used in each sentence."
For this experiment we use our n-gram estima-tor as described in Section 4.2.
"We evaluate the per-word entropy for nouns, verbs, deter-miners, and prepositions."
The results are given in Figure 3 (solid lines).
"The results indicate that entropy of the closed class words increases with sentence number, which presumably means that non-lexical effects (e.g. usage) are present."
We also want to check for presence of lexical effects.
It has been shown[REF_CITE]that lexical effects can be easily captured by caching.
"In its simplest form, caching in-volves keeping track of words occurring in the previous sentences and assigning for each word w a caching probability P c (w) ="
"P C(w) , where w C(w)"
C(w) is the number of times w occurs in the previous sentences.
This probability is then mixed with the regular probability (in our case - smoothed trigram) as follows:
P mixed (w) = (1 − λ)P ngram (w) + λP c (w) where λ was picked to be 0.1.
This new prob-ability model is known to have lower entropy.
"More complex caching techniques are possible[REF_CITE], but are not necessary for this experiment."
"Thus, if lexical effects are present, we expect the model that uses caching to provide lower entropy estimates."
The results are given in Fig-ure 3 (dashed lines).
"We can see that caching gives a significant improvement for nouns and a small one for verbs, and gives no improvement for the closed-class parts of speech."
This shows that lexical effects are present for the open-class parts of speech and (as we assumed in the previ-ous experiment) are absent for the closed-class parts of speech.
"Since we have proven the pres-ence of the non-lexical effects in the previous experiment, we can see that both lexical and non-lexical effects are present."
"We have proposed a fundamental principle of language generation, namely the entropy rate constancy principle."
"We have shown that en-tropy of the sentences taken without context in-creases with the sentence number, which is in agreement with the above principle."
We have also examined the causes of this increase and shown that they are both lexical (primarily for open-class parts of speech) and non-lexical.
"These results are interesting in their own right, and may have practical implications as well."
"In particular, they suggest that language modeling may be a fruitful way to approach is-sues of contextual influence in text."
"Of course, to some degree language-modeling caching work has always recognized this, but this is rather a crude use of context and does not address the issues which one normally thinks of when talking about context."
"We have seen, however, that entropy measurements can pick up much more subtle influences, as evidenced by the results for determiners and prepositions where we see no caching influence at all, but nev-ertheless observe increasing entropy as a func-tion of sentence number."
This suggests that such measurements may be able to pick up more obviously semantic contextual influences than simply the repeating words captured by caching models.
"For example, sentences will differ in how much useful contextual information they carry."
Are there useful generalizations to be made?
"E.g., might the previous sentence always be the most useful, or, perhaps, for newspa-per articles, the first sentence?"
Can these mea-surements detect such already established con-textual relations as the given-new distinction?
What about other pragmatic relations?
All of these deserve further study.
We would like to acknowledge the members of the Brown Laboratory for Linguistic Informa-tion Processing and particularly Mark Johnson for many useful discussions.
Also thanks to Daniel Jurafsky who early on suggested the in-terpretation of our data that we present here.
This research has been supported in part by NSF grants[REF_CITE]and[REF_CITE].
¢¡h£h¤ ¥j¦ §L¥  «=¥ ¢­h®¯¥ °²± ®³°´£hµF­r§L®r§º§&gt;£&gt;¥ °¯·4»&gt;&gt; ©¼·®½§¹¤ ¤ ¥ °¾«=¡W§&gt;­h¥°³¡F£¡F|1§L¥©r§¹¤°³·)¡F£¿¤ ¥ §^±± ¥ °¯¤ ¥ &gt;£r§L®³Àh¤  ¶ª¡F¦ §°½£Á °´¶h®³ ®´§&gt;£hµh­r§Lµ&gt;| &gt;¦Å·¨Á°´£h·¡F¦ ¶r¡h¦ §L¥ ¤ ¶h®³¡&quot;&gt;°³¥&quot;  &gt;Ð )È8£¡F£h&gt;¤&gt;¤ ÉrÆ&quot;Ê¸&gt;± ¥ °¯·q·®´§L¤ ¤ °¾«= °³¡F£ ¡L¬r»&gt; ^¥  &quot;¡F¦ Ó &gt;®³°¯¤ &gt; © ·®´§L¤ ¤ °²«= °³¡F£¥ ¡ §Ç£hÆ ®´§&gt;£hµh­r§Lµ&gt;(ÕHÈH¥ &gt;£h.¥ (¡ £hÆ×·®´§L¤ ¤ ¤  &gt;£hµh­r§Lµ&gt;&gt; &gt;É &gt; ¦ §L·ÀW¡¹¬ªØ&gt;ÙªÂÛÚFÜÝÕH©r§L¤ &gt;ÞrÂ²ßUÜ1ÖÂ#+¤ · ± ¡h£h|¤ ¥j¦  &gt;°³¥ ¤¢¥j¨h¿â&gt;Ìã ä ^=å æ  &quot;¤ ®½§U£hµF­r§Lµ&gt;¤&quot; Àr£&gt;¥ §L· ± ¥  ¤ °³¡F£¡L¬Q¤ 1§&gt;£U¥  °¯¤^ÉF¥ ¡ ¤ ¨h¡YÆè¥j¨r§L¥á·¡F&gt;.°´£U¬K¡h¦ 1§L¥ °³¡F£ §U©r¡F­h¥;|¥j¨h°´¦Ê¥j¦§U£h¤ ®´§L¥ °³¡F£h¤Ê°½£§1¤&gt;£é©r¸h¥j¦·¡h£hà®´§§¹·¥&gt;£U± µh­r§Lµ&gt; ~ ­h¤  ¡L¬ ¢­h®³¥ °³®³°´£U± ¬H¡F¦µh­r§L®º§^&gt;£h·S¡L¬=+¥ °´¨híÔS£hµU®¯°³¤j¨î»&gt;¤+·®´§L¤&gt;¤ °¾«= ?±  §L·À¡L¬SØ&gt;ÞrÂ²ï&gt;Ü×ÕH©r§¹¤ ®³°½£h1Þ&gt;ÞªÂ²Þ&gt;Ü1ÖÂ ð ñ s7u¹&lt;U5)òSnShu&gt;?{ 5+s
ÏvÆ(ÄZÓ&lt;*À ÔÁÄ=*Ï ¿ÁÓ&lt;Õ
ÅMÝd×*À ÏvÆ^&lt;×&lt;¿ËÚ@É=*Ï×JÆ*ÅØÐZÒÙÄÛÚZÅmÆ^Ä *ÂoÆ*ÅMÇ&lt;&lt;¿É^¿ÁÉmÄÏvÐÛÏ¾ =ÏvÅ@*¿ÁÀÑÄÖ6Ä¾oÄ==*Æ ÏÜÐZÒyÄZÓÉ@ÐZÆ*ÅÏ
Ä Âo¾JÆ&lt;^ÅMÓÄZÀ^ÏáÐZÒâÏ*&lt;*¿ÁÉ¾JÐqÇÕZÆ@Ï*¿ËÐ7Ó&lt;Ä=(ÄZÀvÂoÀÀvÅmÚgÖÆ^ã ÖwÊoÄZÀ*ÅMÇ ÅmÆ(*Ä=^ÇKÄZÓ&lt;ÇæÏ*¾JÐ7ÀvÅ¹ÐZÒ *Ï &lt;¾ ÅÇJÅmÒ|Ö6¾JÅMÄZÇ&lt;ÀæÄZÀÄ/{Ò ÐZÆ^ÍçÐZÒèÏ*&lt;¾ ÅM¿ËÆÉmÄZÀvÅ {Æ^&lt;¿ÁÉ^¾ØÍ`ÅMÄZÓ&lt;ÇéÄZÔÁ¿ËÕ7Ó&lt;ÀëÏ*¾JÅ&lt;:ÀêÏÄZÉmÝd×*¾JÐ7ÀvÅHÉmÄZÀvÅ&lt;¿ÁÀ^¿ËÏ*/{Ò Æ^^Ç
Ò *ÀÇ&lt;ÅMÓÅmÏvÅMÉ&lt;ÀvÅ@Ï*&lt;Ç ¿ÁÀ^ÄZÍTÊo¿ÁÕ7×*¾JÅÄ&lt;=Ä=Â&lt;Ï*ÂoÆ¿ËÐ7ÓíÆ*ÐZÂ&lt;^Æ ^¿ÁÄ&lt;×=ÏvÅÔËÅMÀÛÄZÓÅMÝd×&lt;&lt;¿ËÚÇáÏ=ÄZÔËÅMÓgÏ*¾&lt;Å ÄZÓoÇîÉmÄZÀvÅTÍ`Ä=^Æ ïZÅmÆÏvÆ^ÄZÓ&lt;*À |Ò ÐZÆ^ÍjÄ=*Ï ¿ËÐ7Ó ä ð ñ[REF_CITE]. .RU ó
"Å =Ä Æ*Å É@Ð7Ó&lt;Ç&lt;×&lt;@É Ï*,Æ*ÅMÀvÅMÄ=(Æ ^É =*Ï ¿ÁÉ2ÂoÄ=^Æ ÄEÖ Âo¾JÆ(ÄZÀ*¿ÁÓJÕô×&lt;À^¿ÁÓJÕèÄZÓjÐZÆ(Ç&lt;¿ÁÓ&lt;Ä=Æ*&lt;¿ÁÉ@*Ï ¿ÁÐ7Ó&lt;=Ä *Æ ãZÞ7Æ* ß2ÐZÆ^&lt;Ç À¹¿ÁÓêÄZÓÙ¿ÁÓ&lt;*¾îÏ*=Â&lt;ÂoÆ*ÐZÂ&lt;^Æ ¿ÁÄ=ÏvÅ[Å@õdÂoÆ*ÅMÀÖ *¿ËÐ7ÓoÀT¿ÁÓBÏ*&lt;¾ ÅM¿ËÆ[&lt;¿ËÏ*¿ËÐ7Ó&lt;&lt;¿ÁÉ@*Ï ¿ËÐ7ÓoÄ=Æ*ãZäê÷q¿ÁÓ&lt;É@ÅÀ ÄøÇo¿ÁÉ@*Ï ¿ËÐ7Ó&lt;=Ä Æ*ãùÅ@õqÂoÔÁÄZ¿ÁÓoÀBÄé¾JÅMÄZÇ&lt;ßVÐZÆ^Ç&amp;ÊgãÛÀ * ß2ÐZÆ^Ç&lt;À(ú}@Å õqÂ&lt;Æ*ÅMÀ*^À ¿ËÐ7Ó&lt;*¾JÅÜÇo¿ÁÉ@*Ï ¿ËÐ7Ó&lt;Ä=*Æ ãgÖwÊoÄZÀvÅMÇûÂoÄ=^Æ ÄEÖ Âo¾JÆ(ÄZÀ*¿ÁÓJÕüÏvÆ^ÄZÓ&lt;*À ÔÄ=ÏvÅMÀýÄZÓ ¿ÁÓJÂo×&lt;Ï/ÀvÅMÓgÏvÅMÓ&lt;É@Åø¿ÁÓgÏvÐûÄ *À ¿ÁÍ`ÂoÔËÅmÆ`ÀvÅMÓgÏvÅMÓ&lt;É@&lt;=*@É Æ^¿ËÊLÅMÀ`Ä:É@ÐZÆ*Å"
"É@[*Ï ¾JÅBÇ&lt;¿É@Ï*¿ËÐ7Ó&lt;Ä=*Æ ãgÖwÊoÄZÀvÅMÇþÂoÄ=Æ^Ä=^ÄZÀÖ ¿ÁÓJÕJÞZÏ*Æ Õ7×&lt;*Í`ÅMÓ¾oÄ=Ï ÀvÏvÆ^×oÉ@Ï*×JÆ**,Å ÐZÒòÄìÚZÅmÆ^ÄZÓ*&lt;Êë¿ÁÓgÏvÐìÏ^À ÔÁÄ=*Ï ¿ÁÓJÕèÄèÂ*¾oÄ=&lt;^Æ ÅMÇ&lt;¿ÁÉmÄ*=ÏvÅ@Ö ÂoÔËÅmÆ¹ÅMÝd×=ÄÉmÄZÓ/ÏvÆ^ÄZÓ&lt;&lt;¿ËÚÀ*=ÔÁÄÄZÔËÅMÓgÏ=ÏvÅ ,Ð»ÚZÅmÆÚZÅmÆ^(ÔËÐgÐZï[ÿJÐZÆTÅ:Ï*¾JÅÍ`¿ÀvÏ@*=Ä ïZÅ  ¿Ó ÏvÐ *vÓJÐZÏ¾JÐqÇ ÓJÐZÏ*¿ÁÉ@ÅTÏ*¾JÅôÍ`¿ÁÀvÏ*Ä=ïZÅ  qä ÔËÅMÄZÇ&lt;Ä=ÀVÏÆ^*Ä=¾JÅÂo¾,&lt;^ÀvÐ7ÔÁ×JÏÆ ÄZÀ***¾JÅ,Êo¿ËÕZÕZÅMÀvÏVÐZÊoÀvÏ^¿ËÏ*Àmäêå¹Ó&lt;*ÄZÉmÔËÅ¹¿ÁÓÓÅj¿ÁÀèÏ*¾&lt;Ä=&lt;Ïì¿ËÏ=Ä ÏÖ"
"The paper describes the application of k- Means, a standard clustering technique, to the task of inducing semantic classes for German verbs."
"Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausi-ble clustering of 57 verbs into 14 classes."
"The automatic clustering was evaluated against independently motivated, hand-constructed semantic verb classes."
"A series of post-hoc cluster analyses ex-plored the influence of specific frames and frame groups on the coherence of the verb classes, and supported the tight connec-tion between the syntactic behaviour of the verbs and their lexical meaning com-ponents."
"A long-standing linguistic hypothesis asserts a tight connection between the meaning components of a verb and its syntactic behaviour: To a certain ex-tent, the lexical meaning of a verb determines its be-haviour, particularly with respect to the choice of its arguments."
The theoretical foundation has been es-tablished in extensive work on semantic verb classes such[REF_CITE]for English and[REF_CITE]for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties.
"From a practical point of view, a verb classifi-cation supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge."
"For example, the En-glish verb classification has been used for applica-tions such as machine translati[REF_CITE], word sense disambiguati[REF_CITE], and document classificati[REF_CITE]."
Various attempts have been made to infer conve-niently observable morpho-syntactic and semantic properties for English verb classes ([REF_CITE]; Schulte im[REF_CITE]).
To our knowledge this is the first work to ob-tain German verb classes automatically.
We used a robust statistical parser[REF_CITE]to ac-quire purely syntactic subcategorisation information for verbs.
The information was provided in form of probability distributions over verb frames for each verb.
"There were two conditions: the first with relatively coarse syntactic verb subcategorisa-tion frames, the second a more delicate classifica-tion subdividing the verb frames of the first con-dition using prepositional phrase information (case plus preposition)."
"In both conditions verbs were clustered using k-Means, an iterative, unsupervised, hard clustering method with well-known properties, cf.[REF_CITE]."
"The goal of a series of cluster analyses was (i) to find good values for the parameters of the clustering process, and (ii) to explore the role of the syntactic frame descrip-tions in verb classification, to demonstrate the im-plicit induction of lexical meaning components from syntactic properties, and to suggest ways in which the syntactic information might further be refined."
Our long term goal is to support the development of high-quality and large-scale lexical resources.
The syntactic subcategorisation frames for German verbs were obtained by unsupervised learning in a statistical grammar framework (Schulte im[REF_CITE]): a German context-free grammar contain-ing frame-predicting grammar rules and information about lexical heads was trained on 25 million words of a large German newspaper corpus.
The lexi-calised version of the probabilistic grammar served as source for syntactic descriptors for verb frames (Schulte im[REF_CITE]).
The verb frame types contain at most three arguments.
"Possible arguments in the frames are nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns (r), prepositional phrases (p), expletive es (x), non-finite clauses (i), finite clauses (s-2 for verb second clauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and copula constructions (k)."
"For example, subcategorising a direct (accusative case) object and a non-finite clause would be represented by nai."
"We defined a total of 38 subcategorisation frame types, according to the verb subcategorisa-tion potential in the German grammar[REF_CITE], with few further restrictions on ar-gument combination."
We extracted verb-frame distributions from the trained lexicalised grammar.
Table 1 shows an example distribution for the verb glauben ‘to think/believe’ (for probability values 1%).
We also created a more delicate version of subcate-gorisation frames that discriminates between differ-ent kinds of pp-arguments.
"This was done by dis-tributing the frequency mass of prepositional phrase frame types (np, nap, ndp, npr, xp) over the prepo- sitional phrases, according to their frequencies in the corpus."
"Prepositional phrases are referred to by case and preposition, such as ‘Dat.mit’, ‘Akk.für’."
"The resulting lexical subcategorisation for reden and the frame type np whose total joint probability is 0.35820, is displayed in Table 2 (for probability val-ues 1%)."
The subcategorisation frame descriptions were for-mally evaluated by comparing the automatically generated verb frames against manual definitions in the German dictionary Duden – Das Stilwörterbuch[REF_CITE].
The F-score was 65.30% with and 72.05% without prepositional phrase in-formation: the automatically generated data is both easy to produce in large quantities and reliable enough to serve as proxy for human judgement (Schulte im[REF_CITE]).
"Semantic verb classes have been defined for sev-eral languages, with dominant examples concern-ing English[REF_CITE]and Spanish[REF_CITE]."
"The basic linguistic hypothesis underly-ing the construction of the semantic classes is that verbs in the same class share both meaning compo-nents and syntactic behaviour, since the meaning of a verb is supposed to influence its behaviour in the sentence, especially with regard to the choice of its arguments."
We hand-constructed a concise classification with 14 semantic verb classes for 57 German verbs before we initiated any clustering experiments.
"We have on hand a larger set of verbs and a more elaborate clas-sification, but choose to work on the smaller set for the moment, since an important component of our research program is an informative post-hoc analysis which becomes infeasible with larger datasets."
The semantic aspects and majority of verbs are closely related to Levin’s English classes.
They are consis-tent with the German verb classification[REF_CITE]as far as the relevant verbs appear in his less extensive semantic ‘fields’. 1.
"Aspect: anfangen, aufhören, beenden, begin-nen, enden 2."
"Propositional Attitude: ahnen, denken, glauben, vermuten, wissen 3. Transfer of Possession (Obtaining): bekom-men, erhalten, erlangen, kriegen 4. Transfer of Possession (Supply): bringen, liefern, schicken, vermitteln, zustellen 5."
"Manner of Motion: fahren, fliegen, rudern, segeln 6."
"Emotion: ärgern, freuen 7."
"Announcement: ankündigen, bekanntgeben, eröffnen, verkünden 8."
"Description: beschreiben, charakterisieren, darstellen, interpretieren 9."
"Insistence: beharren, bestehen, insistieren, pochen 10."
"Position: liegen, sitzen, stehen 11."
"Support: dienen, folgen, helfen, unterstützen 12."
"Opening: öffnen, schließen 13."
"Consumption: essen, konsumieren, lesen, saufen, trinken 14."
"Weather: blitzen, donnern, dämmern, nieseln, regnen, schneien"
"The class size is between 2 and 6, no verb ap-pears in more than one class."
"For some verbs this is something of an oversimplification; for example, the verb bestehen is assigned to verbs of insistence, but it also has a salient sense more related to existence."
"Similarly, schließen is recorded under open/close, in spite of the fact it also has a meaning related to infer-ence and the formation of conclusions."
"The classes include both high and low frequency verbs, because we wanted to make sure that our clustering technol-ogy was exercised in both data-rich and data-poor situations."
"The corpus frequencies range from 8 to 31,710."
"Our target classification is based on semantic in-tuitions, not on our knowledge of the syntactic be-haviour."
"As an extreme example, the semantic class Support contains the verb unterstützen, which syn-tactically requires a direct object, together with the three verbs dienen, folgen, helfen which dominantly subcategorise an indirect object."
In what follows we will show that the semantic classification is largely recoverable from the patterns of verb-frame occur-rence.
Clustering is a standard procedure in multivariate data analysis.
"It is designed to uncover an inher-ent natural structure of the data objects, and the equivalence classes induced by the clusters provide a means for generalising over these objects."
"In our case, clustering is realised on verbs: the data objects are represented by verbs, and the data features for describing the objects are realised by a probability distribution over syntactic verb frame descriptions."
"Clustering is applicable to a variety of areas in Natural Language Processing, e.g. by utilising class type descriptions such as in machine transla-ti[REF_CITE], word sense disambiguati[REF_CITE], and document classificati[REF_CITE], or by applying clusters for smoothing such as in machine translati[REF_CITE], or probabilistic grammars[REF_CITE]."
"We performed clustering by the k-Means algo-rithm as proposed[REF_CITE], which is an un-supervised hard clustering method assigning data objects to exactly clusters."
Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster (centroid) and re-calculating cluster centroids until no further changes take place.
One parameter of the clustering process is the distance measure used.
"Standard choices include the cosine, Euclidean distance, Manhattan metric, and variants of the Kullback-Leibler (KL) diver-gence."
"We concentrated on two variants of KL in Equation (1): information radius, cf."
"Equation (2), and skew divergence, recently shown as an effective measure for distributional similarity[REF_CITE], cf.    ! #&quot; $ Equation (3).  (1) &amp; )*(+ ,(*# ) (*+ % (2) %. /(:/ ;1&lt;! (3)"
"Measures (2) and (3) can tolerate zero values in the probability distribution, because they work with a weighted average of the two distributions compared / ."
"For the skew-divergence, we set the weight to 0.9, as was done by Lee."
"Furthermore, because the k-Means algorithm is sensitive to its starting clusters, we explored the op-tion of initialising the cluster centres based on other clustering algorithms."
"We performed agglomerative hierarchical clustering on the verbs which first as-signs each verb to its own cluster and then iteratively determines the two closest clusters and merges them, until the specified number of clusters is left."
"We tried several amalgamation methods: single-linkage, complete-linkage, average verb distance, distance between cluster centroids, and Ward’s method."
"The clustering was performed as follows: the 57 verbs were associated with probability distributions over frame types 1 (in condition 1 there were 38 frame types, while in the more delicate condition 2 there were 171, with a concomitant increase in data sparseness), and assigned to starting clusters (ran-domly or by hierarchical clustering)."
"The k-Means algorithm was then allowed to run for as many itera-tions as it takes to reach a fixed point, and the result-ing clusters were interpreted and evaluated against the manual classes."
"Related work on English verb classification or clustering utilised supervised learning by decision trees[REF_CITE], or a method re-lated to hierarchical clustering (Schulte im[REF_CITE])."
The task of evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires us to assess the similarity be-tween two sets of equivalence relations.
"As noted[REF_CITE], it is useful to have an evaluation measure that does not depend on the choice of sim-ilarity measure or on the original dimensionality of the input data, since that allows meaningful compar-ison of results for which these parameters vary."
"This is similar to the perspective[REF_CITE], who present, in the context of the MUC co-reference evaluation scheme, a model-theoretic measure of the similarity between equivalence classes."
Strehl et al. consider a clustering that partitions objects (  ) into clusters; the clusters  of   are the sets for which .
We call the cluster result and the desired gold-standard .
"For measuring the quality of an indi-vidual cluster, the cluster purity of each cluster is defined by its largest  , the number of mem-bers that are projected into the same class ."
"The measure is biased towards small clusters, with the extreme case of singleton clusters, which is an undesired property for our (linguistic) needs."
"To capture the quality of a whole clustering, Strehl et al. combine the mutual information be-tween and (based on the shared verb member-ship  ) with a scaling factor corresponding to the numbers of verbs in the respective clusters, and . #&quot; $ &amp; &quot; $! &quot;! # ! %# &amp; $ # &quot;! # 6    (4)"
"This manipulation is designed to remove the bias towards small clusters: [Footnote_2] using the 57 verbs from our study we generated 50 random clusters for each cluster size between [Footnote_1] and 57, and evaluated the re-sults against the gold standard, returning the best re-sult for each replication."
"2 In the absence of the penalty, mutual information would attain its maximum (which is the entropy of &apos; ) not only when A is correct but also when ( contains only singleton clusters."
"1 We also tried various transformations and variations of the probabilities, such as frequencies and binarisation, but none proved as effective as the probabilities."
We found that even using the scaling factor the measure favours smaller clus-ters.
"But this bias is strongest at the extremes of the range, and does not appear to impact too heavily on our results."
Unfortunately none of Strehl et al’s measures have all the properties which we intuitively require from a measure of linguistic cluster quality.
"For example, if we restrict attention to the case in which all verbs in an inferred cluster are drawn from the same actual class, we would like it to be the case that the evalua-tion measure is a monotonically increasing function of the size of the inferred cluster."
"We therefore intro-duced an additional, more suitable measure for the evaluation of individual clusters, based on the rep-resentation of equivalence classes as sets of pairs."
"It turns out that pairwise precision and recall have some of the counter-intuitive properties that we ob-jected to in Strehl et al’s measures, so we adjust pair-wise precision with a scaling factor based on the size of the hypothesised cluster. &amp; number of correct pairs in 56( (5) number of verbs in"
"We call this measure , for adjusted pairwise precision."
As with any other measure of individual cluster quality we can associate a quality value with a clustering which assigns each of the items to a cluster   by taking a weighted average over the qualities of the individual clusters. 6 &amp; (6) %
"Figures 1 and 2 summarise the two evaluation measures for overall cluster quality, showing the variation with the KL-based distance measures and with different strategies for seeding the initial cluster centres in the k-Means algorithm."
"Figure 1 displays quality scores referring to the coarse condition 1 subcategorisation frame types, Figure 2 refers to the clustering results obtained by verb descriptions based on the more delicate condition 2 subcategori-sation frame types including PP information."
"Base-line values are 0.017 (APP) and 0.229 (MI), calcu-lated as average on the evaluation of 10 random clus-ters."
"Optimum values, as calculated on the manual classification, are 0.291 (APP) and 0.493 (MI)."
"The evaluation function is extremely non-linear, which leads to a severe loss of quality with the first few clustering mistakes, but does not penalise later mis-takes to the same extent."
"From the methodological point of view, the clus-tering evaluation gave interesting insights into k- Means’ behaviour on the syntactic frame data."
"The more delicate verb-frame classification, i.e. the re-finement of the syntactic verb frame descriptions by prepositional phrase specification, improved the clustering results."
"This does not go without saying: there was potential for a sparse data problem, since even frequent verbs can only be expected to inhabit a few frames."
"For example, the verb anfangen with a corpus frequency of 2,554 has zero counts for 138 of the 171 frames."
Whether the improvement really matters in an application task is left to further re-search.
We found that randomised starting clusters usu-ally give better results than initialisation from a hi-erarchical clustering.
Hierarchies imposing a strong structure on the clustering (such as single-linkage: the output clusterings contain few very large and many singleton clusters) are hardly improved by k- Means.
Their evaluation results are noticeably be-low those for random clusters.
"But initialisation us-ing Ward’s method, which produces tighter clusters and a narrower range of cluster sizes does outper-form random cluster initialisation."
"Presumably the issue is that the other hierarchical clustering meth-ods place k-Means in a local minimum from which it cannot escape, and that uniformly shaped cluster initialisation gives k-Means a better chance of avoid-ing local minima, even with a high degree of pertur-bation."
"The clustering setup, proceeding and results provide a basis for a linguistic investigation concerning the German verbs, their syntactic properties and seman-tic classification."
"The following clustering result is an intuitively plausible semantic verb classification, accompanied by the cluster quality scores , and class labels illustrating the majority vote of the verbs in the clus-ter. [Footnote_3]"
"3 Verbs that are part of the majority are shown in bold face, others in plain text. Where there is no clear majority, both class labels are given."
"The cluster analysis was obtained by running k- Means on a random cluster initialisation, with infor-mation radius as distance measure; the verb descrip-tion contained condition 2 subcategorisation frame types with PP information. a) ahnen, vermuten, wissen (0.75) Propositional Attitude b) denken, glauben (0.33)"
"Propositional Attitude c) anfangen, aufhören, beginnen, beharren, en-den, insistieren, rudern (0.88)"
"Aspect d) liegen, sitzen, stehen (0.75)"
"Position e) dienen, folgen, helfen (0.75)"
"Support f) nieseln, regnen, schneien (0.75)"
Weather g) dämmern (0.00)
"Weather h) blitzen, donnern, segeln (0.25)"
"Weather i) bestehen, fahren, fliegen, pochen (0.4)"
"Insisting or Manner of Motion j) freuen, ärgern (0.33)"
"Emotion k) essen, konsumieren, saufen, trinken, verkün-den (1.00)"
"Consumption l) bringen, eröffnen, lesen, liefern, schicken, schließen, vermitteln, öffnen (0.78)"
"Supply m) ankündigen, beenden, bekanntgeben, bekom-men, beschreiben, charakterisieren, darstellen, erhalten, erlangen, interpretieren, kriegen, unterstützen (1.00) Description and Obtaining n) zustellen (0.00) Supply"
We compared the clustering to the gold standard and examined the underlying verb frame distribu-tions.
"We undertook a series of post-hoc cluster analyses to explore the influence of specific frames and frame groups on the formation of verb classes, such as: what is the difference in the clustering re-sult (on the same starting clusters) if we deleted all frame types containing an expletive es (frame types including x)?"
Space limitations allow us only a few insights.
Clusters (a) and (b) are pure sub-classes of the semantic verb class Propositional Attitude.
"The verbs agree in their syntactic subcategori-sation of a direct object (na) and finite clauses (ns-2, ns-dass); denken and glauben are assigned to a different cluster, because they also appear as intransitives, subcategorise the prepositional phrase Akk.an, and show espe-cially strong probabilities for ns-2."
Deleting na or frames containing s from the verb de-scription destroys the coherent clusters.
"Cluster (c) contains two sub-classes from As-pect and Insistence, polluted by the verb rud-ern ‘to row’."
"All Aspect verbs show a 50% preference for an intransitive usage, and a mi-nor 20% preference for the subcategorisation of non-finite clauses."
"By mistake, the infre-quent verb rudern (corpus frequency 49) shows a similar preference for ni in its frame distri-bution and therefore appears within the same cluster as the Aspect verbs."
The frame confu-sion has been caused by parsing mistakes for the infrequent verb; ni is not among the frames possibly subcategorised by rudern.
"Even though the verbs beharren and insistieren have characteristic frames np:Dat.auf and ns-2, they share an affinity for n with the as-pect verbs."
"When eliminating n from the fea-ture description of the verbs, the cluster is re-duced to those verbs using ni."
Cluster (d) is correct: Position.
"The syn-tactic usage of the three verbs is rather individual with strong probabilities for n, np:Dat.auf and np:Dat.in."
Even the elimination of any of the three frame features does not cause a separation of the verbs in the clustering.
"Cluster (j) represents the semantic class Emo-tion which, in German, has a highly charac-teristic signature in its strong association with reflexive frames; the cluster evaporates if we remove the distinctions made in the r feature group. zustellen in cluster (n) represents a singleton because of its extraordinarily strong preference ( 50%) for the ditransitive usage."
Eliminat-ing the frame from the verb description assigns zustellen to the same cluster as the other verbs of Transfer of Possession (Supply).
"Recall that we used two different sets of syntac-tic frames, the second of which makes more delicate distinctions in the area of prepositional phrases."
"As pointed out in Section 5, refining the syntactic verb information by PPs was helpful for the semantic clustering."
"But, contrary to our original intuitions, the detailed prepositional phrase information is less useful in the clustering of verbs with obligatory PP arguments than in the clustering of verbs where the PPs are optional; we performed a first test on the role of PP information: eliminating all PP informa-tion from the verb descriptions (not only the delicate PP information in condition 2, but also PP argument information in the coarse condition 1 frames) pro-duced obvious deficiencies in most of the semantic classes, among them Weather and Support, whose verbs do not require PPs as arguments."
"A second test confirmed the finding: we augmented our coarse-grained verb frame repertoire with a much reduced set of PPs, those commonly assumed as argument PPs."
This provides some but not all of the PP in-formation in condition 2.
"The clustering result is deficient mainly in its classification of the verbs of Propositional Attitude, Support, Opening, and few of these subcategorise for PPs."
"Clusters such as (k) to (l) suggest directions in which it might be desirable to subdivide the verb frames, for example by adding a limited amount of information about selectional preferences."
"Pre-vious work has shown that sparse data issues pre-clude across the board incorporation of selectional information (Schulte im[REF_CITE]), but a rough distinction such as physical object vs. abstraction on the direct object slot could, for example, help to split verkünden from the other verbs in cluster (k)."
The linguistic investigation gives some insight into the reasons for the success of our (rather sim-ple) clustering technique.
We successfully exploited the connection between the syntactic behaviour of a verb and its meaning components.
"The cluster-ing result shows a good match to the manually de-fined semantic verb classes, and in many cases it is clear which of and how the frames are influential in the creation of which clusters."
"We showed that we acquired implicit components of meaning through a syntactic extraction from a corpus, since the seman-tic verb classes are strongly related to the patterns in the syntactic descriptors."
Everything in this study suggests that the move to larger datasets is an appro-priate next move.
The paper presented the application of k-Means to the task of inducing semantic classes for German verbs.
"Based on purely syntactic probability distri-butions over verb subcategorisation frames, we ob-tained an intuitively plausible clustering of 57 verbs into 14 classes."
The automatic clustering was evalu-ated against hand-constructed semantic verb classes.
"A series of post-hoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes, and supported the tight connection between the syntactic behaviour of the verbs and their meaning components."
"Future work will concern the extension of the clustering experiments to a larger number of verbs, both for the scientific purpose of refining our un-derstanding of the semantic and syntactic status of verb classes and for the more applied goal of creat-ing a large, reliable and high quality lexical resource for German."
"For this task, we will need to further refine our verb classes, further develop the reper-toire of syntactic frames which we use, perhaps im-prove the statistical grammar from which the frames were extracted and find techniques which allow us to selectively include such information about selec-tional preferences as is warranted by the availabil-ity of training data and the capabilities of clustering technology."
Context is used in many NLP systems as an indicator of a term’s syntactic and se-mantic function.
The accuracy of the sys-tem is dependent on the quality and quan-tity of contextual information available to describe each term.
"However, the quan-tity variable is no longer fixed by lim-ited corpus resources."
"Given fixed train-ing time and computational resources, it makes sense for systems to invest time in extracting high quality contextual in-formation from a fixed corpus."
"However, with an effectively limitless quantity of text available, extraction rate and repre-sentation size need to be considered."
"We use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
Context plays an important role in many natural lan-guage tasks.
"For example, the accuracy of part of speech taggers or word sense disambiguation sys-tems depends on the quality and quantity of con-textual information these systems can extract from the training data."
"When predicting the sense of a word, for instance, the immediately preceding word is likely to be more important than the tenth previ-ous word; similar observations can be made about POS taggers or chunkers."
"A crucial part of train-ing these systems lies in extracting from the data high-quality contextual information, in the sense of defining contexts that are both accurate and corre-lated with the information (the POS tags, the word senses, the chunks) the system is trying to extract."
"The quality of contextual information is often de-termined by the size of the training corpus: with less data available, extracting context information for any given phenomenon becomes less reliable."
"However, corpus size is no longer a limiting fac-tor: whereas up to now people have typically worked with corpora of around one million words, it has be-come feasible to build much larger document collec-tions; for example,[REF_CITE]report on experiments with a one billion word corpus."
"When using a much larger corpus and scaling the context space, there are, however, other trade-offs to take into consideration: the size of the corpus may make it unfeasible to train some systems because of efficiency issues or hardware costs; it may also result in an unmanageable expansion of the extracted con-text information, reducing the performance of the systems that have to make use of this information."
"This paper reports on experiments that try to es-tablish some of the trade-offs between corpus size, processing time, hardware costs and the perfor-mance of the resulting systems."
We report on ex-periments with a large corpus (around 300 mil-lion words).
"We trained a thesaurus extraction sys-tem with a range of context-extracting front-ends to demonstrate the interaction between context quality, extraction time and representation size."
Thesauri have traditionally been used in informa-tion retrieval tasks to expand words in queries with synonymous terms (e.g.[REF_CITE]).
"More re- cently, semantic resources have also been used in collocation discovery[REF_CITE], smoothing and model estimati[REF_CITE]and text classificati[REF_CITE]."
"Unfortunately, thesauri are very ex-pensive and time-consuming to produce manually, and tend to suffer from problems of bias, inconsis-tency, and lack of coverage."
"In addition, thesaurus compilers cannot keep up with constantly evolving language use and cannot afford to build new thesauri for the many subdomains that information extraction and retrieval systems are being developed for."
There is a clear need for methods to extract thesauri auto-matically or tools that assist in the manual creation and updating of these semantic resources.
Most existing work on thesaurus extraction and word clustering is based on the general observation that related terms will appear in similar contexts.
The differences tend to lie in the way “context” is defined and in the way similarity is calculated.
"Most systems extract co-occurrence and syntactic infor-mation from the words surrounding the target term, which is then converted into a vector-space repre-sentation of the contexts that each target term ap-pears[REF_CITE]."
Other systems take the whole document as the context and consider term co-occurrence at the document level[REF_CITE].
"Once these contexts have been defined, these systems then use clustering or nearest neighbour methods to find similar terms."
"Finally, some systems extract synonyms directly without extracting and comparing contextual rep-resentations for each term."
"Instead, these systems recognise terms within certain linguistic patterns (e.g. X, Y and other Zs) which associate synonyms and hyponyms[REF_CITE]."
Thesaurus extraction is a good task to use to ex-periment with scaling context spaces.
"The vector-space model with nearest neighbour searching is simple, so we needn’t worry about interactions be-tween the contexts we select and a learning algo-rithm (such as independence of the features)."
"But also, thesaurus extraction is a task where success has been limited when using small corpora[REF_CITE]; corpora of the order of 300 million words have already been shown to be more success-ful at this task[REF_CITE]."
Vector-space thesaurus extraction can be separated into two independent processes.
The first step ex-tracts the contexts from raw text and compiles them into a vector-space statistical description of the con-texts each potential thesaurus term appears in.
"We define a context relation as a tuple (w, r, w 0 ) where w is a thesaurus term, occurring in relation type r, with another word w 0 in the sentence."
"The type can be grammatical or the position of w 0 in a context window: the relation (dog, direct-obj, walk) indicates that the term dog , was the direct ob-ject of the verb walk ."
"Often we treat the tuple (r, w 0 ) as a single unit and refer to it as an attribute of w. The context extraction systems used for these exper-iments are described in the following section."
The second step in thesaurus extraction performs clustering or nearest-neighbour analysis to deter-mine which terms are similar based on their context vectors.
"Our second component is similar to Grefen-stette’s S EXTANT system, which performs nearest-neighbour calculations for each pair of potential the-saurus terms."
For nearest-neighbour measurements we must define a function to judge the similarity be-tween two context vectors (e.g. the cosine measure) and a function to combine the raw instance frequen-cies for each context relation into weighted vector components.
S EXTANT uses a generalisation of the Jaccard measure to measure similarity.
The Jaccard measure is the cardinality ratio of the intersection and union of attribute sets (atts(w n ) is the attribute set for w n ): | atts(w m ) ∩ atts(w n )| (1) | atts(w m ) ∪ atts(w n )|
"The generalised Jaccard measure allows each rela-tion to have a significance weight (based on word, attribute and relation frequencies) associated with it:"
"P a∈atts(w m )∪atts(w n ) min(wgt(w m , a), wgt(w n , a)) (2) P a∈atts(w m )∪atts(w n ) max(wgt(w m , a), wgt(w n , a))"
"Grefenstette originally used the weighting function: wgt(w i , a j ) = log 2 ( f (w i , a j ) + 1) (3) log 2 (n(a j ) + 1) where f(w i , a j ) is the frequency of the relation and n(a j ) is the number of different words a j appears in relations with."
"However, we have found that using the t-test be-tween the joint and independent distributions of a word and its attribute: wgt(w i , a j ) = p(w i , a j ) − p(w i ) p(a j ) (4) pp(w i ) p(a j ) gives superior performance[REF_CITE]and is therefore used for our experiments."
We have experimented with a number of different systems for extracting the contexts for each word.
"These systems show a wide range in complexity of method and implementation, and hence develop-ment effort and execution time."
The simplest method we implemented extracts the occurrence counts of words within a particular win-dow surrounding the thesaurus term.
These window extractors are very easy to implement and run very quickly.
The window geometries used in this experi-ment are listed in Table 1.
"Extractors marked with an asterisk, for example W(L 1 R 1 ∗), do not distinguish (within the relation type) between different positions of the word w 0 in the window."
At a greater level of complexity we have two shal-low NLP systems which provide extra syntactic in-formation in the extracted contexts.
The first sys-tem is based on the syntactic relation extractor from S EXTANT with a different POS tagger and chunker.
The S EXTANT -based extractor we developed uses a very simple Naı̈ve Bayes POS tagger and chunker.
This is very simple to implement and is extremely fast since it optimises the tag selection locally at the current word rather than performing beam or Viterbi search over the entire sentence.
"After the raw text has been POS tagged and chunked, the S EXTANT re-lation extraction algorithm is run over the text."
This consists of five passes over each sentence that asso-ciate each noun with the modifiers and verbs from the syntactic contexts that it appears in.
"The second shallow parsing extractor we used was the C ASS parser[REF_CITE], which uses cas-caded finite state transducers to produce a limited depth parse of POS tagged text."
We used the out-put of the Naı̈ve Bayes POS tagger output as input to the C ASS .
The context relations used were ex-tracted directly by the tuples program (using e8 demo grammar) included in the C ASS distribution.
The FST parsing algorithm is very efficient and so C ASS also ran very quickly.
The times reported be-low include the Naı̈ve Bayes POS tagging time.
"The final, most sophisticated extractor used was the M INIPAR parser[REF_CITE], which is a broad-coverage principle-based parser."
The context rela-tions used were extracted directly from the full parse tree.
"Although fast for a full parser, M INIPAR was no match for the simpler extractors."
For this experiment we needed a large quantity of text which we could group into a range of corpus sizes.
We combined the BNC and Reuters corpus to produce a 300 million word corpus.
The respective sizes of each are shown in Table 2.
The sentences were randomly shuffled together to produce a sin-gle homogeneous corpus.
This corpus was split into two 150M word corpora over which the main experi-mental results are averaged.
We then created smaller corpora of size 21 down to 641 th of each 150M corpus.
The next section describes the method of evaluating each thesaurus created by the combination of a given context extraction system and corpus size.
"For the purposes of evaluation, we selected 70 single word noun terms for thesaurus extraction."
"To avoid sample bias, the words were randomly selected from Wordnet such that they covered a range of values for the following word properties: occurrence frequency based on frequency counts from the Penn Treebank, BNC and Reuters; number of senses based on the number of Wordnet synsets and Macquarie Thesaurus entries; generality/specificity based on depth of the term in the Wordnet hierarchy; abstractness/concreteness based on even distribu-tion across all Wordnet subtrees."
Table 3 shows some of the selected terms with fre-quency and synonym set data.
For each term we extracted a thesaurus entry with 200 potential syn-onyms and their weighted Jaccard scores.
The most difficult aspect of thesaurus extraction is evaluating the quality of the result.
The sim-plest method of evaluation is direct comparison of the extracted thesaurus with a manually created gold standard[REF_CITE].
However on smaller corpora direct matching alone is often too coarse-grained and thesaurus coverage is a problem.
"Our experiments use a combination of three the-sauri available in electronic form: The Macquarie Thesaurus[REF_CITE], Roget’s Thesaurus[REF_CITE], and the Moby Thesaurus[REF_CITE]."
Each thesaurus is structured differently: Roget’s and Macquarie are topic ordered and the Moby thesaurus is head term ordered.
"Roget’s is quite dated and has low coverage, and contains a deep hierarchy (depth up to seven) with terms grouped in 8696 small syn-onym sets at the leaves of the hierarchy."
"The Mac-quarie consists of 812 large topics (often in antonym related pairs), each of which is separated into 21174 small synonym sets."
Roget’s and the Macquarie provide sense distinctions by placing terms in mul-tiple synonym sets.
The Moby thesaurus consists of 30259 head terms and large synonym lists which conflate all the head term senses.
The extracted the-saurus does not distinguish between different head senses.
"Therefore, we convert the Roget’s and Mac-quarie thesaurus into head term ordered format by combining each small sense set that the head term appears in."
"We create a gold standard thesaurus containing the union of the synonym lists from each thesaurus, giving a total of 23207 synonyms for the 70 terms."
"With these gold standard resources in place, it is possible to use precision and recall measures to cal-culate the performance of the thesaurus extraction systems."
To help overcome the problems of coarse-grained direct comparisons we use three different types of measure to evaluate thesaurus quality: 1. Direct Match (D IRECT ) 2. Precision of the n top ranked synonyms (P(n)) 3.
Inverse Rank (I NV R)
A match is an extracted synonym that appears in the corresponding gold standard synonym list.
The direct match score is the number of such matches for each term.
Precision of the top n is the percentage of matches in the top n extracted synonyms.
"In these experiments, we calculate this for n = 1, 5, and 10."
The inverse rank score is the sum of the inverse rank of each match.
"For example, if matching synonyms appear in the extracted synonym list at ranks 3, 5 and 28, then the inverse rank score is 13 + 15 + 281 = 0.569."
The maximum inverse rank score is 5.878 for a synonym list of 200 terms.
Inverse rank is a good measure of subtle differences in ranked results.
Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.
Since M INIPAR performs morphological analysis on the context relations we have added an existing mor-phological analyser[REF_CITE]to the other extractors.
Table 4 shows the improvement gained by morphological analysis of the attributes and relations for the S[REF_CITE]M corpus.
"The improvement in results is quite significant, as is the reduction in the representation space and num-ber of unique context relations."
"The reduction in the number of terms is a result of coalescing the plu-ral nouns with their corresponding singular nouns, which also reduces data sparseness problems."
The remainder of the results use morphological analysis of both the words and attributes.
Table 5 summarises the average results of ap-plying all of the extraction systems to the two 150M word corpora.
The first thing to note is the time spent extracting contextual information: M INIPAR takes significantly longer to run than the other extractors.
"Secondly, S EXTANT and M INI - PAR have quite similar results overall, but M INIPAR is slightly better across most measures."
"However, S EXTANT runs about 28 times faster than M INI - PAR ."
"Also, M INIPAR extracts many more terms and relations with a much larger representation than S EXTANT ."
"This is partly because M INIPAR ex-tracts more types of relations from the parse tree than S EXTANT , and partly because it extracts ex-tra multi-word terms."
"Amongst the simpler meth-ods, W(L 1 R 1 ) and W(L 1,2 ) give reasonable results."
"The larger windows with low correlation between the thesaurus term and context, extract a massive context representation but the results are about 10% worse than the syntactic extractors."
Overall the precision and recall are relatively poor.
Poor recall is partly due to the gold stan-dard containing some plurals and multi-word terms which account for about 25% of the synonyms.
These have been retained because the M INIPAR and C ASS systems are capable of identifying (at least some) multi-word terms.
"Given a fixed time period (of more than the four days M INIPAR takes) and a fixed 150M corpus we would probably still choose to use M INIPAR unless the representation was too big for our learning algo-rithm, since the thesaurus quality is slightly better."
Table 6 shows what happens to thesaurus quality as we decrease the size of the corpus to 641 th of its original size (2.3M words) for S EXTANT .
Halving the corpus results in a significant reduction for most of the measures.
All five evaluation measures show the same log-linear dependence on the size of the corpus.
Figure 1 shows the same trend for Inverse Rank evaluation of the M INIPAR thesaurus with a log-linear fitting the data points.
"We can use the same curve fitting to estimate the- 



"
Figure 2 is a graph for making engineering deci-sions in conjunction with the data in Table 5.
"For instance, if we fix the total time and computational resources at an arbitrary point, e.g. the point where M INIPAR can process 75M words, we get a best direct match score of 23.5."
"However, we can get the same resultant accuracy by using S EXTANT on a corpus of 116M words or W(L 1 R 1 ) on a corpus of 240M words."
"From Figure 5, extracting contexts from corpora of these sizes would take M[REF_CITE]hours, S EXTANT 2 hours and W(L 1 R 1 ) 12 minutes."
Interpolation on Figure 3 predicts that the extraction would result in 10M unique relations from M INI - PAR and S[REF_CITE]M from W(L 1 R 1 ).
Fig-ure 4 indicates that extraction would result in 550k
M[REF_CITE]S EXTANT terms and 600k W(L 1 R 1 ) terms.
"Given these values and the fact that the time com-plexity of most thesaurus extraction algorithms is at least linear in the number of unique relations and squared in the number of thesaurus terms, it seems S EXTANT may represent the best solution."
"With these size issues in mind, we finally consider some methods to limit the size of the context rep-resentation."
Table 7 shows the results of perform-ing various kinds of filtering on the representation size.
"The FIXED and LEXICON filters run over the full 300M word corpus, but have size limits based on the 150M word corpus."
The FIXED filter does not allow any object/attribute pairs to be added that were not extracted from the 150M word corpus.
The LEX - ICON filter does not allow any objects to be added that were not extracted from the 150M word cor-pus.
The &gt; 1 and &gt; 2 filters prune relations with a frequency of less than or equal to one or two.
The FIXED and LEXICON filters show that counting over larger corpora does produce marginally better re-sults.
"The &gt; 1 and &gt; 2 filters show that the many relations that occur infrequently do not contribute significantly to the vector comparisons and hence don’t impact on the final results, even though they dramatically increase the representation size."
It is a phenomenon common to many NLP tasks that the quality or accuracy of a system increases log-linearly with the size of the corpus.
They demonstrated behaviour of differ-ent learning algorithms with very simple contexts on extremely large corpora.
We have demonstrated the behaviour of a simple learning algorithm on much more complicated contextual information on very large corpora.
Our experiments suggest that the existing method-ology of evaluating systems on small corpora with-out reference to the execution time and representa-tion size ignores important aspects of the evaluation of NLP tools.
These experiments show that efficiently imple-menting and optimising the NLP tools used for con-text extraction is of crucial importance since the in-creased corpus sizes make execution speed an im-portant evaluation factor when deciding between different learning algorithms for different tasks and corpora.
These results also motivate further re-search into improving the asymptotic complexity of the learning algorithms used in NLP systems.
"In the new paradigm, it could well be that far simpler but scalable learning algorithms significantly out-perform existing systems."
"Finally, the mass availability of online text re-sources should be taken on board."
It is important that language engineers and computational linguists continue to try and find new unsupervised or (as Banko and Brill suggest) semi-supervised methods for tasks which currently rely on annotated data.
"It is also important to consider how information ex-tracted by systems such as thesaurus extraction sys-tems can be incorporated into tasks which use pre-dominantly supervised techniques, e.g. in the form of class information for smoothing."
We would like to extend this analysis to at least one billion words for at least the most successful methods and try other tools and parsers for extract-ing the contextual information.
"However, to do this we must look at methods of compressing the vector-space model and approximating the full pair-wise comparison of thesaurus terms."
We would also like to investigate how this thesaurus information can be used to improve the accuracy or generality of other NLP tasks.
"Broad-coverage corpora annotated with semantic role, or argument structure, in-formation are becoming available for the rst time."
Statistical systems have been trained to automatically label seman-tic roles from the output of statistical parsers on unannotated text.
"In this pa-per, we quantify the e ect of parser accu-racy on these systems&apos; performance, and examine the question of whether a at-ter \chunked&quot; representation of the in-put can be as e ective for the purposes of semantic role identi cation."
"Over the past decade, most work in the eld of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quan-ti cation, anaphora, aspect and modality (e.g.[REF_CITE]), to simpler nite-state or sta-tistical systems such[REF_CITE]and[REF_CITE]."
Much of the evaluation of these systems has been conducted on extracting relations for speci c semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understand-ing Conferences.
"Recently, attention has turned to creating cor-pora annotated for argument structure for a broader range of predicates."
The Propbank project at the University of Pennsylvania[REF_CITE]and the FrameNet project at the International Computer Science Institute[REF_CITE]share the goal of document-ing the syntactic realization of arguments of the predicates of the general English lexicon by an-notating a corpus with semantic roles.
"Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary."
John will meet Mary.
John and Mary will meet. (2) The door opened.
Mary opened the door.
"Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpret-ing text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine trans-lation or automatic summarization."
"In this pa-per, we examine how the information provided by modern statistical parsers such[REF_CITE]and[REF_CITE]contributes to solving this problem."
"We measure the e ect of parser accu-racy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction."
"The system rst passed sentences through an au-tomatic parser, extracted syntactic features from the parses, and estimated probabilities for seman-tic roles from the syntactic and lexical features."
"Both training and test sentences were automat-ically parsed, as no hand-annotated parse trees were available for the corpus."
"While the errors introduced by the parser no doubt negatively af-fected the results obtained, there was no direct way of quantifying this e ect."
"Of the systems evaluated for the Message Understanding Confer-ence task,[REF_CITE]made use of an inte-grated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse."
"As in the FrameNet case, the parser was not trained on the corpus for which semantic an-notations were available, and the e ect of better, or even perfect, parses could not be measured."
"One of the di erences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire dataset."
"In this paper, we com-pare the performance of a system based on gold-standard parses with one using automatically gen-erated parser output."
"We also examine whether it is possible that the additional information con-tained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a at or \chunked&quot; representation of the input."
"The results in this paper are primarily derived from the Propbank corpus, and will be compared to earlier results from the FrameNet corpus."
"Be-fore proceeding to the experiments, this section will brie y describe the similarities and di erences between the two sets of data."
"While the goals of the two projects are similar in many respects, their methodologies are quite dif-ferent."
"FrameNet is focused on semantic frames, which are de ned as schematic representation of situations involving various participants, props, and other conceptual roles[REF_CITE]."
"The project methodology has proceeded on a frame-by-frame basis, that is by rst choosing a semantic frame, de ning the frame and its participants or frame elements , and listing the various lexical predicates which invoke the frame, and then nd-ing example sentences of each predicate in the cor-pus (the British National Corpus was used) and annotating each frame element."
"The example sen-tences were chosen primarily for coverage of all the syntactic realizations of the frame elements, and simple examples of these realizations were preferred over those involving complex syntactic structure not immediate relevant to the lexical predicate itself."
"From the perspective of an auto-matic classi cation system, the overrepresentation of rare syntactic realizations may cause the system to perform more poorly than it might on more sta-tistically representative data."
"On the other hand, the exclusion of complex examples may make the task arti cially easy."
Only sentences where the lexical predicate was used \in frame&quot; were anno-tated.
"A word with multiple distinct senses would generally be analyzed as belonging to di erent frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been de ned."
It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure.
"A more complete description of the FrameNet project can be found[REF_CITE], and the rami-cations for automatic classi cation are discussed more thoroughly[REF_CITE]."
The philosophy of the Propbank project can be likened to FrameNet without frames.
"While the semantic roles of FrameNet are de ned at the level of the frame, in Propbank, roles are de ned on a per-predicate basis."
"The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as \temporal&quot; or \locative&quot;."
"While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in de ning roles, and no claims are intended as to optionality or other traditional argument/adjunct tests."
"To date, Propbank has addressed only verbs, where FrameNet includes nouns and ad-jectives."
"Propbank&apos;s annotation process has pro-ceeded from the most to least common verbs, and all examples of each verb from the corpus are an-notated."
"Thus, the data for each predicate are statistically representative of the corpus, as are the frequencies of the predicates themselves."
"An-notation takes place with reference to the Penn Treebank trees | not only are annotators shown the trees when analyzing a sentence, they are con-strained to assign the semantic labels to portions of the sentence corresponding to nodes in the tree."
"Propbank annotators tag all examples of a given verb, regardless of word sense."
"The tagging guide-lines for a verb may contain many \rolesets&quot;, cor-responding to word sense at a relatively coarse-grained level."
"The need for multiple rolesets is determined by the roles themselves, that is, uses of the verb with di erent arguments are given sep-arate rolesets."
"However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used."
Sense tagging is planned for a second pass through the data.
In many cases the roleset can be deter-mined from the argument annotations themselves.
"However, we did not make any attempt to distin-guish sense in our experiments, and simply at-tempted to predict argument labels based on the identity of the lexical predicate."
"In previous work using the FrameNet corpus,[REF_CITE]developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser[REF_CITE]."
We will brie y review their probability model before adapting the system to handle unparsed data.
"Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: 



"
Parse Tree Path: This feature is designed to capture the syntactic relation of a constituent to the predicate.
"It is de ned as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Fig-ure 2."
"Although the path is composed as a string of symbols, our systems will treat the string as an atomic value."
"The path includes, as the rst element of the string, the part of speech of the predicate, and, as the last ele-ment, the phrase type or syntactic category of the sentence constituent marked as an ar-gument."
Position: This feature simply indicates whether the constituent to be labeled occurs before or after the predicate de ning the semantic frame.
"This feature is highly correlated with grammatical function, since subjects will gen-erally appear before a verb, and objects after."
"This feature may overcome the shortcom-ings of reading grammatical function from the parse tree, as well as errors in the parser out-put. ment in the parse tree and # downward movement."
"The distinction between active and pas-sive verbs plays an important role in the con-nection between semantic role and grammat-ical function, since direct objects of active verbs correspond to subjects of passive verbs."
"From the parser output, verbs were classi ed as active or passive by building a set of 10 passive-identifying patterns."
Each of the pat-terns requires both a passive auxiliary (some form of \to be&quot; or \to get&quot;) and a past par-ticiple.
"Head Word: Lexical dependencies provide im-portant information in labeling semantic roles, as one might expect from their use in statistical models for parsing."
"Since the parser used assigns each constituent a head word as an integral part of the parsing model, the head words of the constituents can be read from the parser output."
"For example, in a communication frame, noun phrases headed by \Bill&quot;, \brother&quot;, or \he&quot; are more likely to be the Speaker , while those headed by \proposal&quot;, \story&quot;, or \question&quot; are more likely to be the Topic ."
"To predict argument roles in new data, we wish to estimate the probability of each role given these ve features and the predicate p : P ( r j pt; path; position; voice; hw; p )."
"Due to the sparsity of the data, it is not possible to estimate this probability from the counts in the training."
"Instead, we estimate probabilities from various subsets of the features, and interpolate a linear combination of the resulting distributions."
"The interpolation is performed over the most speci c distributions for which data are available, which can be thought of as choosing the topmost distri-butions available from a backo lattice, shown in Figure 3."
"We applied the same system, using the same features to a preliminary release of the Propbank data."
"The dataset used contained annotations for 26,138 predicate-argument structures containing 65,364 individual arguments and containing exam-ples from 1,527 lexical predicates (types)."
"In order to provide results comparable with the statistical parsing literature, annotations[REF_CITE]of the Treebank were used as the test set; all other sections were included in the training set."
"The system was tested under two conditions, one in which it is given the constituents which are arguments to the predicate and merely has to predict the correct role, and one in which it has to both nd the arguments in the sentence and label them correctly."
Results are shown in Tables 1 and 2.
"Although results for Propbank are lower than for FrameNet, this appears to be primarily due to the smaller number of training examples for each predicate, rather than the di erence in annotation style between the two corpora."
"The FrameNet data contained at least ten examples from each predicate, while 17% of the Propbank data had fewer than ten training examples."
Removing these examples from the test set gives 84.1% accuracy with gold-standard parses and 80.5% accuracy with automatic parses.
"As our path feature is a somewhat unusual way of looking at parse trees, its behavior in the sys-tem warrants a closer look."
The path feature is most useful as a way of nding arguments in the unknown boundary condition.
"Removing the path feature from the known-boundary system results in only a small degradation in performance, from 82.3% to 81.7%."
One reason for the relatively small impact may be sparseness of the feature | 7% of paths in the test set are unseen in training data.
"The most common values of the feature are shown in Table 3, where the rst two rows cor-respond to standard subject and object positions."
"One reason for sparsity is seen in the third row: in the Treebank, the adjunction of an adverbial phrase or modal verb can cause an additional VP node to appear in our path feature."
We tried two variations of the path feature to address this prob-lem.
"The rst collapses sequences of nodes with the same label, for example combining rows 2 and 3 of Table 3."
"The second variation uses only two values for the feature: NP under S (subject posi-tion), and NP under VP (object position)."
Nei-ther variation improved performance in the known boundary condition.
"As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system."
Many recent information extraction systems for limited domains have relied on nite-state systems that do not build a full parse tree for the sentence being analyzed.
"Among such systems,[REF_CITE]built nite-state recognizers for vari-ous entities, which were then cascaded to form recognizers for higher-level relations, while[REF_CITE]used low-level \chunks&quot; from a general-purpose syntactic analyzer as observa- tions in a trained Hidden Markov Model."
"Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided."
It is also possible that this approach may be more robust to error than parsers.
"Al-though we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an argument of a particular predicate, and what its relation to the predicate is, those decisions may be so frequently incorrect that a much simpler system can do just as well."
"In this section we test this hypothesis by comparing a system which is given only a at, \chunked&quot; representation of the input sentence to the parse-tree-based systems described above."
"In this representation, base-level constituent bound-aries and labels are present, but there are no de-pendencies between constituents, as shown by the following sample sentence: (3) [ NP Big investment banks] [ VP refused to step] [ ADVP up] [ PP to] [ NP the plate] [ V P to support] [ NP the beleaguered oor traders] [ PP by] [ VP buying] [ NP big blocks] [ PP of] [ NP stock] , [ NP traders] [ VP say] ."
Our chunks were derived from the Tree-bank trees using the conversion described by Tjong[REF_CITE].
"Thus, the experiments were carried out using \gold-standard&quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunk-based system."
"The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate con-stituent."
"We will examine the contribution of each of these information sources, beginning with the problem of assigning the correct role in the case where the boundaries of the arguments in the sen-tence are known, and then turning to the problem of nding arguments in the sentence."
"When the argument boundaries are known, the grammatical relationship of the the constituent to the predicate turns out to be of little value."
"Removing the path feature from the system de-scribed above results in only a small degradation in performance, from 82.3% to 81.7%."
"While the path feature serves to distinguish subjects from objects, the combination of the constituent po-sition before or after the predicate and the ac-tive/passive voice feature serves the same purpose."
"However, this result still makes use of the parser output for nding the constituent&apos;s head word."
"We implemented a simple algorithm to guess the argument&apos;s head word from the chunked output: if the argument begins at a chunk boundary, taking the last word of the chunk, and in all other cases, taking the rst word of the argument."
"This heuris-tic matches the head word read from the parse tree 77% of the the time, as it correctly identi es the nal word of simple noun phrases as the head, the preposition as the head of prepositional phrases, and the complementizer as the head of sentential complements."
"Using this process for determining head words, the system drops to 77.0% accuracy, indicating that identifying the relevant head word from semantic role prediction is in itself an impor-tant function of the parser."
This chunker-based re-sult is only slightly lower than the 79.2% obtained using automatic parses in the known boundary condition.
These results for the known boundary condition are summarized in Table 4.
Path Head Accuracy gold parse gold parse 82.3 auto parse auto parse 79.2 not used gold parse 81.7 not used chunks 77.0
We might expect the information provided by the parser to be more important in identifying the arguments in the sentence than in assigning them the correct role.
"While it is easy to guess whether a noun phrase is a subject or object given only its position relative to the predicate, identifying complex noun phrases and determining whether they are arguments of a verb may be more diÆcult without the attachment information provided by the parser."
"To test this, we implemented a system in which the argument labels were assigned to chunks, with the path feature used by the parse-tree-based sys-tem replaced by a number expressing the distance in chunks to the left or right of the predicate."
"In par-ticular, sentential complements (which comprise 11% of the data) and prepositional phrases (which comprise 10%) always correspond to more than one chunk, and therefore cannot be correctly la-beled by our system which assigns roles to single chunks."
"In fact, this system achieves 27.6% preci-sion and 22.0% recall."
"In order to see how much of the performance degradation is caused by the diÆculty of nding exact argument boundaries in the chunked rep-resentation, we can relax the scoring criteria to count as correct all cases where the system cor-rectly identi es the rst chunk belonging to an argument."
"For example, if the system assigns the correct label to the preposition beginning a prepo-sitional phrase, the argument will be counted as correct, even though the system does not nd the argument&apos;s righthand boundary."
"With this scoring regime, the chunk-based system performs at 49.5% precision and 35.1% recall, still signi -cantly lower than the 57.7% precision/50.0% recall for exact matches using automatically generated parses."
Results for the unknown boundary condi-tion are summarized in Table 5.
"Precision Recall gold parse 71.1 64.4 auto parse 57.7 50.0 chunk 27.6 22.0 chunk, relaxed scoring 49.5 35.1"
"As an example for comparing the behavior of the tree-based and chunk-based systems, consider the following sentence, with human annotations showing the arguments of the predicate support : (4) [ ARG 0"
"Big investment banks] refused to step up to the plate to support [ ARG 1 the beleaguered oor traders] [ MNR by buying big blocks of stock] , traders say ."
"Our tree-based system assigned the following anal-ysis: (5) Big investment banks refused to step up to the plate to support [ ARG 1 the beleaguered oor traders] [ MNR by buying big blocks of stock] , traders say ."
"In this case, the system failed to nd the predi-cate&apos;s ARG0 relation, because it is syntactically distant from the verb support ."
"The original Tree-bank syntactic tree contains a trace which would allow one to recover this relation, co-indexing the empty subject position of support with the noun phrase \Big investment banks&quot;."
"However, our automatic parser output does not include such traces, nor does our system make use of them."
"The chunk-based system assigns the following ar-gument labels: (6) Big investment banks refused to step up to [ ARG 0 the plate] to support [ ARG 1 the beleaguered oor traders] by buying big blocks of stock , traders say ."
"Here, as before, the true ARG0 relation is not found, and it would be diÆcult to imagine iden-tifying it without building a complete syntactic parse of the sentence."
"But now, unlike in the tree-based output, the ARG0 label is mistakenly attached to a noun phrase immediately before the predicate."
The ARG1 relation in direct object po-sition is fairly easily identi able in the chunked representation as a noun phrase directly follow-ing the verb.
"The prepositional phrase expressing the Manner relation, however, is not identi ed by the chunk-based system."
"The tree-based system&apos;s path feature for this constituent is VB &quot; VP # PP, which identi es the prepositional phrase as at-taching to the verb, and increases its probability of being assigned an argument label."
The chunk-based system sees this as a prepositional phrase appearing as the second chunk after the predi-cate.
"Although this may be a typical position for the Manner relation, the fact that the preposition attaches to the predicate rather than to its direct object is not represented."
"In interpreting these results, it is important to keep in mind the di erences between this task and other information extraction datasets."
"In comparison to the domain-speci c relations eval-uated by the Message Understanding Conference (MUC) tasks, we have a wider variety of relations but fewer training instances for each."
The rela-tions may themselves be less susceptible to nite state methods.
"For example, a named-entity sys-tem which indenti es corporation names can go a long way towards nding the \employment&quot; re-lation of MUC, and similarly systems for tagging genes and proteins help a great deal for relations in the biomedical domain."
"Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing deci-sions more important in nding argument bound-aries."
"They also involve abstract relations, with a wide variety of possible llers for each role."
"Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk&apos;s other words or the inter-vening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such[REF_CITE],[REF_CITE]and[REF_CITE]."
"While a more elaborate nite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate."
"By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a at representation of the data."
"We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation."
"Not only the constituent structure but also head word information, pro-duced as a side product, are important features."
"Parsers, however, still have a long way to go."
Our results using hand-annotated parse trees show that improvements in parsing should translate di-rectly into better semantic interpretations.
"Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylva-nia and from the Propbank project, DoD[REF_CITE]-00C-2136."
"In many types of technical texts, meaning is embedded in noun compounds."
A language un-derstanding program needs to be able to inter-pret these in order to ascertain sentence mean-ing.
"We explore the possibility of using an ex-isting lexical hierarchy for the purpose of plac-ing words from a noun compound into cate-gories, and then using this category member-ship to determine the relation that holds be-tween the nouns."
"In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical do-main, obtaining classification accuracy of ap-proximately 90%."
"Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hi-erarchy must the algorithm descend before all the terms within the subhierarchy behave uni-formly with respect to the semantic relation in question?"
"We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning rela-tions to noun compounds."
A major difficulty for the interpretation of sentences from technical texts is the complex structure of noun phrases and noun compounds.
"Consider, for example, this title, taken from a biomedical journal abstract:"
Open-labeled long-term study of the subcutaneous sumatriptan efficacy and tolerability in acute mi-graine treatment.
"An important step towards being able to interpret such technical sentences is to analyze the meaning of noun compounds, and noun phrases more generally."
Interpretation of noun compounds (NCs) is highly de-pendent on lexical information.
"Thus we explore the use of a large corpus (Medline) and a large lexical hierarchy (MeSH, Medical Subject Headings) to determine the re-lations that hold between the words in noun compounds."
"Surprisingly, we find that we can simply use the juxta-position of category membership within the lexical hier-archy to determine the relation that holds between pairs of nouns."
"For example, for the NCs leg paresis, skin numbness, and hip pain, the first word of the NC falls into the[REF_CITE](Body Regions) category, and the second word falls into the C10 (Nervous System Diseases) cat-egory."
From these we can declare that the relation that holds between the words is “located in”.
"Similarly, for influenza patients and aids survivors, the first word falls under C02 (Virus Diseases) and the second is found in M01.643 (Patients), yielding the “afflicted by” relation."
"Using this technique on a subpart of the category space, we obtain 90% accuracy overall."
"In some sense, this is a very old idea, dating back to the early days of semantic nets and semantic grammars."
"The critical difference now is that large lexical resources and corpora have become available, thus allowing some of those old techniques to become feasible in terms of coverage."
"However, the success of such an approach de-pends on the structure and coverage of the underlying lex-ical ontology."
"In the following sections we discuss the linguistic mo-tivations behind this approach, the characteristics of the lexical ontology MeSH, the use of a corpus to examine the problem space, the method of determining the rela-tions, the accuracy of the results, and the problem of am-biguity."
The paper concludes with related work and a discussion of future work.
"One way to understand the relations between the words in a two-word noun compound is to cast the words into a head-modifier relationship, and assume that the head noun has an argument structure, much the way verbs do, as well as a qualia structure in the sense[REF_CITE]."
"Then the meaning of the head noun determines what kinds of things can be done to it, what it is made of, what it is a part of, and so on."
"For example, consider the noun knife ."
"Knives are cre-ated for particular activities or settings, can be made of various materials, and can be used for cutting or manip-ulating various kinds of things."
"A set of relations for knives, and example NCs exhibiting these relations is shown below: (Used-in): kitchen knife, hunting knife (Made-of): steel knife, plastic knife (Instrument-for): carving knife (Used-on): meat knife, putty knife (Used-by): chef’s knife, butcher’s knife"
Some relationships apply to only certain classes of nouns; the semantic structure of the head noun determines the range of possibilities.
"Thus if we can capture regularities about the behaviors of the constituent nouns, we should also be able to predict which relations will hold between them."
We propose using the categorization provided by a lex-ical hierarchy for this purpose.
"Using a large collection of noun compounds, we assign semantic descriptors from the lexical hierarchy to the constituent nouns and deter-mine the relations between them."
This approach avoids the need to enumerate in advance all of the relations that may hold.
"Rather, the corpus determines which relations occur."
MeSH (Medical Subject Headings) 1 is the National Li-brary of Medicine’s controlled vocabulary thesaurus; it consists of set of terms arranged in a hierarchical struc-ture.
"For example, tree A corresponds to Anatomy, tree B to Organisms, tree C to Diseases and so on."
"Every branch has several sub-branches; Anatomy, for example, consists of Body Regions (A01), Musculoskeletal System (A02), Digestive System (A03) etc."
We refer to these as “level 0” categories.
"These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level 1 children of Body Regions."
"The longer the ID of the MeSH term, the longer the path from the root and the more precise the description."
"For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous"
System Diseases) and so on.
"There are over 35,000 unique IDs[REF_CITE]."
Many words are assigned more than one MeSH ID and so occur in more than one location within the hierarchy; thus the structure of MeSH can be interpreted as a network.
Some of the categories are more homogeneous than others.
"The tree A (Anatomy) for example, seems to be quite homogeneous; at level 0, the nodes are all part of (meronymic to) Anatomy: the Digestive (A03), Respi-ratory (A04) and the Urogenital (A05) Systems are all part of anatomy; at level [Footnote_1], the Biliary Tract (A03.159) and the Esophagus (A03.365) are part of the Digestive System (level 0) and so on."
1[URL_CITE]the work reported in this paper uses[REF_CITE].
Thus we assume that every node is a (body) part of the parent node (and all the nodes above it).
Tree C for Diseases is also homogeneous; the child nodes are a kind of (hyponym of) the disease at the par-ent node: Neoplasms (C04) is a kind of Disease C and Hamartoma (C04.445) is a kind of Neoplasms.
"Other trees are more heterogeneous, in the sense that the meanings among the nodes are more diverse."
"Infor-mation Science (L01), for example, contains, among oth-ers, Communications Media (L01.178), Computer Secu-rity (L01.209) and Pattern Recognition (L01.725)."
An-other heterogeneous sub-hierarchy is Natural Science (H01).
"Among the children of H01 we find Chemistry (parent of Biochemistry), Electronics (parent of Ampli-fiers and Robotics), Mathematics (Fractals, Game The-ory and Fourier Analysis)."
"In other words, we find a wide range of concepts that are not described by a simple rela-tionship."
"These observations suggest that once an algorithm de-scends to a homogeneous level, words falling into the subhierarchy at that level (and below it) behave similarly with respect to relation assignment."
"In this and the next section, we describe how we investi-gated the hypothesis:"
"For all two-word noun compounds (NCs) that can be characterized by a category pair (CP), a particular semantic relationship holds between the nouns comprising those NCs."
The kinds of relations we found are similar to those described in Section 2.
"Note that, in this analysis we fo-cused on determining which sets of NCs fall into the same relation, without explicitly assigning names to the rela-tions themselves."
"Furthermore, the same relation may be described by many different category pairs (see Section 5.5)."
"First, we extracted two-word noun compounds from approximately 1M titles and abstracts from the Med-line collection of biomedical journal articles, resulting indicates the number of unique NCs that fall under the CP."
Only in about 1M NCs.
"The NCs were extracted by finding adjacent word pairs in which both words are tagged as nouns by a tagger and appear in the MeSH hierarchy, and the words preceding and following the pair do not appear in MeSH [Footnote_2] Of these two-word noun compounds, 79,677 were unique."
"2 Clearly, this simple approach results in some erroneous ex-tractions."
Next we used MeSH to characterize the NCs according to semantic category(ies).
"For example, the NC fibroblast growth was categorized into A11.329.228 (Fibroblasts) and G07.553.481 (Growth)."
Note that the same words can be represented at differ-ent levels of description.
"For example, fibroblast growth can be described by the MeSH descriptors A11.329.228 G07.553.481 (original level), but also[REF_CITE](Cell and Physiological Processes) or A11.329 G07.553 (Con-nective Tissue Cells and Growth and Embryonic Devel-opment)."
"If a noun fell under more than one MeSH ID, we made multiple versions of this categorization."
We re-fer to the result of this renaming as a category pair (CP).
"We placed these CPs into a two-dimensional table, with the MeSH category for the first noun on the X axis, and the MeSH category for the second noun on the Y axis."
Each intersection indicates the number of NCs that are classified under the corresponding two MeSH cate-gories.
"A visualization tool[REF_CITE]allowed us to explore the dataset to see which areas of the category space are most heavily populated, and to get a feeling for whether the distribution is uniform or not (see Figure 1)."
"If our hypothesis holds (that NCs that fall within the same category pairs are assigned the same re-lation), then if most of the NCs fall within only a few category pairs then we only need to determine which re-lations hold between a subset of the possible pairs."
"Thus, the more clumped the distribution, the easier (potentially) our task is."
"Figure 1 shows that some areas in the CP space have a higher concentration of unique NCs (the Anatomy, and the E through N sub-hierarchies, for ex-ample), especially when we focus on those for which at least 50 unique NCs are found."
"Given the promising nature of the NC distributions, the question remains as to whether or not the hypothesis holds."
"To answer this, we examined a subset of the CPs to see if we could find positions within the sub-hierarchies for which the relation assignments for the member NCs are always the same."
We first selected a subset of the CPs to examine in detail.
"For each of these we examined, by hand, 20% of the NCs they cover, paraphrasing the relation between the nouns, and seeing if that paraphrase was the same for all the NCs in the group."
"If it was the same, then the current levels of the CP were considered to be the correct levels of descrip-tion."
"If, on the other hand, several different paraphrases were found, then the analysis descended one level of the hierarchy."
This repeated until the resulting partition of the NCs resulted in uniform relation assignments.
"For example, all the following NCs were mapped to the same CP, A01 (Body Regions) and A07 (Cardiovascular System): scalp arteries, heel capillary, shoulder artery, ankle artery, leg veins, limb vein, forearm arteries, fin-ger capillary, eyelid capillary, forearm microcirculation, hand vein, forearm veins, limb arteries, thigh vein, foot vein ."
"All these NCs are “similar” in the sense that the relationships between the two words are the same; there-fore, we do not need to descend either hierarchy."
"We call the pair (A01, A07) a “rule”, where a rule is a CP for which all the NCs under it have the same relationship."
"In the future, when we see an NC mapped to this rule, we will assign this semantic relationship to it."
"On the other hand, the following NCs, having the[REF_CITE](Body Regions) and M01 (Persons), do not have the same relationship between the component words: ab-domen patients, arm amputees, chest physicians, eye pa-tients, skin donor ."
"The relationships are different depend-ing on whether the person is a patient, a physician or a donor."
"We therefore descend the M01 sub-hierarchy, ob-taining the following clusters of NCs:"
"In other words, to correctly assign a relationship to these NCs, we needed to descend one level for the second word."
"The resulting rules in this case are ([REF_CITE].643), (A01, M01.150) etc."
Figure 2 shows one CP for which we needed to descend 3 levels.
"In our collection, a total of 2627 CPs at level 0 have at least 10 unique NCs."
We randomly selected 250 of such CPs for analysis.
"We also analyzed 21 of the 90 CPs for which the sec-ond noun was H01 (Natural Sciences); we decided to ana-lyze this portion of the MeSH hierarchy because the NCs with H01 as second noun are frequent in our collection, and because we wanted to test the hypothesis that we do indeed need to descend farther for heterogeneous parts of MeSH."
"Finally, we analyzed three CPs in category C (Dis-eases); the most frequent CP in terms of the total number of non-unique NCs is C04 (Neoplasms) A11 (Cells), with 30606 NCs; the second CP w[REF_CITE](27520 total NCs) and the fifth most frequent,[REF_CITE]with 20617 total NCs; we analyzed these CPs."
"We started with the CPs at level 0 for both words, de-scending when the corresponding clusters of NCs were not homogeneous and stopping when they were."
We did this for 20% of the NCs in each CP.
The results were as follows.
"We descended one level most of the time for the sub-hierarchies E (Analytical, Diagnostic and Therapeu-tic Techniques), G (Biological Sciences) and N (Health Care) (around 50% of the time for these categories com-bined)."
We never descended for B (Organisms) and did so only for A13 (Animal Structures) in A.
"This was to be able to distinguish a few non-homogeneous subcategories (e.g., milk appearing among body parts, thus forcing a distinction between buffalo milk and cat forelimb)."
"For CPs with H01 as the second noun, of the 21 CPs analyzed, we observed the following (level number, count) pairs: (0, 1) (1, 8) (2, 12)."
"In all but three cases, the descending was done for the second noun only."
"This may be because the second noun usually plays the role of the head noun in two-word noun compounds in English, thus requiring more specificity."
"Alternatively, it may reflect the fact that for the exam-ples we have examined so far, the more heterogeneous terms dominate the second noun."
Further examination is needed to answer this decisively.
"We tested the resulting classifications by developing a randomly chosen test set (20% of the NCs for each CP), entirely distinct from the labeled set, and used the classifications (rules) found above to automatically pre-dict which relations should be assigned to the member NCs."
"An independent evaluator with biomedical training checked these results manually, and found high accura-cies: For the CPs which contained a noun in the Anatomy domain, the assignments of new[REF_CITE].2% accu-rate computed via intra-category averaging, and 91.3% accurate with extra-category averaging."
"For the CPs in the Natural Sciences (H01) we found 81.6% accuracy via intra-category averaging, and 78.6% accuracy with extra-category averaging."
For the three CPs in the C04 category we obtained 100% accuracy.
"The total accuracy across the portions of the A, H01 and C04 hierarchies that we analyzed were 89.6% via intra-category averaging, and 90.8% via extra-category averaging."
The lower accuracy for the Natural Sciences category illustrates the dependence of the results on the proper-ties of the lexical hierarchy.
We can generalize well if the sub-hierarchies are in a well-defined semantic rela-tion with their ancestors.
"If they are a list of “unrelated” topics, we cannot use the generalization of the higher lev-els; most of the mistakes for the Natural Sciences CPs oc-curred in fact when we failed to descend for broad terms such as Physics."
Performing this evaluation allowed us to find such problems and update the rules; the resulting categorization should now be more accurate.
An important issue is whether this method is an economic way of classifying the NCs.
"The advantage of the high level description is, of course, that we need to assign by hand many fewer relationships than if we used all CPs at their most specific levels."
Our approach provides gener-alization over the “training” examples in two ways.
"First, we find that we can use the juxtaposition of categories in a lexical hierarchy to identify semantic relationships."
"Second, we find we can use the higher levels of these cat-egories for the assignments of these relationships."
To assess the degree of this generalization we calcu-lated how many CPs are accounted for by the classifica-tion rules created above for the Anatomy categories.
"In other words, if we know th[REF_CITE]unequivocally de-termines a relationship, how many possible (i.e., present in our collection) CPs are there that are “covered by”[REF_CITE]and that we do not need to consider explicitly?"
It turns out that our 415 classification rules cover 46001 possible CP pairs [Footnote_3] .
"3 Although we began with 250 CPs in the A category, when a descend operation is performed, the CP is split into two or more CPs at the level below. Thus the total number of CPs after all assignments are made was 415."
"This, and the fact that we achieve high accuracies with these classification rules, show that we successfully use MeSH to generalize over unique NCs."
A common problem for NLP tasks is ambiguity.
In this work we observe two kinds: lexical and “relationship” ambiguity.
"As an example of the former, mortality can refer to the state of being mortal or to death rate."
"As an example of the latter, bacteria mortality can either mean “death of bacteria” or “death caused by bacteria”."
"In some cases, the relationship assignment method de-scribed here can help disambiguate the meaning of an ambiguous lexical item."
"Milk for example, can be both Animal Structures (A13) and Food and Beverages (J02)."
"Consider the NCs chocolate milk, coconut milk that fall under the CPs (B06 -Plants-, J02) and (B06, A13)."
"The CP (B06, J02) contains 180 NCs (other examples are berry wines, cocoa beverages) while (B06, A13) has only 6 NCs (4 of which with milk)."
"Assuming then that (B06, A13) is “wrong”, we will assign only (B06, J02) to chocolate milk, coconut milk, therefore disambiguat-ing the sense for milk in this context (Beverage)."
"Anal-ogously, for buffalo milk, caprine milk we also have two CPs (B02, J02) (B02, A13)."
"In this case, however, it is easy to show that only (B02 -Vertebrates-, A13) is the correct one (i.e. yielding the correct relationship) and we then assign the MeSH sense A13 to milk."
"Nevertheless, ambiguity may be a problem for this method."
"We see five different cases: 1) Single MeSH senses for the nouns in the NC (no lex-ical ambiguity) and only one possible relationship which can predicted by the CP; that is, no ambiguity."
"For in-stance, in abdomen radiography, abdomen is classified exclusively under Body Regions and radiography ex-clusively under Diagnosis, and the relationship between them is unambiguous."
"Other examples include aciclovir treatment (Heterocyclic Compounds, Therapeutics) and adenocarcinoma treatment (Neoplasms, Therapeutics). 2) Single MeSH senses (no lexical ambiguity) but mul-tiple readings for the relationships that therefore cannot be predicted by the CP."
It was quite difficult to find exam-ples of this case; disambiguating this kind of NC requires looking at the context of use.
"The examples we did find include hospital databases which can be databases re-garding (topic) hospitals, databases found in (location) or owned by hospitals."
Education efforts can be efforts done through (education) or done to achieve education.
Kidney metabolism can be metabolism happening in (lo-cation) or done by the kidney.
"Immunoglobulin stain-ing, (D12 -Amino Acids, Peptides-, and Proteins, E05 - Investigative Techniques-) can mean either staining with immunoglobulin or staining of immunoglobulin. 3) Multiple MeSH mappings but only one possible re-lation."
One example of this case is alcoholism treatment where treatment is Therapeutics (E02) and alcoholism is both Disorders of Environmental Origin (C21) and Men-tal Disorders (F03).
"For this NC we have therefore 2 CPs: (C21, E02) as in wound treatments, injury rehabilitation and (F03, E02) as in delirium treatment, schizophrenia therapeutics."
"The multiple mappings reflect the conflict-ing views on how to classify the condition of alcoholism, but the relationship does not change. 4) Multiple MeSH mappings and multiple relations that can be predicted by the different CPs."
"For exam-ple, Bread diet can mean either that a person usually eats bread or that a physician prescribed bread to treat a con-dition."
"This difference is reflected by the different map-pings: diet is both Investigative Techniques (E05) and Metabolism and Nutrition (G06), bread is Food and Bev-erages (J02)."
"In these cases, the category can help disam-biguate the relation (as opposed to in case 5 below); word sense disambiguation algorithms that use context may be helpful. 5) Multiple MeSH mappings and multiple relations that cannot be predicted by the different CPs."
"As an ex-ample of this case, bacteria mortality can be both “death of bacteria” or “death caused by bacteria”."
"The multiple mapping for mortality (Public Health, Information Sci-ence, Population Characteristics and Investigative Tech-niques) does not account for this ambiguity."
"Similarly, for inhibin immunization, the first noun falls under Hor-mones and Amino Acids, while immunization falls under"
Environment and Public Health and Investigative Tech-niques.
"The meanings are immunization against inhibin or immunization using inhibin, and they cannot be dis-ambiguated using only the MeSH descriptors."
We currently do not have a way to determine how many instances of each case occur.
"Cases 2 and 5 are the most problematic; however, as it was quite difficult to find ex-amples for these cases, we suspect they are relatively rare."
A question arises as to if representing nouns using the topmost levels of the hierarchy causes a loss in informa-tion about lexical ambiguity.
"In effect, when we represent the terms at higher levels, we assume that words that have multiple descriptors under the same level are very similar, and that retaining the distinction would not be useful for most computational tasks."
"For example, osteosarcoma occurs twice in MeSH, as C04.557.450.565.575.650 and C04.557.450.795.620."
"When described at level 0, both descriptors reduce to C04, at level 1 to C04.557, remov-ing the ambiguity."
"By contrast, microscopy also occurs twice, but under E05.595 and H01.671.606.624."
Reduc-ing these descriptors to level 0 retains the two distinct senses.
"To determine how often different senses are grouped together, we calculated the number of MeSH senses for words at different levels of the hierarchy."
"Table 1 shows a histogram of the number of senses for the first noun of all the unique NCs in our collection, the average degree of ambiguity and the average description lengths. [Footnote_4] The average number of MeSH senses is always less than two, and increases with length of description, as is to be ex-pected."
4 We obtained very similar results for the second noun.
"We observe that 3.6% of the lexical ambiguity is at lev-els higher that 2, 16%[REF_CITE].4%[REF_CITE]% at L0."
Level 1 and 2 combined account for more than 80% of the lexical ambiguity.
"This means that when a noun has mul-tiple senses, those senses are more likely to come from different main subtrees of MeSH (A and B, for exam-ple), than from different deeper nodes in the same subtree (H01.671.538 vs. H01.671.252)."
"This fits nicely with our method of describing the NCs with the higher levels of the hierarchy: if most of the ambiguity is at the highest levels (as these results show), information about lexical ambiguity is not lost when we describe the NCs using the higher levels of MeSH."
"Ideally, however, we would like to reduce the lexical ambiguity for similar senses and to retain it when the senses are semantically distinct (like, for example, for diet in case 4)."
"In other words, ideally, the ambiguity left at the levels of our rules accounts for only (and for all) the semantically different senses."
"Fur-ther analysis is needed, but the high accuracy we obtained in the classification seems to indicate that this indeed is what is happening."
"Because we determine the possible relations in a data-driven manner, the question arises of how often does the same semantic relation occur for different category pairs."
"To determine the answer, we could (i) look at all the CPs, give a name to the relations and “merge” the CPs that have the same relationships; or (ii) draw a sample of NC examples for a given relation, look at the CPs for those examples and verify that all the NCs for those CPs are indeed in the same relationship."
"We may not be able to determine the total number of relations, or how often they repeat across different CPs, until we examine the full spectrum of CPs."
"However, we did a preliminary analysis to attempt to find relation repe-tition across category pairs."
"As one example, we hypoth-esized a relation afflicted by and verified that it applies to all the CPs of the form (Disease C,[REF_CITE].643), e.g.: anorexia (C23) patients, cancer (C04) survivor, in-fluenza (C02) patients."
"This relation also applies to some of the F category (Psychiatry), as in delirium (F03) pa-tients, anxiety (F01) patient."
"It becomes a judgement call whether to also include NCs such as eye (A01) patient, gallbladder (A03) pa-tients, and more generally, all the (Anatomy, Patients) pairs."
"The question is, is “afflicted-by (unspecified) Dis-ease in Anatomy Part” equivalent to “afflicted by Dis-ease?”"
The answer depends on one’s theory of rela-tional semantics.
Another quandary is illustrated by the
"NCs adolescent cancer, child tumors, adult dementia (in which adolescent, child and adult are Age Groups) and the heads are Diseases."
"Should these fall under the af-flicted by relation, given the references to entire groups?"
Several approaches have been proposed for empirical noun compound interpretation.
"Lauer &amp;[REF_CITE]point out that there are three components to the prob-lem: identification of the compound from within the text, syntactic analysis of the compound (left versus right as-sociation), and the interpretation of the underlying se-mantics."
"Several researchers have tackled the syntactic analysis[REF_CITE],[REF_CITE],[REF_CITE], usually using a variation of the idea of finding the subconstituents elsewhere in the cor-pus and using those to predict how the larger compounds are structured."
"We are interested in the third task, interpretation of the underlying semantics."
Most related work relies on hand-written rules of one kind or another.
"Barker &amp;[REF_CITE]describe noun com-pounds as triplets of information: the first constituent, the second constituent, and a marker that can indicate a num-ber of syntactic clues."
"Relations are initially assigned by hand, and then new ones are classified based on their sim-ilarity to previously classified NCs."
"However, similarity at the lexical level means only that the same word occurs; no generalization over lexical items is made."
The algo-rithm is assessed in terms of how much it speeds up the hand-labeling of relations.
"In previous work[REF_CITE], we demonstrated the utility of using a lexical hierarchy for assigning relations to two-word noun compounds."
"We use machine learning algorithms and MeSH to success-fully generalize from training instances, achieving about 60% accuracy on an 18-way classification problem us-ing a very small training set."
"That approach is bottom up and requires good coverage in the training set; the ap-proach described in this paper is top-down, characteriz-ing the lexical hierarchies explicitly rather than implicitly through machine learning algorithms."
Many approaches attempt to automatically assign seman-tic roles (such as case roles) by computing semantic similarity measures across a large lexical hierarchy; pri-marily using WordNet[REF_CITE].
Budanitsky &amp;[REF_CITE]provide a comparative analysis of such algo-rithms.
"However, it is uncommon to simply use the hier-archy directly for generalization purposes."
Many re-searchers have noted that WordNet’s words are classi-fied into senses that are too fine-grained for standard NLP tasks.
"For example,[REF_CITE]notes that the noun book is assigned to seven different senses, including fact and section, subdivision."
Thus most users of WordNet must contend with the sense disambiguation issue in or-der to use the lexicon.
"The most closely related use of a lexical hierarchy that we know of is that of Li &amp;[REF_CITE], which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet."
"This is then used to determine acceptable classes for verb argu-ment structure, and for the prepositional phrase attach-ment problem and is found to perform as well as or better than existing algorithms."
"Additionally,[REF_CITE]“tag” veterinary text using a small set of semantic labels, assigned in much the same way a parser works, and describe this in the context of prepositional phrase attachment."
We have provided evidence that the upper levels of a lex-ical hierarchy can be used to accurately classify the re-lations that hold between two-word technical noun com-pounds.
In this paper we focus on biomedical terms us-ing the biomedical lexical ontology MeSH.
"It may be that such technical, domain-specific terminology is better be-haved than NCs drawn from more general text; we will have to assess the technique in other domains to fully as-sess its applicability."
Several issues need to be explored further.
"First, we need to ensure that this technique works across the full spectrum of the lexical hierarchy."
"We have demonstrated the likely usefulness of such an exercise, but all of our analysis was done by hand."
"It may be useful enough to simply complete the job manually; however, it would be preferable to automate some or all of the analysis."
There are several ways to go about this.
One approach would be to use existing statistical similarity measures[REF_CITE]to attempt to identify which subhierar-chies are homogeneous.
"Another approach would be to see if, after analyzing more CPs, those categories found to be heterogeneous should be assumed to be heteroge-neous across classifications, and similarly for those that seem to be homogeneous."
The second major issue to address is how to extend the technique to multi-word noun compounds.
"We will need to distinguish between NCs such as acute migraine treat-ment and oral migraine treatment , and handle the case when the relation must first be found between the left-most words."
Thus additional steps will be needed; one approach is to compute statistics to indicate likelihood of the various CPs.
Finding noun compound relations is part of our larger effort to investigate what we call statistical semantic pars-ing (as[REF_CITE]; see[REF_CITE]for a nice overview).
"For example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming Congenital anomalies of tra-cheobronchial branching patterns into a form that allows questions to be answered such as “What kinds of irreg-ularities can occur in lung structure?”"
"We hope that by compositional application of relations to entities, such in-ferences will be possible."
"Acknowledgements We thank Kaichi Sung for her work on the relation labeling, Steve Maiorano for his support of this research, and the anonymous reviewers for their comments on the paper."
This research was sup-ported by a grant from ARDA.
This paper introduces new learning al-gorithms for natural language processing based on the perceptron algorithm.
"We show how the algorithms can be efficiently applied to exponential sized representa-tions of parse trees, such as the “all sub-trees” (DOP) representation described[REF_CITE], or a representation tracking all sub-fragments of a tagged sentence."
"We give experimental results showing sig-nificant improvements on two tasks: pars-ing Wall Street Journal text, and named-entity extraction from web data."
"The perceptron algorithm is one of the oldest algo-rithms in machine learning, going back[REF_CITE]."
"It is an incredibly simple algorithm to implement, and yet it has been shown to be com-petitive with more recent learning methods such as support vector machines – see (Freund &amp;[REF_CITE]) for its application to image classification, for example."
This paper describes how the perceptron and voted perceptron algorithms can be used for pars-ing and tagging problems.
"Crucially, the algorithms can be efficiently applied to exponential sized repre-sentations of parse trees, such as the “all subtrees” (DOP) representation described[REF_CITE], or a representation tracking all sub-fragments of a tagged sentence."
It might seem paradoxical to be able to ef-ficiently learn and apply a model with an exponential number of features. [Footnote_1]
"1 Although see[REF_CITE]for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper."
The key to our algorithms is the “kernel” trick ([REF_CITE]discuss kernel methods at length).
We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms.
This leads to polynomial time [Footnote_2] algorithms for training and apply-ing the perceptron.
"2 i.e., polynomial in the number of training examples, and the size of trees or sentences in training and test data."
The kernels we describe are re-lated to the kernels over discrete structures[REF_CITE].
A previous paper[REF_CITE]showed improvements over a PCFG in parsing the ATIS task.
In this paper we show that the method scales to far more complex domains.
"In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model[REF_CITE]."
"In the second domain, detecting named-entity boundaries in web data, we show a 15.6% rel-ative error reduction (an improvement in F-measure from 85.3% to 87.6%) over a state-of-the-art model, a maximum-entropy tagger."
"This result is derived using a new kernel, for tagged sequences, described in this paper."
"Both results rely on a new approach that incorporates the log-probability from a baseline model, in addition to the “all-fragments” features."
This paper focuses on the task of choosing the cor-rect parse or tag sequence for a sentence from a group of “candidates” for that sentence.
The candi-dates might be enumerated by a number of methods.
"The experiments in this paper use the top candi-dates from a baseline probabilistic model: the model[REF_CITE]for parsing, and a maximum-entropy tagger for named-entity recognition."
The choice of representation is central: what fea-tures should be used as evidence in choosing  be-tween candidates?
We will use a function to denote a -dimensional feature vector that rep-resents a tree or  tagged sequence .
There are many .
An obvious example  for parse possibilities for trees is to have one component of for each rule in a context-free grammar that underlies the trees.
This is the representation used by Stochastic Context-Free Grammars.
"The feature vector tracks the counts of rules in the tree , thus encoding the sufficient statistics for the SCFG."
"Given a representation, and two structures and , the inner product between the structures can be defined as  "
"The idea of inner products between feature vectors is central to learning algorithms such as Support Vector Machines (SVMs), and is also central to the ideas in this paper."
"Intuitively, the inner product is a similarity measure between objects: structures with  similar feature vectors will have high values for ."
"More formally, it has been observed that many algorithms can be implemented using inner products between training examples alone, without direct access to the feature vectors themselves."
"As we will see in this paper, this can be crucial for the efficiency of learning with certain representations."
"Following  the SVM literature, we call a function of two objects and a “kernel” if it can be shown that is an inner product in some feature space ."
This section formalizes the idea of linear models for parsing or tagging.
"The method is related to the boosting approach to ranking problems[REF_CITE], the Markov Random Field methods[REF_CITE], and the boosting approaches for parsing[REF_CITE]."
The set-up is as fol-
Training data is a set of example input/output #&quot; %$ lows: pairs.
In parsing the training examples are ! where each ! is a sentence and each &quot; is the correct tree for that sentence.
"We assume some way of enumerating a set &amp; of candidates for a particular sentence - . , We $ use to training data, and * ! denote the ( ’th candidate + for the # ) ’th . . . sentenceto denotein the set of candidates for ! ."
"Without loss of generality &amp; we / take &quot; ). to be the correct candidate &amp; for ! (i.e.,"
Each candidate is represented by a feature vector in the space .
The parameters of the model are also a vector 1 .
The out-put 243658792 of ;=: the &lt;?A&gt; @CB-DFE model 1  on a .training or test example ! is
"The key question, having defined a representation , is how to set the parameters 1 ."
"We discuss one method for setting the weights, the perceptron algo-rithm, in the next section."
Figure 1(a) shows the perceptron algorithm applied to the ranking task.
"The method assumes a training set as described in section 3.1, and a representation of parse trees."
"The algorithm maintains a param-eter vector 1 , which is initially set to be all zeros."
"The algorithm then makes a pass over the training set, only updating the parameter vector when a mis-take is made on an example."
"The parameter vec-tor update is very simple, involving &amp; adding the dif-ference of the  offending  examples’ representations ( 1 1HG in the figure)."
"Intu-itively, this update has the effect of increasing the parameter values for features in the correct tree, and downweighting the parameter values for features in the competitor."
"See[REF_CITE]for dis-cussion of the perceptron algorithm, including an overview of various theorems justifying this way of setting the parameters."
"Briefly, the perceptron algo-rithm is guaranteed [Footnote_3] to find a hyperplane that cor-rectly classifies all training points, if such a hyper-plane exists (i.e., the data is “separable”)."
3 To find such a hyperplane the algorithm must be run over the training set repeatedly until no mistakes are made. The al-gorithm in figure 1 includes just a single pass over the training set.
"Moreover, the number of mistakes made will be low, providing that the data is separable with “large margin”, and this translates to guarantees about how the method generalizes to test examples. (Freund &amp;[REF_CITE]) give theorems showing that the voted per-ceptron (a variant described below) generalizes well even given non-separable data."
"Figure 1(b) shows an equivalent algorithm to the perceptron, an algorithm which we will call the “dual form” of the perceptron."
"The dual-form &amp; 1 , in-al-gorithm does not store a parameter vector Q P for ) stead W . . . storing (  a . set . . of."
"Thedualscoreparametersfor a parse, is de-fined by the dual &amp; parameters  as &amp;"
M  B P&amp; E Q P  L
"This is in contrast to 1  , the score in the original algorithm."
"In spite of these differences the algorithms give &amp; identical &amp; results on &amp; training and test exam- 1 ples: Q to  see this  , it can # ,beandverifiedhence thatthat M"
"N L P P , throughout training."
The important difference between the algorithms lies in the analysis of their computational complex-ity.
"Say N g is the size of the training set, i.e., g ."
"Also, take to be the dimensional-ity of the parameter vector 1 ."
Then the algorithm in figure L  1(a) takes h gi time.
"This follows be- [Footnote_4] must be calculated for each member L of cause the training set, and each calculation of involves h time."
"4 If the vectors jlk-mon are sparse, then p can be taken to be the number of non-zero elements of j , assuming that it takes qrk-pAn time to add feature vectors with qrk-pAn non-zero elements, or to take inner products."
Now say the time taken to compute the inner product between two examples is s .
The run- s ning time of the algorithm in figure 1(b) is h g .
"This follows because throughout the algorithm the number of non-zero dual parameters  is bounded by , and hence the calculation of M takes at most h s time. (Note that the dual form algorithm runs in quadratic time in the number of training examples , because gut .)"
The dual algorithm is therefore more efficient in cases where swvxv/ .
"This might seem unlikely to be the case – naively, it would be expected  that the time to calculate the inner product be-tween two vectors to be at least h ."
"But it turns out that for some high-dimensional representations the inner product can be calculated in much bet-ter than h time, making the dual form algorithm more efficient than the original algorithm."
The dual-form algorithm goes back[REF_CITE].
See[REF_CITE]for more explanation of the algorithm.
"(Freund &amp;[REF_CITE]) describe a refinement of the perceptron algorithm, the “voted perceptron”."
They give theory which suggests that the voted per-ceptron is preferable in cases of noisy or unsepara-ble data.
The training phase of the algorithm is un-changed – the change is in how the method is applied to test examples.
"The algorithm in figure 1(b)  can be considered &quot; . . . ,towherebuild &amp;M a y seriesis definedof hypothesesas MJy&amp; , for z &amp; Q P   M y B y P E"
MJy is the scoring &quot; function from the algorithm trained on just the first training &quot; examples.
The output of a model trained on the first examples for a sentence ! is {|y ! } ;: &lt;a&gt; @%E MJy  .
"Thus the training algorithm can be considered [ to construct a sequence of models, { . . . { ."
"On a test sentence ! , each of these &quot;+ functions ~W . . . , will."
"Thereturnvotedits $ own parse tree, { y ! for perceptron picks in the set ;{ the most likely tree  as ! that . . . which { [ ! occurs. most often ! {"
"Note &amp; thatthe MJy identityis easily  derived &amp; from MJy G , through N &amp;[; , Q yP  y MJy y JM y  ."
"Be- cause of this the voted perceptron can be imple-mented with the same number of kernel calculations, and hence roughly the same computational complex-ity, as the original perceptron."
"We now consider a representation that tracks all sub-trees seen in training data, the representation stud-ied extensively[REF_CITE]."
See figure 2 for an example.
Conceptually we begin by enumer-ating W all tree fragments that occur in the training data . . . .
Note that this is done only implicitly.
Each tree is represented by a dimensional vector where the ) ’th component counts the number of oc-curences  of the ) ’th tree fragment.
"Define the func-to be the number , of occurences of the ) ’th tion astree  fragment in  tree ,  so that . .. is  now # . representedNote that will be huge (a given tree will have a number of sub-trees that is exponential in its size)."
Because of this we aim to design algorithms whose computational complexity is independent of .
"The key to our efficient use of this representa-tion is , a dynamic programming algorithm that com-putes the inner product between two examples and in polynomial (in the size of the trees in-volved), rather than h , time."
The algorithm is pleteness we repeat it here.
"We first define the setdescribed in (Collins and Duffy , 2001), but , for com-of nodes in trees and as and respec-tively W ."
"We define the indicator function  to be if sub-tree , ) is seen rooted ,  at node N [a &gt;4   and 0 other-wise  . }"
It follows N [8 &gt;4 that   and .
"The first step to efficient computation of the inner product is the following property if : if , ? R  f ,  N  4&gt; N    N f [  &gt;4 f  , f ,    N  N [? 8&gt; [? "
N [  &gt;8 
"N   ,     N ?[  8&gt;  "
"N [  8&gt;  , , N   , where we define   ."
"Next, we note that  can be , computeddefinitionef-: ficiently, , due to the following recursiveand , If the  productions at are different "
"If the , productions. at are , the same ,  and , and are , pre-terminals, thenand  and . [Footnote_5]"
"5 Pre-terminals are nodes directly above words in the surface string, for example the N, V, and D symbols in Figure 2."
"Else if the productions at are the same and and are not pre-terminals, [  B [? E  G  , , 4&amp;   (  ( #   is the number , of children , where of in the tree; because  the  productions  at / are the same, we have ."
"The ) ’th child-node of ), . , is that  To see that this  recursive N  definition  issimplycorrectcounts, note the number of common , subtrees that are found rooted at both and."
"The first two cases , are trivially correct."
"The last, recursive, definition , fol-lows because a common subtree for and can be formed by taking the production at / , to-gether with a choice at each child of simply tak-ing the non-terminal at that child, or any one of the common sub-trees at that child."
Thus there are at the ) ’th child. (Note )  £)  
"GT  )£ ) ## possible choices that a similar recursion is de-scribed by Goodman[REF_CITE], Goodman’s application being the conversion of Bod’s model ,[REF_CITE]to an , equivalent PCFG  .)  "
"N It [a P is [   clear , , from the identity , ¤-¤ , ¤0 that  , and  the recursive definition , of  ¤ can be calculated in h time: the matrix of  values can be filled in, then summed. [Footnote_6]"
"6 This can be a pessimistic estimate of the runtime. A more usefulcharacterization k-® #¯ ®  n +° is ± that  ² it ± runs  suchintimethatlinearthe productionsinthenumberat of ® membersand ®  are the same. In our data we have found the number of nodes with identical productions to be approximately linear in the size of the trees, so the running time is also close to linear in the size of the trees."
Since there will be many more tree fragments of larger size – say depth four versus depth three – it makes sense to downweight the contribu-tion of larger tree fragments to the kernel.
"This v can ¥ ¦ be achieved W by introducing a parameter , and modifying , definitionsthe ¥ baseof case  toandbe , re-re-cursive &amp; B case E  of the and , ## spectively ¥J§ 4[  [   G¨ ( + .  "
"This , _ cor- responds N ¥ D ©6ª \ to  a modified  , (  kernel where «! )F¬a­ is the number of rules in the ) ’th fragment."
This is roughly equiva-lent to having a prior that large sub-trees will be less useful in the learning task.
"The second problem we consider is tagging, where each word in a sentence is mapped to one of a finite set of tags."
"The tags might represent part-of-speech tags, named-entity boundaries, base noun-phrases, or other structures."
"In the experiments in this paper we consider named-entity recognition. $ pairsA tagged  «³ sequence  ! . . . ³ is [=´ a ! sequence [ where of ³ wordis the/state ) ’th word, and ! is the tag for that word."
The par-ticular representation we consider is similar to the all sub-trees representation for trees.
"A tagged-sequence “fragment” is a subgraph that contains a subsequence of state labels, where each label may or may not contain the word below it."
See figure 3 for an example.
Each tagged sequence is represented by  a dimensional vector where the ) ’th component counts the number of occurrences of the ) ’th fragment in .
"The inner product under this representation can be calculated using dynamic programming in a very , similar way , to the tree algorithm."
We firstanddefineasthe set of states in tagged sequences and respectively.
Each state has an asso-ciated label and an associated word W .
"We define the indicator function  if fragment ) to be is seen , with left-most state ,  at node N [  4&gt; ,  and  0 other-and wise  .  "
It follows N [  &gt;4 that   .
"As before, some simple algebra if shows if that ,  ,f  N [a 8&gt; , "
"N [  N4&gt;   ,  where we define  define  &quot;¶."
"Next, for any given state to be the state , to the right of in the structure &quot; , ."
An analogous definition holds for ­«µ .
"Then  can be computed using , :dynamic programming , , due to a recursiveanddefinition , If the V state U labels at are different  . , same, If the , state labels atand  andare &quot; differentare , # the , but the words  G· at  &quot; , then  ,  ."
"Else , if the state labelsandat are  andthe ­«µ &quot; same , # are, thenthe same, and the  words G c¹¸ at   &quot; ."
There are a couple of useful modifications U ¥º¦W to this kernel.
"One is to introduce a parameter v , which penalizes larger substructures ,  ."
"The ,  recur-"
"G sive W ¥ definitions &quot; are  ­«µ modfied &quot;, # andto be  , cC¥G    &quot;­«µ  N ­«µ ¥&quot;D ©6ª \# respectively   ."
This gives an inner product where !«F¬a­) is the number of state labels in the ) th fragment.
Another useful modification is as follows.
"Define might be defined to be ¾ 1 if ³ and ³ are both capitalized: in this case )f¿ is a looser notion ¾ of similarity than the exact match be modified to:criterion of )f¿ . , Finally, the definition , of  can  "
"If labels at ,  are different ,  , , , ."
"Else   U ¾ G G .ÁÀ¥Â,¸ ) f¿  G U .ÁÀ &quot;¾ ) ¿ , ##³ , ³ #³  &quot;³ where ³ , ³ are the words at and respec-tively."
"This inner product implicitly includes fea-tures which track word features, and thus can make better use of sparse data."
We used the same data set as that described[REF_CITE].
The Penn Wall Street Journal tree-bank[REF_CITE]was used as training and test data.
"Sections 2-21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set."
"The remaining 4,000 sentences were used as development data, and for tuning parame-ters of the algorithm."
"Model 2[REF_CITE]was used to parse both the training and test data, produc-ing multiple hypotheses for each sentence."
"In or-der to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sen-tence chunks, each chunk being parsed with a model trained on the remaining 34,000 sentences (this pre-vented the initial model from being unrealistically “good” on the training sentences)."
"The representation we use incorporates the prob-ability from the original model, as well as the all-subtrees representation."
We introduce a pa-rameter Ã which controls the  relative contribu-tion of the two terms.
"If Ä ability of a  tree under  the  original ,  is . . . the log prob-probability # the feature vector under , the all subtrees represen- ,  model, and is tation  ,  then the  new  representation . . . # , andis the , inner , ÃÇÄ product  between ÃÇÄ  Ä two G examples  and."
This isallows the perceptron algorithm to use the probability from the original model as well as the subtrees information to rank trees.
"We would thus expect the model to do at least as well as the original probabilistic , model , ."
"The algorithm in figure 1(b) was  applied  to the problem, with the inner  product used in the definition of M ."
The algorithm in 1(b) runs in approximately quadratic time in the number of training examples.
"This made it somewhat ex-pensive to run the algorithm over all 36,000 training sentences in one pass."
"Instead, we broke the training set into 6 chunks of roughly equal size, and trained 6 separate perceptrons on these data sets."
"This has the advantage of reducing training time, both be-cause of the quadratic dependence on training set size, and also because it is easy to train the 6 models in parallel."
The outputs from the 6 runs on test ex-amples were combined through the voting procedure described in section 3.4.
Figure 4 shows the results for the voted percep- ¥ tron with the U tree c kernel U .
The parameters Ã and were set to . and ÁÉ. respectively through tun-ing U .ÁÊCË on theabsolutedevelopmentimprovementset.
"Thein methodaverage showspreci-a sion ¦  and recall (from 88.2% to 88.8% on sentences words), a 5.1% relative reduction in er-ror."
"The boosting method[REF_CITE]showed 89.6%/89.9% recall and precision on reranking ap-proaches for the same datasets (sentences less than 100 words in length).[REF_CITE]describes a different method which achieves very similar per-formance[REF_CITE].[REF_CITE]describes experiments giving 90.6%/90.8% recall and preci-sion for sentences of less than 40 words in length, using the all-subtrees representation, but using very different algorithms and parameter estimation meth-ods from the perceptron algorithms in this paper (see section 7 for more discussion)."
Over a period of a year or so we have had over one million words of named-entity data annotated.
"The data is drawn from web pages, the aim being to sup-port a question-answering system over web data."
"A number of categories are annotated: the usual peo-ple, organization and location categories, as well as less frequent categories such as brand-names, scien-tific terms, event titles (such as concerts) and so on."
"As a result, we created a training set of 53,609 sen-tences (1,047,491 words), and a test set of 14,717 sentences (291,898 words)."
The task we consider is to recover named-entity boundaries.
We leave the recovery of the categories of entities to a separate stage of processing.
We eval-uate different methods on the task through precision and recall. [Footnote_7]
"7 If a method proposes Ì entities on the test set, and Í of these are correct then the precision of a method is Î¼¼ÏÑÐlÍ#ÒfÌ . Similarly, if Ó is the number of entities in the human annotated version of the test set, then the recall is #Î ¼¼ÏÔÐÕÍÒ6Ó ."
"The problem can be framed as a tag-ging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all."
"As a baseline model we used a maximum entropy tagger, very similar to the one described[REF_CITE]."
"Maximum en-tropy taggers have been shown to be highly com-petitive on a number of tagging tasks, such as part-of-speech tagging[REF_CITE], and named-entity recognition (Borthwick et. al 1998)."
Thus the maximum-entropy tagger we used represents a serious baseline for the task.
"We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described[REF_CITE])."
"As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data using a beam search which keeps the top 20 hypotheses at each stage of a left-to-right search."
"In training the voted percep-tron we split the training data into a 41,992 sen-tence training set, and a 11,617 sentence develop-ment set."
"The training set was split into 5 portions, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5."
In this way the whole training data was decoded.
"In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set."
"As in the parsing , experiments , , the final kernel in-corporates the probability  from  the ÃÇÄ maximum  Ä en- G  where Ä   tropy tagger, i.e. is the ; log-likelihood  of under the tagging model, is the tagging kernel described previously, and Ã is a parameter weighting the two ¥ terms."
"The other free parame-ter in the kernel is , which determines how quickly larger structures are downweighted."
"In running sev-eral training runs with different parameter values, and then testing error rates on the development  set . c , _ best .ÁÀ parameter."
"Figure 5valuesshowsweresultsfoundonwerethe Ã test data, the ¥ for the baseline maximum-entropy tagger, and the voted perceptron."
The results show a 15.6% relative improvement in F-measure.
"The methods explicitly deal with the param-eters associated with subtrees, with sub-sampling of tree fragments making the computation manageable."
"Even after this, Bod’s method is left with a huge grammar:[REF_CITE]describes a grammar with over 5 million sub - structures."
"The method requires search for the 1,000 most probable derivations un-der this grammar, using beam search, presumably a challenging computational task given the size of the grammar."
"In spite of these problems,[REF_CITE]gives excellent results for the method on parsing Wall Street Journal text."
"The algorithms in this paper have a different flavor, avoiding the need to explic-itly deal with feature vectors that track all subtrees, and also avoiding the need to sum over an exponen-tial number of derivations underlying a given tree.[REF_CITE]gives a polynomial time con-version of a DOP model into an equivalent PCFG whose size is linear in the size of the training set."
The method uses a similar recursion to the common sub-trees recursion described in this paper.
"Good-man’s method still leaves exact parsing under the model intractable (because of the need to sum over multiple derivations underlying the same tree), but he gives an approximation to finding the most prob-able tree, which can be computed efficiently."
"From a theoretical point of view, it is difficult to find motivation for the parameter estimation meth-ods used[REF_CITE]– see[REF_CITE]for discussion."
"In contrast, the parameter estimation methods in this paper have a strong theoretical basis (see[REF_CITE]chapter 2 and (Freund &amp;[REF_CITE]) for statistical theory underlying the perceptron)."
"For related work on the voted perceptron algo-rithm applied to NLP problems, see[REF_CITE]and[REF_CITE].[REF_CITE]describes ex-periments on the same named-entity dataset as in this paper, but using explicit features rather than ker-nels.[REF_CITE]describes how the voted per-ceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm ap-plied to ranking tasks."
Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the exper-iments.
Thanks to Rob Schapire and Yoram Singer for many useful discussions.
"We present a stochastic parsing system consisting of a Lexical-Functional Gram-mar (LFG), a constraint-based parser and a stochastic disambiguation model."
We re-port on the results of applying this sys-tem to parsing the UPenn Wall Street Journal (WSJ) treebank.
The model com-bines full and partial parsing techniques to reach full grammar coverage on unseen data.
The treebank annotations are used to provide partially labeled data for dis-criminative statistical estimation using ex-ponential models.
Disambiguation perfor-mance is evaluated by measuring matches of predicate-argument relations on two distinct test sets.
"On a gold standard of manually annotated f-structures for a sub-set of the WSJ treebank, this evaluation reaches 79% F-score."
An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score.
Statistical parsing using combined systems of hand-coded linguistically fine-grained grammars and stochastic disambiguation components has seen con-siderable progress in recent years.
"However, such at-tempts have so far been confined to a relatively small scale for various reasons."
"Firstly, the rudimentary character of functional annotations in standard tree-banks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems."
"Rather, parameter esti-mation for such models had to resort to unsupervised techniques[REF_CITE], or training corpora tailored to the specific grammars had to be created by parsing and manual disam-biguation, resulting in relatively small training sets of around 1,000 sentences[REF_CITE]."
"Furthermore, the effort involved in coding broad-coverage grammars by hand has often led to the spe-cialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percent-age of sentences for which at least one analysis is found) on free text."
The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained hand-coded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank[REF_CITE].
"The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis, is tack-led in our approach by an extension of a full-fledged Lexical-Functional Grammar (LFG) and a constraint-based parser with partial parsing tech-niques."
"In the absence of a complete parse, a so-called “ FRAGMENT grammar” allows the input to be analyzed as a sequence of well-formed chunks."
The set of fragment parses is then chosen on the basis of a fewest-chunk method.
With this combination of full and partial parsing techniques we achieve 100% grammar coverage on unseen data.
Another goal of this work is the best possible ex-ploitation of the WSJ treebank for discriminative es-timation of an exponential model on LFG parses.
We define discriminative or conditional criteria with re- spect to the set of grammar parses consistent with the treebank annotations.
Such data can be gathered by applying labels and brackets taken from the tree-bank annotation to the parser input.
The rudimen-tary treebank annotations are thus used to provide partially labeled data for discriminative estimation of a probability model on linguistically fine-grained parses.
"Concerning empirical evaluation of disambigua-tion performance, we feel that an evaluation measur-ing matches of predicate-argument relations is more appropriate for assessing the quality of our LFG-based system than the standard measure of match-ing labeled bracketing on section 23 of the WSJ treebank."
The first evaluation we present measures matches of predicate-argument relations in LFG f-structures (henceforth the LFG annotation scheme) to a gold standard of manually annotated f-structures for a representative subset of the WSJ treebank.
The evaluation measure counts the number of predicate-argument relations in the f-structure of the parse selected by the stochastic model that match those in the gold standard annotation.
Our parser plus stochastic disambiguator achieves 79% F-score un-der this evaluation regime.
"Furthermore, we employ another metric which maps predicate-argument relations in LFG f-structures to the dependency relations (henceforth the DR annotation scheme) proposed[REF_CITE]."
Evaluation with this metric measures the matches of dependency relations to Carroll et al.’s gold standard corpus.
"For a direct comparison of our results with Carroll et al.’s system, we computed an F-score that does not distinguish different types of dependency relations."
Under this measure we obtain 76% F-score.
This paper is organized as follows.
"Section 2 describes the Lexical-Functional Grammar, the constraint-based parser, and the robustness tech-niques employed in this work."
In section 3 we present the details of the exponential model on LFG parses and the discriminative statistical estimation technique.
Experimental results are reported in sec-tion 4.
A discussion of results is in section 5.
The grammar used for this project was developed in the ParGram project[REF_CITE].
"It uses LFG as a formalism, producing c(onstituent)-structures (trees) and f(unctional)-structures (attribute value matrices) as output."
The c-structures encode con-stituency.
"F-structures encode predicate-argument relations and other grammatical information, e.g., number, tense."
"The XLE parser[REF_CITE]was used to produce packed represen-tations, specifying all possible grammar analyses of the input."
"The grammar has 314 rules with regular expres-sion right-hand sides which compile into a collec-tion of finite-state machines with a total of 8,759 states and 19,695 arcs."
The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized.
"As such, most nouns, adjectives, and adverbs have no explicit lexical entry."
"The main verb lexicon con-tains 9,652 verb stems and 23,525 subcategorization frame-verb stem entries; there are also lexicons for adjectives and nouns with subcategorization frames and for closed class items."
"For estimation purposes using the WSJ treebank, the grammar was modified to parse part of speech tags and labeled bracketing."
A stripped down ver-sion of the WSJ treebank was created that used only those POS tags and labeled brackets relevant for determining grammatical relations.
The WSJ la-beled brackets are given LFG lexical entries which constrain both the c-structure and the f-structure of the parse.
"For example, the WSJ’s ADJP-PRD la-bel must correspond to an AP in the c-structure and an XCOMP in the f-structure."
"In this version of the corpus, all WSJ labels with -SBJ are retained and are restricted to phrases corresponding to SUBJ in the LFG grammar; in addition, it contains NP under VP ( OBJ and OBJ th in the LFG grammar), all -LGS tags ( OBL - AG ), all -PRD tags ( XCOMP ), VP under VP ( XCOMP ), SBAR- ( COMP ), and verb POS tags under VP (V in the c-structure)."
"For example, our labeled bracketing of wsj 1305.mrg is [NP-SBJ His credibility] is/VBZ also [PP-PRD on the line] in the investment community."
Some mismatches between the WSJ labeled bracketing and the LFG grammar remain.
These often arise when a given constituent fills a gram-matical role in more than one clause.
"For exam-ple, in wsj 1303.mrg Japan’s Daiwa Securities Co. named Masahiro Dozen president. , the noun phrase Masahiro Dozen is labeled as an NP-SBJ."
"However, the LFG grammar treats it as the OBJ of the ma-trix clause."
"As a result, the labeled bracketed version of this sentence does not receive a full parse, even though its unlabeled, string-only counterpart is well-formed."
"Some other bracketing mismatches remain, usually the result of adjunct attachment."
"Such mis-matches occur in part because, besides minor mod- ifications to match the bracketing for special con-structions, e.g., negated infinitives, the grammar was not altered to mirror the idiosyncrasies of the WSJ bracketing."
"To increase robustness, the standard grammar has been augmented with a FRAGMENT grammar."
"This grammar parses the sentence as well-formed chunks specified by the grammar, in particular as Ss, NPs, PPs, and VPs."
These chunks have both c-structures and f-structures corresponding to them.
Any token that cannot be parsed as one of these chunks is parsed as a TOKEN chunk.
The TOKEN s are also recorded in the c- and f-structures.
The grammar has a fewest-chunk method for determining the correct parse.
"For example, if a string can be parsed as two NPs and a VP or as one NP and an S, the NP-S option is chosen."
"A sample FRAGMENT c-structure and f-structure are shown in Fig. 1 for wsj 0231.mrg (The golden share was scheduled to expire at the beginning of), an incomplete sentence; the parser builds one S chunk and then one TOKEN for the stranded preposition."
A final capability of XLE that increases cov-erage of the standard-plus-fragment grammar is a SKIMMING technique.
Skimming is used to avoid timeouts and memory problems.
"When the amount of time or memory spent on a sentence exceeds a threshhold, XLE goes into skimming mode for the constituents whose processing has not been completed."
"When XLE skims these remaining con-stituents, it does a bounded amount of work per sub-tree."
This guarantees that XLE finishes processing a sentence in a polynomial amount of time.
"In pars-ing section 23, 7.2% of the sentences were skimmed; 26.1% of these resulted in full parses, while 73.9% were FRAGMENT parses."
"The grammar coverage achieved 100% of section 23 as unseen unlabeled data: 74.7% as full parses, 25.3% FRAGMENT and/or SKIMMED parses."
We employed the well-known family of exponential models for stochastic disambiguation.
In this paper we are concerned with conditional exponential mod-els of the form: p λ (x|y) =
"Z λ (y) −1 e λ·f(x) where X(y) is the set of parses for sentence y, Z λ (y) ="
"P x∈X(y) e λ·f(x) is a normalizing con-stant, λ = (λ 1 ,... ,λ n ) ∈"
"IR n is a vector of log-parameters, f = (f 1 ,... ,f n ) is a vector of property-functions f i : X → IR for i = 1,... ,n on the set of parses X , and λ · f(x) is the vector dot n product P i=1 λ i f i (x)."
"In our experiments, we used around 1000 complex property-functions comprising information about c-structure, f-structure, and lexical elements in parses, similar to the properties used[REF_CITE]."
"For example, there are property func-tions for c-structure nodes and c-structure subtrees, indicating attachment preferences."
High versus low attachment is indicated by property functions count-ing the number of recursively embedded phrases.
"Other property functions are designed to refer to f-structure attributes, which correspond to gram-matical functions in LFG, or to atomic attribute-value pairs in f-structures."
"More complex property functions are designed to indicate, for example, the branching behaviour of c-structures and the (non)-parallelism of coordinations on both c-structure and f-structure levels."
"Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented[REF_CITE]are included in the model."
"Here tuples of head words, argument words, and grammatical relations are ex-tracted from the training sections of the WSJ, and fed into a finite mixture model for clustering gram-matical relations."
The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses.
Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing[REF_CITE].
"In discriminative es-timation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized."
"Since the discriminative task is kept in mind during estimation, discriminative methods can yield improved performance."
"In our case, discrimi-native criteria cannot be defined directly with respect to “correct labels” or “gold standard” parses since the WSJ annotations are not sufficient to disam-biguate the more complex LFG parses."
"However, in-stead of retreating to unsupervised estimation tech-niques or creating small LFG treebanks by hand, we use the labeled bracketing of the WSJ training sec-tions to guide discriminative estimation."
"That is, dis-criminative criteria are defined with respect to the set of parses consistent with the WSJ annotations. 1"
"The objective function in our approach, denoted by P(λ), is the joint of the negative log-likelihood −L(λ) and a Gaussian regularization term −G(λ) on the parameters λ."
"Let {(y j ,z j )} mj=1 be a set of training data, consisting of pairs of sentences y and partial annotations z, let X(y, z) be the set of parses for sentence y consistent with annotation z, and let X(y) be the set of all parses produced by the gram-mar for sentence y."
"Furthermore, let p[f] denote the expectation of function f under distribution p."
Then P(λ) can be defined for a conditional exponential model p λ (z|y) as:
P (λ) = −L(λ) − G(λ) m n λ 2i = − log Y p λ (z j |y j ) +
"X 2σ 2ij=[Footnote_1] i=1 X(y j ,z j ) e λ·f(x) = − X log PP m n λ 2i + X 2σ 2 j=1 X(y j ) e λ·f(x) ii=1 m = − X log X e λ·f(x) j=1 X(y j ,z j ) m n λ 2i + X log X e λ·f(x) +"
"1 An earlier approach using partially labeled data for estimat-ing stochastics parsers is Pereira and Schabes’s (1992) work on training PCFG from partially bracketed data. Their approach differs from the one we use here in that Pereira and Schabes take an EM-based approach maximizing the joint likelihood of the parses and strings of their training data, while we maximize the conditional likelihood of the sets of parses given the corre-sponding strings in a discriminative estimation setting."
X 2σ 2 . i j=1 X(y j ) i=1
"Intuitively, the goal of estimation is to find model pa- rameters which make the two expectations in the last equation equal, i.e. which adjust the model param-eters to put all the weight on the parses consistent with the annotations, modulo a penalty term from the Gaussian prior for too large or too small weights."
"Since a closed form solution for such parame-ters is not available, numerical optimization meth-ods have to be used."
"In our experiments, we applied a conjugate gradient routine, yielding a fast converg-ing optimization algorithm where at each iteration the negative log-likelihood P(λ) and the gradient vector have to be evaluated. 2"
"For our task the gra-dient takes the form: ∂P (λ) ∂P (λ) ∂P (λ) ∇P (λ) = , , . . . , , and ∂λ 1 ∂λ 2 ∂λ n m ∂P (λ) e λ·f(x) f i (x) ∂λ i = − X( X j=1 x∈X(y j ,z j )"
"P x∈X(y ,z ) e λ·f(x) j j e λ·f(x) f i (x) λ i . − X ) + σ i[Footnote_2] x∈X(y j ) P x∈X(y ) e λ·f(x) j"
"2 An alternative numerical method would be a combination of iterative scaling techniques with a conditional EM algorithm[REF_CITE]. However, it has been shown exper-imentally that conjugate gradient techniques can outperform it-erative scaling techniques by far in running time[REF_CITE]."
"The derivatives in the gradient vector intuitively are again just a difference of two expectations m m λ i − X p λ [f i |y j , z j ] + X p λ [f i |y j ] + σ 2 . ij=1 j=1"
"Note also that this expression shares many common terms with the likelihood function, suggesting an ef-ficient implementation of the optimization routine."
The basic training data for our experiments are sec-tions 02-21 of the WSJ treebank.
"As a first step, all sections were parsed, and the packed parse forests unpacked and stored."
"For discriminative estimation, this data set was restricted to sentences which re-ceive a full parse (in contrast to a FRAGMENT or SKIMMED parse) for both its partially labeled and its unlabeled variant."
"Furthermore, only sentences which received at most 1,000 parses were used."
"From this set, sentences of which a discriminative learner cannot possibly take advantage, i.e. sen-tences where the set of parses assigned to the par-tially labeled string was not a proper subset of the parses assigned the unlabeled string, were removed."
"These successive selection steps resulted in a fi-nal training set consisting of 10,000 sentences, each with parses for partially labeled and unlabeled ver-sions."
"Altogether there were 150,000 parses for par-tially labeled input and 500,000 for unlabeled input."
"For estimation, a simple property selection pro-cedure was applied to the full set of around 1000 properties."
This procedure is based on a frequency cutoff on instantiations of properties for the parses in the labeled training set.
The result of this proce-dure is a reduction of the property vector to about half its size.
"Furthermore, a held-out data set was created from section 24 of the WSJ treebank for ex-perimental selection of the variance parameter of the prior distribution."
"This set consists of 120 sentences which received only full parses, out of which the most plausible one was selected manually."
"Two different sets of test data were used: (i) 700 sen-tences randomly extracted from section 23 of the WSJ treebank and given gold-standard f-structure annotations according to our LFG scheme, and (ii) 500 sentences from the Brown corpus given gold standard annotations[REF_CITE]accord-ing to their dependency relations (DR) scheme. [Footnote_3]"
"3 Both corpora are available online. The WSJ f-structure bank[URL_CITE], and Carroll et al.’s corpus[URL_CITE]."
Annotating the WSJ test set was bootstrapped by parsing the test sentences using the LFG gram-mar and also checking for consistency with the Penn Treebank annotation.
"Starting from the (some-times fragmentary) parser analyses and the Tree-bank annotations, gold standard parses were created by manual corrections and extensions of the LFG parses."
Manual corrections were necessary in about half of the cases.
The average sentence length of the WSJ f-structure bank is 19.8 words; the average number of predicate-argument relations in the gold-standard f-structures is 31.2.
"Performance on the LFG-annotated WSJ test set was measured using both the LFG and DR metrics, thanks to an f-structure-to-DR annotation mapping."
Performance on the DR-annotated Brown test set was only measured using the DR metric.
"The LFG evaluation metric is based on the com-parison of full f-structures, represented as triples relation(predicate, argument)."
The predicate-argument relations of the f-structure for one parse of the sentence Meridian will pay a premium of $30.5 million to assume $2 billion in deposits. are shown in Fig. 2.
"The DR annotation for our example sentence, ob-tained via a mapping from f-structures to Carroll et al’s annotation scheme, is shown in Fig. 3."
"Superficially, the LFG and DR representations are very similar."
One difference between the annotation schemes is that the LFG representation in general specifies more relation tuples than the DR represen-tation.
"Also, multiple occurences of the same lex-ical item are indicated explicitly in the LFG rep-resentation but not in the DR representation."
"The main conceptual difference between the two an-notation schemes is the fact that the DR scheme crucially refers to phrase-structure properties and word order as well as to grammatical relations in the definition of dependency relations, whereas the"
LFG scheme abstracts away from serialization and phrase-structure.
Facts like this can make a correct mapping of LFG f-structures to DR relations prob-lematic.
"Indeed, we believe that we still underesti-mate by a few points because of DR mapping diffi-culties. [Footnote_4]"
"4[REF_CITE]for more detail on the DR an-notation scheme, and see[REF_CITE]for more de-tail on the differences between the DR and the LFG annotation schemes, as well as on the difficulties of the mapping from LFG f-structures to DR annotations."
"In our evaluation, we report F-scores for both types of annotation, LFG and DR, and for three types of parse selection, (i) lower bound: random choice of a parse from the set of analyses (averaged over 10 runs), (ii) upper bound: selection of the parse with the best F-score according to the annotation scheme used, and (iii) stochastic: the parse selected by the stochastic disambiguator."
The error reduc-tion row lists the reduction in error rate relative to the upper and lower bounds obtained by the stochas-tic disambiguation model.
F-score is defined as 2 × precision × recall/(precision + recall).
"Table 1 gives results for 700 examples randomly selected from section 23 of the WSJ treebank, using both LFG and DR measures."
Table 1: Disambiguation results for 700 randomly selected examples from section 23 of the WSJ tree-bank using LFG and DR measures.
"The effect of the quality of the parses on disam-biguation performance can be illustrated by break-ing down the F-scores according to whether the parser yields full parses, FRAGMENT , SKIMMED , or SKIMMED + FRAGMENT parses for the test sentences."
The percentages of test examples which belong to the respective classes of quality are listed in the first row of Table 2.
F-scores broken down according to classes of parse quality are recorded in the follow- ing rows.
"The first column shows F-scores for all parses in the test set, as in Table 1."
The second col-umn shows the best F-scores when restricting atten-tion to examples which receive only full parses.
"The third column reports F-scores for examples which receive only non-full parses, i.e. FRAGMENT or SKIMMED parses or SKIMMED + FRAGMENT parses."
"Columns 4-6 break down non-full parses according to examples which receive only FRAGMENT , only SKIMMED , or only SKIMMED + FRAGMENT parses."
Results of the evaluation on Carroll et al.’s Brown test set are given in Table 3.
Evaluation results for the DR measure applied to the Brown corpus test set broken down according to parse-quality are shown in Table 2.
In Table 3 we show the DR measure along with an evaluation measure which facilitates a direct com-parison of our results to those[REF_CITE].
This dependency-only (DO) measure thus does not reflect mismatches between arguments and modi-fiers in a small number of cases.
"Note that since for the evaluation on the Brown corpus, no heldout data were available to adjust the variance parame-ter of a Bayesian model, we used a plain maximum-likelihood model for disambiguation on this test set."
Table 3: Disambiguation results on 500 Brown cor-pus examples using DO measure and DR measures.
We have presented a first attempt at scaling up a stochastic parsing system combining a hand-coded linguistically fine-grained grammar and a stochas-tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining specialized constraint-based parsing techniques for LFG grammars with partial parsing techniques.
"Fur-thermore, a maximal exploitation of treebank anno-tations for estimating a distribution on fine-grained LFG parses is achieved by letting grammar analyses which are consistent with the WSJ labeled bracket-ing define a gold standard set for discriminative es-timation."
"The combined system trained on WSJ data achieves full grammar coverage and disambiguation performance of 79% F-score on WSJ data, and 76% F-score on the Brown corpus test set."
"While disambiguation performance of around 79% F-score on WSJ data seems promising, from one perspective it only offers a 3% absolute im-provement over a lower bound random baseline."
"We think that the high lower bound measure high-lights an important aspect of symbolic constraint-based grammars (in contrast to treebank gram-mars): the symbolic grammar already significantly restricts/disambiguates the range of possible analy-ses, giving the disambiguator a much narrower win-dow in which to operate."
"As such, it is more appro-priate to assess the disambiguator in terms of reduc-tion in error rate (36% relative to the upper bound) than in terms of absolute F-score."
Both the DR and LFG annotations broadly agree in their measure of error reduction.
The lower reduction in error rate relative to the upper bound for DR evaluation on the Brown corpus can be attributed to a corpus effect that has also been observed[REF_CITE]for training and testing PCFGs on the WSJ and Brown corpora. [Footnote_5]
5 Gildea reports a decrease from 86.1%/86.6% re-call/precision on labeled bracketing to 80.3%/81% when going from training and testing on the WSJ to training on the WSJ and testing on the Brown corpus.
"Breaking down results according to parse quality shows that irrespective of evaluation measure and corpus, around 4% overall performance is lost due to non-full parses, i.e. FRAGMENT , or SKIMMED , or SKIMMED + FRAGMENT parses."
"Due to the lack of standard evaluation measures and gold standards for predicate-argument match-ing, a comparison of our results to other stochastic parsing systems is difficult."
"To our knowledge, so far the only direct point of comparison is the parser[REF_CITE]which is also evaluated on Carroll et al.’s test corpus."
"They report an F-score of 75.1% for a DO evaluation that ignores predicate labels, counting only dependencies."
"Under this mea-sure, our system achieves 76.1% F-score."
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification-based grammar (UBG).
"Existing algo-rithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statis-tics needed to estimate a grammar from a training corpus."
This paper describes a graph-based dynamic programming algo-rithm for calculating these statistics from the packed UBG parse representations[REF_CITE]which does not require enumerating all parses.
"Like many graphical algorithms, the dynamic programming algorithm’s complexity is worst-case exponential, but is often poly-nomial."
"The key observation is that by using Maxwell and Kaplan packed repre-sentations, the required statistics can be rewritten as either the max or the sum of a product of functions."
This is exactly the kind of problem which can be solved by dynamic programming over graphical models.
Stochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to de-fine probability distributions over the parses of a uni-fication grammar.
"These grammars can incorporate virtually all kinds of linguistically important con-straints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning."
"Un-fortunately, the maximum likelihood estimator Ab-ney proposed for SUBGs seems computationally in-tractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar."
This set is infinite (so exhaustive enumer-ation is impossible) and presumably has a very com-plex structure (so sampling estimates might take an extremely long time to converge).
The conditional maximum likelihood estimator proposed by Johnson et al. requires statistics that depend on the set of all parses of the strings in the training cor- pus.
"For most linguistically realistic grammars this set is finite, and for moderate sized grammars and training corpora this estimation procedure is quite feasible."
"However, our recent experiments involve training from the Wall Street Journal Penn Tree-bank, and repeatedly enumerating the parses of its 50,000 sen-tences is quite time-consuming."
Matters are only made worse because we have moved some of the constraints in the grammar from the unification com-ponent to the stochastic component.
"This broadens the coverage of the grammar, but at the expense of massively expanding the number of possible parses of each sentence."
In the mid-1990s unification-based parsers were developed that do not enumerate all parses of a string but instead manipulate and return a “packed” rep-resentation of the set of parses.
This paper de-scribes how to find the most probable parse and the statistics required for estimating a SUBG from the packed parse set representations proposed[REF_CITE].
This makes it pos-sible to avoid explicitly enumerating the parses of the strings in the training corpus.
"The methods proposed here are analogues of the well-known dynamic programming algorithms for Probabilistic Context-Free Grammars (PCFGs); specifically the Viterbi algorithm for finding the most probable parse of a string, and the Inside-Outside algorithm for estimating a PCFG from un-parsed training data. [Footnote_1]"
"1 However, because we use conditional estimation, also known as discriminative training, we require at least some dis-criminating information about the correct parse of a string in order to estimate a stochastic unification grammar."
"In fact, because Maxwell and Kaplan packed representations are just Truth Main-tenance System (TMS) representations (Forbus and de[REF_CITE]), the statistical techniques described here should extend to non-linguistic applications of TMSs as well."
Dynamic programming techniques have been applied to log-linear models before.
They describe a technique for calculating the statistics required to estimate a log-linear parsing model with non-local properties from packed feature forests.
The rest of this paper is structured as follows.
The next section describes unification grammars and Maxwell and Kaplan packed representation.
The following section reviews stochastic unifica-tion grammars[REF_CITE]and the statistical quantities required for efficiently estimating such grammars from parsed training data[REF_CITE].
The final substantive section of this paper shows how these quantities can be defined directly in terms of the Maxwell and Kaplan packed repre-sentations.
The notation used in this paper is as follows.
"Vari-ables are written in upper case italic, e.g., X, Y , etc., the sets they range over are written in script, e.g., X , Y, etc., while specific values are written in lower case italic, e.g., x, y, etc."
"In the case of vector-valued entities, subscripts indicate particular components."
This section characterises the properties of unifica-tion grammars and the Maxwell and Kaplan packed parse representations that will be important for what follows.
This characterisation omits many details about unification grammars and the algorithm by which the packed representations are actually con-structed; see[REF_CITE]for de-tails.
A parse generated by a unification grammar is a finite subset of a set F of features.
"Features are parse fragments, e.g., chart edges or arcs from attribute-value structures, out of which the packed representa-tions are constructed."
"For this paper it does not mat-ter exactly what features are, but they are intended to be the atomic entities manipulated by a dynamic programming parsing algorithm."
A grammar defines a set Ω of well-formed or grammatical parses.
Each parse ω ∈ Ω is associated with a string of words Y (ω) called its yield.
Note that except for trivial grammars F and Ω are infinite.
"If y is a string, then let Ω(y) = {ω ∈ Ω|Y (ω) = y} and F(y) = S ω∈Ω(y) {f ∈ ω}."
"That is, Ω(y) is the set of parses of a string y and F(y) is the set of features appearing in the parses of y. In the gram-mars of interest here Ω(y) and hence also F(y) are finite."
Maxwell and Kaplan’s packed representations of-ten provide a more compact representation of the set of parses of a sentence than would be obtained by merely listing each parse separately.
"The intu-ition behind these packed representations is that for most strings y, many of the features in F(y) occur in many of the parses Ω(y)."
"This is often the case in natural language, since the same substructure can appear as a component of many different parses."
"Packed feature representations are defined in terms of conditions on the values assigned to a vec-tor of variables X. These variables have no direct linguistic interpretation; rather, each different as-signment of values to these variables identifies a set of features which constitutes one of the parses in the packed representation."
"A condition a on X is a function from X to {0,1}."
"While for uniformity we write conditions as functions on the entire vec-tor X, in practice Maxwell and Kaplan’s approach produces conditions whose value depends only on a few of the variables in X, and the efficiency of the algorithms described here depends on this."
"A packed representation of a finite set of parses is a quadruple R = (F 0 , X, N, α), where: • F 0 ⊇ F(y) is a finite set of features, • X is a finite vector of variables, where each variable X ` ranges over the finite set X ` , • N is a finite set of conditions on X called the no-goods, [Footnote_2] and • α is a function that maps each feature f ∈ F 0 to a condition α f on X."
"2 The name “no-good” comes from the TMS literature, and was used by Maxwell and Kaplan. However, here the no-goods actually identify the good variable assignments."
"A vector of values x satisfies the no-goods N iff N(x) = 1, where N(x) ="
Q η∈N η(x).
"Each x that satisfies the no-goods identifies a parse ω(x) = {f ∈ F 0 |α f (x) = 1}, i.e., ω is the set of features whose conditions are satisfied by x. We require that each parse be identified by a unique value satisfying the no-goods."
"That is, we require that: ∀x, x 0 ∈ X if N(x) ="
N(x 0 ) = 1 and ω(x) = ω(x 0 ) then x = x 0 (1)
"Finally, a packed representation R represents the set of parses Ω(R) that are identified by values that satisfy the no-goods, i.e., Ω(R) = {ω(x)|x ∈ X , N(x) = 1}."
The SUBG parsing and estimation algorithms described in this paper use Maxwell and Kaplan’s parsing algorithm as a subroutine.
"This section reviews the probabilistic framework used in SUBGs, and describes the statistics that must be calculated in order to estimate the pa-rameters of a SUBG from parsed training data."
"For a more detailed exposition and descriptions of regularization and other important details, see[REF_CITE]."
"The probability distribution over parses is defined in terms of a finite vector g = (g 1 ,... ,g m ) of properties."
"A property is a real-valued function of parses Ω.[REF_CITE]placed no restric-tions on what functions could be properties, permit-ting properties to encode arbitrary global informa-tion about a parse."
"However, the dynamic program-ming algorithms presented here require the informa-tion encoded in properties to be local with respect to the features F used in the packed parse representa-tion."
"Specifically, we require that properties be de-fined on features rather than parses, i.e., each feature f ∈ F is associated with a finite vector of real values (g 1 (f), . . . , g m (f)) which define the property func-tions for parses as follows: g k (ω) ="
"X g k (f), for k = 1 . . . m. (2) f∈ω"
"That is, the property values of a parse are the sum of the property values of its features."
"In the usual case, some features will be associated with a single property (i.e., g k (f) is equal to 1 for a specific value of k and 0 otherwise) , and other features will be as-sociated with no properties at all (i.e., g(f) = 0)."
"This requires properties be very local with re-spect to features, which means that we give up the ability to define properties arbitrarily."
Note how-ever that we can still encode essentially arbitrary linguistic information in properties by adding spe-cialised features to the underlying unification gram-mar.
"For example, suppose we want a property that indicates whether the parse contains a reduced rela-tive clauses headed by a past participle (such “gar-den path” constructions are grammatical but often almost incomprehensible, and alternative parses not including such constructions would probably be pre-ferred)."
"Under the current definition of properties, we can introduce such a property by modifying the underlying unification grammar to produce a certain “diacritic” feature in a parse just in case the parse ac-tually contains the appropriate reduced relative con-struction."
"Thus, while properties are required to be local relative to features, we can use the ability of the underlying unification grammar to encode essen-tially arbitrary non-local information in features to introduce properties that also encode non-local in-formation."
"A Stochastic Unification-Based Grammar is a triple (U,g,θ), where U is a unification grammar that defines a set Ω of parses as described above, g = (g 1 , . . . , g m ) is a vector of property functions as just described, and θ = (θ 1 , . .. , θ m ) is a vector of non-negative real-valued parameters called property weights."
The probability P θ (ω) of a parse ω ∈ Ω is:
P θ (ω) =
"W θ (ω), where: Z θ m W θ (ω) ="
"Y θ gj j (ω) , and j=1 Z θ = X W θ (ω 0 ) ω 0 ∈Ω"
"Intuitively, if g j (ω) is the number of times that prop-erty j occurs in ω then θ j is the ‘weight’ or ‘cost’ of each occurrence of property j and Z θ is a normal-ising constant that ensures that the probability of all parses sums to 1."
Now we discuss the calculation of several impor-tant quantities for SUBGs.
"In each case we show that the quantity can be expressed as the value that maximises a product of functions or else as the sum of a product of functions, each of which depends on a small subset of the variables X. These are the kinds of quantities for which dynamic programming graphical model algorithms have been developed."
In parsing applications it is important to be able to extract the most probable (or MAP) parse ω̂(y) of string y with respect to a SUBG.
This parse is: ω̂(y) = argmax W θ (ω) ω∈Ω(y)
"Given a packed representation (F 0 , X, N, α) for the parses Ω(y), let x̂(y) be the x that identifies ω̂(y)."
"Since W θ (ω̂(y)) &gt; 0, it can be shown that: m j (ω(x)) x̂(y) = argmax N(x) Y θ gjx∈X j=1 m = argmax N(x) Y θ P j f∈ω(x) g j (f) x∈X j=1 m = argmax N(x) Y θ j P f∈F0 α f (x)g j (f) x∈X j=1 m = argmaxN(x) Y Y θ αj f (x)g j (f) x∈X j=1 f∈F 0 α f (x) g j (f)  = argmaxN(x) Y Y θ m f∈F 0  j=1 j  x∈X (3) = argmax Y η(x)"
"Y h θ,f (x) x∈X η∈N f∈F 0 g j (f) if α f (x) = 1 and where h θ,f (x) ="
"Q mj=1 θ j h θ,f (x) = 1 if α f (x) = 0."
"Note that h θ,f (x) de-pends on exactly the same variables in X as α f does."
"As (3) makes clear, finding x̂(y) involves maximis-ing a product of functions where each function de-pends on a subset of the variables X. As explained below, this is exactly the kind of maximisation that can be solved using graphical model techniques."
"We now turn to the estimation of the property weights θ from a training corpus of parsed data D = (ω 1 , . . . , ω n )."
"As explained[REF_CITE], one way to do this is to find the θ that maximises the conditional likelihood of the training corpus parses given their yields. (Johnson et al. actually maximise conditional likelihood regularized with a Gaussian prior, but for simplicity we ignore this here)."
"If y i is the yield of the parse ω i , the conditional likelihood of the parses given their yields is: n W θ (ω i )"
L D (θ) =
Y i=1 Z θ (Ω(y i )) where Ω(y) is the set of parses with yield y and:
Z θ (S) = X W θ (ω). ω∈S
Then the maximum conditional likelihood estimate θ̂ of θ is θ̂ = argmax θ L D (θ).
"Now calculating W θ (ω i ) poses no computational problems, but since Ω(y i ) (the set of parses for y i ) can be large, calculating Z θ (Ω(y i )) by enumerating each ω ∈ Ω(y i ) can be computationally expensive."
"However, there is an alternative method for calcu-lating Z θ (Ω(y i )) that does not involve this enumera-tion."
"As noted above, for each yield y i , i = 1, . . . , n, Maxwell’s parsing algorithm returns a packed fea-ture structure R i that represents the parses of y i , i.e., Ω(y i ) ="
Ω(R i ).
"A derivation parallel to the one for (3) shows that for R = (F 0 , X, N, α):"
"Z θ (Ω(R)) = X Y η(x) Y h θ,f (x) (4) x∈X η∈N f∈F 0 (This derivation relies on the isomorphism between parses and variable assignments in (1))."
It turns out that this type of sum can also be calculated using graphical model techniques.
"In general, iterative numerical procedures are re-quired to find the property weights θ that maximise the conditional likelihood L D (θ)."
"While there are a number of different techniques that can be used, all of the efficient techniques require the calculation of conditional expectations E θ [g k |y i ] for each prop-erty g k and each sentence y i in the training corpus, where:"
X g(ω)P θ (ω|y) E θ [g|y] = ω∈Ω(y) = P ω∈Ω(y) g(ω)W θ (ω) Z θ (Ω(y))
"For example, the Conjugate Gradient algorithm, which was used by Johnson et al., requires the cal-culation not just of L D (θ) but also its derivatives ∂L"
D (θ)/∂θ k .
It is straight-forward to show: n ∂L
D (θ) =
L D (θ) X (g k (ω i ) − E θ [g k |y i ]) .∂θ k θ k i=1
"We have just described the calculation of L D (θ), so if we can calculate E θ [g k |y i ] then we can calcu-late the partial derivatives required by the Conjugate Gradient algorithm as well."
"Again, let R = (F 0 , X, N, α) be a packed repre-sentation such that Ω(R) = Ω(y i )."
"First, note that (2) implies that:"
E θ [g k |y i ] = X g k (f) P({ω : f ∈ ω}|y i ). f∈F 0
Note that P({ω : f ∈ ω}|y i ) involves the sum of weights over all x ∈ X subject to the conditions that N(x) = 1 and α f (x) = 1.
Thus P({ω : f ∈ ω}|y i ) can also be expressed in a form that is easy to evaluate using graphical techniques.
Z θ (Ω(R))P θ ({ω : f ∈ ω}|y i ) 0 (x) (5) = X α f (x) Y η(x)
"Y h θ,f x∈X η∈N f 0 ∈F 0"
In this section we briefly review graphical model algorithms for maximising and summing products of functions of the kind presented above.
"It turns out that the algorithm for maximisation is a gener-alisation of the Viterbi algorithm for HMMs, and the algorithm for computing the summation in (5) is a generalisation of the forward-backward algo-rithm for HMMs[REF_CITE]."
"Viewed abstractly, these algorithms simplify these expres-sions by moving common factors over the max or sum operators respectively."
These techniques are now relatively standard; the most well-known ap-proach involves junction trees[REF_CITE].
"We adopt the approach approach de-scribed[REF_CITE], which is a straightforward generalization of HMM dynamic programming with minimal assumptions and pro-gramming overhead."
"However, in principle any of the graphical model computational algorithms can be used."
"The quantities (3), (4) and (5) involve maximisa-tion or summation over a product of functions, each of which depends only on the values of a subset of the variables"
"X. There are dynamic programming algorithms for calculating all of these quantities, but for reasons of space we only describe an algorithm for finding the maximum value of a product of func-tions."
These graph algorithms are rather involved.
It may be easier to follow if one reads Example 1 before or in parallel with the definitions below.
To explain the algorithm we use the following no-tation.
"If x and x 0 are both vectors of length m then x = j x 0 iff x and x 0 disagree on at most their jth components, i.e., x k = x 0k for k = 1, . .. , j − 1, j + 1, . . . m."
"If f is a function whose domain is X, we say that f depends on the set of variables d(f) = {X j |∃x, x 0 ∈ X, x = j x 0 , f(x) 6= f(x 0 )}."
"That is, X j ∈ d(f) iff changing the value of X j can change the value of f."
"The algorithm relies on the fact that the variables in X = (X 1 ,... ,X n ) are ordered (e.g., X 1 pre-cedes X 2 , etc.), and while the algorithm is correct for any variable ordering, its efficiency may vary dramatically depending on the ordering as described below."
"Let H be any set of functions whose do-mains are X. We partition H into disjoint subsets H 1 , . . . , H n+1 , where H j is the subset of H that de-pend on X j but do not depend on any variables or-dered before X j , and H n+1 is the subset of H that do not depend on any variables at all (i.e., they are con-stants). [Footnote_3] That is , H j = {H ∈ H|X j ∈ d(H),∀i &lt; j X i 6∈ d(H)} and H n+1 = {H ∈ H|d(H) = ∅}."
"3 Strictly speaking this does not necessarily define a parti-tion, as some of the subsets H j may be empty."
"As explained in section 3.1, there is a set of func-tions A such that the quantities we need to calculate have the general form:"
M max = max Y A(x) (6) x∈X A∈A (7) x̂ = argmax Y A(x). x∈X A∈A
M max is the maximum value of the product expres-sion while x̂ is the value of the variables at which the maximum occurs.
In a SUBG parsing application x̂ identifies the MAP parse.
"The procedure depends on two sequences of func-tions M i , i = 1, . . . , n + 1 and V i , i = 1, . .. , n. Informally, M i is the maximum value attained by the subset of the functions"
"A that depend on one of the variables X 1 , . . . , X i , and V i gives information about the value of X i at which this maximum is at-tained."
"To simplify notation we write these functions as functions of the entire set of variables X, but usu-ally depend on a much smaller set of variables."
"The M i are real valued, while each V i ranges over X i ."
"Let M = {M 1 , . . . , M n }."
"Recall that the sets of functions A and M can be both be partitioned into disjoint subsets A 1 , . . . , A n+1 and M 1 , . . . , M n+1 respectively on the basis of the variables each A i and M i depend on."
"The definition of the M i and V i , i = 1, . . . , n is as follows: max Y A(x 0 ) Y M(x 0 ) (8) M i (x) = x 0 ∈X M∈M i s.t. x 0 = i x A∈A i V i (x) = argmax Y A(x 0 ) Y M(x 0 ) x 0 ∈X A∈A i M∈M i s.t. x 0 = i x"
"M n+1 receives a special definition, since there is no variable X n+1 .  M  (9)   M n+1 =  A∈A"
Y A   Y M∈M n+1  n+1
"The definition of M i in (8) may look circular (since M appears in the right-hand side), but in fact it is not."
"First, note that M i depends only on variables ordered after X i , so if M j ∈ M i then j &lt; i. More specifically, d(M i ) =    A∈A [ d(A) ∪ [ d(M)  \ {X i }. i M∈M i"
"Thus we can compute the M i in the order M 1 , . . . , M n+1 , inserting M i into the appropriate set M k , where k &gt; i, when M i is computed."
"We claim that M max = M n+1 . (Note that M n+1 and M n are constants, since there are no variables ordered after X n )."
"To see this, consider the tree T whose nodes are the M i , and which has a directed edge from M i to M j iff M i ∈ M j (i.e., M i appears in the right hand side of the definition (8) of M j )."
"T has a unique root M n+1 , so there is a path from every M i to M n+1 ."
Let i ≺ j iff there is a path from M i to M j in this tree.
"Then a simple induction shows that M j is a function from d(M j ) to a max-imisation over each of the variables X i where i ≺ j of Q i≺j,A∈A A. i"
"Further, it is straightforward to show that V i (x̂) = x̂ i (the value x̂ assigns to X i )."
"By the same argu-ments as above, d(V i ) only contains variables or-dered after X i , so V n = x̂ n ."
"Thus we can evaluate the V i in the order V n , . . . , V 1 to find the maximising assignment x̂."
"Example 1 Let X = { X 1 , X 2 , X 3 , X 4 , X 5 , X 6 , X 7 } and set A = {a(X 1 ,X 3 ), b(X 2 ,X 4 ), c(X 3 , X 4 , X 5 ), d(X 4 , X 5 ), e(X 6 , X 7 )}."
"We can represent the sharing of variables in A by means of a undirected graph G A , where the nodes of G A are the variables X and there is an edge in G A connecting X i to X j iff ∃A ∈"
"A such that both X i , X j ∈ d(A)."
G A is depicted below.
X 1 X 3 X 5 X 6 r r r r r r r X 2 X 4 X 7
"Starting with the variable X 1 , we compute M 1 and V 1 : x 4 ∈X 4 V 4 (x 5 ) = argmax d(x 4 , x 5 )M 2 (x 4 )M 3 (x 4 , x 5 ) x 4 ∈X 4"
"Note that M 5 is a constant, reflecting the fact that in G A the node X 5 is not connected to any node or-dered after it."
M max = M 8 = M 5 M 7
"Finally, we evaluate V 7 , . . . , V 1 to find the maximis-ing assignment x̂."
We now briefly consider the computational com-plexity of this process.
"Clearly, the number of steps required to compute each M i is a polynomial of or-der |d(M i )|+1, since we need to enumerate all pos-sible values for the argument variables d(M i ) and for each of these, maximise over the set X i ."
"Fur-ther, it is easy to show that in terms of the graph G A , d(M j ) consists of those variables X k , k &gt; j reach-able by a path starting at X j and all of whose nodes except the last are variables that precede X j ."
"Since computational effort is bounded above by a polynomial of order |d(M i )| + 1, we seek a variable ordering that bounds the maximum value of |d(M i )|."
"Unfortunately, finding the ordering that minimises the maximum value of |d(M i )| is an NP-complete problem."
"However, there are several efficient heuris-tics that are reputed in graphical models community to produce good visitation schedules."
It may be that they will perform well in the SUBG parsing applica-tions as well.
"This paper shows how to apply dynamic program-ming methods developed for graphical models to SUBGs to find the most probable parse and to ob-tain the statistics needed for estimation directly from Maxwell and Kaplan packed parse representations. i.e., without expanding these into individual parses."
"The algorithm rests on the observation that so long as features are local to the parse fragments used in the packed representations, the statistics required for parsing and estimation are the kinds of quantities that dynamic programming algorithms for graphical models can perform."
"Since neither Maxwell and Ka-plan’s packed parsing algorithm nor the procedures described here depend on the details of the underly-ing linguistic theory, the approach should apply to virtually any kind of underlying grammar."
"Obviously, an empirical evaluation of the algo-rithms described here would be extremely useful."
"The algorithms described here are exact, but be-cause we are working with unification grammars and apparently arbitrary graphical models we can-not polynomially bound their computational com-plexity."
"However, it seems reasonable to expect that if the linguistic dependencies in a sentence typ-ically factorize into largely non-interacting cliques then the dynamic programming methods may offer dramatic computational savings compared to current methods that enumerate all possible parses."
"It might be interesting to compare these dy-namic programming algorithms with a standard unification-based parser using a best-first search heuristic. (To our knowledge such an approach has not yet been explored, but it seems straightforward: the figure of merit could simply be the sum of the weights of the properties of each partial parse’s frag-ments)."
"Because such parsers prune the search space they cannot guarantee correct results, unlike the al-gorithms proposed here."
"Such a best-first parser might be accurate when parsing with a trained gram-mar, but its results may be poor at the beginning of parameter weight estimation when the parameter weight estimates are themselves inaccurate."
"Finally, it would be extremely interesting to com-pare these dynamic programming algorithms to the ones described[REF_CITE]."
"It seems that the Maxwell and Kaplan packed repre-sentation may permit more compact representations than the disjunctive representations used by Miyao et al., but this does not imply that the algorithms proposed here are more efficient."
Further theoreti-cal and empirical investigation is required.
"We present a framework for statistical machine translation of natural languages based on direct maximum entropy mod-els, which contains the widely used sour-ce-channel approach as a special case."
"All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables."
This approach allows a baseline machine translation system to be extended easily by adding new feature functions.
We show that a baseline statistical machine transla-tion system is significantly improved us-ing this approach.
"We are given a source (‘French’) sentence f 1J = f 1 , . . . , f j , . .. , f J , which is to be translated into a target (‘English’) sentence e I1 = e 1 , . . . , e i , . . . , e I ."
"Among all possible target sentences, we will choose the sentence with the highest probability: 1 ê I1 = argmax {P r(e I1 |f 1J )} ([Footnote_1]) e I1"
"1 The notational convention will be as follows. We use the symbol Pr(·) to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·)."
"The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language."
"According to Bayes’ decision rule, we can equiva-lently to Eq. 1 perform the following maximization: ê I1 = argmax {P r(e I1 ) · P r(f 1J |e I1 )} (2) e I1"
This approach is referred to as source-channel ap-proach to statistical MT.
"Sometimes, it is also re-ferred to as the ‘fundamental equation of statisti-cal MT’[REF_CITE]."
"Here, Pr(e I1 ) is the language model of the target language, whereas P r(f 1J |e I1 ) is the translation model."
"Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach."
"Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently."
The overall architecture of the source-channel ap-proach is summarized in Figure 1.
"In general, as shown in this figure, there may be additional trans-formations to make the translation task simpler for the algorithm."
"Typically, training is performed by applying a maximum likelihood approach."
"If the language model Pr(e I1 ) = p γ (e I1 ) depends on pa-rameters γ and the translation model Pr(f 1J |e I1 ) = p θ (f 1J |e I1 ) depends on parameters θ, then the opti-mal parameter values are obtained by maximizing the likelihood on a parallel training corpus f S1 ,e S1[REF_CITE]:"
Y S θ̂ = argmax p θ (f s |e s ) (3) θ s=1 Y S γ̂ = argmax p γ (e s ) (4) γ s=1
We obtain the following decision rule: ê I1 = argmax{p γ̂ (e I1 ) · p θ̂ (f 1J |e I1 )} (5) e I1
State-of-the-art statistical MT systems are based on this approach.
"Yet, the use of this decision rule has various problems: 1."
The combination of the language model p γ̂ (e I1 ) and the translation model p θ̂ (f 1J |e I1 ) as shown in Eq. 5 can only be shown to be optimal if the true probability distributions p γ̂ (e I1 ) =
P r(e I1 ) and p θ̂ (f 1J |e I1 ) =
Pr(f 1J |e I1 ) are used.
"Yet, we know that the used models and training methods provide only poor approximations of the true probability distributions."
"Therefore, a different combination of language model and translation model might yield better results. 2."
There is no straightforward way to extend a baseline statistical MT model by including ad-ditional dependencies. 3.
"Often, we observe that comparable results are obtained by using the following decision rule instead of Eq. 5[REF_CITE]: ê I1 = argmax{p γ̂ (e I1 ) · p θ̂ (e I1 |f 1J )} (6) e I1"
"Here, we replaced p θ̂ (f 1J |e 1I ) by p θ̂ (e I1 |f 1J )."
"From a theoretical framework of the source-channel approach, this approach is hard to jus-tify."
"Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search."
"As alternative to the source-channel approach, we directly model the posterior probability P r(e I1 |f 1J )."
An especially well-founded framework for doing this is maximum entropy[REF_CITE].
"In this framework, we have a set of M feature func-tions h m (e I1 , f 1J ), m = 1, . . . , M. For each feature function, there exists a model parameter λ m ,m = 1, . . . , M. The direct translation probability is given by:"
"P r(e I1 |f 1J ) = p λ M (e I1 |f 1J ) (7) 1 P M = P exp[ m P =1 λ m h m (e I1 , f 1J )] (8) e 0I1 exp [ Mm=1 λ m h m (e 0I1 , f 1J )]"
This approach has been suggested[REF_CITE]for a natural lan-guage understanding task.
"We obtain the following decision rule: n o ê 1I = argmax P r(e I1 |f 1J ) e I1 n X M o = argmax λ m h m (e I1 , f 1J ) e I1 m=1"
"Hence, the time-consuming renormalization in Eq. 8 is not needed in search."
The overall architecture of the direct maximum entropy models is summarized in Figure 2.
"Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use the following two feature functions: h 1 (e I1 , f 1J ) = log p γ̂ (e I1 ) (9) h 2 (e I1 , f 1J ) = log p θ̂ (f 1J |e I1 ) (10) and set λ 1 = λ 2 = 1."
"Optimizing the corresponding parameters λ 1 and λ 2 of the model in Eq. 8 is equiv-alent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or pattern recognition."
The use of an ‘inverted’ translation model in the unconventional decision rule of Eq. 6 results if we use the feature function logPr(e I1 |f 1J ) instead of log Pr(f 1J |e I1 ).
"In this framework, this feature can be as good as log P r(f 1J |e 1I )."
"It has to be empirically verified, which of the two features yields better re-sults."
"We even can use both features log P r(e I1 |f 1J ) and logPr(f 1J |e I1 ) , obtaining a more symmetric translation model."
"As training criterion, we use the maximum class posterior probability criterion: ( ) X S λ̂ M1 = argmax log p λ M (e s |f s ) (11) 1 λ M1 s=1"
This corresponds to maximizing the equivocation or maximizing the likelihood of the direct transla-tion model.
This direct optimization of the poste-rior probability in Bayes decision rule is referred to as discriminative training[REF_CITE]because we directly take into account the overlap in the proba-bility distributions.
The optimization problem has one global optimum and the optimization criterion is convex.
"Typically, the probability P r(f 1J |e I1 ) is decomposed via additional hidden variables."
"In statistical align-ment models P r(f 1J , a J1 |e I1 ), the alignment a 1J is in-troduced as a hidden variable:"
X P r(f 1J |e I1 ) =
"P r(f 1J , a J1 |e I1 ) a J1"
The alignment mapping is j → i = a j from source position j to target position i = a j .
Search is performed using the so-called maximum approximation:    X ê 1I = argmax P r(e I1 ) ·
"P r(f 1J , a J1 |e I1 ) e I1 a J1 ( ) ≈ argmax P r(e I1 ) · max P r(f 1J , a J1 |e 1I ) e I1 a J1"
"Hence, the search space consists of the set of all pos-sible target language sentences e I1 and all possible alignments a J1 ."
"Generalizing this approach to direct translation models, we extend the feature functions to in-clude the dependence on the additional hidden vari-able."
"Using M feature functions of the form h m (e I1 , f 1J , a 1J ), m = 1, . .. , M, we obtain the fol-lowing model:"
"P r(e I1 , a J1 |f 1J ) =³P ´ exp M λ m h m (e I1 , f 1J , a J1 ) ³ m P =1 ´ = P e 0I1 ,a 0J1 exp M λ h (e 0I , f J , a 0J1 ) m=1 m m 1 1"
"Obviously, we can perform the same step for transla-tion models with an even richer structure of hidden variables than only the alignment a J1 ."
"To simplify the notation, we shall omit in the following the de-pendence on the hidden variables of the model."
"As specific MT method, we use the alignment tem-plate approach[REF_CITE]."
"The key elements of this approach are the alignment templates, which are pairs of source and target language phrases to-gether with an alignment between the words within the phrases."
The advantage of the alignment tem-plate approach compared to single word-based sta-tistical translation models is that word context and local changes in word order are explicitly consid-ered.
The alignment template model refines the transla-tion probability P r(f 1J |e I1 ) by introducing two hid-den variables z 1K and a K1 for the K alignment tem-plates and the alignment of the alignment templates:
X P r(f 1J |e I1 ) =
"P r(a K1 |e I1 ) · z 1K ,a K1 P r(z 1K |a K1 , e I1 ) · P r(f 1J |z 1K , a K1 , e I1 )"
"Hence, we obtain three different probability distributions: P r(a 1K |e I1 ), P r(z 1K |a K1 , e I1 ) and Pr(f 1J |z 1K , a K1 , e I1 )."
"Here, we omit a detailed de-scription of modeling, training and search, as this is not relevant for the subsequent exposition."
"For fur-ther details, see[REF_CITE]."
"To use these three component models in a direct maximum entropy approach, we define three dif-ferent feature functions for each component of the translation model instead of one feature function for the whole translation model p(f 1J |e I1 )."
"The feature functions have then not only a dependence on f 1J and e I1 but also on z 1K , a K1 ."
"So far, we use the logarithm of the components of a translation model as feature functions."
This is a very convenient approach to improve the quality of a baseline system.
"Yet, we are not limited to train only model scaling factors, but we have many possi-bilities: • We could add a sentence length feature: h(f 1J , e I1 ) ="
"This corresponds to a word penalty for each produced target word. • We could use additional language models by using features of the following form: h(f 1J , e I1 ) = h(e I1 ) • We could use a feature that counts how many entries of a conventional lexicon co-occur in the given sentence pair."
"Therefore, the weight for the provided conventional dictionary can be learned."
"The intuition is that the conventional dictionary is expected to be more reliable than the automatically trained lexicon and therefore should get a larger weight. • We could use lexical features, which fire if a certain lexical relationship (f, e) occurs:  "
"X J X I h(f 1J , e I1 ) =  δ(f, f j )  · δ(e, e i ) j=1 i=1 • We could use grammatical features that relate certain grammatical dependencies of source and target language."
"For example, using a func-tion k(·) that counts how many verb groups ex-ist in the source or the target sentence, we can define the following feature, which is 1 if each of the two sentences contains the same number of verb groups: h(f 1J , e I1 ) = δ(k(f J1 ), k(e I1 )) (12)"
"In the same way, we can introduce semantic features or pragmatic features such as the di-alogue act classification."
We can use numerous additional features that deal with specific problems of the baseline statistical MT system.
"In this paper, we shall use the first three of these features."
"As additional language model, we use a class-based five-gram language model."
This feature and the word penalty feature allow a straight-forward integration into the used dynamic program-ming search algorithm[REF_CITE].
"As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature."
"To train the model parameters λ M1 of the direct trans-lation model according to Eq. 11, we use the GIS (Generalized Iterative Scaling) algorithm[REF_CITE]."
"It should be noted that, as was already shown[REF_CITE], by applying suitable transformations, the GIS algo-rithm is able to handle any type of real-valued fea-tures."
"To apply this algorithm, we have to solve var-ious practical problems."
"The renormalization needed in Eq. 8 requires a sum over a large number of possible sentences, for which we do not know an efficient algorithm."
"Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences."
The set of considered sentences is computed by an appropriately extended version of the used search algorithm[REF_CITE]computing an approximate n-best list of trans-lations.
"Unlike automatic speech recognition, we do not have one reference sentence, but there exists a num-ber of reference sentences."
"Yet, the criterion as it is described in Eq. 11 allows for only one reference translation."
"Hence, we change the criterion to al-low R s reference translations e s,1 , . . . , e s,R s for the sentence e s : ( ) X S 1 X R s λ̂ M1 = argmax log p λ M (e s,r |f s ) s=1"
R s r=1 1 λ M1
We use this optimization criterion instead of the op-timization criterion shown in Eq. 11.
"In addition, we might have the problem that no single of the reference translations is part of the n-best list because the search algorithm performs prun-ing, which in principle limits the possible transla-tions that can be produced given a certain input sen-tence."
"To solve this problem, we define for max-imum entropy training each sentence as reference translation that has the minimal number of word er-rors with respect to any of the reference translations."
"We present results on the V ERBMOBIL task, which is a speech translation task in the domain of appoint-ment scheduling, travel planning, and hotel reser- vati[REF_CITE]."
Table 1 shows the cor-pus statistics of this task.
"We use a training cor-pus, which is used to train the alignment template model and the language models, a development cor-pus, which is used to estimate the model scaling fac-tors, and a test corpus."
"Table 1: Characteristics of training corpus (Train), manual lexicon (Lex), development corpus (Dev), test corpus (Test)."
"So far, in machine translation research does not exist one generally accepted criterion for the evalu-ation of the experimental results."
"Therefore, we use a large variety of different criteria and show that the obtained results improve on most or all of these cri-teria."
"In all experiments, we use the following six error criteria: • SER (sentence error rate): The SER is com-puted as the number of times that the generated sentence corresponds exactly to one of the ref-erence translations used for the maximum en-tropy training. • WER (word error rate): The WER is computed as the minimum number of substitution, inser-tion and deletion operations that have to be per-formed to convert the generated sentence into the target sentence. • PER (position-independent WER): A short-coming of the WER is the fact that it requires a perfect word order."
"The word order of an acceptable sentence can be different from that of the target sentence, so that the WER mea-sure alone could be misleading."
"To overcome this problem, we introduce as additional mea-sure the position-independent word error rate (PER)."
"This measure compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, there is not only used a sin-gle reference translation, as for the WER, but a whole set of reference translations."
"For each translation hypothesis, the edit distance to the most similar sentence is calculated[REF_CITE]. • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference trans-lations with a penalty for too short sentences[REF_CITE]."
"Unlike all other eval-uation criteria used here, BLEU measures ac-curacy, i.e. the opposite of error rate."
"Hence, large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary."
Each trans-lated sentence was judged by a human exam-iner according to an error scale from 0.0 to 1.0[REF_CITE]. • IER (information item error rate): The test sen-tences are segmented into information items.
"For each of them, if the intended information is conveyed and there are no syntactic errors, the sentence is counted as correct[REF_CITE]."
"In the following, we present the results of this ap-proach."
Table 2 shows the results if we use a direct translation model (Eq. 6).
"As baseline features, we use a normal word tri-gram language model and the three component mod-els of the alignment templates."
The first row shows the results using only the four baseline features with λ 1 = ··· = λ 4 = 1.
The second row shows the result if we train the model scaling factors.
We see a systematic improvement on all error rates.
"The fol-lowing three rows show the results if we add the word penalty, an additional class-based five-gram language model and the conventional dictionary fea-tures."
We observe improved error rates for using the word penalty and the class-based language model as additional features.
Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm.
We see that the sentence error rates converges after about 4000 iterations.
We do not observe significant overfitting.
Table 3 shows the resulting normalized model scaling factors.
Multiplying each model scaling fac-tor by a constant positive value does not affect the decision rule.
We see that adding new features also has an effect on the other model scaling factors.
"The use of direct maximum entropy translation mod-els for statistical machine translation has been sug- 



 gested[REF_CITE]."
They train models for natural language un-derstanding rather than natural language translation.
"In contrast to their approach, we include a depen-dence on the hidden variable of the translation model in the direct translation model."
"Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems."
"In speech recognition, training the parameters of the acoustic model by optimizing the (average) mu-tual information and conditional entropy as they are defined in information theory is a standard approach[REF_CITE]."
Combining various probabilistic models for speech and language mod-eling has been suggested[REF_CITE].
"We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach."
It allows a baseline MT system to be extended easily by adding new feature functions.
We have shown that a base-line statistical MT system can be significantly im-proved using this framework.
"There are two possible interpretations for a statis-tical MT system structured according to the source-channel approach, hence including a model for Pr(e I1 ) and a model for Pr(f 1J |e I1 )."
We can inter-pret it as an approximation to the Bayes decision rule in Eq. 2 or as an instance of a direct maximum en-tropy model with feature functions log Pr(e I1 ) and log Pr(f 1J |e I1 ).
"As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation."
"Yet, the main advantage comes from the large number of additional possibilities that we obtain by using the second interpretation."
An important open problem of this approach is the handling of complex features in search.
An in-teresting question is to come up with features that allow an efficient handling using conventional dy-namic programming search algorithms.
"In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recogniti[REF_CITE]."
This paper describes a decoding algorithm for a syntax-based translation model[REF_CITE].
The model has been extended to incorporate phrasal translations as presented here.
"In con-trast to a conventional word-to-word sta-tistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign lan-guage."
"As the model size becomes huge in a practical setting, and the decoder consid-ers multiple syntactic structures for each word alignment, several pruning tech-niques are necessary."
"We tested our de-coder in a Chinese-to-English translation system, and obtained better results than IBM Model 4."
We also discuss issues con-cerning the relation between this decoder and a language model.
"A statistical machine translation system based on the noisy channel model consists of three components: a language model (LM), a translation model (TM), and a decoder."
"For a system which translates from a foreign language to English , the LM gives a prior probability P  and the TM gives a chan-nel translation probability P  ."
These models are automatically trained using monolingual (for the LM) and bilingual (for the TM) corpora.
"A decoder then finds the best English sentence given a foreign sentence that maximizes P  , which also maxi-mizes P    according to Bayes’ rule."
A different decoder is needed for different choices of LM and TM.
"Since P  and P  are not sim-ple probability tables but are parameterized models, a decoder must conduct a search over the space de-fined by the models."
"For the IBM models defined by a pioneering paper[REF_CITE], a de-coding algorithm based on a left-to-right search was described[REF_CITE]."
"Recently[REF_CITE]introduced a syntax-based TM which utilized syntactic structure in the chan-nel input, and showed that it could outperform the IBM model in alignment quality."
"In contrast to the IBM models, which are word-to-word models, the syntax-based model works on a syntactic parse tree, so the decoder builds up an English parse tree given a sentence in a foreign language."
"This pa-per describes an algorithm for such a decoder, and reports experimental results."
Other statistical machine translation systems such[REF_CITE]and[REF_CITE]also pro-duce a tree given a sentence .
"Their models are based on mechanisms that generate two languages at the same time, so an English tree is obtained as a subproduct of parsing ."
"However, their use of the LM is not mathematically motivated, since their models do not decompose into P  and  unlike the noisy channel model."
"Section 2 briefly reviews the syntax-based TM, and Section 3 describes phrasal translation as an ex-tension."
Section 4 presents the basic idea for de-coding.
"As in other statistical machine translation systems, the decoder has to cope with a huge search space."
Section 5 describes how to prune the search space for practical decoding.
Section 6 shows exper-imental results.
"Section 7 discusses LM issues, and is followed by conclusions."
The syntax-based TM defined[REF_CITE]assumes an English parse tree as a channel input.
"The channel applies three kinds of stochastic operations on each node  : reordering children nodes ( ), inserting an optional extra word to the left or right of the node ( ), and translating leaf words ( ). 1"
"These operations are independent of each other and are conditioned on the features ( , , ) of the node."
Figure 1 shows an example.
The child node sequence of the top node VB is re-ordered from PRP-VB1-VB2 into PRP-VB2-VB1 as seen in the second tree (Reordered).
An extra word ha is inserted at the leftmost node PRP as seen in the third tree (Inserted).
The English word He un-der the same node is translated into a foreign word kare as seen in the fourth tree (Translated).
"After these operations, the channel emits a foreign word sentence by taking the leaves of the modified tree."
"Formally, the channel probability P   is 8 8 78 6 P  ! &quot;$# % P &lt;; = &quot; &amp; (*),-+ /) .102023548 8 3:9 8 8 8 8 8 8 8 8 8 @?H  BJ &quot;DEC&quot; EC&lt;F&lt;F G G &quot; &quot; ifotherwise = is terminal P &lt;; = $&quot; # &gt; where K  [N\^] , L *M  Q  Q  Q ] U  U  U ] , and _ is a se-quence of leaf words of a tree transformed by K from ."
"The model tables   ,   , and   are called the r-table, n-table, and t-table, respectively."
"These tables contain the probabilities of the channel operations ( , , ) conditioned by the features ( , , )."
"In Figure 1, the r-table specifies the prob-ability of having the second tree (Reordered) given the first tree."
The n-table specifies the probability of having the third tree (Inserted) given the second tree.
The t-table specifies the probability of having the fourth tree (Translated) given the third tree.
The probabilities in the model tables are automat-ically obtained by an EM-algorithm using pairs of (channel input) and (channel output) as a training corpus.
"Usually a bilingual corpus comes as pairs of translation sentences, so we need to parse the cor-pus."
"As we need to parse sentences on the channel input side only, many X-to-English translation sys-tems can be developed with an English parser alone."
"The conditioning features ( , , ) can be any-thing that is available on a tree , however they should be carefully selected not to cause data-sparseness problems."
"Also, the choice of fea-tures may affect the decoding algorithm."
"In our experiment, a sequence of the child node label was used for , a pair of the node label and the parent label was used for , and the identity of the English word is used for ."
"For exam-ple,   P PRP-VB2-VB1 PRP-VB1-VB2 for the top node in Figure 1."
"Similarly for the node PRP,  P right, ha VB-PRP and   P kare he ."
More detailed examples are found[REF_CITE].
"In[REF_CITE], the translation is a 1-to-1 lexical translation from an English word o to a foreign word p , i.e.,  \  ."
"To allow non 1-to-1 translation, such as for idiomatic phrases or compound nouns, we extend the model as follows."
First we use fertility t as used in IBM models to allow [Footnote_1]-to-N mapping. 8 78 | 9 ?  Bs&quot;$# ?   v1x:yzyzy{vP| Z} &quot;Y# ~&lt; Z} &quot; 3u9 ?  Z} &quot;
1 The channel operations are designed to model the differ-ence in the word order (SVO for English vs. VSO for Arabic) and case-marking schemes (word positions in English vs. case-marker particles in Japanese).
"For N-to-N mapping, we allow direct transla-tion  of an English phrase  to a foreign phrase [p  at non-terminal tree nodes as 9 9 5 &lt; k&quot;$# ?   v x yzyzy{v | } } x yzyzy{} 8 &quot; | ~ &lt;} 9 } x y yzy}Pb&quot; 78 9 # 3 ?  } } x yzyzy}Pb&quot;  : and linearly mix this phrasal translation with the word-to-word 8 8 translation 8 8 , i.e., 8 8 8 8 P &lt;; = &quot;$# 5S 5 &lt;  &quot; &quot; H   &quot;&lt;F  &quot; if  is non-terminal."
"In practice, the phrase lengths ( À , Á ) are limited to reduce the model size."
"In our ex-periment Æ (Section 5), we restricted them as ÂT&lt;\Â ÁÄÃÅÇÆ"
"À  , to avoid pairs of extremely differ-ent lengths."
This formula was obtained by randomly sampling the length of translation pairs.
"Our statistical MT system is based on the noisy-channel model, so the decoder works in the reverse direction of the channel."
"Given a supposed chan-nel output (e.g., a French or Chinese sentence), it will find the most plausible channel input (an En-glish parse tree) based on the model parameters and the prior probability of the input."
"In the syntax-based model, the decoder’s task is to find the most plausible English parse tree given an observed foreign sentence."
"Since the task is to build a tree structure from a string of words, we can use a mechanism similar to normal parsing, which builds an English parse tree from a string of English words."
"Here we need to build an English parse tree from a string of foreign (e.g., French or Chinese) words."
"To parse in such an exotic way, we start from an English context-free grammar obtained from the training corpus, [Footnote_2] and extend the grammar to in- corporate the channel operations in the translation model."
2 The training corpus for the syntax-based model consists of
"For each non-lexical rule in the original En-glish grammar (such as “VP Ì VB NP PP”), we supplement it with reordered rules (e.g. “VP Ì NP PP VB”, “VP Ì NP VB PP ”, etc.) and asso-ciate them with the original English order and the reordering probability from the r-table."
"Similarly, rules such as “VP Ì VP X” and “X Ì word” are added for extra word insertion, and they are associ-ated with a probability from the n-table."
"For each lexical rule in the English grammar, we add rules such as “englishWord Ì foreignWord” with a prob-ability from the t-table."
"Now we can parse a string of foreign words and build up a tree, which we call a decoded tree."
An example is shown in Figure 2.
The decoded tree is built up in the foreign language word order.
"To ob-tain a tree in the English order, we apply the reverse of the reorder operation (back-reordering) using the information associated to the rule expanded by the r-table."
"In Figure 2, the numbers in the dashed oval near the top node shows the original english order."
"Then, we obtain an English parse tree by remov-ing the leaf nodes (foreign words) from the back-reordered tree."
"Among the possible decoded trees, we pick the best tree in which the product of the LM probability (the prior probability of the English tree) and the TM probability (the probabilities associated pairs of English parse trees and foreign sentences. with the rules in the decoded tree) is the highest."
The use of an LM needs consideration.
Theoret-ically we need an LM which gives the prior prob-ability of an English parse tree.
"However, we can approximate it with an n-gram LM, which is well-studied and widely implemented."
We will discuss this point later in Section 7.
"If we use a trigram model for the LM, a con-venient implementation is to first build a decoded-tree forest and then to pick out the best tree using a trigram-based forest-ranking algorithm as described[REF_CITE]."
"The ranker uses two leftmost and rightmost leaf words to efficiently calculate the trigram probability of a subtree, and finds the most plausible tree according to the trigram and the rule probabilities."
This algorithm finds the optimal tree in terms of the model probability — but it is not practical when the vocabulary size and the rule size grow.
The next section describes how to make it practical.
We use our decoder for Chinese-English translation in a general news domain.
The TM becomes very huge for such a domain.
"In our experiment (see Sec-tion 6 for details), there are about 4M non-zero en-tries in the trained   table."
"We applied the simple al-gorithm from Section 4, but this experiment failed — no complete translations were produced."
Even four-word sentences could not be decoded.
"This is not only because the model size is huge, but also be-cause the decoder considers multiple syntactic struc-tures for the same word alignment, i.e., there are several different decoded trees even when the trans-lation of the sentence is the same."
We then applied the following measures to achieve practical decod-ing.
"The basic idea is to use additional statistics from the training corpus. beam search: We give up optimal decoding by using a standard dynamic-programming parser with beam search, which is similar to the parser used[REF_CITE]."
"A standard dynamic-programming parser builds up õ nonterminal, input-substring ö tuples from bottom-up according to the grammar rules."
"When the parsing cost [Footnote_3] comes only from the features within a subtree (TM cost, in our case), the parser will find the optimal tree by keep-ing the single best subtree for each tuple."
3 rule-cost = h ÿ (rule-probability)
"When the cost depends on the features outside of a subtree, we need to keep all the subtrees for possible differ-ent outside features (boundary words for the trigram LM cost) to obtain the optimal tree."
"Instead of keep-ing all the subtrees, we only retain subtrees within a beam width for each input-substring."
"Since the out-side features are not considered for the beam prun-ing, the optimality of the parse is not guaranteed, but the required memory size is reduced. t-table pruning: Given a foreign (Chinese) sen-tence to the decoder, we only consider English words o for each foreign word p such that P   is high ø ."
"In addition, only limited part-of-speech labels are considered to reduce the number of possible decoded-tree ø structures."
"Thus we only use the top-5 ( o , ) pairs ranked by"
"P &lt;}\^ v5û&quot; # P &quot; P &lt;} úz&quot; P  }\ùaúzaü&quot; P &quot; ý P &quot; P &lt;} úz&quot; P Z} &quot;þy ø Notice that ø P   is a model parameter, and that P and P  are obtained from the parsed training corpus. phrase pruning: We only consider limited pairs (  , p[ ) for phrasal translation (see"
The pair must appear more than once in the Viterbi alignments [Footnote_4] of the training corpus.
4 Viterbi alignment is the most probable word alignment ac-cording to the trained TM tables.
"Then we use the top-10 pairs ranked similarly ø to ø t-table pruning above, except we replace P P  with P  and use trigrams to estimate P  ."
"By this prun-ing, we effectively remove junk phrase pairs, most of which come from misaligned sentences or untrans-lated phrases in the training corpus. r-table pruning: To reduce the number of rules for the decoding grammar, we use the top-N  rules ranked by P rule P reord so that N P rule P reord @  ö uT , where P rule is a prior probability of the rule (in the original En-glish order) found in the parsed English corpus, and P reord is the reordering probability in the TM."
The product is a rough estimate of how likely a rule is used in decoding.
"Because only a limited number of reorderings are used in actual translation, a small number of rules are highly probable."
"In fact, among a total of 138,662 reorder-expanded rules, the most likely 875 rules contribute 95% of the probability mass, so discarding the rules which contribute the lower 5% of the probability mass efficiently elimi-nates more than 99% of the total rules. zero-fertility words: An English word may be translated into a null (zero-length) foreign word."
"This happens when the fertility     ö , and such English word o (called a zero-fertility word) must be inserted during the decoding."
"The decoding parser is modified to allow inserting zero-fertility words, but unlimited insertion easily blows up the memory space."
Therefore only limited insertion is allowed.
"Observing the Viterbi alignments of the training cor-pus, the top-20 frequent zero-fertility words [Footnote_5] cover over 70% of the cases, thus only those are allowed to be inserted."
"5 They are the, to, of, a, in, is, be, that, on, and, are, for, will, with, have, it, ’s, has, i, and by."
Also we use syntactic context to limit the insertion.
"For example, a zero-fertility word in is inserted as IN when “PP Ì IN NP-A” rule is applied."
"Again, observing the Viterbi alignments, the top-20 frequent contexts cover over 60% of the cases, so we allow insertions only in these contexts."
This kind of context sensitive insertion is possible because the decoder builds a syntactic tree.
Such se-lective insertion by syntactic context is not easy for a word-for-word based IBM model decoder.
"The pruning techniques shown above use extra ø statistics ø from the training corpus, such as P , P  , and P rule ."
"These statistics may be consid-ered as a part of the LM P  , and such syntactic probabilities are essential when we mainly use tri-grams for the LM."
"In this respect, the pruning is use-ful not only for reducing the search space, but also improving the quality of translation."
"We also use statistics from the Viterbi alignments, such as the phrase translation frequency and the zero-fertility context frequency."
These are statistics which are not modeled in the TM.
"The frequency count is essen-tially a joint probability P   , while the TM uses a conditional probability P   ."
Utilizing statistics outside of a model is an important idea for statis-tical machine translation in general.
"For example, a decoder[REF_CITE]uses alignment template statistics found in the Viterbi alignments."
This section describes results from our experiment using the decoder as described in the previous sec-tion.
We used a Chinese-English translation corpus for the experiment.
"After discarding long sentences (more than 20 words in English), the English side of the corpus consisted of about 3M words, and it was parsed with Collins’ parser[REF_CITE]."
Train-ing the TM took about 8 hours using a 54-node unix cluster.
Table 1 shows the decoding performance for the test sentences.
"The first system ibm4 is a reference system, which is based on IBM Model4."
The second and the third (syn and syn-nozf) are our decoders.
"Both used the same decoding algorithm and prun-ing as described in the previous sections, except that syn-nozf allowed no zero-fertility insertions."
The average decoding speed was about 100 seconds 6 per sentence for both syn and syn-nozf.
"As an overall decoding performance measure, we used the BLEU metric[REF_CITE]."
"This measure is a geometric average of n-gram accu-racy, adjusted by a length penalty factor LP. [Footnote_7] The n-gram accuracy (in percentage) is shown in Table 1 as P1/P2/P3/P4 for unigram/bigram/trigram/4-gram."
7 BLEU # 6 3u9 !&quot;  1GB memory.
"Overall, our decoder performed better than the IBM system, as indicated by the higher BLEU score."
"We obtained better n-gram accuracy, but the lower LP score penalized the overall score."
"Interestingly, the system with no explicit zero-fertility word insertion (syn-nozf) performed better than the one with zero-fertility insertion (syn)."
"It seems that most zero-fertility words were already included in the phrasal translations, and the explicit zero-fertility word in-sertion produced more garbage than expected words."
"To verify that the pruning was effective, we re-laxed the pruning threshold and checked the decod-ing coverage for the first 92 sentences of the test data."
Table 2 shows the result.
"On the left, the r-table pruning was relaxed from the 95% level to 98% or 100%."
"On the right ø , the t-table pruning was relaxed from the top-5 ( o , ) pairs to the top-10 or top-20 pairs."
The system r95 and w5 are identical to syn-nozf in Table 1.
"When r-table pruning was relaxed from 95% to 98%, only about half (47/92) of the test sentences were decoded, others were aborted due to lack of memory."
"When it was further relaxed to 100% (i.e., no pruning was done), only 20 sentences were de-coded."
"Similarly, when the t-table pruning threshold was relaxed, fewer sentences could be decoded due to the memory limitations."
"Although our decoder performed better than the  [Footnote_6] &quot; H $ ü - # &quot; &amp;# % H , and LP #  if (# [Footnote_6] &apos; H , where 6 # Pü ) ) +* LP."
6 Using a single-[REF_CITE]Mhz Pentium III unix system with
6 Using a single-[REF_CITE]Mhz Pentium III unix system with
"LP # # if , # , is the system output length, and H is the reference length."
"IBM system in the BLEU score, the obtained gain was less than what we expected."
We have thought the following three reasons.
"First, the syntax of Chi-nese is not extremely different from English, com-pared with other languages such as Japanese or Ara-bic."
"Therefore, the TM could not take advantage of syntactic reordering operations."
"Second, our de-coder looks for a decoded tree, not just for a de-coded sentence."
"Thus, the search space is larger than IBM models, which might lead to more search errors caused by pruning."
"Third, the LM used for our sys-tem was exactly the same as the LM used by the IBM system."
Decoding performance might be heavily in-fluenced by LM performance.
"In addition, since the TM assumes an English parse tree as input, a trigram LM might not be appropriate."
We will discuss this point in the next section.
Phrasal translation worked pretty well.
Figure 3 shows the top-20 frequent phrase translations ob-served in the Viterbi alignment.
The leftmost col-umn shows how many times they appeared.
Most of them are correct.
"It even detected frequent sentence-to-sentence translations, since we only imposed a relative length limit for phrasal translations (Sec-tion 3)."
"However, some of them, such as the one with (in cantonese), are wrong."
"We expected that these junk phrases could be eliminated by phrase pruning (Section 5), however the junk phrases present many times in the corpus were not effectively filtered out."
The BLEU score measures the quality of the decoder output sentences.
We were also interested in the syn-tactic structure of the decoded trees.
The leftmost tree in Figure 4 is a decoded tree from the syn-nozf system.
"Surprisingly, even though the decoded sen-tence is passable English, the tree structure is totally unnatural."
We assumed that a good parse tree gives high trigram probabilities.
But it seems a bad parse tree may give good trigram probabilities too.
We also noticed that too many unary rules (e.g. “NPB Ì PRN”) were used.
This is because the reordering probability is always 1.
"To remedy this, we added CFG probabilities (PCFG) in the decoder search, i.e., it now looks for a tree which maximizes P trigram P cfg P TM ."
The CFG probability was obtained by counting the rule frequency in the parsed English side of the train-ing corpus.
The middle of Figure 4 is the output for the same sentence.
"The syntactic structure now looks better, but we found three problems."
"First, the BLEU score is worse (0.078)."
"Second, the decoded trees seem to prefer noun phrases."
"In many trees, an entire sentence was decoded as a large noun phrase."
"Third, it uses more frequent node reordering than it should."
The BLEU score may go down because we weighed the LM (trigram and PCFG) more than the TM.
"For the problem of too many noun phrases, we thought it was a problem with the corpus."
"Our train-ing corpus contained many dictionary entries, and the parliament transcripts also included a list of par-ticipants’ names."
This may cause the LM to prefer noun phrases too much.
Also our corpus contains noise.
There are two types of noise.
"One is sentence alignment error, and the other is English parse error."
"The corpus was sentence aligned by automatic soft-ware, so it has some bad alignments."
"When a sen-tence was misaligned, or the parse was wrong, the Viterbi alignment becomes an over-reordered tree as it picks up plausible translation word pairs first and reorders trees to fit them."
"To see if it was really a corpus problem, we se-lected a good portion of the corpus and re-trained the r-table."
"To find good pairs of sentences in the corpus, we used the following: 1) Both English and Chinese sentences end with a period. 2) The En- glish word is capitalized at the beginning. 3) The sentences do not contain symbol characters, such as colon, dash etc, which tend to cause parse errors. 4) The Viterbi-ratio [Footnote_8] is more than the average of the pairs which satisfied the first three conditions."
8 Viterbi-ratio is the ratio of the probability of the most plau-sible alignment with the sum of the probabilities of all the align-ments. Low Viterbi-ratio is a good indicator of misalignment or parse error.
"Using the selected sentence pairs, we retrained only the r-table and the PCFG."
The rightmost tree in Figure 4 is the decoded tree using the re-trained TM.
"The BLEU score was improved (0.085), and the tree structure looks better, though there are still problems."
An obvious problem is that the goodness of syntactic structure depends on the lexical choices.
"For example, the best syntactic structure is different if a verb requires a noun phrase as object than it is if it does not."
The PCFG-based LM does not handle this.
"At this point, we gave up using the PCFG as a component of the LM."
Using only trigrams obtains the best result for the BLEU score.
"However, the BLEU metric may not be affected by the syntac-tic aspect of translation quality, and as we saw in Figure 4, we can improve the syntactic quality by introducing the PCFG using some corpus selection techniques."
"Also, the pruning methods described in Section 5 use syntactic statistics from the training corpus."
"Therefore, we are now investigating more sophisticated LMs such[REF_CITE]which incorporate syntactic features and lexical informa-tion."
We have presented a decoding algorithm for a syntax-based statistical machine translation.
The translation model was extended to incorporate phrasal translations.
"Because the input of the chan-nel model is an English parse tree, the decoding al-gorithm is based on conventional syntactic parsing, and the grammar is expanded by the channel oper-ations of the TM."
"As the model size becomes huge in a practical setting, and the decoder considers mul-tiple syntactic structures for a word alignment, effi-cient pruning is necessary."
We applied several prun-ing techniques and obtained good decoding quality and coverage.
The choice of the LM is an impor-tant issue in implementing a decoder for the syntax-based TM.
"At present, the best result is obtained by using trigrams, but a more sophisticated LM seems promising."
Human evaluations of machine translation are extensive but expensive.
Human eval-uations can take months to finish and in-volve human labor that can not be reused.
"We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run."
We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. [Footnote_1]
"1 So we call our method the bilingual evaluation understudy, B LEU ."
"Human evaluations of machine translation (MT) weigh many aspects of translation, including ade-quacy, fidelity , and fluency of the translation ([REF_CITE]; White and O’[REF_CITE])."
A compre-hensive catalog of MT evaluation techniques and their rich literature is given[REF_CITE].
"For the most part, these various human evaluation ap-proaches are quite expensive[REF_CITE]."
"More-over, they can take weeks or months to finish."
This is a big problem because developers of machine trans-lation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas.
We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from the evaluation bottleneck.
"Developers would bene-fit from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation."
We propose such an evalua-tion method in this paper.
How does one measure translation performance?
"The closer a machine translation is to a professional human translation, the better it is."
This is the cen-tral idea behind our proposal.
"To judge the quality of a machine translation, one measures its closeness to one or more reference human translations accord-ing to a numerical metric."
"Thus, our MT evaluation system requires two ingredients: 1. a numerical “translation closeness” metric 2. a corpus of good quality human reference trans-lations"
"We fashion our closeness metric after the highly suc-cessful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for le-gitimate differences in word choice and word or-der."
The main idea is to use a weighted average of variable length phrase matches against the reference translations.
This view gives rise to a family of met-rics using various weighting schemes.
We have se-lected a promising baseline metric from this family.
"In Section 2, we describe the baseline metric in detail."
"In Section 3, we evaluate the performance of B LEU ."
"In Section 4, we describe a human evaluation experiment."
"In Section 5, we compare our baseline metric performance with human evaluations."
"Typically, there are many “perfect” translations of a given source sentence."
These translations may vary in word choice or in word order even when they use the same words.
And yet humans can clearly dis-tinguish a good translation from a bad one.
"For ex-ample, consider these two candidate translations of a Chinese source sentence:"
Candidate 1: It is a guide to action which ensures that the military always obeys the commands of the party.
Candidate 2: It is to insure the troops forever hearing the activity guidebook that party direct.
"Although they appear to be on the same subject, they differ markedly in quality."
"For comparison, we pro-vide three reference human translations of the same sentence below."
Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.
Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.
Reference 3: It is the practical guide for the army always to heed the directions of the party.
"It is clear that the good translation, Candidate 1, shares many words and phrases with these three ref-erence translations, while Candidate 2 does not."
We will shortly quantify this notion of sharing in Sec-tion 2.1.
"But first observe that Candidate 1 shares &quot;It is a guide to action&quot; with Reference 1, &quot;which&quot; with Reference 2, &quot;ensures that the military&quot; with Reference 1, &quot;always&quot; with Ref-erences 2 and 3, &quot;commands&quot; with Reference 1, and finally &quot;of the party&quot; with Reference 2 (all ig-noring capitalization)."
"In contrast, Candidate 2 ex-hibits far fewer matches, and their extent is less."
It is clear that a program can rank Candidate 1 higher than Candidate 2 simply by comparing n-gram matches between each candidate translation and the reference translations.
"Experiments over large collections of translations presented in Section 5 show that this ranking ability is a general phe-nomenon, and not an artifact of a few toy examples."
The primary programming task for a B LEU imple-mentor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches.
These matches are position-independent.
"The more the matches, the better the candidate translation is."
"For simplicity, we first fo-cus on computing unigram matches."
The cornerstone of our metric is the familiar pre-cision measure.
"To compute precision, one simply counts up the number of candidate translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation."
"Unfortunately, MT sys-tems can overgenerate “reasonable” words, result-ing in improbable, but high-precision, translations like that of example 2 below."
Intuitively the prob-lem is clear: a reference word should be considered exhausted after a matching candidate word is iden-tified.
We formalize this intuition as the modified unigram precision.
"To compute this, one first counts the maximum number of times a word occurs in any single reference translation."
"Next, one clips the to-tal count of each candidate word by its maximum reference count, 2 adds these clipped counts up, and divides by the total (unclipped) number of candidate words."
"2 Count clip = min(Count,Max Ref Count). In other words, one truncates each word’s count, if necessary, to not exceed the largest count observed in any single reference for that word."
Candidate: the the the the the the the.
Reference 1: The cat is on the mat.
Reference 2: There is a cat on the mat.
Modified Unigram Precision = 2/7. [Footnote_3]
"3 As a guide to the eye, we have underlined the important words for computing modified precision."
"In Example 1, Candidate 1 achieves a modified unigram precision of 17/18; whereas Candidate 2 achieves a modified unigram precision of 8/14."
"Similarly, the modified unigram precision in Exam-ple 2 is 2/7, even though its standard unigram pre-cision is 7/7."
Modified n-gram precision is computed similarly for any n: all candidate n-gram counts and their corresponding maximum reference counts are col-lected.
"The candidate counts are clipped by their corresponding reference maximum value, summed, and divided by the total number of candidate n-grams."
"In Example 1, Candidate 1 achieves a mod-ified bigram precision of 10/17, whereas the lower quality Candidate 2 achieves a modified bigram pre-cision of 1/13."
"In Example 2, the (implausible) can-didate achieves a modified bigram precision of 0."
This sort of modified n-gram precision scoring cap-tures two aspects of translation: adequacy and flu-ency.
A translation using the same words (1-grams) as in the references tends to satisfy adequacy.
The longer n-gram matches account for fluency. [Footnote_4]
"4 B LEU only needs to match human judgment when averaged over a test corpus; scores on individual sentences will often vary from human judgments. For example, a system which produces the fluent phrase “East Asian economy” is penalized heavily on the longer n-gram precisions if all the references happen to read “economy of East Asia.” The key to B LEU ’s success is that all systems are treated similarly and multiple human translators with different styles are used, so this effect cancels out in com-parisons between systems."
How do we compute modified n-gram precision on a multi-sentence test set?
"Although one typically evaluates MT systems on a corpus of entire docu-ments, our basic unit of evaluation is the sentence."
"A source sentence may translate to many target sen-tences, in which case we abuse terminology and re-fer to the corresponding target sentences as a “sen-tence.”"
We first compute the n-gram matches sen-tence by sentence.
"Next, we add the clipped n-gram counts for all the candidate sentences and divide by the number of candidate n-grams in the test corpus to compute a modified precision score,p n , for the entire test corpus. p n = ∑ ∑ Count clip (n-gram) C ∈{Candidates} n-gram ∈ C ∑ Count(n-gram 0 ) .∑"
C 0 ∈{Candidates} n-gram 0 ∈ C 0
"To verify that modified n-gram precision distin-guishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human transla-tor and a standard (poor) machine translation system using [Footnote_4] reference translations for each of 127 source sentences."
"4 B LEU only needs to match human judgment when averaged over a test corpus; scores on individual sentences will often vary from human judgments. For example, a system which produces the fluent phrase “East Asian economy” is penalized heavily on the longer n-gram precisions if all the references happen to read “economy of East Asia.” The key to B LEU ’s success is that all systems are treated similarly and multiple human translators with different styles are used, so this effect cancels out in com-parisons between systems."
The average precision results are shown in Figure 1.
The strong signal differentiating human (high pre-cision) from machine (low precision) is striking.
The difference becomes stronger as we go from un-igram precision to 4-gram precision.
It appears that any single n-gram precision score can distinguish between a good translation and a bad translation.
"To be useful, however, the metric must also reliably distinguish between translations that do not differ so greatly in quality."
"Furthermore, it must distinguish between two human translations of differing quality."
This latter requirement ensures the continued valid-ity of the metric as MT approaches human transla-tion quality.
"To this end, we obtained a human translation by someone lacking native proficiency in both the source (Chinese) and the target language (English)."
"For comparison, we acquired human translations of the same documents by a native English speaker."
We also obtained machine translations by three commer-cial systems.
These five “systems” — two humans and three machines — are scored against two refer-ence professional human translations.
The average modified n-gram precision results are shown in Fig-ure 2.
"Each of these n-gram statistics implies the same ranking: H2 (Human-2) is better than H1 (Human-1), and there is a big drop in quality between H1 and S3 (Machine/System-3)."
S3 appears better than S2 which in turn appears better than S1.
"Remarkably, this is the same rank order assigned to these “sys-tems” by human judges, as we discuss later."
"While there seems to be ample signal in any single n-gram precision, it is more robust to combine all these sig-nals into a single number metric."
How should we combine the modified precisions for the various n-gram sizes?
A weighted linear av-erage of the modified precisions resulted in encour-aging results for the [Footnote_5] systems.
"5 The geometric average is harsh if any of the modified pre-cisions vanish, but this should be an extremely rare event in test corpora of reasonable size (for N max ≤ 4)."
"However, as can be seen in Figure 2, the modified n-gram precision de-cays roughly exponentially with n: the modified un-igram precision is much larger than the modified bi-gram precision which in turn is much bigger than the modified trigram precision."
A reasonable averag-ing scheme must take this exponential decay into ac-count; a weighted average of the logarithm of modi-fied precisions satisifies this requirement.
"B LEU uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modified n-gram precisions. 5,[Footnote_6] Experi-mentally, we obtain the best correlation with mono- lingual human judgments using a maximum n-gram order of 4, although 3-grams and 5-grams give com-parable results."
6 Using the geometric average also yields slightly stronger correlation with human judgments than our best results using an arithmetic average.
"A candidate translation should be neither too long nor too short, and an evaluation metric should en-force this."
"To some extent, the n-gram precision al-ready accomplishes this."
N-gram precision penal-izes spurious words in the candidate that do not ap-pear in any of the reference translations.
"Addition-ally, modified precision is penalized if a word oc-curs more frequently in a candidate translation than its maximum reference count."
This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references.
"However, modified n-gram precision alone fails to enforce the proper translation length, as is illustrated in the short, absurd example below."
Candidate: of the
Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.
Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.
Reference 3: It is the practical guide for the army always to heed the directions of the party.
"Because this candidate is so short compared to the proper length, one expects to find inflated pre-cisions: the modified unigram precision is 2/2, and the modified bigram precision is 1/1."
"Traditionally, precision has been paired with recall to overcome such length-related problems."
"However, B LEU considers multiple reference trans-lations, each of which may use a different word choice to translate the same source word."
"Further-more, a good candidate translation will only use (re-call) one of these possible choices, but not all."
"In-deed, recalling all choices leads to a bad translation."
Here is an example.
Candidate 1: I always invariably perpetu-ally do.
Candidate 2: I always do.
Reference 1: I always do.
Reference 2: I invariably do.
Reference 3: I perpetually do.
"The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate."
"Thus, naı̈ve recall computed over the set of all reference words is not a good measure."
"Admittedly, one could align the refer-ence translations to discover synonymous words and compute recall on concepts rather than words."
"But, given that reference translations vary in length and differ in word order and syntax, such a computation is complicated."
Candidate translations longer than their refer-ences are already penalized by the modified n-gram precision measure: there is no need to penalize them again.
"Consequently, we introduce a multiplicative brevity penalty factor."
"With this brevity penalty in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order."
"Note that neither this brevity penalty nor the modified n-gram precision length effect directly considers the source length; in-stead, they consider the range of reference transla-tion lengths in the target language."
We wish to make the brevity penalty 1.0 when the candidate’s length is the same as any reference trans-lation’s length.
"For example, if there are three ref-erences with lengths 12, 15, and 17 words and the candidate translation is a terse 12 words, we want the brevity penalty to be 1."
We call the closest refer-ence sentence length the “best match length.”
"One consideration remains: if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sen-tences would be punished harshly."
"Instead, we com-pute the brevity penalty over the entire corpus to al-low some freedom at the sentence level."
"We first compute the test corpus’ effective reference length, r, by summing the best match lengths for each can-didate sentence in the corpus."
"We choose the brevity penalty to be a decaying exponential in r/c, where c is the total length of the candidate translation corpus."
We take the geometric mean of the test corpus’ modified precision scores and then multiply the re-sult by an exponential brevity penalty factor.
"Cur-rently, case folding is the only text normalization performed before computing the precision."
"We first compute the geometric average of the modified n-gram precisions, p n , using n-grams up to length N and positive weights w n summing to one."
"Next, let c be the length of the candidate transla-tion and r be the effective reference corpus length."
"We compute the brevity penalty BP, 1 if c &gt; r BP = e (1−r/c) if c ≤ r ."
B LEU =
BP · exp ∑ w n log p n !.
"The ranking behavior is more immediately apparent in the log domain,"
"N r log B LEU = min(1 − ,0) + ∑ w n log p n .c n=1"
"In our baseline, we use N = 4 and uniform weights w n = 1/N."
The B LEU metric ranges from 0 to 1.
Few transla-tions will attain a score of 1 unless they are identi-cal to a reference translation.
"For this reason, even a human translator will not necessarily score 1."
"It is important to note that the more reference trans-lations per sentence there are, the higher the score is."
"Thus, one must be cautious making even “rough” comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references."
Table 1 shows the B LEU scores of the 5 systems against two refer-ences on this test corpus.
The MT systems S2 and S3 are very close in this metric.
"Hence, several questions arise:"
"To answer these questions, we divided the test cor-pus into 20 blocks of 25 sentences each, and com-puted the B LEU metric on these blocks individually."
We thus have 20 samples of the B LEU metric for each system.
"We computed the means, variances, and paired t-statistics which are displayed in Table 2."
The t-statistic compares each system with its left neighbor in the table.
"For example, t = 6 for the pair S1 and S2."
"Note that the numbers in Table 1 are the B LEU metric on an aggregate of 500 sentences, but the means in Table 2 are averages of the B LEU metric on aggregates of 25 sentences."
"As expected, these two sets of results are close for each system and dif-fer only by small finite block size effects."
"Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are sta-tistically very significant."
The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.
How many reference translations do we need?
We simulated a single-reference test corpus by ran-domly selecting one of the 4 reference translations as the single reference for each of the 40 stories.
"In this way, we ensured a degree of stylistic variation."
The systems maintain the same rank order as with multiple references.
"This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator."
We had two groups of human judges.
"The first group, called the monolingual group, consisted of 10 native speakers of English."
"The second group, called the bilingual group, consisted of 10 native speakers of Chinese who had lived in the United States for the past several years."
None of the human judges was a professional translator.
The humans judged our 5 standard systems on a Chinese sentence sub-set extracted at random from our 500 sentence test corpus.
"We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chi-nese source and English translations."
We prepared a web page with these translation pairs randomly or-dered to disperse the five translations of each source sentence.
All judges used this same webpage and saw the sentence pairs in the same order.
They rated each translation from 1 (very bad) to 5 (very good).
The monolingual group made their judgments based only on the translations’ readability and fluency.
"As must be expected, some judges were more lib-eral than others."
And some sentences were easier to translate than others.
"To account for the intrin-sic difference between judges and the sentences, we compared each judge’s rating for a sentence across systems."
We performed four pairwise t-test compar-isons between adjacent systems as ordered by their aggregate average score.
Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% con-fidence interval about the mean.
"We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5-point scale), while S3 is judged a little better (by 0.114)."
"Both differences are significant at the 95% level. [Footnote_7] The human H1 is much better than the best system, though a bit worse than human H2."
"7[REF_CITE]% confidence interval comes from t-test, assuming that the data comes from a T-distribution with N degrees of free-dom. N varied from 350 to 470 as some judges have skipped some sentences in their evaluation. Thus, the distribution is close to Gaussian."
"This is not surprising given that H1 is not a native speaker of either Chinese or English, whereas H2 is a native English speaker."
"Again, the difference between the human translators is signifi-cant beyond the 95% level."
Figure 3: Monolingual Judgments - pairwise differ-ential comparison
Figure 4 shows the same results for the bilingual group.
"They also find that S3 is slightly better than S2 (at 95% confidence) though they judge that the human translations are much closer (indistinguish-able at 95% confidence), suggesting that the bilin-guals tended to focus more on adequacy than on flu-ency."
Figure 4: Bilingual Judgments - pairwise differential comparison
Figure 5 shows a linear regression of the monolin-gual group scores as a function of the B LEU score over two reference translations for the 5 systems.
The high correlation coefficient of 0.99 indicates that B LEU tracks human judgment well.
Particularly interesting is how well B LEU distinguishes between S2 and S3 which are quite close.
Figure 6 shows the comparable regression results for the bilingual group.
The correlation coefficient is 0.96.
We now take the worst system as a reference point and compare the B LEU scores with the human judg- ment scores of the remaining systems relative to the worst system.
"We took the B LEU , monolingual group, and bilingual group scores for the 5 systems and linearly normalized them by their correspond-ing range (the maximum and minimum score across the 5 systems)."
The normalized scores are shown in Figure 7.
This figure illustrates the high correlation between the B LEU score and the monolingual group.
Of particular interest is the accuracy of B LEU ’s esti-mate of the small difference between S2 and S3 and the larger difference between S3 and H1.
The figure also highlights the relatively large gap between MT systems and human translators. [Footnote_8]
8 Crossing this chasm for Chinese-English translation ap-pears to be a significant challenge for the current state-of-the-art systems.
"In addition, we sur-mise that the bilingual group was very forgiving in judging H1 relative to H2 because the monolingual group found a rather large difference in the fluency of their translations."
Figure 7: B LEU vs Bilingual and Monolingual Judg-ments
We believe that B LEU will accelerate the MT R&amp;D cycle by allowing researchers to rapidly home in on effective modeling ideas.
"Our belief is reinforced by a recent statistical analysis of B LEU ’s correla-tion with human judgment for translation into En-glish from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families[REF_CITE]!"
B LEU ’s strength is that it correlates highly with human judg- ments by averaging out individual sentence judg-ment errors over a test corpus rather than attempting to divine the exact human judgment for every sen-tence: quantity leads to quality.
"Finally, since MT and summarization can both be viewed as natural language generation from a tex-tual context, we believe B LEU could be adapted to evaluating summarization or similar NLG tasks."
Acknowledgments This work was partially sup-ported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract
The views and findings contained in this material are those of the authors and do not necessarily reflect the position of pol-icy of the Government and no official endorsement should be inferred.
We gratefully acknowledge comments about the geometric mean by John Makhoul of BBN and dis-cussions with George Doddington of NIST.
We es-pecially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of Chinese-English MT systems.
Categorial grammar has traditionally used the λ-calculus to represent meaning.
"We present an alternative, dependency-based perspective on linguistic meaning and sit-uate it in the computational setting."
This perspective is formalized in terms of hy-brid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be rep-resented in a single meaning formalism.
"Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations com-positionally."
"The λ-calculus has enjoyed many years as the stan-dard semantic encoding for categorial grammars and other grammatical frameworks, but recent work has highlighted its inadequacies for both linguistic and computational concerns of representing natural lan-guage semantics[REF_CITE]."
"The latter couples a resource-sensitive cate-gorial proof theory[REF_CITE]to hybrid logic[REF_CITE]to formalize a dependency-based perspective on meaning, which we call here Hybrid Logic Dependency Semantics (HLDS)."
"In this pa-per, we situate HLDS in the computational context by explicating its properties as a framework for com-putational semantics and linking it to Combinatory Categorial Grammar (CCG)."
The structure of the paper is as follows.
"In x 2, we briefly introduce CCG and how it links syntax and semantics, and then discuss semantic represen-tations that use indexes to identify subparts of logi-cal forms. x 3 introduces HLDS and evaluates it with respect to the criteria of other computational seman-tics frameworks. x 4 shows how we can build HLDS terms using CCG with unification and x 5 shows how intonation and information structure can be incorpo-rated into the approach."
"Traditionally, categorial grammar has captured meaning using a (simply typed) λ-calculus, build-ing semantic structure in parallel to the categorial in-ference[REF_CITE]."
"For example, a (simplified) CCG lexical en-try for a verb such as wrote is given in (1)."
Rules of combination are defined to operate on both categories and λ-terms simultaneously.
"For exam-ple, the rules allow the following derivation for Ed wrote books. (2) Ed wrote books n :Ed ( s n n ) = n :λx : λy : write ( y ; x ) n :books &gt; s n n : λy : write ( y ; books &lt; ) s : write ( Ed ; books )"
Derivations like (2) give rise to the usual sort of predicate-argument structure whereby the order in which the arguments appear (and are bound by the λ’s) is essentially constitutive of their meaning.
"Thus, the first argument could be taken to corre-spond to the writer, whereas the second argument corresponds to what is being written."
"One deficiency of λ-calculus meaning representa-tions is that they usually have to be type-raised to the worst case to fully model quantification, and this can reverberate and increase the complexity of syn-tactic categories since a verb like wrote will need to be able to take arguments with the types of general-ized quantifiers."
The approach we advocate in this paper does not suffer from this problem.
"For CCG, the use of the λ-terms is simply a con-venient device to bind arguments when presenting derivations[REF_CITE]."
"In implementations, a more common strategy is to compute semantic rep-resentations via unification, a tactic explicitly em-ployed in Unification Categorial Grammar (UCG)[REF_CITE]."
"Using a unification paradigm in which atomic categories are bundles of syntactic and semantic information, we can use an entry such as (3) for wrote in place of (1)."
"In the unification set-ting, (3) permits a derivation analogous to (2)."
"For creating predicate-argument structures of this kind, strategies using either λ-terms or unification to bind arguments are essentially notational vari-ants."
"However, UCG goes beyond simple predicate-argument structures to instead use a semantic repre-sentation language called Indexed Language (InL)."
"The idea of using indexes stems from Davidson (event variables), and are a commonly used mech-anism in unification-based frameworks and theories for discourse representation."
InL attaches one to ev-ery formula representing its discourse referent.
This results in a representation such as (4) for the sen-tence Ed came to the party.
"InL thus flattens logical forms to some extent, using the indexes to spread a given entity or event through multiple predications."
"The use of indexes is crucial for UCG’s account of modifiers, and as we will see later, we exploit such referents to achieve similar ends when coupling HLDS and CCG."
Minimal Recursion Semantics (MRS)[REF_CITE]is a frame-work for computational semantics that is designed to simplify the work of algorithms which produce or use semantic representations.
"MRS provides the means to represent interpretations with a flat, un-derspecified semantics using terms of the predicate calculus and generalized quantifiers."
"Flattening is achieved by using an indexation scheme involving labels that tag particular groups of elementary pred-ications (EPs) and handles (here, h 1 ; h 2 ;::: ) that ref-erence those EPs."
Underspecification is achieved by using unresolved handles as the arguments for scope-bearing elements and declaring constraints (with the = q operator) on how those handles can be resolved.
Different scopes can be reconstructed by equating unresolved handles with the labels of the other EPs obeying the = q constraints.
"For example, (5) would be given as the representation for every dog chases some white cat. (5) h h 0 ;f h 1 :every ( x ; h 2 ; h 3 ) ; h 4 :dog ( x ) ; h 11 :cat ( y ) ; h 8 :some ( y ; h 9 ; h 10 ) ; h 11 :white ( y ) ; h 7 :chase ( x ; y ) g; f h 0 = q h 7 ; h 2 = q h 4 ; h 9 = q h 11 gi"
"Copestake et al. argue that these flat representa-tions facilitate a number of computational tasks, in-cluding machine translation and generation, without sacrificing linguistic expressivity."
"Also, flatness per-mits semantic equivalences to be checked more eas-ily than in structures with deeper embedding, and underspecification simplifies the work of the parser since it does not have to compute every possible reading for scope-bearing elements."
We call this unified perspective combining many levels of meaning Hybrid Logic Dependency Semantics (HLDS).
"We begin by discussing how hy-brid logic extends modal logic, then look at the rep-resentation of linguistic meaning via hybrid logic terms."
"Though modal logic provides a powerful tool for encoding relational structures and their properties, it contains a surprising inherent asymmetry: states (“worlds”) are at the heart of the model theory for modal logic, but there are no means to directly reference specific states using the object language."
This inability to state where exactly a proposition holds makes modal logic an inadequate representa-tion framework for practical applications like knowl-edge representati[REF_CITE]or temporal rea-soning[REF_CITE].
"Because of this, compu-tational work in knowledge representation has usu-ally involved re-engineering first-order logic to suit the task, e.g., the use of metapredicates such as Hold of Kowalski and Allen."
"Unfortunately, such logics are often undecidable."
Hybrid logic extends standard modal logic while retaining decidability and favorable complexity[REF_CITE](cf.[REF_CITE]for a com-plexity roadmap).
"The strategy is to add nominals, a new sort of basic formula with which we can ex-plicitly name states in the object language."
"Next to propositions, nominals are first-class citizens of the object language: formulas can be formed using both sorts, standard boolean operators, and the satisfac-tion operator “@”."
"A formula @ i p states that the formula p holds at the state named by i. 1 (There are more powerful quantifiers ranging over nomi-nals, such as # , but we do not consider them here.)"
With nominals we obtain the possibility to explic-itly refer to the state at which a proposition holds.
"A standard modal temporal logic with the modalities F and P (future and past, respectively) cannot cor-rectly represent an utterance such as Ed finished the book because it is unable to refer to the specific time at which the event occurred."
"The addition of nomi-nals makes this possible, as shown in (6), where the nominal i represents the Reichenbachian event time."
"Furthermore, many temporal properties can be de-fined in terms of pure formulas which use nominals and contain no propositional variables."
"For example, the following term defines the fact that the relations for F and P are mutually converse:"
It is also possible to encode a variety of other rep-resentations in terms of hybrid logics.
"For example, nominals correspond to tags in attribute-value matri-ces (AVMs), so the hybrid logic formula in (8) cor-responds to the AVM in (9). (8) h SUBJ i ( i ^ h"
AGR i singular ^ h PRED i dog ) 2 ^ h COMP &quot; ih
SUBJ i i #3 (9) 666
"1 A few notes on our conventions: p ; q ; r are variables over any hybrid logic formula; i ; j ; k are variables over nominals; d i and h i denote nominals (for dependent and head, respectively)."
A crucial aspect of hybrid logic is that nominals are at the heart of a sorting strategy.
Different sorts of nominals can be introduced to build up a rich sortal ontology without losing the perspicuity of a propositional setting.
"Additionally, we can reason about sorts because nominals are part and parcel of the object language."
We can extend the language of hybrid logic with f Sort :Nominal g to facilitate the ex-plicit statement of what sort a nominal is in the lan-guage and carry this modification into one of the ex-isting tableaux methods for hybrid logic to reason ef-fectively with this information.
This makes it possi-ble to capture the rich ontologies of lexical databases like WordNet in a clear and concise fashion which would be onerous to represent in first-order logic.
"Hybrid logic enables us to logically capture two es-sential aspects of meaning in a clean and compact way, namely ontological richness and the possibility to refer."
"Logically, we can represent an expression’s linguistically realized meaning as a conjunction of modalized terms, anchored by the nominal that iden-tifies the head’s proposition:"
V (10) @ h ( proposition h δ i i ( d i ^ dep i ))
"Dependency relations are modeled as modal rela-tions h δ i i , and with each dependent we associate a nominal d i , representing its discourse referent."
"As an exam-ple, the sentence Ed wrote a long book in London receives the represention in (11). (11) @ h 1 ( write ^ h A"
CT i ( d 0 ^ Ed ) ^ h P AT i ( d 5 ^ book ^h
GR i ( d 7 ^ long )) ^ h L OC i ( d 9 ^ London ))
"The modal relations A CT , P AT , L OC , and GR stand for the dependency relations Actor, Patient, Loca-tive, and General Relationship, respectively."
Contextual reference can be modeled as a state-ment that from the current state (anaphor) there should be an accessible antecedent state at which particular conditions hold.
"Thus, assuming an ac-cessibility relation XS, we can model the meaning of the pronoun he as in (12)."
"During discourse interpretation, this statement is evaluated against the discourse model."
The pronoun is resolvable only if a state where male holds is XS-accessible in the discourse model.
"Different acces-sibility relations can be modeled, e.g. to distinguish a local context (for resolving reflexive anaphors like himself ) from a global context[REF_CITE]."
"Finally, the rich temporal ontology underlying models of tense and aspect such[REF_CITE]can be captured using the sorting strategy."
Earlier work[REF_CITE]already explored such ideas.
HLDS employs hybrid logic to integrate Moens and Steedman’s no-tion of the event nucleus directly into meaning rep-resentations.
The event nucleus is a tripartite struc-ture reflecting the underlying semantics of a type of event.
"The event is related to a preparation (an ac-tivity bringing the event about) and a consequent (a state ensuing to the event), which we encode as the modal relations P REP and C ONS , respectively."
"Dif-ferent kinds of states and events are modeled as dif-ferent sorts of nominals, shown in (13) using the no-tation introduced above. (13) @ f Activity :e1 g h P REP if Achievement :e2 g ^ @ f Achievement :e2 g h C ONS if State :e3 g"
"Given the event nucleus in (13), the representation in (11) becomes (14), where the event is thus located at a specific time in the past."
"Hybrid logic’s flexibility makes it amenable to representing a wide variety of semantic phenomena in a propositional setting, and it can furthermore be used to formulate a discourse theory[REF_CITE]."
"Here we consider the properties of HLDS with respect to the four main criteria laid out[REF_CITE]which a computational se-mantics framework must meet: expressive adequacy, grammatical compatibility, computational tractabil-ity, and underspecifiability."
Expressive adequacy refers to a framework’s abil-ity to correctly express linguistic meaning.
"HLDS was designed not only with this in mind, but as its central tenet."
"In addition to providing the means to represent the usual predicate-valency relations, it explicitly marks the named dependency relations between predicates and their arguments and modi-fiers."
These different dependency relations are not just labels: they all have unique semantic imports which project new relations in the context of differ-ent heads.
"HLDS also tackles the representation of tense and aspect, contextual reference, and informa-tion structure, as well as their interaction with dis-course."
The criterion of grammatical compatibility re-quires that a framework be linkable to other kinds of grammatical information.
It should furthermore be straight-forward to use our approach to hook HLDS up to other unification-based frameworks.
The definition of computational tractability states that it must be possible to check semantic equiva-lence of different formulas straightforwardly.
"Like MRS, HLDS provides the means to view linguis-tic meaning in a flattened format and thereby ease the checking of equivalence."
A CT i d 0 ^h P AT i d 5 ^h L OC i d 9 ) ^ @ d 0
Ed ^ @ d 5 book ^ @ d 9 London ^ @ d 7 long ^ @ d 5 h GR i d 7
This example clarifies how the use of nominals is related to the indexes of UCG’s InL and the labels of MRS.
"However, there is an important difference: nominals are full citizens of the object language with semantic import and are not simply a device for spreading meaning across several elementary predi-cations."
They simultaneously represent tags on sub-parts of a logical form and discourse referents on which relations are predicated.
"Because it is possi-ble to view an HLDS term as a flat conjunction of the heads and dependents inside it, the benefits de-scribed by Copestake et al. with respect to MRS’s flatness thus hold for HLDS as well."
Computational tractability also requires that it is straightforward to express relationships between representations.
This can be done in the object lan-guage of HLDS as hybrid logic implicational state-ments which can be used with proof methods to dis-cover deeper relationships.
Kruijff’s model connect-ing linguistic meaning to a discourse context is one example of this.
"Underspecifiability means that semantic represen-tations should provide means to leave some semantic distinctions unresolved whilst allowing partial terms to be flexibly and monotonically resolved. (5) shows how MRS leaves quantifier scope underspecified, and such formulas can be transparently encoded in HLDS."
CT i h 4 ^ h P
AT i h 11 ) ^ @ h 1 ( every ^ h R ESTR i i ^ h B
ODY i j ) ^ @ h 8 ( some ^ h R ESTR i k ^ h B ODY i l ) ^ @ h 4 dog ^ @ h 11 cat ^ @ h 11 h GR i ( h 12 ^ white ) ^ @ i h QEQ i h 4 ^ @ k h QEQ i h 11
MRS-style underspecification is thus replicated by declaring new nominals and modeling = q as a modal relation between nominals.
"When constructing the fully-scoped structures generated by an underspeci-fied one, the = q constraints must be obeyed accord-ing to the qeq condition of Copestake etal."
"Because HLDS is couched directly in terms of hybrid logic, we can concisely declare the qeq condition as the following implication: (17) @ i h i ! @ _ h i j ( @ i B"
ODY i k ^ @ h k QEQ i j ) QEQ j
"Alternatively, it would in principle be possible to adopt a truly modal solution to the representation of quantifiers."
The complexity of generalized quantification is then pushed into the model theory instead of forcing the representation to carry the burden.
"In Dependency Grammar Logic (DGL),[REF_CITE]couples HLDS to a resource-sensitive categorial proof theory (CTL)[REF_CITE]."
"Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG."
"First, parsing with CCG gram-mars for substantial fragments is generally more efficient than with CTL grammars with similar coverage."
"Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG[REF_CITE]."
"Second, syntactic features (modeled by unary modalities) in CTL have no intuitive semantic reflection, whereas CCG can relate syntactic and semantic features perspicuously using unification."
"Finally, CCG has a detailed syntactic account of the realization of information structure in English."
"To link syntax and semantics in derivations, ev-ery logical form in DGL expresses a nominal iden-tifying its head in the format @ i p."
"This handles de-pendents in a linguistically motivated way through a linking theory: given the form of a dependent, its (possible) role is established, after which its mean-ing states that it seeks a head that can take such a role."
"However, to subsequently bind that dependent into the verb’s argument slot requires logical axioms about the nature of various dependents."
"This not only requires extra reduction steps to arrive at the desired logical form, but could also lead to problems depending on the underlying theory of roles."
"We present an alternative approach to binding de-pendents, which overcomes these problems without abandoning the linguistic motivation."
"Because we work in a lexicalist setting, we can compile the ef-fects of the linguistic linking theory directly into cat-egory assignments."
"The first difference in our proposal is that argu-ments express only their own nominal, not the nom-inal of a head as well."
"For example, proper nouns receive categories such as (18)."
"This entry highlights our relaxation of the strict con-nection between syntactic and semantic types tradi-tionally assumed in categorial grammars, a move in line with the MRS approach."
"In contrast with DGL, the semantic portion of a syntactic argument in our system does not declare the role it is to take and does not identify the head it is to be part of."
Instead it identifies only its own referent.
"Without using additional inference steps, this is transmuted via unification into a form similar to DGL’s in the result category. (19) is an example of the kind of head category needed. (19) sleeps ` s : @ h 2 ( sleep ^h"
A i ^ n CT ( i p )) n : @ i p
"To derive[REF_CITE]and (19) combine via back-ward application to produce (20), the same term as that built in DGL using one step instead of several."
"To produce HLDS terms that are fully compati-ble with the way th[REF_CITE]model discourse, we need to mark the infor-mativity of dependents as contextually bound (CB) and contextually nonbound (NB)."
"In DGL, these ap-pear as modalities in logical forms that are used to create a topic-focus articulation that is merged with the discourse context."
"For example, the sentence he wrote a book would receive the following (simpli-fied) interpretation: (21) @ h 1 ([ NB ] write ^ [ NB ] h P AT i ( d 5 ^ book ) ^ [ CB ] h A"
CT i ( d 6 ^ h XS i ( d 3 ^ male )))
DGL uses feature-resolving unary modalities[REF_CITE]to instantiate the values of in-formativity.
"In unification-based approaches such as CCG, the transferal of feature information into semantic representations is standard practice."
We thus employ the feature inf and mark informativity in logical forms with values resolved syntactically. (22) Ed ` n inf = CB : @ d 1[REF_CITE]sleeps ` s : @ h 2 ([ NB ] sleep ^ [ q ] h A i ^ n CT ( i p )) n inf = q :@ i p
Combining these entries using backward application gives the following result for Ed sleeps: (24) s : @ h 2 ([ NB ] sleep ^ [ h CB ]
A CT i ( d ^ 1 Ed ))
A major benefit of having nominals in our rep-resentations comes with adjuncts.
"With HLDS, we consider the prepositional verbal modifier in the sen-tence Ed sleeps in the bed as an optional Locative dependent of sleeps."
"To implement this, we fol-low DGL in identifying the discourse referent of the head with that of the adjunct."
"However, unlike DGL, this is compiled into the category for the adjunct. (25) in ` ( s : @ ^ h i ( p [ r ] L OC i ( j ^ q )) n s :@ i p ) = n inf = r :@ j q"
"To derive the sentence Ed sleeps in the bed (see Figure 1), we then need the following further entries: (26) the ` n inf = CB :p = n inf ="
NB :p (27) bed ` n inf =
NB : @ d 3 bed
"This approach thus allows adjuncts to insert their semantic import into the meaning of the head, mak-ing use of nominals in a manner similar to the use of indexes in Unification Categorial Grammar."
Information Structure (IS) in English is in part deter-mined by intonation.
"For example, given the ques-tion in (28), an appropriate response would be (29). [Footnote_2] (28) I know what Ed READ ."
"2 Following Pierrehumbert’s notation, the intonational con-tour L+H* indicates a low-rising pitch accent, H* a sharply-rising pitch accent, and both LH% and LL% are boundary tones."
But what did Ed WRITE ? (29) (Ed WROTE ) ( A BOOK ).
LH% H* LL%
"Steedman calls segments such as Ed wrote of (29) the theme of the sentence, and a book the rheme."
"The former indi-cates the part of the utterance that connects it with the preceding discourse, whereas the latter provides information that moves the discourse forward."
"In the context of Discourse Representation The-ory,[REF_CITE]represents IS by splitting DRT structures into a topic/focus articula-tion of the form TOPIC ./"
We represent this in HLDS as a term incorporating the ./ opera-tor.
"Equating topic and focus with Steedman’s theme and rheme, we encode the interpretation of (29) as: (30) @ h 7 ([ CB ] write ^ [ CB ] h A"
CT i ( d 1 ^ Ed ) ./ [ NB ] h P AT i ( d 4 ^ book ))
DGL builds such structures by using a rewriting sys-tem to produce terms with topic/focus articulation from the terms produced by the syntax.
"Steedman uses the pitch accents to produce lexi-cal entries with values for the INFORMATION fea-ture, which we call here sinf."
"L+H* and H* set the value of this feature as θ (for theme) or ρ (for rheme), respectively."
"He also employs cate-gories for the boundary tones that carry blocking values for sinf which stop incomplete intonational phrases from combining with others, thereby avoid-ing derivations for utterances with nonsensical into-nation contours."
Our approach is to incorporate the syntactic as-pects of Steedman’s analysis with DGL’s rewriting system for using informativity to partition senten-tial meaning.
"In addition to using the syntactic fea-ture sinf , we allow intonation marking to instantiate the values of the semantic informativity feature inf ."
"Thus, we have the following sort of entry: (31) WROTE (L+H*) ` s sinf = θ :φ n n inf = w ; sinf = θ :@ i p = n inf = x ; sinf = θ :@ j q φ = @ h 2 ([ CB ] write ^ [ w ] h A i ^ ^ h i ^"
CT ( i p ) [ x ] P AT ( j q ))
"We therefore straightforwardly reap the syntactic benefits of Steedman’s intonation analysis, while IS itself is determined via DGL’s logical form rewrit-ing system operating on the modal indications of informativity produced during the derivation."
"The articulation of IS can thus be performed uniformly across languages, which use a variety of strategies including intonation, morphology, and word order variation to mark the informativity of different el-ements."
The resulting logical form plugs directly into DGL’s architecture for incorporating sentence meaning with the discourse.
"Since it is couched in hybrid logic, HLDS is ide-ally suited to be logically engineered to the task at hand."
"Hybrid logic can be made to do exactly what we want, answering to the linguistic intuitions we want to formalize without yielding its core assets – a rich propositional ontology, decidability, and favor-able computational complexity."
"Various aspects of meaning, like dependency re-lations, contextual reference, tense and aspect, and information structure can be perspicuously encoded with HLDS, and the resulting representations can be built compositionally using CCG."
"CCG has close affinities with dependency grammar, and it provides a competitive and explanatorily adequate basis for a variety of phenomena ranging from coordination and unbounded dependencies to information struc-ture."
"Nonetheless, the approach we describe could in principle be fit into other unification-based frame-works like Head-Driven Phrase Structure Grammar."
Hybrid logic’s utility does not stop with senten-tial meaning.
It can also be used to model dis-course interpretation and is closely related to log-ics for knowledge representation.
This way we can cover the track from grammar to discourse with a single meaning formalism.
"We do not need to trans-late or make simplifying assumptions for different processing modules to communicate, and we can freely include and use information across different levels of meaning."
"We have implemented a (preliminary) Java pack-age for creating and manipulating hybrid logic terms and connected it to Grok, a CCG parsing system. [Footnote_3] The use of HLDS has made it possible to improve the representation of the lexicon."
3 The software is available[URL_CITE]and[URL_CITE]under an open source license.
"Hybrid logic nom-inals provide a convenient and intuitive manner of localizing parts of a semantic structure, which has made it possible to greatly simplify the use of inher-itance in the lexicon."
Logical forms are created as an accumulation of different levels in the hierarchy including morphological information.
This is partic-ularly important since the system does not otherwise support typed feature structures with inheritance.
"Hybrid logics provide a perspicuous logical lan-guage for representing structures in temporal logic, description logic, AVMs, and indeed any relational structure."
Terms of HLDS can thus be marshalled into terms of these other representations with the potential of taking advantage of tools developed for them or providing input to modules expecting them.
"In future work, we intend to combine techniques for building wide-coverage statistical parsers for CCG[REF_CITE]with corpora that have explicitly marked semantic dependency relations (such as the Prague Dependency Treebank and NEGRA) to produce HLDS terms as the parse output."
This paper describes a wide-coverage sta-tistical parser that uses Combinatory Cat-egorial Grammar (CCG) to derive de-pendency structures.
"The parser differs from most existing wide-coverage tree-bank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies."
"A set of dependency structures used for train-ing and testing the parser is obtained from a treebank of CCG normal-form deriva-tions, which have been derived (semi-) au-tomatically from the Penn Treebank."
"The parser correctly recovers over 80% of la-belled dependencies, and around 90% of unlabelled dependencies."
"Most recent wide-coverage statistical parsers have used models based on lexical dependencies (e.g.[REF_CITE],[REF_CITE])."
"However, the de-pendencies are typically derived from a context-free phrase structure tree using simple head percolation heuristics."
"This approach does not work well for the long-range dependencies involved in raising, con-trol, extraction and coordination, all of which are common in text such as the Wall Street Journal."
The potential advantage from using such an expressive grammar is to facili-tate recovery of such unbounded dependencies.
"As well as having a potential impact on the accuracy of the parser, recovering such dependencies may make the output more useful."
CCG is unlike other formalisms in that the stan-dard predicate-argument relations relevant to inter-pretation can be derived via extremely non-standard surface derivations.
"This impacts on how best to de-fine a probability model for CCG, since the “spuri-ous ambiguity” of CCG derivations may lead to an exponential number of derivations for a given con-stituent."
"In addition, some of the spurious deriva-tions may not be present in the training data."
"One solution is to consider only the normal-form[REF_CITE]derivation, which is the route taken[REF_CITE]. [Footnote_1]"
"1 Another, more speculative, possibility is to treat the alter-native derivations as hidden and apply the EM algorithm."
Another problem with the non-standard surface derivations is that the standard PARSEVAL per-formance measures over such derivations are unin-formative[REF_CITE].
"Such measures have been criticised[REF_CITE]and[REF_CITE], who propose recovery of head-dependencies characterising predicate-argument re-lations as a more meaningful measure."
"If the end-result of parsing is interpretable predicate-argument structure or the related depen-dency structure, then the question arises: why build derivation structure at all?"
"A CCG parser can directly build derived structures, including long-"
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 327-334. range dependencies."
"These derived structures can be of any form we like—for example, they could in principle be standard Penn Treebank structures."
"Since we are interested in dependency-based parser evaluation, our parser currently builds dependency structures."
"Furthermore, since we want to model the dependencies in such structures, the probability model is defined over these structures rather than the derivation."
"The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG deriva-tions developed for use with another (normal-form) CCG parser ("
"The treebank of derivations, which we call CCG-bank[REF_CITE], was in turn derived (semi-)automatically from the hand-annotated Penn Treebank."
"In CCG, most language-specific aspects of the gram-mar are specified in the lexicon, in the form of syn-tactic categories that identify a lexical item as either a functor or argument."
"For the functors, the category specifies the type and directionality of the arguments and the type of the result."
"For example, the follow-ing category for the transitive verb bought specifies its first argument as a noun phrase (NP) to its right and its second argument as an NP to its left, and its result as a sentence: (1) bought := S NP  NP"
"For parsing purposes, we extend CCG categories to express category features, and head-word and de-pendency information directly, as follows:"
"The feature dcl specifies the category’s S result as a declarative sentence, bought identifies its head, and the numbers denote dependency relations."
"Heads and dependencies are always marked up on atomic categories (S, N, NP, PP, and conj in our implemen-tation)."
"The categories are combined using a small set of typed combinatory rules, such as functional applica-tion and composition (see[REF_CITE]for de-tails)."
"Derivations are written as follows, with under-lines indicating combinatory reduction and arrows indicating the direction of the application: (3) Marks bought Brooks NP Marks S dcl bought NP 1  NP 2 NP Brooks S dcl bought NP 1 S dcl bought"
"Formally, a dependency is defined as a 4-tuple: h f f s h a , where h f is the head word of the func-tor, 2 f is the functor category (extended with head and dependency information), s is the argument slot, and h a is the head word of the argument—for exam-ple, the following is the object dependency yielded by the first step of derivation (3): (4) bought S dcl bought NP 1  NP 2 2 Brooks"
"Variables can also be used to denote heads, and used via unification to pass head information from one category to another."
"For example, the expanded category for the control verb persuade is as follows:"
"The head of the infinitival complement’s subject is identified with the head of the object, using the vari-able X. Unification then “passes” the head of the ob-ject to the subject of the infinitival, as in standard unification-based accounts of control. [Footnote_3]"
"3 The extension of CCG categories in the lexicon and the la-belled data is simplified in the current system to make it entirely automatic. For example, any word with the same category (5) as persuade gets the object-control extension. In certain rare cases (such as promise) this gives semantically incorrect depen-dencies in both the grammar and the data (promise Brooks to go has a structure meaning promise Brooks that Brooks will go)."
"The kinds of lexical items that use the head pass-ing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns."
Among the constructions that project unbounded dependencies are relativisation and right node raising.
"The follow-ing category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: (6) who := NP X NP X,1  S dcl [Footnote_2] NP X"
2 Note that the functor does not always correspond to the lin-guistic notion of a head.
"The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter)."
"Type-raising ( ) and functional composition ( ), along with co-indexing of heads, mediate transmission of the head of the NP the company onto the object of buy."
"The corresponding dependencies are given in the following figure, with the convention that arcs point away from arguments."
The relevant argument slot in the functor category labels the arcs.
"Note that we encode the subject argument of the to category as a dependency relation (Marks is a “subject” of to), since our philosophy at this stage is to encode every argument as a dependency, where possible."
The number of dependency types may be reduced in future work.
"The DAG-like nature of the dependency structures makes it difficult to apply generative modelling tech-niques[REF_CITE], so we have defined a conditional model, similar to the model[REF_CITE](see also the condi-tional model[REF_CITE])."
"While the model[REF_CITE]is technically unsound[REF_CITE], our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model."
Future work will look at alternative models. [Footnote_4]
"4 The reentrancies creating the DAG-like structures are fairly limited, and moreover determined by the lexical categories. We conjecture that it is possible to define a generative model that includes the deep dependencies."
"The parse selection component must choose the most probable dependency structure, given the sen-tence S. A sentence S w 1 t 1  w 2 t 2  w n t n is assumed to be a sequence of word, pos-tag pairs."
"For our purposes, a dependency structure π is a C D pair, where C c 1 c 2  c n is the se-quence of categories assigned to the words, and"
D ! h f i f i s i h a i #&quot; i 1  m $ is the set of de-pendencies.
The probability of a dependency struc-ture can be written as follows: (7) P π % P C D &quot; S &amp; P C &quot; S P D &quot; C S
The probability P C &quot; S can be approximated as follows: (8) P C &quot; S &amp;&apos; ∏ ni ( 1 P c i &quot; X i where X i is the local context for the ith word.
"We have explained elsewhere[REF_CITE]how suit-able features can be defined in terms of the word, pos-tag pairs in the context, and how maximum en-tropy techniques can be used to estimate the proba-bilities, following[REF_CITE]."
"We assume that each argument slot in the cat-egory sequence is filled independently, and write P D &quot; C S as follows: (9) P D &quot; C S % ∏ mi ( 1 P h a i &quot; C S where h a i is the head word filling the argument slot of the ith dependency, and m is the number of de-pendencies entailed by the category sequence C."
The estimation method is based[REF_CITE].
"We assume that the probability of a dependency only depends on those words involved in the dependency, together with their categories."
"We follow Collins and base the estimate of a dependency probability on the following intuition: given a pair of words, with a pair of categories, which are in the same sen- tence, what is the probability that the words are in a particular dependency relationship?"
"We again follow Collins in defining the following functions, where ) is the set of words in the data, and * is the set of lexical categories. + C , a - b ./- , c - d . for a - c 021 and b - d 043 is the number of times that word-category pairs , a - b . and , c - d . are in the same word-category sequence in the training data. + C R - , a - b ./- , c - d . is the number of times that , a - b . and , c - d . are in the same word-category sequence, with a and c in dependency relation R. + F R 5 , a - b /. - , c - d . is the probability that a and c are in de-pendency relation R, given that , a - b . and , c - d . are in the same word-category sequence."
The relative frequency estimate of the probability F R &quot; a b  c d is as follows: (10) F̂ R &quot; a b  c d  CC 7 R &lt;7 98: a 9 a 8 b 8 b ;&lt;&lt;;8 98 c 9 8 cd 8 d &gt;; &gt;;= =
The probability P h a i &quot; C S can now be approxi-mated as follows:
F̂ 7 R @ 9 h fi 8 f i &lt;; 8 9 h ai 8 c ai &gt;; = (11) P h a i &quot; C S ?&apos; ∑ nj A F̂ 7 R @ 9 h fi 8 f &lt;; 8:9 i w j 8 c j ;&gt;= 1 where c a i is the lexical category of the argument head a i .
"The normalising factor ensures that the probabilities for each argument slot sum to one over all the word-category pairs in the sequence. [Footnote_5] This factor is constant for the given category sequence, but not for different category sequences."
"5 One of the problems with the model is that it is deficient, as-signing probability mass to dependency structures not licensed by the grammar."
"However, the dependency structures with high enough P C &quot; S to be among the highest probability structures are likely to have similar category sequences."
"Thus we ignore the normalisation factor, thereby simplifying the parsing process. (A similar argument is used[REF_CITE]in the context of his parsing model.)"
"The estimate in equation 10 suffers from sparse data problems, and so a backing-off strategy is em-ployed."
"We omit details here, but there are four lev-els of back-off: the first uses both words and both categories; the second uses only one of the words and both categories; the third uses the categories only; and a final level substitutes pos-tags for the categories."
"One final point is that, in practice, the number of dependencies can vary for a given category sequence (because multiple arguments for the same slot can be introduced through coordination), and so a geo-metric mean of p π is used as the ranking function, averaged by the number of dependencies in D."
The parser analyses a sentence in two stages.
"First, in order to limit the number of categories assigned to each word in the sentence, a “supertagger”[REF_CITE]assigns to each word a small number of possible lexical categories."
"The supertag-ger (described[REF_CITE]) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability cate-gory for that word, given the surrounding context."
"Note that the supertagger does not provide a single category sequence for each sentence, and the final sequence returned by the parser (along with the de-pendencies) is determined by the probability model described in the previous section."
"The supertagger is performing two roles: cutting down the search space explored by the parser, and providing the category-sequence model in equation 8."
"The supertagger consults a “category dictionary” which contains, for each word, the set of categories the word was seen with in the data."
"If a word ap-pears at least K times in the data, the supertagger only considers categories that appear in the word’s category set, rather than all lexical categories."
"The second parsing stage applies a CKY bottom-up chart-parsing algorithm, as described[REF_CITE]."
"The combinatory rules currently used by the parser are as follows: functional ap-plication (forward and backward), generalised for-ward composition, backward composition, gener-alised backward-crossed composition, and type-raising."
There is also a coordination rule which con-joins categories of the same type. [Footnote_6]
"6 Restrictions are placed on some of the rules, such as that given[REF_CITE]for backward-crossed composition (p.62)."
"Type-raising is applied to the categories NP, PP, and S adj  NP (adjectival phrase); it is currently implemented by simply adding pre-defined sets of type-raised categories to the chart whenever an NP, PP or S adj  NP is present."
"The sets were chosen on the basis of the most frequent type-raising rule instantiations in sections 02-21 of the CCGbank, which resulted in 8 type-raised categories for NP, and 2 categories each for PP and S adj  NP."
"As well as combinatory rules, the parser also uses a number of lexical rules and rules involving punc-tuation."
The set of rules consists of those occurring roughly more than 200 times in sections 02-21 of the CCGbank.
"For example, one rule used by the parser is the following:"
This rule creates a nominal modifier from an ing-form of a verb phrase.
A set of rules allows the parser to deal with com-mas (all other punctuation is removed after the su-pertagging phase).
"For example, one kind of rule treats a comma as a conjunct, which allows the NP object in John likes apples, bananas and pears to have three heads, which can all be direct objects of like. [Footnote_7]"
7 These rules are currently applied deterministically. In fu-ture work we will investigate approaches which integrate the rule applications with the statistical model.
The search space explored by the parser is re-duced by exploiting the statistical model.
"First, a constituent is only placed in a chart cell if there is not already a constituent with the same head word, same category, and some dependency structure with a higher or equal score (where score is the geomet-ric mean of the probability of the dependency struc-ture)."
This tactic also has the effect of eliminat-ing “spuriously ambiguous” entries from the chart— cf.[REF_CITE].
"Second, a constituent is only placed in a cell if the score for its dependency struc-ture is within some factor, α, of the highest scoring dependency structure for that cell."
The word-category sequences needed for estimat-ing the probabilities in equation 8 can be read di-rectly from the CCGbank.
"To obtain dependencies for estimating P D &quot; C S , we ran the parser over the trees, tracing out the combinatory rules applied dur-ing the derivation, and outputting the dependencies."
This method was also applied to the trees in section 23 to provide the gold standard test set.
"Not all trees produced dependency structures, since not all categories and type-changing rules in the CCGbank are encoded in the parser."
We obtained dependency structures for roughly 95% of the trees in the data.
"For evaluation purposes, we increased the coverage on section 23 to 99 0% (2 352 sen-tences) by identifying the cause of the parse failures and adding the additional rules and categories when creating the gold-standard; so the final test set con-sisted of gold-standard dependency structures from 2 352 sentences."
The coverage was increased to en-sure the test set was representative of the full section.
"We emphasise that these additional rules and cate-gories were not made available to the parser during testing, or used for training."
"Initially the parser was run with β 0 01 for the supertagger (an average of 3 [Footnote_8] categories per word), K 20 for the category dictionary, and α 0 001 for the parser."
8 A small number of sentences in the Penn Treebank do not appear in the CCGbank (see[REF_CITE]).
A time-out was applied so that the parser was stopped if any sentence took longer than 2 CPU minutes to parse.
"With these parameters, 2 098 of the 2 352 sentences received some anal-ysis, with 206 timing out and 48 failing to parse."
"To deal with the 48 no-analysis cases, the cut-off for the category-dictionary, K, was increased to 100."
"To deal with the 206 time-out cases, β was increased to 0 05, which resulted in 181 of the 206 sentences then receiving an analysis, with 18 failing to parse, and 7 timing out."
"So overall, almost 98% of the 2 352 unseen sentences were given some analy-sis."
"To return a single dependency structure, we chose the most probable structure from the S dcl categories spanning the whole sentence."
"If there was no such category, all categories spanning the whole string were considered."
"To measure the performance of the parser, we com-pared the dependencies output by the parser with those in the gold standard, and computed precision and recall figures over the dependencies."
"Recall that a dependency is defined as a 4-tuple: a head of a functor, a functor category, an argument slot, and a head of an argument."
"Figures were calculated for la-belled dependencies (LP,LR) and unlabelled depen-dencies (UP,UR)."
"To obtain a point for a labelled de-pendency, each element of the 4-tuple must match exactly."
"Note that the category set we are using dis-tinguishes around 400 distinct types; for example, tensed transitive buy is treated as a distinct category from infinitival transitive buy."
Thus this evaluation criterion is much more stringent than that for a stan-dard pos-tag label-set (there are around 50 pos-tags used in the Penn Treebank).
"To obtain a point for an unlabelled dependency, the heads of the functor and argument must appear together in some relation (either as functor or argu-ment) for the relevant sentence in the gold standard."
"The results are shown in Table 1, with an additional column giving the category accuracy."
"As an additional experiment, we conditioned the dependency probabilities in 10 on a “distance mea-sure” (∆)."
"Distance has been shown to be a use-ful feature for context-free treebank style parsers (e.g.[REF_CITE],[REF_CITE]), although our hypothesis was that it would be less useful here, be-cause the CCG grammar provides many of the con-straints given by ∆, and distance measures are biased against long-range dependencies."
"We tried a number of distance measures, and the one used here encodes the relative position of the heads of the argument and functor (left or right), counts the number of verbs between argument and functor (up to 1), and counts the number of punctu-ation marks (up to 2)."
"The results are also given in Table 1, and show that, as expected, adding distance gives no improvement overall."
An advantage of the dependency-based evalua-tion is that results can be given for individual de-pendency relations.
"Labelled precision and recall[REF_CITE]for the most frequent dependency types are shown in Table 2 (for the model without distance measures). [Footnote_9] The columns # deps give the total num-ber of dependencies, first the number put forward by the parser, and second the number in the gold stan-dard."
"9 Currently all the modifiers in nominal compounds are anal-ysed in CCGbank as N N, as a default, since the structure of the compound is not present in the Penn Treebank. Thus the scores for N N are not particularly informative. Removing these rela-tions reduces the overall scores by around 2%. Also, the scores in Table 2 are for around 95% of the sentences[REF_CITE]be-cause of the problem obtaining gold standard dependency struc-tures for all sentences, noted earlier."
F-score is calculated as (2*LP*LR)/(LP+LR).
"We also give the scores for the dependencies cre-ated by the subject and object relative pronoun cat-egories, including the headless object relative pro-noun category."
We would like to compare these results with those of other parsers that have presented dependency-based evaluations.
"However, the few that exist[REF_CITE]have used either different data or different sets of dependen-cies (or both)."
In future work we plan to map our CCG dependencies onto the set used by Carroll and Briscoe and parse their evaluation corpus so a direct comparison can be made.
"As far as long-range dependencies are concerned, it is similarly hard to give a precise evaluation."
"Note that the scores in Table 2 currently conflate extracted and in-situ arguments, so that the scores for the di-rect objects, for example, include extracted objects."
"The scores for the relative pronoun categories give a good indication of the performance on extraction cases, although even here it is not possible at present to determine exactly how well the parser is perform-ing at recovering extracted arguments."
"In an attempt to obtain a more thorough anal-ysis, we analysed the performance of the parser on the 24 cases of extracted objects in the gold-standard[REF_CITE](development set) that were passed down the object relative pronoun category NP X NP X  S dcl  NP X . 10[REF_CITE](41.7%) were recovered correctly by the parser; [Footnote_10] were in-correct because the wrong category was assigned to the relative pronoun, 3 were incorrect because the relative pronoun was attached to the wrong noun, and 1 was incorrect because the wrong category was assigned to the predicate from which the object was extracted."
10 The number of extracted objects need not equal the occur-rences of the category since coordination can introduce more than one object per category.
The tendency for the parser to assign the wrong category to the relative pronoun in part reflects the fact that complementiser that is fifteen times as frequent as object relative pronoun that.
"However, the supertagger alone gets 74% of the ob-ject relative pronouns correct, if it is used to provide a single category per word, so it seems that our de-pendency model is further biased against object ex-tractions, possibly because of the technical unsound-ness noted earlier."
"It should be recalled in judging these figures that they are only a first attempt at recovering these long-range dependencies, which most other wide-coverage parsers make no attempt to recover at all."
"To get an idea of just how demanding this task is, it is worth looking at an example of object relativiza-tion that the parser gets correct."
Figure 2 gives part of a dependency structure returned by the parser for a sentence from section 00 (with the relations omit-ted). [Footnote_11] Notice that both respect and confidence are objects of had.
11 The full sentence is The events of April through June dam-aged the respect and confidence which most Americans previ-ously had for the leaders of China.
"The relevant dependency quadruples found by the parser are the following: (13) , which - NP X NP X,1  S dcl 2 NP X - 2 - had . , which - NP X NP X,1  S dcl 2 NP X - 1 - confidence . , which - NP X NP X,1  S dcl 2 NP X - 1 - respect . , had - S dcl had NP 1  NP 2 - 2 - confidence . , had - S dcl had NP 1  NP 2 - 2 - respect ."
"This paper has shown that accurate, efficient wide-coverage parsing is possible with CCG."
"Along[REF_CITE], this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed."
The parser is able to capture a number of long-range dependencies that are not dealt with by ex-isting treebank parsers.
Capturing such dependen- cies is necessary for any parser that aims to sup-port wide-coverage semantic analysis—say to sup-port question-answering in any domain in which the difference between questions like Which company did Marks sue? and Which company sued Marks? matters.
"An advantage of our approach is that the recovery of long-range dependencies is fully inte-grated with the grammar and parser, rather than be-ing relegated to a post-processing phase."
"Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies."
"However, we believe that the results are promising."
In future work we will present an evaluation which teases out the differences in extracted and in-situ arguments.
"For the purposes of the statistical modelling, we are also considering building alterna-tive structures that include the long-range dependen-cies, but which can be modelled using better moti-vated probability models, such as generative mod-els."
"This will be important for applying the parser to tasks such as language modelling, for which the pos-sibility of incremental processing of CCG appears particularly attractive."
This paper compares a number of gen-erative probability models for a wide-coverage Combinatory Categorial Gram-mar (CCG) parser.
These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
"According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given[REF_CITE]for a lin-guistically less expressive grammar."
"In contrast[REF_CITE], we find a signif-icant improvement from modeling word-word dependencies."
The currently best single-model statistical parser[REF_CITE]achieves Parseval scores of over 89% on the Penn Treebank.
"However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parseval measures without committing itself on certain se-mantically significant decisions, such as predicting null elements arising from deletion or movement."
The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of ex-traction and coordination.
"We present a number of models over syntac-tic derivations of Combinatory Categorial Grammar (CCG, see[REF_CITE]and[REF_CITE], this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations."
"CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars, distin-guishing for example between many classes of verbs with different subcategorization frames."
"As a re-sult, the categorial lexicon extracted for this purpose from the training corpus has 1207 categories, com-pared with the 48 POS-tags of the Penn Treebank."
"On the other hand, grammar rules in CCG are lim-ited to a small number of simple unary and binary combinatory schemata such as function application and composition."
"This results in a smaller and less overgenerating grammar than standard PCFGs (ca. 3,000 rules when instantiated with the above cate-gories in sections 02-21, instead of &gt; 12,400 in the original Treebank representati[REF_CITE])."
"Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers."
"Therefore, we also evaluate performance using a dependency evalua-tion reported[REF_CITE], which counts word-word dependencies as determined by local trees and their labels."
"According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie. left or right, which is implicit in CCG categories) de-fines a h P ; H ; S i dependency between the head word of S, w S , and the head word of H, w H ."
This measure is neutral with respect to the branching factor.
"Fur-thermore, as noted[REF_CITE], it does not penalize equivalent analyses of multiple modi- fiers."
"In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not what the label of the local tree is which defines this depen-dency), scores can be compared across grammars with different sets of labels and different kinds of trees."
"In order to compare our performance with the parser[REF_CITE], we also evaluate our best model according to the dependency evaluation introduced for that parser."
For further discussion we refer the reader[REF_CITE].
CCGbank is a corpus of CCG normal-form deriva-tions obtained by translating the Penn Tree-bank trees using an algorithm described[REF_CITE].
"Almost all types of construction—with the exception of gap-ping and UCP (“Unlike Coordinate Phrases”) are covered by the translation procedure, which pro-cesses 98.3% of the sentences in the training corpus ([REF_CITE]-21) and 98.5% of the sentences in the test corpus[REF_CITE]."
The grammar contains a set of type-changing rules similar to the lexical rules described[REF_CITE].
Figure 1 shows a derivation taken from CCGbank.
"Cate-gories, such as (( S [ b ] n NP ) = PP ) ="
"NP , encode unsat-urated subcat frames."
"The complement-adjunct dis-tinction is made explicit; for instance as a nonexec-utive director is marked up as PP-CLR in the Tree-bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier."
"The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank."
"The verbal categories in CCGbank carry fea-tures distinguishing declarative verbs (and auxil-iaries) from past participles in past tense, past par-ticiples for passive, bare infinitives and ing-forms."
"There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement."
The derivations in CCGbank are “normal-form” in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary.
The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process.
"This model was originally described[REF_CITE], where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1."
"Given a (parent) node with category P, choose the expansion exp of P, where exp can be leaf (for lexical categories), unary (for unary ex-pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right)."
"If P is a leaf node, generate its head word w."
"Otherwise, generate the category of its head daughter H."
"If P is binary branching, gen-erate the category of its non-head daughter S (a complement or modifier of H)."
"The model itself includes no prior knowledge spe-cific to CCG other than that it only allows unary and binary branching trees, and that the sets of nontermi-nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability)."
"All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus."
We replace all rare words in the training data with their POS-tag.
"For all experiments reported here and in section 5, the frequency threshold was set to 5."
The performance of the baseline model is shown in the top row of table 3.
For six out of the 2379 sentences in our test corpus we do not get a parse. [Footnote_1]
1 We conjecture that the minor variations in coverage among the other models (except Grandparent) are artefacts of the beam.
The reason is that a lexicon consisting of the word-category pairs observed in the training corpus does not contain all the entries required to parse the test corpus.
"We discuss a simple, but imperfect, solution to this problem in section 7."
"State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes."
We too can extend the baseline model described in the previous section by including more features.
"Like the models[REF_CITE], the additional features in our model are generated probabilistically, whereas in the parser[REF_CITE]distance measures are assumed to be a function of the already generated structure and are not generated explicitly."
"In order to estimate the conditional probabilities of our model, we recursively smooth empirical es-timates ê i of specific conditional distributions with (possible smoothed) estimates of less specific distri-butions ẽ i 1 , using linear interpolation: ẽ i = λê i + ( 1 λ ) ẽ i 1 λ is a smoothing weight which depends on the par-ticular distribution. [Footnote_2]"
"2 We compute λ in the same way[REF_CITE], p. 185."
"When defining models, we will indicate a back-off level with a # sign between conditioning vari-ables, eg."
"A ; B # C # D means that we interpolate P̂ ( ::: j A ; B ; C ; D ) with P̃ ( ::: j A ; B ; C ) , which is an in-terpolation of P̂ ( ::: j A ; B ; C ) and P̂ ( ::: j A ; B ) ."
"The coordination feature We define a boolean feature, conj, which is true for constituents which expand to coordinations on the head path."
"S , + conj S = NP , + conj NP S = NP , conj S = NP [ c ] , + conj shares S = ( S n NP ) ( S n NP ) ="
"NP conj S = NP [ c ] , conj NP but S = ( S n NP ) ( S n NP ) ="
NP buys IBM NP sells Lotus
This feature is generated at the root of the sentence with P ( conj j TOP ) .
"For binary expansions, conj H is generated with P ( conj H j H ; S ; con j P ) and conj S is generated with P ( conj S j S # P ; exp P ; H ; conj P ) ."
Ta-ble 1 shows how conj is used as a conditioning vari-able.
"This is intended to allow the model to cap-ture the fact that, for a sentence without extraction, a CCG derivation where the subject is type-raised and composed with the verb is much more likely in right node raising constructions like the above."
The impact of the grandparent feature[REF_CITE]showed that a PCFG estimated from a version of the Penn Treebank in which the label of a node’s parent is attached to the node’s own label yields a substantial improvement (LP/LR: from 73.5%/69.7% to 80.0%/79.2%).
"The inclusion of an additional grandparent feature gives[REF_CITE]a slight improvement in the Maximum Entropy inspired model, but a slight decrease in performance for an MLE model."
"Table 3 (Grandparent) shows that a grammar transfor-mation like Johnson’s does yield an improvement, but not as dramatic as in the Treebank-CFG case."
At the same time coverage is reduced (which might not be the case if this was an additional feature in the model rather than a change in the representation of the categories).
"Both of these results are to be expected—CCG categories encode more contextual information than Treebank labels, in particular about parents and grandparents; therefore the his-tory feature might be expected to have less impact."
"Moreover, since our category set is much larger, appending the parent node will lead to an even more fine-grained partitioning of the data, which then results in sparse data problems."
"Distance measures for CCG Our distance mea-sures are related to those proposed[REF_CITE], which are appropriate for binary trees (un-like those[REF_CITE])."
"Every node has a left distance measure, ∆ L , measuring the distance from the head word to the left frontier of the constituent."
There is a similar right distance measure ∆ R .
"We implemented three different ways of measuring dis-tance: ∆ Adjacency measures string adjacency (0, 1 or 2 and more intervening words); ∆ Verb counts inter-vening verbs (0 or 1 and more); and ∆ Pct counts in-tervening punctuation marks (0, 1, 2 or 3 and more)."
"These ∆s are generated by the model in the follow-ing manner: at the root of the sentence, generate ∆ L with P ( ∆ L j TOP ) , and ∆ R with P ( ∆ R j TOP ; ∆ L ) ."
"Then, for each expansion, if it is a unary expan-sion, ∆ LH = ∆ LP and ∆ RH = ∆ RP with a probabil-ity of 1."
"If it is a binary expansion, only the ∆ in the direction of the sister changes, with a probability of P ( ∆ LH j ∆ LP H#P ; S ) if exp = right , and analo-gously for exp = left . ∆ LS and ∆ RS are conditioned on S and the ∆ of H and P in the direction of S: P ( ∆ LS j S#∆ RP ; ∆ RH ) and P ( ∆ RS j S ; ∆ LS #∆ RP ; ∆ RH ) ."
They are then used as further conditioning variables for the other distributions as shown in table 1.
Table 3 also gives the Parseval and dependency scores obtained with each of these measures. ∆ Pct has the smallest effect.
"However, our model does not yet contain anything like the hard constraint on punctuation marks[REF_CITE]."
It can therefore be assumed that the main influence of lexical head features (words and preterminals) in Collins’ Model 1 is on the structural probabilities.
"In CCG, by contrast, preterminals are lexical cat-egories, encoding complete subcategorization infor-mation."
They therefore encode more information about the expansion of a nonterminal than Treebank POS-tags and thus are more constraining.
"Generating a constituent’s lexical category c at its maximal projection (ie. either at the root of the tree, TOP, or when generating a non-head daughter S), and using the lexical category as conditioning vari- able (LexCat) increases performance of the baseline model as measured by h P ; H ; S i by almost 3%."
"In this model, c S , the lexical category of S depends on the category S and on the local tree in which S is generated."
"However, slightly worse performance is obtained for LexCatDep, a model which is identical to the original LexCat model, except that c S is also conditioned on c H , the lexical category of the head node, which introduces a dependency between the lexical categories."
"Since there is so much information in the lexical categories, one might expect that this would reduce the effect of conditioning the expansion of a con-stituent on its head word w."
"However, we did find a substantial effect."
Generating the head word at the maximal projection (HeadWord) increases perfor-mance by a further 2%.
"Finally, conditioning w S on w H , hence including word-word dependencies, (HWDep) increases performance even more, by an-other 3.5%, or 8.3% overall."
This is in stark contrast to Gildea’s findings for Collins’ Model 1.
We conjecture that the reason why CCG benefits more from word-word dependencies than Collins’
Model 1 is that CCG allows a cleaner parametriza-tion of these surface dependencies.
"In Collins’ Model 1, w S is conditioned not only on the local tree h P ; H ; S i , c H and w H , but also on the distance ∆ between the head and the modifier to be generated."
"However, Model 1 does not incorporate the notion of subcategorization frames."
"Instead, the distance measure was found to yield a good, if imperfect, ap-proximation to subcategorization information."
"Using our notation, Collins’ Model 1 generates w S with the following probability:"
P Collins1 ( w S j c S ; ∆ ; P ; H ; S ; c H ; w H ) = λ 1 P̂ ( w S j c S ; ∆ ; P ; H ; S ; c H ; w H ) + ( 1 λ 1 ) λ 2 P̂ ( w S j c S ; ∆ ; P ; H ; S ; c H )+ ( 1 λ 2 ) P̂ ( w S j c S ) —whereas the CCG dependency model generates w S as follows:
P CCGdep ( w S j c S ; P ; H ; S ; c H ; w H ) = λP̂ ( w S j c S ; P ; H ; S ; c H ; w H )+ ( 1 λ ) P̂ ( w S j c S )
"Since our P, H, S and c H are CCG categories, and hence encode subcategorization information, the lo-cal tree always identifies a specific argument slot."
Therefore it is not necessary for us to include a dis-tance measure in the dependency probabilities.
"The h P ; H ; S i labeled dependencies we report are not directly comparable[REF_CITE], since CCG categories encode subcategorization frames."
"For instance, if the direct object of a verb has been recognized as such, but a PP has been mistaken as a complement (whereas the gold standard says it is an adjunct), the fully labeled dependency eval-uation h P ; H ;"
S i will not award a point.
"Therefore, we also include in Table 3 a more comparable eval-uation h S i which only takes the correctness of the non-head category into account."
The reported fig-ures are also deflated by retaining verb features like tensed/untensed.
"If this is done (by stripping off all verb features), an improvement of 0.6% on the h P ; H ; S i score for our best model is obtained."
"When incorporating the adjacency distance mea-sure or the coordination feature into the dependency model (HWDep∆ and HWDepConj), overall per-formance is lower than with the dependency model alone."
We conjecture that this arises from data sparseness.
It cannot be concluded from these re-sults alone that the lexical dependencies make struc-tural information redundant or superfluous.
"Instead, it is quite likely that we are facing an estimation problem similar[REF_CITE], who reports that the inclusion of the grandparent feature worsens performance of an MLE model, but improves per-formance if the individual distributions are modelled using Maximum Entropy."
"This intuition is strength-ened by the fact that, on casual inspection of the scores for individual sentences, it is sometimes the case that the lexicalized models perform worse than the unlexicalized models."
"All of the experiments described above use the POS-tags as given by CCGbank (which are the Treebank tags, with some corrections necessary to acquire cor-rect features on categories)."
It is reasonable to as-sume that this input is of higher quality than can be produced by a POS-tagger.
"We therefore ran the dependency model on a test corpus tagged with the POS-tagger[REF_CITE], which is trained on the original Penn Treebank (see HWDep (+ tag-ger) in Table 3)."
"Performance degrades slightly, which is to be expected, since our approach makes so much use of the POS-tag information for un-known words."
"However, a POS-tagger trained on CCGbank might yield slightly better results."
"For example, in the interpretation of a coordinate structure like “buy and sell shares”, shares will head an object of both buy and sell."
"Similarly, in examples like “buy the company that wins”, the relative con-struction makes company depend upon both buy as object and wins as subject."
"As is well known[REF_CITE], DAG-like dependencies cannot in gen-eral be modeled with a generative approach of the kind taken here [Footnote_3] ."
3 It remains to be seen whether the more restricted reentran-cies of CCG will ultimately support a generative model.
The following table compares the two parsers according to the evaluation of surface and deep dependencies given[REF_CITE].
We use Clark et al.’s parser to generate these de-pendencies from the output of our parser (see[REF_CITE]) [Footnote_4] .
"4 Due to the smaller grammar and lexicon of Clark et al., our parser can only be evaluated on slightly over 94% of the sen-tences in section 23, whereas the figures[REF_CITE]are on 97%."
"One of the advantages of CCG is that it provides a simple, surface grammatical analysis of extraction and coordination."
"We investigate whether our best model, HWDep, predicts the correct analyses, using the development section 00."
"Coordination There are two instances of argu-ment cluster coordination (constructions like cost $ 5,000 in July and $ 6,000 in August) in the devel-opment corpus."
"Of these, HWDep recovers none correctly."
"This is a shortcoming in the model, rather than in CCG: the relatively high probability both of the NP modifier analysis of PPs like in July and of NP coordination is enough to misdirect the parser."
"On these, we obtain a labeled re-call and precision of 67.0%/67.3%."
"Interestingly, on the 24 instances of right node raising (coordination of ( S [ : ] n NP ) = NP ), our parser achieves higher per-formance, with labeled recall and precision of 79.2% and 73.1%."
Figure 2 gives an example of the output of our parser on such a sentence.
Extraction Long-range dependencies are not cap-tured by the evaluation used here.
"However, the ac-curacy for recovering lexical categories for words with “extraction” categories, such as relative pro-nouns, gives some indication of how well the model detects the presence of such dependencies."
"The most common category for subject relative pronouns, ( NP n NP ) = ( S [ dcl ] n NP ) , has been recov-ered with precision and recall of 97.1% (232 out of 239) and 94.3% (232/246)."
Embedded subject extraction requires the special lexical category (( S [ dcl ] n NP ) = NP ) = ( S [ dcl ] n NP ) for verbs like think.
"On this category, the model achieves a precision of 100% (5/5) and recall of 83.3% (5/6)."
"The case the parser misanalyzed is due to lexical coverage: the verb agree occurs in our lex-icon, but not with this category."
"The most common category for object relative pronouns, ( NP n NP ) = ( S [ dcl ] = NP ) , has a recall of 76.2% (16 out of 21) and precision of 84.2% (16/19)."
"Free object relatives, NP = ( S [ dcl ] = NP ) , have a recall of 84.6% (11/13), and precision of 91.7% (11/12)."
"However, object extraction appears more frequently as a reduced relative (the man John saw), and there are no lexical categories indicating this ex-traction."
Reduced relative clauses are captured by a type-changing rule NP n NP !
S [ dcl ] =
"This rule was applied 56 times in the gold standard, and 70 times by the parser, out of which 48 times it corre-sponded to a rule in the gold standard (or 34 times, if the exact bracketing of the S [ dcl ] ="
NP is taken into account—this lower figure is due to attachment de-cisions made elsewhere in the tree).
These figures are difficult to compare with stan-dard Treebank parsers.
"Despite the fact that the original Treebank does contain traces for move-ment, none of the existing parsers try to gener-ate these traces (with the exception of Collins’ Model 3, for which he only gives an overall score of 96.3%/98.8% P/R for subject extraction and 81.4%/59.4% P/R for other cases)."
"The only “long range” dependency for which Collins gives numbers is subject extraction h SBAR, WHNP, SG, R i , which has labeled precision and recall of 90.56% and 90.56%, whereas the CCG model achieves a labeled precision and recall of 94.3% and 96.[Footnote_5]% on the most frequency subject extraction dependency h NP n NP , ( NP n NP ) = ( S [ dcl ] n NP ) , S [ dcl ] n NP i , which occurs 262 times in the gold standard and was produced 256 times by our parser."
"5 We compute λ in the same way[REF_CITE], p. 185."
"However, out of the 15 cases of this relation in the gold standard that our parser did not return, 8 were in fact analyzed as subject extraction of bare infinitivals h NP n NP , ( NP n NP ) = ( S [ b ] n NP ) , S [ b ] n NP i , yielding a com-bined recall of 97.3%."
"The most serious problem facing parsers like the present one with large category sets is not so much the standard problem of unseen words, but rather the problem of words that have been seen, but not with the necessary category."
"For standard Treebank parsers, the latter problem does not have much impact, if any, since the Penn Treebank tagset is fairly small, and the grammar un-derlying the Treebank is very permissive."
"However, for CCG this is a serious problem: the first three rows in table 4 show a significant difference in per-formance for sentences with complete lexical cover-age (“No missing”) and sentences with missing lex-ical entries (“Missing”)."
"Using the POS-tags in the corpus, we can estimate the lexical probabilities P ( w j c ) using a linear in-terpolation between the relative frequency estimates P̂ ( w j c ) and the following approximation: 5"
P̃ tags ( w j c ) = ∑ t 2 tags P̂ ( w j t ) P̂ ( t j c )
We smooth the lexical probabilities as follows:
P̃ ( w j c ) = λP̂ ( w j c ) + ( 1 λ ) P̃ tags ( w j c )
Table 4 shows the performance of the baseline model with a frequency cutoff of 5 and 10 for rare words and with a smoothed and non-smoothed lexi-con. [Footnote_6]
6 Smoothing was only done for categories with a total fre-quency of 100 or more.
"This frequency cutoff plays an important role here - smoothing with a small cutoff yields worse performance than not smoothing, whereas smooth-ing with a cutoff of 10 does not have a significant impact on performance."
"Smoothing the lexicon in this way does make the parser more robust, result-ing in complete coverage of the test set."
"However, it does not affect overall performance, nor does it alle-viate the problem for sentences with missing lexical entries for seen words."
"We have compared a number of generative probabil-ity models of CCG derivations, and shown that our best model recovers 89.9% of word-word dependen-cies on section 23 of CCGbank."
"These figures are surprisingly close to the figure of 90.9% reported[REF_CITE]on section 00, given that, in order to allow a direct comparison, we have used the same interpolation technique and beam strategy[REF_CITE], which are very unlikely to be as well-tuned to our kind of grammar."
"As is to be expected, a statistical model of a CCG extracted from the Treebank is less robust than a model with an overly permissive grammar such[REF_CITE]."
This problem seems to stem mainly from the incomplete coverage of the lexicon.
We have shown that smoothing can compensate for en-tirely unknown words.
"However, this approach does not help on sentences which require previously un-seen entries for known words."
"We would expect a less naive approach such as applying morphologi-cal rules to the observed entries, together with better smoothing techniques, to yield better results."
We have also shown that a statistical model of CCG benefits from word-word dependencies to a much greater extent than a less linguistically moti-vated model such as Collins’ Model 1.
"This indi-cates to us that, although the task faced by a CCG parser might seem harder prima facie, there are advantages to using a more linguistically adequate grammar."
This paper proposes a new method for word translation disambiguation using a machine learning technique called ‘Bilingual Bootstrapping’.
Bilingual Bootstrapping makes use of ˈ in learning ˈ a small number of classified data and a large number of unclassified data in the source and the target languages in translation.
It constructs classifiers in the two languages in parallel and repeatedly boosts the performances of the classifiers by further classifying data in each of the two languages and by exchanging between the two languages information regarding the classified data.
Experimental results indicate that word translation disambiguation based on Bilingual Bootstrapping consistently and significantly outperforms the existing methods based on ‘Monolingual Bootstrapping’.
We address here the problem of word translation disambiguation.
"For instance, we are concerned with an ambiguous word in English (e.g., ‘plant’), ‘ Ꮉॖ (gongchang)’ and ‘ ỡ⠽ (zhiwu)’)."
"Our which has multiple translations in Chinese (e.g., goal is to determine the correct Chinese translation of the ambiguous English word, given an English sentence which contains the word."
"Word translation disambiguation is actually a special case of word sense disambiguation (in the example above, ‘gongchang’ corresponds to the sense of ‘factory’ and ‘zhiwu’ corresponds to the sense of ‘vegetation’). [Footnote_1]"
"1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs."
"In this paper, we propose a new method for word translation disambiguation using a bootstrapping technique we have developed."
We refer to the technique as ‘Bilingual Bootstrapping (BB)’.
"In order to evaluate the performance of BB, we conducted some experiments on word translation disambiguation using the BB technique and the MB technique."
All of the results indicate that BB consistently and significantly outperforms MB.
"The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method."
"In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label)."
Many methods for word sense disambiguation using a supervised learning technique have been proposed.
"They include those using Naïve Bayes[REF_CITE], Decision List[REF_CITE], Nearest Neighbor[REF_CITE], Transformation Based Learning[REF_CITE], Neural Network (Towell and"
"Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set[REF_CITE]."
"The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words)."
"Since preparing supervised learning data is expensive (in many cases, manually labeling data is required), it is desirable to develop a bootstrapping method that starts learning with a small number of classified data but is still able to achieve high performance under the help of a large number of unclassified data which is not expensive anyway."
"When applied to our current task, his method starts learning with a small number of English sentences which contain an ambiguous English word and which are respectively assigned with the correct Chinese translations of the word."
"It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data."
It also adopts the heuristics of ‘one sense per discourse’[REF_CITE]to further classify unclassified sentences.
"By repeating the above processes, it can create an accurate classifier for word translation disambiguation."
"For other related work, see, for example,[REF_CITE]."
"Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping."
"In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese."
"It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese."
The data in English and in Chinese are supposed to be not in parallel but from the same domain.
"BB constructs classifiers for English to Chinese translation disambiguation by repeating the following two steps: (1) constructing classifiers for each of the languages on the basis of the classified data in both languages, (2) using the constructed classifiers in each of the languages to classify some unclassified data and adding them to the classified training data set of the language."
The reason that we can use classified data in both languages at step (1) is that words in one language generally have translations in the other and we can find their translation relationship by using a dictionary.
"Let E denote a set of words in English, C a set of words in Chinese, and T a set of links in a translation dictionary as shown in Figure 1. (Any two linked words can be translation of each other.)"
"Mathematically, T is defined as a relation between E and C , i.e., T ⊆ E ×C ."
"Let ε stand for a random variable on E, γ a random variable on C. Also let e stand for a random variable on E, c a random variable on C, and t a random variable on T. While ε and γ represent words to be translated, e and c represent context words."
"For ε ,an English word T ε = {t | t = ( ε , γ ′),t ∈T} represents the links from it, and C ε ={ γ ′|( ε , γ ′)∈T} represents the Chinese words which are linked to it."
"For a Chinese word γ , let T γ ={t | t = ( ε ′, γ ),t ∈T} and E γ ={ ε ′|( ε ′, γ )∈T} ."
We can define C e and E c similarly.
"Let e denote a sequence of words (e.g., a sentence or a text) in English e = {e 1 ,e 2 ,L,e m }, e i ∈ E (i = 1,2,L,m) ."
"Let c denote a sequence of words in Chinese c = {c 1 ,c 2 ,L,c n }, c i ∈C (i = 1,2,L,n) ."
We view e and c as examples representing context information for translation disambiguation.
"For an English word ε , we define a binary classifier for resolving each of its translation ambiguities in T ε in a general form as:"
"P ε (t | e), t ∈T ε &amp; P ε (t | e), t ∈T ε −{t}, where e denotes an example in English."
"Similarly, for a Chinese word γ , we define a classifier as:"
"P γ (t | c), t ∈T γ &amp; P γ (t | c), t ∈T γ −{t}, where c denotes an example in Chinese."
"Let L ε denote a set of classified examples in English, each representing one context of ε L ε = {(e 1 ,t 1 ) ε ,(e 2 ,t 2 ) ε ,L,(e k ,t k ) ε }, t i ∈T ε (i = 1,2,L,k), and U ε a set of unclassified examples in English, each representing one context of ε U ε ={(e 1 ) ε ,(e 2 ) ε ,L,(e l ) ε }."
"Similarly, we denote the sets of classified and unclassified examples with respect to γ in Chinese L γ U γ andas respectively."
"Furthermore, we have"
"L E = U L ε ,L C = U L γ ,U E = U U ε ,U C = U U γ . ε ∈E γ ∈C ε ∈E γ ∈C"
We perform Bilingual Bootstrapping as described in Figure 2.
"Hereafter, we will only explain the process for English (left-hand side); the process for Chinese (right-hand side) can be conducted similarly."
"While we can in principle employ any kind of classifier in BB, we use here a Naïve Bayesian Classifier."
"At step 1 in BB, we construct the classifier as described in Figure 3."
"At step 2, for each example e, we calculate with the Naïve Bayesian Classifier: λ * (e) = max P ε (t | e) = max P ε (t)P ε (e | t) . t∈T ε P ε (t | e) t∈T ε P ε (t )P ε (e | t )"
The second equation is based on Bayes’ rule.
"In the calculation, we assume that the context words in e (i.e., e 1 ,e 2 , L ,e m ) are independently generated from P ε (e | t) and thus we have m P ε (e | t) = ∏ P ε (e | t). i i=1"
We can calculate P ε (e | t ) similarly.
"For P ε (e | t) , we calculate it at step 1 by linearly combining P ε (E) (e | t) estimated from English and P ε (C) (e | t) estimated from Chinese:"
"P ε (e | t) = (1− α − β )P ε (E) (e | t) (1) + α P ε (C) (e | t) + β P (U) (e), where 0 ≤ α ≤1 , 0 ≤ β ≤1 , α + β ≤1 , and P ( U ) ( e ) is a uniform distribution over E , which is used for avoiding zero probability."
"In this way, we estimate P ε (e| t) using information from not only English but also Chinese."
"For P ε (E) ( e | t ) , we estimate it with MLE (Maximum Likelihood Estimation) using L ε as data."
"For P ε ( C ) ( e | t ) , we estimate it as is described in Section 3.4."
"For the sake of readability, we rewrite P ε ( C ) ( e | t ) as P(e |t) ."
We define a finite mixture model of specific ε we assume that the data in
"L γ = {(c 1 ,t 1 ) γ ,(c 2 ,t 2 ) γ ,L,(c h ,t h ) γ }, t i ∈T γ (i =1,L,h), ∀ γ ∈C ε are independently generated on the basis of the model."
"We can, therefore, employ the Expectation and Maximization Algorithm (EM Algorithm)[REF_CITE]to estimate the parameters of the model including P(e |t) ."
We also use the relation T in the estimation.
"Initially, we set  1 P(c | e,t) =  | C | , if c∈C e , e   0, if c∉C e P(e|t)= 1 , e∈E. | E |"
We next estimate the parameters by iteratively updating them ass described in Figure 4 until they converge.
"Here f (c,t) stands for the frequency of c related to t."
The context information in Chinese is then ‘translated’ into that in English through the links in T.
We note that Monolingual Bootstrapping is a special case of Bilingual Bootstrapping (consider the situation in which α equals 0 in formula (1)).
"Moreover, it seems safe to say that BB can always perform better than MB."
The many-to-many relationship between the words in the two languages stands out as key to the higher performance of BB.
Suppose that the classifier with respect to ‘plant’ has two decisions (denoted as A and B in Figure 5).
"Further suppose that the classifiers with respect to ‘gongchang’ and ‘zhiwu’ in Chinese have two decisions respectively, (C and D) (E and F)."
"A and D are equivalent to each other (i.e., they represent the same sense), and so are B and E."
Assume that examples are classified after several iterations in BB as depicted in Figure 5.
"Here, circles denote the examples that are correctly classified and crosses denote the examples that are incorrectly classified."
"Since A and D are equivalent to each other, we can ‘translate’ the examples with D and use them to boost the performance of classification to A."
"This is because the misclassified examples (crosses) with D are those mistakenly classified from C and they will not have much negative effect on classification to A, even though the translation from Chinese into English can introduce some noises."
Similar explanations can be stated to other classification decisions.
"In contrast, MB only uses the examples in A and B to construct a classifier, and when the number of misclassified examples increases (this is inevitable in bootstrapping), its performance will stop improving."
"While it is possible to straightforwardly apply the algorithm of BB described in Section 3 to word translation disambiguation, we use here a variant of it for a better adaptation to the task and for a fairer comparison with existing technologies."
"The variant of BB has four modifications. (1) It actually employs an ensemble of the Naïve Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC[REF_CITE]."
"In an ensemble, it creates different NBCs using as data the words within different window sizes surrounding the word to be disambiguated (e.g., ‘plant’ or ‘zhiwu’) and further constructs a new classifier by linearly combining the NBCs. (2) It employs the heuristics of ‘one sense per discourse’ (cf.,[REF_CITE]) after using an ensemble of NBCs. (3) It uses only classified data in English at the beginning. (4) It individually resolves ambiguities on selected English words such as ‘plant’, ‘interest’."
"As a result, in the case of ‘plant’; for example, the classifiers with respect to ‘gongchang’ and ‘zhiwu’ only make classification decisions to D and E but not C and F (in Figure 5)."
It calculates λ * ( c ) as λ * ( c ) = P( c |t) and sets θ = 0 at the right-hand side of step 2.
We consider here two implementations of MB for word translation disambiguation.
"In the first implementation, in addition to the basic algorithm of MB, we also use (1) an ensemble of Naïve Bayesian Classifiers, (2) the heuristics of ‘one sense per discourse’, and (3) a small number of classified data in English at the beginning."
We will denote this implementation as MB-B hereafter.
The second implementation is different from the first one only in (1).
"That is, it employs as a classifier a decision list instead of an ensemble of NBCs."
"This implementation is exactly the one proposed[REF_CITE], and we will denote it as MB-D hereafter."
MB-B and MB-D can be viewed as the state-of-the-art methods for word translation disambiguation using bootstrapping.
We conducted two experiments on English-Chinese translation disambiguation.
"We first applied BB, MB-B, and MB-D to translation of the English words ‘line’ and ‘interest’ using a benchmark data [URL_CITE] ."
The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word
"Sense Disambiguation (WSD) on the two words (e.g.,[REF_CITE])."
"We adopted from the HIT dictionary [Footnote_3] the Chinese translations of the two English words, as listed in Table 1."
3 The dictionary is created by Harbin Institute of Technology.
One sense of the words corresponds to one group of translations.
"We then used the benchmark data as our test data. (For the word ‘interest’, we only used its four major senses, because the remaining two minor senses occur in only 3.3% of the data)"
"As classified data in English, we defined a ‘seed word’ for each group of translations based on our intuition (cf., Table 1)."
Each of the seed words was then used as a classified ‘sentence’.
This way of creating classified data is similar to that[REF_CITE].
"As unclassified data in English, we collected sentences in news articles from a web site[URL_CITE]and as unclassified data in Chinese, we collected sentences in news articles from another web site (news.cn.tom.com)."
We observed that the distribution of translations in the unclassified data was balanced.
Table 2 shows the sizes of the data.
"Note that there are in general more unclassified sentences in Chinese than in English because an English word usually has several Chinese words as translations (cf., Figure 5)."
"As a translation dictionary, we used the HIT dictionary, which contains about 76000[REF_CITE]0 English words, and 118000 links."
"We then used the data to conduct translation disambiguation with BB, MB-B, and MB-D, as described in Section 5."
"For both BB and MB-B, we used an ensemble of five Naïve Bayesian Classifiers with the window sizes being ± 1, ± 3, ± [URL_CITE], ± 7, ± 9 words."
"For both BB and MB-B, we set the parameters of β , b, and θ to 0.2, 15, and 1.5 respectively."
"The parameters were tuned based on our preliminary experimental results on MB-B, they were not tuned, however, for BB."
"For the BB specific parameter α , we set it to 0.[URL_CITE], which meant that we treated the information from English and that from Chinese equally."
Table 3 shows the translation disambiguation accuracies of the three methods as well as that of a baseline method in which we always choose the major translation.
"Figures 6 and 7 show the learning curves of MB-D, MB-B, and BB."
Figure 8 shows the accuracies of BB with different α values.
"From the results, we see that BB consistently and significantly outperforms both MB-D and MB-B. The results from the sign test are statistically significant (p-value &lt; 0.001)."
"Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf.,[REF_CITE])."
"Although BB is a method nearly equivalent to one based on unsupervised learning, it still performs favorably well when compared with the supervised methods (note that since the experimental settings are different, the results cannot be directly compared)."
We also conducted translation on seven of the twelve English words studied[REF_CITE].
Table 5 shows the list of the words.
"For each of the words, we extracted about 200 sentences containing the word from the Encarta 4 English corpus and labeled those sentences with Chinese translations ourselves."
We used the labeled sentences as test data and the remaining sentences as unclassified data in English.
We also used the sentences in the Great Encyclopedia 5 Chinese corpus as unclassified data in Chinese.
"We defined, for each translation, a seed word in English as a classified example (cf., Table 5)."
"We did not, however, conduct translation disambiguation on the words ‘crane’, ‘sake’, ‘poach’, ‘axes’, and ‘motion’, because the first four words do not frequently occur in the Encarta corpus, and the accuracy of choosing the major translation for the last word has already exceeded 98%."
"We next applied BB, MB-B, and MB-D to word translation disambiguation."
The experiment settings were the same as those in Experiment 1.
"From Table 6, we see again that BB significantly outperforms MB-D and MB-B. (We will describe the results in detail in the full version of this paper.)"
"Note that the results of MB-D here cannot be directly compared with those[REF_CITE], mainly because the data used are different."
We investigated the reason of BB’s outperforming MB and found that the explanation on the reason in Section 4 appears to be true according to the following observations. (1)
"In a Naïve Bayesian Classifier, words having large values of probability ratio P(e|t) have P(e | t ) strong influence on the classification of t when they occur, particularly, when they frequently occur."
We collected the words having large values of probability ratio for each t in both BB and MB-B and found that BB obviously has more ‘relevant words’ than MB-B. Here ‘relevant words’ for t refer to the words which are strongly indicative to t on the basis of human judgments.
"Table 7 shows the top ten words in terms of probability ratio for the ‘ ߽ᙃ ’ translation (‘money paid for the use of money’) with respect to BB and MB-B, in which relevant words are underlined."
"Figure 9 shows the numbers of relevant words for the four translations of ‘interest’ with respect to BB and MB-B. (2) From Figure 8, we see that the performance of BB remains high or gets higher when α becomes larger than 0.4 (recall that β was fixed to 0.2)."
"This result strongly indicates that the information from Chinese has positive effects on disambiguation. (3) One may argue that the higher performance of BB might be attributed to the larger unclassified data size it uses, and thus if we increase the unclassified data size for MB, it is likely that MB can perform as well as BB."
We conducted an additional experiment and found that this is not the case.
"Actually, the accuracies of MB-B cannot further improve when unlabeled data sizes increase."
This paper has presented a new word translation disambiguation method using a bootstrapping technique called Bilingual Bootstrapping.
Experimental results indicate that BB significantly outperforms the existing Monolingual Bootstrapping technique in word translation disambiguation.
This is because BB can effectively make use of information from two sources rather than from one source as in MB.
"In this paper, we investigate the practical applicability of Co-Training for the task of building a classifier for reference reso-lution."
We are concerned with the ques-tion if Co-Training can significantly re-duce the amount of manual labeling work and still produce a classifier with an ac-ceptable performance.
A major obstacle for natural language processing systems which analyze natural language texts or utterances is the need to identify the entities re-ferred to by means of referring expressions.
"Among referring expressions, pronouns and definite noun phrases (NPs) are the most prominent."
"Supervised machine learning algorithms were used for pronoun resolution with good results[REF_CITE], and for definite NPs with fairly good re-sults[REF_CITE]."
"However, the defi-ciency of supervised machine learning approaches is the need for an unknown amount of annotated train-ing data for optimal performance."
"So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training[REF_CITE]."
"Among others Co-Training was applied to document classificati[REF_CITE], named-entity recogniti[REF_CITE], noun phrase bracketing[REF_CITE], and statistical parsing[REF_CITE]."
In this paper we apply Co-Training to the problem of reference reso-lution in German texts from the tourism domain in order to provide answers to the following questions:
Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)?
How much labeled training data is required for achieving a reasonable performance?
"First, we discuss features that have been found to be relevant for the task of reference resolution, and describe the feature set that we are using (Section 2)."
"Then we briefly introduce the Co-Training paradigm (Section 3), which is followed by a description of the corpus we use, the corpus annotation, and the way we prepared the data for using a binary classifier in the Co-Training algorithm (Section 4)."
In Section 5 we specify the experimental setup and report on the results.
"Driven by the necessity to provide robust systems for the MUC system evaluations, researchers began to look for those features which were particular im-portant for the task of reference resolution."
"While most features for pronoun resolution have been de-scribed in the literature for decades, researchers only recently began to look for robust and cheap features, i.e., those which perform well over several domains and can be annotated (semi-) automatically."
"Also, the relative quantitative contribution of each of these features came into focus only after the advent of corpus-based and statistical methods."
"In the follow-ing, we describe a few earlier contributions with re-spect to the features used."
"Decision tree algorithms were used for ref-erence resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al. (2001, C5.0)."
This approach requires the definition of a set of training features de-scribing pairs of anaphors and their antecedents.
"They do not mention all of these explicitly but emphasize the features POS-tag, grammatical role, semantic class and distance."
The set of semantic classes they use appears to be rather elaborated and highly domain-dependent.
"They mention that it was important for the training data to contain transitive positives, i.e., all possible coreference relations within an anaphoric chain."
They dis-tinguish between features which focus on individ-ual noun phrases (e.g. Does noun phrase contain a name?) and features which focus on the anaphoric relation (e.g. Do both share a common NP?).
It was criticized[REF_CITE]that the features used[REF_CITE]are highly id-iosyncratic and applicable only to one particular do-main.
"However, only a defined subset of all possible ref-erence resolution cases was considered relevant in the MUC-5 task description, e.g., only entity refer-ences."
"For this case, the domain-dependent features may have been particularly important, making it dif-ficult to compare the results of this approach to oth-ers working on less restricted domains."
They show a part of their decision tree in which the weak string identity feature (i.e. iden-tity after determiners have been removed) appears to be the most important one.
"They also report on the relative contribution of the features where the three features weak string identity, alias (which maps named entities in order to resolve dates, per-son names, acronyms, etc.) and appositive seem to cover most of the cases (the other nine features con-tribute only 2.3% F-measure for MUC-6 texts and 1% F-measure for MUC-7 texts)."
"They only used pairs of anaphors and their closest antecedents as positive examples in training, but evaluated accord-ing[REF_CITE]."
"They use the features shown in Table 2, all of which are obtained automatically without any manual tagging."
The feature semantic class used[REF_CITE]seems to be a domain-dependent one which can only be used for the MUC domain and similar ones.
We consider the features we use for our weakly supervised approach to be domain-independent.
We distinguish between features assigned to noun phrases and features assigned to the potential coref-erence relation.
They are listed in Table 3 together with their respective possible values.
In the liter-ature on reference resolution it is claimed that the antecedent’s grammatical function and its realiza-tion are important.
Hence we introduce the features ante gram func and ante npform.
The identity in grammatical function of a potential anaphor and an-tecedent is captured in the feature syn par.
"Since in German the gender and the semantic class do not necessarily coincide (i.e. objects are not necessarily neuter as in English) we also provide a semantic-class feature which captures the difference between human, concrete, and abstract objects."
This basi-cally corresponds to the gender attribute in English.
"The feature wdist captures the distance in words be-tween anaphor and antecedent, the feature ddist cap-tures the distance in sentences, the feature mdist the number of markables (NPs) between anaphor and antecedent."
"Features like the string ident and sub-string match features were used by other researchers[REF_CITE], while the features ante med and ana med were used[REF_CITE]in order to improve the performance for definite NPs."
"The minimum edit distance (MED) computes the simi-larity of strings by taking into account the minimum number of editing operations (substitutions s, inser-tions i, deletions d) needed to transform one string into the other[REF_CITE]."
The MED is computed from these editing operations and the length of the potential antecedent m or the length of the anaphor n.
Co-Training[REF_CITE]is a meta-learning algorithm which exploits unlabeled in ad-dition to labeled training data for classifier learn-ing.
A Co-Training classifier is complex in the sense that it consists of two simple classifiers (most often
"Naive Bayes, e.g.[REF_CITE]and[REF_CITE])."
"Initially, these classifiers are trained in the conventional way using a small set of size L of labeled training data."
"In this process, each of the two classifiers is trained on a different subset of features of the training data."
"These feature subsets are commonly referred to as different views that the classifiers have on the data, i.e., each classi-fier describes a given instance in terms of different features."
The Co-Training algorithm is supposed to bootstrap by gradually extending the training data with self-labeled instances.
It utilizes the two classi-fiers by letting them in turn label the p best positive and n best negative instances from a set of size P of unlabeled training data (referred to in the litera-ture as the pool).
"Instances labeled by one classifier are then added to the other’s training data, and vice versa."
"After each turn, both classifiers are re-trained on their augmented training sets, and the pool is re- filled with unlabeled training instances drawn at random."
This process is repeated either for a given number of iterations I or until all the unla-beled data has been labeled.
In particular the defi-nition of the two data views appears to be a crucial factor which can strongly influence the behaviour of Co-Training.
"A number of requirements for these views are mentioned in the literature, e.g., that they have to be disjoint or even conditionally indepen-dent (but cf."
"Another im-portant factor is the ratio between p and n, i.e., the number of positive and negative instances added in each iteration."
These values are commonly chosen in such a way as to reflect the empirical class distri-bution of the respective instances.
"Our corpus consists of 250 short German texts (total 36924 tokens, 9399[REF_CITE]anaphoric NPs) about sights, historic events and persons in Heidelberg."
The average length of the texts was 149 tokens.
The texts were POS-tagged using TnT[REF_CITE].
A basic identification of markables (i.e. NPs) was ob-tained by using the NP-Chunker Chunkie[REF_CITE].
The POS-tagger was also used for assigning attributes to markables (e.g. the NP form).
The automatic annotation was followed by a man- ual correction and annotation phase in which further tags were assigned to the markables.
In this phase manual coreference annotation was performed as well.
"In our annotation, coreference is represented in terms of a member attribute on markables (i.e., noun phrases)."
Markables with the same value in this attribute are considered coreferring expressions.
The annotation was performed by two students.
The reliability of the annotations was checked using the kappa statistic[REF_CITE].
The problem of coreference resolution can easily be formulated in such a way as to be amenable to Co-Training.
"The most straightforward definition turns the task into a binary classification: Given a pair of potential anaphor and potential antecedent, classify as positive if the antecedent is in fact the closest an-tecedent, and as negative otherwise."
Note that the re-striction of this rule to the closest antecedent means that transitive antecedents (i.e. those occuring fur-ther upwards in the text as the direct antecedent) are treated as negative in the training data.
We favour this definition because it strengthens the pre-dictive power of the word distance between poten-tial anaphor and potential antecedent (as expressed in the wdist feature).
"From our annotated corpus, we created one initial training and test data set."
"For each text, a list of noun phrases in document order was generated."
"This list was then processed from end to beginning, the phrase at the current position being considered as a potential anaphor."
"Beginning with the directly pre-ceding position, each noun phrase which appeared before was combined with the potential anaphor and both entities were considered a potential antecedent-anaphor pair."
"If applied to a text with noun phrases, this algorithm produces a total of 68739-6;@ =: &lt;?&gt; noun phrase pairs."
"However, a number of filters can reasonably be applied at this point."
"An antecedent-anaphor pair is discarded if the anaphor is an indefinite NP, if one entity is embedded into the other, e.g., if the potential anaphor is the head of the poten-tial antecedent NP (or vice versa), if both entities have different values in their se-mantic class attributes [Footnote_1] , if either entity has a value other than 3rd person singular or plural in its agreement feature, if both entities have different values in their agreement features 2 ."
"1 This filter applies only if none of the expressions is a pro-noun. Otherwise, filtering on semantic class is not possible be-"
"For some texts, these heuristics reduced to up to 50% the potential antecedent-anaphor pairs, all of which would have been negative cases."
We regard these cases as irrelevant because they do not con-tribute any knowledge for the classifier.
"After appli-cation of these filters, the remaining candidate pairs were labeled as follows:"
Pairs of anaphors and their direct (i.e. clos-est) antecedents were labeled P.
This means that each anaphoric expression produced ex-actly one positive instance.
Pairs of anaphors and their indirect (transitive) antecedents were labeled TP.
Pairs of anaphors and those non-antecedents which occurred before the direct antecedent were labeled N.
The number of negative in-stances that each expression produced thus de-pended on the number of non-antecedents oc-curring before the direct antecedent (if any).
Pairs of anaphors and non-antecedents were la-beled DN (distant N) if at least one true an-tecedent occurred in between.
From this set the last 50 texts were used as a test set.
"From this set, all instances with class DN and TP were removed, resulting in a test set of 11033 instances."
Removing DNs and TPs was motivated by the fact that initial experimentation with C4.5 had indicated that a four way classification gives no advantage over a two way classification.
"In ad-dition, this kind of test set approximates the deci-sions made by a simple resolution algorithm that looks for an antecedent from the current position up-wards until it finds one or reaches the beginning."
"Hence, our results are only indirectly comparable with the ones obtained by an evaluation according[REF_CITE]."
"However, in this paper we only compare results of this direct binary antecedent-anaphor pair decision."
The remaining texts were split in two sets of 50 resp. 150 texts.
"From the first, our labeled train-ing set was produced by removing all instances with class DN and TP."
The second set was used as our un-labeled training set.
"From this set, no instances were removed because no knowledge whatsoever about the data can be assumed in a realistic setting."
For our experiments we implemented the standard Co-Training algorithm (as described in Section 3) in Java using the Weka machine learning library [Footnote_3] .
"In contrast to other Co-Training approaches, we did not use Naive Bayes as base classifiers, but J48 decision trees, which are a Weka re-implementation of C4.5."
The use of decision tree classifiers was motivated by the observation that they appeared to perform better on the task at hand.
We conducted a number of experiments to inves-tigate the question if Co-Training is beneficial for the task of training a classifier for coreference res-olution.
"In previous work[REF_CITE]we obtained quite different results for different types of anaphora, i.e. if we split the data according to the ana np feature into personal and possessive pro-nouns (PPER PPOS), proper names (NE), and def-inite NPs (def NP)."
"Therefore we performed Co-Training experiments on subsets of our data defined by these NP forms, and on the whole data set."
"We determined the features for the two differ-ent views with the following procedure: We trained classifiers on each feature separately and chose the best one, adding the feature which produced it as the first feature of view 1."
"We then trained classifiers on all remaining features separately, again choosing the best one and adding its feature as the first feature of view [Footnote_2]."
2 This filter applies only if the anaphor is a pronoun. This re-striction is necessary because German allows for cases where an antecedent is referred back to by a non-pronoun anaphor which has a different grammatical gender.
"In the next step, we enhanced the first classi-fier by combining it with all remaining features sep-arately."
The classifier with the best performance was then chosen and its new feature added as the second feature of view 1.
"We then enhanced the second clas-sifier in the same way by selecting from the remain-ing features the one that most improved it, adding this feature as the second one of view 2."
"This pro-cess was repeated until no features were left or no significant improvement was achieved, resulting in the views shown in Table 4 (features marked na were not available for the respective class)."
This way we determined two views which performed reasonably well separately.
"For Co-Training, we committed ourselves to fixed parameter settings in order to reduce the complexity of the experiments."
"Settings are given in the relevant subsections, where the following abbreviations are used: L=size of labeled training set, P/N=number of positive/negative instances added per iteration."
All reported Co-Training results are averaged over 5 runs utilizing randomized sequences of unlabeled instances.
"We compare the results we obtained with Co-Training with the initial result before the Co-Training process started (zero iterations, both views combined; denoted as XX 0its in the plots)."
"For this, we used a conventional C4.5 decision tree classi-fier (J48 implementation, default settings) on labeled training data sets of the same size used for the re-spective Co-Training experiment."
We did this in or-der to verify the quality of the training data and for obtaining reference values for comparison with the
"In Figure 1, three curves and three baselines are plotted:[REF_CITE](L=20), 20 0its is the baseline, i.e. the initial result obtained by just com-bining the two initial classifiers."
"The other settings were: P=1, N=1, Pool=10."
"As can be seen, the baselines slightly outperform the Co-Training curves (except for 100)."
Then we ran the Co-Training experiment with the NP form NE (i.e. proper names).
"Since the dis-tribution of positive and negative examples in the la-beled training data was quite different from the pre-vious experiment, we used P=1, N=33, Pool=120."
"Since all results with L B 200 were equally poor, we started with L=200, where the results were closer to ones of classifiers using the whole data set."
The resulting Co-Training curve degrades substantially.
"However, with a training size of 1000 and 2000 the Co-Training curves are above their baselines. def NP."
"In the next experiment we tested the NP form def NP, a concept which can be expected to be far more difficult to learn than the previous two NP forms."
"Used settings were P=1, N=30, Pool=120."
"For L B 500, F-measure was near 0."
With L=500 the Co-Training curve is way below the baseline.
"How-ever, with L=1000 and L=2000 Co-Training does show some improvement."
"In the last experiment we trained our classi-fier on all NP forms, using P=1, N=33, Pool=120."
With L=200 the baseline clearly outperforms Co-Training.
"Co-Training with L=1000 initially rises above the baselines, but then decreases after about 15 to 20 iterations."
With L=2000 the Co-Training curve approximates its baseline and then degener-ates.
Supervised learning of reference resolution classi-fiers is expensive since it needs unknown amounts of annotated data for training.
"However, refer-ence resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure[REF_CITE]."
"Unsupervised learn-ing might be an alternative, since it does not need any annotation at all."
"However, the cost is the de-crease in performance to about 53% F-measure on the same data[REF_CITE]which may be unsuitable for a lot of tasks."
In this paper we tried to pioneer a path between the unsupervised and the supervised paradigm by using the Co-Training meta-learning algorithm.
"The results, however, are mostly negative."
"Al-though we did not try every possible setting for the Co-Training algorithm, we did experiment with dif-ferent feature views, Pool sizes and positive/negative increments, and we assume the settings we used are reasonable."
It seems that Co-Training is use-ful in rather specialized constellations only.
"For the classes PPER PPOS, NE and All, our Co-Training experiments did not yield any benefits worth re-porting."
"Only for def NP, we observed a consid-erable improvement from about 17% to about 25% F-measure using an initial training set of 1000 la-beled instances, and from about 19% to about 28% F-measure using 2000 labeled training instances."
"Although based on much more labeled training data, these experiments did not yield significantly better results."
"In this case, therefore, Co-Training seems to be able to save manual annotation work."
"On the other hand, the definition of the feature views is non-trivial for the task of training a reference res-olution classifier, where no obvious or natural fea-ture split suggests itself."
"In practical terms, there-fore, this could outweigh the advantage of annota- tion work saved."
"Another finding of our work is that for personal and possessive pronouns, rather small numbers of labeled training data (about 100) seem to be suffi-cient for obtaining classifiers with a performance of about 80% F-measure."
"To our knowledge, this fact has not yet been reported in the literature."
"While we restricted ourselves in this work to rather small sets of labeled training data, future work on Co-Training will include further experi-ments with larger data sets."
"The work presented here has been partially funded by the German Ministry of Research and Technology as part of the E MBASSI project (01[REF_CITE]D/2, 01[REF_CITE]S 8), by Sony International (Europe) GmbH and by the Klaus Tschira Foundation."
"We would like to thank our an-notators Anna Björk Nikulásdôttir, Berenike Loos and Lutz Wind."
"This paper refines the analysis of co-training, defines and evaluates a new co-training algorithm that has theo-retical justification, gives a theoreti-cal justification for the Yarowsky algo-rithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions."
"The term bootstrapping here refers to a prob-lem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier."
"The plen-itude of unlabeled natural language data, and the paucity of labeled data, have made boot-strapping a topic of interest in computational linguistics."
"Current work has been spurred by two papers,[REF_CITE]and[REF_CITE]."
"Blum and Mitchell propose a conditional in-dependence assumption to account for the effi-cacy of their algorithm, called co-training, and they give a proof based on that conditional in-dependence assumption."
"They also give an in-tuitive explanation of why co-training works, in terms of maximizing agreement on unla-beled data between classifiers based on different “views” of the data."
"Finally, they suggest that the Yarowsky algorithm is a special case of the co-training algorithm."
"The Blum and Mitchell paper has been very influential, but it has some shortcomings."
"The proof they give does not actually apply directly to the co-training algorithm, nor does it directly justify the intuitive account in terms of classifier agreement on unlabeled data, nor, for that mat-ter, does the co-training algorithm directly seek to find classifiers that agree on unlabeled data."
"Moreover, the suggestion that the Yarowsky al-gorithm is a special case of co-training is based on an incidental detail of the particular applica-tion that Yarowsky considers, not on the prop-erties of the core algorithm."
"In recent work,[REF_CITE]prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “view” of the data."
This addresses one of the shortcomings of the original co-training paper: it gives a proof that justifies searching for classifiers that agree on unlabeled data.
I extend this work in two ways.
"First,[REF_CITE]assume the same conditional independence assumption as proposed by Blum and Mitchell."
"I show that that independence as-sumption is remarkably powerful, and violated in the data; however, I show that a weaker as-sumption suffices."
"Second, I give an algorithm that finds classifiers that agree on unlabeled data, and I report on an implementation and empirical results."
"Finally, I consider the question of the re-lation between the co-training algorithm and the Yarowsky algorithm."
"I suggest that the Yarowsky algorithm is actually based on a dif-ferent independence assumption, and I show that, if the independence assumption holds, the Yarowsky algorithm is effective at finding a high-precision classifier."
"A bootstrapping problem consists of a space of instances X, a set of labels L, a function"
"Y : X → L assigning labels to instances, and a space of rules mapping instances to la-bels."
"Rules may be partial functions; we write F(x) = ⊥ if F abstains (that is, makes no pre-diction) on input x. “Classifier” is synonymous with “rule”."
It is often useful to think of rules and labels as sets of instances.
A binary rule F can be thought of as the characteristic function of the set of instances {x : F(x) = +}.
Multi-class rules also define useful sets when a particular target class ` is understood.
"For any rule F, we write F ` for the set of instances {x : F(x) = `}, or (ambiguously) for that set’s characteristic function."
"We write F̄ ` for the complement of F ` , either as a set or characteristic function."
Note that F̄ ` contains instances on which F abstains.
We write F ¯` for {x : F(x) =6 ` ∧ F(x) =6 ⊥}.
"When F does not abstain, F̄ ` and F ¯` are identical."
"Finally, in expressions like Pr[F = +|Y = +] (with square brackets and “Pr”), the functions F(x) and Y (x) are used as random variables."
"By contrast, in the expression P(F|Y ) (with parentheses and “P”), F is the set of instances for which F(x) = +, and Y is the set of in-stances for which Y (x) = +."
"Blum and Mitchell assume that each instance x consists of two “views” x 1 , x 2 ."
We can take this as the assumption of functions X 1 and X 2 such that X 1 (x) = x 1 and X 2 (x) = x 2 .
They propose that views are conditionally independent given the label.
"Definition 1 A pair of views x 1 , x 2 satisfy view independence just in case:"
"Pr[X 1 = x 1 |X 2 = x 2 , Y = y] = Pr[X 1 = x 1 |Y = y] Pr[X 2 = x 2 |X 1 = x 1 , Y = y] ="
Pr[X 2 = x 2 |Y = y]
"A classification problem instance satisfies view independence just in case all pairs x 1 , x 2 satisfy view independence."
There is a related independence assumption that will prove useful.
"Let us define H 1 to con-sist of rules that are functions of X 1 only, and define H 2 to consist of rules that are functions of X 2 only."
"Definition 2 A pair of rules F ∈ H 1 , G ∈ H 2 satisfies rule independence just in case, for all u, v, y:"
"Pr[F = u|G = v,Y = y] = Pr[F = u|Y = y] and similarly for F ∈ H 2 , G ∈ H 1 ."
A classi-fication problem instance satisfies rule indepen-dence just in case all opposing-view rule pairs satisfy rule independence.
"If instead of generating H 1 and H 2 from X 1 and X 2 , we assume a set of features F (which can be thought of as binary rules), and take H 1 = H 2 = F, rule independence reduces to the Naive Bayes independence assumption."
The following theorem is not difficult to prove; we omit the proof.
Theorem 1 View independence implies rule independence.
Blum and Mitchell’s paper suggests that rules that agree on unlabelled instances are useful in bootstrapping.
Definition 3 The agreement rate between rules F and G is
"Pr[F = G|F, G =6 ⊥]"
Note that the agreement rate between rules makes no reference to labels; it can be deter-mined from unlabeled data.
The algorithm that Blum and Mitchell de-scribe does not explicitly search for rules with good agreement; nor does agreement rate play any direct role in the learnability proof given in the Blum and Mitchell paper.
The second lack is emended[REF_CITE].
"They show that, if view inde-pendence is satisfied, then the agreement rate between opposing-view rules F and G upper bounds the error of F (or G)."
The following statement of the theorem is simplified and as-sumes non-abstaining binary rules.
"Theorem 2 For all F ∈ H 1 , G ∈ H 2 that sat-isfy rule independence and are nontrivial predic-tors in the sense that min u Pr[F = u] &gt; Pr[F =6"
"G], one of the following inequalities holds:"
Pr[F =6 Y ] ≤ Pr[F 6= G] Pr[F̄ =6 Y ] ≤ Pr[F =6 G]
"If F agrees with G on all but ² unlabelled in-stances, then either F or F̄ predicts Y with er-ror no greater than ²."
A small amount of la-belled data suffices to choose between F and F̄.
I give a geometric proof sketch here; the reader is referred to the original paper for a for-mal proof.
Consider figures 1 and 2.
"In these diagrams, area represents probability."
"For ex-ample, the leftmost box (in either diagram) rep-resents the instances for which Y = +, and the area of its upper left quadrant represents Pr[F = +,G = +,Y = +]."
"Typically, in such a diagram, either the horizontal or vertical line is broken, as in figure 2."
"In the special case in which rule independence is satisfied, both hori-zontal and vertical lines are unbroken, as in fig-ure 1."
Theorem 2 states that disagreement upper bounds error.
"First let us consider a lemma, to wit: disagreement upper bounds minority prob-abilities."
"Define the minority value of F given Y = y to be the value u with least probability Pr[F = u|Y = y]; the minority probability is the probability of the minority value. (Note that minority probabilities are conditional probabili-ties, and distinct from the marginal probability min u Pr[F = u] mentioned in the theorem.)"
"In figure 1a, the areas of disagreement are the upper right and lower left quadrants of each box, as marked."
The areas of minority values are marked in figure 1b.
It should be obvious that the area of disagreement upper bounds the area of minority values.
"The error values of F are the values opposite to the values of Y : the error value is − when Y = + and + when Y = −. When minority values are error values, as in figure 1, disagree-ment upper bounds error, and theorem 2 follows immediately."
"However, three other cases are possible."
One possibility is that minority values are opposite to error values.
"In this case, the minority val-ues of F̄ are error values, and disagreement be-tween F and G upper bounds the error of F̄."
This case is admitted by theorem 2.
"In the final two cases, minority values are the same regardless of the value of Y. In these cases, however, the predictors do not satisfy the “non-triviality” condition of theorem 2, which re-quires that min u Pr[F = u] be greater than the disagreement between F and G."
Rule independence is a very strong assumption; one remarkable consequence will show just how strong it is.
The precision of a rule F is de-fined to be Pr[Y = +|F = +]. (We continue to assume non-abstaining binary rules.)
"If rule in-dependence holds, knowing the precision of any one rule allows one to exactly compute the preci-sion of every other rule given only unlabeled data and knowledge of the size of the target concept."
Let F and G be arbitrary rules based on in-dependent views.
We first derive an expression for the precision of F in terms of G. Note that the second line is derived from the first by rule independence.
P(FG) = P(F|GY )
P(GY ) +
P(F|GȲ )P(GȲ ) = P(F|Y )P(GY ) +
P(F|Ȳ )P(GȲ ) P(G|F) =
P(Y |F)P(G|Y ) + [1 − P(Y |F)]P(G|Ȳ ) P(Y |F) = PP((GG||FY ))−P−P((GG||ȲȲ ))
"To compute the expression on the righthand side of the last line, we require P(Y |G), P(Y ), P(G|F), and P(G)."
"The first value, the preci-sion of G, is assumed known."
"The second value, P (Y ), is also assumed known; it can at any rate be estimated from a small amount of labeled data."
"The last two values, P(G|F) and P(G), can be computed from unlabeled data."
"Thus, given the precision of an arbitrary rule G, we can compute the precision of any other-view rule F ."
Then we can compute the precision of rules based on the same view as G by using the precision of some other-view rule F.
Hence we can compute the precision of every rule given the precision of any one.
The empirical investigations described here and below use the data set[REF_CITE].
"The task is to classify names in text as person, location, or organization."
"There is an unlabeled training set containing 89,305 in-stances, and a labeled test set containing 289 persons, 186 locations, 402 organizations, and 123 “other”, for a total of 1,000 instances."
Instances are represented as lists of features.
"Intrinsic features are the words making up the name, and contextual features are features of the syntactic context in which the name oc-curs."
"For example, consider Bruce Kaplan, president of Metals Inc."
This text snippet con-tains two instances.
"The first has intrinsic fea-tures N:Bruce-Kaplan, C:Bruce, and C:Kaplan (“N” for the complete name, “C” for “con-tains”), and contextual feature M:president (“M” for “modified by”)."
"The second instance has intrinsic features N:Metals-Inc, C:Metals, C:Inc, and contextual feature X:president-of (“X” for “in the context of”)."
"Let us define Y (x) = + if x is a “location” instance, and Y (x) = − otherwise."
"We can es-timate P(Y ) from the test sample; it contains 186/1000 location instances, giving P(Y) = .186."
Let us treat each feature F as a rule predict-ing + when F is present and − otherwise.
The precision of F is P(Y |F).
The internal feature N:New-York has precision 1.
"This permits us to compute the precision of various contextual fea-tures, as shown in the “Co-training” column of Table 1."
We note that the numbers do not even look like probabilities.
"The cause is the failure of view independence to hold in the data, com-bined with the instability of the estimator. (The “Yarowsky” column uses a seed rule to estimate"
"P(Y |F), as is done in the Yarowsky algorithm, and the “Truth” column shows the true value of P (Y |F ).)"
"Nonetheless, the unreasonableness of view inde-pendence does not mean we must abandon the-orem 2."
"In this section, we introduce a weaker assumption, one that is satisfied by the data, and we show that theorem 2 holds under this weaker assumption."
"There are two ways in which the data can di-verge from conditional independence: the rules may either be positively or negatively corre-lated, given the class value."
"Figure 2a illus-trates positive correlation, and figure 2b illus-trates negative correlation."
"If the rules are negatively correlated, then their disagreement (shaded in figure 2) is larger than if they are conditionally independent, and the conclusion of theorem 2 is maintained a for-tiori."
"Unfortunately, in the data, they are posi-tively correlated, so the theorem does not apply."
Let us quantify the amount of deviation from conditional independence.
"We define the condi-tional dependence of F and G given Y = y to be d y = 1 X | Pr[G = v|Y = y, F = u]−Pr[G = v|Y = y]| 2 u,v If F and G are conditionally independent, then d y = 0."
"This permits us to state a weaker version of rule independence: Definition 4 Rules F and G satisfy weak rule dependence just in case, for y ∈ {+, −}: d y ≤ p 2 q 1 − p 1 2p 1 q 1 where p 1 = min u Pr[F = u|Y = y], p 2 = min u Pr[G = u|Y = y], and q 1 = 1 − p 1 ."
"By definition, p 1 and p 2 cannot exceed 0.5."
"If p 1 = 0.5, then weak rule dependence reduces to independence: if p 1 = 0.5 and weak rule depen-dence is satisfied, then d y must be 0, which is to say, F and G must be conditionally indepen-dent."
"However, as p 1 decreases, the permissible amount of conditional dependence increases."
"We can now state a generalized version of the-orem 2: Theorem 3 For all F ∈ H 1 , G ∈ H 2 that satisfy weak rule dependence and are nontrivial predictors in the sense that min u Pr[F = u] &gt; Pr[F =6 G], one of the following inequalities holds:"
Y ] ≤ Pr[F 6= G] Pr[F̄ =6 Y ] ≤ Pr[F =6 G]
Consider figure 3.
"This illustrates the most relevant case, in which F and G are positively correlated given Y . (Only the case Y = + is shown; the case Y = − is similar.)"
We assume that the minority values of F are error values; the other cases are handled as in the discussion of theorem 2.
Let u be the minority value of G when Y = +.
"In figure 3, a is the probability that G = u when F takes its minority value, and b is the proba-bility that G = u when F takes its majority value."
The value r = a − b is the difference.
Note that r = 0 if F and G are conditionally inde-pendent given Y = +.
"In fact, we can show that r is exactly our measure d y of conditional dependence: 2d y = |a − p 2 | + |b − p 2 | + |(1 − a) − (1 − p 2 )| +|(1 − b) − (1 − p 2 )| = |a − b| + |a − b| = 2r"
"Hence, in particular, we may write d y = a − b."
"Observe further that p 2 , the minority proba-bility of G when Y = +, is a weighted average of a and b, namely, p 2 = p 1 a+q 1 b. Combining this with the equation d y = a−b allows us to express a and b in terms of the remaining variables, to wit: a = p 2 + q 1 d y and b = p 2 − p 1 d y ."
"In order to prove theorem 3, we need to show that the area of disagreement (B ∪ C) upper bounds the area of the minority value of F (A∪ B)."
"This is true just in case C is larger than A, which is to say, if bq 1 ≥ ap 1 ."
Substituting our expressions for a and b into this inequality and solving for d y yields: d y ≤ p 2 q 1 − p 1 2p 1 q 1
"In short, disagreement upper bounds the mi-nority probability just in case weak rule depen-dence is satisfied, proving the theorem."
"Dasgupta, Littman, and McAllester suggest a possible algorithm at the end of their paper, but they give only the briefest suggestion, and do not implement or evaluate it."
"I give here an algorithm, the Greedy Agreement Algorithm, that constructs paired rules that agree on un-labeled data, and I examine its performance."
The algorithm is given in figure 4.
"It begins with two seed rules, one for each view."
"At each iteration, each possible extension to one of the rules is considered and scored."
"The best one is kept, and attention shifts to the other rule."
"A complex rule (or classifier) is a list of atomic rules H, each associating a single feature h with a label `."
"H(x) = ` if x has feature h, and H(x) = ⊥ otherwise."
A given atomic rule is permitted to appear multiple times in the list.
"Each atomic rule occurrence gets one vote, and the classifier’s prediction is the label that re-ceives the most votes."
"In case of a tie, there is no prediction."
"The cost of a classifier pair (F,G) is based on a more general version of theorem 2, that admits abstaining rules."
The following theorem is based[REF_CITE].
"Theorem 4 If view independence is satisfied, and if F and G are rules based on different views, then one of the following holds:"
"Pr[F 6= Y |F 6= ⊥] ≤ µ−δδ Pr[F̄ 6= Y |F̄ =6 ⊥] ≤ µ−δδ where δ = Pr[F 6= G|F,G 6= ⊥], and µ = min u Pr[F = u|F 6= ⊥]."
"In other words, for a given binary rule F , a pes-simistic estimate of the number of errors made by F is δ/(µ − δ) times the number of instances labeled by F, plus the number of instances left unlabeled by F. Finally, we note that the cost of F is sensitive to the choice of G, but the cost of F with respect to G is not necessarily the same as the cost of G with respect to F. To get an overall cost, we average the cost of F with respect to G and G with respect to F."
Figure 5 shows the performance of the greedy agreement algorithm after each iteration.
"Be-cause not all test instances are labeled (some are neither persons nor locations nor organiza-tions), and because classifiers do not label all instances, we show precision and recall rather than a single error rate."
The contour lines show levels of the F-measure (the harmonic mean of precision and recall).
"The algorithm is run to convergence, that is, until no atomic rule can be found that decreases cost."
It is interesting to note that there is no significant overtrain-ing with respect to F-measure.
"The final values are 89.2/80.4/84.5 recall/precision/F-measure, which compare favorably with the performance of the Yarowsky algorithm (83.3/84.6/84.0).[REF_CITE]add a special final round to boost recall, yielding 91.2/80.0/85.2 for the Yarowsky algorithm and 91.3/80.1/85.3 for their version of the original co-training algo-rithm."
All four algorithms essentially perform equally well; the advantage of the greedy agree-ment algorithm is that we have an explanation for why it performs well.
"For Yarowsky’s algorithm, a classifier again con-sists of a list of atomic rules."
The prediction of the classifier is the prediction of the first rule in the list that applies.
"The algorithm constructs a classifier iteratively, beginning with a seed rule."
"In the variant we consider here, one atomic rule is added at each iteration."
"An atomic rule F ` is chosen only if its precision, Pr[G ` = +|F ` = +] (as measured using the labels assigned by the current classifier G), exceeds a fixed threshold θ. 1"
Yarowsky does not give an explicit justifica-tion for the algorithm.
I show here that the algorithm can be justified on the basis of two independence assumptions.
"In what follows, F represents an atomic rule under consideration, and G represents the current classifier."
"Recall that Y ` is the set of instances whose true label is `, and G ` is the set of instances {x : G(x) = `}."
"We write G ∗ for the set of instances labeled by the current classifier, that is, {x : G(x) 6= ⊥}."
The first assumption is precision indepen-dence.
Definition 5 Candidate rule F ` and classifier G satisfy precision independence just in case
"P (Y ` |F ` , G ∗ ) ="
P (Y ` |F ` )
A bootstrapping problem instance satisfies pre-cision independence just in case all rules G and all atomic rules F ` that nontrivially overlap with G (both F ` ∩G ∗ and F ` −G ∗ are nonempty) sat-isfy precision independence.
"Precision independence is stated here so that it looks like a conditional independence assump-tion, to emphasize the similarity to the analysis of co-training."
"In fact, it is only “half” an in-dependence assumption—for precision indepen-dence, it is not necessary that"
"P(Y ` |F̄ ` ,G ∗ ) ="
P (Y ` |F̄ ` ).
The second assumption is that classifiers make balanced errors.
"P (Y ` , G ¯` |F ` ) ="
"P (Y `¯ , G ` |F ` )"
Let us first consider a concrete (but hypo-thetical) example.
"Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes."
Then the initial precision is 1 and recall is 0.[Footnote_1].
"1[REF_CITE], citing[REF_CITE], actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting."
"Suppose further that we add an atomic rule that correctly labels 19 new in-stances, and incorrectly labels one new instance."
The rule’s precision is 0.95.
The precision of the new classifier (the old classifier plus the new atomic rule) is 119/120 = 0.99.
Note that the new precision lies between the old precision and the precision of the rule.
"We will show that this is always the case, given precision independence and balanced errors."
"We need to consider several quantities: the precision of the current classifier, P(Y ` |G ` ); the precision of the rule under consideration, P(Y ` |F ` ); the precision of the rule on the cur-rent labeled set, P(Y ` |F ` G ∗ ); and the precision of the rule as measured using estimated labels, P (G ` |F ` G ∗ )."
"The assumption of balanced errors implies that measured precision equals true precision on labeled instances, as follows. (We assume here that all instances have true labels, hence that Ȳ ` = Y ¯` .)"
"This, combined with precision independence, implies that the precision of F ` as measured on the labeled set is equal to its true precision P (Y ` |F ` )."
Now consider the precision of the old and new classifiers at predicting `.
"Of the instances that the old classifier labels `, let A be the num-ber that are correctly labeled and B be the number that are incorrectly labeled."
Defining N t =
"A + B, the precision of the old classifier is Q t = A/N t ."
"Let ∆A be the number of new instances that the rule under consideration cor-rectly labels, and let ∆B be the number that it incorrectly labels."
"Defining n = ∆A + ∆B, the precision of the rule is q = ∆A/n."
"The precision of the new classifier is Q t+1 = (A + ∆A)/N t+1 , which can be written as:"
N t n Q t+1 = N t+1 Q t + N t+1 q
"That is, the precision of the new classifier is a weighted average of the precision of the old classifier and the precision of the new rule."
"An immediate consequence is that, if we only accept rules whose precision exceeds a given threshold θ, then the precision of the new classi-fier exceeds θ."
"Since measured precision equals true precision under our previous assumptions, it follows that the true precision of the final clas-sifier exceeds θ if the measured precision of ev-ery accepted rule exceeds θ."
"Moreover, observe that recall can be written as:"
A = N t Q N ` N ` t where N ` is the number of instances whose true label is `.
"If Q t &gt; θ, then recall is bounded below by N t θ/N ` , which grows as N t grows."
Hence we have proven the following theorem.
"Theorem 5 If the assumptions of precision in-dependence and balanced errors are satisfied, then the Yarowsky algorithm with threshold θ obtains a final classifier whose precision is at least θ."
"Moreover, recall is bounded below by N t θ/N ` , a quantity which increases at each round."
"Intuitively, the Yarowsky algorithm increases recall while holding precision above a threshold that represents the desired precision of the final classifier."
"The empirical behavior of the algo-rithm, as shown in figure 6, is in accordance with this analysis."
"We have seen, then, that the Yarowsky algo-rithm, like the co-training algorithm, can be jus-tified on the basis of an independence assump-tion, precision independence."
"It is important to note, however, that the Yarowsky algorithm is not a special case of co-training."
Precision in-dependence and view independence are distinct assumptions; neither implies the other. [Footnote_2]
"2 To see that view independence does not imply pre-cision indepence, consider an example in which G = Y always. This is compatible with rule independence, but it implies that P(Y |FG) = 1 and P(Y |FḠ) = 0, violat-ing precision independence."
"To sum up, we have refined previous work on the analysis of co-training, and given a new co-training algorithm that is theoretically justified and has good empirical performance."
"We have also given a theoretical analysis of the Yarowsky algorithm for the first time, and shown that it can be justified by an indepen-dence assumption that is quite distinct from the independence assumption that co-training is based on."
"We present an unsupervised approach to recognizing discourse relations of CON - TRAST , EXPLANATION - EVIDENCE , CON - DITION and ELABORATION that hold be-tween arbitrary spans of texts."
"We show that discourse relation classifiers trained on examples that are automatically ex-tracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not ex-plicitly marked by cue phrases."
"In the field of discourse research, it is now widely agreed that sentences/clauses are usually not un-derstood in isolation, but in relation to other sen-tences/clauses."
"Given the high level of interest in explaining the nature of these relations and in pro-viding definitions for them[REF_CITE], it is surprising that there are no ro-bust programs capable of identifying discourse rela-tions that hold between arbitrary spans of text."
"Con-sider, for example, the sentence/clause pairs below. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most of its profits from the sale of expensive, high-technology systems like laser-designated missiles, air-craft electronic warfare systems, tactical ra-dios, anti-radiation bombs and battlefield mo-bility systems."
"In these examples, the discourse markers But and because help us figure out that a CONTRAST re-lation holds between the text spans in (1) and an EXPLANATION - EVIDENCE relation holds between the spans in (2)."
"Unfortunately, cue phrases do not signal all relations in a text."
"In the corpus of Rhetori-cal Structure trees[URL_CITE]marcu/discourse/) built[REF_CITE], for example, we have observed that only 61 of 238 CONTRAST relations and 79 out of 307 EXPLANATION - EVIDENCE rela-tions that hold between two adjacent clauses were marked by a cue phrase."
So what shall we do when no discourse markers are used?
"If we had access to ro-bust semantic interpreters, we could, for example, infer from sentence 1.a that “can-not buy arms legally(libya)”, infer from sen-tence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in (1)."
"Unfortunately, the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences."
"The discourse relation definitions proposed by others[REF_CITE]are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well."
"In spite of the difficulty of determining the dis-course relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications."
"First, a discourse relation recognizer would enable the development of im-proved discourse parsers and, consequently, of high performance single document summarizers[REF_CITE]."
"In multidocument summarizati[REF_CITE], it would enable the development of summa-rization programs capable of identifying contradic-tory statements both within and across documents and of producing summaries that reflect not only the similarities between various documents, but also their differences."
"In question-answering, it would enable the development of systems capable of an-swering sophisticated, non-factoid queries, such as “what were the causes of X?” or “what contradicts Y?”, which are beyond the state of the art of current systems[REF_CITE]."
"In this paper, we describe experiments aimed at building robust discourse-relation classification sys-tems."
"To build such systems, we train a family of Naive Bayes classifiers on a large set of examples that are generated automatically from two corpora: a corpus of 41,147,805 English sentences that have no annotations, and BLIPP, a corpus of 1,796,386 automatically parsed English sentences[REF_CITE], which is available from the Linguistic Data Consortium[URL_CITE]"
We study empir-ically the adequacy of various features for the task of discourse relation classification and we show that some discourse relations can be correctly recognized with accuracies as high as 93%.
"In order to build a discourse relation classifier, one first needs to decide what relation definitions one is going to use."
"In Section 1, we simply relied on the reader’s intuition when we claimed that a CON -"
TRAST relation holds between the sentences in (1).
"In reality though, associating a discourse relation with a text span pair is a choice that is clearly in-fluenced by the theoretical framework one is willing to adopt."
"If we adopt, for example, Knott and Sanders’s (1998) account, we would say that the relation between sentences 1.a and 1.b is ADDITIVE , because no causal connection exists between the two sentences, PRAGMATIC , because the relation pertains to illocutionary force and not to the propositional content of the sentences, and NEGATIVE , because the relation involves a CONTRAST between the two sentences."
"In the same framework, the relation between clauses 2.a and 2.b will be labeled as CAUSAL - SEMANTIC - POSITIVE - NONBASIC ."
"In Lascarides and Asher’s theory (1993), we would label the relation between 2.a and 2.b as EXPLANATION because the event in 2.b explains why the event in 2.a happened (perhaps by CAUSING it)."
"In Hobbs’s theory (1990), we would also label the relation between 2.a and 2.b as EXPLANATION because the event asserted by 2.b CAUSED or could CAUSE the event asserted in 2.a."
"And[REF_CITE], we would label sentence pairs 1.a, 1.b as CONTRAST because the situations presented in them are the same in many respects (the purchase of arms), because the situations are different in some respects (Libya cannot buy arms legally while Rwanda can), and because these situations are compared with respect to these differences."
"By a similar line of reasoning, we would label the relation between 2.a and 2.b as EVIDENCE ."
The discussion above illustrates two points.
"First, it is clear that although current discourse theories are built on fundamentally different principles, they all share some common intuitions."
"Sure, some theo-ries talk about “negative polarity” while others about “contrast”."
"Some theories refer to “causes”, some to “potential causes”, and some to “explanations”."
"But ultimately, all these theories acknowledge that there are such things as CONTRAST , CAUSE , and EXPLA - NATION relations."
"Second, given the complexity of the definitions these theories propose, it is clear why it is difficult to build programs that recognize such relations in unrestricted texts."
"Current NLP tech-niques do not enable us to reliably infer from sen- tence 1.a that “cannot buy arms legally(libya)” and do not give us access to general purpose knowledge bases that assert that “similar(libya,rwanda)”."
The approach we advocate in this paper is in some respects less ambitious than current approaches to discourse relations because it relies upon a much smaller set of relations than those used[REF_CITE]or[REF_CITE].
"In our work, we decide to focus only on four types of relations, which we call: CONTRAST , CAUSE - EXPLANATION - EVIDENCE ( CEV ), CONDITION , and ELABORA - TION . (We define these relations in Section 2.2.)"
"In other respects though, our approach is more ambi-tious because it focuses on the problem of recog-nizing such discourse relations in unrestricted texts."
"In other words, given as input sentence pairs such as those shown in (1)–(2), we develop techniques and programs that label the relations that hold be-tween these sentence pairs as CONTRAST , CAUSE - EXPLANATION - EVIDENCE , CONDITION , ELABO - RATION or NONE - OF - THE - ABOVE , even when the discourse relations are not explicitly signalled by discourse markers."
The discourse relations we focus on are defined at a much coarser level of granularity than in most discourse theories.
"For example, we con-sider that a CONTRAST relation holds between two text spans if one of the following relations holds: CONTRAST , ANTITHESIS , CONCESSION , or OTH - ERWISE , as defined[REF_CITE], CONTRAST or VIOLATED EXPECTATION , as defined[REF_CITE], or any of the relations character-ized by this regular expression of cognitive prim-itives, as defined[REF_CITE]: ( CAUSAL ADDITIVE ) – ( SEMANTIC PRAGMATIC ) – NEGATIVE ."
"In other words, in our approach, we do not distinguish between contrasts of semantic and pragmatic nature, contrasts specific to violated ex-pectations, etc."
Table 1 shows the definitions of the relations we considered.
The advantage of operating with coarsely defined discourse relations is that it enables us to automat-ically construct relatively low-noise datasets that can be used for learning.
"For example, by extract-ing sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sen- tence pair shown in (1), we can automatically col-lect many examples of CONTRAST relations."
"And by extracting sentences that contain the keyword “be-cause”, we can automatically collect many examples of CAUSE - EXPLANATION - EVIDENCE relations."
"As previous research in linguistics[REF_CITE]and computational linguis-tics[REF_CITE]show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE - EXPLANATION ."
So we can expect the ex-amples we extract to be noisy.
"However, empiri-cal work[REF_CITE]and[REF_CITE]suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations. (In the RST corpus built[REF_CITE], 89 out of the 106 occurrences of “but” that occur at the begin-ning of a sentence signal a CONTRAST relation that holds between the sentence that contains the word “but” and the sentence that precedes it.)"
Our hope is that simple extraction methods are sufficient for collecting low-noise training corpora.
"In order to collect training cases, we mined in an unsupervised manner two corpora."
"The first corpus, which we call Raw, is a corpus of 1 billion words of unannotated English (41,147,805 sentences) that we created by catenating various corpora made avail-able over the years by the Linguistic Data Consor-tium."
"The second, called BLIPP, is a corpus of only 1,796,386 sentences that were parsed automatically[REF_CITE]."
We extracted from both cor-pora all adjacent sentence pairs that contained the cue phrase “But” at the beginning of the second sen-tence and we automatically labeled the relation be-tween the two sentence pairs as CONTRAST .
"We also extracted all the sentences that contained the word “but” in the middle of a sentence; we split each ex-tracted sentence into two spans, one containing the words from the beginning of the sentence to the oc-currence of the keyword “but” and one containing the words from the occurrence of “but” to the end of the sentence; and we labeled the relation between the two resulting text spans as CONTRAST as well."
"Table 2 lists some of the cue phrases we used in order to extract CONTRAST , CAUSE - EXPLANATION - EVIDENCE , ELABORATION , and"
CONDITION relations and the number of examples extracted from the Raw corpus for each type of dis-course relation.
"In the patterns in Table 2, the sym-bols BOS and EOS denote BeginningOfSentence and EndOfSentence boundaries, the “ ” stand for occurrences of any words and punctuation marks, the square brackets stand for text span boundaries, and the other words and punctuation marks stand for the cue phrases that we used in order to extract dis-course relation examples."
"For example, the pattern [BOS Although ,] [ EOS] is used in order to extract examples of CONTRAST relations that hold between a span of text delimited to the left by the cue phrase “Although” occurring in the beginning of a sentence and to the right by the first occurrence of a comma, and a span of text that contains the rest of the sentence to which “Although” belongs."
"We also extracted automatically 1,000,000 exam-ples of what we hypothesize to be non-relations, by randomly selecting non-adjacent sentence pairs that are at least 3 sentences apart in a given text."
We label such examples NO - RELATION - SAME - TEXT .
"And we extracted automatically 1,000,000 examples of what we hypothesize to be cross-document non-relations, by randomly selecting two sentences from distinct documents."
"As in the case of CONTRAST and CONDITION , the NO - RELATION examples are also noisy because long distance relations are com-mon in well-written texts."
"We hypothesize that we can determine that a CON - TRAST relation holds between the sentences in (3) even if we cannot semantically interpret the two sen-tences, simply because our background knowledge tells us that good and fails are good indicators of contrastive statements."
John is good in math and sciences. (3) Paul fails almost every class he takes.
"Similarly, we hypothesize that we can determine that a CONTRAST relation holds between the sentences in (1), because our background knowledge tells us that embargo and legally are likely to occur in con-texts of opposite polarity."
"In general, we hypothe-size that lexical item pairs can provide clues about the discourse relations that hold between the text spans in which the lexical items occur."
"To test this hypothesis, we need to solve two problems."
"First, we need a means to acquire vast amounts of background knowledge from which we can derive, for example, that the word pairs good – fails and embargo – legally are good indicators of CONTRAST relations."
The extraction patterns de-scribed in Table 2 enable us to solve this problem. [Footnote_1]
"1 Note that relying on the list of antonyms provided by Word-net[REF_CITE]is not enough because the semantic rela-tions in Wordnet are not defined across word class boundaries. For example, Wordnet does not list the “antonymy”-like relation between embargo and legally."
"Second, given vast amounts of training material, we need a means to learn which pairs of lexical items are likely to co-occur in conjunction with each dis-course relation and a means to apply the learned pa-rameters to any pair of text spans in order to deter-mine the discourse relation that holds between them."
We solve the second problem in a Bayesian proba-bilistic framework.
"We assume that a discourse relation that holds between two text spans,  , is determined by the word pairs in the cartesian product defined over the words in the two text spans !&quot;#%$&amp; ."
"In general, a word pair    ($ can “signal” any relation ."
"We determine the most likely discourse relation that holds between two text spans  and ) by taking the maximum over *+ ,+-.0* %/ 6# , which according to Bayes rule, amounts to taking the maximum over 0* 60-, .*+/ +:=; 6&lt; , &gt;)? @&amp;A(;=&lt; , ? ."
"If we assume that the word pairs in the cartesian prod-uct are independent, &gt;)?  is equivalent to #F GIH9J=K    ."
"The values .  are computed using maximum likelihood estimators, which are smoothed using the Laplace method[REF_CITE]."
"For each discourse relation pair 6Y? , we train a word-pair-based classifier using the automatically derived training examples in the Raw corpus, from which we first removed the cue-phrases used for ex-tracting the examples."
"This ensures that our classi- fiers do not learn, for example, that the word pair if – then is a good indicator of a CONDITION re-lation, which would simply amount to learning to distinguish between the extraction patterns used to construct the corpus."
"We test each classifier on a test corpus of 5000 examples labeled with Y and 5000 examples labeled with  , which ensures that the baseline is the same for all combinations Y and  , namely 50%."
Table 3 shows the performance of all discourse relation classifiers.
"As one can see, each classifier outperforms the 50% baseline, with some classifiers being as accurate as that that distinguishes between CAUSE - EXPLANATION - EVIDENCE and ELABORA - TION relations, which has an accuracy of 93%."
We have also built a six-way classifier to distinguish be-tween all six relation types.
"This classifier has a performance of 49.7%, with a baseline of 16.67%, which is achieved by labeling all relations as CON - TRASTS ."
"We also examined the learning curves of various classifiers and noticed that, for some of them, the ad-dition of training examples does not appear to have a significant impact on their performance."
"For exam-ple, the classifier that distinguishes between CON - TRAST and CAUSE - EXPLANATION - EVIDENCE rela-tions has an accuracy of 87.1% when trained on 2,000,000 examples and an accuracy of 87.3% when trained on 4,771,534 examples."
We hypothesized that the flattening of the learning curve is explained by the noise in our training data and the vast amount of word pairs that are not likely to be good predictors of discourse relations.
"To test this hypothesis, we decided to carry out a second experiment that used as predictors only a subset of the word pairs in the cartesian product defined over the words in two given text spans."
"To achieve this, we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus."
"As expected, the BLIPP corpus yielded much fewer learning cases: 185,846 CON - TRAST ; 44,776 CAUSE - EXPLANATION - EVIDENCE ; 55,699 CONDITION ; and 33,369 ELABORA - TION relations."
"To these examples, we added 58,000 NO - RELATION - SAME -[REF_CITE]000 NO - RELATION - DIFFERENT - TEXTS relations."
To each text span in the BLIPP corpus corre-sponds a parse tree[REF_CITE].
"We wrote a simple program that extracted the nouns, verbs, and cue phrases in each sentence/clause."
We call these the most representative words of a sen-tence/discourse unit.
"For example, the most repre-sentative words of the sentence in example (4), are those shown in italics."
"Italy’s unadjusted industrial production fell in Jan- (4) uary 3.4% from a year earlier but rose 0.4% from December, the government said"
We repeated the experiment we carried out in con-junction with the Raw corpus on the data derived from the BLIPP corpus as well.
Table 4 summarizes the results.
"Overall, the performance of the systems trained on the most representative word pairs in the BLIPP corpus is clearly lower than the performance of the systems trained on all the word pairs in the Raw corpus."
"But a direct comparison between two clas-sifiers trained on different corpora is not fair be-cause with just 100,000 examples per relation, the systems trained on the Raw corpus are much worse than those trained on the BLIPP data."
"The learning curves in Figure 1 are illuminating as they show that if one uses as features only the most representative word pairs, one needs only about 100,000 training examples to achieve the same level of performance one achieves using 1,000,000 training examples and features defined over all word pairs."
"Also, since the learning curve for the BLIPP corpus is steeper than the learning curve for the Raw corpus, this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs (unanno-tated data)."
The results in Section 3 indicate clearly that massive amounts of automatically generated data can be used to distinguish between discourse relations defined as discussed in Section 2.2.
What the experiments in Section 3 do not show is whether the classifiers built in this manner can be of any use in conjunction with some established discourse theory.
"To test this, we used the corpus of discourse trees built in the style of RST[REF_CITE]."
"We automati-cally extracted from this manually annotated corpus all CONTRAST , CAUSE - EXPLANATION - EVIDENCE , CONDITION and ELABORATION relations that hold between two adjacent elementary discourse units."
"Since RST[REF_CITE]employs a finer grained taxonomy of relations than we used, we applied the definitions shown in Table 1."
"That is, we considered that a CONTRAST relation held be-tween two text spans if a human annotator labeled the relation between those spans as ANTITHESIS , CONCESSION , OTHERWISE or CONTRAST ."
"We re-trained then all classifiers on the Raw corpus, but this time without removing from the corpus the cue phrases that were used to generate the training ex-amples."
"We did this because when trying to deter-mine whether a CONTRAST relation holds between two spans of texts separated by the cue phrase “but”, for example, we want to take advantage of the cue phrase occurrence as well."
We employed our clas-sifiers on the manually labeled examples extracted from Carlson et al.’s corpus (2001).
Table 5 displays the performance of our two way classifiers for rela-tions defined over elementary discourse units.
"The table displays in the second row, for each discourse relation, the number of examples extracted from the RST corpus."
"For each binary classifier, the table lists in bold the accuracy of our classifier and in non-bold font the majority baseline associated with it."
The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations.
"For example, the results show that the classifiers can be used to distinguish between CONTRAST and CAUSE - EXPLANATION - EVIDENCE relations, as defined in RST, but not so well between ELABORATION and any other relation."
"This result is consistent with the discourse model proposed[REF_CITE], who suggest that ELABORATION relations are too ill-defined to be part of any dis-course theory."
The analysis above is informative only from a machine learning perspective.
"From a linguistic perspective though, this analysis is not very use-ful."
"If no cue phrases are used to signal the re-lation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most fre-quently used relations[REF_CITE]."
"Fortu-nately, with the classifiers described here, one can label some of the unmarked discourse relations cor-rectly."
"For example, the RST-annotated corpus[REF_CITE]contains 238 CONTRAST rela-tions that hold between two adjacent elementary dis-course units."
"Of these, only 61 are marked by a cue phrase, which means that a program trained only on Carlson et al.’s corpus could identify at most 61/238 of the CONTRAST relations correctly."
"Be-cause Carlson et al.’s corpus is small, all unmarked relations will be likely labeled as ELABORATION s. However, when we run our CONTRAST vs. ELAB - ORATION classifier on these examples, we can la-bel correctly 60 of the 61 cue-phrase marked re-lations and, in addition, we can also label 123 of the 177 relations that are not marked explicitly with cue phrases."
This means that our classifier con-tributes to an increase in accuracy from 7[ \ ]E@^ @_  @[?b to EcdA[ (\4^@_?]E@^ _@`Vafe@@e b !!!
"Similarly, out ^ of the 307 CAUSE - EXPLANATION - EVIDENCE rela-tions that hold between two discourse units in Carl-son et al.’s corpus, only 79 are explicitly marked."
"A program trained only on Carlson et al.’s cor-pus, would, therefore, identify at most 79 of the 307 relations correctly."
"When we run our CAUSE - EXPLANATION - EVIDENCE vs. ELABORATION clas-sifier on these examples, we labeled correctly 73 of the 79 cue-phrase-marked relations and 102 of the 228 unmarked relations."
This corresponds to an increase in accuracy from eEg?]EEc_ +eha @^ [?b to &gt;eE_UAV\ c??^ E] _Ec+eiakj?@e b .
"In a seminal paper,[REF_CITE]have recently shown that massive amounts of data can be used to significantly increase the performance of confusion set disambiguators."
"In our paper, we show that massive amounts of data can have a ma-jor impact on discourse processing research as well."
Our experiments show that discourse relation clas-sifiers that use very simple features achieve unex-pectedly high levels of performance when trained on extremely large data sets.
Developing lower-noise methods for automatically collecting training data and discovering features of higher predictive power for discourse relation classification than the features presented in this paper appear to be research avenues that are worthwhile to pursue.
"Over the last thirty years, the nature, number, and taxonomy of discourse relations have been among the most controversial issues in text/discourse lin-guistics."
This paper does not settle the controversy.
"Rather, it raises some new, interesting questions be-cause the lexical patterns learned by our algorithms can be interpreted as empirical proof of existence for discourse relations."
"If text production was not governed by any rules above the sentence level, we should have not been able to improve on any of the baselines in our experiments."
Our results sug-gest that it may be possible to develop fully auto-matic techniques for defining empirically justified discourse relations.
This work was supported by the National Science Foundation under grant num-ber[REF_CITE]and by the Advanced Research and Development Activity (ARDA)’s Advanced Ques-tion Answering for Intelligence (AQUAINT) Pro-gram under contract number[REF_CITE]-02-C-0007.
"Mobile interfaces need to allow the user and system to adapt their choice of com-munication modes according to user pref-erences, the task at hand, and the physi-cal and social environment."
"We describe a multimodal application architecture which combines finite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output."
Our testbed appli-cation MATCH (Multimodal Access To City Help) provides a mobile multimodal speech-pen interface to restaurant and sub-way information for New York City.
"In urban environments tourists and residents alike need access to a complex and constantly changing body of information regarding restaurants, theatre schedules, transportation topology and timetables."
"This information is most valuable if it can be de-livered effectively while mobile, since places close and plans change."
"Mobile information access devices (PDAs, tablet PCs, next-generation phones) offer limited screen real estate and no keyboard or mouse, making complex graphical interfaces cumbersome."
Multimodal interfaces can address this problem by enabling speech and pen input and output combining speech and graphics (See[REF_CITE]for a detailed overview of previous work on multimodal input and output).
"Since mobile devices are used in different physical and social environments, for different tasks, by different users, they need to be both flexible in in-put and adaptive in output."
"Users need to be able to provide input in whichever mode or combination of modes is most appropriate, and system output should be dynamically tailored so that it is maximally effec-tive given the situation and the user’s preferences."
"We present our testbed multimodal application MATCH (Multimodal Access To City Help) and the general purpose multimodal architecture underlying it, that: is designed for highly mobile applications; enables flexible multimodal input; and provides flex-ible user-tailored multimodal output."
Highly mobile MATCH is a working city guide and navigation system that currently enables mobile users to access restaurant and subway information for New York City (NYC).
"MATCH runs standalone on a Fujitsu pen computer (Figure 1), and can also run in client-server mode across a wireless network."
Flexible multimodal input Users interact with a graphical interface displaying restaurant listings and a dynamic map showing locations and street infor-mation.
"They are free to provide input using speech, by drawing on the display with a stylus, or by us-ing synchronous multimodal combinations of the two modes."
"For example, a user might ask to see cheap"
"Italian restaurants in Chelsea by saying show cheap italian restaurants in chelsea, by circling an area on the map and saying show cheap italian restaurants in this neighborhood; or, in a noisy or public envi-ronment, by circling an area and writing cheap and italian (Figure 2)."
The system will then zoom to the appropriate map location and show the locations of restaurants on the map.
"Users can ask for information about restaurants, such as phone numbers, addresses, and reviews."
"For example, a user might circle three restaurants as in Figure 3 and say phone numbers for these three restaurants (or write phone)."
Users can also manipulate the map interface directly.
"For exam-ple, a user might say show upper west side or circle an area and write zoom."
"Flexible multimodal output MATCH provides flexible, synchronized multimodal generation and can take initiative to engage in information-seeking subdialogues."
"If a user circles the three restaurants in Figure 3 and writes phone, the system responds with a graphical callout on the display, synchronized with a text-to-speech (TTS) prompt of the phone number, for each restaurant in turn (Figure 4)."
The system also provides subway directions.
"If the user says How do I get to this place? and circles one of the restaurants displayed on the map, the system will ask Where do you want to go from?"
"The user can then respond with speech (e.g., 25th Street and 3rd Avenue), with pen by writing (e.g., 25th St &amp; 3rd Ave), or multimodally ( e.g, from here with a circle gesture indicating location)."
The system then calcu-lates the optimal subway route and dynamically gen-erates a multimodal presentation of instructions.
"It starts by zooming in on the first station and then grad-ually zooms out, graphically presenting each stage of the route along with a series of synchronized TTS prompts."
Figure 5 shows the final display of a sub-way route heading downtown on the 6 train and trans-ferring to the L train Brooklyn bound.
"User-tailored generation MATCH can also pro-vide a user-tailored summary, comparison, or rec-ommendation for an arbitrary set of restaurants, us-ing a quantitative model of user preferences[REF_CITE]."
"The system will only discuss restau-rants that rank highly according to the user’s dining preferences, and will only describe attributes of those restaurants the user considers important."
"This per-mits concise, targeted system responses."
"For exam-ple, the user could say compare these restaurants and circle a large set of restaurants (Figure 6)."
"If the user considers inexpensiveness and food quality to be the most important attributes of a restaurant, the system response might be:"
The multimodal architecture supporting MATCH consists of a series of agents which communicate through a facilitator MCUBE (Figure 7).
MCUBE is a Java-based facilitator which enables agents to pass messages either to single agents or groups of agents.
"It serves a similar function to sys-tems such as OAA[REF_CITE], the use of KQML for messaging[REF_CITE], and the Communicator hub[REF_CITE]."
Agents may reside either on the client device or elsewhere on the network and can be implemented in multiple differ-ent languages.
"MCUBE messages are encoded in XML, providing a general mechanism for message parsing and facilitating logging."
"Multimodal User Interface Users interact with the system through the Multimodal UI, which is browser-based and runs in Internet Explorer."
"This greatly facilitates rapid prototyping, authoring, and reuse of the system for different applications since anything that can appear on a webpage (dynamic HTML, ActiveX controls, etc.) can be used in the visual component of a multimodal user inter-face."
A TCP/IP control enables communication with MCUBE.
"MATCH uses a control that provides a dynamic pan-able, zoomable map display."
The control has ink handling capability.
This enables both pen-based in-teraction (on the map) and normal GUI interaction (on the rest of the page) without requiring the user to overtly switch ‘modes’.
"When the user draws on the map their ink is captured and any objects potentially selected, such as currently displayed restaurants, are identified."
The electronic ink is broken into a lat-tice of strokes and sent to the gesture recognition and handwriting recognition components which en-rich this stroke lattice with possible classifications of strokes and stroke combinations.
The UI then trans-lates this stroke lattice into an ink meaning lattice representing all of the possible interpretations of the user’s ink and sends it to MMFST.
In order to provide spoken input the user must tap a click-to-speak button on the Multimodal UI.
"We found that in an application such as MATCH which provides extensive unimodal pen-based interaction, it is preferable to use click-to-speak rather than pen-to-speak or open-mike."
"With pen-to-speak, spurious speech results received in noisy environments can disrupt unimodal pen commands."
The Multimodal UI also provides graphical output capabilities and performs synchronization of multi-modal output.
"For example, it synchronizes the dis-play actions and TTS prompts in the answer to the route query mentioned in Section 1."
Speech Recognition MATCH uses AT&amp;T’s Wat-son speech recognition engine.
A speech manager running on the device gathers audio and communi-cates with a recognition server running either on the device or on the network.
The recognition server pro-vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture and handwriting recognition agents provide possible classifications of electronic ink for the UI.
Recogni-tions are performed both on individual strokes and combinations of strokes in the input ink lattice.
"The handwriting recognizer supports a vocabulary of 285 words, including attributes of restaurants (e.g. ‘chi-nese’,‘cheap’) and zones and points of interest (e.g. ‘soho’,‘empire’,‘state’,‘building’)."
"The gesture rec-ognizer recognizes a set of 10 basic gestures, includ-ing lines, arrows, areas, points, and question marks."
It uses a variant of Rubine’s classic template-based gesture recognition algorithm[REF_CITE]trained on a corpus of sample gestures.
In addition to classi- fying gestures the gesture recognition agent also ex-tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-written words provide a rich visual vocabulary for multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-tice as symbol complexes of the following form: G FORM MEANING (NUMBER TYPE) SEM.
"FORM indicates the physical form of the gesture and has val-ues such as area, point, line, arrow."
MEANING indi-cates the meaning of that form; for example an area can be either a loc(ation) or a sel(ection).
"NUMBER and TYPE indicate the number of entities in a selec-tion (1,2,3, many) and their type (rest(aurant), the-atre)."
"SEM is a place holder for the specific content of the gesture, such as the points that make up an area or the identifiers of objects in a selection."
When multiple selection gestures are present an aggregation technique[REF_CITE]is employed to overcome the problems with deictic plurals and numerals described[REF_CITE].
Aggregation augments the ink meaning lattice with aggregate gestures that result from com-bining adjacent selection gestures.
"This allows a de-ictic expression like these three restaurants to com-bine with two area gestures, one which selects one restaurant and the other two, as long as their sum is three."
"For example, if the user makes two area ges-tures, one around a single restaurant and the other around two restaurants (Figure 3), the resulting ink meaning lattice will be as in Figure 8."
"The first ges-ture (node numbers 0-7) is either a reference to a location (loc.) (0-3,7) or a reference to a restaurant (sel.) (0-2,4-7)."
"The second (nodes 7-13,16) is either a reference to a location (7-10,16) or to a set of two restaurants (7-9,11-13,16)."
"The aggregation process applies to the two adjacent selections and adds a se-lection of three restaurants (0-2,4,14-16)."
"If the user says show chinese restaurants in this neighborhood and this neighborhood, the path containing the two locations (0-3,7-10,16) will be taken when this lat-tice is combined with speech in MMFST."
"If the user says tell me about this place and these places, then the path with the adjacent selections is taken (0-2,4- 9,11-13,16)."
"If the speech is tell me about these or phone numbers for these three restaurants then the aggregate path (0-2,4,14-16) will be chosen."
Multimodal Integrator (MMFST) MMFST re-ceives the speech lattice (from the Speech Manager) and the ink meaning lattice (from the UI) and builds a multimodal meaning lattice which captures the po-tential joint interpretations of the speech and ink in- puts.
MMFST is able to provide rapid response times by making unimodal timeouts conditional on activity in the other input mode.
"MMFST is notified when the user has hit the click-to-speak button, when a speech result arrives, and whether or not the user is inking on the display."
"When a speech lattice arrives, if inking is in progress MMFST waits for the ink meaning lat-tice, otherwise it applies a short timeout (1 sec.) and treats the speech as unimodal."
"When an ink meaning lattice arrives, if the user has tapped click-to-speak MMFST waits for the speech lattice to arrive, other-wise it applies a short timeout (1 sec.) and treats the ink as unimodal."
MMFST uses the finite-state approach to multi-modal integration and understanding proposed[REF_CITE].
"Possibilities for multimodal integration and understanding are cap-tured in a three tape device in which the first tape represents the speech stream (words), the second the ink stream (gesture symbols) and the third their com-bined meaning (meaning symbols)."
"In essence, this device takes the speech and ink meaning lattices as inputs, consumes them using the first two tapes, and writes out a multimodal meaning lattice using the third tape."
The three tape finite-state device is sim-ulated using two transducers: G:W which is used to align speech and ink and G W:M which takes a com-posite alphabet of speech and gesture symbols as in-put and outputs meaning.
The ink meaning lattice G and speech lattice W are composed with G:W and the result is factored into an FSA G W which is com-posed with G W:M to derive the meaning lattice M.
"In order to capture multimodal integration using finite-state methods, it is necessary to abstract over specific aspects of gestural content[REF_CITE]."
"For example, all possible se-quences of coordinates that could occur in an area gesture cannot be encoded in the finite-state device."
"We employ the approach proposed[REF_CITE]in which the ink meaning lattice is converted to a transducer I:G, where G are gesture symbols (including SEM) and I contains both gesture symbols and the specific contents."
"I and G differ only in cases where the gesture symbol on G is SEM, in which case the corresponding I symbol is the specific interpretation."
After multimodal integration a pro-jection G:M is taken from the result G W:M machine and composed with the original I:G in order to rein-corporate the specific contents that were left out of the finite-state process (I:G o G:M = I:M).
"The multimodal finite-state transducers used at runtime are compiled from a declarative multimodal context-free grammar which captures the structure and interpretation of multimodal and unimodal com-mands, approximated where necessary using stan-dard approximation techniques[REF_CITE]."
"This grammar captures not just multimodal integra-tion patterns but also the parsing of speech and ges-ture, and the assignment of meaning."
In Figure 9 we present a small simplified fragment capable of han-dling MATCH commands such as phone numbers for these three restaurants.
"A multimodal CFG differs from a normal CFG in that the terminals are triples: W:G:M, where W is the speech stream (words), G the ink stream (gesture symbols) and M the meaning stream (meaning symbols)."
An XML representation for meaning is used to facilate parsing and logging by other system components.
The meaning tape sym-bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is empty in a given terminal.
When the user says phone numbers for these three restaurants and circles two groups of restau-rants (Figure 3).
The gesture lattice (Figure 8) is turned into a transducer I:G with the same sym-bol on each side except for the SEM arcs which are split.
"For example, path 15-16 SEM([id1,id2,id3]) becomes [id1,id2,id3]:SEM."
After G and the speech W are integrated using G:W and G W:M. The G path in the result is used to re-establish the connection between SEM symbols and their specific contents in I:G (I:G o G:M = I:M).
"The meaning read off I:M is &lt; cmd &gt; &lt; phone &gt; &lt; restaurant &gt; [id1,id2,id3] &lt; /restaurant &gt; &lt; /phone &gt; &lt; /cmd &gt; ."
This is passed to the multimodal dialog manager (MDM) and from there to the Multimodal UI resulting in a display like Figure 4 with coordinated TTS output.
"Since the speech input is a lattice and there is also potential for ambiguity in the multimodal grammar, the output from MMFST to MDM is an N-best list of potential multimodal interpretations."
Multimodal Dialog Manager (MDM)
The MDM is based on previous work on speech-act based mod-els of dialog[REF_CITE].
It uses a Java-based toolkit for writing dialog managers that is similar in philosophy to TrindiKit[REF_CITE].
It includes several rule-based processes that operate on a shared state.
"The state includes system and user intentions and beliefs, a di-alog history and focus space, and information about the speaker, the domain and the available modalities."
"The processes include interpretation, update, selec-tion and generation processes."
The interpretation process takes as input an N-best list of possible multimodal interpretations for a user input from MMFST.
"It rescores them according to a set of rules that encode the most likely next speech act given the current dialogue context, and picks the most likely interpretation from the result."
The update process updates the dialogue context according to the system’s interpretation of user input.
"It augments the dialogue history, focus space, models of user and sys-tem beliefs, and model of user intentions."
It also al-ters the list of current modalities to reflect those most recently used by the user.
The selection process determines the system’s next move(s).
"In the case of a command, request or ques-tion, it first checks that the input is fully specified (using the domain ontology, which contains informa-tion about required and optional roles for different types of actions); if it is not, then the system’s next move is to take the initiative and start an information-gathering subdialogue."
"If the input is fully specified, the system’s next move is to perform the command or answer the question; to do this, MDM communicates with the UI."
"Since MDM is aware of the current set of preferred modalities, it can provide feedback and responses tailored to the user’s modality preferences."
The generation process performs template-based generation for simple responses and updates the sys-tem’s model of the user’s intentions after generation.
"The text planner is used for more complex genera- tion, such as the generation of comparisons."
"In the route query example in Section 1, MDM first receives a route query in which only the destination is specified How do I get to this place?"
In the se-lection phase it consults the domain model and de-termines that a source is also required for a route.
It adds a request to query the user for the source to the system’s next moves.
This move is selected and the generation process selects a prompt and sends it to the TTS component.
The system asks Where do you want to go from?
If the user says or writes 25th Street and 3rd Avenue then MMFST will assign this input two possible interpretations.
Either this is a re-quest to zoom the display to the specified location or it is an assertion of a location.
"Since the MDM dia-logue state indicates that it is waiting for an answer of the type location, MDM reranks the assertion as the most likely interpretation."
A generalized overlay process[REF_CITE]is used to take the content of the assertion (a location) and add it into the partial route request.
The result is deter-mined to be complete.
The UI resolves the location to map coordinates and passes on a route request to the SUBWAY component.
We found this traditional speech-act based dia-logue manager worked well for our multimodal inter-face.
"Critical in this was our use of a common seman-tic representation across spoken, gestured, and multi-modal commands."
"The majority of the dialogue rules operate in a mode-independent fashion, giving users flexibility in the mode they choose to advance the di-alogue."
"On the other hand, mode sensitivity is also important since user modality choice can be used to determine system mode choice for confirmation and other responses."
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database of the NYC subway system.
"When it receives a route request with the desired source and destination points from the Multimodal UI, it explores the search space of possible routes to identify the optimal one, using a cost function based on the number of transfers, over-all number of stops, and the walking distance from the station at each end."
It builds a list of actions re-quired to reach the destination and passes them to the multimodal generator.
Multimodal Generator and Text-to-speech The multimodal generator processes action lists from SUBWAY and other components and assigns appro-priate prompts for each action using a template-based generator.
The result is a ‘score’ of prompts and ac-tions which is passed to the Multimodal UI.
The Mul- timodal UI plays this ‘score’ by coordinating changes in the interface with the corresponding TTS prompts.
AT&amp;T’s Natural Voices TTS engine is used to pro-vide the spoken output.
"When the UI receives a mul-timodal score, it builds a stack of graphical actions such as zooming the display to a particular location or putting up a graphical callout."
It then sends the prompts to be rendered by the TTS server.
"As each prompt is synthesized the TTS server sends progress notifications to the Multimodal UI, which pops the next graphical action off the stack and executes it."
Text Planner and User Model
"The text plan-ner receives instructions from MDM for execution of ‘compare’, ‘summarize’, and ‘recommend’ com-mands."
It employs a user model based on multi-attribute decision theory[REF_CITE].
"For example, in order to make a comparison between the set of restaurants shown in Figure 6, the text planner first ranks the restaurants within the set ac-cording to the predicted ranking of the user model."
"Then, after selecting a small set of the highest ranked restaurants, it utilizes the user model to decide which restaurant attributes are important to mention."
The resulting text plan is converted to text and sent to TTS[REF_CITE].
A user model for someone who cares most highly about cost and secondly about food quality and decor leads to a system response such as that in Compare-A above.
"A user model for someone whose selections are driven by food quality and food type first, and cost only second, results in a system response such as that shown in Compare-B."
"Among the selected restaurants, the following of-fer exceptional overall value."
Babbo’s price is 60 dollars.
It has superb food quality.
Il Mulino’s price is 65 dollars.
It has superb food quality.
Uguale’s price is 33 dollars.
It has excellent food.
Note that the restaurants selected for the user who is not concerned about cost includes two rather more expensive restaurants that are not selected by the text planner for the cost-oriented user.
"Multimodal Logger User studies, multimodal data collection, and debugging were accomplished by in-strumenting MATCH agents to send details of user inputs, system processes, and system outputs to a log-ger agent that maintains an XML log designed for multimodal interactions."
"Our critical objective was to collect data continually throughout system devel-opment, and to be able to do so in mobile settings."
"While this rendered the common practice of video-taping user interactions impractical, we still required high fidelity records of each multimodal interaction."
"To address this problem, MATCH logs the state of the UI and the user’s ink, along with detailed data from other components."
"These components can in turn dynamically replay the user’s speech and ink as they were originally received, and show how the sys-tem responded."
"The browser- and component-based architecture of the Multimodal UI facilitated its reuse in a Log Viewer that reads multimodal log files, re-plays interactions between the user and system, and allows analysis and annotation of the data."
"MATCH’s logging system is similar in function to STAMP[REF_CITE], but does not require multimodal interactions to be videotaped and allows rapid re-configuration for different annotation tasks since it is browser-based."
"The ability of the system to log data standalone is important, since it enables testing and collection of multimodal data in realistic mobile environments without relying on external equipment."
Our multimodal logging infrastructure enabled MATCH to undergo continual user trials and evalu-ation throughout development.
Repeated evaluations with small numbers of test users both in the lab and in mobile settings[REF_CITE]have guided the design and iterative development of the system.
This iterative development approach highlighted several important problems early on.
"For example, while it was originally thought that users would for-mulate queries and navigation commands primarily by specifying the names of New York neighborhoods, as in show italian restaurants in chelsea, early field test studies in the city revealed that the need for neighborhood names in the grammar was minimal compared to the need for cross-streets and points of interest; hence, cross-streets and a sizable list of land-marks were added."
Other early tests revealed the need for easily accessible ‘cancel’ and ‘undo’ fea-tures that allow users to make quick corrections.
"We also discovered that speech recognition performance was initially hindered by placement of the ‘click-to-speak’ button and the recognition feedback box on the bottom-right side of the device, leading many users to speak ‘to’ this area, rather than toward the microphone on the upper left side."
This placement also led left-handed users to block the microphone with their arms when they spoke.
Moving the but-ton and the feedback box to the top-left of the device resolved both of these problems.
"After initial open-ended piloting trials, more struc-tured user tests were conducted, for which we devel-oped a set of six scenarios ordered by increasing level of difficulty."
These required the test user to solve problems using the system.
These scenarios were left as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner later this evening at a Thai restaurant on the Upper West Side near her apartment on 95th St. and Broadway.
"Unfortunately, you forgot what time you’re supposed to meet her, and you can’t reach her by phone."
Use MATCH to find the restaurant and write down the restaurant’s telephone number so you can check on the reservation time.
Test users received a brief tutorial that was inten-tionally vague and broad in scope so the users might overestimate the system’s capabilities and approach problems in new ways.
"There were five sub-jects (2 male, 3 female) none of whom had been in-volved in system development."
All of these five tests were conducted indoors in offices.
There were an average of 12.75 multimodal ex-changes (pairs of user input and system response) per scenario.
The overall time per scenario varied from 1.5 to to 15 minutes.
The longer completion times resulted from poor ASR performance for some of the users.
"Although ASR accuracy was low, overall task completion was high, suggesting that the multimodal aspects of the system helped users to complete tasks."
"Unimodal pen commands were recognized more suc-cessfully than spoken commands; however, only 19% of commands were pen only."
"In ongoing work, we are exploring strategies to increase users’ adoption of more robust pen-based and multimodal input."
MATCH has a very fast system response time.
"Benchmarking a set of speech, pen, and multimodal commands, the average response time is approxi-mately 3 seconds (time from end of user input to sys-tem response)."
We are currently completing a larger scale scenario-based evaluation and an independent evaluation of the functionality of the text planner.
"In addition to MATCH, the same multimodal ar-chitecture has been used for two other applications: a multimodal interface to corporate directory infor-mation and messaging and a medical application to assist emergency room doctors."
The medical proto-type is the most recent and demonstrates the utility of the architecture for rapid prototyping.
System devel-opment took under two days for two people.
The MATCH architecture enables rapid develop-ment of mobile multimodal applications.
"Combin-ing finite-state multimodal integration with a speech-act based dialogue manager enables users to interact flexibly using speech, pen, or synchronized combina-tions of the two depending on their preferences, task, and physical and social environment."
The system responds by generating coordinated multimodal pre-sentations adapted to the multimodal dialog context and user preferences.
Features of the system such as the browser-based UI and general purpose finite-state architecture for multimodal integration facili-tate rapid prototyping and reuse of the technology for different applications.
The lattice-based finite-state approach to multimodal understanding enables both multimodal integration and dialogue context to com-pensate for recognition errors.
The multimodal log-ging infrastructure has enabled an iterative process of pro-active evaluation and data collection through-out system development.
Since we can replay multi-modal interactions without video we have been able to log and annotate subjects both in the lab and in NYC throughout the development process and use their input to drive system development.
Spoken dialogue systems promise effi-cient and natural access to information services from any phone.
"Recently, spo-ken dialogue systems for widely used ap-plications such as email, travel informa-tion, and customer care have moved from research labs into commercial use."
These applications can receive millions of calls a month.
"This huge amount of spoken dialogue data has led to a need for fully automatic methods for selecting a subset of caller dialogues that are most likely to be useful for further system improve-ment, to be stored, transcribed and further analyzed."
This paper reports results on automatically training a Problematic Di-alogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.
We show that using fully automatic features we can identify classes of problematic dialogues with accuracies from 67% to 89%.
Spoken dialogue systems promise efficient and nat-ural access to a large variety of information services from any phone.
"Deployed systems and research prototypes exist for applications such as personal email and calendars, travel and restaurant informa-tion, personal banking, and customer care."
"Within the last few years, several spoken dialogue systems for widely used applications have moved from re-search labs into commercial use[REF_CITE]."
These applications can receive millions of calls a month.
There is a strong require-ment for automatic methods to identify and extract dialogues that provide training data for further sys-tem development.
"As a spoken dialogue system is developed, it is first tested as a prototype, then fielded in a limited setting, possibly running with human supervisi[REF_CITE], and finally deployed."
"At each stage from research prototype to deployed commer-cial application, the system is constantly undergoing further development."
"When a system is prototyped in house or first tested in the field, human subjects are often paid to use the system and give detailed feedback on task completion and user satisfacti[REF_CITE]."
"Even when a system is deployed, it often keeps evolving, either because customers want to do different things with it, or because new tasks arise out of develop-ments in the underlying application."
"However, real customers of a deployed system may not be willing to give detailed feedback."
"Thus, the widespread use of these systems has created a data management and analysis problem."
"System designers need to constantly track system performance, identify problems, and fix them."
"Sys-tem modules such as automatic speech recognition (ASR), natural language understanding (NLU) and dialogue management may rely on training data col-lected at each phase."
ASR performance assessment relies on full transcription of the utterances.
"Dia-logue manager assessment relies on a human inter-face expert reading a full transcription of the dia-logue or listening to a recording of it, possibly while examining the logfiles to understand the interaction between all the components."
"However, because of the high volume of calls, spoken dialogue service providers typically can only afford to store, tran-scribe, and analyze a small fraction of the dialogues."
"Therefore, there is a great need for methods for both automatically evaluating system performance, and for extracting subsets of dialogues that provide good training data for system improvement."
"This is a difficult problem because by the time a system is deployed, typically over 90% of the dialogue inter-actions result in completed tasks and satisfied users."
Dialogues such as these do not provide very use-ful training data for further system development be-cause there is little to be learned when the dialogue goes well.
Previous research on spoken dialogue evaluation proposed the application of automatic classifiers for identifying and predicting of problematic dialogues[REF_CITE]for the purpose of automatically adapting the dialogue man-ager.
Here we apply similar methods to the dialogue corpus data-mining problem described above.
We report results on automatically training a Problem-atic Dialogue Identifier (PDI) to classify problem-atic human-computer dialogues using the[REF_CITE]DARPA Communicator corpus.
Section 2 describes our approach and the dialogue corpus.
Section 3 describes how we use the DATE dialogue act tagging scheme to define input features for the PDI.
Section 4 presents a method and results for automatically predicting task completion.
Sec-tion 5 presents results for predicting problematic di-alogues based on the user’s satisfaction.
We show that we identify task failure dialogues with 85% ac-curacy (baseline 59%) and dialogues with low user satisfaction with up to 89% accuracy.
We discuss the application of the PDI to data mining in Section 6.
"Finally, we summarize the paper and discuss future work."
Our experiments apply CLASSIFICATION and RE - GRESSION trees (CART)[REF_CITE]to train a Problematic Dialogue Identifier (PDI) from a corpus of human-computer dialogues.
CLASSI - FICATION trees are used for categorical response variables and REGRESSION trees are used for con-tinuous response variables.
CART trees are binary decision trees.
"A CLASSIFICATION tree specifies what queries to perform on the features to maximize CLASSIFICATION ACCURACY , while REGRESSION trees derive a set of queries to maximize the COR - RELATION of the predicted value and the original value."
"Like other machine learners, CART takes as input the allowed values for the response variables; the names and ranges of values of a fixed set of input features; and training data specifying the response variable value and the input feature values for each example in a training set."
"Below, we specify how the PDI was trained, first describing the corpus, then the response variables, and finally the input features derived from the corpus."
Corpus: We train and test the PDI on the DARPA Communicator[REF_CITE]corpus of 1242 dia-logues.
"This corpus represents interactions with real users, with eight different Communicator travel planning systems, over a period of six months from April[REF_CITE]."
The dialogue tasks range from simple domestic round trips to multileg inter-national trips requiring both car and hotel arrange-ments.
The corpus includes logfiles with logged events for each system and user turn; hand transcrip-tions and automatic speech recognizer (ASR) tran-scription for each user utterance; information de-rived from a user profile such as user dialect region; and a User Satisfaction survey and hand-labelled Task Completion metric for each dialogue.
We ran-domly divide the corpus into 80% training (894 dia-logues) and 20% testing (248 dialogues).
"Defining the Response Variables: In principle, either low User Satisfaction or failure to complete the task could be used to define problematic dia-logues."
"Therefore, both of these are candidate re-sponse variables to be examined."
The User Satisfac-tion measure derived from the user survey ranges be-tween 5 and 25.
"Task Completion is a ternary mea-sure where no Task Completion is indicated by 0, completion of only the airline itinerary is indicated by 1, and completion of both the airline itinerary and ground arrangements, such as car and hotel book-ings, is indicated by 2."
"We also defined a binary ver-sion of Task Completion, where Binary Task Com-pletion=0 when no task or subtask was complete (equivalent to Task Completion=0), and Binary Task Completion=1 where all or some of the task was complete (equivalent to Task Completion=1 or Task Completion=2)."
Figure 1 shows the frequency of dialogues for varying User Satisfaction for cases where Task Completion is 0 (solid line) and Task Completion is greater than 0 (dotted lines).
Note that Task Com-pletion is 1 or 2 for a number of dialogues for which User Satisfaction is low.
"Figure 2 illustrates such a dialogue (system turns are labelled S, user turns as U, and ASR hypotheses as REC)."
"Here, low User Satisfaction may be due to the fact that the user had to repeat herself many times before the system un-derstood the departure city."
"An automatic surrogate for ASR accuracy (such as ASR confidence) would not be adequate for identifying this problematic di-alogue, because here either the dialogue manager or the SLU component is at fault."
Another dialogue subset of interest in Figure 1 is one for which Task Completion is 0 but User Satisfaction is high.
A common cause for non-Task Completion in these di-alogues is database access problems.
The fact that the interaction went well until this point is not cap-tured by the Task Completion metric.
Thus we de-cided to use both User Satisfaction and Task Com-pletion as response variables in separate experiments in order to allow the PDI to capture the relationship between these two variables.
Input Features: Both User Satisfaction and Task Completion predictors are trained using two types of automatically extractable features: ([Footnote_1]) logfile fea-tures; and (2) system dialogue act types.
1 We assume this is automatically derivable by automatic number identification (ANI).
"The logfile features include efficiency metrics such as the number of system and user turns spent on the task; the total time of the dialogue in milliseconds; and situational metrics, such as phone type 1 which affects ASR."
The system dialogue act types are extracted from the logfiles using the DATE (Dia-logue Act Tagging for Evaluation) automatic tag- ging scheme[REF_CITE].
The purpose of these features is to extract numerical correlates of system dialogue behaviors.
This dialogue act la-belling procedure is detailed in Section 3.
Figure 3 summarizes the types of features used to train the User Satisfaction predictor.
"In addition to the efficiency metrics and the DATE labels, Task Success can itself be used as a predictor."
"This can either be the hand-labelled feature or an approxima-tion as predicted by the Task Completion Predictor, described in Section 4."
Figure 4 shows the system design for automatically predicting User Satisfac- tion with the three types of input features.
The dialogue act labelling of the corpus follows the DATE tagging scheme[REF_CITE].
"In DATE, utterance classification is done along three cross-cutting orthogonal dimensions."
The CONVERSATIONAL - DOMAIN dimension specifies the domain of discourse that an utterance is about.
The SPEECH ACT dimension captures distinctions between communicative goals such as requesting information ( REQUEST - INFO ) or presenting infor-mation ( PRESENT - INFO ).
The TASK - SUBTASK di-mension specifies which travel reservation subtask the utterance contributes to.
"The SPEECH ACT and CONVERSATIONAL - DOMAIN dimensions are gen-eral across domains, while the TASK - SUBTASK di-mension is domain- and sometimes system-specific."
"Within the conversational domain dimension, DATE distinguishes three domains (see Figure 5)."
The ABOUT - TASK domain is necessary for evaluat-ing a dialogue system’s ability to collaborate with a speaker on achieving the task goal.
The ABOUT - COMMUNICATION domain reflects the system goal of managing the verbal channel of communication and providing evidence of what has been under-stood.
All implicit and explicit confirmations are about communication.
The ABOUT - SITUATION - FRAME domain pertains to the goal of managing the user’s expectations about how to interact with the system.
Examples of each speech act are shown in Figure 6.
"The TASK - SUBTASK dimension distinguishes among 28 subtasks, some of which can also be grouped at a level below the top level task."
"The TOP - LEVEL - TRIP task describes the task which con-tains as its subtasks the ORIGIN , DESTINATION ,"
"DATE , TIME , AIRLINE , TRIP - TYPE , RETRIEVAL and ITINERARY tasks."
The GROUND task includes both the HOTEL and CAR - RENTAL subtasks.
The HOTEL task includes both the HOTEL - NAME and HOTEL - LOCATION subtasks. [Footnote_2]
"2 ABOUT - SITUATION - FRAME utterances are not specific to any particular task and can be used for any subtask, for example, system statements that it misunderstood. Such utterances are given a “meta” dialogue act status in the task dimension."
"For the DATE labelling of the corpus, we imple-mented an extended version of the pattern matcher that was used for tagging the[REF_CITE]corpus[REF_CITE]."
This method identified and labelled an utterance or utterance se-quence automatically by reference to a database of utterance patterns that were hand-labelled with the DATE tags.
"Before applying the pattern matcher, a named-entity labeler was applied to the system utterances, matching named-entities relevant in the travel domain, such as city, airport, car, hotel, airline names etc.."
The named-entity labeler was also ap-plied to the utterance patterns in the pattern database to allow for generality in the expression of com-municative goals specified within DATE.
"For this named-entity labelling task, we collected vocabulary lists from the sites, which maintained such lists for developing their system. 3 The extension of the pat-tern matcher for the 2001 corpus labelling was done because we found that systems had augmented their inventory of named entities and utterance patterns from 2000 to 2001, and these were not accounted for by the 2000 tagger database."
"For the extension, we collected a fresh set of vocabulary lists from the sites and augmented the pattern database with ad-ditional 800 labelled utterance patterns."
We also implemented a contextual rule-based postprocessor that takes any remaining unlabelled utterances and attempts to label them by looking at their surround-ing DATE labels.
More details about the extended tagger can be found[REF_CITE].
A hand evaluation of 10 randomly se-lected dialogues from each system shows that we achieved a classification accuracy of 96 at the ut-terance level.
"For User Satisfaction Prediction, we found that the distribution of DATE acts were better captured by using the frequency normalized over the total number of dialogue acts."
"In addition to these un-igram proportions, the bigram frequencies of the DATE dialogue acts were also calculated."
"In the fol-lowing two sections, we discuss which DATE labels are discriminatory for predicting Task Completion and User Satisfaction."
"In order to automatically predict Task Comple-tion, we train a CLASSIFICATION tree to catego-rize dialogues into Task Completion=0, Task Com-pletion=1 or Task Completion=2."
"Recall that a CLASSIFICATION tree attempts to maximize CLAS - SIFICATION ACCURACY , results for Task Comple-tion are thus given in terms of percentage of dia-logues correctly classified."
The majority class base-line is 59.[Footnote_3]% (dialogues where Task Completion=1).
3 The named entities were preclassified into their respective semantic classes by the sites.
The tree was trained on a number of different in-put features.
"The most discriminatory ones, how-ever, were derived from the DATE tagger."
"We use the primitive DATE tags in conjunction with a feature called GroundCheck (GC), a boolean fea-ture indicating the existence of DATE tags related to making ground arrangements, specifically re-quest info:hotel name, request info:hotel location, offer:hotel and offer:rental."
Table 1 gives the results for Task Completion pre-diction accuracy using the various types of features.
"The first row is for predicting ternary Task Comple-tion, and the second for predicting binary Task Com-pletion."
Using automatic logfile features (ALF) is not effective for the prediction of either types of Task Completion.
"However, the use of GroundCheck re-sults in an accuracy of 79% for the ternary Task Completion which is significantly above the base-line (df = 247, t = -6.264, p .0001)."
Adding in the other DATE features yields an accuracy of 85%.
"For Binary Task Completion it is only the use of all the DATE features that yields an improvement over the baseline of 92%, which is significant (df = 247, t = 5.83, p .0001)."
A diagram of the trained decision tree for ternary Task Completion is given in Figure 7.
"At any junc-tion in the tree, if the query is true then one takes the path down the right-hand side of the tree, oth-erwise one takes the left-hand side."
The leaf nodes contain the predicted value.
The GroundCheck fea-ture is at the top of the tree and divides the data into Task Completion 2 and Task Completion 2.
"If GroundCheck 1, then the tree estimates that Task Completion is 2, which is the best fit for the data given the input features."
"If GroundCheck 0 and there is an acknowledgment of a booking, then probably a flight has been booked, therefore, Task Completion is predicted to be 1."
"Interestingly, if there is no acknowledgment of a booking then Task Completion 0, unless the system got to the stage of asking the user for an airline preference and if re-quest info:top level trip 2."
More than one of these DATE types indicates that there was a problem in the dialogue and that the information gathering phase started over from the beginning.
The binary Task Completion decision tree simply checks if an acknowledgement:flight booking has occurred.
"If it has, then Binary Task Com-pletion=1, otherwise it looks for the DATE act about situation frame:instruction:meta situation info, which captures the fact that the system has told the user what the system can and cannot do, or has informed the user about the current state of the task."
"This must help with Task Completion, as the tree tells us that if one or more of these acts are observed then Task Completion=1, otherwise Task Completion=0."
Quantitative Results: Recall that REGRESSION trees attempt to maximize the CORRELATION of the predicted value and the original value.
"Thus, the re-sults of the User Satisfaction predictor are given in terms of the correlation between the predicted User Satisfaction and actual User Satisfaction as calcu-lated from the user survey."
"Here, we also provide R for comparison with previous studies."
Table 2 gives the correlation results for User Satisfaction for dif-ferent feature sets.
The User Satisfaction predictor is trained using the hand-labelled Task Completion feature for a topline result and using the automati-cally obtained Task Completion (Auto TC) for the fully automatic results.
We also give results using Binary Task Completion (BTC) as a substitute for Task Completion.
The first column gives results us-ing features extracted from the logfile; the second column indicates results using the DATE unigram proportions and the third column indicates results when both the DATE unigram and bigram features are available.
The first row of Table 2 indicates that perfor-mance across the three feature sets is indistinguish-able when hand-labelled Task Completion (HL TC) is used as the Task Completion input feature.
A comparison of Row 1 and Row 2 shows that the PDI performs significantly worse using only auto-matic features (z = 3.18).
"Row 2 also indicates that the DATE bigrams help performance, although the difference between R = .438 and R = .472 is not significant."
"The third and fourth rows of Table 1 indicate that for predicting User Satisfaction, Bi-nary Task Completion is as good as or better than Ternary Task  Completion) uses .hand-labelledThe highest correlationBinary Taskof"
Completion and the logfile features and DATE uni-gram proportions and bigram counts.
"Again, we see that the Automatic Binary Task Completion (Auto BTC) performs significantly worse than the hand-labelled version (z = -3.18)."
Row 4 includes the best totally automatic system: using Automatic Binary yields a correlation of 0.484 (  ).
Task Completion and DATE unigrams and bigrams
Regression Tree Interpretation: It is interest-ing to examine the trees to see which features are used for predicting User Satisfaction.
A metric called Feature Usage Frequency indicates which fea-tures are the most discriminatory in the CART tree.
"Specifically, Feature Usage Frequency counts how often a feature is queried for each data point, nor-malized so that the sum of Feature Usage Frequency values for all the features sums to one."
"The higher a feature is in the tree, the more times it is queried."
"To calculate the Feature Usage Frequency, we grouped the features into three types: Task Completion, Log-file features and DATE frequencies."
Feature Us-age Frequency for the logfile features is 37%.
"Task Completion occurs only twice in the tree, however, it makes up 31because it occurs at the top of the tree."
The Feature Usage Frequency for DATE cat-egory frequency is 32%.
We will discuss each of these three groups of features in turn.
"The most used logfile feature is TurnsOnTask which is the number of turns which are task- oriented, for example, initial instructions on how to use the system are not taken as a TurnOnTask."
Shorter dialogues tend to have a higher User Sat-isfaction.
This is reflected in the User Satisfaction scores in the tree.
"However, dialogues which are long ([REF_CITE]) can be satisfactory (User Satisfaction = 15.2) as long as the task that is com-pleted is long, i.e., if ground arrangements are made in that dialogue (Task Completion=2)."
"If ground ar-rangements are not made, the User Satisfaction is lower (11.6)."
"Phone type is another important fea-ture queried in the tree, so that dialogues conducted over corded phones have higher satisfaction."
This is likely to be due to better recognition performance from corded phones.
"As mentioned previously, Task Completion is at the top of the tree and is therefore the most queried feature."
This captures the relationship between Task Completion and User Satisfaction as illustrated in Figure 1.
"Finally, it is interesting to examine which DATE tags the tree uses."
"If there have been more than three acknowledgments of bookings, then several legs of a journey have been successfully booked, therefore User Satisfaction is high."
"In particular, User Satisfaction is high if the system has asked if the user would like a price for their itinerary which is one of the final dialogue acts a system does before the task is completed."
The DATE act about comm:apology:meta slu reject is a measure of the system’s level of misunderstanding.
"There-fore, the more of these dialogue act types the lower User Satisfaction."
"This part of the tree uses length in a similar way described earlier, whereby long di-alogues are only allocated lower User Satisfaction if they do not involve ground arrangements."
Users do not seem to mind longer dialogues as long as the system gives a number of implicit confirma-tions.
The dialogue act request info:top level trip usually occurs at the start of the dialogue and re-quests the initial travel plan.
"If there are more than one of this dialogue act, it indicates that a START-OVER occurred due to system failure, and this leads to lower User Satisfaction."
A rule containing the bigram request info:depart day month date+USER states that if there is more than one occurrence of this request then User Satisfaction will be lower.
USER is the single category used for user-turns.
No auto-matic method of predicting user speech act is avail-able yet for this data.
"A repetition of this DATE bigram indicates that a misunderstanding occurred the first time it was requested, or that the task is multi-leg in which case User Satisfaction is gener- ally lower."
"The tree that uses Binary Task Completion is identical to the tree described above, apart from one binary decision which differentiates dialogues where Task Completion=1 and Task Completion=2."
"Instead of making this distinction, it just uses dia-logue length to indicate the complexity of the task."
"In the original tree, long dialogues are not penalized if they have achieved a complex task (i.e. if Task Completion=2)."
The Binary Task Completion tree has no way of making this distinction and therefore just penalizes very long dialogues (where[REF_CITE]).
"The Feature Usage Frequency for the Task Completion features is reduced from 31% to 21%, and the Feature Usage Frequency for the log-file features increases to 47%."
We have shown that this more general tree produces slightly better re-sults.
"So far, we have described a PDI that predicts User Satisfaction as a continuous variable."
"For data min-ing, system developers will want to extract dialogues with predicted User Satisfaction below a particular threshold."
This threshhold could vary during dif-ferent stages of system development.
"As the sys-tem is fine tuned there will be fewer and fewer dia-logues with low User Satisfaction, therefore in order to find the interesting dialogues for system develop-ment one would have to raise the User Satisfaction threshold."
"In order to illustrate the potential value of our PDI, consider an example threshhold of 12 which divides the data into 73.4% good dialogues where[REF_CITE]which is our baseline result."
Table 3 gives the recall and precision for the PDIs described above which use hand-labelled Task Com-pletion and Auto Task Completion.
"In the data, 26.6% of the dialogues are problematic (User Sat-isfaction is under 12), whereas the PDI using hand-labelled Task Completion predicts that 21.8% are problematic."
"Of the problematic dialogues, 54.5% are classified correctly (Recall)."
Of the dialogues that it classes as problematic 66.7% are problematic (Precision).
The results for the automatic system show an improvement in Recall: it identifies more problematic dialogues correctly (66.7%) but the pre-cision is lower.
What do these numbers mean in terms of our orig-inal goal of reducing the number of dialogues that need to be transcribed to find good cases to use for system improvement?
"If one had a budget to transcribe 20% of the dataset containing 100 dia-logues, then by randomly extracting 20 dialogues, one would transcribe 5 problematic dialogues and 15 good dialogues."
"Using the fully automatic PDI, one would obtain 12 problematic dialogues and 8 good dialogues."
"To look at it another way, to extract 15 problematic dialogues out of 100, 55% of the data would need transcribing."
This is a massive improvement over randomly choosing dia-logues.
This paper presented a Problematic Dialogue Identi-fier which system developers can use for evaluation and to extract problematic dialogues from a large dataset for system development.
We describe PDIs for predicting both Task Completion and User Satis-faction in the DARPA[REF_CITE]corpus.
There has been little previous work on recogniz-ing problematic dialogues.
"However, a number of studies have been done on predicting specific errors in a dialogue, using a variety of automatic and hand-labelled features, such as ASR confidence and se-mantic labels[REF_CITE]."
Pre-vious work on predicting problematic dialogues be-fore the end of the dialogue[REF_CITE]achieved accuracies of 87% using hand-labelled fea-tures (baseline 67%).
Our automatic Task Comple-tion PDI achieves an accuracy of 85%.
"Previous work also predicted User Satisfaction by applying multi-variate linear regression features with and without DATE features and showed  thatto DATE  improved  (Walkertheetmodelal., 2001fit from)."
Our best model has an  .
One potential explanation for this difference is that the DATE features are most useful in combination with non-automatic features such as Word Accuracy which the previous study used.
The User Satisfaction PDI using fully automatic features achieves a correlation of 0.484.
"In future work, we hope to improve our results by trying different machine learning methods; includ-ing the user’s dialogue act types as input features; and testing these methods in new domains."
The work reported in this paper was partially funded by DARPA contract[REF_CITE]-99-3-0003.
µ &quot;¾k¹p¿Q¹kÀ¾k¹±ÀÁ·U¿º-Â@¿`·nÃwÀ¼¸: ¹±ÀÉÈC¸@ÂFÈ:::Æ (ÈQÐ:ÃwÀ¼Í`Ã::: ?Êw¶UÓ8¶U¸8Êw¶U¸8·nÀ¼¶¾UÔ+.Õ ¸:Æ Êw¶UÄ±ºÉ»CÀ¼¸::::ÂFÓ:¹±ÀÉÈC¸ ¿ÇÌ8È`Î:¹=::=}º ¹nÃ::ÍCÎ8¿QÍ`¶¾UÙCÃ[Ì8¶U¹k×:&amp;È×&amp;¶UË`¶UÄ}:: %:ÂFÓ:} &amp;À¾¸:&amp;: Ó8¿ÇÓ8¶UÄ½&amp;×&amp;¶Ó:@¿Q¸0¶UÂFÓwÀ¼ÄkÀ·U¿} ¹nÃ8¿Q¹]:&amp;¶ (.ÃwÀÁ·nÃF¾k»w¸:Æ ((:Ó #.× :Ã ¶U¸ ::Æ ÍÇºàÀ¾kÃ% ÃwÀ¼¸:¶¾k¶`Ô&amp;:Ä(Än¶¾kÎwº¼¹p¾(¾kÃ:ÈQ×Û¹nÃ8¿Q¹ ¿ºÉ¹nÃ:È`Î:Í`Ã&quot;: ¾±Î:ÂFÓ: ¾±Â@}¿ ºÑº®¾k¶U¹&amp;ÈQÐ(Ó::Æ @¿Q¹kÀ¼È`¸8¾%·U¿Q¸0Ì8ÈwÈ¾k¹F¹nÃ:¶ Ìw»ìëíîÞCÎ8¿}ºÑÀ¼¹k»-ÈÇÐ³¹nÃ:::¶LÎÃwÀ¼¸::¸wÀ¼ÂFÓ¶¾k¶Ó8¿QÄp¾k¶¾: :¶`Ô
"Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries."
We present a novel algorithm for translating named entity phrases using easily obtain-able monolingual and bilingual resources.
We report on the application and evalua-tion of this algorithm in translating Arabic named entities to English.
We also com-pare our results with the results obtained from human translations and a commer-cial system for the same task.
"Named entity phrases are being introduced in news stories on a daily basis in the form of personal names, organizations, locations, temporal phrases, and monetary expressions."
"While the identifica-tion of named entities in text has received sig-nificant attention (e.g.,[REF_CITE]and[REF_CITE]), translation of named entities has not."
"This translation problem is especially challenging because new phrases can appear from nowhere, and because many named-entities are do-main specific, not to be found in bilingual dictionar-ies."
A system that specializes in translating named en-tities such as the one we describe here would be an important tool for many NLP applications.
Statisti- cal machine translation systems can use such a sys-tem as a component to handle phrase translation in order to improve overall translation quality.
Cross-Lingual Information Retrieval (CLIR) systems could identify relevant documents based on translations of named entity phrases provided by such a sys-tem.
"Question Answering (QA) systems could ben-efit substantially from such a tool since the answer to many factoid questions involve named entities (e.g., answers to who questions usually involve Per-sons/Organizations, where questions involve Loca-tions, and when questions involve Temporal Ex-pressions)."
"In this paper, we describe a system for Arabic-English named entity translation, though the tech-nique is applicable to any language pair and does not require especially difficult-to-obtain resources."
The rest of this paper is organized as follows.
"In Section 2, we give an overview of our approach."
"In Section 3, we describe how translation candidates are generated."
"In Section 4, we show how mono-lingual clues are used to help re-rank the translation candidates list."
"In Section 5, we describe how the candidates list can be extended using contextual in-formation."
We conclude this paper with the evalua-tion results of our translation algorithm on a test set.
We also compare our system with human translators and a commercial system.
The frequency of named-entity phrases in news text reflects the significance of the events they are associ-ated with.
"When translating named entities in news stories of international importance, the same event will most likely be reported in many languages in-cluding the target language."
"Instead of having to come up with translations for the named entities of-ten with many unknown words in one document, sometimes it is easier for a human to find a docu-ment in the target language that is similar to, but not necessarily a translation of, the original document and then extract the translations."
Let’s illustrate this idea with the following example:
"We would like to translate the named entities that appear  in  the following Arabic ! excerpt #!&quot; $% : &amp; ( *),+   :5 $!6  #$ &quot; %$&apos; 798 ;: &lt; 0 - . 102./ 4 = . + 3 - @&gt; ? 8 4 =B: ADC0 E 4 &lt; Q A * GH4SR J : 6A &lt; 4 =6 EBK *  4 0&lt; M+ + - L &gt;0 &gt;7 $+ 7"
X  X 0 4  [ \ $]  E
The Arabic newspaper article from which we ex-tracted this excerpt is about negotiations between the US and North Korean authorities regarding the search for the remains of US soldiers who died dur-ing the Korean war.
"We presented the Arabic document to a bilingual speaker  $ and asked - them . tohzāntranslate”, “ - &gt; the 7 + locationsāwnsā- “ L 0 &gt; 7 T  tšwzyn W 0 M ˘ n”, and “ kwǧānǧ.”"
"The translations they provided were Chozin Reserve, Onsan, and Kojanj."
"It is obvious that the human attempted to sound out names and despite coming close, they failed to get them correctly as we will see later."
"When translating unknown or unfamiliar names, one effective approach is to search for an English document that discusses the same subject and then extract the translations."
"For this example, we start by creating the following Web query that we use with the search engine:"
"Search Query 1: soldiers remains, search, North Korea, and US."
This query returned many hits.
The top document returned by the search engine 1 we used contained the following paragraph:
"The targeted area is near Unsan, which saw several battles between the U.S. 1[URL_CITE]"
Army’s 8th Cavalry regiment and Chinese troops who launched a surprise offensive in late 1950.
This allowed us to create a more precise query by adding Unsan to the search terms:
"Search Query 2: soldiers remains, search, North Korea, US, and Unsan."
This search query returned only 3 documents.
The first one is the above document.
The third is the top level page for the second document.
The second document contained the following excerpt:
"Kaechon includes an area nick-named the ”Gauntlet,” where the U.S. Army’s 2nd Infantry Division conducted its famous fighting withdrawal along a narrow road through six miles of Chinese ambush positions[REF_CITE]."
The Chosin Reservoir campaign left ap-proximately 750 Marines and soldiers missing in action from both the east and west sides of the reservoir in northeastern North Korea.
This human translation method gives us the cor-rect translation for the names we are interested in.
"Inspired by this, our goal is to tackle the named en-tity translation problem using the same approach de-scribed above, but fully automatically and using the least amount of hard-to-obtain bilingual resources."
"As shown in Figure 1, the translation process in our system is carried out in two main steps."
"Given a named entity in the source language, our transla-tion algorithm first generates a ranked list of transla-tion candidates using bilingual and monolingual re-sources, which we describe in the Section 3."
"Then, the list of candidates is re-scored using different monolingual clues (Section 4)."
"Named entity phrases can be identified fairly accurately (e.g., Bikelet al. (1999) report an F-[REF_CITE].9%)."
"In addition to identify-ing phrase boundaries, named-entity identifiers also provide the category and sub-category of a phrase (e.g., ENTITY NAME, and PERSON)."
Different types of named entities are translated differently and hence our candidate generator has a specialized module for each type.
"Numerical and temporal ex-pressions typically use a limited set of vocabulary words (e.g., names of months, days of the week, etc.) and can be translated fairly easily using simple translation patterns."
"Therefore, we will not address them in this paper."
"Instead we will focus on person names, locations, and organizations."
"But before we present further details, we will discuss how words can be transliterated (i.e., “sounded-out”), which is a crucial component of our named entity translation algorithm."
Transliteration is the process of replacing words in the source language with their approximate pho-netic or spelling equivalents in the target language.
Transliteration between languages that use similar alphabets and sound systems is very simple.
"How-ever, transliterating names from Arabic into English is a non-trivial task, mainly due to the differences in their sound and writing systems."
Vowels in Ara-bic come in two varieties: long vowels and short vowels.
"Short vowels are rarely written in Arabic in newspaper text, which makes pronunciation and meaning highly ambiguous."
"Also, there is no one-to-one correspondence between Arabic sounds and English sounds."
"For example, English P and B are both mapped into Arabic “ b”; Arabic “ h. ” and “ h-” into English H; and so on."
The transliteration pro-cess is based on a generative model of how an En-glish name is transliterated into Arabic.
"It consists of several steps, each is defined as a probabilistic model represented as a finite state machine."
"First, an English word is generated according to its uni-gram probabilities  ."
"Then, the English word is pronounced with probability , which is col-lected directly from an English pronunciation dictio-nary."
"Finally, the English phoneme sequence is con-verted into Arabic writing with probability   ."
"According to this model, the transliteration proba-bility is given by the following equation:      (1)"
The transliterations proposed by this model are generally accurate.
"However, one serious limita-tion of this method is that only English words with known pronunciations can be produced."
"Also, hu-man translators often transliterate words based on how they are spelled in the source language."
"For example, Graham is transliterated into Arabic as “  ġrāhām” and not as “ ġrām”."
"To ad-dress these limitations, we extend this approach by using a new spelling-based model in addition to the phonetic-based model."
"The spelling-based model we propose (described in detail[REF_CITE]) directly maps English letter sequences into Arabic letter se-quences with probability , which are trained on a small English/Arabicname list without the need for English pronunciations."
"Since no pronunciations are needed, this list is easily obtainable for many lan-guage pairs."
We also extend the model  to in-clude a letter trigram model in addition to the word unigram model.
This makes it possible to generate words that are not already defined in the word uni-gram model.
The transliteration score according to this model is given by:    (2)
The phonetic-based and spelling-based models are combined into a single transliteration model.
The transliteration score for an English word given an Arabic word is a linear combination of the phonetic-based and the spelling-based transliter-ation scores as follows:            (3)
Person names are almost always transliterated.
The translation candidates for typical person names are generated using the transliteration module described above.
Finite-state devices produce a lattice con-taining all possible transliterations for a given name.
The candidate list is created by extracting the n-best transliterations for a given name.
The score of each candidate in the list is the transliteration probabil-ity as given by Equation 3.
"For example, the name “ - 0 ?( klyntwn byl” is transliterated into: Bell Clinton, Bill Clinton, Bill Klington, etc."
"Words in organization and location names, on the other hand, are either translated (e.g., “ - .  hzā-n” as Reservoir) or transliterated (e.g., “ L ˘ 0 &gt; $ 7 tšwzyn” as Chosin), and it is not clear when a word must be translated and when it must be transliter-ated."
"So to generate translation candidates for a given phrase , words in the phrase are first trans-lated using a bilingual dictionary and they are also transliterated."
Our candidate generator combines the dictionary entries and n-best transliterations for each word in the given phrase into a regular expres-sion that accepts all possible permutations of word translation/transliteration combinations.
"In addition to the word transliterations and translations, En-glish zero-fertility words (i.e., words that might not have Arabic equivalents in the named entity phrase such as of and the) are considered."
This regular expression is then matched against a large English news corpus.
All matches are then scored according to their individual word translation/transliteration scores.
"The score for a given candidate is given by a modified IBM Model 1 probability[REF_CITE]as follows:         (4)   ! #&quot;%$ &amp; !  (5) where ( is the length of , ) is the length of , is a scaling factor based ! on the number of matches of found, and ! is the index of the En-glish word aligned with  ! accordingis a linear tocombinationalignment ."
"The probability $  of the transliteration and translation score, where the translation score is a uniform probability over all dictionary entries for ! ."
"The scored matches form the list of translation “candidates  L GH al-h. nāzyrFor example *  W hlyǧ, the” includescandidateBaylistof Pigsfor ˘ ˘ and Gulf of Pigs."
"Once a ranked list of translation candidates is gen-erated for a given phrase, several monolingual En-glish resources are used to help re-rank the list."
"The candidates are re-ranked according to the following equation: +-, . /   +#1  &amp;/ 547698  &amp;/ (6) where 698  &amp;/ is the re-scoring factor used."
Straight Web Counts:[REF_CITE]used phrase Web frequency to disambiguate possible En-glish translations for German and Spanish com-pound nouns.
We use normalized Web counts of named entity phrases as the first re-scoring fac-tor used  to rescore translation  candidates.
"For the “ - 0 ?( klyntwn byl” example, the top two translation candidates are A@ Bell Clinton with translit-eration B := score &gt; &quot; ;: &lt;?= &gt; and Bill Clinton with score ."
The B Web frequency counts of these two names are:  and HIGJ= respectively.
"This gives  &quot ; &quot; B B us revised scores of : 4 = &gt; and : H 4 = &gt; , respectively, which leads to the correct translation being ranked highest."
It is important to consider counts for the full name rather than the individual words in the name to get person name “accurate counts. *
To M illustrate - 02. thisǧwnpoint.”
"Theconsidertranslit-thekyl eration module proposes Jon and John as possible transliterations for the first name, and Keele and Kyl among others for the last name."
"The normalized counts for the individual words are: (John, 0.9269), (Jon, 0.0688), (Keele, 0.0032), and (Kyl, 0.0011)."
"To use these normalized counts to score and rank the first name/last name combinations in a way sim-ilar to a unigram language model, we would get the following name/score pairs: (John Keele, 0.003), (John Kyl, 0.001), (Jon Keele, 0.0002), and (Jon Kyl, CK:  4 =  &gt; )."
"However, the normalized phrase counts for the possible full names are: (Jon Kyl, 0.8976), (John Kyl, 0.0936), (John Keele, 0.0087), and (Jon Keele, 0.0001), which is more desirable as Jon Kyl is an often-mentioned US Senator."
"Co-reference: When a named entity is first men-tioned in a news article, typically the full form of the phrase (e.g., the full name of a person) is used."
"Later references to the name often use a shortened version of the name (e.g, the last name of the person)."
Short-ened versions are more ambiguous by nature than the full version of a phrase and hence more difficult to translate.
"Also, longer phrases tend to have more accurate Web counts than shorter ones as we have shown above."
"For example, the phrase “ 0  al-"
G mǧls” is translated !(&apos; as [ nwāb the House of Rep-resentatives.
The word “ al-mǧls” [Footnote_2] might be used for later references to this phrase.
2 “ al-mǧls” is the same word as “ mǧls” but with the definite article a- attached.
"In that “case ! , &apos;( we [ areal-mǧlsconfronted” whichwithis ambiguousthe task of translatingand could refer to a number of things including: the Council when referring to “ G mǧls” (the Se-al- mn curity Council); the House when referring to ‘ 0 6 al-nwāb  mǧls” (the House of Representatives); and as the Assembly when referring to “ al- mt  mǧls” (National Assembly)."
"If we are able to determine that in fact it was re-ferring to the House of Representatives, then, we can translate it accurately as the House."
This can be done by comparing the shortened phrase with the rest of the named entity phrases of the same type.
"If the shortened phrase is found to be a sub-phrase of only one other phrase, then, we conclude that the short-ened phrase is another reference to the same named entity."
In that case we use the counts of the longer phrase to re-rank the candidates of the shorter one.
Contextual Web Counts: In some cases straight Web counting does not help the re-scoring.
"For - + ex- L ample, the Q top two translation candidates for “ mārwn  +  dwnāld” are Donald Martin and Don-ald Marron."
"Their straight Web counts are 2992 and 2509, respectively."
These counts do not change the ranking of the candidates list.
We next seek a more accurate counting method by counting phrases only if they appear within a certain context.
"Using search engines, this can be done using the boolean operator AND."
"For the previous example, we use Wall Street as the contextual information In this case we get the counts 15 and 113 for Donald Martin and Donald Marron, respectively."
This is enough to get the cor-rect translation as the top candidate.
The challenge is to find the contextual informa-tion that provide the most accurate counts.
We have experimented with several techniques to identify the contextual information automatically.
Some of these techniques use document-wide contextual informa-tion such as the title of the document or select key terms mentioned in the document.
One way to iden-tify those key terms is to use the tf.idf measure.
Oth-ers use contextual information that is local to the named entity in question such as the words that precede and/or succeed the named entity or other named entities mentioned closely to the one in ques-tion.
The re-scoring methods described above assume that the correct translation is in the candidates list.
"When it is not in the list, the re-scoring will fail."
"To ad-dress this situation, we need to extrapolate from the candidate list."
We do this by searching for the cor-rect translation rather than generating it.
We do that by using sub-phrases from the candidates list or by searching for documents in the target lan-guage similar to the one being translated.
"For ex-ample, for a person name, instead of searching for the full name, we search for the first name and the last name separately."
"Then, we use the IdentiFinder named entity identifier[REF_CITE]to iden-tify all named entities in the top retrieved docu-ments for each sub-phrase."
"All named entities of the type of the named entity in question (e.g., PER-SON) found in the retrieved documents and that con-tain the sub-phrase used in the search are scored us-ing our transliteration module and added to the list of translation candidates, and the re-scoring is re-peated."
"To illustrate this method, consider the name “ - ! nān 4 &lt; 0 M kwfy.”"
"Our translation module proposes: Coffee Annan, Coffee Engen, Coffee Anton, Coffee Anyone, and Covey Annan but not the correct trans-lation Kofi Annan."
"We would like to find the most common person names that have either one of Coffee or Covey as a first name; or Annan, Engen, Anton, or Anyone as a last name."
One way to do this is to search using wild cards.
"Since we are not aware of any search engine that allows wild-card Web search, we can perform a wild-card search instead over our news corpus."
"The problem is that our news corpus is dated material, and it might not contain the infor-mation we are interested in."
"In this case, our news corpus, for example, might predate the appointment of Kofi Annan as the Secretary General of the UN."
"Alternatively, using a search engine, we retrieve the top matching documents for each of the names Coffee, Covey, Annan, Engen, Anton, and Anyone."
All person names found in the retrieved documents that contain any of the first or last names we used in the search are added to the list of translation candi-dates.
We hope that the correct translation is among the names found in the retrieved documents.
The re-scoring procedure is applied once more on the ex-panded candidates list.
"In this example, we add Kofi Annan to the candidate list, and it is subsequently ranked at the top."
"To address cases where neither the correct trans-lation nor any of its sub-phrases can be found in the list of translation candidates, we attempt to search for, instead of generating, translation candidates."
This can be done by searching for a document in the target language that is similar to the one being translated from the source language.
This is es-pecially useful when translating named entities in news stories of international importance where the same event will most likely be reported in many lan-guages including the target language.
We currently do this by repeating the extrapolation procedure de-scribed above but this time using contextual infor-mation such as the title of the original document to find similar documents in the target language.
"Ide-ally, one would use a Cross-Lingual IR system to find relevant documents more successfully."
This section presents our evaluation results on the named entity translation task.
"We compare the trans-lation results obtained from human translations, a commercial MT system, and our named entity trans-lation system."
"The evaluation corpus consists of two different test sets, a development test set and a blind test set."
The first set consists of 21 Arabic newspaper articles taken from the political affairs section of the daily newspaper Al-Riyadh.
Named entity phrases in these articles were hand-tagged ac-cording to the MUC[REF_CITE]guidelines.
They were then translated to English by a bilingual speaker (a native speaker of Arabic) given the text they appear in.
The Arabic phrases were then paired with their English translations.
The blind test set consists of 20 Arabic newspaper articles that were selected from the political section of the Arabic daily Al-Hayat.
"The articles have al-ready been translated into English by professional translators. [Footnote_3] Named entity phrases in these articles were hand-tagged, extracted, and paired with their English translations to create the blind test set."
3 The Arabic articles along with their English translations were part of the[REF_CITE]Multilingual corpus.
"Table 1 shows the distribution of the named entity phrases into the three categories PERSON, ORGA-NIZATION , and LOCATION in the two data sets."
The English translations in the two data sets were reviewed thoroughly to correct any wrong transla-tions made by the original translators.
"For example, to find the correct translation of a politician’s name, official government web pages were used to find the correct spelling."
"In cases where the translation could not be verified, the original translation provided by the human translator was considered the “correct“ translation."
The Arabic phrases and their correct translations constitute the gold-standard translation for the two test sets.
"According to our evaluation criteria, only transla-tions that match the gold-standard are considered as correct."
"In some cases, this criterion is too rigid, as it will consider perfectly acceptable translations as incorrect."
"However, since we use it mainly to com-pare our results with those obtained from the human translations and the commercial system, this crite-rion is sufficient."
The actual accuracy figures might be slightly higher than what we report here.
"In order to evaluate human performance at this task, we compared the translations by the original human translators with the correct translations on the gold-standard."
"The errors made by the original human translators turned out to be numerous, ranging from simple spelling errors (e.g., Custa Rica vs. Costa Rica) to more serious errors such as transliteration errors (e.g., John Keele vs. Jon Kyl) and other trans-lation errors (e.g., Union Reserve Council vs. Fed-eral Reserve Board)."
The Arabic documents were also translated us-ing a commercial Arabic-to-English translation sys-tem. [Footnote_4] The translation of the named entity phrases are then manually extracted from the translated text.
4 We used Sakhr’s Web-based translation system available[URL_CITE]
"When compared with the gold-standard, nearly half of the phrases in the development test set and more than a third of the blind test were translated incor-rectly by the commercial system."
"The errors can be classified into several categories including: poor transliterations (e.g., Koln Baol vs. Colin Pow-ell), translating a name instead of sounding it out (e.g., O’Neill’s urine vs. Paul O’Neill), wrong translation (e.g., Joint Corners Organization vs. Joint Chiefs of Staff) or wrong word order (e.g.,the Church of the Orthodox Roman)."
"Table 2 shows a detailed comparison of the trans-lation accuracy between our system, the commercial system, and the human translators."
The translations obtained by our system show significant improve-ment over the commercial system.
"In fact, in some cases it outperforms the human translator."
"When we consider the top-20 translations, our system’s overall accuracy (84%) is higher than the human’s (75.3%) on the blind test set."
This means that there is a lot of room for improvement once we consider more effec-tive re-scoring methods.
"Also, the top-20 list in itself is often useful in providing phrasal translation can-didates for general purpose statistical machine trans-lation systems or other NLP systems."
"The strength of our translation system is in trans-lating person names, which indicates the strength of our transliteration module."
This might also be attributed to the low named entity coverage of our bilingual dictionary.
"In some cases, some words that need to be translated (as opposed to transliter-ated) are not found in our bilingual dictionary which may lead to incorrect location or organization trans-lations but does not affect person names."
The rea-son word translations are sometimes not found in the dictionary is not necessarily because of the spotty coverage of the dictionary but because of the way we access definitions in the dictionary.
"Only shal-low morphological analysis (e.g., removing prefixes and suffixes) is done before accessing the dictionary, whereas a full morphological analysis is necessary, especially for morphologically rich languages such as Arabic."
"Another reason for doing poorly on or-ganizations is that acronyms and abbreviations in the Arabic text (e.g., “ + wās,” the Saudi Press Agency) are currently not handled by our system."
The blind test set was selected from the[REF_CITE]Multilingual Corpus.
The FBIS data is col-lected by the Foreign Broadcast Information Service for the benefit of the US government.
We suspect that the human translators who translated the docu-ments into English are somewhat familiar with the genre of the articles and hence the named entities
Table 3: This table shows the accuracy after each translation module.
The modules are applied incremen-tally.
Straight Web Counts re-score candidates based on their Web counts.
Contextual Web Counts uses Web counts within a given context (we used here title of the document as the contextual information).
"In Co-reference, if the phrase to be translated is part of a longer phrase then we use the the ranking of the candidates for the longer phrase to re-rank the candidates of the short one, otherwise we leave the list as is. that appear in the text."
"On the other hand, the devel-opment test set was randomly selected by us from our pool of Arabic articles and then submitted to the human translator."
"Therefore, the human translations in the blind set are generally more accurate than the human translations in the development test."
Another reason might be the fact that the human translator who translated the development test is not a profes-sional translator.
The only exception to this trend is organizations.
"After reviewing the translations, we discovered that many of the organization translations provided by the human translator in the blind test set that were judged incorrect were acronyms or abbreviations for the full name of the organization (e.g., the INC in-stead of the Iraqi National Congress)."
"As we described earlier in this paper, our transla-tion system first generates a list of translation can-didates, then re-scores them using several re-scoring methods."
The list of translation candidates we used for these experiments are of size 20.
The re-scoring methods are applied incrementally where the re-ranked list of one module is the input to the next module.
Table 3 shows the translation accuracy af-ter each of the methods we evaluated.
"The most effective re-scoring method was the simplest, the straight Web counts."
"This is because re-scoring methods are applied incrementally and straight Web counts was the first to be applied, and so it helps to resolve the “easy” cases, whereas the other methods are left with the more “difficult” cases."
It would be interesting to see how rearrang-ing the order in which the modules are applied might affect the overall accuracy of the system.
"The re-scoring methods we used so far are in gen-eral most effective when applied to person name translation because corpus phrase counts are already being used by the candidate generator for produc-ing candidates for locations and organizations, but not for persons."
"Also, the re-scoring methods we used were initially developed and applied to per-son names."
More effective re-scoring methods are clearly needed especially for organization names.
One method is to count phrases only if they are tagged by a named entity identifier with the same tag we are interested in.
This way we can elimi- when translating “nate counting wrong translations W h such as enthusiasm. mās” (Hamas).
We have presented a named entity translation algo-rithm that performs at near human translation ac-curacy when translating Arabic named entities to English.
The algorithm uses very limited amount of hard-to-obtain bilingual resources and should be easily adaptable to other languages.
We would like to apply to other languages such as Chinese and Japanese and to investigate whether the current al-gorithm would perform as well or whether new al-gorithms might be needed.
"Currently, our translation algorithm does not use any dictionary of named entities and they are trans-lated on the fly."
Translating a common name incor-rectly has a significant effect on the translation ac-curacy.
We would like to experiment with adding a small named entity translation dictionary for com-mon names and see if this might improve the overall translation accuracy.
"We describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment, and evaluate the improved dictionary using a version of the Competitive Linking algorithm."
We demonstrate a problem faced by the Com-petitive Linking algorithm and present an approach to ameliorate it.
"In particular, we rebuild the bilingual dictionary by cluster-ing similar words in a language and as-signing them a higher cooccurrence score with a given word in the other language than each single word would have other-wise."
Experimental results show a signifi-cant improvement in precision and recall for word alignment when the improved dicitonary is used.
Word alignment is a well-studied problem in Natu-ral Language Computing.
"This is hardly surprising given its significance in many applications: word-aligned data is crucial for example-based machine translation, statistical machine translation, but also other applications such as cross-lingual information retrieval."
"Since it is a hard and time-consuming task to hand-align bilingual data, the automation of this task receives a fair amount of attention."
"In this pa-per, we present an approach to improve the bilin-gual dictionary that is used by word alignment al-gorithms."
"Our method is based on similarity scores between words, which in effect results in the clus-tering of morphological variants."
One line of related work is research in clustering based on word similarities.
This problem is an area of active research in the Information Retrieval com-munity.
"For instance,[REF_CITE]present an algorithm that first clusters what are assumedly variants of the same word, then further refines the clusters using a cooccurrence related measure."
Word variants are found via a stemmer or by clustering all words that begin with the same three letters.
An-other technique uses similarity scores based on N-grams (e.g.[REF_CITE]).
The similarity of two words is measured using the number of N-grams that their occurrences have in common.
"As in our ap-proach, similar words are then clustered into equiv-alence classes."
"Other related work falls in the category of word alignment, where much research has been done."
A number of algorithms have been proposed and eval-uated for the task.
A source language word  and a target language word are said to cooccur if  occurs in a source language sen-tence and occurs in the corresponding target lan-guage sentence.
"Cooccurrence scores then are then counts for all word pairs  and  , where  is in the source language vocabulary and is in the tar-get language vocabulary."
"Often, the scores also take into account the marginal probabilites of each word and sometimes also the conditional probabilities of one word given the other."
"Aside from the classic statistical approach[REF_CITE], a number of other algorithms have been developed."
This infor-mation serves to build equivalence classes of words based on suffices.
A different approach was pro-posed[REF_CITE].
This approach models word alignments as flow networks.
"Determining the word alignments then amounts to solving the net-work, for which there are known algorithms."
"From these anchors, alignments are ex-panded in both directions, so that entire segments can be aligned."
The algorithm that this work was based on is the Competitive Linking algorithm.
We used it to test our improved dictionary.
Competitive Linking was described by Melamed (1997; 1998; 2000).
"It com-putes all possible word alignments in parallel data, and ranks them by their cooccurrence or by a similar score."
Then links between words (i.e. alignments) are chosen from the top of the list until no more links can be assigned.
There is a limit on the number of links a word can have.
In its basic form the Compet-itive Linking algorithm[REF_CITE]allows for only up to one link per word.
"However, this one-to-one/zero-to-one assumption is relaxed by redefining the notion of a word."
We implemented the basic Competitive Linking al-gorithm as described above.
"For each pair of paral-lel sentences, we construct a ranked list of possible links: each word in the source language is paired with each word in the target language."
"Then for each word pair the score is looked up in the dictio-nary, and the pairs are ranked from highest to lowest score."
"If a word pair does not appear in the dictio-nary, it is not ranked."
"The algorithm then recursively links the word pair with the highest cooccurrence, then the next one, etc."
"In our implementation, link-ing is performed on a sentence basis, i.e. the list of possible links is constructed only for one sentence pair at a time."
"Our version allows for more than one link per word, i.e. we do not assume one-to-one or zero-to-one alignments between words."
"Furthermore, our implementation contains a threshold that specifies how high the cooccurrence score must be for the two words in order for this pair to be considered for a link."
"In our experiments, we used a baseline dictionary, rebuilt the dictionary with our approach, and com-pared the performance of the alignment algorithm between the baseline and the rebuilt dictionary."
The dictionary that was used as a baseline and as a ba-sis for rebuilding is derived from bilingual sentence-aligned text using a count-and-filter algorithm:
"Count: for each source word type, count the number of times each target word type cooc-curs in the same sentence pair, as well as the total number of occurrences of each source and target type."
"Filter: after counting all cooccurrences, re-tain only those word pairs whose cooccurrence probability is above a  defined  threshold."
"To beretained, a word pair , must satisfy      &quot;!$&amp;# % ! $(  where ) *+ is the number of times the two words cooccurred."
"By making the threshold vary with frequency, one can control the tendency for infrequent words to be included in the dictionary as a result of chance col-locations."
"In our experiments, we varied the threshold from 0.005 to 0.01 and 0.02."
"It should be noted that there are many possible algorithms that could be used to derive the baseline dictionary, e.g. ., - , pointwise mutual information, etc."
An overview of such approaches can be found[REF_CITE].
"In our work, we preferred to use the above-described method, because it this method is utilized in the example-based MT system being developed in our gro[REF_CITE]."
It has proven useful in this context.
"As the scores in the dictionary are based on surface form words, statistical alignment algorithms such as Competitive Linking face the problem of inflected and derived terms."
"For instance, the English word liberty can be translated into French as a noun (lib-erté), or else as an adjective (libre), the same adjec-tive in the plural (libres), etc."
"This happens quite fre-quently, as sentences are often restructured in trans-lation."
"In such a case, liberté, libre, libres, and all the other translations of liberty in a sense share their cooccurrence scores with liberty."
"This can cause problems especially because there are words that are overall frequent in one language (here, French), and that receive a high cooccurrence count regardless of the word in the other language (here, English)."
"If the cooccurrence score between liberty and an un-related but frequent word is higher than libres, then the algorithm will prefer a link between liberty and le over a link between liberty and libres, even if the latter is correct."
"As for a concrete example from the training data used in this study, consider the English word oil."
This word is quite frequent in the training data and thus cooccurs at high counts with many target lan-guage words [Footnote_1] .
"1 We used Hansards data, see the evaluation section for de-tails."
"In this case, the target language is French."
The cooccurrence dictionary contains the following entries for oil among other entries: oil - et 543 oil - dans 118  oil - pétrole 259 oil - pétrolière 61 oil - pétrolières 61
"It can be seen that words such as et and dans re-ceive higher coccurrence scores with oil than some correct translations of oil, such as pétrolière, and pétrolières, and, in the case of et, also pétrole."
This will cause the Competitive Linking algorithm to fa-vor a link e.g. between oil and et over a link between oil and pétrole.
"In particular, word variations can be due to in-flectional morphology (e.g. adjective endings) and derivational morphology (e.g. a noun being trans- lated as an adjective due to sentence restructuring)."
"Both inflectional and derivational morphology will result in words that are similar, but not identical, so that cooccurrence counts will score them separately."
Below we describe an approach that addresses these two problems.
"In principle, we cluster similar words and assign them a new dictionary score that is higher than the scores of the individual words."
"In this way, the dictionary is rebuilt."
This will influence the ranked list that is produced by the algorithm and thus the final alignments.
Rebuilding the dictionary is based largely on sim-ilarities between words.
We have implemented an algorithm that assigns a similarity score to a pair of words  .
"The score is higher for a pair of sim-ilar words, while it favors neither shorter nor longer words."
"The algorithm finds the number of match-ing characters between the words, while allowing for insertions, deletions, and substitutions."
"The con-cept is thus very closely related to the Edit distance, with the difference that our algorithm counts the matching characters rather than the non-matching ones."
The length of the matching substring (which is not necessarily continguous) is denoted by Match-StringLength).
"At each step, a character from is compared to a character from ."
"If the characters are identical, the count for the MatchStringLength is incremented."
Then the algorithm checks for redupli-cation of the character in one or both of the words.
Reduplication also results in an incremented Match-StringLength.
"If the characters do not match, the al-gorithm skips one or more characters in either word."
Then the longest common substring is put in re-lation to the length of the two words.
This is done so as to not favor longer words that would result in a higher MatchStringLength than shorter words.
The similarity score of and is then computed using  &apos ; *&apos;    the following formula:    &quot ;!     #%$
This similarity scoring provides the basis for our newly built dictionary.
"The algorithm proceeds as follows: For any given source language word , there are target language words &apos;&amp;  )( such that the cooccurrence score &apos;* ,+ ,+ * , is greater than 0."
"Note that in most cases is much smaller than the size of the target language vocabulary, but also much greater than ."
"For the words &apos;&amp;  )( , the algo-rithm computes the similarity  score for each word  , where  pair ."
Note that this computation is potentially very complex.
The number of word pairs grows exponentially as grows.
"This problem is addressed by excluding word pairs whose cooccurrence scores are low, as will be discussed in more detail later."
"In the following, we use a greedy bottom-up clus-tering algorithm[REF_CITE]to cluster those words that have high similarity scores."
"The clustering algorithm is initialized to clus-ters, where each cluster contains exactly one of the words &amp;  )( ."
"In the first step, the algorithm clus-ters the pair of words with the maximum similar-scoreity score.  "
"The new cluster  , whichalso storesin thisacasesimilarityis the similarity score of the two clustered words."
"In the following steps, the algorithm again merges those two clusters  that  have."
Thetheclusteringhighest cansimilarityoccur inscoreone of three ways: 1.
Merge two clusters that each contain  one wordof the.
Then the similarity score merged cluster will be the similarity score of the word pair. 2. Merge a cluster * that contains a single word and a cluster  * that contains &quot;! % ! $# words &apos;&amp;   and has .
"Then the sim-ilarity score of the merged cluster is the aver-age similarity score of the -word cluster, av-eraged with the similarity scores between the single word and all words in the cluster."
"This means that the algorithm computes the similar-ity score between the single word in cluster averages them with * and each of the words  in * cluster: * , and &amp;(% *&apos; # ) #  +-, &apos; /. &apos; # 6 10 $ 1,/, &apos; 0  +3254 ) ,  # 0&amp;0 ! , &apos; 0 %! 3."
Merge two clusters that each contain more than a single word.
"In this case, the algo-rithm proceeds as in the second case, but av-erages the added similarity score over all word * with 7 pairs."
Suppose there exists  a * cluster  words &amp;   and and a cluster * with  words  %&amp;  !   % $! # and * .
"Then is computed as follows: &amp;%;: ) % * # &apos; ) #  -+ , &apos; /. &apos; # 0 &lt; 6@? = 1 $ 1,/, &apos; 0  3254+ ) ,  # 0&amp;0 1 ! / , , : 0  3254+ ) &lt; $=&gt;0 ! , &apos; 0 1 ! , : 0 %!"
"Clustering proceeds until a threshold,  , is exhausted."
"If none of the possible merges would re-sult in  a new cluster  wouldwhosebeaverageat leastsimilarity  ,scoreclus-tering stops."
"Then the dictionary entries are mod-ified as follows: suppose that words  are clustered, where all words  cooccur with source language word ."
"Furthermore, denote the cooccurrence score of the word pair and  by  ."
"Then A /B/C E# DGF  , A &amp;B/C # 0 &apos;+,+,** in the rebuilt dictionary the en-try will be A //B C #ED % +"
"I 6 F  , /&amp;B C 0 C EJ# C  &amp;C + replaced with A if"
Not all words are considered for clustering.
"First, we compiled a stop list of target language words that are never clustered, regardless of their similarity and cooccurrence scores with other words."
The words on the stop list are the 20 most frequent words in the target language training data.
Section M argues why this exclusion makes sense: one of the goals of clustering is to enable variations of a word to receive a higher dictionary score than words that are very common overall.
"Furthermore, we have decided to exclude words from clustering that account for only few of the cooccurrences of  ."
"In particular, a separate thresh-old, *&apos;+,,+ * ON + , controls how high the cooccurrence score with  has to be in relation to all other scores between  and a target language word. &apos;* ,+ ,+ * ON + is expressed as follows: a word qualifies for clus- % 6 Q!P P !! ! % # ( % 6 TS ( tering if * ,+ ,+ * ON + &apos;"
"As before, %&amp;  )( are all the target language words that cooccur with source language word ."
"Similarly to the most frequent words, dictionary scores for word pairs that are too rare for clustering remain unchanged."
"This exclusion makes sense because words that cooccur infrequently are likely not translations of each other, so it is undesirable to boost their score by clustering."
"Furthermore, this threshold helps keep the complexity of the operation under control."
"The fewer words qualify for clustering, the fewer simi-larity scores for pairs of words have to be computed."
"We trained three basic dictionaries using part of the Hansard data, around five megabytes of data (around 20k sentence pairs and 850k words)."
"The basic dic-tionaries were built using the algorithm described in section 3, with three different thresholds: 0.005, 0.01, and 0.02."
"In the following, we will refer to these dictionaries as as Dict0.005, Dict0.01, and Dict0.02. 50 sentences were held back for testing."
These sentences were hand-aligned by a fluent speaker of French.
No one-to-one assumption was enforced.
"A word could thus align to zero or more words, where no upper limit was enforced (although there is a nat-ural upper limit)."
The Competitive Linking algorithm was then run with multiple parameter settings.
"In one setting, we varied the N L7 maximum."
"Fornumberexampleof,linksif theallowedmaximumper word, number is 2, then a word can align to 0, 1, or 2 words in the parallel sentence."
"In other settings, we en-forced a minimum score  in the &apos;*  + bilingual."
"This meansdictionarythat for a link to be accepted, two  words &apos;*  + .cannotIn the rebuiltbe aligneddictionariesif their score,  is &apos;* below  + is applied in the same way."
The dictionary was also rebuilt using a number of different parameter settings.
"The two parameters that can be varied when rebuilding  the dictionary currence threshold &apos;* ,+ +,* ON + .  and theenforcescooc-are the similarity threshold that all words within one cluster  must have."
"Theansec-av-ond threshold, &apos;* +,,+ * ON + , enforces that only certain erage similarity score of at least words are considered for clustering."
"Those words for more than O  *&apos;+,,+ * ON + that are considered for clustering should account of the cooccur-rences of the source language word with any tar- * ,+ ,+ * ON + , its entry in the dictionary remains un-get language word."
"If a word falls below threshold &apos; changed, and it is not clustered with any other word."
Below we summarize the values each parameter was set to. maxlinks Used in Competitive Linking algo-rithm:
Maximum number of words any word can be aligned with.
"Set to: 1, 2, 3. minscore Used in Competitive Linking algo-rithm: Minimum score of a word pair in the dictionary to be considered as a possible link."
"Set to: 1, 2, 4, 6, 8, 10, 20, 30, 40, 50. minsim Used in rebuilding dictionary: Mini-mum average similarity score of the words in a cluster."
"Set to: 0.6, 0.7, 0.8. coocsratio Used in rebuilding dictionary:"
"O &apos;+,+, * ON + is the minimum percentage of all * cooccurrences of a source language word with any target language word that are accounted for by one target language word."
Set to: 0.003.
"Thus varying the parameters, we have constructed various dictionaries by rebuilding the three baseline dictionaries."
"Here, we report on results on three dic-tionaries where minsim was set to 0.7 and coocsra-tio was set to 0.003."
"For these parameter settings, we observed robust results, although other parame-ter settings also yielded positive results."
Precision and recall was measured using the hand-aligned 50 sentences.
Precision was defined as the percentage of links that were correctly pro-posed by our algorithm out of all links that were proposed.
Recall is defined as the percentage of links that were found by our algorithm out of all links that should have been found.
"In both cases, the hand-aligned data was used as a gold standard."
The F-measure - @@!
P  ( ( @ ! 8 88 8 combines !
P @ ! precision and recall: -  N  .
The following figures and tables illustrate that the Competitive Linking algorithm performs favorably when a rebuilt dictionary is used.
Table 1 lists the improvement in precision and recall for each of the dictionaries.
"The table shows the values when the minscore score is set to 50, and up to 1 link was allowed per word."
"Furthermore, the p-values of a 1-tailed t-test are listed, indicating these performance boosts are in mostly highly statistically significant for these parameter settings, where some of the best results were observed."
The following figures (figures 1-9) serve to illus-trate the impact of the algorithm in greater detail.
"All figures plot the precision, recall, and f-measure per-formance against different minscore settings, com-paring rebuilt dictionaries to their baselines."
"For each dictionary, three plots are given, one for each maxlinks setting, i.e. the maximum number of links allowed per word."
"The curve names indicate the type of the curve (Precision, Recall, or F-measure), the maximum number of links allowed per word (1, 2, or 3), the dictionary used (Dict0.005, Dict0.01, or Dict0.02), and whether the run used the base-line dictionary or the rebuilt dictionary (Baseline or Cog7.3)."
It can be seen that our algorithm leads to sta-ble improvement across parameter settings.
"In few cases, it drops below the baseline when minscore is low."
"Overall, however, our algorithm is robust - it improves alignment regardless of how many links are allowed per word, what baseline dictionary is used, and boosts both precision and recall, and thus also the f-measure."
"To return briefly to the example cited in section , we can now show how the dictionary rebuild has affected these entries."
In dictionary they now look as follows: oil - et 262 oil - dans 118  oil - pétrole 434 oil - pétrolière 434 oil - pétrolières 434
"The fact that pétrole, pétrolière, and pétrolières now receive higher scores than et and dans is what causes the alignment performance to increase."
We have demonstrated how rebuilding a dictionary can improve the performance (both precision and re-call) of a word alignment algorithm.
The algorithm proved robust across baseline dictionaries and vari-ous different parameter settings.
"Although a small test set was used, the improvements are statistically significant for various parameter settings."
We have shown that computing similarity scores of pairs of words can be used to cluster morphological variants of words in an inflected language such as French.
"It will be interesting to see how the similarity and clustering method will work in conjunction with other word alignment algorithms, as the dictionary rebuilding algorithm is independent of the actual word alignment method used."
"Furthermore, we plan to explore ways to improve the similarity scoring algorithm."
"For instance, we can assign lower match scores when the characters are not identical, but members of the same equiva-lence class."
The equivalence classes will depend on the target language at hand.
"For instance, in Ger-man, a and ä will be assigned to the same equiva-lence class, because some inflections cause a to be-come ä. An improved similarity scoring algorithm may in turn result in improved word alignments."
"In general, we hope to move automated dictio-nary extraction away from pure surface form statis-tics and toward dictionaries that are more linguisti- cally motivated."
This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not rec-ommended (thumbs down).
The classifi-cation of a review is predicted by the average semantic orientation of the phrases in the review that contain adjec-tives or adverbs.
"A phrase has a positive semantic orientation when it has good as-sociations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”)."
"In this paper, the semantic orientation of a phrase is calculated as the mutual infor-mation between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”."
A review is classified as recommended if the average semantic ori-entation of its phrases is positive.
"The al-gorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations)."
The ac-curacy ranges from 84% for automobile reviews to 66% for movie reviews.
"If you are considering a vacation in Akumal, Mex-ico, you might go to a search engine and enter the query “Akumal travel review”."
"However, in this case, Google 1 reports about 5,000 matches."
It would be useful to know what fraction of these matches recommend Akumal as a travel destina-tion.
"With an algorithm for automatically classify-ing a review as “thumbs up” or “thumbs down”, it would be possible for a search engine to report such summary statistics."
This is the motivation for the research described here.
Other potential appli-cations include recognizing “flames” (abusive newsgroup messages)[REF_CITE]and develop-ing new kinds of search tools[REF_CITE].
"In this paper, I present a simple unsupervised learning algorithm for classifying a review as rec-ommended or not recommended."
The algorithm takes a written review as input and produces a classification as output.
The first step is to use a part-of-speech tagger to identify phrases in the in-put text that contain adjectives or adverbs[REF_CITE].
The second step is to estimate the semantic orientation of each extracted phrase (Hatzivassi-loglou &amp;[REF_CITE]).
"A phrase has a posi-tive semantic orientation when it has good associations (e.g., “romantic ambience”) and a negative semantic orientation when it has bad as-sociations (e.g., “horrific events”)."
"The third step is to assign the given review to a class, recommended or not recommended, based on the average seman-tic orientation of the phrases extracted from the re-view."
"If the average is positive, the prediction is that the review recommends the item it discusses."
"Otherwise, the prediction is that the item is not recommended."
The PMI-IR algorithm is employed to estimate the semantic orientation of a phrase[REF_CITE].
PMI-IR uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words or phrases.
The se- 1[URL_CITE]mantic orientation of a given phrase is calculated by comparing its similarity to a positive reference word (“excellent”) with its similarity to a negative reference word (“poor”).
"More specifically, a phrase is assigned a numerical rating by taking the mutual information between the given phrase and the word “excellent” and subtracting the mutual information between the given phrase and the word “poor”."
"In addition to determining the direction of the phrase’s semantic orientation (positive or nega-tive, based on the sign of the rating), this numerical rating also indicates the strength of the semantic orientation (based on the magnitude of the num-ber)."
The algorithm is presented in Section 2.
"Their algorithm performs well, but it is designed for isolated adjectives, rather than phrases containing adjectives or adverbs."
"This is discussed in more detail in Section 3, along with other related work."
"The classification algorithm is evaluated on 410 reviews from Epinions [URL_CITE] , randomly sampled from four different domains: reviews of automobiles, banks, movies, and travel destinations."
Reviews at Epinions are not written by professional writers; any person with a Web browser can become a member of Epinions and contribute a review.
Each of these 410 reviews was written by a different au-thor.
"Of these reviews, 170 are not recommended and the remaining 240 are recommended (these classifications are given by the authors)."
Always guessing the majority class would yield an accu-racy of 59%.
"The algorithm achieves an average accuracy of 74%, ranging from 84% for automo-bile reviews to 66% for movie reviews."
The ex-perimental results are given in Section 4.
"The interpretation of the experimental results, the limitations of this work, and future work are discussed in Section 5."
Potential applications are outlined in Section 6.
"Finally, conclusions are pre-sented in Section 7."
The first step of the algorithm is to extract phrases containing adjectives or adverbs.
"Past work has demonstrated that adjectives are good indicators of subjective, evaluative sentences (Hatzivassiloglou &amp;[REF_CITE])."
"However, although an isolated adjective may indi-cate subjectivity, there may be insufficient context to determine semantic orientation."
"For example, the adjective “unpredictable” may have a negative orientation in an automotive review, in a phrase such as “unpredictable steering”, but it could have a positive orientation in a movie review, in a phrase such as “unpredictable plot”."
"Therefore the algorithm extracts two consecutive words, where one member of the pair is an adjective or an adverb and the second provides context."
First a part-of-speech tagger is applied to the review[REF_CITE]. [URL_CITE] Two consecutive words are extracted from the review if their tags conform to any of the patterns in Table 1.
"The JJ tags indicate adjectives, the NN tags are nouns, the RB tags are adverbs, and the VB tags are verbs. [Footnote_4] The second pattern, for example, means that two consecutive words are extracted if the first word is an adverb and the second word is an adjective, but the third word (which is not extracted) cannot be a noun."
4[REF_CITE]for a complete description of the tags.
"NNP and NNPS (singular and plural proper nouns) are avoided, so that the names of the items in the review cannot influence the classification."
"The second step is to estimate the semantic ori-entation of the extracted phrases, using the PMI-IR algorithm."
This algorithm uses mutual information as a measure of the strength of semantic associa-tion between two words (Church &amp;[REF_CITE]).
"PMI-IR has been empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL), obtaining a score of 74%[REF_CITE]."
"For comparison, Latent Se-mantic Analysis (LSA), another statistical measure of word association, attains a score of 64% on the same 80 TOEFL questions (Landauer &amp;[REF_CITE])."
"The Pointwise Mutual Information (PMI) be-tween two words, word 1 and word 2 , is defined as follows (Church &amp;[REF_CITE]): p(word 1 &amp; word 2 ) PMI(word 1 , word 2 ) = log 2 (1) p(word 1 ) p(word 2 )"
"Here, p(word 1 &amp; word 2 ) is the probability that word 1 and word 2 co-occur."
"If the words are statisti-cally independent, then the probability that they co-occur is given by the product p(word 1 ) p(word 2 )."
The ratio between p(word 1 &amp; word 2 ) and p(word 1 ) p(word 2 ) is thus a measure of the degree of statistical dependence between the words.
The log of this ratio is the amount of information that we acquire about the presence of one of the words when we observe the other.
"The Semantic Orientation (SO) of a phrase, phrase, is calculated here as follows:"
"PMI(phrase, “excellent”) (2) - PMI(phrase, “poor”)"
"The reference words “excellent” and “poor” were chosen because, in the five star review rating sys-tem, it is common to define one star as “poor” and five stars as “excellent”."
SO is positive when phrase is more strongly associated with “excellent” and negative when phrase is more strongly associ-ated with “poor”.
PMI-IR estimates PMI by issuing queries to a search engine (hence the IR in PMI-IR) and noting the number of hits (matching documents).
"The fol-lowing experiments use the AltaVista Advanced Search engine [URL_CITE] , which indexes approximately 350 million web pages (counting only those pages that are in English)."
I chose AltaVista because it has a NEAR operator.
"The AltaVista NEAR operator constrains the search to documents that contain the words within ten words of one another, in either order."
Previous work has shown that NEAR per-forms better than AND when measuring the strength of semantic association between words[REF_CITE].
"Let hits(query) be the number of hits returned, given the query query."
"The following estimate of SO can be derived from equations (1) and (2) with some minor algebraic manipulation, if co-occurrence is interpreted as NEAR:"
SO(phrase) = hits(phrase NEAR “excellent”) hits(“poor”) (3) log 2 hits(phrase
NEAR “poor”) hits(“excellent”)
Equation (3) is a log-odds ratio[REF_CITE].
"To avoid division by zero, I added 0.01 to the hits."
I also skipped phrase when both hits(phrase
NEAR “excellent”) and hits(phrase
NEAR “poor”) were (simultaneously) less than four.
"These numbers (0.01 and 4) were arbitrarily cho-sen. To eliminate any possible influence from the testing data, I added “AND (NOT host:epinions)” to every query, which tells AltaVista not to include the Epinions Web site in its searches."
The third step is to calculate the average seman-tic orientation of the phrases in the given review and classify the review as recommended if the av-erage is positive and otherwise not recommended.
Table 2 shows an example for a recommended review and Table 3 shows an example for a not recommended review.
Both are reviews of the Bank of America.
Both are in the collection of 410 reviews from Epinions that are used in the experi-ments in Section 4.
This work is most closely related to Hatzivassi-loglou and McKeown’s (1997) work on predicting the semantic orientation of adjectives.
They note that there are linguistic constraints on the semantic orientations of adjectives in conjunctions.
"As an example, they present the following three sen-tences (Hatzivassiloglou &amp;[REF_CITE]): 1."
The tax proposal was simple and well-received by the public. 2.
The tax proposal was simplistic but well-received by the public. 3. (*)
The tax proposal was simplistic and well-received by the public.
"The third sentence is incorrect, because we use “and” with adjectives that have the same semantic orientation (“simple” and “well-received” are both positive), but we use “but” with adjectives that have different semantic orientations (“simplistic” is negative)."
All conjunctions of adjectives are extracted from the given corpus. 2.
A supervised learning algorithm combines multiple sources of evidence to label pairs of adjectives as having the same semantic orienta-tion or different semantic orientations.
The re-sult is a graph where the nodes are adjectives and links indicate sameness or difference of semantic orientation. 3.
"A clustering algorithm processes the graph structure to produce two subsets of adjectives, such that links across the two subsets are mainly different-orientation links, and links in-side a subset are mainly same-orientation links. 4."
"Since it is known that positive adjectives tend to be used more frequently than negative adjectives, the cluster with the higher average frequency is classified as having positive se-mantic orientation."
"This algorithm classifies adjectives with accuracies ranging from 78% to 92%, depending on the amount of training data that is available."
"The algo-rithm can go beyond a binary positive-negative dis-tinction, because the clustering algorithm (step 3 above) can produce a “goodness-of-fit” measure that indicates how well an adjective fits in its as-signed cluster."
"Although they do not consider the task of clas-sifying reviews, it seems their algorithm could be plugged into the classification algorithm presented in Section 2, where it would replace PMI-IR and equation (3) in the second step."
"However, PMI-IR is conceptually simpler, easier to implement, and it can handle phrases and adverbs, in addition to iso-lated adjectives."
"As far as I know, the only prior published work on the task of classifying reviews as thumbs up or down is Tong’s (2001) system for generating sen-timent timelines."
This system tracks online discus-sions about movies and displays a plot of the number of positive sentiment and negative senti-ment messages over time.
"Messages are classified by looking for specific phrases that indicate the sentiment of the author towards the movie (e.g., “great acting”, “wonderful visuals”, “terrible score”, “uneven editing”)."
Each phrase must be manually added to a special lexicon and manually tagged as indicating positive or negative sentiment.
"The lexicon is specific to the domain (e.g., movies) and must be built anew for each new domain."
The company Mindfuleye [URL_CITE] offers a technology called Lexant™ that appears similar to Tong’s (2001) system.
Other related work is concerned with determin-ing subjectivity (Hatzivassiloglou &amp;[REF_CITE]).
The task is to distinguish sentences that present opinions and evaluations from sentences that objectively present factual informati[REF_CITE].
"In several of these applications, the first step is to recognize that the text is subjec-tive and then the natural second step is to deter-mine the semantic orientation of the subjective text."
"For example, a flame detector cannot merely detect that a newsgroup message is subjective, it must further detect that the message has a negative semantic orientation; otherwise a message of praise could be classified as a flame."
"The directionality of a docu-ment is determined by its deep argumentative structure, rather than a shallow analysis of its ad-jectives."
"Sentences are interpreted metaphorically in terms of agents exerting force, resisting force, and overcoming resistance."
It seems likely that there could be some benefit to combining shallow and deep analysis of the text.
Table 4 describes the 410 reviews from Epinions that were used in the experiments. 170 (41%) of the reviews are not recommended and the remain-ing 240 (59%) are recommended.
Always guessing the majority class would yield an accuracy of 59%.
The third column shows the average number of phrases that were extracted from the reviews.
Table 5 shows the experimental results.
"Except for the travel reviews, there is surprisingly little variation in the accuracy within a domain."
"In addi- tion to recommended and not recommended, Epin-ions reviews are classified using the five star rating system."
The third column shows the correlation be-tween the average semantic orientation and the number of stars assigned by the author of the re-view.
The results show a strong positive correla-tion between the average semantic orientation and the author’s rating out of five stars.
"A natural question, given the preceding results, is what makes movie reviews hard to classify?"
Table 6 shows that classification by the average
"SO tends to err on the side of guessing that a review is not recommended, when it is actually recommended."
"This suggests the hypothesis that a good movie will often contain unpleasant scenes (e.g., violence, death, mayhem), and a recommended movie re- view may thus have its average semantic orienta-tion reduced if it contains descriptions of these un-pleasant scenes."
"However, if we add a constant value to the average SO of the movie reviews, to compensate for this bias, the accuracy does not improve."
"This suggests that, just as positive re-views mention unpleasant things, so negative re-views often mention pleasant scenes."
Table 7 shows some examples that lend support to this hypothesis.
"For example, the phrase “more evil” does have negative connotations, thus an SO of -4.384 is appropriate, but an evil character does not make a bad movie."
"The difficulty with movie reviews is that there are two aspects to a movie, the events and actors in the movie (the elements of the movie), and the style and art of the movie (the movie as a gestalt; a unified whole)."
This is likely also the explanation for the lower accuracy of the Cancun reviews: good beaches do not necessarily add up to a good vacation.
"On the other hand, good automotive parts usually do add up to a good automobile and good banking services add up to a good bank."
It is not clear how to address this issue.
Future work might look at whether it is possible to tag sentences as discussing elements or wholes.
Another area for future work is to empirically compare PMI-IR and the algorithm[REF_CITE].
"Although their algo-rithm does not readily extend to two-word phrases, I have not yet demonstrated that two-word phrases are necessary for accurate classification of reviews."
"On the other hand, it would be interesting to evalu-ate PMI-IR on the collection of 1,336 hand-labeled adjectives that were used in the experiments[REF_CITE]."
A related question for future work is the relationship of ac-curacy of the estimation of semantic orientation at the level of individual phrases to accuracy of re-view classification.
"Since the review classification is based on an average, it might be quite resistant to noise in the SO estimate for individual phrases."
But it is possible that a better SO estimator could produce significantly better classifications.
Context of Sample Anyone who saw the trailer in
"Phrase: the theater over the course of the last year will never forget the images of Japanese war planes swooping out of the blue skies, flying past the children playing baseball, or the truly remarkable shot of a bomb falling from an enemy plane into the deck of the USS Arizona."
Equation (3) is a very simple estimator of se-mantic orientation.
It might benefit from more so-phisticated statistical analysis[REF_CITE].
One possibility is to apply a statistical significance test to each estimated SO.
"There is a large statistical literature on the log-odds ratio, which might lead to improved results on this task."
"This paper has focused on unsupervised classi-fication, but average semantic orientation could be supplemented by other features, in a supervised classification system."
"The other features could be based on the presence or absence of specific words, as is common in most text classification work."
"This could yield higher accuracies, but the intent here was to study this one feature in isola-tion, to simplify the analysis, before combining it with other features."
Table 5 shows a high correlation between the average semantic orientation and the star rating of a review.
"I plan to experiment with ordinal classi-fication of reviews in the five star rating system, using the algorithm[REF_CITE]."
"For ordinal classification, the average semantic orienta-tion would be supplemented with other features in a supervised classification system."
A limitation of PMI-IR is the time required to send queries to AltaVista.
Inspection of Equation (3) shows that it takes four queries to calculate the semantic orientation of a phrase.
"However, I cached all query results, and since there is no need to recalculate hits(“poor”) and hits(“excellent”) for every phrase, each phrase requires an average of slightly less than two queries."
"As a courtesy to AltaVista, I used a five second delay between que-ries. [Footnote_8][REF_CITE]reviews yielded 10,658 phrases, so the total time required to process the corpus was roughly 106,580 seconds, or about 30 hours."
"8 This line of research depends on the good will of the major search engines. For a discussion of the ethics of Web robots,[URL_CITE]For query robots, the proposed extended standard for robot exclusion would be useful.[URL_CITE]"
"This might appear to be a significant limitation, but extrapolation of current trends in computer memory capacity suggests that, in about ten years, the average desktop computer will be able to easily store and search AltaVista’s 350 million Web pages."
This will reduce the processing time to less than one second per review.
There are a variety of potential applications for automated review rating.
"As mentioned in the in- troduction, one application is to provide summary statistics for search engines."
"Given the query “Akumal travel review”, a search engine could re-port, “There are 5,000 hits, of which 80% are thumbs up and 20% are thumbs down.”"
"The search results could be sorted by average semantic orien-tation, so that the user could easily sample the most extreme reviews."
"Similarly, a search engine could allow the user to specify the topic and the rating of the desired reviews[REF_CITE]."
Preliminary experiments indicate that semantic orientation is also useful for summarization of re-views.
A positive review could be summarized by picking out the sentence with the highest positive semantic orientation and a negative review could be summarized by extracting the sentence with the lowest negative semantic orientation.
Epinions asks its reviewers to provide a short description of pros and cons for the reviewed item.
A pro/con summarizer could be evaluated by measuring the overlap between the reviewer’s pros and cons and the phrases in the review that have the most extreme semantic orientation.
Another potential application is filtering “flames” for newsgroups[REF_CITE].
"There could be a threshold, such that a newsgroup mes-sage is held for verification by the human modera-tor when the semantic orientation of a phrase drops below the threshold."
A related use might be a tool for helping academic referees when reviewing journal and conference papers.
"Ideally, referees are unbiased and objective, but sometimes their criti-cism can be unintentionally harsh."
"It might be pos-sible to highlight passages in a draft referee’s report, where the choice of words should be modi-fied towards a more neutral tone."
"Tong’s (2001) system for detecting and track-ing opinions in on-line discussions could benefit from the use of a learning algorithm, instead of (or in addition to) a hand-built lexicon."
"With auto-mated review rating (opinion rating), advertisers could track advertising campaigns, politicians could track public opinion, reporters could track public response to current events, stock traders could track financial opinions, and trend analyzers could track entertainment and technology trends."
This paper introduces a simple unsupervised learn-ing algorithm for rating a review as thumbs up or down.
"The algorithm has three steps: (1) extract phrases containing adjectives or adverbs, (2) esti-mate the semantic orientation of each phrase, and (3) classify the review based on the average se-mantic orientation of the phrases."
"The core of the algorithm is the second step, which uses PMI-IR to calculate semantic orientati[REF_CITE]."
"In experiments with 410 reviews from Epin-ions, the algorithm attains an average accuracy of 74%."
"It appears that movie reviews are difficult to classify, because the whole is not necessarily the sum of the parts; thus the accuracy on movie re-views is about 66%."
"On the other hand, for banks and automobiles, it seems that the whole is the sum of the parts, and the accuracy is 80% to 84%."
Travel reviews are an intermediate case.
Previous work on determining the semantic ori-entation of adjectives has used a complex algo-rithm that does not readily extend beyond isolated adjectives to adverbs or longer phrases[REF_CITE].
The simplicity of PMI-IR may encourage further work with semantic orientation.
"The limitations of this work include the time required for queries and, for some applications, the level of accuracy that was achieved."
The former difficulty will be eliminated by progress in hard-ware.
The latter difficulty might be addressed by using semantic orientation combined with other features in a supervised classification algorithm.
"Answer Validation is an emerging topic in Question Answering, where open do-main systems are often required to rank huge amounts of candidate answers."
We present a novel approach to answer valida-tion based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of Web information.
Experiments carried out on the[REF_CITE]judged-answer collec-tion show that the approach achieves a high level of performance (i.e. 81% suc-cess rate).
The simplicity and the effi-ciency of this approach make it suitable to be used as a module in Question Answer-ing systems.
Open domain question-answering (QA) systems search for answers to a natural language question either on the Web or in a local document collec-tion.
"Different techniques, varying from surface pat-terns[REF_CITE]to deep seman-tic analysis[REF_CITE], are used to extract the text fragments containing candidate answers."
Several systems apply answer validation techniques with the goal of filtering out improper candidates by check-ing how adequate a candidate answer is with re-spect to a given question.
These approaches rely on discovering semantic relations between the ques-tion and the answer.
"As an example,[REF_CITE]describes answer validation as an abductive inference process, where an answer is valid with respect to a question if an explanation for it, based on background knowledge, can be found."
"Although theoretically well motivated, the use of se-mantic techniques on open domain tasks is quite ex-pensive both in terms of the involved linguistic re-sources and in terms of computational complexity, thus motivating a research on alternative solutions to the problem."
This paper presents a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploit-ing the redundancy of Web information.
The hy-pothesis is that the number of documents that can be retrieved from the Web in which the question and the answer co-occur can be considered a significant clue of the validity of the answer.
"Documents are searched in the Web by means of validation pat-terns, which are derived from a linguistic process-ing of the question and the answer."
In order to test this idea a system for automatic answer validation has been implemented and a number of experiments have been carried out on questions and answers pro-vided by the[REF_CITE]participants.
The advan-tages of this approach are its simplicity on the one hand and its efficiency on the other.
Automatic techniques for answer validation are of great interest for the development of open do-main QA systems.
The availability of a completely automatic evaluation procedure makes it feasible QA systems based on generate and test approaches.
"In this way, until a given answer is automatically"
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 425-432. proved to be correct for a question, the system will carry out different refinements of its searching crite-ria checking the relevance of new candidate answers."
"In addition, given that most of the QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of the relevance of an answer with respect to a given question will speed up both algorithm refinement and testing."
The paper is organized as follows.
Section 2 presents the main features of the approach.
Section 3 describes how validation patterns are extracted from a question-answer pair by means of specific question answering techniques.
Section 4 explains the basic algorithm for estimating the answer validity score.
Section 5 gives the results of a number of experi-ments and discusses them.
"Finally, Section 6 puts our approach in the context of related works."
Given a question and a candidate answer the an-swer validation task is defined as the capability to as-sess the relevance of with respect to .
We assume open domain questions and that both answers and questions are texts composed of few tokens (usually less than 100).
"This is compatible with the[REF_CITE]data, that will be used as examples throughout this paper."
"We also assume the availability of the Web, considered to be the largest open domain text corpus containing information about almost all the different areas of the human knowledge."
"The intuition underlying our approach to an-swer validation is that, given a question-answer pair ([ , ]), it is possible to formulate a set of valida-tion statements whose truthfulness is equivalent to the degree of relevance of with respect to ."
"For instance, given the question “What is the capital of the USA?”, the problem of validating the answer “Washington” is equivalent to estimating the truth-fulness of the validation statement “The capital of the USA is Washington”."
"Therefore, the answer validation task could be reformulated as a problem of statement reliability."
There are two issues to be addressed in order to make this intuition effective.
"First, the idea of a validation statement is still insuf-ficient to catch the richness of implicit knowledge that may connect an answer to a question: we will attack this problem defining the more flexible idea of a validation pattern."
"Second, we have to design an effective and efficient way to check the reliability of a validation pattern: our solution relies on a pro-cedure based on a statistical count of Web searches."
Answers may occur in text passages with low similarity with respect to the question.
"Passages telling facts may use different syntactic construc-tions, sometimes are spread in more than one sen-tence, may reflect opinions and personal attitudes, and often use ellipsis and anaphora."
"For instance, if the validation statement is “The capital of USA is Washington”, we have Web documents containing passages like those reported in Table 1, which can not be found with a simple search of the statement, but that nevertheless contain a significant amount of knowledge about the relations between the question and the answer."
We will refer to these text fragments as validation fragments.
"A common feature in the above examples is the co-occurrence of a certain subset of words (i.e. “capital”,“USA” and “Washington”)."
"We will make use of validation patterns that cover a larger portion of text fragments, including those lexically similar to the question and the answer (e.g. fragments 4 and 5 in Table 1) and also those that are not similar (e.g. fragment 2 in Table 1)."
In the case of our example a set of validation statements can be generalized by the validation pattern: [capital text USA text Washington] where text is a place holder for any portion of text with a fixed maximal length.
To check the correctness of with respect to we propose a procedure that measures the number of occurrences on the Web of a validation pattern derived from and .
"A useful feature of such pat-terns is that when we search for them on the Web they usually produce many hits, thus making statis-tical approaches applicable."
"In contrast, searching for strict validation statements generally results in a small number of documents (if any) and makes sta-tistical methods irrelevant."
"A number of techniques used for finding collocations and co-occurrences of words, such as mutual information, may well be used to search co-occurrence tendency between the question and the candidate answer in the Web."
If we verify that such tendency is statistically significant we may consider the validation pattern as consistent and therefore we may assume a high level of correla-tion between the question and the candidate answer.
"Starting from the above considerations and given a question-answer pair   , we propose an answer validation procedure based on the following steps: 1. Compute the set of representative keywords and both from and from ; this step is carried out using linguistic techniques, such as answer type identification (from the question) and named entities recognition (from the an-swer); 2."
From the extracted keywords compute the vali-dation pattern for the pair [  ]; 3. Submit the patterns to the Web and estimate an answer validity score considering the number of retrieved documents.
In our approach a validation pattern consists of two components: a question sub-pattern (Qsp) and an answer sub-pattern (Asp).
Building the Qsp.
A Qsp is derived from the input question cutting off non-content words with a stop-words filter.
The remaining words are expanded with both synonyms and morphological forms in order to maximize the recall of retrieved docu-ments.
"Synonyms are automatically extracted from the most frequent sense of the word in WordNet[REF_CITE], which considerably reduces the risk of adding disturbing elements."
"As for morphol-ogy, verbs are expanded with all their tense forms (i.e. present, present continuous, past tense and past participle)."
Synonyms and morphological forms are added to the Qsp and composed in an OR clause.
The following example illustrates how the Qsp is constructed.
"Given the[REF_CITE]question “When did Elvis Presley die?”, the stop-words filter removes “When” and “did” from the input."
"Then synonyms of the first sense of “die” (i.e. “decease”, “perish”, etc.) are extracted from WordNet."
"Finally, morphological forms for all the corresponding verb tenses are added to the Qsp."
The resultant Qsp will be the following: [Elvis text Presley text (die OR died OR dying OR perish OR ...)]
Building the Asp.
An Asp is constructed in two steps.
"First, the answer type of the question is iden-tified considering both morpho-syntactic (a part of speech tagger is used to process the question) and semantic features (by means of semantic predicates defined on the WordNet taxonomy; see[REF_CITE]for details)."
"Possible answer types are: DATE , MEASURE , PERSON , LOCATION , ORGANI - ZATION , DEFINITION and GENERIC ."
DEFINITION is the answer type peculiar to questions like “What is an atom?” which represent a considerable part (around 25%) of the[REF_CITE]corpus.
The an-swer type GENERIC is used for non definition ques-tions asking for entities that can not be classified as named entities (e.g. the questions: “Material called linen is made from what plant?” or “What mineral helps prevent osteoporosis?”)
"In the second step, a rule-based named entities recognition module identifies in the answer string all the named entities matching the answer type cat-egory."
"If the category corresponds to a named en-tity, an Asp for each selected named entity is cre-ated."
"If the answer type category is either DEFINI - TION or GENERIC , the entire answer string except the stop-words is considered."
"In addition, in order to maximize the recall of retrieved documents, the Asp is expanded with verb tenses."
The following example shows how the Asp is created.
"Given the TREC question “When did Elvis Presley die?” and the candidate answer “though died in 1977 of course some fans maintain”, since the answer type category is DATE the named entities recognition module will select [1977] as an answer sub-pattern."
The answer validation algorithm queries the Web with the patterns created from the question and an-swer and after that estimates the consistency of the patterns.
We use a Web-mining algorithm that considers the number of pages retrieved by the search engine.
"In contrast, qualitative approaches to Web mining (e.g.[REF_CITE]) analyze the document content, as a result considering only a relatively small num-ber of pages."
For information retrieval we used the AltaVista search engine.
Its advanced syntax allows the use of operators that implement the idea of vali-dation patterns introduced in Section 2.
"Queries are composed using NEAR, OR and AND boolean opera-tors."
The NEAR operator searches pages where two words appear in a distance of no more than 10 to-kens: it is used to put together the question and the answer sub-patterns in a single validation pattern.
The OR operator introduces variations in the word order and verb forms.
"Finally, the AND operator is used as an alternative to NEAR, allowing more dis-tance among pattern elements."
If the question sub-pattern does not return any document or returns less than a certain thresh-old (experimentally set to 7) the question pattern is relaxed by cutting one word; in this way a new query is formulated and submitted to the search en-gine.
This is repeated until no more words can be cut or the returned number of documents becomes higher than the threshold.
Pattern relaxation is per-formed using word-ignoring rules in a specified or-der.
"Such rules, for instance, ignore the focus of the question, because it is unlikely that it occurs in a validation fragment; ignore adverbs and adjectives, because are less significant; ignore nouns belonging to the WordNet classes “abstraction”, “psychologi-cal feature” or “group”, because usually they specify finer details and human attitudes."
"Names, numbers and measures are preferred over all the lower-case words and are cut last."
"The Web-mining module submits three searches to the search engine: the sub-patterns [Qsp] and [Asp] and the validation pattern [QAp], this last built as the composition [Qsp NEAR Asp]."
"The search en-gine returns respectively:   ,   and  ! ."
"The probability &quot;# NEAR of a pattern in the Web is calculated by: &quot;   %$ &amp; !!(&quot;*) +, # where ! where is the &amp; number (&quot; ) of +, pagesis theinmaximumthe Webappears and number of pages that can be returned by the search engine."
We set this constant experimentally.
How-ever in two of the formulas we use (i.e. Point-wise Mutual &amp; Information (&quot; -) +. andmayCorrectedbe ignoredConditional.Probability)
"The joint probability P(Qsp,Asp) is calculated by means of the validation pattern probability: &quot; /%0$ &quot;## ("
"We have tested three alternative measures to es-timate the degree of relevance of Web searches: Pointwise Mutual Information, Maximal Likelihood Ratio and Corrected Conditional Probability, a vari-ant of Conditional Probability which considers the asymmetry of the question-answer relation."
Each measure provides an answer validity score: high val-ues are interpreted as strong evidence that the vali-dation pattern is consistent.
"This is a clue to the fact that the Web pages where this pattern appears con-tain validation fragments, which imply answer accu-racy."
Pointwise Mutual Information (PMI)[REF_CITE]has been widely used to find co-occurrence in large corpora. &quot; 65&amp;
"Qsp,Asp %$ #&quot; &quot; Qsp # Qsp  , &quot; Asp # Asp"
"PMI(Qsp,Asp) is used as a clue to the internal coherence of the question-answer validation pattern QAp."
"Substituting the probabilities in the PMI for-mula with the previously introduced Web statistics, we obtain:  Qsp  Asp 7 &amp; (&quot; -) +.  "
Maximal Likelihood Ratio (MLHR) is also used for word co-occurrence mining[REF_CITE].
"We decided to check MLHR for answer validation because it is supposed to outperform PMI in case of sparse data, a situation that may happen in case of questions with complex patterns that return small number of hits. 6 &amp; &lt;: ;&gt;=   ?"
"A$ @CB%DFEHGCI :   :   IJ$ :  ,M  :  .R , : where   $U@X[ M $ [VLY \\ , R $ .Y ]] #K M$ [YV^\\ _!_ ,Y ]] PQM $$!  , PaR$ , K !R $@   @"
Here ! @ ! is the number of appearances of Qsp when Asp is not present and it is calculated as  *(@    .
"Similarly, !@  is the number of Web pages &amp;  where (&quot; -) +."
"Asp @ does .not appear and it is calculatedas Corrected Conditional Probability (CCP) in contrast with PMI and MLHR, CCP is not symmetric (e.g. generally #&quot;  $f ded #&quot;  ! )."
This is based on the fact that we search for the occurrence of the answer pattern Asp only in the cases when Qsp is present.
"The sta-tistical evidence for this can be measured through #&quot;  ?h j ! , however this value is corrected with #&quot;   in the denominator, to avoid the cases when high-frequency words and patterns are taken as relevant answers.  $ &quot;#&quot;#?h j #&quot;"
For CCP we obtain: !  !
"R  7 &amp; (&quot; ) +,  ! !"
Consider an example taken from the question an-swer corpus of the main task[REF_CITE]: “Which river in US is known as Big Muddy?”.
"The question keywords are: “river”, “US”, “known”, “Big”, “Muddy”."
The search of the pattern [river NEAR US NEAR (known OR know OR...)
"NEAR Big NEAR Muddy] returns 0 pages, so the algorithm re-laxes the pattern by cutting the initial noun “river”, according to the heuristic for discarding a noun if it is the first keyword of the question."
The second pat-tern [US NEAR (known OR know OR...)
"Big NEAR Muddy] also returns 0 pages, so we apply the heuristic for ignoring verbs like “know”, “call” and abstract nouns like “name”."
"The third pattern [US NEAR Big NEAR Muddy] returns 28 pages, which is over the experimentally set threshold of seven pages."
One of the 50 byte candidate answers from the[REF_CITE]answer collection is “recover Missis-sippi River”.
"Taking into account the answer type LOCATION , the algorithm considers only the named entity: “Mississippi River”."
"To calculate answer validity score (in this example PMI) for [Missis-sippi River], the procedure constructs the validation pattern: [US NEAR Big NEAR Muddy NEAR Mis-sissippi River] with the answer sub-pattern [Missis-sippi River]."
"These two patterns are passed to the search engine, and the returned numbers of pages are substituted in the mutual information expression at the places of !   and   respectively; the previously obtained number (i.e. 28) is substituted at the place of  ! ."
In this way an answer validity score of 55.5 is calculated.
It turns out that this value is the maximal validity score for all the answers of this question.
Other cor-rect answers from the[REF_CITE]collection con-tain as name entity “Mississippi”.
"Their answer va-lidity score is 11.8, which &amp;  is greater s  than 1. &lt;w 2 xSy and **z + also greater than m-noBk7 {$ WHWHn|,W ."
This score (i.e. 11.8) classifies them as relevant answers.
"On the other hand, all the wrong answers has validity score below 1 and as a result all of them are classified as irrelevant answer candi-dates."
A number of experiments have been carried out in order to check the validity of the proposed answer validation technique.
"As a data set, the 492 ques-tions of the[REF_CITE]database have been used."
"For each question, at most three correct answers and three wrong answers have been randomly selected from the[REF_CITE]participants’ submissions, re-sulting in a corpus of 2726 question-answer pairs (some question have less than three positive answers in the corpus)."
"As said before, AltaVista was used as search engine."
A baseline for the answer validation experiment was defined by considering how often an answer oc-curs in the top 10 documents among those (1000 for each question) provided by NIST[REF_CITE]participants.
"An answer was judged correct for a question if it appears at least one time in the first 10 documents retrieved for that question, otherwise it was judged not correct."
Baseline results are re-ported in Table 2.
We carried out several experiments in order to check a number of working hypotheses.
Three in-dependent factors were considered: Estimation method.
"We have implemented three measures (reported in Section 4.2) to estimate an an-swer validity score: PMI, MLHR and CCP."
We wanted to estimate the role of two different kinds of thresholds for the assessment of answer validation.
"In the case of an absolute thresh-old, if the answer validity score for a candidate an-swer is below the threshold, the answer is considered wrong, otherwise it is accepted as relevant."
"In a sec-ond type of experiment, for every question and its corresponding answers the program chooses the an-swer with the highest validity score and calculates a relative threshold on that basis (i.e. *.+ ,*}$ K 7 &amp;  s  ,xSy**z + )."
However the relative threshold should be larger than a certain minimum value.
We wanted to check performance variation based on different types[REF_CITE]questions.
"In particular, we have separated defini-tion and generic questions from true named entities questions."
Tables 2 and 3 report the results of the automatic answer validation experiments obtained respectively on all the[REF_CITE]questions and on the subset of definition and generic questions.
"For each esti-mation method we report precision, recall and suc-cess rate."
"Success rate best represents the perfor-mance of the system, being the percent of [  ] pairs where the result given by the system is the same as the TREC judges’ opinion."
"Precision is the percent of   pairs estimated by the algorithm as rele-vant, for which the opinion of TREC judges was the same."
Recall shows the percent of the relevant an-swers which the system also evaluates as relevant.
"The best results on the 492 questions corpus (CCP measure with relative threshold) show a success rate of 81.25%, i.e. in 81.25% of the pairs the system evaluation corresponds to the human evaluation, and confirms the initial working hypotheses."
Precision and re-call are respectively 20-30% and 68-87% above the baseline values.
These results demonstrate that the intuition behind the approach is motivated and that the algorithm provides a workable solution for an-swer validation.
The experiments show that the average difference between the success rates obtained for the named entity questions (Table 3) and the full[REF_CITE]question set (Table 2) is 5.1%.
This means that our approach performs better when the answer entities are well specified.
Another conclusion is that the relative threshold demonstrates superiority over the absolute threshold in both test sets (average 2.3%).
"However if the per-cent of the right answers in the answer set is lower, then the efficiency of this approach may decrease."
The best results in both question sets are ob-tained by applying CCP.
Such non-symmetric for-mulas might turn out to be more applicable in gen-eral.
"As conditional corrected (CCP) is not a clas-sical co-occurrence measure like PMI and MLHR, we may consider its high performance as proof for the difference between our task and classic co-occurrence mining."
"Another indication for this is the fact that MLHR and PMI performances are compa-rable, however in the case of classic co-occurrence search, MLHR should show much better success rate."
It seems that we have to develop other mea-sures specific for the question-answer co-occurrence mining.
"Although there is some recent work addressing the evaluation of QA systems, it seems that the idea of using a fully automatic approach to answer valida-tion has still not been explored."
"For instance, the approach presented[REF_CITE]is semi-automatic."
The proposed methodology for answer validation relies on computing the overlapping be-tween the system response to a question and the stemmed content words of an answer key.
All the answer keys corresponding to the 198 TREC-8 ques-tions have been manually constructed by human an-notators using the TREC corpus and external re-sources like the Web.
The idea of using the Web as a corpus is an emerging topic of interest among the computational linguists community.
The[REF_CITE]QA track demonstrated that Web redundancy can be exploited at different levels in the process of finding answers to natural language questions.
Several studies (e.g.[REF_CITE]) suggest that the application of Web search can improve the preci- sion of a QA system by 25-30%.
A common feature of these approaches is the use of the Web to intro-duce data redundancy for a more reliable answer ex-traction from local text collections.[REF_CITE]suggests a probabilistic algorithm that learns the best query paraphrase of a question searching the Web.
Other approaches suggest training a question-answering system on the Web[REF_CITE].
The Web-mining algorithm presented in this pa-per is similar to the PMI-IR (Pointwise Mutual Information - Information Retrieval) described[REF_CITE].
Turney uses PMI and Web retrieval to decide which word in a list of candidates is the best synonym with respect to a target word.
"How-ever, the answer validity task poses different pe-culiarities."
We search how the occurrence of the question words influence the appearance of answer words.
"Therefore, we introduce additional linguis-tic techniques for pattern and query formulation, such as keyword extraction, answer type extraction, named entities recognition and pattern relaxation."
We have presented a novel approach to answer val-idation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploit-ing the redundancy of Web information.
Results ob-tained on the[REF_CITE]QA corpus correlate well with the human assessment of answers’ correctness and confirm that a Web-based algorithm provides a workable solution for answer validation.
We describe a case study in which a memory-based learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks.
We compare the algo-rithm’s performance on this parsing task with varying training set sizes (yielding learning curves) and different input repre-sentations.
"In particular we compare in-put consisting of words only, a variant that includes word form information for low-frequency words, gold-standard POS only, and combinations of these."
"The word-based shallow parser displays an appar-ently log-linear increase in performance, and surpasses the flatter POS-based curve at about 50,000 sentences of training data."
"The low-frequency variant performs even better, and the combinations is best."
Com-parative experiments with a real POS tag-ger produce lower results.
We argue that we might not need an explicit intermediate POS-tagging step for parsing when a suffi-cient amount of training material is avail-able and word form information is used for low-frequency words.
It is common in parsing to assign part-of-speech (POS) tags to words as a first analysis step provid-ing information for further steps.
"In many early parsers, the POS sequences formed the only input to the parser, i.e. the actual words were not used except in POS tagging."
"Later, with feature-based grammars, information on POS had a more central place in the lexical entry of a word than the identity of the word itself, e.g. MAJOR and other HEAD fea-tures[REF_CITE]."
"In the early days of statistical parsers, POS were explicitly and often ex-clusively used as symbols to base probabilities on; these probabilities are generally more reliable than lexical probabilities, due to the inherent sparseness of words."
"In modern lexicalized parsers, POS tagging is of-ten interleaved with parsing proper instead of be-ing a separate preprocessing module[REF_CITE]."
He suggests that this is due to the usefulness of POS for estimating back-off prob-abilities.
Abney’s (1991) chunking parser consists of two modules: a chunker and an attacher.
"The chunker divides the sentence into labeled, non-overlapping sequences (chunks) of words, with each chunk con-taining a head and (nearly) all of its premodi-fiers, exluding arguments and postmodifiers."
"His chunker works on the basis of POS information alone, whereas the second module, the attacher, also uses lexical information."
Chunks as a sepa-rate level have also been used[REF_CITE]and[REF_CITE].
This brief overview shows that the main reason for the use of POS tags in parsing is that they provide
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 433-440. useful generalizations and (thereby) counteract the sparse data problem."
"However, there are two objec-tions to this reasoning."
"First, as naturally occurring text does not come POS-tagged, we first need a mod-ule to assign POS."
"This tagger can base its decisions only on the information present in the sentence, i.e. on the words themselves."
"The question then arises whether we could use this information directly, and thus save the explicit tagging step."
The second ob-jection is that sparseness of data is tightly coupled to the amount of training material used.
"As train-ing material is more abundant now than it was even a few years ago, and today’s computers can handle these amounts, we might ask whether there is now enough data to overcome the sparseness problem for certain tasks."
"To answer these two questions, we designed the following experiments."
The task to be learned is a shallow parsing task (described below).
"In one experiment, it has to be performed on the basis of the “gold-standard”, assumed-perfect POS taken di-rectly from the training data, the Penn Treebank[REF_CITE], so as to abstract from a par-ticular POS tagger and to provide an upper bound."
"In another experiment, parsing is done on the ba-sis of the words alone."
"In a third, a special en-coding of low-frequency words is used."
"Finally, words and POS are combined."
"In all experiments, we increase the amount of training data stepwise and record parse performance for each step."
This yields four learning curves.
"The word-based shallow parser displays an apparently log-linear increase in perfor-mance, and surpasses the flatter POS-based curve at about 50,000 sentences of training data."
"The low-frequency variant performs even better, and the com-binations is best."
Comparative experiments with a real POS tagger produce lower results.
The paper is structured as follows.
"In Section 2 we describe the parsing task, its input representation, how this data was extracted from the Penn Treebank, and how we set up the learning curve experiments using a memory-based learner."
Section 3 provides the experimental learning curve results and analyses them.
Section 4 contains a comparison of the effects with gold-standard and automatically assigned POS.
"We review related research in Section 5, and formu-late our conclusions in Section 6."
We chose a shallow parsing task as our benchmark task.
"If, to support an application such as infor-mation extraction, summarization, or question an-swering, we are only interested in parts of the parse tree, then a shallow parser forms a viable alterna-tive to a full parser."
Our shallow parsing task is a combination of chunking (finding and labelling non-overlapping syntactically functional sequences) and what we will call function tagging.
Our chunks and functions are based on the annotations in the third release of the Penn Treebank[REF_CITE].
Below is an example of a tree and the corresponding chunk (sub-scripts on brackets) and function (superscripts on headwords) annotation: [  Once   ] [  he   ] [ was held  ] [  for   ] [  three months  ] [  without  ] [ being charged  ] .
"Nodes in the tree are labeled with a syntactic cat-egory and up to four function tags that specify gram-matical relations (e.g. SBJ for subject), subtypes of adverbials (e.g. TMP for temporal), discrepan-cies between syntactic form and syntactic function (e.g. NOM for non-nominal constituents function-ing nominally) and notions like topicalization."
Our chunks are based on the syntactic part of the con-stituent label.
The conversion program is the same as used for the[REF_CITE]shared task (Tjong[REF_CITE]).
"Head words of chunks are assigned a function code that is based on the full constituent label of the parent and of ancestors with a different category, as in the case of VP/S-NOM in the example."
"To formulate the task as a machine-learnable classi-fication task, we use a representation that encodes the joint task of chunking and function-tagging a sentence in per-word classification instances."
"As illustrated in Table 2.1, an instance (which corre-sponds to a row in the table) consists of the val-ues for all features (the columns) and the function-chunk code for the focus word."
The features de-scribe the focus word and its local context.
"For the chunk part of the code, we adopt the “Inside”, “Outside”, and “Between” (IOB) encoding originat-ing[REF_CITE]."
"For the function part of the code, the value is either the function for the head of a chunk, or the dummy value NOFUNC for all non-heads."
"For creating the POS-based task, all words are replaced by the gold-standard POS tags associated with them in the Penn Treebank."
"For the combined task, both types of fea-tures are used simultaneously."
"When the learner is presented with new instances from heldout material, its task is thus to assign the combined function-chunk codes to either words or POS in context."
"From the sequence of predicted function-chunk codes, the complete chunking and function assignment can be reconstructed."
"How-ever, predictions can be inconsistent, blocking a straightforward reconstruction of the complete shal-low parse."
"We employed the following four rules to resolve such problems: (1) When an O chunk code is followed by a B chunk code, or when an I chunk code is followed by a B chunk code with a different chunk type, the B is converted to an I. (2) When more than one word in a chunk is given a function code, the function code of the rightmost word is taken as the chunk’s function code. (3) If all words of the chunk receive NOFUNC tags, a prior function code is assigned to the rightmost word of the chunk."
"This prior, estimated on the training set, represents the most frequent function code for that type of chunk."
"To measure the success of our learner, we com-pute the precision, recall and their harmonic mean, the F-score [Footnote_1] with =1[REF_CITE]."
"1 F &quot;!$# &amp;(% %&apos; , precision +) -* , precision &apos; recall , recall"
"In the combined function-chunking evaluation, a chunk is only counted as correct when its boundaries, its type and its function are identified correctly."
"Our total data set consists of all 74,024 sentences in the Wall Street Journal, Brown and ATIS Cor-pus subparts of the Penn Treebank III."
"We ran-domized the order of the sentences in this dataset, and then split it into ten 90%/10% partitionings with disjoint 10% portions, in order to run 10-fold cross-validation experiments[REF_CITE]."
"To provide differently-sized train-ing sets for learning curve experiments, each train-ing set (of 66,627 sentences) was also clipped at the following sizes: 100 sentences, 500, 1000, 2000, 5000, 10,000, 20,000 and 50,000."
All data was con-verted to instances as illustrated in Table 2.1.
"For the total data set, this yields [Footnote_1],637,268 instances, one for each word or punctuation mark. 62,472 word types occur in the total data set, and 874 different function-chunk codes."
"1 F &quot;!$# &amp;(% %&apos; , precision +) -* , precision &apos; recall , recall"
"Arguably, the choice of algorithm is not crucial in learning curve experiments."
"First, we aim at mea-suring relative differences arising from the selection of types of input."
"Second, there are indications that increasing the training set of language processing tasks produces much larger performance gains than varying among algorithms at fixed training set sizes; moreover, these differences also tend to get smaller with larger data sets[REF_CITE]."
Memory-based learning[REF_CITE]is a supervised inductive learning algorithm for learning classification tasks.
"Memory-based learning treats a set of labeled (pre-classified) training instances as points in a multi-dimensional feature space, and stores them as such in an instance base in mem-ory (rather than performing some abstraction over them)."
"Classification in memory-based learning is performed by the -NN algorithm[REF_CITE]that searches for the ‘nearest neighbors’ according to the distance function between two in- stances . and / , 0213.54687/ :9&lt;&gt;; ?A="
"CBED@ ?[REF_CITE]H ? 4JI ? 7 , where K is the number of features, D ? is a weight for feature L , and F estimates the difference between the two instances’ values at the L th feature."
The classes of the nearest neighbors then determine the class of the new case.
"In our experiments, we used a variant of the IB 1 memory-based learner and classifier as implemented in TiMBL[REF_CITE]."
"On top of the - NN kernel of IB 1 we used the following metrics that fine-tune the distance function and the class voting automatically: (1) The weight (importance) of a fea-ture L , D ? , is estimated in our experiments by com-puting its gain ratio MON ?[REF_CITE]."
This is the algorithm’s default choice. (2) Differences be-tween feature values (i.e. words or POS tags) are es-timated by the real-valued outcome of the modified value difference metric[REF_CITE]. (3) was set to seven.
This and the previous parameter setting turned out best for a chunking task using the same algorithm as reported by Veenstra and van den[REF_CITE]. (4) Class voting among the nearest neighbours is done by weighting each neighbour’s vote by the inverse of its distance to the test example[REF_CITE].
"The switch point of TRIBL was set to 1 for the words only and POS only ex-periments, i.e. a decision-tree split was made on the most important feature, the focus word, respectively focus POS."
"For the experiments with both words and POS, the switch point was set to 2 and the algorithm was forced to split on the focus word and focus POS."
The metrics under 1) to 4) then apply to the remain-ing features.
We report the learning curve results in three para-graphs.
"In the first, we compare the performance of a plain words input representation with that of a gold-standard POS one."
In the second we intro-duce a variant of the word-based task that deals with low-frequency words.
The last paragraph describes results with input consisting of words and POS tags.
"Words only versus POS tags only As illus-trated in Figure 1, the learning curves of both the word-based and the POS-based representation are upward with more training data."
The word-based curve starts much lower but flattens less; in the tested range it has an approximately log-linear growth.
"Given the measured results, the word-based curve surpasses the POS-based curve at a training set size between 20,000 and 50,000 sentences."
"This proves two points: First, experiments with a fixed training set size might present a misleading snapshot."
"Sec-ond, the amount of training material available today is already enough to make words more valuable in-put than (gold-standard!)"
"Low-frequency word encoding variant If TRIBL encounters an unknown word in the test ma-terial, it stops already at the decision tree stage and returns the default class without even using the in-formation provided by the context."
This is clearly disadvantageous and specific to this choice of al- gorithm.
A more general shortcoming is that the word form of an unknown word often contains use-ful information that is not available in the present setup.
"To overcome these two problems, we applied wh[REF_CITE]calls “attenuation” to all words occurring ten times or less in training material."
"If such a word ends in a digit, it is converted to the string “MORPH-NUM”; if the word is six charac-ters or longer it becomes “MORPH-XX” where XX are the final two letters, else it becomes “MORPH-SHORT”."
"If the first letter is capitalised, the atten-uated form is “MORPH-CAP”."
This produces se-quences such as A number of MORPH-ts were MORPH-ly MORPH-ed by traders . ( A number of developments were negatively interpreted by traders ).
We applied this attenuation method to all training sets.
All words in test material that did not occur as words in the atten-uated training material were also attenuated follow-ing the same procedure.
The curve resulting from the attenuated word-based experiment is also displayed in Figure 1.
The curve illustrates that the attenuated representation performs better than the pure word-based one at all reasonable training set sizes.
"However the effect clearly diminuishes with more training data, so we cannot exclude that the two curves will meet with yet more training data."
"Combining words with POS tags Although the word-based curve, and especially its attenuated vari-ant, end higher than the POS-based curve, POS might still be useful in addition to words."
We there-fore also tested a representation with both types of features.
"As shown in Figure 1, the “attenuated word + gold-standard POS” curve starts close to the gold-standard POS curve, attains break-even with this curve at about 500 sentences, and ends close to but higher than all other curves, including the “attenu-ated word” curve."
"Although the performance increase through the ad-dition of POS becomes smaller with more train-ing data, it is still highly significant with maximal training set size."
"As the tags are the gold-standard tags taken directly from the Penn Treebank, this re-sult provides an upper bound for the contribution of POS tags to the shallow parsing task under inves-tigation."
"Automatic POS tagging is a well-studied task[REF_CITE], and reported errors in the range of 2–6% are common."
"To investigate the ef-fect of using automatically assigned tags, we trained MBT, a memory-based tagger[REF_CITE], on the training portions of our 10-fold cross-validation experiment for the maximal data and let it predict tags for the test material."
"The memory-based tagger attained an accuracy of 96.7% ( R 0.1; 97.0% on known words, and 80.9% on unknown words)."
We then used these MBT POS instead of the gold-standard ones.
"The results of these experiments, along with the equivalent results using gold-standard POS, are dis-played in Table 2."
"As they show, the scores with au-tomatically assigned tags are always lower than with the gold-standard ones."
"When taken individually, the difference in F-scores of the gold-standard ver-sus the MBT POS tags is 1.6 points."
"Combined with words, the MBT POS contribute 0.5 points (com-pared against words taken individually); combined with attenuated words, they contribute 0.3 points."
This is much less than the improvement by the gold-standard tags (1.7 points) but still significant.
"How-ever, as the learning curve experiments showed, this is only a snapshot and the improvement may well diminish with more training data."
A breakdown of accuracy results shows that the highest improvement in accuracy is achieved for fo-cus words in the MORPH-SHORT encoding.
"In these cases, the POS tagger has access to more infor-mation about the low-frequency word (e.g. its suffix) than the attenuated form provides."
This suggests that this encoding is not optimal.
F-scores range between 91.4 and 92.8.
The first two articles mention that words and (automatically assigned) POS together perform better than POS alone.
"Chunking is one part of the task studied here, so we also computed performance on chunks alone, ignoring function codes."
"Indeed the learning curve of words combined with gold-standard POS crosses the POS-based curve before 10,000 sentences on the chunking subtask."
Tjong[REF_CITE]give an overview of the CoNLL shared task of chunking.
The types and definitions of chunks are identical to the ones used here.
Training material again consists of the 9000 Wall Street Journal sentences with automatically assigned POS tags.
"The best F-score (93.5) is higher than the 91.5 F-score attained on chunking in our study using attenuated words only, but using the maximally-sized training sets."
"With gold-standard POS and attenuated words we attain an F-score of 94.2; with MBT POS tags and atten-uated words, 92.8."
"In the CoNLL competition, all three best systems used combinations of classifiers instead of one single classifier."
"In addition, the effect of our mix of sentences from different corpora on top of WSJ is not clear."
They report an F-score of 69.8 for a training size of 3299 words of elementary school reading comprehension tests.
"In contrast to these studies, we do not chunk before find-ing grammatical relations; rather, chunking is per-formed simultaneously with headword function tag-ging."
"Measuring F-scores on the correct assign-ment of functions to headwords in our study, we at-tain 78.2 F-score using words, 80.1 using attenuated words, 80.9 using attenuated words combined with gold-standard POS, and 79.7 using attenuated words combined with MBT POS (which is slightly worse than with attenuated words only)."
Our function tag-ging task is easier than finding grammatical relations as we tag a headword of a chunk as e.g. a subject in isolation whereas grammatical relation assign-ment also includes deciding which verb this chunk is the subject of.
The last transducer then uses the function tags to extract subject/verb and object/verb relations (from French text).
POS are normally considered useful information in shallow and full parsing.
Our learning curve experi-ments show that:
T The relative merit of words versus POS as in-put for the combined chunking and function-tagging task depends on the amount of training data available.
T The absolute performance of words depends on the treatment of rare words.
The additional use of word form information (attenuation) im-proves performance.
T The addition of POS also improves perfor-mance.
"In this and the previous case, the effect becomes smaller with more training data."
Experiments with the maximal training set size show that:
T Addition of POS maximally yields an improve-ment of 1.7 points on this data.
T With realistic POS the improvement is much smaller.
Preliminary analysis shows that the improvement by realistic POS seems to be caused mainly by a supe-rior use of word form information by the POS tag-ger.
We therefore plan to experiment with a POS tagger and an attenuated words variant that use ex-actly the same word form information.
In addition we also want to pursue using the combined chunker and grammatical function tagger described here as a first step towards grammatical relation assignment.
We present an architecture for the integra-tion of shallow and deep NLP components which is aimed at flexible combination of different language technologies for a range of practical current and future appli-cations.
"In particular, we describe the inte-gration of a high-level HPSG parsing sys-tem with different high-performance shal-low components, ranging from named en-tity recognition to chunk parsing and shal-low clause recognition."
"The NLP com-ponents enrich a representation of natu-ral language text with layers of new XML meta-information using a single shared data structure, called the text chart."
"We de-scribe details of the integration methods, and show how information extraction and language checking applications for real-world German text benefit from a deep grammatical analysis."
"Over the last ten years or so, the trend in application-oriented natural language processing (e.g., in the area of term, information, and answer extraction) has been to argue that for many purposes, shallow natural language processing (SNLP) of texts can provide sufficient information for highly accurate and useful tasks to be carried out."
"Since the emer-gence of shallow techniques and the proof of their utility, the focus has been to exploit these technolo- gies to the maximum, often ignoring certain com-plex issues, e.g. those which are typically well han-dled by deep NLP systems."
"Up to now, deep natural language processing (DNLP) has not played a sig-nificant role in the area of industrial NLP applica-tions, since this technology often suffers from insuf-ficient robustness and throughput, when confronted with large quantities of unrestricted text."
"Current information extractions (IE) systems therefore do not attempt an exhaustive DNLP analy-sis of all aspects of a text, but rather try to analyse or “understand” only those text passages that contain relevant information, thereby warranting speed and robustness wrt. unrestricted NL text."
"What exactly counts as relevant is explicitly defined by means of highly detailed domain-specific lexical entries and/or rules, which perform the required mappings from NL utterances to corresponding domain knowl-edge."
"However, this “fine-tuning” wrt. a particular application appears to be the major obstacle when adapting a given shallow IE system to another do-main or when dealing with the extraction of com-plex “scenario-based” relational structures."
"In fact,[REF_CITE]have shown that the cur-rent IE technology seems to have an upper perfor-mance level of less than 60% in such cases."
"It seems reasonable to assume that if a more accurate analy-sis of structural linguistic relationships could be pro-vided (e.g., grammatical functions, referential rela-tionships), this barrier might be overcome."
"Actually, the growing market needs in the wide area of intel-ligent information management systems seem to re-quest such a break-through."
In this paper we will argue that the quality of cur-
"Computational Linguistics (ACL), Philadelphia,[REF_CITE]pp. 441-448. rent SNLP-based applications can be improved by integrating DNLP on demand in a focussed manner, and we will present a system that combines the fine-grained anaysis provided by HPSG parsing with a high-performance SNLP system into a generic and flexible NLP architecture."
"Owing to the fact that deep and shallow technologies are complementary in nature, integration is a non-trivial task: while SNLP shows its strength in the areas of efficiency and robustness, these aspects are problematic for DNLP systems."
"On the other hand, DNLP can deliver highly precise and fine-grained linguistic analyses."
The challenge for integration is to combine these two paradigms according to their virtues.
"Probably the most straightforward way to inte-grate the two is an architecture in which shallow and deep components run in parallel, using the results of DNLP, whenever available."
"While this kind of ap-proach is certainly feasible for a real-time applica-tion such as Verbmobil, it is not ideal for processing large quantities of text: due to the difference in pro-cessing speed, shallow and deep NLP soon run out of sync."
"To compensate, one can imagine two possi-ble remedies: either to optimize for precision, or for speed."
"The drawback of the former strategy is that the overall speed will equal the speed of the slow-est component, whereas in case of the latter, DNLP will almost always time out, such that overall preci-sion will hardly be distinguishable from a shallow-only system."
"What is thus called for is an integrated, flexible architecture where components can play at their strengths."
"Partial analyses from SNLP can be used to identify relevant candidates for the focussed use of DNLP, based on task or domain-specific crite-ria."
"Furthermore, such an integrated approach opens up the possibility to address the issue of robustness by using shallow analyses (e.g., term recognition) to increase the coverage of the deep parser, thereby avoiding a duplication of efforts."
"Likewise, integra-tion at the phrasal level can be used to guide the deep parser towards the most likely syntactic anal-ysis, leading, as it is hoped, to a considerable speed-up."
The WHITEBOARD architecture defines a platform that integrates the different NLP components by en-riching an input document through XML annota-tions.
XML is used as a uniform way of represent-ing and keeping all results of the various processing components and to support a transparent software infrastructure for LT-based applications.
"It is known that interesting linguistic information —especially when considering DNLP— cannot efficiently be represented within the basic XML markup frame-work (“typed parentheses structure”), e.g., linguistic phenomena like coreferences, ambiguous readings, and discontinuous constituents."
The WHITEBOARD architecture employs a distributed multi-level repre-sentation of different annotations.
"Instead of trans-lating all complex structures into one XML docu-ment, they are stored in different annotation layers (possibly non-XML, e.g. feature structures)."
Hyper-links and “span” information together support effi-cient access between layers.
Linguistic information of common interest (e.g. constituent structure ex-tracted from HPSG feature structures) is available in XML format with hyperlinks to full feature struc-ture representations externally stored in correspond-ing data files.
Fig. 1 gives an overview of the architecture of the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and a specification describing the components and con- figuration options requested.
"The core WHAM en-gine has an XML markup storage (external “offline” representation), and an internal “online” multi-level annotation chart (index-sequential access)."
"Follow-ing the trichotomy of NLP data representation mod-els[REF_CITE], the XML markup contains additive information, while the multi-level chart contains positional and abstraction-based in-formation, e.g., feature structures representing NLP entities in a uniform, linguistically motivated form."
Applications and the integrated components ac-cess the WHAM results through an object-oriented programming (OOP) interface which is designed as general as possible in order to abstract from component-specific details (but preserving shallow and deep paradigms).
The interfaces of the actu-ally integrated components form subclasses of the generic interface.
New components can be inte-grated by implementing this interface and specifying DTDs and/or transformation rules for the chart.
"The OOP interface consists of iterators that walk through the different annotation levels (e.g., token spans, sentences), reference and seek operators that allow to switch to corresponding annotations on a different level (e.g., give all tokens of the current sentence, or move to next named entity starting from a given token position), and accessor meth-ods that return the linguistic information contained in the chart."
"Similarily, general methods support navigating the type system and feature structures of the DNLP components."
The resulting output of the WHAM can be accessed via the OOP interface or as XML markup.
"The WHAM interface operations are not only used to implement NLP component-based applica-tions, but also for the integration of deep and shallow processing components itself."
"Shallow analysis is performed by SPPC, a rule-based system which consists of a cascade of weighted finite–state components responsible for performing subsequent steps of the linguistic anal-ysis, including: fine-grained tokenization, lexico-morphological analysis, part-of-speech filtering, named entity (NE) recognition, sentence bound- ary detection, chunk and subclause recognition, see[REF_CITE]for details."
"SPPC is capable of pro-cessing vast amounts of textual data robustly and ef-ficiently (ca. 30,000 words per second in standard PC environment)."
We will briefly describe the SPPC components which are currently integrated with the deep components.
Each token identified by a tokenizer as a poten-tial word form is morphologically analyzed.
"For each token, its lexical information (list of valid read-ings including stem, part-of-speech and inflection information) is computed using a fullform lexicon of about 700,000 entries that has been compiled out from a stem lexicon of about 120,000 lemmas."
"Af-ter morphological processing, POS disambiguation rules are applied which compute a preferred read-ing for each token, while the deep components can back off to all readings."
NE recognition is based on simple pattern matching techniques.
"Proper names (organizations, persons, locations), temporal expres-sions and quantities can be recognized with an av-erage precision of almost 96% and recall of 85%."
"Furthermore, a NE–specific reference resolution is performed through the use of a dynamic lexicon which stores abbreviated variants of previously rec-ognized named entities."
"Finally, the system splits the text into sentences by applying only few, but highly accurate contextual rules for filtering implau-sible punctuation signs."
These rules benefit directly from NE recognition which already performs re-stricted punctuation disambiguation.
"The HPSG Grammar is based on a large–scale grammar for German[REF_CITE], which was further developed in the VERBMOBIL project for translation of spoken language[REF_CITE]."
"After VERBMOBIL the grammar was adapted to the requirements of the LKB/PET system[REF_CITE], and to written text, i.e., extended with constructions like free relative clauses that were ir-relevant in the VERBMOBIL scenario."
"The grammar consists of a rich hierarchy of 5,069 lexical and phrasal types."
"The core grammar contains 23 rule schemata, 7 special verb move-ment rules, and 17 domain specific rules."
All rule schemata are unary or binary branching.
"The lexicon contains 38,549 stem entries, from which more than 70% were semi-automatically acquired from the an-notated NEGRA corpus[REF_CITE]."
"The grammar parses full sentences, but also other kinds of maximal projections."
"In cases where no full analysis of the input can be provided, analyses of fragments are handed over to subsequent modules."
Such fragments consist of maximal projections or single words.
The HPSG analysis system currently integrated in the WHITEBOARD system is PET[REF_CITE].
"Initially, PET was built to experiment with different techniques and strategies to process unification-based grammars."
The resulting sys-tem provides efficient implementations of the best known techniques for unification and parsing.
"As an experimental system, the original design lacked open interfaces for flexible integration with external components."
"For instance, in the beginning of the WHITEBOARD project the system only ac-cepted fullform lexica and string input."
In collabora-tion with Ulrich Callmeier the system was extended.
"Instead of single word input, input items can now be complex, overlapping and ambiguous, i.e. essen-tially word graphs."
"We added dynamic creation of atomic type symbols, e.g., to be able to add arbitrary symbols to feature structures."
"With these enhance-ments, it is possible to build flexible interfaces to external components like morphology, tokenization, named entity recognition, etc."
Morphology and POS The coupling between the morphology delivered by SPPC and the input needed for the German HPSG was easily established.
The morphological classes of German are mapped onto HPSG types which expand to small feature struc-tures representing the morphological information in a compact way.
A mapping to the output of SPPC was automatically created by identifying the corre-sponding output classes.
"Currently, POS tagging is used in two ways."
"First, lexicon entries that are marked as preferred by the shallow component are assigned higher priority than the rest."
"Thus, the probability of finding the cor-rect reading early should increase without excluding any reading."
"Second, if for an input item no entry is found in the HPSG lexicon, we automatically create a default entry, based on the part–of–speech of the preferred reading."
"This increases robustness, while avoiding increase in ambiguity."
Named Entity Recognition Writing HPSG gram-mars for the whole range of NE expressions etc. is a tedious and not very promising task.
"They typi-cally vary across text sorts and domains, and would require modularized subgrammars that can be easily exchanged without interfering with the general core."
This can only be realized by using a type interface where a class of named entities is encoded by a gen-eral HPSG type which expands to a feature structure used in parsing.
We exploit such a type interface for coupling shallow and deep processing.
The classes of named entities delivered by shallow processing are mapped to HPSG types.
"However, some fine-tuning is required whenever deep and shallow pro-cessing differ in the amount of input material they assign to a named entity."
"An alternative strategy is used for complex syn-tactic phrases containing NEs, e.g., PPs describ-ing time spans etc."
"It is based on ideas from Explanation–based Learning (EBL, see[REF_CITE]) for natural language analy-sis, where analysis trees are retrieved on the basis of the surface string."
"In our case, the part-of-speech sequence of NEs recognised by shallow analysis is used to retrieve pre-built feature structures."
These structures are produced by extracting NEs from a corpus and processing them directly by the deep component.
"If a correct analysis is delivered, the lexical parts of the analysis, which are specific for the input item, are deleted."
We obtain a sceletal analysis which is underspecified with respect to the concrete input items.
The part-of-speech sequence of the original input forms the access key for this structure.
"In the application phase, the underspeci-fied feature structure is retrieved and the empty slots for the input items are filled on the basis of the con-crete input."
"The advantage of this approach lies in the more elaborate semantics of the resulting feature struc-tures for DNLP, while avoiding the necessity of adding each and every single name to the HPSG lex-icon."
"Instead, good coverage and high precision can be achieved using prototypical entries."
"Lexical Semantics When first applying the origi-nal VERBMOBIL HPSG grammar to business news articles, the result was that 78.49% of the miss-ing lexical items were nouns (ignoring NEs)."
"In the integrated system, unknown nouns and NEs can be recognized by SPPC, which determines morpho-syntactic information."
"It is essential for the deep sys-tem to associate nouns with their semantic sorts both for semantics construction, and for providing se-mantically based selectional restrictions to help con-straining the search space during deep parsing."
"Ger-maNet[REF_CITE]is a large lexical database, where words are associated with POS in-formation and semantic sorts, which are organized in a fine-grained hierarchy."
"The HPSG lexicon, on the other hand, is comparatively small and has a more coarse-grained semantic classification."
"To provide the missing sort information when re-covering unknown noun entries via SPPC, a map-ping from the GermaNet semantic classification to the HPSG semantic classificati[REF_CITE]is applied which has been automatically ac-quired."
The training material for this learning pro-cess are those words that are both annotated with se-mantic sorts in the HPSG lexicon and with synsets of GermaNet.
The learning algorithm computes a mapping relevance measure for associating seman-tic concepts in GermaNet with semantic sorts in the HPSG lexicon.
"For evaluation, we examined a cor-pus of 4664 nouns extracted from business news that were not contained in the HPSG lexicon. 2312 of these were known in GermaNet, where they are assigned 2811 senses."
"With the learned mapping, the GermaNet senses were automatically mapped to HPSG semantic sorts."
The evaluation of the map-ping accuracy yields promising results:[REF_CITE].52% of the cases the computed sort with the highest rel-evance probability was correct.
"In the remaining 20.70% of the cases, the correct sort was among the first three sorts."
In the previous paragraphs we described strategies for integration of shallow and deep processing where the focus is on improving DNLP in the domain of lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for the integration of shallow and deep analysis at the level of phrasal syntax by guiding the deep syntac-tic parser towards a partial pre-partitioning of com-plex sentences provided by shallow analysis sys-tems.
"This strategy can reduce the search space, and enhance parsing efficiency of DNLP."
"Stochastic Topological Parsing The traditional syntactic model of topological fields divides basic clauses into distinct fields: so-called pre-, middle-and post-fields, delimited by verbal or senten-tial markers."
"This topological model of German clause structure is underspecified or partial as to non-sentential constituent boundaries, but provides a linguistically well-motivated, and theory-neutral macrostructure for complex sentences."
Due to its linguistic underpinning the topological model pro-vides a pre-partitioning of complex sentences that is (i) highly compatible with deep syntactic structures and (ii) maximally effective to increase parsing ef-ficiency.
"At the same time (iii) partiality regarding the constituency of non-sentential material ensures the important aspects of robustness, coverage, and processing efficiency."
"In[REF_CITE]we present a corpus-driven stochastic topological parser for German, based on a topological restructuring of the NEGRA corpus[REF_CITE]."
For topological tree-bank conversion we build on methods and results[REF_CITE].
The stochastic topological parser follows the probabilistic model of non-lexicalised PCFGs[REF_CITE].
"Due to abstraction from constituency decisions at the sub-sentential level, and the essentially POS-driven nature of topologi-cal structure, this rather simple probabilistic model yields surprisingly high figures of accuracy and cov-erage (see Fig.2 and[REF_CITE]for more detail), while context-free parsing guarantees efficient processing."
"The next step is to elaborate a (partial) map-ping of shallow topological and deep syntactic struc-tures that is maximally effective for preference-gui- ded deep syntactic analysis, and thus, efficiency im-provements in deep syntactic processing."
"Such a mapping is illustrated for a verb-second clause in Fig.3, where matching constituents of topological and deep-syntactic phrase structure are indicated by circled nodes."
"With this mapping defined for all sen-tence types, we can proceed to the technical aspects of integration into the WHITEBOARD architecture and XML text chart, as well as preference-driven HPSG analysis in the PET system."
"An evaluation has been started using the NEGRA corpus, which contains about 20,000 newspaper sen-tences."
The main objectives are to evaluate the syn-tactic coverage of the German HPSG on newspaper text and the benefits of integrating deep and shallow analysis.
"The sentences of the corpus were used in their original form without stripping, e.g. parenthe-sized insertions."
"We extended the HPSG lexicon semi-automatically from about 10,000 to 35,000 stems, which roughly corresponds to 350,000 full forms."
"Then, we checked the lexical coverage of the deep system on the whole corpus, which resulted in 28.6% of the sentences being fully lexically analyzed."
"The corresponding experiment with the integrated system yielded an improved lexical coverage of 71.4%, due to the techniques described in section 3."
"This increase is not achieved by manual extension, but only through synergy between the deep and shallow components."
"To test the syntactic coverage, we processed the subset of the corpus that was fully covered lexically (5878 sentences) with deep analysis only."
The re-sults are shown in table 4 in the second column.
"In order to evaluate the integrated system we processed 20,568 sentences from the corpus without further ex-tension of the HPSG lexicon (see table 4, third col-umn)."
We expect better over-all results once this problem is removed.
"Since typed feature structures (TFS) in Whiteboard serve as both a representation and an interchange format, we developed a Java package (JTFS) that implements the data structures, together with the necessary operations."
"These include a lazy-copying unifier, a subsumption and equivalence test, deep copying, iterators, etc."
"JTFS supports a dynamic construction of typed feature structures, which is im-portant for information extraction."
Information extraction in Whiteboard benefits both from the integration of the shallow and deep analy-sis results and from their processing methods.
We chose management succession as our application domain.
Two sets of template filling rules are defined: pattern-based and unification-based rules.
"The pattern-based rules work directly on the output delivered by the shallow analysis, for example, 1 $ &apos;% &amp; (*)*+ +-, ./1032% (1) Nachfolger 4 von person out 1 5 ."
"This rule matches expressions like Nachfolger von Helmut Kohl (successor of) which contains two string tokens Nachfolger and von followed by a per-son name, and fills the slot of person out with the recognized person name Helmut Kohl."
The pattern-based grammar yields good results by recognition of local relationships as in (1).
The unification-based rules are applied to the deep analysis re-sults.
"Given the fine-grained syntactic and seman-tic analysis of the HPSG grammar and its robust-ness (through SNLP integration), we decided to use the semantic representation (MRS, see[REF_CITE]) as additional input for IE."
"The reason is that MRSs express precise relationships between the chunks, in particular, in constructions involving (combinations of) free word order, long distance de-pendencies, control and raising, or passive, which are very difficult, if not impossible, to recognize for a pattern-based grammar."
"E.g., the short sentence (2) illustrates a combination of free word order, con-trol, and passive."
The subject of the passive verb wurde gebeten is located in the middle field and is at the same time the subject of the infinitive verb zu übernehmen.
"A deep (HPSG) analysis can recog-nize the dependencies quite easily, whereas a pattern based grammar cannot determine, e.g., for which verb Peter Miscke or Dietmar Hopp is the subject. (2) Peter Miscke following was Dietmar Hopp asked, the development sector to take over."
"Peter Miscke Entwicklungsabteilung zu zufolge wurde Dietmar Hopp übernehmen. gebeten, die “ According to Peter Miscke, Dietmar Hopp was asked to take over the development sector.”"
We employ typed feature structures (TFS) as our modelling language for the definition of scenario template types and template element types.
"There-fore, the template filling results from shallow and deep analysis can be uniformly encoded in TFS."
"As a side effect, we can easily adapt JTFS unification for the template merging task, by interperting the par-tially filled templates from deep and shallow anal-ysis as constraints."
"E.g., to extract the relevant in-formation from the above sentence, the following unification-based rule can be applied: 67 : =&lt;&lt;&lt; 7 PERSON IN 7 7 DIVISION 6 9 : ; &lt;; PRED “übernehmen” 8 MRS 8 AGENT THEME 9"
Another area where DNLP can support existing shallow-only tools is grammar and controlled lan-guage checking.
"Due to the scarce distribution of true errors (Becker et al., to appear), there is a high a priori probability for false alarms."
"As the num-ber of false alarms decides on user-acceptance, pre-cision is of utmost importance and cannot easily be traded for recall."
"Current controlled language checking systems for German, such as MULTILINT[URL_CITE]or FLAG[URL_CITE]build exclusively on SNLP: while checking of local errors (e.g. NP-internal agreement, prepositional case) can be performed quite reliably by such a system, error types involv-ing non-local dependencies, or access to grammati-cal functions are much harder to detect."
"The use of DNLP in this area is confronted with several system-atic problems: first, formal grammars are not always available, e.g., in the case of controlled languages; second, erroneous sentences lie outside the language defined by the competence grammar, and third, due to the sparse distribution of errors, a DNLP system will spend most of the time parsing perfectly well-formed sentences."
"Using an integrated approach, a shallow checker can be used to cheaply identify ini-tial error candidates, while false alarms can be elim- inated based on the richer annotations provided by the deep parser."
In this paper we reported on an implemented sys-tem called WHITEBOARD which integrates differ-ent shallow components with a HPSG–based deep system.
The integration is realized through the metaphor of textual annotation.
"To best of our knowledge, this is the first implemented system which integrates high-performance shallow process-ing with an advanced deep HPSG–based analysis system."
"There exists only very little other work that considers integration of shallow and deep NLP using an XML–based architecture, most notably[REF_CITE]."
"However, their integration efforts are largly limited to the level of POS tag in-formation."
We present a document compression sys-tem that uses a hierarchical noisy-channel model of text production.
Our compres-sion system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input.
"The system then uses a sta-tistical hierarchical model of text produc-tion in order to drop non-important syn-tactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length."
The sys-tem outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text.
Our results support the claim that discourse knowledge plays an important role in document summariza-tion.
Single document summarization systems proposed to date fall within one of the following three classes:
Extractive summarizers simply select and present to the user the most important sentences in a text — see[REF_CITE]for comprehensive overviews of the methods and algorithms used to accomplish this.
Extraction-based summarizers often produce out-puts that contain non-important sentence fragments.
"For example, the hypothetical extractive summary of Text (1), which is shown in Table 1, can be com-pacted further by deleting the clause “which is al-ready almost enough to win”."
"Headline-based sum-maries, such as that shown in Table 1, are usually indicative of a text’s content but not informative, grammatical, or coherent."
"By repeatedly applying a sentence-simplification algorithm one sentence at a time, one can compress a text; yet, the outputs gen-erated in this way are likely to be incoherent and to contain unimportant information."
"When summa-rizing text, some sentences should be dropped alto-gether."
"Ideally, we would like to build systems that have the strengths of all these three classes of approaches."
"The “Document Compression” entry in Table 1 shows a grammatical, coherent summary of Text (1), which was generated by a hypothetical document compression system that preserves the most impor-tant information in a text while deleting sentences, phrases, and words that are subsidiary to the main message of the text."
"Obviously, generating coher-ent, grammatical summaries such as that produced by the hypothetical document compression system in Table 1 is not trivial because of many conflicting goals 1 ."
The deletion of certain sentences may result in incoherence and information loss.
The deletion of certain words and phrases may also lead to ungram-maticality and information loss.
The mayor is now looking for re-election.
"John Doe (1) has already secured the vote of most democrats in his constituency, which is already almost enough to win."
"But without the support of the governer, he is still on shaky grounds."
"In this paper, we present a document compression system that uses hierarchical models of discourse and syntax in order to simultaneously manage all these conflicting goals."
Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input.
"The system then uses a sta-tistical hierarchical model of text production in or-der to drop non-important syntactic and discourse units so as to generate coherent, grammatical doc-ument compressions of arbitrary length."
The system outperforms both a baseline and a sentence-based compression system that operates by simplifying se-quentially all sentences in a text.
The document compression task is conceptually simple.
"Given a document , our  goal is to produce a new document by “dropping” words from ."
"In order to achieve this goal, we extent the noisy-channel model proposed by Knight &amp;[REF_CITE]."
"Their system compressed sen-tences by dropping syntactic constituents, but could be applied to entire documents only on a sentence-by-sentence basis."
"As discussed in Section 1, this is not adequate because the resulting summary may contain many compressed sentences that are irrele-vant."
"In order to extend Knight &amp; Marcu’s approach beyond the sentence level, we need to “glue” sen-tences together in a tree structure similar to that used at the sentence level."
Rhetorical Structure Theory (RST)[REF_CITE]provides us this “glue.”
The tree in Figure 1 depicts the RST structure of Text ([Footnote_1]).
"1 A number of other systems use the outputs of extrac-tive summarizers and repair them to improve coherence[REF_CITE]. Unfortunately, none of these seems flexible enough to produce in one shot good summaries that are simul-taneously coherent and grammatical."
"In RST, discourse structures are non-binary trees whose leaves correspond to elementary discourse units (EDUs), and whose internal nodes correspond to contiguous text spans."
Each internal node in an RST tree is characterized by a rhetor-ical relation.
"For example, the first sentence in Text (1) provides BACKGROUND information for inter-preting the information in sentences 2 and 3, which are in a C ONTRAST relation (see Figure 1)."
"Each re-lation holds between two adjacent non-overlapping text spans called NUCLEUS and SATELLITE . (There are a few exceptions to this rule: some relations, such as LIST and CONTRAST , are multinuclear.)"
The dis-tinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer’s purpose than the satellite.
Our system is able to analyze both the discourse structure of a document and the syntactic structure of each of its sentences or EDUs.
It then compresses the document by dropping either syntactic or dis-course constituents.
"For a given document , we want to find the summary text that maximizes ."
"Using $%&amp;&quot;&amp;&quot; Bayes rule, we flip this so we end up maximizing $%&amp;&quot; ."
"Thus, we are left with modelling two (&quot; probability distributions: , the probability of a document given a summary , and , the probability of a summary."
We assume that we are given the discourse structure of each document and the syntactic structures of each of its EDUs.
"The intuitive way of thinking about this applica-tion of Bayes rule, reffered to as the noisy-channel model, is that we start with a summary and add “noise” to it, yielding a longer document ."
"The noise added in our model consists of words, phrases and discourse units."
"For instance, given the document “John Doe has secured the vote of most democrats.” we could add words to it (namely the word “already”) to gener-ate “John Doe has already secured the vote of most democrats.”"
"We could also choose to add an en-tire syntactic constituent, for instance a prepositional phrase, to generate “John Doe has secured the vote of most democrats in his constituency.”"
These are both examples of sentence expansion as used previ-ously by Knight &amp;[REF_CITE].
"Our system, however, also has the ability to ex-pand on a core message by adding discourse con-stituents."
"For instance, it could decide to add another discourse constituent to the original summary “John Doe has secured the vote of most democrats” by CONTRAST ing the information in the summary with the uncertainty regarding the support of the gover-nor, thus yielding the text: “John Doe has secured the vote of most democrats."
"But without the support of the governor, he is still on shaky ground.”"
"As in any noisy-channel application, there are three parts that we have to account for if we are to build a complete document compression system: the channel model, the source model and the decoder."
We describe each of these below. (&quot;
"The source model assigns to a string the probabil-ity , the probability that the summary is good English."
"Ideally, the source model should disfavor ungrammatical sentences and documents containing incoherently juxtaposed sentences. )*%(&quot;"
The channel model assigns to any docu- ment/summary pair a probability .
This models the extent to which is a good + expansion of .
"For instance, if is “The mayor is now looking for re-election.”, is “The mayor is now looking for re-election.  "
"He has to secure the vote of the democrats.” and is “The major is now looking for  ,,%(&quot; .%&amp;&quot; re-election."
"Sharks have sharp teeth.”, we expect to be higher than because expands on by elaboration, while shifts to a different topic, yielding an incoherent text."
The decoder searches through all possible sum-maries of a document for the summary )*%(&quot; (&quot; that maximizes the posterior probability .
Each of these parts is described below.
The job of the source model is to assign a score to a compression independent of the original document.
"That is, the source model should measure how good English a summary is (independent of whether it is a good compression or not)."
"Currently, we use a bigram measure of quality (trigram scores were also tested but failed to make a difference), combined with non-lexicalized context-free syntac- &lt;BC;&gt;=@? )&amp;&quot; (/&quot; &amp;&quot; :&lt;&gt;; @= ? (&quot; : tic probabilities and context-free discourse probabil-ities, giving."
"It would be better to use a lexical-ized context free grammar, but that was not possible given the decoder used."
The channel model is allowed to add syntactic constituents (through a stochastic operation called constituent-expand) or discourse units (through an-other stochastic operation called EDU-expand).
Both of these operations are performed on a com-bined discourse/syntax tree called the DS-tree.
The DS-tree for Text (1) is shown in Figure 1 for refer- ence.
Suppose we start with the summary “The mayor is looking for re-election.”
"A constituent- expand operation could insert a syntactic con-stituent, such as “this year” anywhere in the syntac-tic tree of ."
"A constituent-expand operation could also add single words: for instance the word “now” [ could be added between “is” and “looking,” yielding “The mayor is now looking for re-election.”"
The probability of inserting this word is based on the syntactic structure of the node into which it’s in-serted.
"Since our constituent-expand stochastic operation simply reimplements Knight and Marcu’s model, we do not focus on them here."
We refer the reader[REF_CITE]for the details.
"In addition to adding syntactic constituents, our \ system is also able to add discourse units."
Consider the summary “John Doe has already secured the vote of most democrats in his consituency.”
"Through a sequence of discourse expansions, we can expand upon this summary to reach the original text."
A com-plete discourse expansion process that would occur starting from this initial summary to generate the original document is shown in Figure 2.
"In this figure, we can follow the sequence of steps required to generate our original text, begin-ning with our summary ."
"First, through an op-eration D-Project (“D” for “D”iscourse), we in-crease the depth of the tree, adding an intermediate  ] &quot; N UC =S PAN node."
This projection adds a factor of Nuc=Span Nuc=Span Nuc=Span to the probabil-ity of this sequence of operations (as is shown under the arrow).
"We are now able to perform the second operation, D-Expand, with which we expand on the core mes-sage contained in by adding a satellite which eval-uates the information presented in ."
"This expansion & lt;BC^ adds the probability of performing the expansion (called the discourse expansion probabilities, .  &quot ; ] ]"
"An example discourse expansion probability, writ-ten Nuc=Span Nuc=Span Sat=Eval Nuc=Span Nuc=Span , reflects the probability of adding an eval-uation satellite onto a nuclear span)."
"The rest of Figure 2 shows some of the remaining steps to produce the original document, each step la-beled with the appropriate probability factors."
"Then, the probability of the entire expansion is the prod-uct of all those listed probabilities combined with $%&amp;&quot; the appropriate probabilities from the syntax side of things."
"In order to produce the final score for a document/summary pair, we multiply together each of the expansion probabilities in the path lead-ing from to ."
"For estimating the parameters for the discourse models, we used an RST corpus of 385 Wall Street Journal articles from the Penn Treebank, which we obtained from LDC."
"The documents in the corpus range in size from 31 to 2124 words, with an av-erage of 458 words per document."
Each document is paired with a discourse structure that was manu- ally built in the style of RST. (See[REF_CITE]for details concerning the corpus and the an-notation process.)
"From this corpus, we were able to estimate parameters for a discourse PCFG using standard maximum likelihood methods."
Human annotators were asked which EDUs were most important; suppose in the example DS-tree (Figure 1) the annotators marked the second and fifth EDUs (the starred ones).
"These stars are propagated up, so that any discourse unit that has a descendent considered important is also consid-ered important."
"From these annotations, we could deduce that, to compress a N UC =C ONTRAST that has two children, N UC =S PAN and S AT = EVALUATION , we can drop the evaluation satellite."
"Similarly, we can compress a N UC =C ONTRAST that has two children, S AT = CONDITION and N UC =S PAN by dropping the first discourse constituent."
"Finally, we can compress the R OOT deriving into S AT =B ACKGROUND N UC =S PAN by dropping the S AT =B ACKGROUND constituent."
"We keep counts of each of these examples and, once col-lected, we normalize them to get the discourse ex-pansion probabilities."
") & amp;&quot; $%&amp;&quot;  %,&quot; The goal of the decoder is to combine with to get ."
"There are a vast number of potential compressions of a large DS-tree, but we can efficiently pack them into a shared-forest structure, as described in detail by Knight &amp;[REF_CITE]."
"Each entry in the shared-forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.2."
"Once we have generated a forest representing all possible compressions of the  original document, we want to extract the best (or the -best) trees, taking into account both the ex-pansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabili-ties of the source model."
"Thankfully, such a generic extractor has already been built[REF_CITE]."
"For our purposes, the extractor selects the trees with the best combination of LM and expansion scores after performing an exhaustive search over all possi-ble summaries."
"It returns a list of such trees, one for each possible length."
The system developed works in a pipelined fash-ion as shown in Figure 3.
The first step along the pipeline is to generate the discourse structure.
"To do this, we use the decision-based discourse parser described[REF_CITE][Footnote_2] ."
"2 The discourse parser achieves an f-score of for EDU   identification, for identifying hierarchical spans, for nuclearity identification and for relation tagging."
"Once we have the dis-course structure, we send each EDU off to a syn-        tactic parser[REF_CITE]."
The syntax trees of the EDUs are then merged with the discourse tree in the forest generator to create a DS-tree similar to that shown in Figure 1.
From this DS-tree we gener-ate a forest that subsumes all possible compressions.
This forest is then passed on to the forest ranking system which is used as decoder[REF_CITE].
"The decoder gives us a list of possible compressions, for each possible length."
Example compressions of Text (1) are shown in Figure 4 together with their respective log-probabilities.
"In order to choose the “best” compression at any possible length, we cannot rely only on the log-probabilities, lest the system always choose the shortest possible compression."
"In order to compen-sate for this, we normalize by length."
"However, in practice, simply dividing the log-probability by the length of the compression is insufficient for longer  documents."
"Experimentally, we found a reasonable  &apos; metric was to, for a compression of length , divide each log-probability by ."
"This was the job of the length chooser from Figure 3, and enabled us to choose a single compression for each document, which was used for evaluation. (In Figure 4, the compression chosen by the length selector is itali-cized and was the shortest one [Footnote_3] .)"
"3 This tends to be the case for very short documents, as the compressions never get sufficiently long for the length normal-ization to have an effect."
"For testing, we began with two sets of data."
"The   first set is drawn from the Wall Street Journal (WSJ)  portion of the Penn Treebank and consists of doc-uments, each containing between and words."
"The second set is drawn from a collection of stu-  .   dent compositions and consists of documents, each containing between and words."
We call this set the MITRE corpus[REF_CITE].
We would liked to have run evaluations on longer docu-ments.
"Unfortunately, the forests generated even for relatively small documents are huge."
"Because there are an exponential number of summaries that can be generated for any given text [Footnote_4] , the decoder runs out of memory for longer documents; therefore, we se-lected shorter subtexts from the original documents."
"4 In theory, a text of words has possible compressions."
We used both the WSJ and Mitre data for eval-uation because we wanted to see whether the per-formance of our system varies with text genre.
"Mitre data consists mostly of short sentences (av-erage document length from Mitre is sentences), quite in constrast to the typically long sentences in ¡ the Wall Street Journal articles (average document length from WSJ is sentences)."
"For purpose of comparison, the Mitre data was compressed using five systems:"
Random: Drops random words (each word has a 50% chance of being dropped (baseline).
NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.
NeATS is among the best performers in the large scale summarization evaluati[REF_CITE].
"In recent years, text summarization has been enjoying a period of revival."
Two workshops on Automatic Summarization were held in 2000 and 2001.
"However, the area is still being fleshed out: most past efforts have focused only on single-document summarizati[REF_CITE], and no standard test sets and large scale evaluations have been reported or made available to the English-speaking research community except the TIPSTER SUMMAC Text Summarization evaluati[REF_CITE]."
"To address these issues, the Document Understanding Conference (DUC) sponsored by the National Institute of Standards and Technology (NIST) started in 2001 in the United States."
The Text Summarization Challenge (TSC) task under the NTCIR (NII-NACSIS Test Collection for IR Systems) project started in 2000 in Japan.
DUC and TSC both aim to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants.
In this paper we describe a multi-document summarization system NeATS.
It attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.
"We outline the NeATS system and describe how it performs content selection, filtering, and presentation in Section 2."
Section 3 gives a brief overview of the evaluation procedure used[REF_CITE].
"Section 4 discusses evaluation metrics, and Section 5 the results."
We conclude with future directions.
NeATS is an extraction-based multi-document summarization system.
"It leverages techniques proved effective in single document summarization such as: term frequency[REF_CITE], sentence positi[REF_CITE], stigma words[REF_CITE], and a simplified version of MMR[REF_CITE]to select and filter content."
"To improve topic coverage and readability, it uses term clustering, a ‘buddy system’ of paired sentences, and explicit time annotation."
Most of the techniques adopted by NeATS are not new.
"However, applying them in the proper places to summarize multiple documents and evaluating the results on large scale common tasks are new."
"Given an input of a collection of sets of newspaper articles, NeATS generates summaries in three stages: content selection, filtering, and presentation."
We describe each stage in the following sections.
The goal of content selection is to identify important concepts mentioned in a document collection.
"For example,[REF_CITE]New York, World Trade Center, Twin Towers, Osama bin Laden, and al-Qaida are key concepts for a document collection about the[REF_CITE]terrorist attacks in the US. • Croatia republic"
"In a key step for locating important sentences, NeATS computes the likelihood ratio λ[REF_CITE]to identify key concepts in unigrams, bigrams, and trigrams [Footnote_1] , using the on- topic document collection as the relevant set and the off-topic document collection as the irrelevant set."
"1 Closed class words (of, in, and, are, and so on) were ignored in constructing unigrams, bigrams and trigrams."
Figure 1 shows the top 5 concepts with their relevancy scores (-[Footnote_2]λ) for the topic “Slovenia Secession from Yugoslavia” in the[REF_CITE]test collection.
2.2.3 Maximum Marginal Relevancy
This is similar to the idea of topic signature introduced[REF_CITE].
"With the individual key concepts available, we proceed to cluster these concepts in order to identify major subtopics within the main topic."
Clusters are formed through strict lexical connection.
"For example, Milan and Kucan are grouped as “Milan Kucan” since “Milan Kucan” is a key bigram concept; while Croatia, Yugoslavia, Slovenia, republic, and are joined due to the connections as follows: • Slovenia Croatia • Croatia Slovenia • Yugoslavia Slovenia • republic Slovenia"
"Each sentence in the document set is then ranked, using the key concept structures."
An example is shown in Figure 2.
"The ranking algorithm rewards most specific concepts first; for example, a sentence containing “Milan Kucan” has a higher score than a sentence contains only either Milan or Kucan."
A sentence containing both Milan and Kucan but not in consecutive order gets a lower score too.
"This ranking algorithm performs relatively well, but it also results in many ties."
"Therefore, it is necessary to apply some filtering mechanism to maintain a reasonably sized sentence pool for final presentation."
"NeATS uses three different filters: sentence position, stigma words, and maximum marginal relevancy."
Sentence position has been used as a good important content filter since the late 60s[REF_CITE].
It was also used as a baseline in a preliminary multi-document summarization study[REF_CITE]with relatively good results.
We apply a simple sentence filter that only retains the lead 10 sentences.
"Some sentences start with • conjunctions (e.g., but, although, however), • the verb sayand its derivatives, • quotation marks, • pronouns such as he, she, and they, and usually cause discontinuity in summaries."
"Since we do not use discourse level selection criteria à la[REF_CITE], we simply reduce the scores of these sentences to avoid including them in short summaries."
The content selection and filtering methods described in the previous section only concern individual sentences.
They do not consider the redundancy issue when two top ranked sentences refer to similar things.
"To address the problem, we use a simplified version of CMU’s MMR[REF_CITE]algorithm."
A sentence is added to the summary if and only if its content has less than X percent overlap with the summary.
The overlap ratio is computed using simple stemmed word overlap and the threshold X is set empirically.
NeATS so far only considers features pertaining to individual sentences.
"As we mentioned in Section 2.2.2, we can demote some sentences containing stigma words to improve the cohesion and coherence of summaries."
"However, we still face two problems: definite noun phrases and events spread along an extended timeline."
We describe these problems and our solutions in the following sections.
The problem of definite noun phrases can be illustrated in Figure 3.
These sentences are from documents of the[REF_CITE]topic US[REF_CITE].
"According to pure sentence scores, sentence 3 of document[REF_CITE]- 0079 has a higher score (34.60) than sentence 1 (32.20) and should be included in the shorter summary (size=“50”)."
"However, if we select sentence 3 without also including sentence 1, the definite noun phrase “The record $3.9 billion drought relief program of 1988” seems to come without any context."
"To remedy this problem, we introduce a buddy system to improve cohesion and coherence."
Each sentence is paired with a suitable introductory sentence unless it is already an introductory sentence.
In[REF_CITE]we simply used the first sentence of its document.
This assumes lead sentences provide introduction and context information about what is coming next.
One main problem in multi-document summarization is that documents in a collection might span an extended time period.
"For example, the[REF_CITE]topic “Slovenia Secession from Yugoslavia” contains 11 documents dated from 1988 to 1994, from 5 different sources 2 ."
"Although a source document for single-document summarization might contain information collected across an extended time frame and from multiple sources, the author at least would synchronize them and present them in a coherent order."
"In multi-document summarization, a date expression such as Monday occurring in two different documents might mean the same date or different dates."
"For example, sentences in the 100 word summary shown in Figure 4 come from 3 main time periods, 1990, 1991, and 1994."
"If no absolute time references are given, the summary might mislead the reader to think that all the events mentioned in the four summary sentences occurred in a single week."
"Therefore, time disambiguation and normalization are very important in multi-document summarization."
"As the first attempt, we use publication dates as reference points and compute actual dates for the following date expressions: • weekdays (Sunday, Monday, etc); • (past | next | coming) + weekdays; • today, yesterday, last night."
We then order the summary sentences in their chronological order.
Figure 4 shows an example 100 words summary with time annotations.
Each sentence is marked with its publication date and a reference date (MM/DD/YY) is inserted after every date expression.
"Before we present our results, we describe the corpus and evaluation procedures of the Document[REF_CITE]."
"DUC is a new evaluation series supported by NIST under TIDES, to further progress in summarization and enable researchers to participate in large-scale experiments."
"There were three tasks in 2001: (1) Fully automatic summarization of a single document. (2) Fully automatic summarization of multiple documents: given a set of document on a single subject, participants were required to create [Footnote_4] generic summaries of the entire set with approximately 50, 100, 200, and 400 words. 30 document sets of approximately 10 documents each were provided with their 50, 100, 200, and 400 human written summaries for training (training set) and another 30 unseen sets were used for testing (test set). (3) Exploratory summarization: participants were encouraged to investigate alternative approaches in summarization and report their results."
4 Do sentences in a summary fit in with their surrounding sentences?
NeATS participated only in the fully automatic multi-document summarization task.
A total of 12 systems participated in that task.
The training data were distributed in early[REF_CITE]and the test data were distributed in mid-[REF_CITE].
Results were submitted to NIST for evaluation by July 1 st .
"NIST assessors who created the ‘ideal’ written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors’ summaries, and baseline summaries."
"In addition, two baseline summaries were created automatically as reference points."
"The first baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection."
"The second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and so on until it had a summary of 50, 100, 200, or 400 words."
NIST used the Summary Evaluation Environment (SEE) 2.0 developed by one of the authors[REF_CITE]to support its human evaluation process.
"Using SEE, the assessors evaluated the quality of the system’s text (the peer text) as compared to an ideal (the model text)."
The two texts were broken into lists of units and displayed in separate windows.
In[REF_CITE]the sentence was used as the smallest unit of evaluation.
"SEE 2.0 provides interfaces for assessors to judge the quality of summaries in grammatically [Footnote_3] , cohesion 4 , and coherence [Footnote_5] at five different levels: all, most, some, hardly any, or none."
3 Does a summary follow the rule of English grammatical rules independent of its content?
5 Is the content of a summary expressed and organized in an effectiv e way?
"It also allows assessors to step through each model unit, mark all system units sharing content with the current model unit, and specify that the marked system units express all, most, some or hardly any of the content of the current model unit."
One goal[REF_CITE]was to debug the evaluation procedures and identify stable metrics that could serve as common reference points.
NIST did not define any official performance metric[REF_CITE].
It released the raw evaluation results[REF_CITE]participants and encouraged them to propose metrics that would help progress the field.
Recall at different compression ratios has been used in summarization research[REF_CITE]to measure how well an automatic system retains important content of original documents.
Assume we have a system summary S s and a model summary S m .
"The number of sentences occurring in both S s and S m is N a , the number of sentences in S s is N s , and the number of sentences in S m is N m ."
Recall is defined as N a /N m .
The Compression Ratio is defined as the length of a summary (by words or sentences) divided by the length of its original document.
"However, applying recall[REF_CITE]without modification is not appropriate because:"
Multiple system units contribute to multiple model units. 2. S s and S m do not exactly overlap. 3.
Overlap judgment is not binary.
"For example, in an evaluation session an assessor judged system units S1.1 and S10.4 as sharing some content with model unit M2.2."
"Unit S1.1 says “Thousands of people are feared dead” and unit M2.2 says “3,000 and perhaps … 5,000 people have been killed”."
"Are “thousands” equivalent to “3,000 to 5,000” or not?"
Both of them report a “6.9” earthquake.
"But the second part of system unit S10.4, “in an area so isolated…”, seems to share some content with model unit M4.4 “the quake was centered in a remote mountainous area”."
Are these two equivalent?
This example highlights the difficulty of judging the content coverage of system summaries against model summaries and the inadequacy of using recall as defined.
"As we mentioned earlier, NIST assessors not only marked the sharing relations among system units (SU) and model units (MU), they also indicated the degree of match, i.e., all, most, some, hardly any, or none."
This enables us to compute weighted recall.
Different versions of weighted recall were proposed[REF_CITE]participants.
They then proceeded to compare system performances at different threshold levels.
"They defined recall at threshold t, Recall t , as follows:"
Number of MUs marked at or abovet Totalnumber of MUs in the modelsummary
"We used the completeness of coverage as coverage score, C, instead of threshold: 1 for all, 3/4 for most, 1/2 for some, and 1/4 for hardly any, 0 for none."
"To avoid confusion with the recall used in information retrieval, we call our metric weighted retention, Retention w , and define it as follows: (Number of MUs marked)• C"
"Totalnumber of MUs in the modelsummary if we ignore C and set it always to 1, we obtain an unweighted retention, Retention 1 ."
We used Retention 1 in our evaluation to illustrate that relative system performance changes when different evaluation metrics are chosen.
"Therefore, it is important to have common and agreed upon metrics to facilitate large scale evaluation efforts."
Precision is also a common measure.
"Borrowed from information retrieval research, precision is used to measure how effectively a system generates good summary sentences."
It is defined as N a / N s .
Precision in a fixed length summary output is equal to recall since N s = N m .
"However, due to the three reasons stated at the beginning of the previous section, no straightforward computation of the traditional precision is available[REF_CITE]."
"If we count the number of model units that are marked as good summary units and are selected by systems, and use the number of model units in various summary lengths as the sample space, we obtain a precision metric equal to Retention 1 ."
"Alternatively, we can count how many unique system units share content with model units and use the total number of system units as the sample space."
"We define this as pseudo precision, Precision p , as follows:"
Number of SUs marked Totalnumber of SUs in thesystemsummary
Most of the participants[REF_CITE]reported their pseudo precision figures.
We present the performance of NeATS[REF_CITE]in content and quality measures.
"With respect to content, we computed Retention 1 , Retention w , and Precision p using the formulas defined in the previous section."
The scores are shown in Table 1 (overall average and per size).
"Analyzing all systems’ results according to these, we made the following observations. (1) NeATS (system N) is consistently ranked among the top 3 in average and per size Retention 1 and Retention w . (2) NeATS’s performance for averaged pseudo precision equals human’s at about 58% (P pall ). (3) The performance in weighted retention is really low."
Even humans [Footnote_6] score only 29% (R w all ).
6 NIST assessors wrote two separate summaries per topic. One was used to judge all system summaries and the two baselines. The other was used to determine the (potential) upper bound.
This indicates low inter-human agreement (which we take to reflect the undefinedness of the ‘generic summary’ task).
"However, the unweighted retention of humans is 53%."
This suggests assessors did write something similar in their summaries but not exactly the same; once again illustrating the difficulty of summarization evaluation. (4)
"Despite the low inter-human agreement, humans score better than any system."
They outscore the nearest system by about 11% in averaged unweighted retention (R 1 all : 53% vs. 42%) and weighted retention (R w all : 29% vs. 18%).
There is obviously still considerable room for systems to improve. (5) System performances are separated into two major groups by baseline 2 (B2: coverage baseline) in averaged weighted retention.
This confirms that lead sentences are good summary sentence candidates and that one does need to cover all documents in a topic to achieve reasonable performance in multi-document summarization.
NeATS’s strategies of filtering sentences by position and adding lead sentences to set context are proved effective. (6) Different metrics result in different performance rankings.
"This is demonstrated by the top 3 systems T, N, and Y. If we use the averaged unweighted retention (R 1 all ), Y is the best, followed by N, and then T; if we choose averaged weighted retention (R w all ), T is the best, followed by N, and then Y. The reversal of T and Y due to different metrics demonstrates the importance of common agreed upon metrics."
"We believe that metrics have to take coverage score (C, Section 4.1.1) into consideration to be reasonable since most of the content sharing among system units and model units is partial."
"The recall at threshold t, Recall t (Section 4.1.1), proposed[REF_CITE], is a good example."
"In their evaluation, NeATS ranked second at t=1, 3, 4 and first at t=2. (7) According to Table 1, NeATS performed better on longer summaries (400 and 200 words) based on weighted retention than it did on shorter ones."
This is the result of the sentence extraction-based nature of NeATS.
We expect that systems that use syntax-based algorithms to compress their output will thereby gain more space to include additional important material.
"For example, System Y was the best in shorter summaries."
The results confirm this is a very effective strategy in composing short summaries.
"However, the quality of the summaries suffered because of the unconventional syntactic structure of news headlines (Table 2)."
"Table 2 shows the macro-averaged scores for the humans, two baselines, and 12 systems."
"We assign a score of 4 to all, 3 to most, 2 to some, 1 to hardly any, and 0 to none."
"The value assignment is for convenience of computing averages, since it is more appropriate to treat these measures as stepped values instead of continuous ones."
"With this in mind, we have the following observations. (1) Most systems scored well in grammaticality."
This is not a surprise since most of the participants extracted sentences as summaries.
But no system or human scored perfect in grammaticality.
"This might be due to the artifact of cutting sentences at the 50, 100, 200, and 400 words boundaries."
"Only system Y scored lower than 3, which reflects its headline inclusion strategy. (2) When it came to the measure for cohesion the results are confusing."
"If even the human-made summaries score only 2.74 out of 4, it is unclear what this category means, or how the assessors arrived at these scores."
"However, the humans and baseline 1 (lead baseline) did score in the upper range of 2 to 3 and all others had scores lower than 2.5."
Some of the systems (including B2) fell into the range of 1 to 2 meaning some or hardly any cohesion.
"The lead baseline (B1), taking the first 50, 100, 200, 400 words from the last document of a topic, did well."
"On the contrary, the coverage baseline (B2) did poorly."
This indicates the difficulty of fitting sentences from different documents together.
Even selecting continuous sentences from the same document (B1) seems not to work well.
We need to define this metric more clearly and improve the capabilities of systems in this respect. (3) Coherence scores roughly track cohesion scores.
Most systems did better in coherence than in cohesion.
The human is the only one scoring above 3.
Again the room for improvement is abundant. (4) NeATS did not fare badly in quality measures.
"It was in the same categories as other top performers: grammaticality is between most and all, cohesion, some and most, and coherence, some and most."
"This indicates the strategies employed by NeATS (stigma word filtering, adding lead sentence, and time annotation) worked to some extent but left room for improvement."
"We described a multi-document summarization system, NeATS, and its evaluation[REF_CITE]."
We were encouraged by the content and readability of the results.
"As a prototype system, NeATS deliberately used simple methods guided by a few principles: • Extracting important concepts based on reliable statistics. • Filtering sentences by their positions and stigma words. • Reducing redundancy using MMR. • Presenting summary sentences in their chronological order with time annotations."
These simple principles worked effectively.
"However, the simplicity of the system also lends itself to further improvements."
We would like to apply some compression techniques or use linguistic units smaller than sentences to improve our retention score.
The fact that NeATS performed as well as the human in pseudo precision but did less well in retention indicates its summaries might include good but duplicated information.
Working with sub-sentence units should help.
"To improve NeATS’s capability in content selection, we have started to parse sentences containing key unigram, bigram, and trigram concepts to identify their relations within their concept clusters."
"To enhance cohesion and coherence, we are looking into incorporating discourse processing techniques[REF_CITE]or Radev and McKeown’s (1998) summary operators."
We are analyzing the DUC evaluation scores in the hope of suggesting improved and more stable metrics.
The paper proposes and empirically moti-vates an integration of supervised learning with unsupervised learning to deal with human biases in summarization.
"In par-ticular, we explore the use of probabilistic decision tree within the clustering frame-work to account for the variation as well as regularity in human created summaries."
The corpus of human created extracts is created from a newspaper corpus and used as a test set.
We build probabilistic de-cision trees of different flavors and in-tegrate each of them with the clustering framework.
Experiments with the cor-pus demonstrate that the mixture of the two paradigms generally gives a signif-icant boost in performance compared to cases where either of the two is considered alone.
"That appears somewhat con-tradictory given that a supervised approach should be able to exploit human supplied information about which sentence to include in an extract and which not to, whereas an unsupervised approach blindly chooses sentences according to some selection scheme."
"An interesting question is, why this should be the case."
The reason may have to do with the variation in human judgments on sentence selection for a sum-mary.
"In a study to be described later, we asked stu-dents to select 10% of a text which they find most important for making a summary."
"If they agree per-fectly on their judgments, then we will have only 10% of a text selected as most important."
"However, what we found was that about half of a text were marked as important, indicating that judgments can vary widely among humans."
"Curiously, however,[REF_CITE]also found that a supervised system fares much better when tested on data exhibiting high agreement among humans than an unsupervised sys-tem."
Their finding suggests that there are indeed some regularities (or biases) to be found.
So we might conclude that there are two aspects to human judgments in summarization; they can vary but may exhibit some biases which could be usefully exploited.
The issue is then how we might model them in some coherent framework.
The goal of the paper is to explore a possible in-tegration of supervised and unsupervised paradigms as a way of responding to the issue.
"Taking a de-cision tree and clustering as representing the respec-tive paradigm, we will show how coupling them pro-vides a summarizer that better approximates human judgments than either of the two considered alone."
"To our knowledge, none of the prior work on sum-marization (e.g.,[REF_CITE]) explicitly ad-dressed the issue of the variability inherent in human judgments in summarization tasks."
"One technical problem associated with the use of a decision tree as a summarizer is that it is not able to rank sentences, which it must be able do, to allow for the generation of a variable-length summary."
"In re-sponse to the problem, we explore the use of a prob-abilistic decision tree as a ranking model."
"First, let us review some general features of probabilistic de-cision tree (ProbDT, henceforth)[REF_CITE]."
"ProbDT works like a usual decision tree except that rather than assigning each instance to a single class, it distributes each instance among classes."
"For each instance x i , the strength of its membership to each of the classes is determined by P(c k | x i ) for each class c k ."
Consider a binary decision tree in Fig 1.
"Let X 1 and X 2 represent non-terminal nodes, and Y 1 and Y 2 leaf nodes. ‘1’ and ‘0’ on arcs denote values of some attribute at X 1 and X 2 . θ iy and θ in repre-sent the probability that a given instance assigned to the node i is labeled as yes and no, repectively."
"Abusing the terms slightly, let us assume that X 1 and X 2 represent splitting attributes as well at respective nodes."
Then the probability that a given instance with X 1 = 1 and XP 2 = 0 is labeled as yes (no) is θ y2 (θ n2 ).
Note that c θ jc = 1 for a given node j.
"Now to rank sentences with ProbDT simply in-volves finding the probability that each sentence is assigned to a particular class designating sentences worthy of inclusion in a summary (call it ‘Select’ class) and ranking them accordingly. (Hereafter and throughout the rest of the paper, we say that a sen-tence is wis if it is worthy of inclusion in a summary: thus a wis sentence is a sentence worthy of inclusion in a summary.)"
"The probabiliy that a sentence u is labeled as wis is expressed as in Table 1, where ~u is a vector representation of u, consisting of a set of values for features of u; α is a smoothing function, e.g., Laplace’s law; t(u~) is some leaf node assigned to ~u; and DT represents some decision tree used to classify ~u."
"As an unsupervised summarizer, we use diversity based summarization (DBS)[REF_CITE]."
It takes a cluster-and-rank approach to generating summaries.
The idea is to form a sum-mary by collecting sentences representative of di-verse topics discussed in the text.
"A nice feature about their approach is that by creating a summary covering potential topics, which could be marginal to the main thread of the text, they are in fact able to accommodate the variability in sentence selection: some people may pick up subjects (sentences) as important which others consider irrelevant or only marginal for summarization."
"DBS accomodates this situation by picking them all, however marginal they might be."
"More specifically, DBS is a tripartite process con-sisting of the following: 1. Find-Diversity: find clusters of lexically sim-ilar sentences in text. (In particular, we repre-sent a sentence here a vector of tfidf weights of index terms it contains.) 2."
"Reduce-Redundancy: for each cluster found, choose a sentence that best represents that clus-ter. 3."
"Generate-Summary: collect the representa-tive sentences, put them in some order, and re-turn them to the user."
"Find-Diversity is based on the K-means clustering algorithm, which they extended with Minimum De-scription Length Principle (MDL)[REF_CITE]as a way of optimiz-ing K-means."
"Reduce-Redundancy is a tfidf based ranking model, which assigns weights to sentences in the cluster and returns a sentence that ranks high-est."
The weight of a sentence is given as the sum of tfidf scores of terms in the sentence.
Combining ProbDT and DBS is done quite straight-forwardly by replacing Reduce-Redundacy with ProbDT.
"Thus instead of picking up a sentence with the highest tfdif based weight, DBS/ProbDT at-tempts to find a sentences with the highest score for P (Select | u~, DT)."
The following lists a set of features used for encod-ing a sentence in ProbDT.
Most of them are either length- or location-related features. 1 &lt;LocSen&gt;
"The location of a sentence X defined by: #S(X) − 1 #S(Last Sentence) ‘#S(X)’ denotes an ordinal number indicating the position of X in a text, i.e. #S(kth sentence) = k. ‘Last Sentence’ refers to the last sentence in a text."
LocSen takes values between 0 and NN−[Footnote_1] .
"1 Note that one may want to add tfidf to a set of features for a decision tree or, for that matter, to use features other than tfidf for representing sentences in clustering. The idea is worthy of consideration, but not pursued here."
N is the number of sentences in the text. & lt;LocPar&gt;
"The location of a paragraph in which a sentence X occurs given by: #P ar(X) − 1 #Last P aragraph ‘#Par(X)’ denotes an ordinal number indicat-ing the position of a paragraph containing X. ‘#Last Paragraph’ is the position of the last para-graph in a text, represented by the ordinal number. & lt;LocWithinPar&gt;"
"The location of a sentence X within a paragraph in which it appears. #S(X) − #S(P ar Init Sen) Length(P ar(X)) ‘Par Init Sen’ refers to the initial sentence of a para-graph in which X occurs, ‘Length(Par(X))’ denotes the number of sentences that occur in that paragraph."
"LocWithinPar takes continuous values ranging from 0 to l−1l , where l is the length of a paragraph: a paragraph initial sentence would have 0 and a para-graph final sentence l−1l . &lt;LenText&gt; The text length in Japanese charac-ter i.e. kana, kanji."
"To examine the generality of our approach, we con-sider, in addition to C4.5[REF_CITE], the fol-lowing decision tree algorithms."
"C4.5 is used with default options, e.g., CF=25%."
MDL-DT stands for a decision tree with MDL based pruning.
It strives to optimize the decision tree by pruning the tree in such a way as to produce the shortest (minimum) description length for the tree.
The description length refers to the num-ber of bits required for encoding information about the decision tree.
"MDL ranks, along with Akaike Information Criterion (AIC) and Bayes Informa-tion Criterion (BIC), as a standard criterion in ma-chine learning and statistics for choosing among possible (statistical) models."
"As shown empirically[REF_CITE]for discourse do-main, pruning DT with MDL significantly reduces the size of tree, while not compromising perfor-mance."
SSDT or Subspace Splitting Decision Tree repre-sents another form of decision tree algorithm.[REF_CITE]
"The goal of SSDT is to discover pat-terns in highly biased data, where a target class, i.e., the class one likes to discover something about, ac-counts for a tiny fraction of the whole data."
"Note that the issue of biased data distribution is particularly relevant for summarization, as a set of sentences to be identified as wis usually account for a very small portion of the data."
SSDT begins by searching the entire data space for a cluster of positive cases and grows the cluster by adding points that fall within some distance to the center of the cluster.
"If the splitting based on the cluster offers a better Gini index than simply using positive class, white circles represent negative class."
"SSDT starts with a small spherical cluster of pos-itive points (solid circle) and grows the cluster by ‘absorbing’ positive points around it (dashed circle). one of the attributes to split the data, SSDT splits the data space based on the cluster, that is, forms one re-gion outside of the cluster and one inside. [Footnote_3] It repeats the process recursively on each subregions spawned until termination conditions are met."
"3 For a set S of data P with k classes, its Gini index is given as: Gini(S) = 1 − ki p 2i , where p i denotes the probability of observing class i in S."
Figure 2 gives a snapshot of SSDT at work.
"SSDT locates some clusters of positive points, develops spherical clus-ters around them."
"With its particular focus on positive cases, SSDT is able to provide a more precise characterization of them, compared, for instance, to C4.5."
"The number of sentences to extract varied from two to four, depending on the length of a text."
The age of subjects varied from 18 to 45.
"Texts were of about the same size in terms of character counts and the number of para-graphs, and were selected randomly from articles that appeared in a Japanese financial daily (Nihon-Keizai-[REF_CITE])."
"There were, on aver-age, 19.98 sentences per text."
The kappa agreement among subjects was 0.25.
"The result is in a way consistent[REF_CITE], who report a low inter-subject agreement on paragraph extracts from encyclope-dias and also[REF_CITE]on a sen-tence selection task in the cable news domain."
"While there are some work[REF_CITE]which do report high agreement rates, their success may be attributed to particularities of texts used, as suggested[REF_CITE]."
"Thus, the question of whether it is possible to establish an ideal sum-mary based on agreement is far from settled, if ever."
"In the face of this, it would be interesting and per-haps more fruitful to explore another view on sum-mary, that the variability of a summary is the norm rather than the exception."
"In the experiments that follow, we decided not to rely on a particular level of inter-coder agree-ment to determine whether or not a given sentence is wis."
"Instead, we used agreement threshold to dis-tinguish between wis and non-wis sentences: for a given threshold K, a sentence is considered wis (or positive) if it has at least K votes in favor of its in-clusion in a summary, and non-wis (negative) if not."
"Thus if a sentence is labeled as positive at K ≥ 1, it means that there are one or more judges taking that sentence as wis."
"We examined K from 1 to 5. (On average, seven people are assigned to one arti-cle."
"However, one would rarely see all of them unan-imously agree on their judgments.)"
Table 3 shows how many positive/negative in-stances one would get at a given agreement thresh-old.
"At K ≥ 1, out of 1424 instances, i.e., sen-tences, 707 of them are marked positive and 717 are marked negative, so positive and negative instances are evenly spread across the data."
"On the other hand, at K ≥ 5, there are only 72 positive instances."
This means that there is less than one occurrence of wis case per article.
"In the experiments below, each probabilistic ren-dering of the DTs, namely, C4.5, MDL-DT, and SSDT is trained on the corpus, and tested with and without the diversity extension (Find-Diversity)."
"When used without the diversity component, each ProbDT works on a test article in its entirety, pro-ducing the ranked list of sentences."
A summary with compression rate γ is obtained by selecting top γ percent of the list.
"When coupled with Find-Diversity, on the other hand, each ProbDT is set to work on each cluster discovered by the diversity component, producing multiple lists of sentences, each corresponding to one of the clusters identified."
A summary is formed by collecting top ranking sen-tences from each list.
Evaluation was done by 10-fold cross vali-dation.
"For the purpose of comparison, we also ran the diversity based model as given[REF_CITE]and a tfidf based ranking model[REF_CITE](call it Z model), which simply ranks sentences according to the tfidf score and selects those which rank highest."
"Recall that the diversity based model (DBS)[REF_CITE]consists in Find-Diversity and the ranking model[REF_CITE], which they call Reduce-Redundancy."
Tables 4-8 show performance of each ProbDT and its combination with the diversity (clustering) com-ponent.
It also shows performance of Z model and DBS.
"In the tables, the slashed ‘V’ after the name of a classifier indicates that the relevant classifier is diversity-enabled, meaning that it is coupled with the diversity extension."
Notice that each decision tree here is a ProbDT and should not be confused with its non-probabilistic counterpart.
"Also worth noting is that DBS is in fact Z/V, that is, diversity-enabled Z model."
"Returning to the tables, we find that for most of the times, the diversity component has clear ef-fects on ProbDTs, significantly improving their per-formance."
"All the figures are in F-measure, i.e.,"
F = 2∗PP+∗RR .
"In fact this happens regardless of a par-ticular choice of ranking model, as performance of Z is also boosted with the diversity component."
"Not surprisingly, effects of supervised learning are also evident: diversity-enabled ProbDTs generally out-perform DBS (Z/V) by a large margin."
"What is sur-prising, moreover, is that diversity-enabled ProbDTs are superior in performance to their non-diversity counterparts (with a notable exception for SSDT at K ≥ 1), which suggests that selecting marginal sen-tences is an important part of generating a summary."
"Another observation about the results is that as one goes along with a larger K, differences in per-formance among the systems become ever smaller: at K ≥ 5, Z performs comparably to C4.5, MDL, and SSDT either with or without the diversity com-ponent."
The decline of performance of the DTs may be caused by either the absence of recurring patterns in data with a higher K or simply the paucity of positive instances.
"At the moment, we do not know which is the case here."
"It is curious to note, moreover, that MDL-DT is not performing as well as C4.5 and SSDT at K ≥ 1, K ≥ 2, and K ≥ 3."
The reason may well have to do with the general properties of MDL-DT.
Re-call that MDL-DT is designed to produce as small a decision tree as possible.
"Therefore, the resulting tree would have a very small number of nodes cov-ering the entire data space."
"Consider, for instance, a hypothetical data space in Figure 3."
"Assume that MDL-DT bisects the space into region A and B, pro-ducing a two-node decision tree."
"The problem with the tree is, of course, that point x and y in region B will be assigned to the same probability under the probabilistic tree model, despite the fact that point x is very close to region A and point y is far out."
"This problem could happen with C4.5, but in MDL-DT, which covers a large space with a few nodes, points in a region could be far apart, making the problem more acute."
Thus the poor performance of MDL-DT may be attributable to its extensive use of pruning.
"As a way of exploiting human biases towards an in-creased performance of the summarizer, we have ex-plored approaches to embedding supervised learn-ing within a general unsupervised framework."
"In the paper, we focused on the use of decision tree as a plug-in learner."
"We have shown empirically that the idea works for a number of decision trees, including C4.5, MDL-DT and SSDT."
"Coupled with the learn-ing component, the unsupervised summarizer based on clustering significantly improved its performance on the corpus of human created summaries."
"More importantly, we found that supervised learners per-form better when coupled with the clustering than when working alone."
We argued that that has to do with the high variation in human created summaries: the clustering component forces a decision tree to pay more attention to sentences marginally relevant to the main thread of the text.
"While ProbDTs appear to work well with rank-ing, it is also possible to take a different approach: for instance, we may use some distance metric in in-stead of probability to distinguish among sentences."
It would be interesting to invoke the notion like pro-totype modeler[REF_CITE]and see how it might fare when used as a ranking model.
"Moreover, it may be worthwhile to explore some non-clustering approaches to representing the diversity of contents of a text, such[REF_CITE]’s summarizer 1 (GLS1, for short), where a sentence is selected on the basis of its similarity to the text it belongs to, but which ex-cludes terms that appear in previously selected sen-tences."
"While our preliminary study indicates that GLS1 produces performance comparable and even superior to DBS on some tasks in the document re-trieval domain, we have no results available at the moment on the efficacy of combining GLS1 and ProbDT on sentence extraction tasks."
"Finally, we note that the test corpus used for evaluation is somewhat artificial in the sense that we elicit judgments from people on the summary-worthiness of a particular sentence in the text."
"Per-haps, we should look at naturally occurring ab-stracts or extracts as a potential source for train-ing/evaluation data for summarization research."
"Be-sides being natural, they usually come in large num-ber, which may alleviate some concern about the lack of sufficient resources for training learning al-gorithms in summarization."
"This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities."
"Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature."
"In this way, the NER problem can be resolved effectively."
Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.
It shows that the performance is significantly better than reported by any other machine-learning system.
"Moreover, the performance is even consistently better than those based on handcrafted rules."
Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.
"In the taxonomy of computational linguistics tasks, it falls under the domain of &quot;information extraction&quot;, which extracts specific kinds of information from documents as opposed to the more general task of &quot;document management&quot; which seeks to extract all of the information found in a document."
"Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management."
"The atomic elements of information extraction -- indeed, of language as a whole -- could be considered as the &quot;who&quot;, &quot;where&quot; and &quot;how much&quot; in a sentence."
"NER performs what is known as surface parsing, delimiting sequences of tokens that answer these important questions."
"NER can also be used as the first step in a chain of processors: a next level of processing could relate two or more NEs, or perhaps even give semantics to that relationship using a verb."
"In this way, further processing could discover the &quot;what&quot; and &quot;how&quot; of a sentence or body of text."
"While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance."
"There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues."
"During last decade, NER has drawn more and more attention from the NE tasks [[REF_CITE]] [[REF_CITE]] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups."
"Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher."
"Typical systems are Univ. of Sheffield&apos;s LaSIE-II [Humphreys+98],"
ISOQuest&apos;s NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s LTG [Mikheev+98] [Mikheev+99] for English NER.
These systems are mainly rule-based.
"However, rule-based approaches lack the ability of coping with the problems of robustness and portability."
Each new source of text requires significant tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep.
"The current trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one."
"The representative machine-learning approaches used in NER are HMM (BBN&apos;s IdentiFinder in [Miller+98] [Bikel+99] and KRDL&apos;s system [Yu+98] for Chinese NER.), Maximum Entropy (New York Univ.&apos;s MEME in [Borthwick+98] [[REF_CITE]]) and Decision Tree (New York Univ.&apos;s system in [[REF_CITE]] and SRA&apos;s system in [Bennett+97])."
"Besides, variant ofa Eric Brill&apos;s transformation-based rules [[REF_CITE]] has been applied to the problem [Aberdeen+95]."
"Among these approaches, the evaluation performance of HMM is higher than those of others."
"The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text."
"Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [[REF_CITE]] used in decoding the NE-class state sequence."
"However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [[REF_CITE]] [[REF_CITE]]."
"This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts."
"As defined in [[REF_CITE]], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above."
The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context.
"In order to effectively apply and integrate internal and external evidences, we present a NER system using a HMM."
"The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL&apos;2000 [Tjong+00]."
"Here, a NE is regarded as a chunk, named &quot;NE-Chunk&quot;."
"To date, our system has been successfully trained and applied in English NER."
"To our knowledge, our system outperforms any published machine-learning systems."
"Moreover, our system even outperforms any published rule-based systems."
The layout of this paper is as follows.
Section 2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.
Section 3 explains the word feature used to capture both the internal and external evidences.
Section 4 describes the back-off schemes used to tackle the sparseness problem.
Section 5 gives the experimental results of our system.
Section 6 contains our remarks and possible extensions of the proposed work.
"Given a token sequence G 1n = g 1 g 2 Lg n , the goal of NER is to find a stochastic optimal tag sequence T 1n = t 1 t 2 Lt n that maximizes (2-1)"
"P(T 1n ,G 1n ) log P(T 1n | G 1n ) = log P(T 1n ) + log P(T n[Footnote_1] ) ⋅ P(G 1n )"
"1 In traditional HMM to maximise logP(T 1n | G 1n ) , first we apply Bayes&apos; rule:"
The second item in (2-1) is the mutual information between T 1n and G 1n .
"In order to simplify the computation of this item, we assume mutual information independence: n MI(T 1n ,G 1n ) = ∑ MI(t i ,G 1n ) or (2-2) i=1 log P(T 1n ,G 1n ) = ∑ n log P(t i ,G 1n ) (2-3)"
P(T 1n ) ⋅ P(G 1n ) i=1
P(t i ) ⋅ P(G 1n )
"Applying it to equation (2.1), we have: n log P(T 1n | G 1n ) = log P(T 1n ) − ∑ log P(t i ) i=1 (2-4) n + ∑ log P(t i | G 1n ) i=1"
"The basic premise of this model is to consider the raw text, encountered when decoding, as though it had passed through a noisy channel, where it had been originally marked with NE tags."
The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.
"It is obvious that our generative model is reverse to the generative model of traditional HMM 1 , as used"
"P(T 1 n |G 1 n ) = P(T 1 n ,G 1 n ) P(G 1 n ) and have: in BBN&apos;s IdentiFinder, which models the original process that generates the NE-class annotated words from the original NE tags."
Another difference is that our model assumes mutual information independence (2-2) while traditional HMM assumes conditional probability independence (I-1).
Assumption (2-2) is much looser than assumption (I-1) because assumption (I-1) has the same effect with the sum of assumptions (2-2) and (I-3) 2 .
"In this way, our model can apply more context information to determine the tag of current token."
"From equation (2-4), we can see that: 1) The first item can be computed by applying chain rules."
"In ngram modeling, each tag is assumed to be probabilistically dependent on the N-1 previous tags. 2) The second item is the summation of log probabilities of all the individual tags. 3) The third item corresponds to the &quot;lexical&quot; component of the tagger."
We will not discuss both the first and second items further in this paper.
"This paper will focus on n the third item ∑ log P(t i | G 1n ) , which is the main i=1 difference between our tagger and other traditional HMM-based taggers, as used in BBN&apos;s IdentiFinder."
"Ideally, it can be estimated by using the forward-backward algorithm [[REF_CITE]] recursively for the 1 st -order [[REF_CITE]] or 2 nd -order HMMs [Watson+92]."
"However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4)."
argmaxlog P(T 1n |G 1n )
T = argmax(log P(G 1n |T 1n ) + log P(T 1n ))
T Then we assume conditional probability n independence: P(G 1n |T 1n ) = ∏
P(g i |t i ) (I-1) i=1 and have: argmaxlog P(T 1n |G 1n )
T n (I-2) = argmax( ∑ log P(g i |t i ) + log P(T 1n ))
"For NE-chunk tagging, we have token g i =&lt; f i ,w i &gt; , where W 1n = w 1 w 2 Lw n is the word sequence and F 1n = f 1 f 2 L f n is the word-feature sequence."
"In the meantime, NE-chunk tag t i is structural and consists of three parts: 1) Boundary Category : BC = {0, 1, [Footnote_2], 3}."
2 We can obtain equation (I-2) from (2.4) by assuming log P(t i |G n ) = log P(g i |t i ) (I-3) 1
Here 0 means that current word is a whole entity and 1/2/3 means that current word is at the beginning/in the middle/at the end of an entity. 2) Entity Category : EC.
This is used to denote the class of the entity name. 3) Word Feature : WF.
"Because of the limited number of boundary and entity categories, the word feature is added into the structural tag to represent more accurate models."
"Obviously, there exist some constraints between t i−1 and t i on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence t i−1 t i is valid / invalid while &quot;valid on&quot; means t i−1 t i is valid with an additional condition EC i−1 = EC i ."
Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.
"As stated above, token is denoted as ordered pairs of word-feature and word itself: g i =&lt; f i ,w i &gt; ."
"Here, the word-feature is a simple deterministic computation performed on the word and/or word string with appropriate consideration of context as looked up in the lexicon or added to the context."
"In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features."
The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.
"Our model captures three types of internal sub-features: 1) f 1 : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2 : internal semantic feature of important triggers; 3) f [URL_CITE] : internal gazetteer feature. 1) f 1 is the basic sub-feature exploited in this model, as shown in Table 2 with the descending order of priority."
"For example, in the case of non-disjoint feature classes such as ContainsDigitAndAlpha and ContainsDigitAndDash, the former will take precedence."
"The first eleven features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates."
The rest of the features distinguish types of capitalization and all other words such as punctuation marks.
"In particular, the FirstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that AllCaps and CapPeriod are computed before FirstWord, and take precedence.)"
This sub-feature is language dependent.
"Fortunately, the feature computation is an extremely small part of the implementation."
"This kind of internal sub-feature has been widely used in machine-learning systems, such as BBN&apos;s IdendiFinder and New York Univ.&apos;s MENE."
"The rationale behind this sub-feature is clear: a) capitalization gives good evidence of NEs in Roman languages; b) Numeric symbols can automatically be grouped into categories. 2) f 2 is the semantic classification of important triggers, as seen in Table 3, and is unique to our system."
It is based on the intuitions that important triggers are useful for NER and can be classified according to their semantics.
This sub-feature applies to both single word and multiple words.
"This set of triggers is collected semi-automatically from the NEs and their local context of the training data. 3) Sub-feature f 3 , as shown in Table 4, is the internal gazetteer feature, gathered from the look-up gazetteers: lists of names of persons, organizations, locations and other kinds of named entities."
This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string.
"In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive 3 , a list of 10,000 organization names from websites such as Yahoo [URL_CITE] and a list of 10,000 famous people from websites such as Scope Systems [URL_CITE] ."
Gazetters have been widely used in NER systems to improve performance.
"For external evidence, only one external macro context feature f 4 , as shown in Table 5, is captured in our model. f 4 is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.)."
This sub-feature is unique to our system.
The intuition behind this is the phenomena of name alias.
"During decoding, the NEs already recognized from the document are stored in a list."
"When the system encounters a NE candidate, a name alias algorithm is invoked to dynamically determine its relationship with the NEs in the recognized list."
"Initially, we also consider part-of-speech (POS) sub-feature."
"However, the experimental result is disappointing that incorporation of POS even decreases the performance by 2%."
"This may be because capitalization information of a word is submerged in the muddy of several POS tags and the performance of POS tagging is not satisfactory, especially for unknown capitalized words (since many of NEs include unknown capitalized words.)."
"Therefore, POS is discarded."
"Given the model in section 2 and word feature in section 3, the main problem is how to n compute ∑ P(t i /G 1n ) ."
"Ideally, we would have i=1 sufficient training data for every event whose conditional probability we wish to calculate."
"Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above."
"In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate P ( t i / G 1n ) : 1) First level back-off scheme is based on different contexts of word features and words themselves, and G 1n in P ( t i / G 1n ) is approximated in the descending order of f i−2 f i−1 f i w i , f i w i f i+1 f i+2 , f i−1 f i w i , f i w i f i+1 , f i−1 w i−1 f i , f i f i+1 w i+1 , f i−2 f i−1 f i , f i f i+1 f i+2 , f i w i , f i−2 f i−1 f i , f i f i+1 and f i . 2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and f k is approximated in the descending order of f k1 f k2 f k3 f k4 , f k1 f k3 , f k 1 f k 4 , f k 1 f k 2 and f k 1 ."
"In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data."
"For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data."
"For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others."
"Here, the precision (P) measures the number of correct NEs in the answer file over the total number of NEs in the answer file and the recall (R) measures the number of correct NEs in the answer file over the total number of NEs in the key file while F-measure is the weighted harmonic mean of precision and recall:"
F = ( β 2 + 1) RP with β 2 =1.
It shows that the β 2 R + P performance is significantly better than reported by any other machine-learning system.
"Moreover, the performance is consistently better than those based on handcrafted rules."
Composition F P R 77.6 81.0 74.1 f = f 1 87.4 88.6 86.1 f = f 1 f 2 89.3 90.5 88.2 f = f 1 f 2 f 3 92.9 92.6 93.1 f = f 1 f 2 f 4 94.1 93.7 94.5 f = f 1 f 2 f 3 f 4
"With any learning technique, one important question is how much training data is required to achieve acceptable performance."
More generally how does the performance vary as the training data size changes?
The result is shown in Figure 2 for MUC-7 NE task.
It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.
It also shows that our system still has some room for performance improvement.
This may be because of the complex word feature and the corresponding sparseness problem existing in our system.
Another important question is about the effect of different sub-features.
"Table 8 answers the question on MUC-7 NE task: 1) Applying only f 1 gives our system the performance of 77.6%. 2) f 2 is very useful for NER and increases the performance further by 10% to 87.4%. 3) f 4 is impressive too with another 5.5% performance improvement. 4) However, f 3 contributes only further 1.2% to the performance."
This may be because information included in f 3 has already been captured by f 2 and f 4 .
"Actually, the experiments show that the contribution of f 3 comes from where there is no explicit indicator information in/around the NE and there is no reference to other NEs in the macro context of the document."
"The NEs contributed by f 3 are always well-known ones, e.g. Microsoft, IBM and Bach (a composer), which are introduced in texts without much helpful context."
"This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes&apos; rule, is applied."
"Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem."
It also shows that our NER system can reach &quot;near human performance&quot;.
"To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system."
"While the experimental results have been impressive, there is still much that can be done potentially to improve the performance."
"In the near feature, we would like to incorporate the following into our system: • List of domain and application dependent person, organization and location names. • More effective name alias algorithm. • More effective strategy to the back-off modeling and smoothing."
This paper describes how a machine-learning named entity recognizer (NER) on upper case text can be improved by us-ing a mixed case NER and some unlabeled text.
"The mixed case NER can be used to tag some unlabeled mixed case text, which are then used as additional training mate-rial for the upper case NER."
"We show that this approach reduces the performance gap between the mixed case NER and the upper case NER substantially, by 39% for MUC-6 and 22% for MUC-7 named en-tity test data."
"Our method is thus useful in improving the accuracy of NERs on up-per case text, such as transcribed text from automatic speech recognizers where case information is missing."
"In this paper, we propose using a mixed case named entity recognizer (NER) that is trained on labeled text, to further train an upper case NER."
"In the Sixth and Seventh Message Understanding Confer-ences[REF_CITE], the named entity task consists of labeling named entities with the classes PERSON, ORGANIZATION, LOCA-TION, DATE, TIME, MONEY, and PERCENT."
"We conducted experiments on upper case named entity recognition, and showed how unlabeled mixed case text can be used to improve the results of an up-per case NER on the official MUC-6 and MUC-7"
"Mixed Case: Consuela Washington, a longtime House staffer and an expert in securities laws, is a leading candidate to be chairwoman of the Securities and Exchange Commission in the Clinton administration."
"Upper Case: CONSUELA WASHINGTON, A LONGTIME HOUSE STAFFER AND AN EX-PERT IN SECURITIES LAWS, IS A LEADING CANDIDATE TO BE CHAIRWOMAN OF THE SECURITIES AND EXCHANGE COMMIS-SION IN THE CLINTON ADMINISTRATION. test data."
"Besides upper case text, this approach can also be applied on transcribed text from auto-matic speech recognizers in Speech Normalized Or-thographic Representation (SNOR) format, or from optical character recognition (OCR) output."
"For the English language, a word starting with a capital let-ter often designates a named entity."
Upper case NERs do not have case information to help them to distinguish named entities from non-named en-tities.
"When data is sparse, many named entities in the test data would be unknown words."
This makes upper case named entity recognition more difficult than mixed case.
Even a human would experience greater difficulty in annotating upper case text than mixed case text (Figure 1).
"We propose using a mixed case NER to “teach” an upper case NER, by making use of unlabeled mixed case text."
"With the abundance of mixed case un- labeled texts available in so many corpora and on the Internet, it will be easy to apply our approach to improve the performance of NER on upper case text."
Our approach does not satisfy the usual as-sumptions of co-training[REF_CITE].
"Intuitively, however, one would expect some infor-mation to be gained from mixed case unlabeled text, where case information is helpful in pointing out new words that could be named entities."
We show empirically that such an approach can indeed im-prove the performance of an upper case NER.
"In Section 5, we show that for MUC-6, this way of using unlabeled text can bring a relative reduc-tion in errors of 38.68% between the upper case and mixed case NERs."
For MUC-7 the relative reduction in errors is 22.49%.
"Considerable amount of work has been done in recent years on NERs, partly due to the Mes-sage Understanding Conferences[REF_CITE]."
"Machine learning methods such as BBN’s IdentiFinder (Bikel, Schwartz, and[REF_CITE]) and Borthwick’s MENE[REF_CITE]have shown that machine learning NERs can achieve comparable performance with systems using hand-coded rules."
"Bikel, Schwartz, and[REF_CITE]have also shown how mixed case text can be automatically converted to upper case SNOR or OCR format to train NERs to work on such formats."
There is also some work on un-supervised learning for mixed case named entity recognition ([REF_CITE];
"However, features were extracted using a parser, and perfor-mance was evaluated differently (the classes were person, organization, location, and noise)."
We suspect that it will be hard for purely unsupervised methods to perform as well as super-vised ones.
"There is much recent research on co-training, such[REF_CITE]."
Most co-training methods involve using two classifiers built on different sets of features.
"Instead of using distinct sets of features,[REF_CITE]used dif-ferent classification algorithms to do co-training."
"Each set of features can be used to build a classifier, resulting in two independent classifiers, A and B. Classifications by A on unlabeled data can then be used to further train classifier B, and vice versa."
"Intuitively, the in-dependence assumption is there so that the classifi-cations of A would be informative to B. When the independence assumption is violated, the decisions of A may not be informative to B. In this case, the positive effect of having more data may be offset by the negative effect of introducing noise into the data (classifier A might not be always correct)."
"However, the comparison they made is between co-training and self-training."
"In self-training, only one classifier is used to tag unla-beled data, after which the more confidently tagged data is reused to train the same classifier."
Many natural language processing problems do not show the natural feature split displayed by the web page classification task studied in previous co-training work.
Our work does not really fall under the paradigm of co-training.
"Instead of co-operation between two classifiers, we used a stronger classi-fier to teach a weaker one."
"In addition, it exhibits the following differences: (1) the features are not at all independent (upper case features can be seen as a subset of the mixed case features); and (2) The additional features available to the mixed case sys-tem will never be available to the upper case system."
Co-training often involves combining the two differ-ent sets of features to obtain a final system that out-performs either system alone.
"In our context, how-ever, the upper case system will never have access to some of the case-based features available to the mixed case system."
"Due to the above reason, it is unreasonable to expect the performance of the upper case NER to match that of the mixed case NER."
"However, we still manage to achieve a considerable reduction of errors between the two NERs when they are tested on the official MUC-6 and MUC-7 test data."
We use the maximum entropy framework to build two classifiers: an upper case NER and a mixed case NER.
"The upper case NER does not have ac-cess to case information of the training and test data, and hence cannot make use of all the features used by the mixed case NER."
We will first describe how the mixed case NER is built.
More details of this mixed case NER and its performance are given[REF_CITE].
Our approach is similar to the MENE system[REF_CITE].
Each word is assigned a name class based on its features.
"Each name class is subdivided into 4 classes, i.e., N begin, N continue, N end, and N unique."
"Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class)."
"The maximum entropy framework estimates proba-bilities based on the principle of making as few as-sumptions as possible, other than the constraints im-posed."
"Such constraints are derived from training data, expressing some relationship between features and outcome."
The probability distribution that sat-isfies the above property is the one with the high-est entropy.
"It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra, Della Pietra, and[REF_CITE]):   &quot;!$ # %&apos;&amp; (   where refers to the outcome, the history (or con-text), and  is a normalization function."
"In addi-tion, each feature function )  ( $ is a binary func-tion."
"For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: ( * ,+ -if = true, previous word = the ) otherwise"
The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS)[REF_CITE].
This is an iterative method that improves the estimation of the parameters at each iteration.
The features we used can be divided into 2 classes: local and global.
"Local features are features that are based on neighboring tokens, as well as the token itself."
Global features are extracted from other oc-currences of the same token in the whole document.
Features in the maximum entropy framework are binary.
Feature selection is implemented using a fea-ture cutoff: features seen less than a small count dur-ing training will not be used.
We group the features used into feature groups.
Each group can be made up of many binary features.
"For each token . , zero, one, or more of the features in each group are set to 1."
The local feature groups are:
Non-Contextual Feature: This feature is set to 1 for all tokens.
This feature imposes constraints that are based on the probability of each name class during training.
"MUC data contains SGML tags, and a doc-ument is divided into zones (e.g., headlines and text zones)."
The zone to which a token belongs is used as a feature.
"For example, in MUC-6, there are four zones (TXT, HL, DATELINE, DD)."
"Hence, for each token, one of the four features zone-TXT, zone-HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0."
Case and Zone:
"If the token . starts with a cap-ital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1."
"If it is made up of all capital letters, then (allCaps, zone) is set to 1."
"If it contains both upper and lower case letters, then (mixedCaps, zone) is set to 1."
A token that is allCaps will also be initCaps.
This group consists of (3 total number of possible zones) features.
"Case and Zone of .0/ and 21. : Similarly, if .0/ (or 31. ) is initCaps, a feature (initCaps, zone)  (or (initCaps, zone) 7: ;&lt;= ) is set to 1, etc."
"Token Information: This group consists of 10 features based on the string . , as listed in Table 1."
"For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc."
First Word: This feature group contains only one feature firstword.
"If the token is the first word of a sentence, then this feature is set to 1."
"Otherwise, it is set to 0."
Lexicon Feature: The string of the token . is used as a feature.
This group contains a large num-ber of features (one for each token string present in the training data).
At most one feature in this group will be set to 1.
"If . is seen infrequently during training (less than a small count), then . will not se-lected as a feature and all features in this group are set to 0."
Lexicon Feature of Previous and Next Token: The string of the previous token . 1 and the next token .&gt;/ is used with the initCaps information of . .
"If . has initCaps, then a feature (initCaps, .?/ ) 4&lt; is set to 1."
"If . is not initCaps, then (not-initCaps, .&gt;/ )  is set to 1."
Same for .01 .
"In the case where the next token . / is a hyphen, then ?A/ @ is also used as a feature: (initCaps, B. /A@ ) . is set to 1."
"This is because in many cases, the use of hyphens can be considered to be optional (e.g., “third-quarter” or “third quarter”)."
"Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1."
"Dictionaries: Due to the limited amount of train-ing material, name dictionaries have been found to be useful in the named entity task."
The sources of our dictionaries are listed in Table 2.
"A token . is tested against the words in each of the four lists of location names, corporate names, person first names, and person last names."
"If . is found in a list, the corresponding feature for that list will be set to 1."
"For example, if Barry is found in the list of person first names, then the feature PersonFirstName will be set to 1."
"Similarly, the tokens C. / and D1. are tested against each list, and if found, a correspond-ing feature will be set to 1."
"For example, if B. / is found in the list of person first names, the feature PersonFirstName &lt;4 57698 is set to 1."
"Month Names, Days of the Week, and Num-bers:"
"If . is one of January, February, . . ., Decem-ber, then the feature MonthName is set to 1."
"If . is one of Monday, Tuesday, . . ., Sunday, then the fea-ture DayOfTheWeek is set to 1."
"If . is a number string (such as one, two, etc), then the feature Num-berString is set to 1."
Suffixes and Prefixes:
This group contains only two features: Corporate-Suffix and Person-Prefix.
"Two lists, Corporate-Suffix-List (for corporate suf-fixes) and Person-Prefix-List (for person prefixes), are collected from the training data."
For a token . that is in a consecutive sequence of initCaps tokens .21 E (GFGFGFH( .
GFGFGFH( ( ?. /
"I , if any of the tokens from .?/ to 0. /"
"I is in Corporate-Suffix-List, then a fea-ture Corporate-Suffix is set to 1."
"If any of the to-kens from .?1 E?1 to 31. is in Person-Prefix-List, then another feature Person-Prefix is set to 1."
"Note that we check for &gt;. 1 ?E 1 , the word preceding the consecutive sequence of initCaps tokens, since per-son prefixes like Mr., Dr. etc are not part of person names, whereas corporate suffixes like Corp., Inc. etc are part of corporate names."
The global feature groups are:
"InitCaps of Other Occurrences: There are 2 fea-tures in this group, checking for whether the first oc-currence of the same word in an unambiguous posi- tion (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps."
"For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occur-rences might be more accurate than its own."
Corporate Suffixes and Person Prefixes of Other Occurrences:
"With the same Corporate-Suffix-List and Person-Prefix-List used in local fea-tures, for a token . seen elsewhere in the same docu-ment with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1."
"Acronyms: Words made up of all capitalized let-ters in the text zone will be stored as acronyms (e.g., IBM)."
The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.
"Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique."
"For example, if “FCC” and “Federal Communica-tions Commission” are both found in a document, then “Federal” has A begin set to 1, “Communica-tions” has A continue set to 1, “Commission” has A end set to 1, and “FCC” has A unique set to 1."
"Sequence of Initial Caps: In the sentence “Even News Broadcasting Corp., noted for its accurate re-porting, made the erroneous announcement.”, a NER may mistake “Even News Broadcasting Corp.” as an organization name."
"However, it is unlikely that other occurrences of “News Broadcasting Corp.” in the same document also co-occur with “Even”."
This group of features attempts to capture such informa-tion.
"For every sequence of initial capitalized words, its longest substring that occurs in the same docu-ment is identified."
"For this example, since the se-quence “Even News Broadcasting Corp.” only ap-pears once in the document, its longest substring that occurs in the same document is “News Broadcasting Corp.”."
"In this case, “News” has an additional fea-ture of I begin set to 1,“Broadcasting” has an addi-tional feature of I continue set to 1, and “Corp.” has an additional feature of I end set to 1."
Unique Occurrences and Zone:
This group of features indicates whether the word . is unique in the whole document. . needs to be in initCaps to be considered for this feature.
"If . is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where . appears."
"All features used for the mixed case NER are used by the upper case NER, except those that require case information."
"Among local features, Case and Zone, InitCap-Period, and OneCap are not used by the upper case NER."
"Among global features, only Other-CS and Other-PP are used for the upper case NER, since the other global features require case information."
"For Corporate-Suffix and Person-Prefix, as the se-quence of initCaps is not available in upper case text, only the next word (previous word) is tested for Corporate-Suffix (Person-Prefix)."
"During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique)."
"To eliminate such sequences, we define a transition probability between word classes J K to be equal to 1 if the sequence is admissible, and 0 otherwise."
The probability of the classes K GFGFGFN( ( K I assigned P to the words in a sentence O in a document is defined as follows:
I J  GFGFGFN( ( K I
O ( P L  J K L O ( P  J  L K L 1 ( where J  L O ( P is determined by the maximum entropy classifier.
A dynamic programming algo-rithm is then used to select the sequence of word classes with the highest probability.
The teaching process is illustrated in Figure 2.
This process can be divided into the following steps:
"First, a mixed case NER (MNER) is trained from some initial corpus S , man-ually tagged with named entities."
This corpus is also converted to upper case in order to train another up-per case NER (UNER).
UNER is required by our method of example selection.
Baseline Test on Unlabeled Data.
Apply the trained MNER on some unlabeled mixed case texts to produce mixed case texts that are machine-tagged with named entities (text-mner-tagged).
"Convert the original unlabeled mixed case texts to upper case, and similarly apply the trained UNER on these texts to obtain upper case texts machine-tagged with named entities (text-uner-tagged)."
Compare text-mner-tagged and text-uner-tagged and select tokens in which the classification by MNER differs from that of UNER.
"The class assigned by MNER is considered to be correct, and will be used as new training data."
These tokens are collected into a set SUT .
Retraining for Final Upper Case NER.
Both S and S3T are used to retrain an upper case NER.
"How-ever, tokens from S are given a weight of 2 (i.e., each token is used twice in the training data), and to-kens from SDT a weight of 1, since S is more reliable than S T (human-tagged versus machine-tagged)."
"For manually labeled data (corpus C), we used only the official training data provided by the MUC-6 and MUC-7 conferences, i.e., using MUC-6 train-ing data and testing on MUC-6 test data, and us-ing MUC-7 training data and testing on MUC-7 test data. [Footnote_1] The task definitions for MUC-6 and MUC-7 are not exactly identical, so we could not com-bine the training data."
1 MUC data can be obtained from the Linguistic Data Con-sortium[URL_CITE]
"The original MUC-6 training data has a total of approximately 160,000 tokens and"
"In this paper, we have shown that the performance of NERs on upper case text can be improved by using a mixed case NER with unlabeled text."
"Named en-tity recognition on mixed case text is easier than on upper case text, where case information is unavail-able."
"By using the teaching process, we can reduce the performance gap between mixed and upper case NER by as much as 39% for MUC-6 and 22% for MUC-7."
"This approach can be used to improve the performance of NERs on speech recognition output, or even for other tasks such as part-of-speech tag-ging, where case information is helpful."
"With the abundance of unlabeled text available, such an ap-proach requires no additional annotation effort, and hence is easily applicable."
"This way of teaching a weaker classifier can also be used in other domains, where the task is to in-fer V P ZY V W ( \X [ ,isandavailablean abundance."
"If one possessesof unlabeleda seconddata classifier V ( W X such that provides addi-tional “useful” information that can be utilized by this second classifier, then one can use this second P classifier to automatically P tag the unlabeled data , and select from examples that can be used to sup-plement the training data for training V]W^X ."
"This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the applica-tion being the recovery of named-entity boundaries in a corpus of web data."
The first approach uses a boosting algorithm for ranking problems.
The second ap-proach uses the voted perceptron algo-rithm.
"Both algorithms give compara-ble, significant improvements over the maximum-entropy baseline."
"The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples."
Recent work in statistical approaches to parsing and tagging has begun to consider methods which in-corporate global features of candidate structures.
"Examples of such techniques are Markov Random Fields[REF_CITE], and boosting algorithms[REF_CITE]."
One appeal of these methods is their flexibility in incor-porating features into a model: essentially any fea-tures which might be useful in discriminating good from bad structures can be included.
"A second ap-peal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures."
"This discriminative property is shared by the methods[REF_CITE], and also the Conditional Random Field methods[REF_CITE]."
"In a previous paper[REF_CITE], a boosting al-gorithm was used to rerank the output from an ex- isting statistical parser, giving significant improve-ments in parsing accuracy on Wall Street Journal data."
"Similar boosting algorithms have been applied to natural language generation, with good results,[REF_CITE]."
In this paper we apply rerank-ing methods to named-entity extraction.
"A state-of-the-art (maximum-entropy) tagger is used to gener-ate 20 possible segmentations for each input sen-tence, along with their probabilities."
We describe a number of additional global features of these can-didate segmentations.
These additional features are used as evidence in reranking the hypotheses from the max-ent tagger.
"We describe two learning algo-rithms: the boosting method[REF_CITE], and a variant of the voted perceptron algorithm, which was initially described in (Freund &amp;[REF_CITE])."
We applied the methods to a corpus of over one million words of tagged web data.
"The methods give signif-icant improvements over the maximum-entropy tag-ger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method)."
"One contribution of this paper is to show that ex-isting reranking methods are useful for a new do-main, named-entity tagging, and to suggest global features which give improvements on this task."
"We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task."
"It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish)."
It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work.
Over a period of a year or so we have had over one million words of named-entity data annotated.
"The data is drawn from web pages, the aim being to sup-port a question-answering system over web data."
"A number of categories are annotated: the usual peo-ple, organization and location categories, as well as less frequent categories such as brand-names, scien-tific terms, event titles (such as concerts) and so on."
"From this data we created a training set of 53,609 sentences ([Footnote_1],047,491 words), and a test set of 14,717 sentences (291,898 words)."
"1 In initial experiments, we found that forcing the tagger to recover categories as well as the segmentation, by exploding the number of tags, reduced performance on the segmentation task, presumably due to sparse data problems."
The task we consider is to recover named-entity boundaries.
We leave the recovery of the categories of entities to a separate stage of processing. 1 We evaluate different methods on the task through pre-cision and recall.
"If a method proposes entities on the test set, and of these are correct (i.e., an entity is marked by the annotator with exactly the same span as  that proposed) then the precision of a method is ."
"Similarly, if is the total number of en-tities in the human  annotated version of the test set, then the recall is ."
"The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respec-tively for these three cases)."
"As a baseline model we used a maximum entropy tagger, very similar to the ones described in ([REF_CITE]; Borthwick et. al 1998;[REF_CITE])."
"Max-ent tag-gers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tag-ging[REF_CITE], named-entity recognition (Borthwick et. al 1998), and information extraction tasks[REF_CITE]."
Thus the maximum-entropy tagger we used represents a serious baseline for the task.
"We used the following features (sev-eral of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction):"
"The word being tagged, the previous word, and the next word."
"The previous tag, and the previous two tags (bi-gram and trigram features)."
"A compound feature of three fields: (a) Is the word at the start of a sentence?; (b) does the word occur in a list of words which occur more frequently as lower case rather than upper case words in a large corpus of text? (c) the type of the first letter of the word, where    is defined as ‘A’ if is a capitalized letter, ‘a’ if is a lower-case letter, ‘0’ if is a digit, and otherwise."
"For example, if the word Animal is seen at the start of a sentence, and it occurs in the list of frequent lower-cased words, then it would be mapped to the feature 1-1-A."
The word with each character mapped to its   .
"For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa."
"The word with each character mapped to its type, but repeated consecutive character types are not repeated in the mapped string."
"For example, An-imal would be mapped to Aa, G.M. would again be mapped to A.A.."
The tagger was applied and trained in the same way as described[REF_CITE].
"The feature templates described above are used to create a set of binary features  ! , where is the tag, and is the “history”, or context."
An example is )( * if t = S and the $&quot; %# # !&amp; ) + word being tagged = “Mr.” otherwise
"The parameters of the model are , for &apos;. &amp; / / / , defining a conditional distribution over the tags given a history as   ;:=&lt;;&gt;? @ 1  &amp; 3 &lt;BA 3 5C7 ;=: &lt; A &gt; ? @"
The parameters are trained using Generalized Iter-ative Scaling.
"Following[REF_CITE], we only include features which occur 5 times or more in training data."
"In decoding, we use a beam search to recover 20 candidate tag sequences for each sen-tence (the sentence is decoded from left to right, with the top 20 most probable hypotheses being stored at each point)."
"As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data."
"The baseline method is to take the most probable candi-date for each test data sentence, and then to calculate precision and recall figures."
"Our aim is to come up with strategies for reranking the test data candidates, in such a way that precision and recall is improved."
"In developing a reranking strategy, the 53,609 sentences of training data were split into a 41,992 sentence training portion, and a 11,617 sentence de-velopment set."
"The training portion was split into 5 sections, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5."
"In a similar way, a model trained on the 41,992 sen-tence set was used to produce 20 hypotheses for each sentence in the development set."
The module we describe in this section generates global features for each candidate tagged sequence.
"As input it takes a sentence, along with a proposed segmentation (i.e., an assignment of a tag for each word in the sentence)."
"As output, it produces a set of feature strings."
We will use the following tagged sentence as a running example in this section:
"Whether/N you/N ’/N re/N an/N aging/N flower/N child/N or/N a/N clueless/N Gen/S Xer/C ,/N “/N"
"The/S Day/C They/C Shot/C John/C Lennon/C ,/N ” /N playing/N at/N the/N Dougherty/S Arts/C Center/C ,/N entertains/N the/N imagi-nation/N ./N"
An example feature type is simply to list the full strings of entities that appear in the tagged input.
"In this example, this would give the three features"
WE=Gen Xer WE=The Day
They Shot John Lennon WE=Dougherty Arts Center
Here WE stands for “whole entity”.
"Throughout this section, we will write the features in this format."
"The start of the feature string indicates the feature type (in this case WE), followed by =."
"Following the type, there are generally 1 or more words or other symbols, which we will separate with the symbol ."
"A seperate module in our implementation takes the strings produced by the global-feature generator, and hashes them to integers."
"For ex-ample, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively."
"Conceptually, is represented / / / by a large number the candidate of features FE  for GH&amp; is thewhere number of distinct feature strings in training data."
"In this example, only I&quot;$#%#  , KJ%%L #  and ML%%# #  take the value , all other features being zero."
We now introduce some notation with which to de-scribe the full set of global features.
"First, we as-sume the following primitives of an input candidate:  for N. &amp; / / / is the . ’ th tag in the tagged sequence.  for 0. &amp; / / !/ O is the . ’ th word.  for 0. &amp; / / !/ O is if P begins with a lower-case letter, otherwise. / / /!O   for .T&amp; is a transformation of P , where the transformation is applied in the same way as the final feature type in the maximum entropy tagger."
"Each character in the word is mapped to its   , but repeated consecutive character types are not repeated in the mapped string."
"For example, Animal would be mapped to Aa in this feature, G.M. would again be mapped to A.A.. / / /!O  for S. &amp; is the same as  , but has an additional flag appended."
The flag indi-cates whether or not the word appears in a dic-tionary of words which appeared more often lower-cased than capitalized in a large corpus of text.
"In our example, Animal appears in the lexicon, but G.M. does not, so the two values for  would be Aa1 and A.A.0 respectively."
"In addition,  P ! and  are all defined to be NULL if .XW or .XY"
Most of the features we describe are anchored on entity boundaries in the candidate segmentation.
We will use “feature templates” to describe the features that we used.
"As an example, suppose that an entity is seen from words G to inclusive in a segmenta-tion."
Then the WE feature described in the previous section can be generated by the template
WE= P E P &quot; / / / Prq
"Applying this template to the three entities in the running example generates the three feature strings ple, consider the template FF=  &quot; described in the previous section."
As another / / / q exam-.
"This will generate a feature string for each of the entities / / / q in a candidate, this time using the values  rather than P E / / / P q ."
"For the full set of feature tem-plates that are anchored around entities, see figure 1."
A second set of feature templates is anchored around quotation marks.
"In our corpus, entities (typ-ically with long names) are often seen surrounded by quotes."
"For example, “The Day They Shot John Lennon”, the name of a band, appears in the running example."
"Define G to be the index of any double quo-tation marks in the candidate, to be the index of the next (matching) double quotation marks if they ap-pear in the candidate."
"Additionally, define  to be the index of the last word beginning with a lower case letter, upper case letter, or digit within the quo-tation marks."
The first set of feature templates tracks the values of for the words within quotes: [Footnote_2]
"2 We only included these features if vxwzy|{nz} ~ , to prevent an explosion in the length of feature strings."
Q=  %E : EVop&quot; @ : &quot; @ / / / q q
Q2= : %E tI&quot; @ : nE tI&quot; @ uE %E / / / : q po &quot; @ : q op&quot; @
The next set of feature templates are sensitive to whether the entire sequence between quotes is tagged as a named entity.
"Define  s to be if %EVop&quot;X&amp; S, and  =C for . G&amp;  s / / /  (i.e.,  s &amp; if the sequence of words within the quotes is tagged as a single entity)."
"Also define  to be the number of upper cased words within the quotes,  to be the number of lower case words, and  to be if    , otherwise."
Then two other templates are:
QF=  s   : EVop&quot; @ q J
QF2=  s  : EVop&quot; @ q J
In the “The Day They Shot John Lennon” example we would have  s &amp; provided that the entire se-quence within quotes was tagged as an entity.
"Ad-ditionally, &amp; , &amp; , and &amp; ."
"The val-ues for : EVop&quot; @ and q J would be  and  (these features are derived from The and Lennon, which re-spectively do and don’t appear in the capitalization lexicon)."
This would give QF=    and QF2=   .
"At this point, we have fully described the repre-sentation used as input to the reranking algorithms."
The maximum-entropy tagger gives 20 proposed segmentations for each input sentence.
"Each can-didate is represented by the log probability   from the tagger, as well as / the / / values of the global features KE  for G&amp; ."
"In the next sec-tion we describe algorithms which blend these two sources of information, the aim being to improve upon a strategy which just takes the candidate from the tagger with the highest score for   ."
This section introduces notation for the reranking task.
The framework is derived by the transforma-tion from ranking problems to a margin-based clas-sification problem[REF_CITE].
"It is also related to the Markov Random Field methods for parsing suggested[REF_CITE], and the boosting methods for parsing[REF_CITE]."
We consider the following set-up:
Training data is a set of example input/output pairs  .
In tagging we would have training examples G %  where each G is a sentence and each is the correct sequence of tags for that sentence.
We assume some way of enumerating a set of candidates for a particular sentence.
"We use  to denote the  ’th candidate for  the . ’ th / / / sentence in training data, and  G &amp;  to denote &quot; %  the set of candidates for  ."
"In this paper, the top  outputs from a maximum entropy tagger are used as the set of candidates."
"Without loss of generality we take &quot; to be the candidate for G which has the most correct tags, i.e., is closest to being correct. [Footnote_3] | &gt; is the probability that the base model assigns to  &gt;  ."
"3 In the event that multiple candidates get the same, highest score, the candidate with the highest value of log-likelihood § under the baseline model is taken as ¨ 5x© _ ."
"We define   &gt; &amp;   &gt;  . additional features, KE"
We assume / / / a set of for G&amp; .
"The features could be arbitrary functions of the candidates; our hope is to include features which help in discriminating good candi-dates from bad ones. of Finally, the parameters of  the P # model P &quot; / / / are P¤£ a  vector."
"The  parameters, ¡¢&amp; ranking function is defined as £  % &amp; P #  ¥ P !E FE  &quot;"
This function assigns a real-valued number to a can-didate .
"It will be taken to be a measure of the plausibility of a candidate, higher scores meaning higher plausibility."
"As such, it assigns a ranking to different candidate structures for the same sentence, and in particular the output on a training or test ex-ample G is ªU«n­¬| : E @  % ."
"In this paper we take the features KE to be fixed, the learning problem being to choose a good setting for the parameters ¡ ."
In some parts of this paper we will use vec-tor notation. / / / Define    . ² Then  totheberankingthe vectorscore   ! &quot;  can also be written as  %&amp;  where ¶ µ · is the dot product between vectors ¶ and · .
The first algorithm we consider is the boosting algo-rithm for ranking described[REF_CITE].
The algorithm is a modification of the method[REF_CITE].
"The method can be considered to be a greedy algorithm for finding the parameters ¡ that minimize the loss function ¥ ¥  : ¯ 5x©» &gt;¼&apos;@ t º : ¯ B5 © _ &gt;¼&apos;@  &amp;  ¹KJ where as before,  %¾¡¿µh²&amp;  ."
The theo-retical motivation for this algorithm goes back to the PAC model of learning.
"Intuitively, it is useful to note that this loss function is an upper bound on the number of “ranking errors”, a ranking error being a case where an incorrect candidate gets a higher value for  than a correct candidate."
"This follows because for all , t ¯ ÁÀFÂ  , where we define ÀFÂ  to be for  , and otherwise."
Hence ¥ ¥   ÀFÂÇÆ &gt; Ã  ¹KJ where  &gt;&amp;  &gt; &quot;%  &gt;% .
Note that the number of ranking errors is 3 [Footnote_3]  ¹KJ  &gt;lÃ .
"3 In the event that multiple candidates get the same, highest score, the candidate with the highest value of log-likelihood § under the baseline model is taken as ¨ 5x© _ ."
"As an initial step, P # is set to be"
P Ë# ªU«n&amp; ­¬|Î ÌxÍ ¥ ¥ Î BÏK: : ¯ 5B©@ t ÏK: ¯ B5 © _ b@ @  ¹KJ and all other parameters P E for GÐ&amp; / / / are set to be zero.
The algorithm then proceeds for  iter-ations (  is usually chosen by cross validation on a development set).
"At each iteration, a single feature is chosen, and its weight is updated."
"Suppose the current parameter values are ¡ , and a single feature Ñ is chosen, its weight being updated through an in-crement Ò , i.e., PrÓ &amp; PrÓ Ò ."
"Then the new loss, after this parameter update, will be  Ñ !&amp; ¥ tFÔ 5x©»  :Ö? : ¯ 5x©»!@ t ?: ¯ 5B© _ @D@ &gt; where  &gt;Ø&amp;Ù  &gt; &quot; % È6  &gt;% ."
The boost- Ñ`Ú Ú ing algorithm chooses the feature/update pair !
"Ò which is optimal in terms of minimizing the loss function, i.e.,"
Ñ Ú !Ò Ú ªU«n&amp; X¬ÌxÍÓ &gt;Õ 
Ñ ! (1) and then makes the update PrÓ Û &amp; PrÓ Û ÜÒ Ú .
Figure 2 shows an algorithm which implements this greedy procedure.
"See[REF_CITE]for a full description of the method, including justifica-tion that the algorithm does in fact implement the update in Eq. 1 at each iteration. [Footnote_4] The algorithm re-lies on the following arrays:  oÓ  n. $  &gt; &quot;   &gt; $&amp; &amp;  Ót  .$   &gt; &quot;   &gt;$&amp; È &amp; Þ o &gt;  Ñ ÝFÂÇ   &gt; &quot;   &gt;$&amp; &amp; Þ t &gt;  Ñ ÝFÂÇ   &gt; &quot;   &gt;$&amp; È &amp; Thus  oÓ is an index from features Ñ to cor- rect/incorrect candidate pairs where the ’th feature takes value on the correct candidate, and value on the incorrect candidate."
"4 Strictly speaking, this is only the case if the smoothing pa-rameter â is ã ."
The array  Ót is a simi-
Þ o & gt;  lar index Þ t from features to examples.
The arrays and &gt;  are reverse indices from training examples to features.
"Figure 3 shows the training phase of the percep-tron algorithm, originally introduced[REF_CITE]."
"The algorithm maintains a parameter vector ¡ , which is initially set to be all zeros."
"The algo-rithm then makes a pass over the training set, at each training / / example !/"
O storing a parameter vector ¡ for &amp;. .
The parameter vector is only modified when a mistake is made on an example.
"In this case the update is very simple, involving adding the dif-ference of the offending examples’ representations ( ¡ ß¡ &quot; à² &quot;  in the figure)."
"See &amp;[REF_CITE]chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters."
"In the most basic form of the perceptron, the pa-rameter values ¡á are taken as the final parame-ter settings, and the output / / / on a new test exam-ple with  for &amp; is simply the highest – for Ñ ð"
"Þ t &gt; , – for ð ç Ó t ç&amp; tÓ Üñ&gt; , – æÊ&amp;Ùæò ñ"
"For all features whose values of ç Ó o and/or ç Ó t have ì changed ì , recalculate Þ G   &amp; êê ç Óo È ç"
"Ót êê ê ê ê ê scoring candidate under these parameter values, i.e., Ó where Ñ &amp;ªU«n&apos;¬| á µ4²  . (Freund &amp;[REF_CITE]) describe a refinement of the perceptron, the voted perceptron."
The train-ing phase is identical to that in figure 3.
"Note  , how- / / nO/ ever, that all parameter vectors ¡ for .Ð&amp; are stored."
Thus the training phase O can be thought of as a way of constructing different parame-ter settings.
"Each of these parameter settings will have its own highest ranking candidate, Ó where Ñ &amp;ªU«n­¬|  ¡  ."
"The idea O behind the voted µd² perceptron is to take each of the parameter set-tings to “vote” for a candidate, and the candidate which gets the most votes is returned as the most likely candidate."
See figure 4 for the algorithm. [Footnote_5]
"5 Note that, for reasons of explication, the decoding algo-rithm we 5Mø present ÷ 5 _ isitlessis preferableefficient 5 thanto usenecessarysome book-keeping. For example 5 to,when ÷ avoid recalculation of ùÉvB¨kú;÷ } and !û üþýFÿrû » ùÉvB¨ » ú^÷ } ."
We applied the voted perceptron and boosting algo-rithms to the data described in section 2.3.
Only fea-tures occurring on 5 or more distinct training sen-tences were included in the model.
"This resulted &amp; precision, &amp; recall,  &amp; F-measure."
Fig-ures in parantheses are relative improvements in er-ror rate over the maximum-entropy model.
"All fig-ures are percentages. in 93,777 distinct features."
"The two methods were trained on the training portion (41,992 sentences) of the training set."
We used the development set to pick the best values for tunable parameters in each algo-rithm.
"For boosting, the main parameter to pick is the number of rounds,  ."
"We ran the algorithm for a total of 300,000 rounds, and found that the op-timal value for F-measure on the development set occurred after 83,233 rounds."
"For the voted per-ceptron  , the representation / / / £ ²  waswheretakenistoabepa-a  ! &quot;  vector rameter that influences the relative contribution of the log-likelihood /  term versus the other features."
A value of &amp; was found to give the best re-sults on the development set.
Figure 5 shows the results for the three methods on the test set.
"Both of the reranking algorithms show significant improve-ments over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error re-duction for the voted perceptron."
"In our experiments we found the voted percep-tron algorithm to be considerably more efficient in training, at some cost in computation on test exam-ples."
"Another attractive property of the voted per-ceptron is that it can be used with kernels, for exam-ple the kernels over parse trees described[REF_CITE].[REF_CITE]describe the voted perceptron ap-plied to the named-entity data in this paper, but us-ing kernel-based features rather than the explicit fea-tures described in this paper."
"See[REF_CITE]for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm ap-plied to ranking problems."
"A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy."
This section dis-cusses why this is unlikely to be the case.
The prob-lem described here is closely related to the label bias problem described[REF_CITE].
One straightforward way to incorporate global features into the maximum-entropy model would be to introduce new features %% which indicated whether the tagging decision in the history cre-ates a particular global feature.
"For example, we could introduce a feature )( * if t = N and this decision   &quot; %# !&amp; )+ creates an LWLC=1 feature otherwise"
"As an example, this would take the value if its was tagged as N in the following context,"
"She/N praised/N the/N University/S for/C its/? efforts to because tagging its as N in this context would create an entity whose last word was not capitalized, i.e., University for."
Similar features could be created for all of the global features introduced in this paper.
This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.
"The parameter , &quot;  %# as-sociated with this new feature can only affect the score for a proposed sequence by modifying    at the point at which  &quot; %# !&amp; ."
"In the exam-ple, this means that the LWLC=1 feature can only lower the score for the segmentation by lowering the probability of tagging its as N. But its has almost probably of not appearing as part of an entity, so  Ü2  should be almost whether &quot;  %# is or in this context!"
"The decision which effectively cre-ated the entity University for was the decision to tag for as C, and this has already been made."
The inde-pendence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create glob-ally implausible structures with unreasonably high scores.
See[REF_CITE]section 8.4.2 for a dis-cussion of this problem in the context of parsing.
Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the exper- iments.
"Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions."
This paper presents a revision learn-ing method that achieves high per-formance with small computational cost by combining a model with high generalization capacity and a model with small computational cost.
This method uses a high capacity model to revise the output of a small cost model.
"We apply this method to English part-of-speech tagging and Japanese mor-phological analysis, and show that the method performs well."
"Recently, corpus-based approaches have been widely studied in many natural language pro-cessing tasks, such as part-of-speech (POS) tag-ging, syntactic analysis, text categorization and word sense disambiguation."
"In corpus-based natural language processing, one important is-sue is to decide which learning model to use."
"Various learning models have been studied such as Hidden Markov models (HMMs)[REF_CITE], decision trees[REF_CITE]and maximum entropy models[REF_CITE]."
"Recently, Support Vector Ma-chines (SVMs)[REF_CITE]are getting to be used, which are supervised machine learning algorithm for bi-nary classification."
"SVMs have good generaliza-tion performance and can handle a large num-ber of features, and are applied to some tasks successfully[REF_CITE]."
"However, their computational cost is large and is a weakness of SVMs."
"In general, a trade-off between capacity and com-putational cost of learning models exists."
"For example, SVMs have relatively high generaliza-tion capacity, but have high computational cost."
"On the other hand, HMMs have lower compu-tational cost, but have lower capacity and dif-ficulty in handling data with a large number of features."
Learning models with higher capac-ity may not be of practical use because of their prohibitive computational cost.
This problem becomes more serious when a large amount of data is used.
"To solve this problem, we propose a revision learning method which combines a model with high generalization capacity and a model with small computational cost to achieve high per-formance with small computational cost."
"This method is based on the idea that processing the entire target task using a model with higher ca-pacity is wasteful and costly, that is, if a large portion of the task can be processed easily using a model with small computational cost, it should be processed by such a model, and only difficult portion should be processed by the model with higher capacity."
"Revision learning can handle a general multi-class classification problem, which includes POS tagging, text categorization and many other tasks in natural language processing."
We ap-ply this method to English POS tagging and Japanese morphological analysis.
This paper is organized as follows: Section 2 describes the general multi-class classification problem and the one-versus-rest method which is known as one of the solutions for the prob-lem.
"Section 3 introduces revision learning, and discusses how to combine learning models."
Sec-tion 4 describes one way to conduct Japanese morphological analysis with revision learning.
Section 5 shows experimental results of English POS tagging and Japanese morphological anal-ysis with revision learning.
"Section 6 discusses related works, and Section 7 gives conclusion."
Problems and the One-versus-Rest Method
Let us consider the problem to decide the class of an example x among multiple classes.
Such a problem is called multi-class classification prob-lem.
Many tasks in natural language processing such as POS tagging are regarded as a multi-class classification problem.
"When we only have binary (positive or negative) classification algo-rithm at hand, we have to reformulate a multi-class classification problem into a binary classi-fication problem."
"We assume a binary classifier f(x) that returns positive or negative real value for the class of x, where the absolute value |f(x)| reflects the confidence of the classification."
The one-versus-rest method is known as one of such methods[REF_CITE].
"For one training example of a multi-class problem, this method creates a positive training example for the true class and negative training examples for the other classes."
"As a result, positive and negative examples for each class are generated."
"Suppose we have five candidate classes A, B, C, D and E , and the true class of x is B. Fig-ure 1 (left) shows the created training examples."
Note that there are only two labels (positive and negative) in contrast with the original problem.
"Then a binary classifier for each class is trained using the examples, and five classifiers are cre-ated for this problem."
"Given a test example x 0 , all the classifiers classify the example whether it belongs to a specific class or not."
Its class is decided by the classifier that gives the largest value of f(x 0 ).
The algorithm is shown in Figure 2 in a pseudo-code.
"However, this method has the problem of be-ing computationally costly in training, because the negative examples are created for all the classes other than the true class, and the to-tal number of the training examples becomes large (which is equal to the number of original training examples multiplied by the number of classes)."
"The computational cost in testing is also large, because all the classifiers have to work on each test example."
"As discussed in the previous section, the one-versus-rest method has the problem of compu-tational cost."
This problem become more se-rious when costly binary classifiers are used or when a large amount of data is used.
"To cope with this problem, let us consider the task of POS tagging."
Most portions of POS tagging is not so difficult and a simple POS-based HMMs learning [Footnote_1] achieves more than 95% accuracy sim-ply using the POS context[REF_CITE].
"1 HMMs can be applied to either of unsupervised or supervised learning. In this paper, we use the latter case, i.e., visible Markov Models, where POS-tagged data is used for training."
"This means that the low capacity model is enough to do most portions of the task, and we need not use a high accuracy but costly algorithm in every portion of the task."
This is the base mo-tivation of the revision model we are proposing here.
"Revision learning uses a binary classifier with higher capacity to revise the errors made by the stochastic model with lower capacity as fol-lows: During the training phase, a ranking is assigned to each class by the stochastic model for a training example, that is, the candidate classes are sorted in descending order of its con-ditional probability given the example."
"Then, the classes are checked in their ranking order to create binary classifiers as follows."
"If the class is incorrect (i.e. it is not equal to the true class for the example), the example is added to the training data for that class as a negative exam-ple, and the next ranked class is checked."
"If the class is correct, the example is added to the training data for that class as a positive exam- ple, and the remaining ranked classes are not taken into consideration (Figure 1, right)."
"Us-ing these training data, binary classifiers are cre-ated."
Note that each classifier is a pure binary classifier regardless with the number of classes in the original problem.
The binary classifier is trained just for answering whether the output from the stochastic model is correct or not.
"During the test phase, first the ranking of the candidate classes for a given example is as-signed by the stochastic model as in the training."
Then the binary classifier classifies the example according to the ranking.
"If the classifier an-swers the example as incorrect, the next high-est ranked class becomes the next candidate for checking."
"But if the example is classified as cor-rect, the class of the classifier is returned as the answer for the example."
The algorithm is shown in Figure 3.
The amount of training data generated in the revision learning can be much smaller than that in one-versus-rest.
"Since, in revision learning, negative examples are created only when the stochastic model fails to assign the highest prob-ability to the correct POS tag, whereas negative examples are created for all but one class in the one-versus-rest method."
"Moreover, testing time of the revision learning is shorter, because only one classifier is called as far as it answers as cor-rect, but all the classifiers are called in the one-versus-rest method."
We introduced revision learning for multi-class classification in the previous section.
"How-ever, Japanese morphological analysis cannot be regarded as a simple multi-class classification problem, because words in a sentence are not separated by spaces in Japanese and the mor-phological analyzer has to segment the sentence into words as well as to decide the POS tag of the words."
"So in this section, we describe how to apply revision learning to Japanese morpho-logical analysis."
"For a given sentence, a lattice consisting of all possible morphemes can be built using a mor- pheme dictionary as in Figure 4."
Morphological analysis is conducted by choosing the most likely path on it.
We adopt HMMs as the stochastic model and SVMs as the binary classifier.
"For any sub-paths from the beginning of the sen-tence (BOS) in the lattice, its generative prob-ability can be calculated using HMMs[REF_CITE]."
"We first pick up the end node of the sentence as the current state node, and repeat the following revision learning process backward until the beginning of the sentence."
"Rankings are calculated by HMMs to all the nodes con-nected to the current state node, and the best of these nodes is identified based on the SVMs classifiers."
The selected node then becomes the current state node in the next round.
This can be seen as SVMs deciding whether two adjoining nodes in the lattice are connected or not.
"In Japanese morphological analysis, for any given morpheme µ, we use the following features for the SVMs: 1. the POS tags, the lexical forms and the in-flection forms of the two morphemes pre-ceding µ; 2. the POS tags and the lexical forms of the two morphemes following µ; 3. the lexical form and the inflection form of µ."
"The preceding morphemes are unknown because the processing is conducted from the end of the sentence, but HMMs can predict the most likely preceding morphemes, and we use them as the features for the SVMs."
"English POS tagging is regarded as a special case of morphological analysis where the seg-mentation is done in advance, and can be con-ducted in the same way."
"In English POS tag-ging, given a word w, we use the following fea-tures for the SVMs: 1. the POS tags and the lexical forms of the two words preceding w, which are given by HMMs; 2. the POS tags and the lexical forms of the two words following w; 3. the lexical form of w and the prefixes and suffixes of up to four characters, the exis- tence of numerals, capital letters and hy-phens in w."
This section gives experimental results of En-glish POS tagging and Japanese morphological analysis with revision learning.
Experiments of English POS tagging with revi-sion learning (RL) are performed on the Penn Treebank WSJ corpus.
"The corpus is randomly separated into training data of 41,342 sentences and test data of 11,771 sentences."
The dictio-nary for HMMs is constructed from all the words in the training data.
T3 of ICOPOST release 0.9.0[REF_CITE]is used as the stochastic model for ranking stage.
This is equivalent to POS-based second order HMMs.
SVMs with second order polyno-mial kernel are used as the binary classifier.
"The results are compared with TnT[REF_CITE]based on second order HMMs, and with POS tagger using SVMs with one-versus-rest (1-v-r)[REF_CITE]."
"The accuracies of those systems for known words, unknown words and all the words are shown in Table 1."
The accuracies for both known words and unknown words are improved through revision learning.
"However, revision learning could not surpass the one-versus-rest."
The main difference in the accuracies stems from those for unknown words.
The reason for that seems to be that the dictionary of HMMs for
"POS tagging is obtained from the training data, as a result, virtually no unknown words exist in the training data, and the HMMs never make mistakes for unknown words during the train-ing."
So no example of unknown words is avail-able in the training data for the SVM reviser.
"This is problematic: Though the HMMs handles unknown words with an exceptional method, SVMs cannot learn about errors made by the unknown word processing in the HMMs."
"To cope with this problem, we force the HMMs to make mistakes by eliminating low frequent words from the dictionary."
We eliminated the words appearing only once in the training data so as to make SVMs to learn about unknown words.
The results are shown in Table 1 (row “cutoff-1”).
Such procedure improves the accu-racies for unknown words.
One advantage of revision learning is its small computational cost.
We compare the computa-tion time with the HMMs and the one-versus-rest.
We also use SVMs with linear kernel func-tion that has lower capacity but lower computa-tional cost compared to the second order poly-nomial kernel SVMs.
The experiments are per-formed on an[REF_CITE]MHz processor.
"Table 2 shows the total number of training ex-amples, training time, testing time and accu-racy for each of the five systems."
The training time and the testing time of revision learning are considerably smaller than those of the one-versus-rest.
"Using linear kernel, the accuracy decreases a little, but the computational cost is much lower than the second order polynomial kernel."
We use the RWCP corpus and some additional spoken language data for the experiments of Japanese morphological analysis.
"The corpus is randomly separated into training data of 33,831 sentences and test data of 3,758 sentences."
"As the dictionary for HMMs, we use IPADIC ver-sion 2.4.4 with 366,878 morphemes[REF_CITE]which is originally con-structed for the Japanese morphological ana-lyzer ChaSen[REF_CITE]."
"A POS bigram model and ChaSen version 2.2.8 based on variable length HMMs are used as the stochastic models for the ranking stage, and SVMs with the second order polynomial kernel are used as the binary classifier."
"We use the following values to evaluate Japanese morphological analysis: recall = h# of correct morphemes in system’s outputi , h# of morphemes in test datai precision = h# of correct morphemes in system’s outputi , h# of morphemes in system’s outputi F-measure = 2 × recall × precision . recall + precision"
"The results of the original systems and those with revision learning are shown in Table 3, which provides the recalls, precisions and F-measures for two cases, namely segmentation (i.e. segmentation of the sentences into mor-phemes) and tagging (i.e. segmentation and"
The one-versus-rest method is not used because it is not applicable to mor-phological analysis of non-segmented languages directly.
"When revision learning is used, all the mea-sures are improved for both POS bigram and ChaSen."
Improvement is particularly clear for the tagging task.
The numbers of correct morphemes for each POS category tag in the output of ChaSen with and without revision learning are shown in Ta-ble 4.
Many particles are correctly revised by revision learning.
"The reason is that the POS tags for particles are often affected by the fol-lowing words in Japanese, and SVMs can revise such particles because it uses the lexical forms of the following words as the features."
"This is the advantage of our method compared to simple HMMs, because HMMs have difficulty in han-dling a lot of features such as the lexical forms of words."
Our proposal is to revise the outputs of a stochastic model using binary classifiers.
"Brill studied transformation-based error-driven learn-ing (TBL)[REF_CITE], which conducts POS tagging by applying the transformation rules to the POS tags of a given sentence, and has a resemblance to revision learning in that the sec-ond model revises the output of the first model."
"However, our method differs from TBL in two ways."
"First, our revision learner simply answers whether a given pattern is correct or not, and any types of binary classifiers are applicable."
"Second, in our model, the second learner is ap-plied to the output of the first learner only once."
"In contrast, rewriting rules are applied repeat-edly in the TBL."
"Recently, combinations of multiple learners have been studied to achieve high performance[REF_CITE]."
Such methodologies to com-bine multiple learners can be distinguished into two approaches: one is the multi-expert method and the other is the multi-stage method.
"In the former, each learner is trained and answers inde-pendently, and the final decision is made based on those answers."
"In the latter, the multiple learners are ordered in series, and each learner is trained and answers only if the previous learner rejects the examples."
Revision learning belongs to the latter approach.
"In POS tagging, some studies using the multi-expert method were con-ducted (van[REF_CITE]), and[REF_CITE]combined maximum entropy models, TBL, unigram and trigram, and achieved higher accuracy than any of the four learners (97.2% for WSJ corpus)."
"Regarding the multi-stage methods, cascading[REF_CITE]is well known, and[REF_CITE]proposed the sequential learning model and applied it to POS tagging."
"Their methods differ from revision learning in that each learner behaves in the same way and more than one learner is used in their methods, but in revision learning the stochastic model assigns rankings to candidates and the bi-nary classifier selects the output."
"Furthermore, mistakes made by a former learner are fatal in their methods, but is not so in revision learn-ing because the binary classifier works to revise them."
"The advantage of the multi-expert method is that each learner can help each other even if it has some weakness, and generalization er-rors can be decreased."
"On the other hand, the computational cost becomes large because each learner is trained using every training data and answers for every test data."
"In contrast, multi-stage methods can decrease the computa-tional cost, and seem to be effective when a large amount of data is used or when a learner with high computational cost such as SVMs is used."
"In this paper, we proposed the revision learning method which combines a stochastic model and a binary classifier to achieve higher performance with lower computational cost."
"We applied it to English POS tagging and Japanese morpholog-ical analysis, and showed improvement of accu-racy with small computational cost."
"Compared to the conventional one-versus-rest method, revision learning has much lower com-putational cost with almost comparable accu-racy."
"Furthermore, it can be applied not only to a simple multi-class classification task but also to a wider variety of problems such as Japanese morphological analysis."
We explore how active learning with Sup-port Vector Machines works well for a non-trivial task in natural language pro-cessing.
We use Japanese word segmenta-tion as a test case.
"In particular, we discuss how the size of a pool affects the learning curve."
"It is found that in the early stage of training with a larger pool, more la-beled examples are required to achieve a given level of accuracy than those with a smaller pool."
"In addition, we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool."
The experimen-tal results show that our technique requires less labeled examples than those with the technique in previous research.
Corpus-based supervised learning is now a stan-dard approach to achieve high-performance in nat-ural language processing.
"However, the weakness of supervised learning approach is to need an anno-tated corpus, the size of which is reasonably large."
"Even if we have a good supervised-learning method, we cannot get high-performance without an anno-tated corpus."
The problem is that corpus annotation is labour intensive and very expensive.
"In order to overcome this, some unsupervised learning methods and minimally-supervised methods, e.g.,[REF_CITE], have been proposed."
"However, such methods usually de-pend on tasks or domains and their performance of-ten does not match one with a supervised learning method."
"Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them."
"It is very dif-ferent from passive learning, in which a classifier gets labeled examples randomly."
Active learning is a general framework and does not depend on tasks or domains.
It is expected that active learning will reduce considerably manual annotation cost while keeping performance.
"However, few papers in the field of computational linguistics have focused on this approach[REF_CITE]."
"Although there are many active learning methods with various classi-fiers such as a probabilistic classifier[REF_CITE], we focus on active learning with Sup-port Vector Machines (SVMs) because of their per-formance."
"The Support Vector Machine, which is introduced[REF_CITE], is a powerful new statistical learn-ing method."
"Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth."
"SVMs have been recently applied to several natural language tasks, including text classificati[REF_CITE], chunking[REF_CITE], and dependency analysis[REF_CITE]."
SVMs have been greatly successful in such tasks.
"Additionally, SVMs as well as boosting have good theoretical background."
The objective of our research is to develop an ef-fective way to build a corpus and to create high-performance NL systems with minimal cost.
"As a first step, we focus on investigating how active learning with SVMs, which have demonstrated ex-cellent performance, works for complex tasks in nat-ural language processing."
"For text classification, it is found that this approach is effective[REF_CITE]."
"They used less than 10,000 binary features and less than 10,000 examples."
"However, it is not clear that the approach is readily applicable to tasks which have more than 100,000 features and more than 100,000 examples."
We use Japanese word segmentation as a test case.
"The task is suitable for our purpose because we have to handle combinations of more than 1,000 charac-ters and a very large corpus[REF_CITE]exists."
In this section we give some theoretical definitions of SVMs.
Assume that we are given the training data ( x ; y ; : : : ; ( x ; y ; x 2 R ; y 2 f +1 ; 1 g n i i ) l l ) i i
"The decision function g in SVM framework is de-fined as: g ( x ) = sgn( f ( x )) ([Footnote_1]) f ( x ) = X y i i K ( x ; x l i ) + b ([Footnote_2]) i =1 where K is a kernel function, b 2 R is a thresh-old, and i are weights."
1 The figure described here is based on the algorithm[REF_CITE]for their sequential sampling algorithm.
2[REF_CITE]propose three selection algorithms. The method described here is simplest and computationally ef-ficient.
Besides the i satisfy the following constraints:
C; 8 i and X l 0 i y i = 0 ; i i =1 where C is a missclassification cost.
The x i with non-zero i are called Support Vectors.
"For linear SVMs, the kernel function K is defined as:"
K ( x ; x i ) = x x : i
"In this case, Equation 2 can be written as: f ( x ) = w x + b (3) where w = P li =1 y x i i i ."
To train an SVM is to find the i and the b by solving the following optimiza-tion problem:
X 1 X i j y i y j K ( x ; x l l maximize i j ) i i =1 2 i;j =1 C; 8 i and X l subject to 0 i y i = 0 : i i =1
We use pool-based active learning[REF_CITE].
SVMs are used here instead of probabilistic classifiers used by Lewis and Gale.
Figure 1 shows an algorithm of pool-based active learning 1 .
There can be various forms of the algorithm depending on what kind of example is found informative.
Two groups have proposed an algorithm for SVMs active learning[REF_CITE]2 .
Figure 2 shows the selection algo-rithm proposed by them.
This corresponds to (a) and (b) in Figure 1.
"We observed in our experiments that when using the algorithm in the previous section, in the early stage of training, a classifier with a larger pool requires more examples than that with a smaller pool does (to be described in Section 5)."
"In order to overcome the weakness, we propose two new algorithms."
We call them “Two Pool Algorithm” generically.
"It has two pools, i.e., a primary pool and a secondary one, and moves gradually unlabeled examples to the primary pool from the secondary instead of using a large pool from the start of training."
"The primary pool is used directly for selection of examples which are requested a teacher to label, whereas the secondary is not."
The basic idea is simple.
"Since we cannot get good performance when using a large pool at the beginning of training, we enlarge gradually a pool of unlabeled examples."
The outline of Two Pool Algorithm is shown in Figure 3.
"We describe below two variations, which are different in the condition at (d) in Figure 3."
"Our first variation, which is called Two Pool Al-gorithm A, adds new unlabeled examples to the pri-mary pool when the increasing ratio of support vec- tors in the current classifier decreases, because the gain of accuracy is very little once the ratio is down."
This phenomenon is observed in our experiments (Section 5).
This observation has also been reported in previous studies[REF_CITE].
In Two Pool Algorithm we add new unlabeled ex-amples so that the total number of examples includ-ing both labeled examples in the training set and un-labeled examples in the primary pool is doubled.
"For example, suppose that the size of a initial primary pool is 1,000 examples."
"Before starting training, there are no labeled examples and 1,000 unlabeled examples."
"We add 1,000 new unlabeled examples to the primary pool when the increasing ratio of sup-port vectors is down after t examples has been la-beled."
"Then, there are the t labeled examples and the ( 2 ; 000 t ) unlabeled examples in the primary pool."
"At the next time when we add new unlabeled examples, the number of newly added examples is 2,000 and then the total number of both labeled in the training set and unlabeled examples in the pri-mary pool is 4,000."
"Our second variation, which is called Two Pool Algorithm B, adds new unlabeled examples to the primary pool when the number of support vectors of the current classifier exceeds a threshold d ."
The d is defined as: d = N Æ ; 0 &lt; Æ 100 (4) 100 where Æ is a parameter for deciding when unlabeled examples are added to the primary pool and N is the number of examples including both labeled ex-amples in the training set and unlabeled ones in the primary pool.
The Æ must be less than the percentage of support vectors of a training set [Footnote_3] .
"3 Since typically the percentage of support vectors is small (e.g., less than 30 %), we choose around 10 % for Æ . We need further studies to find the best value of Æ before or during train-ing."
"When deciding how many unlabeled examples should be added to the primary pool, we use the strategy as described in the paragraph above."
Many tasks in natural language processing can be formulated as a classification task (van den[REF_CITE]).
"Japanese word segmentation can be viewed in the same way, too[REF_CITE]."
Let a Japanese character sequence be s = c 1 c 2 c m and a boundary b i exist between c i and c i +1 .
The b i is either +1 (word boundary) or 1 (non-boundary).
The word segmentation task can be defined as de-termining the class of the b i .
We use an SVM to determine it.
We assume that each character c i has two attributes.
The first attribute is a character type ( t i ).
"It can be hiragana [Footnote_4] , katakana, kanji (Chinese characters), numbers, English alphabets, kanji-numbers (num-bers written in Chinese), or symbols."
4 Hiragana and katakana are phonetic characters which rep-resent Japanese syllables. Katakana is primarily used to write foreign words.
A character type gives some hints to segment a Japanese sen-tence to words.
"For example, kanji is mainly used to represent nouns or stems of verbs and adjectives."
"It is never used for particles, which are always writ-ten in hiragana."
"Therefore, it is more probable that a boundary exists between a kanji character and a hi-ragana character."
"Of course, there are quite a few exceptions to this heuristics."
"For example, some proper nouns are written in mixed hiragana, kanji and katakana."
The second attribute is a character code ( k i ).
"The range of a character code is from 1 to 6,879."
6 The variance 2 of a set of selected examples x than that of the latter. i is defined
We use here four characters to decide a word boundary.
"A set of the attributes of c i 1 ;c i ;c i +1 , and c i +2 is used to predict the label of the b i ."
"The set consists of twenty attributes: ten for the char-acter type ( t i 1 t i t i +1 t i +2 , t i 1 t i t i +1 , t i 1 t i , t i 1 , t i t i +1 t i +2 , t i t i +1 , t i , t i +1 t i +2 , t i +1 , t i +2 ), and an-other ten for the character code ( k i 1 k i k i +1 k i +2 , k i 1 k i k i +1 , k i 1 k i , k i 1 , k i k i +1 k i +2 , k i k i +1 , k i , k i +1 k i +2 , k i +1 , and k i +2 )."
We used the EDR Japanese Corpus[REF_CITE]for experiments.
"The corpus is assembled from var-ious sources such as newspapers, magazines, and textbooks."
"We se-lected randomly 20,000 sentences for training and 10,000 sentences for testing."
"Then, we created ex-amples using the feature encoding method in Sec-tion 4."
"Through these experiments we used the orig-inal SVM tools, the algorithm of which is based on SMO (Sequential Minimal Optimization)[REF_CITE]."
We used linear SVMs and set a missclas-sification cost C to 0 : 2 .
"First, we changed the number of labeled examples which were randomly selected."
This is an experi-ment on passive learning.
Table 2 shows the accu-racy at different sizes of labeled examples.
"Second, we changed the number of examples in a pool and ran the active learning algorithm in Sec-tion 3.2."
We use the same examples for a pool as those used in the passive learning experiments.
"We selected 1,000 examples at each iteration of the ac-tive learning."
Figure 4 shows the learning curve of this experiment and Figure 5 is a close-up of Fig-ure 4.
We see from Figure 4 that active learning works quite well and it significantly reduces labeled examples to be required.
Let us see how many la-beled examples are required to achieve 96.0 % ac-curacy.
"In active learning with the pool, the size of which is 2,500 sentences (97,349 examples), only 28,813 labeled examples are needed, whereas in pas-sive learning, about 97,000 examples are required."
That means over 70 % reduction is realized by ac-tive learning.
"In the case of 97 % accuracy, approx-imately the same percentage of reduction is realized when using the pool, the size of which is 20,000 sen-tences (776,586 examples)."
Now let us see how the accuracy curve varies de-pending on the size of a pool.
"Surprisingly, the per-formance of a larger pool is worse than that of a smaller pool in the early stage of training [Footnote_5] ."
"5[REF_CITE]have got the similar results in a text classification task with two small pools: 500 and 1000. However, they have concluded that a larger pool is better than a smaller one because the final accuracy of the former is higher"
One rea-son for this could be that support vectors in selected examples at each iteration from a larger pool make larger clusters than those selected from a smaller pool do.
"In other words, in the case of a larger pool, more examples selected at each iteration would be similar to each other."
"We computed variances 6 of each 1,000 selected examples at the learning itera-tion from 2 to 11 (Table 1)."
"The variances of se- lected examples using the 20,000 sentence size pool is always lower than those using the 1,250 sentence size pool."
The result is not inconsistent with our hy-pothesis.
"Before we discuss the results of Two Pool Algo-rithm, we show in Figure 6 how support vectors of a classifier increase and the accuracy changes when using the 2,500 sentence size pool."
"It is clear that after the accuracy improvement almost stops, the in-crement of the number of support vectors is down."
We also observed the same phenomenon with differ-ent sizes of pools.
We utilize this phenomenon in Algorithm A.
"Next, we ran Two Pool Algorithm A [Footnote_7] ."
"7 In order to stabilize the algorithm, we use the following strategy at (d) in Figure 3: add new unlabeled examples to the primary pool when the current increment of support vectors is less than half of the average increment."
The result is shown in Figure 7.
"The accuracy curve of Algo-rithm A is better than that of the previously proposed method at the number of labeled examples roughly up to 20,000."
"After that, however, the performance of Algorithm A does not clearly exceed that of the previous method."
The result of Algorithm B is shown in Figure 8.
"We have tried three values for Æ : 5 %, 10 %, and 20 %."
"The performance with Æ of 10 %, which is best, is plotted in Figure 8."
"As noted above, the improve-ment by Algorithm A is limited, whereas it is re-markable that the accuracy curve of Algorithm B is always the same or better than those of the previous algorithm with different sizes of pools (the detailed information about the performance is shown in Ta-ble 3)."
8 We computed this by simple interpolation.
"To our knowledge, this is the first paper that reports the empirical results of active learning with SVMs for a more complex task in natural language process-ing than a text classification task."
"The experimental results show that SVM active learning works well for Japanese word segmentation, which is one of such complex tasks, and the naive use of a large pool with the previous method of SVM active learning is less effective."
"In addition, we have proposed a novel technique to improve the learning curve when using a large number of unlabeled examples and have eval- uated it by Japanese word segmentation."
Our tech-nique outperforms the method in previous research and can significantly reduce required labeled exam-ples to achieve a given level of accuracy.
"This paper discusses the supervised learn-ing of morphology using stochastic trans-ducers, trained using the Expectation-Maximization (EM) algorithm."
"Two ap-proaches are presented: first, using the transducers directly to model the process, and secondly using them to define a sim-ilarity measure, related to the Fisher ker-nel method[REF_CITE], and then using a Memory-Based Learn-ing (MBL) technique."
"These are evaluated and compared on data sets from English, German, Slovene and Arabic."
Finite-state methods are in large part adequate to model morphological processes in many languages.
"A standard methodology is that of two-level mor-phology[REF_CITE]which is capable of handling the complexity of Finnish, though it needs substantial extensions to handle non-concatenative languages such as Arabic[REF_CITE]."
"These mod-els are primarily concerned with the mapping from deep lexical strings to surface strings, and within this framework learning is in general difficult[REF_CITE]."
In this paper I present algorithms for learn-ing the finite-state transduction between pairs of un-inflected and inflected words. – supervised learning of morphology.
"The techniques presented here are, however, applicable to learning other types of string transductions."
"Memory-based techniques, based on principles of non-parametric density estimation, are a powerful form of machine learning well-suited to natural lan-guage tasks."
A particular strength is their ability to model both general rules and specific exceptions in a single framework (van den[REF_CITE]).
However they have generally only been used in supervised learning techniques where a class label or tag has been associated to each feature vector.
"Given these manual or semi-automatic class labels, a set of features and a pre-defined distance function new in-stances are classified according to the class label of the closest instance."
"However these approaches are not a complete solution to the problem of learning morphology, since they do not directly produce the transduction."
The problem must first be converted into an appropriate feature-based representation and classified in some way.
"The techniques presented here operate directly on sequences of atomic sym-bols, using a much less articulated representation, and much less input information."
"It is possible to apply the EM algorithm to learn the parameters of stochastic transducers,[REF_CITE].[REF_CITE]showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words."
In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient de- scent algorithm to maximize the conditional proba-bility.
"The models used here are Stochastic Non-Deterministic Finite-State Transducers (FST), or Pair Hidden Markov Models[REF_CITE], a name that emphasizes the similarity of the train-ing algorithm to the well-known Forward-Backward training algorithm for Hidden Markov Models."
"Instead of outputting symbols in a single stream, however, as in normal Hidden Markov Models they output them on two separate streams, the left and right streams."
In general we could have different left and right alphabets; here we assume they are the same.
"At each transition the FST may output the same symbol on both streams, a symbol on the left stream only, or a symbol on the right stream only."
"I call these  ,  and  outputs respectively."
For each state the sum of all these output parameters over the alphabet must be one.  
"Since we are concerned with finite strings rather than indefinite streams of symbols, we have in ad-dition to the normal initial state , an explicit end state , such that the FST terminates when it enters this state."
The FST then defines a joint probabil-ity distribution on pairs of strings from the alphabet.
"Though we are more interested in stochastic trans-ductions, which are best represented by the condi-tional probability of one string given the other, it is more convenient to operate with models of the joint probability, and then to derive the conditional prob-ability as needed later on."
"It is possible to modify the normal dynamic-programming training algorithm for HMMs, the Baum-Welch algorithm[REF_CITE]to work with FSTs as well."
This algorithm will maxi-mize the joint probability of the training data.
We define the forward and backward proba-bilities as follows.
"Given two strings ! ! ! #&quot; and $ ! ! ! $ % we define the forward probabilities (&amp; *) + as the probability that it will start from and output ! ! ,! .- on the left stream, and $ ! ! ! $ / on the right stream and be in state , and the backward probabilities 0 &apos; *) + as the probability that starting from state it will output  ! ! !, &quot; , on the right and ,$ 52/ ! ! ! $ % on the left and then terminate, ie end in state ."
"We can calculate these using the following recur-rence relations: &amp; &apos; *) + 6 =&lt;  $ / &amp; &apos; 7 *) :+ 9 &lt;; &apos;87 &amp; &apos; 7 *) 9 + &lt;; =    &apos;87 = ,  $ /  &amp; &apos;87 *) 9 +:9 &lt;;  0 &apos; *) + &gt;  0 &apos; 7 *) + ?&lt;; =   $ / , =  .-32 , =   0  *) @ + &lt;; = &apos;7 0 &apos;87 *) ? + ?&lt;; =    $ / , = where, in these models,   $ /, is zero un-less - is equal to $ / ."
"Instead of the normal two-dimensional trellis discussed in standard works on HMMs, which has one dimension corresponding to the current state and one corresponding to the posi-tion, we have a three-dimensional trellis, with a di-mension for the position in each string."
"With these modifications, we can use all of the standard HMM algorithms."
"In particular, we can use this as the ba-sis of a parameter estimation algorithm using the expectation-maximization theorem."
We use the for-ward and backward probabilities to calculate the ex-pected number of times each transition will be taken; at each iteration we set the new values of the parame-ters to be the appropriately normalized sums of these expectations.
"Given a FST, and a string , we often need to find the string $ that maximizes ; $ ."
"This is equiv-  alent to the task of finding the most likely string generated by a HMM, which is NP-hard (Casacu-berta and de la[REF_CITE]), but it is possible to sample from the conditional distribution ; $  , which allows an efficient stochastic computation."
"If we consider only what is output on the left stream, the FST is equivalent to a HMM with null transitions corresponding to the , transitions of the FST."
We can remove these using standard techniques and then use this to calculate the left backward probabilities for a particular string : 0 &apos; *) defined as the prob-ability that starting from state the FST generates .-32 ! ! ! #&quot; on the left and terminates.
"Then if one samples from the FST, but weights each transition by the appropriate left backward probability, it will be equivalent to sampling from the conditional distri-bution of $  ."
"We can then find the string $ that is most likely given , by generating randomly from ; $  ."
"After we have generated a number of strings, we can sum ; $  for all the observed strings; if the difference between this sum and 1 is less than the maximum value of ; $  we know we have found the most likely $ ."
"In practice, the distributions we are interested in often have a $ with ; $   ! ; in this case we immediately know that we have found the maximum."
"We then model the morphological process as a transduction from the lemma form to the inflected form, and assume that the model outputs for each input, the output with highest conditional or joint probability with respect to the model."
"There are a number of reasons why this simple approach will not work: first, for many languages the inflected form is lexically not phonologically specified and thus the model will not be able to identify the cor-rect form; secondly, modelling all of the irregular exceptions in a single transduction is computation-ally intractable at the moment."
"One way to improve the efficiency is to use a mixture of models as dis-cussed[REF_CITE], each corresponding to a morphological paradigm."
"The productivity of each paradigm can be directly modelled, and the class of each lexical item can again be memorized."
There are a number of criticisms that can be made of this approach.
Many of the models produced merely memo-rize a pair of strings – this is extremely ineffi-cient.
"Though the model correctly models the produc-tivity of some morphological classes, it mod-els this directly."
A more satisfactory approach would be to have this arise naturally as an emer-gent property of other aspects of the model.
These models may not be able to account for some psycho-linguistic evidence that appears to require some form of proximity or similarity.
In the next section I shall present a technique that addresses these problems.
The method used is a simple application of the infor-mation geometry approach introduced[REF_CITE]in the field of bio-informatics.
The central idea is to use a generative model to ex-tract finite-dimensional features from a symbol se-quence.
"Given a generative model for a string, one can use the sufficient statistics of those generative models as features."
The vector of sufficient statis-tics can be thought of as a finite-dimensional rep-resentation of the sequence in terms of the model.
"This transformation from an unbounded sequence of atomic symbols to a finite-dimensional real vector is very powerful and allows the use of Support Vec-tor Machine techniques for classification.[REF_CITE]recommend that instead of us-ing the sufficient statistics, that the Fisher scores are used, together with an inner product derived from the Fisher information matrix of the model."
The Fisher scores are defined for a data point and a particular model as -  ;  (1) -
"The partial derivative of the log likelihood is easy to calculate as a byproduct of the E-step of the EM algorithm, and has the value for HMMs[REF_CITE]of  -  9  /  - (2) - where - is the indicator variable for the parameter ) , and / is the indicator value for the state + where - leaves state + ; the last term reflects the constraint that the sum of the parameters must be one."
The kernel function is defined as   ! & quot;#$ &amp;% (3) where ! $ is the Fisher information matrix.
"This kernel function thus defines a distance be-tween elements, #  +-* , (4) &apos; &gt;  ) 9 ("
This distance in the feature space then defines a pseudo-distance in the example space.
The name information geometry which is some-times used to describe this approach derives from a geometrical interpretation of this kernel.
"For a parametric model with free parameters, the set of all these models will form a smooth -dimensional manifold in the space of all distributions."
The curva-ture of this manifold can be described by a Rieman-nian tensor – this tensor is just the expected Fisher information for that model.
It is a tensor because it transforms properly when the parametrization is changed.
"In spite of this compelling geometric explanation, there are difficulties with using this approach di-rectly."
"First, the Fisher information matrix cannot be calculated directly, and secondly in natural lan-guage applications, unlike in bio-informatic applica-tions we have the perennial problem of data sparsity, which means that unlikely events occur frequently."
"This means that the scaling in the Fisher scores gives extremely high weights to these rare events, which can skew the results."
Accordingly this work uses the unscaled sufficient statistics.
This is demonstrated below.
"Given a transducer that models the transduction from uninflected to inflected words, we can ex-tract the sufficient statistics from the model in two ways."
We can consider the statistics of the joint model ; $ or the statistics of the conditional model ; $ .
"Here we have used the condi-tional model, since we are interested primarily in the change of the stem, and not the parts of the stem that remain unchanged."
"It is thus possible to use either the features of the joint model or of the conditional model, and it is also possible to either scale the fea-tures or not, by dividing by the parameter value as in Equation 2."
The second term in Equation 2 cor-responding to the normalization can be neglected.
We thus have four possible features that are com-pared on one of the data sets in Table 4.
"Based on the performance here we have chosen the unscaled conditional sufficient statistics for the rest of the ex-periments presented here, which are calculated thus: -   - $  9  -  $  (5) tense of apply (6pl3)."
"This example shows that the most likely transduction is the suffix Id, which is in-correct, but the MBL approach gives the correct re-sult in line 2."
Given an input string we want to find the string $ $ is very close to some ele-such that the pair ment of the training data.
We can do this in a num-ber of different ways.
Clearly if is already in the training set then the distance will be minimized by choosing $ to be one of the outputs that is stored for input $ ; the distance in this case will be zero.
Other-wise we sample repeatedly (here we have taken 100 samples) from the conditional distribution of each of the submodels.
"This in practice seems to give good results, though there are more principled criteria that could be applied."
We give a concrete example using the LING En-glish past tense data set described below.
"Given an unseen verb in its base form, for example apply, in phonetic transcription 6pl3, we generate 100 sam-ples from the conditional distribution."
"The five most likely of these are shown in Table 1, together with the conditional probability, the distance to the clos-est example and the closest example."
"We are using a -nearest-neighbor rule with , since there are irregular words that have com-pletely idionsyncratic inflected forms."
"It would be possible to use a larger value of , which might help with robustness, particularly if the token frequency was also used, since irregular words tend to be more common."
In summary the algorithm proceeds as follows:
We train a small Stochastic Transducer on the pairs of strings using the EM algorithm.
We derive from this model a distance function between two pairs of strings that is sensitive to the properties of this transduction.
We store all of the observed pairs of strings.
"Given a new word, we sample repeatedly from the conditional distribution to get a set of pos-sible outputs."
We select the output such that the input/output pair is closest to one of the oberved pairs.
The data sets used in the experiments are summa-rized in Table 2.
A few additional comments follow.
LING These are in UNIBET phonetic transcription.
EPT In SAMPA transcription.
The training data consists of all of the verbs with a non-zero lemma spoken frequency in the 1.3 million word CO-BUILD corpus.
The test data consists of all the remaining verbs.
This is intended to more accurately reflect the situation of an infant learner.
GP This is a data set of pairs of German nouns in singular and plural form prepared from the CELEX lexical database.
This is a data set prepared[REF_CITE].
"Its consists of pairs of sin-gular and plural nouns, in Modern Standard Arabic, randomly selected from the standard Wehr dictionary in a fully vocalized ASCII transcription."
"It has a mixture of broken and sound plurals, and has been simplified in the sense that rare forms of the broken plural have been removed."
Table 4 shows a comparison of the four possible fea-ture sets on the Ling data.
We compared the performance of the models evaluated using them directly to model the transduction using the conditional likelihood (CL) and using the MBL approach with the unscaled con-ditional features.
"Based on these results, we used the unscaled conditional features; subsequent exper-iments confirmed that these performed best."
The results are summarized in Table 3.
Run-times for these experiments were from about 1 hour to 1 week on a current workstation.
"There are a few re-sults to which these can be directly compared; on the LING data set,[REF_CITE]re-port figures of approximately 90% using a logic pro-gram that learns decision lists for suffixes."
"For the Arabic data sets,[REF_CITE]do not present results on modelling the transduction on words not in the training set; however they re-port scores of 63.8% (0.64%) using a neural network classifier."
"The data is classified according to the type of the plural, and is mapped onto a syllabic skele-ton, with each phoneme represented as a bundle of phonological features. for the data set SLOVENE,[REF_CITE]report scores of 97.4%[REF_CITE].2% for CLOG."
This uses a logic pro-gramming methodology that specifically codes for suffixation and prefixation alone.
"On the very large and complex German data set, we score 70.6%; note however that there is substantial disagreement be-tween native speakers about the correct plural of nonce words[REF_CITE]."
"We observe that the MBL approach significantly outperforms the condi-tional likelihood method over a wide range of ex-periments; the performance on the training data is a further difference, the MBL approach scoring close to 100%, whereas the CL approach scores only a lit-tle better than it does on the test data."
"It is certainly possible to make the conditional likelihood method work rather better than it does in this paper by pay-ing careful attention to convergence criteria of the models to avoid overfitting, and by smoothing the models carefully."
In addition some sort of model size selection must be used.
A major advantage of the MBL approach is that it works well without re- quiring extensive tuning of the parameters.
"In terms of the absolute quality of the results, this depends to a great extent on how phonologically predictable the process is."
"When it is completely predictable, as in SLOVENE the performance ap-proaches 100%; similarly a large majority of the less frequent words in English are completely regu-lar, and accordingly the performance on EPT is very good."
"However in other cases, where the morphol-ogy is very irregular the performance will be poor."
"In particular with the Arabic data sets, the NAKISA data set is very small compared to the complexity of the process being learned, and the MCCARTHY data set is rather noisy, with a large number of er-roneous transcriptions."
"With the German data set, though it is quite irregular, and the data set is not frequency-weighted, so the frequent irregular words are not more likely to be in the training data, there is a lot of data, so the algorithm performs quite well."
"In addition to these formal evaluations we exam-ined the extent to which this approach can account for some psycho-linguistic data, in particular the data collected[REF_CITE]on the mild productivity of irregular forms in the En-glish past tense."
Space does not permit more than a rather crude summary.
"They prepared six data sets of 10 pairs of nonce words together with regular and irregular plurals of them: a sequence of three data sets that were similar to, but progressively fur-ther away from sets of irregular verbs (prototypical-intermediate- and distant- pseudoirregular – PPI IPI and DPI), and another set that were similar to sets of regular verbs (prototypical-, intermediate- and distant- pseudoregular PPR, IPR and DPR)."
"Thus the first data sets contained words like spling which would have a vowel change form of splung and a regular suffixed form of splinged, and the second data sets contained words like smeeb with regular smeebed and irregular smeb."
"They asked subjects for their opinions on the acceptabilities of the stems, and of the regular (suffixed) and irregular (vowel change) forms."
A surprising result of this was that subtracting the rating of the past tense form from the rating of the stem form (in order to control for the varying acceptability of the stem) gave differ-ent results for the two data sets.
"With the pseudo-irregular forms the irregular form got less acceptable as the stems became less like the most similar irreg-ular stems, but with the pseudo-regulars the regular form got more acceptable."
This was taken as evi-dence for the presence of two qualitatively distinct modules in human morphological processing.
"In an attempt to see whether the models presented here could account for these effects, we transcribed the data into UNIBET transcription and tested it with the models prepared for the LING data set."
"We calculated the average negative log probability for each of the six data sets in 3 ways: first we cal-culated the probability of the stem alone to model the acceptability of the stem; secondly we calcu-lated the conditional probability of the regular (suf-fixed form), and thirdly we calculated the condi-tional probability of the irregular (vowel change) form of the word."
Then we calculated the differ-ence between the figures for the appropriate past tense form from the stem form.
This is unjustifiable in terms of probabilities but seems the most natu-ral way of modelling the effects reported[REF_CITE].
These results are presented in Ta-ble 5.
"Interestingly we observed the same effect: a decrease in “acceptability” for irregulars, as they be-came more distant, and the opposite effect for regu-lars."
"In our case though it is clear why this happens – the probability of the stem decreases rapidly, and this overwhelms the mild decrease in the conditional probability."
The productivity of the regular forms is an emergent property of the system.
"This is an advantage over previous work using the EM algorithm with SFST, which directly specified the productivity as a param-eter."
Using the EM algorithm to learn stochastic transduc-ers has been known for a while in the biocomputing field as a generalization of edit distance[REF_CITE].
The Fisher kernel method has not been used in NLP to our knowledge before though we have noted two recent papers that have some points of similarity.
"First,[REF_CITE]derive a Maximum Entropy tagger, by training a HMM and using the most likely state sequence of the HMM as features for the Maximum Entropy tagging model."
"Secondly, (van den[REF_CITE]) presents an ap-proach that is again similar since it uses rules, in-duced using a symbolic learning approach as fea-tures in a nearest-neighbour approach."
We have presented some algorithms for the super-vised learning of morphology using the EM algo-rithm applied to non-deterministic finite-state trans-ducers.
"We have shown that a novel Memory-based learn-ing technique inspired by the Fisher kernel method produces high performance in a wide range of lan-guages without the need for fine-tuning of parame-ters or language specific representations, and that it can account for some psycho-linguistic data."
"These techniques can also be applied to the unsupervised learning of morphology, as described[REF_CITE]."
"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs)."
"The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates."
We describe the-ory justifying the algorithms through a modi cation of the proof of conver-gence of the perceptron algorithm for classi cation problems.
"We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
"Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see[REF_CITE]for their use on part-of-speech tagging, and[REF_CITE]for their use on a FAQ segmentation task."
ME models have the advantage of being quite exible in the features that can be incorporated in the model.
"However, recent theoretical and experimental re-sults[REF_CITE]have highlighted problems with the parameter estimation method for ME models."
"In response to these problems, they describe alternative parameter estimation methods based on Conditional Markov Random Fields (CRFs).[REF_CITE]give exper-imental results suggesting that CRFs can per-form signi cantly better than ME models. tionInalgorithmsthis paperwhichwe describeare naturalparameteralternativesestima-to CRFs."
"The algorithms are based on the percep-tron algorithm[REF_CITE], and the voted or averaged versions of the perceptron described in (Freund &amp;[REF_CITE])."
"These algorithms have been shown by (Freund &amp;[REF_CITE]) to be competitive with modern learning algorithms such as support vector machines; however, they have previously been applied mainly to classi -cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing. tronThisalgorithmpaper describesfor taggingvariantsproblemsof the."
"Thepercep-al-gorithms rely on Viterbi decoding of training examples, combined with simple additive up-dates."
"We describe theory justifying the algo-rithm through a modi cation of the proof of con-vergence of the perceptron algorithm for classi-oncationpart-of-speechproblems.taggingWe giveandexperimentalbase noun phraseresults chunking, in both cases showing improvements over results for a maximum-entropy tagger (a 11.9% relative reduction in error for POS tag-ging, a 5.1% relative reduction in error for NP chunking)."
"Although we concentrate on tagging problems in this paper, the theoretical frame-work and algorithm described in section 3 of this paper should be applicable to a wide va-riety of models where Viterbi-style algorithms can be used for decoding: examples are Proba-bilistic Context-Free Grammars, or ME models for parsing."
See[REF_CITE]for other applica-tions of the voted perceptron to NLP problems. [Footnote_1]
"1 The theorems in section 3, and the proofs in sec-tion 5, apply directly to the work in these other papers."
"In this section, as a motivating example, we de-scribe a special case of the algorithm in this paper: the algorithm applied to a trigram tag-ger."
"In a trigram HMM tagger, each trigram of tags and each tag/word pair have associated parameters."
"We write the parameter associated with a trigram h x; y; z i as x;y;z , and the param-eter associated with a tag/word pair ( t;w ) as eters t;w . toA becommonestimatesapproachof conditionalis to takeprobabilitiesthe param-: x For ;y;z =conveniencelog P ( z j x; we y ), will t;w =uselog wP ( w as j t )short-. hand for a sequence of words [ w [1:1 n ; ] w 2 :::w n ], and t [1: n ] as shorthand for a taq sequence [ t 1 ; t 2 : : : t n ]."
In a trigram tagger the score for a tagged sequence t [1: n ] paired with a word se-quence w [1: n ] is 2 P in =1 t i 2 ;t i 1 ; t i + P ni =1 t i ;w i .
When the parameters are conditional probabil-ities as above this \score&quot; is an estimate of the log of the joint probability P ( w [1: n ] ; t [1: n ] ).
"The Viterbi algorithm can be used to nd the highest scoring tagged sequence under this score. rameterAs an alternativeestimates, thisto maximumpaper will{likelihoodpropose thepa-following estimation algorithm."
"Say the train-ing set consists of n tagged sentences, the i &apos;th sentence being of length n i ."
We will write these examples as ( w [ i 1: n ] ; t i [1: n ] ) for i = 1 : : : n .
Then the training algorithm i is i as follows: of iterationsChoose aoverparameterthe training T de setning. 3 the number to beInitiallyzero. set all parameters x;y;z and t;w algorithmFor t =to1 : nd : : T the ; i =best1 : : : tagged n : Usesequencethe Viterbifor sentence w i [1: n ] under the current parameter settings: we call i this tagged sequence z [1: n i ] .
"For every tag trigram h x;y;z i seen c 1 times in t i [1: n i ] and c 2 times in z [1: n i ] where c 1 6 = c 2 set pair x;y;z h t = ; w i x seen ;y;z + c 1 c 1 times c 2 . inFor( w i [ every 1: i tag/word i ] ; t [1: n i ] ) and c 2 times in ( w i [1: n i ] ;z [1: n i ] ) where n c 1 = 6 c 2 set t; As w =an t; example w + c 1 , c say 2 . the i &apos;th tagged sentence ( w [ i 1: n i ] ; t i [1: n i ] ) in training data is the/D man/N saw/V the/D dog/N and under the current parameter settings the highest scoring tag sequence ( w [ i 1: n i ] ; z [1: n i ] ) is the/D man/N saw/N the/D dog/N Then the parameter update will add 1 to the parameters D;N;V , N;V;D , V;D;N , V;saw and subtract 1 from the parameters D;N;N , N;N;D , fect N;D of ;N ,increasing N;saw . theIntuitivelyparameterthisvalueshas forthefea-ef-tures which were \missing&quot; from the proposed sequence z [1: n i ] , and downweighting parameter values for \incorrect&quot; features in the sequence z [1: n i ] ."
"Note that if z [1: n i ] = t i [1: n ] | i.e., the proposed tag sequence is correct | i no changes are made to the parameter values."
We now describe how to generalize the algorithm to more general representations of tagged se-quences.
"In this section we describe the feature-vector representations which are commonly used in maximum-entropy models for tagging, and whichIn maximum-entropyare also used in thistaggerspaper.[REF_CITE], the tagging prob-lem is decomposed into sequence of decisions in tagging the problem in left-to-right fashion."
At each point there is a \history&quot; { the context in which a tagging decision is made { and the task is to predict the tag given the history.
"Formally, a history is a 4-tuple h t 1 ;t 2 ;w [1: n ] ;i i where t 1 ; t 2 are the previous two tags, w [1: n ] is an ar-ray specifying the n words in the input sentence, and i is the index of the word being tagged."
We useMaximum-entropy H to denote the setmodelsof allrepresentpossible historiesthe tag-. ging task through a feature-vector representation of history-tag pairs.
A feature vector representa-tion :  !
R d is a function that maps a history{tag pair to a d -dimensional feature vec-tor.
Each component s ( h;t ) for s = 1 ::: d could be an arbitrary function of ( h;t ).
"It is common (e.g., see[REF_CITE]) for each feature s to be an indicator function."
"For ex-ample, one such 8 feature might be & gt; 1 if current word w i is the 1000 ( h; t ) = &lt;&gt;: 0 andotherwise t = DT"
Similar features might be de ned for every word/tag pair seen in training data.
"Another feature type might track trigrams of tags, for ex-ample 1001 ( h; t ) = 1 if h t 2 ; t 1 ; t i = h D, N, V i and 0 otherwise."
"Similar features would be de-realnedadvantagefor all trigramsof theseof tagsmodelsseencomesin trainingfrom .theA freedom in de ning these features: for example,[REF_CITE]both describe feature sets which would be diÆcult to incorporate in a generative model. of Inhistoryaddition/tag topairsfeature, we willvectorndrepresentationsit convenient to de ne feature vectors of ( w [1: n ] ;t [1: n ] ) pairs is an entire tag sequence."
"We usewhere w [1: n ] is a sequence of n words, andto t [1 de- : n ] note a function from ( w [1: n ] ;t [1: n ] ) pairs to d-todimensionalas a \globalfeature&quot; vectorsrepresentation."
"We will, inoftencontrastrefer toglobalasrepresentationsa \local&quot; representationconsidered."
Thein thisparticularpaper are simple functions of local representations: s ( w [1: n ] ; t [1: n ] ) = X n s ( h i ; t i ) (1) i =1 where h i = h t i 1 ;t i 2 ;w [1: n ] ;i i .
Each global feature s ( w [1: n ] ;t [1: n ] ) is simply the value for the local representation s summed over all his-tory/tag pairs in ( w [1: n ] ;t [1: n ] ).
"If the local fea-tures are indicator functions, then the global fea-tures will typically be \counts&quot;."
"For example, with 1000 de ned as above, 1000 ( w [1: n ] ;t [1: n ] ) is the number of times the is seen tagged as DT in the pair of sequences ( w [1: n ] ; t [1: n ] )."
Intogethermaximum-entropywith a parametertaggers thevectorfeature 2 vectors R d are used to de ne a conditional probability distri-bution over tags given a history as
P s s s ( h;t ) P ( t j h; ) = e Z ( h; ) where Z ( h; ) =
P l 2T e P s s s ( h;l ) .
The log of this probability has the form log p ( t j h; ) =
"P d probability s =1 s s ( h for ; t )a (log w [1: Z n ] ( ;ht [1 ; : n ) ] ,) andpairhencewill bethe log"
X X d i s =1 s s ( h i ; t i )
X i log Z ( h i ; ) (2) where h i = h t i 1 ;t i 2 ;w [1: n ] ;i i .
"Given parame-ter values , and an input sentence w [1: n ] , the highest probability tagged sequence under the formula in Eq. 2 can be found eÆciently using the Viterbi algorithm. trainingThe parameterset of sentencevector/tagged-sequenceis estimated frompairsa."
"Maximum-likelihood parameter values can be estimated using Generalized Iterative Scaling[REF_CITE], or gradient descent methods."
In some cases it may be preferable to apply a bayesian approach which includes a prior over parameter values.
We now describe an alternative method for es-timating parameters of the model.
"Given a se-quence of words w [1: n ] and a sequence of part of speech tags, t [1: n ] , we will take the \score&quot; of a tagged sequence to be"
X n X d d i =1 s =1 s s ( h i ; t i ) = s X =1 s s ( w [1: n ] ; t [1: n ] ) : where h i is again h t i 1 ;t i 2 ;w [1: n ] ;i i .
"Note that this is almost identical to Eq. 2, but without the local normalization terms log Z ( h i ; )."
"Under this method for assigning scores to tagged se-quences, the highest scoring sequence of tags for an input sentence can be found using the Viterbi algorithm. (We can use an almost identical de-coding algorithm to that for maximum-entropy taggers, the di erence being that local normal-ization terms do not need to be calculated.) ureWe1.thenTheproposealgorithmthe trainingtakes T algorithmpasses overin theg-training sample."
All parameters are initially set to be zero.
Each sentence in turn is decoded us-ing the current parameter settings.
"If the high-est scoring sequence under the current model is not correct, the parameters s are updated in a simple additive fashion. torNotefunctionsthat ,ifthenthe thelocalglobalfeaturesfeatures s are indica-will be counts."
"In this case the update will add s c s d s to each parameter s , where c s is the number of times the s &apos;th feature occurred in the cor-rect tag sequence, and d s is the number of times it occurs in highest scoring sequence under the current model."
"For example, if the features s are indicator functions tracking all trigrams and word/tag pairs, then the training algorithm is identical to that given in section 2.1."
"There is a simple re nement to the algorithm in gure 1, called the \averaged parameters&quot; method."
De ne ts;i to be the value for the s &apos;th parameter after the i &apos;th training example has been processed in pass t over the training data.
Then the P \averaged parameters&quot; are de ned as It s is= simple t =1 :::T to ;i = modify 1 :::n ts;i = the nT algorithmfor all s =to1 store : : : d . this additional set of parameters.
Experiments in section 4 show that the averaged parameters perform signi cantly better than the nal pa-rameters Ts;n .
The theory in the next section gives justi cation for the averaging method.
"In this section we give a general algorithm for problems such as tagging and parsing, and give theorems justifying the algorithm."
We also show how the tagging algorithm in gure 1 is a spe-cial case of this algorithm.
"Convergence theo-rems for the perceptron applied to classi cation problems appear in (Freund &amp;[REF_CITE]) { the results in this section, and the proofs in sec-tion 5, show how the classi cation results can be carriedThe taskover istotoproblemslearn asuchmappingas taggingfrom.inputs x 2 X to outputs y 2 Y ."
"For example, X might be a set of sentences, with Y being a set of pos-sible tag sequences."
Training examples ( x i ; y i ) for i = 1 : : : n .
A function GEN which enumerates a set of candidates GEN ( x ) for an input x .
X A representation Y to a feature vectormapping( x; each y ) 2 ( x R d ; . y ) 2 A parameter vector 2 R d . pingThefromcomponentsan input GEN x to an ; outputand F de( x ne) througha map- F ( x ) = arg y 2 GEN max ( x ) ( x; y ) where ( x; y ) is the inner product parameter s s s ( x; values y ).
Theusinglearningthe trainingtask is toexamplesset the P as evidence. mappedThe taggingto this settingproblemas infollowssection: 2 can be The training examples are sentence/tagged-sequence pairs: x i = w [ i 1: n i ] and y i = t i [1: n i ] for i = 1 : : : n .
"Given a set of possible tags T , we de ne GEN ( w [1: n ] ) ="
"T n , i.e., the function GEN maps an input sentence w [1: n ] to the set of all tag sequences of length n ."
"The( w representation ( x; y ) = feature [1: n ] ; vectors t [1: n ] ) is ( h de ; t )nedwherethrough( h; t ) is a local history/tag pair. (See Eq. 1.) weightsFigure .2 showsIt can anbe verialgorithmed thatforthesettingtrainingthe algorithm for taggers in gure 1 is a special case of thisas justalgorithmdescribed, if.we de ne ( x i ; y i ) ; GEN and theWeconvergencewill now giveof thisa algorithmrst theorem."
Thisregardingtheorem therefore also describes conditions under which the algorithm in gure 1 converges.
"First, we need the following de nition:"
De nition 1 Let GEN ( x i ) =
GEN ( x i ) f y i g .
In other words GEN ( x i ) is the set of incorrect candidates for an example x i .
We will say that a training sequence ( x i ; y i ) for i = 1 : : : n is separable with margin Æ &gt; 0 if there exists some vector U with jj U jj = 1 such that 8 i; 8 z 2 GEN ( x i ) ; U ( x i ; y i )
"U ( x i ; z ) Æ (3) ( jj U jj is the 2-norm of U , i.e., jj U jj = pP s U 2 s .) sectionWe can5 forthena proofstate)the: following theorem (see Theorem 1 For any training sequence ( x i ;y i ) which is separable with margin Æ , then for the perceptron algorithm in gure 2"
Number of mistakes[REF_CITE]where R is a constant ( x i ; z ) such jj R .that 8 i; 8 z 2
"GEN ( x i ) jj ( x i ;y i ) eterThisvectortheorem U whichimpliesmakesthat ifzerothereerrorsis a param-on the training set, then after a nite number of itera-tions the training algorithm will have converged to parameter values with zero training error."
"A crucial point is that the number of mistakes is in-dependent of the number of candidates for each example (i.e. the size of GEN ( x i ) for each i ), depending only on the separation of the training data, where separation is de ned above."
This is important because in many NLP problems GEN ( x ) can be exponential in the size of the inputs.
All of the convergence and generaliza-tion results in this paper depend on notions of separabilityTwo questionsrathercomethantothemindsize.
"Firstof GEN , are. there guarantees for the algorithm if the training data is not separable?"
"Second, performance on a training sample is all very well, but what does this guarantee about how well the algorithm generalizes to newly drawn test examples? (Fre-und &amp;[REF_CITE]) discuss how the theory can be extended to deal with both of these questions."
The next sections describe how these results can be applied to the algorithms in this paper.
In this section we give bounds which apply when the data is not separable.
"First, we need the following de nition:"
"De nition 2 Given a sequence ( x i ;y i ) , for a U , Æ pair de ne m i = U ( x i ; y i ) max z 2 GEN ( x i )"
U ( x i ; z ) and i = max f 0 ;Æ m i g .
"Finally, de ne D U ;Æ = pP ni =1 i 2 . is Theto separatingvalue D U the ;Æ istraininga measuredataofwithhowmarginclose U Æ ."
D U ;Æ is 0 if the vector U separates the data with at least margin Æ .
"If U separates almost all of the examples with margin Æ , but a few examples are incorrectly tagged or have margin less than Æ , Thethenfollowing D U ;Æ willtheoremtake a relativelythen appliessmall(seevaluesec-. tion 5 for a proof):"
"Theorem 2 For any training sequence ( x i ;y i ) , for the rst pass over the training set of the perceptron algorithm in gure 2,"
Number of mistakes min U ;Æ ( R + ÆD 2 U ;Æ ) 2
"GEN ( x i ) where R is jj a ( x constant i ;y ) ( x such ;z ) jj that R , 8 i; and 8 z the 2 min is taken over Æ &gt; 0 , i jj U jj = i 1 ."
"This theorem implies that if the training data is \close&quot; to being separable with margin Æ { i.e., there exists some U such that D U ;Æ is rela-tively small { then the algorithm will again make a small number of mistakes."
Thus theorem 2 shows that the perceptron algorithm can be ro-bustculttoorsomeimpossibletrainingto datatag correctlyexamples. being dif-
"Theorems 1 and 2 give results bounding the number of errors on training samples, but the question we are really interested in concerns guarantees of how well the method generalizes to new test examples."
"Fortunately, there are several theoretical results suggesting that if the perceptron algorithm makes a relatively small number of mistakes on a training sample then it is likely to generalize well to new examples."
"This section describes some of these results, which originally appeared in (Freund &amp;[REF_CITE]), and are derived directly from results[REF_CITE]. tronFirstalgorithmwe de ,nethea modivoted cationperceptronof the."
Wepercep-can consider the rst pass of the perceptron algo-rithm to build a sequence of parameter set-tings 1 ;i for i = 1 ::: n .
"For a given test ex-v i = argmax z 2 GEN ( x ) 1 ;i ample x , each of these will (de x;z ne). anTheoutputvoted perceptron takes the most frequently occurring output in the set f v 1 :::v n g ."
"Thus the voted perceptron is a method where each of the pa-rameter settings 1 ;i for i = 1 ::: n get a sin-gle vote for the output, and the majority wins."
"The averaged algorithm in section 2.5 can be considered to be an approximation of the voted method, with the advantage that a single decod-ing with the averaged parameters can be per-formed, rather than n decodings with each of theIn n analyzingparameterthesettingsvoted.perceptron the one as-sumption we will make is that there is some unknown distribution P ( x; y ) over the set X Y , and that both training and test examples are drawn independently, identically distributed (i.i.d.) from this distribution."
Corollary 1 of (Freund &amp;[REF_CITE]) then states: Theorem 3 (Freund &amp;[REF_CITE])
Assume all ex-h ( x 1 ; y 1 ) i : : : ( x n ; y n ) i be a sequenceatof trainingrandom.examplesLet amples are generated i.i.d. and let ( x n +1 ;y n +1 ) be a test example.
Then the prob-ability (over the choice of all n + 1 examples) that the voted-perceptron algorithm does not predict y n +1 on in-put x n +1 is at most n +2 1 E n +1 min U ;Æ ( R +
"ÆD 2 U ;Æ ) 2 where E n +1 [] is an expected value taken over n + 1 ex-amples, R and D U ;Æ are as de ned above, and the min is taken over Æ &gt; 0 , jj U jj = 1 ."
"We ran experiments on two data sets: part-of-speech tagging on the Penn Wall Street Journal treebank[REF_CITE], and base noun-phrase recognition on the data sets originally in-troduced[REF_CITE]."
"In each case we had a training, development and test set."
"For part-of-speech tagging the training set was sections 0{18 of the treebank, the development set was sections 19{21 and the nal test set was sections 22-24."
"In NP chunking the training set was taken from section 15{18, the development set was section 21, and the test set was section 20."
For POS tagging we report the percentage of correct tags on a test set.
For chunking we report F-measure in recovering bracketings cor-responding to base NP chunks.
"For POS tagging we used identical features to those[REF_CITE], the only di erence being that we did not make the rare word dis-tinction in table 1[REF_CITE](i.e., spelling features were included for all words in training data, and the word itself was used as a feature regardless of whether the word was rare)."
"The feature set takes into account the previous tag and previous pairs of tags in the history, as well as the word being tagged, spelling features of the words being tagged, and various features of the words surrounding the word being tagged. tencesIn the&quot; includedchunkingwordsexperimentsas well astheparts-of-speechinput \sen-for those words from the tagger[REF_CITE]."
"Ta-bleThe3chunkingshows theproblemfeaturesisusedrepresentedin the experimentsas a three-. tag task, where the tags are B, I, O for words beginning a chunk, continuing a chunk, and be-ing outside a chunk respectively."
"All chunks be-gin with a B symbol, regardless of whether the previous word is tagged O or I ."
We applied both maximum-entropy models and the perceptron algorithm to the two tagging problems.
"We tested several variants for each algorithm on the development set, to gain some understanding of how the algorithms&apos; perfor-mance varied with various parameter settings, and to allow optimization of free parameters so that the comparison on the nal test set is a fair one."
"For both methods, we tried the algorithms with feature count cut-o s set at 0 and 5 (i.e., we ran experiments with all features in training data included, or with all features occurring 5 times or more included {[REF_CITE]uses a count cut-o of 5)."
"In the perceptron algo-rithm, the number of iterations T over the train-ing set was varied, and the method was tested with both averaged and unaveraged parameter vectors (i.e., with Ts;n and sT;n , as de ned in section 2.5, for a variety of values for T )."
In the maximum entropy model the number of it-erations of training using Generalized Iterative ScalingFigurewas4 showsvaried.results on development data averagingon the twoimprovestasks.
"Theresultstrendssigniarecantlyfairlyforclearthe: perceptron method, as does including all fea-tures rather than imposing a count cut-o of 5."
"In contrast, the ME models&apos; performance su ers when all features are included."
"The best percep-tron con guration gives improvements over the maximum-entropy models in both cases: an im-provement in F-measure from 92 : 65% to 93 : 53% in chunking, and a reduction from 3 : 28% to 2 : 93% error rate in POS tagging."
"In looking at the results for di erent numbers of iterations on development data we found that averaging not only improves the best result, but also gives much greater stability of the tagger (the non-averaged variant has much greater variance in itsAsscoresa )nal. test, the perceptron and ME tag-gers were applied to the test sets, with the op-timal parameter settings on development data."
On POS tagging the perceptron algorithm gave 2.89% error compared to 3.28% error for the maximum-entropy model (a 11.9% relative re-duction in error).
"In NP chunking the percep-tron algorithm achieves an F-measure of 93.63%, in contrast to an F-measure of 93.29% for the ME model (a 5.1% relative reduction in error)."
This Proofs section of gives the proofs Theorems of theorems 1 and 2.
The proofs are adapted from proofs for the clas-si cation case in (Freund &amp;LetSchapire k be the99)weights. before 1 Proof = 0the. of Suppose k &apos;th Theorem mistakethe k 1 is&apos;th : mademistake.
It followsis madethatat the i &apos;th example.
"Take z to the output proposed at this example, z = argmax y 2 GEN ( x i ) ( x i ; y ) k ."
It follows from the algorithm updates that products of both sides with( x i the ; z ).vectorWe take U : inner k +1 = k + ( x i ; y i )
U k +1 = U k + U ( x i ; y i )
U ( x i ; z ) U k +
"Æ where the inequality follows because of the prop-and therefore U erty of U assumed 1 in=Eq0., 3it. followsBecauseby 1 induc-= 0, cause U tion on k that for jj U all jj k jj , U k +1 jj , k + it 1 follows kÆ . thatBe-k +1 jj"
"We k +1 also jj kÆ derive. an upper bound for jj k +1 jj 2 : jj k +1 jj 2 = jj k jj 2 + jj ( x i ;y i ) ( x i ;z ) jj 2 + k ( ( x i ;y i ) ( x i ;z )) jj k jj 2 + R 2 jj where( x ; y ) the ( x ; inequality z ) jj 2 R follows 2 by assump-because tion, i and i k ( i ( x i ; y i ) z is the highest scoring candidate( x i ;z )) for0 x becauseunder the parameters k ."
It follows by induction i that jj
Combining k +1 jj 2 kR the 2 . bounds jj k +1 jj jj k +1 jj 2 kR 2 gives the result for all kkÆ thatand k 2 Æ 2 jj k +1 jj 2 kR 2 ) k R 2 =Æ 2
"Proof of Theorem 2: We transform the rep-resentation( x; y ) 2 R d + ( x n ; y as) 2 follows R d to.a newFor i representation= 1 : : : d de- ne ( i ( x; ) y =) = if i (( x;y )).=For( j =),10 :: otherwise : n de ne, where d + j x; y is a parameter x; y which x j ; is y j greater than 0."
"Similary, say we are given a U ;Æ pair, and cor-responding values for i as de ned above."
We de ne a modi ed parameter vector U 2 R d + n with U i = U i for i = 1 : : : d and U d + j = j = for j = 1 : : : n .
Under these de nitions it can be veri ed that 8 i; 8 z 2 GEN ( x i ) ; U ( x i ; y i )
U ( x i ; z )
U jj 2 = jj U jj 2 + i 2 i = 2 = 1 + ( Dx U 2 8 i; 8 z 2 GEN ( x i P ) ; jj ( x i ; y i ) i ; z ) jj 2 R 2 + 2 ;Æ = 2
It can be seen that the vector q U = jj U jj separates the data with margin Æ= 1 + D 2 U ;Æ = 2 .
"By the-ceptron algorithm with representationorem 1, this means that the rst pass of themakesper-at most k max ( ) = Æ 1 2 ( R 2 + 2 )(1 +[REF_CITE];Æ ) mis-rithmtakes. withBut representationthe rst pass of isthe original algo- identical to the rst pass of the algorithm with representation tional, becausefeaturesthe parameterfor j =weights1 : : : n foreachthea addi-ect a single example of d training + j data, and do not a ect the classi cation of test data examples."
"Thus the original perceptron algorithm also makes at most k max ( ) mistakes on its rst pass over the with respect to , givingtraining data."
"Finally, we can minimize= p RDk U max , and( ) k max ( p RD U ;Æ ) = ( R 2 +"
"D 2 U ;Æ ) =Æ 2 , implying ;Æ the bound in the theorem."
"We have described new algorithms for tagging, whose performance guarantees depend on a no-tion of \separability&quot; of training data exam-ples."
"The generic algorithm in gure 2, and the theorems describing its convergence prop-erties, could be applied to several other models in the NLP literature."
"For example, a weighted ized as a way of de ning GEN ,context-free grammar can also beandconceptual-, so the weights for generative models such as PCFGs could be trained using this method."
"This paper separates conditional parameter estima-tion, which consistently raises test set accuracy on statistical NLP tasks, from conditional model struc-tures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy."
"Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the in-dependence assumptions of the conditional model structure are unsuited to linguistic sequences."
"The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work."
"The success and widespread adoption of probabilis-tic models in NLP has led to numerous variant meth-ods for any given task, and it can be difficult to tell what aspects of a system have led to its relative suc-cesses or failures."
"As an example, maximum en-tropy taggers have achieved very good performance[REF_CITE], but almost identical perfor-mance has also come from finely tuned HMM mod-els[REF_CITE]."
"Are any performance gains due to the sequence model used, the maximum entropy approach to parameter estima-tion, or the features employed by the system?"
Recent experiments have given conflicting recom-mendations.
"We suggest that the results in the literature, along with the new results we present in this work, can be explained by the following generalizations: • The ability to include better features in a well-founded fashion leads to better performance. • For fixed features, assumptions implicit in the model structure have a large impact on errors. • Maximizing the objective being evaluated has a re-liably positive, but often small, effect."
"It is especially important to study these issues us-ing NLP data sets: NLP tasks are marked by their complexity and sparsity, and, as we show, conclu-sions imported from the machine-learning literature do not always hold in these characteristic contexts."
"In previous work, the structure of a model and the method of parameter estimation were often both changed simultaneously (for reasons of naturalness or computational ease), but in this paper we seek to tease apart the separate effects of these two factors."
"In section 2, we take the Naive-Bayes model, ap-plied to word-sense disambiguation ( WSD ), and train it to maximize various objective functions."
Our ex-periments reaffirm that discriminative objectives like conditional likelihood improve test-set accuracy.
"In section 3, we examine two different model structures for part-of-speech ( POS ) tagging."
"There, we ana-lyze how assumptions latent in conditional structures lower tagging accuracy and produce strange quali-tative behaviors."
"Finally, we discuss related recent findings by other researchers."
"For bag-of-words WSD , we have a corpus D of la-beled examples (s, o)."
"Each o = ho i i is a list of con-text words, and the corresponding s is the correct sense of a fixed target word occuring in that context."
"A particular model for this task is the familiar multi- nomial Naive-Bayes ( NB ) model[REF_CITE], where we assume con-ditional independence between each of the o i ."
This NB model gives a joint distribution over the s and ho i i variables:
"P(s, o) = P(s) Y P(o i |s) i"
It also implicitly makes conditional predictions:
"P(s, o)/ X P(s 0 , o) s 0"
"In NLP , NB models are typically used in this latter way to make conditional decisions, such as chosing the most likely word sense. 1"
The parameters 2 = hθ s ; θ o|s i for this model are the sense priors P(s) and the sense-conditional word distributions P(o|s).
"These are typically set using (smoothed) relative frequency estimators ( RFE s): θ s = P(s) = count(s)/|D| θ o|s = P(o|s) = count(s, o)/ X count(s, o 0 ) o 0"
These intuitive relative frequency estimators are the estimates for 2 which maximize the joint likelihood ( JL ) of D according to the NB model:
"J L(2, D) = Y P(s, o) (s,o)∈D"
A NB model which has been trained to maximize JL will be referred to as NB - JL .
"It is worth emphasiz-ing that, in NLP applications, the model is typically trained jointly, then used for its P(s|o) predictions."
"We can set the parameters in other ways, without changing our model."
"If we are doing classification, we may not care about JL ."
"Rather, we will want to minimize whatever kinds of errors we get charged for."
"The JL objective is the evaluation criterion for language modeling, but a decision process’ evalua-tion is more naturally phrased in terms of P(s|o)."
"If we want to maximize the probability assigned to the correct labeling of the corpus, the appropriate objec-tive is conditional likelihood ( CL ):"
"CL(2, D) = Y P(s|o) (s,o)∈D"
"This focuses on the sense predictions, not the words, which is what we cared about in the first place."
Figure 1 shows an example of the trade-offs be-tween JL and CL .
"Assume there are two classes ([Footnote_1] and 2), two words (a and b), and only 2-word con-texts."
1 A possible use for the joint predictions would be a topic-conditional unigram language model.
"Assume the actual distribution (training and test) is 3 each of (1, ab) and (1, ba) and one (2, aa) for 7 samples."
"Then, as shown in figure 1, the JL -maximizing NB model has priors of 6/7 and 1/7, like the data."
"The actual (joint) distribution is not in the family of NB models, and so it cannot be learned per-fectly."
"Still, the NB - JL assigns reasonable probabili-ties to all occurring events."
"However, its priors cause it to incorrectly predict that aa belongs to class 1."
"On the other hand, maximizing CL will push the prior for sense 1 arbitrarily close to zero."
"As a result, its con-ditional predictions become more accurate at the cost of its joint prediction."
"NB - CL joint prediction assigns vanishing mass to events other than (2, aa), and so its joint likelihood score gets arbitrarily bad."
There are other objectives (or loss functions).
"In the S ENSEVAL competiti[REF_CITE], we guess sense distributions, and our score is the sum of the masses assigned to the correct senses."
This objective is the sum of conditional likelihoods ( SCL ):
"SCL(2, D) = X P(s|o) (s,o)∈D"
"SCL is less appropriate that CL when the model is used as a step in a probabilistic process, rather than in isolation."
"CL is more appropriate for filter pro-cesses, because it highly punishes assigning zero or near-zero probabilities to observed outcomes."
"If we choose single senses and receive a score of either 1 or 0 on an instance, then we have 0/1-loss[REF_CITE]."
This gives the “number correct” and so we refer to the corresponding objective as ac-curacy (Acc):
"Acc(2, D) ="
X δ(s = arg max s 0
"P(s 0 |o)) (s,o)∈D"
"In the following experiments, we illustrate that, for a fixed model structure, it is advantageous to max-imize objective functions which are similar to the evaluation criteria."
"Although in principle we can op-timize any of the objectives above, in practice some are harder to optimize than others."
"As stated above, JL is trivial to maximize with a NB model."
"SCL , since they are continuous in 2, can be opti-mized by gradient methods."
"Acc is not continuous in 2 and is unsuited to direct optimization (indeed, finding an optimum is NP-complete)."
"When optimizing an arbitrary function of 2, we have to make sure that our probabilities remain well-formed."
"If we want to have a well-formed joint NB in-terpretation, we must have non-negative parameters and the inequalities ∀s P θ o o|s ≤ 1 and P θ ≤ 1. s s"
"If we want to be guaranteed a non-deficient joint in-terpretation, we can require equality."
"However, if we relax the equality then we have a larger feasible space which may give better values of our objective."
We performed the following WSD experiments with Naive-Bayes models.
We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [URL_CITE] We set the NB model parameters in several ways.
"We optimized JL (using the RFE s). [Footnote_3] We also optimized SCL and (the log of) CL , using a conju-gate gradient ( CG ) method[REF_CITE]. [Footnote_4]"
"3 Smoothing is an important factor for this task. So that the various estimates would be smoothed as similarly as possible, we smoothed implicitly, by adding smoothing data. We added one instance of each class occurring with the bag containing each vocabulary word once. This gave the same result as add-one smoothing on the RFE s for NB - JL , and ensured that NB - CL would not assign zero conditional probability to any unseen event. The smoothing data did not, however, result in smoothed estimates for SCL ; any conditional probability will sum to one over the smoothing instances. For this objective, we added a penalty term proportional to P θ 2 , which ensured that no con-ditional sense probabilities reached 0 or 1."
"4 All optimization was done using conjugate gradient as-cent over log parameters λ i = logθ i , rather than the given parameters due to sensitivity near zero and improved quality of quadratic approximations during optimization. Linear con-straints over θ are not linear in log space, and were enforced using a quadratic Lagrange penalty method[REF_CITE]."
"For CL and SCL , we optimized each objective both over the space of all distributions and over the subspace of non-deficient models (giving CL ∗ and SCL ∗ )."
Acc was not directly optimized.
Unconstrained CL corresponds exactly to a condi-tional maximum entropy model[REF_CITE].
"This particular case, where there are multiple explanatory variables and a sin-gle categorical response variable, is also precisely the well-studied statistical model of (multinomial) logistic regressi[REF_CITE]."
Its optimization problem is concave (over log parameters) and there-fore has a unique global maximum.
"For CL ∗ , SCL , and SCL ∗ , we are only guaranteed local optima, but in practice we detected no maxima which were not global over the feasible region."
"Figure 2 shows, for each objective maximized, the values of all objectives on both the training and test set."
Optimizing for a given objective generally gave the best score for that objective for both the training set and the test set.
The exception is NB - SCL and NB - SCL * which have lower SCL score than NB - CL and NB - CL *.
This is due to the penalty used for smooth-ing the summed models (see fn. 3).
"Accuracy is higher when optimizing the discrim-inative objectives, CL and SCL , than when optimiz-ing JL (including for macro-averaging, where each word’s contribution to average accuracy is made equal)."
"That these estimates beat NB - JL on accu-racy is unsurprising, since Acc is a discretization of conditional predictions, not joint ones."
"This sup-ports the claim that maximizing conditional likeli-hood, or other discriminative objectives, improves test set accuracy for realistic NLP tasks."
"NB - SCL , though harder to maximize in general, gives better test-set accuracy than NB - CL . [Footnote_5] NB - CL * is some-where between JL and CL for all objectives on the training data."
"5 This difference seems to be partially due to the different smoothing methods used:[REF_CITE]show that quadratic penalties are very effective in practice, while the smoothing-data method is quite crude."
"Its behavior shows that the change from a standard NB approach ( NB - JL ) to a maximum entropy classifier ( NB - CL ) can be broken into two as-pects: a change in objective and an abandonment of a non-deficiency constraint. [Footnote_6] Note that the JL score for NB - CL *, is not very much lower than for NB - JL , despite a large change in CL ."
"6 If one is only interested in the model’s conditional predic-tions, there is no reason to disprefer deficient joint models."
It would be too strong to state that maximizing CL (in particular) and discriminative objectives (in gen-eral) is always better than maximizing JL for improv-ing test-set accuracy.
"Even on the present task, CL strictly beat JL in accuracy for only 15 of 24 words."
Figure 3 shows a plot of the relative accuracy for CL : (Acc CL − Acc JL )/Acc
"The x-axis is the average number of training instances per sense, weighted by the frequency of that sense in the test data."
There is a clear trend that larger training sets saw a larger benefit from using NB - CL .
The scatter in this trend is partially due to the wide range in data set condi-tions.
The data sets exhibit an unusual amount of drift between training and test distributions.
"For ex-ample, the test data for amaze consists entirely of 70 instances of the less frequent of its two training senses, and represents the highest point on this graph, with NB - CL having a relative accuracy increase of 28%."
This drift between the training and test cor-pora generally favors conditional estimates.
"On the other hand, many of these data sets are very small, individually, and 6 of the 7 sets where NB - JL wins are among the 8 smallest, 4 of them in fact being the 4 smallest."
"They argue that unless one has a relatively large data set, one is in fact likely to be better off with the generative estimate."
"Their claim seems too strong here; even smaller data sets often show benefit to accuracy from CL estimation, although all would qualify as small on their scale."
"Since the number of senses and skew towards common senses is so varied between S ENSEVAL -2 words, we turned to larger data sets to test the ef-fective “break-even” size for WSD data, using the hard and line data[REF_CITE]."
Fig-ure 4 shows the accuracy of NB - CL and NB - JL as the amount of training data increases.
"Conditional beats joint for all but the smallest training sizes, and the im-provement is greater with larger training sets."
Only for the line data does the conditional model ever drop below the joint model.
"For this task, then, NB - CL is performing better than expected."
This appears to be due to two ways in which CL estimation is suited to linguistic data.
"First, the Ng and Jordan results do not involve smoothed data."
"Their data sets do not require it like linguistic data does, and smoothing largely prevents the low-data overfitting that can plague conditional models."
"There is another, more interesting reason why con-ditional estimation for this model might work better for an NLP task like WSD than for a general machine learning task."
One signature difficulty in NLP is that the data contains a great many rare observations.
"In the case of WSD , the issue is in telling the kinds of rare events apart."
"Consider a word w which occurs only once, with a sense s."
"In the joint model, smooth-ing ensures that w does not signal s too strongly."
"However, every w which occurs only once with s will receive the same P(w|s)."
"Ideally, we would want to be able to tell the accidental singletons from true in-dicator words."
The conditional model implicitly does this to a certain extent.
"If w occurs with s in an ex-ample where other good indicator words are present, then those other words’ large weights will explain the occurrence of s, and without w having to have a large weight, its expected count with s in that instance will approach 1."
"On the other hand, if no trigger words occur in that instance, there will be no other expla-nation for s other than the presence of w and the other non-indicative words."
"Therefore, w’s weight, and the other words’, will grow until s is predicted sufficiently strongly."
"As a concrete illustration, we isolated two senses of “line” into a two-sense data set."
Sense 1 was “a queue” and sense 2 was “a phone line.”
"In this cor-pus, the words transatlantic and flowers both occur only once, and only with the “phone” sense (plus once with each in the smoothing data)."
"However, transatlantic occurs in the instance thanks, anyway, the transatlantic line 2 died. , while flowers occurs in the longer instance . . . phones with more than one line 2, plush robes, exotic flowers, and complimen-tary wine."
"In the first instance, the only non-singleton content word is died which occurs once with sense 1 and twice with sense 2."
"However, in the other case, phone occurs 191 times with sense 2 and only 4 times with sense 1."
"Additionally, there are more words in the second instance with which flowers can share the burden of increasing its expectation."
"P JL (flowers|2) P JL (transatlantic|2) = = 2 P JL (flowers|1) P JL (transatlantic|1) while with conditional estimation,"
P CL (flowers|2) = 2.05 P CL (flowers|1) P CL (transatlantic|2) = 3.74 P CL (transatlantic|1)
"With joint estimation, both words signal sense 2 with equal strength."
"With conditional estimation, the pre-sense of words like phone cause flowers to indicate sense 2 less strongly that transatlantic."
"Given that the conditional estimation is implicitly differentially weighting rare events in a plausibly way, it is perhaps unsurprising that a task like WSD would see the ben-efits on smaller corpus sizes than would be expected on standard machine-learning data sets. [Footnote_7]"
"7 Interestingly, the common approach of discarding low-count events (for both training speed and overfitting reasons) when estimating the conditional models used in maxent taggers robs the system of the opportunity to exploit this effect of con-ditional estimation."
"These trends are reliable, but sometimes small."
"In practice, one must decide if, for example, a 5% error reduction is worth the added work: CG optimization, especially with constraints, is considerably harder to implement than simple RFE estimates for JL ."
"It is also considerably slower: the total training time for the entire S ENSEVAL -2 corpus was less than 3 seconds for NB - JL , but two hours for NB - CL ."
"We now consider sequence data, with POS tagging as a concrete NLP example."
"In the previous section, we had a single model, but several ways of estimating parameters."
"In this section, we have two different model structures."
"First is the classic hidden Markov model ( HMM ), shown in figure 6a."
"For an instance (s,o), where o = ho i i is a word sequence and s = hs i i is a tag sequence, we write the following (joint) model:"
"P(s, o) = P(s)P(o|s) ="
P(s i |s i−1 )
P(o i |s i ) i where we use a start state s 0 to simplify notation.
The parameters of this model are the transition and emission probabilities.
"Again, we can set these pa-rameters to maximize JL , as is typical, or we can set them to maximize other objectives, without chang-ing the model structure."
"If we maximize CL , we get (possibly deficient)"
HMM s which are instances of the conditional random fields[REF_CITE]. [Footnote_8]
8 The general class of CRF s is more expressive and reduces to deficient HMM s only when they have just these features.
Figure 5 shows the tagging accuracy of an HMM trained to maximize each objective.
JL is the standard HMM .
CL duplicates the simple CRF s[REF_CITE].
"CL ∗ is again an intermediate, where we optimized conditional likelihood but required the HMM to be non-deficient."
"This separates out the ben-efit of the conditional objective from the benefit from the possibility of deficiency (which relates to label bias, see below)."
"In accordance with our observa-tions in the last section, and consistent with the re-sults[REF_CITE], the CL accuracy is slightly higher than JL for this fixed model."
"Another model often used for sequence data is the upward Conditional Markov Model ( CMM ), shown as a graphical model in figure 6b."
This is the model used in maximum entropy tagging.
"The graphical model shown gives a joint distribution over (s,o), just like an HMM ."
"It is a conditionally structured model, in the sense that that distribution can be writ-ten as P(s,o) = P(s|o)P(o)."
"Since tagging only uses P(s|o), we can discard what the model says about P(o)."
"The model as drawn assumes that each observation is independent, but we could add any ar-rows we please among the o i without changing the conditional predictions."
"Therefore, it is common to think about this model as if the joint interpretation were absent, and not to model the observations at all."
"For models which are conditional in the sense of the factorization above, the JL and CL estimates for P(s|o) will always be the same."
"It is therefore tempt-ing to believe that since one can find closed-form CL estimates (the RFE s) for these models, one can gain the benefit of conditional estimation."
"We will show that this is not true, at least not here."
"Adopting the CMM has effects in and of itself, re-gardless of whether a maximum entropy approach is used to populate the P(s|s −1 , o) estimates."
"The ML estimate for this model is the RFE for P(s|s −1 ,o)."
"For tagging, sparsity makes this impossible to reli-ably estimate directly, but even if we could do so, we would have a graphical model with several defects."
Every graphical model embodies conditional inde-pendence assumptions.
The NB model assumes that observations are independent given the class.
The HMM assumes the Markov property that future ob-servations are independent from past ones given the intermediate state.
"Both assumptions are obviously false in the data, but the models do well enough for the tasks we ask of them."
"However, the assumptions in this upward model are worse, both qualitatively and quantitatively."
"It is a conditional model, in that the model can be factored as P(o)P(s|o)."
"As a re-sult, it makes no useful statement about the distribu-tion of the data, making it useless, for example, for generation or language modeling."
But more subtly note that states are independent of future observa-tions.
"As a result, future cues are unable to influ-ence past decisions in certain cases."
"For example, imagine tagging an entire sentence where the first word is an unknown word."
"With this model struc-ture, if we ask about the possible tags for the first word, we will get back the marginal distribution over (sentence-initial) unknown words’ tags, regardless of the following words."
We constructed two taggers.
"One was an HMM , as in figure 6a."
"It was trained for JL , CL ∗ , and"
"The second was a CMM , as in figure 6b."
"We used a maximum entropy model over the (word, tag) and (previous-tag, tag) features to approximate the P(s|s −1 ,o) conditional probabilities."
This CMM is referred to as an MEMM .
A 9-1 split of the Penn tree-bank was used as the data corpus.
"To smooth these models as equally as possible and to give as unified a treatment of unseen words as possible, we mapped all words which occurred only once in training to an unknown token."
New words in the test data were also mapped to this token. [Footnote_9]
"9 Doing so lowered our accuracy by approximately 2% for all models, but gave better-controlled experiments."
"Using these taggers, we examined what kinds of errors actually occurred."
One kind of error tendency in CMM s which has been hypothesized in the liter-ature is called label bi[REF_CITE].
Label bias is a type of explaining-away phenomen[REF_CITE]which can be attributed to the local conditional modeling of each state.
The idea is that states whose following-state distributions have low entropy will be preferred.
Whatever mass arrives at a state must be pushed to successor states; it cannot be dumped on alternate observations as in an HMM .
"In theory, this means that the model can get into a dysfunctional behavior where a trajectory has no relation to the observations but will still stumble onward with high conditional probability."
The sense in which this is an explaining-away phenomenon is that the previous state explains the current state so well that the observation at the current state is effec-tively ignored.
What we found in the case of POS tag-ging was the opposite.
The state-state distributions are on average nowhere near as sharply distributed as the state-observation distributions.
This gives rise to the reverse explaining-away effect.
The observa-tions explain the states above them so well that the previous states are effectively ignored.
We call this observation bias.
"As an example, consider what happens when a word has only a single tag."
"The conditional distri-bution for the tag above that word will always as-sign conditional probability one to that single tag, re-gardless of the previous tag."
"Figure 7 shows the sen-tence All the indexes dove ., in which All should be tagged as a predeterminer ( PDT ). [Footnote_10] Most occurrences of All, however, are as a determiner ( DT , 106/135 vs 26/135), and it is much more common for a sentence to begin with a determiner than a predeterminer."
"10 The treebank predeterminer tag is meant for when words like All are followed by a determiner, as in this case."
The other words occur with only one tag in the tree-bank. 11
"The HMM tags this sentence correctly, be-cause two determiners in a row is rarer than All be-ing a predeterminer (and a predeterminer beginning a sentence)."
"However, the MEMM shows exactly the effect described above, choosing the most common tag ( DT ) for All, since the choice of tag for All does not effect the conditional tagging distribution for the."
"The MEMM parameters do assign a lower weight to the DT DT feature than to the PDT DT feature, but the the ensures a DT tag, regardless."
"Exploiting the joint interpretation of the CMM , what we can do is to unobserve word nodes, leaving the graphical model as it is, but changing the obser-vation status of a given node to “not observed”."
"For example, we can retain our knowledge that the state above the is DT , but “forget” that we know that the word at that position is the."
"If we do inference in this example with the unobserved, taking a weighted sum over all values of that node, then the conditional dis-tribution over tag sequences changes as shown under MEMM † : the correct tagging has once again become most probable."
Unobserving the word itself is not a priori a good idea.
It could easily put too much pres-sure on the last state to explain the fixed state.
This effect is even visible in this small example: the like-lihood of the more typical PDT - DT tag sequence is even higher for MEMM † than the HMM .
"These issues are quite important for NLP , since state-of-the-art statistical taggers are all based on one of these two models."
"In order to check which, if ei-ther, of label or observation bias is actually contribut-ing to tagging error, we performed the following ex-periments with our simple HMM and MEMM taggers."
"First, we measured, on the training data, the entropy of the next-state distribution P(s|s −1 ) for each state s. For both the HMM and MEMM , we then measured the relative overproposal rate for each state: the num-ber of errors where that state was incorrectly pre-dicted in the test set, divided by the overall frequency of that state in the correct answers."
"The label bias hy-pothesis makes a concrete prediction: lower entropy states should have higher relative overproposal val-ues, especially for the MEMM ."
"Figure 8 shows that the trends, if any, are not clear."
"There does appear to be a slight tendency to have higher error on the low-entropy tags for the HMM , but if there is any su-perficial trend for the MEMM , it is the reverse."
"On the other hand, if systematically unobserving unambiguous observations in the MEMM led to an in-crease in accuracy, then we would have evidence of observation bias."
Figure 5 shows that this is exactly the case.
"The error rate of the MEMM drops when we unobserve these single-tag words (from 10.8% to 9.5%), and the error rate in positions before such words drops even more sharply (17.1% to 15.0%)."
The drop in overall error in fact cuts the gap between the HMM and the MEMM by about half.
"The claim here is not that label bias is impossi-ble for MEMM s, nor that state-of-the-art maxent tag-gers would necessarily benefit from the unobserving of fixed-tag words – if there are already (tag, next-word) features in the model, this effect should be far weaker."
The claim is that the independence as-sumptions embodied by the conditionally structured model were the primary root of the lower accuracy for this model.
"Label bias and observation bias are both explaining-away phenomena, and are both con-sequences of these assumptions."
"Explaining-away effects will be found quite generally in conditionally-structured models, and should be carefully consid-ered before such models are adopted."
The effect can be good or bad:
"In the case of the NB - CL model, there was also an explaining-away effects among the words."
This is exactly the cause for flowers being a weaker indicator than transatlantic in our condi-tional estimation example.
"In that case, we wanted certain word occurrences to be explained away by the presence of more explanatory words."
"However, when some of the competing conditioned features are pre-vious local decisions, ignoring them can be harmful."
"First, he examines a PCFG over the ATIS treebank, trained both using RFE s to maximize JL , and using a CG method to maximize what we have been calling CL ∗ ."
"He does not give results for the unconstrained CL , but even in the constrained case, the effects from section 2 occur."
CL and parsing accuracy are both higher using the CL ∗ estimates.
"He also describes a conditional shift-reduce parsing model, but notes that it underperforms the simpler joint formulation."
"We take these two results not as contradictory, but as confirmation that conditional estimation, though of-ten slow, generally improves accuracy, while condi-tional model structures must be used with caution."
The conditional shift-reduce parsing model he de-scribes can be expected to exhibit the same type of competing-variable explaining-away issues that oc-cur in MEMM tagging.
"As an extreme example, if all words have been shifted, the rest of the parser actions will be reductions with probability one."
He shows a test-set accuracy benefit from optimizing accuracy directly.
"Finally, model structure and parameter estimation are not the entirety of factors which determine the be-havior of a model."
"Model features are crucial, and the ability to incorporate richer features in a relatively sensible way also leads to improved models."
This is the main basis of the real world benefit which has been derived from maxent models.
"We have argued that optimizing an objective that is as close to the task “accuracy” as possible is advanta-geous in NLP domains, even in data-poor cases where machine-learning results suggest discriminative ap-proaches may not be reliable."
We have also argued that the model structure is a far more important issue.
"For simple POS tagging, the observation bias effect of the model’s independence assumptions is more evi-dent than label bias as a source of error, but both are examples of explaining-away effects which can arise in conditionally structured models."
"Our results, com-bined with others in the literature, suggest that con-ditional model structure is, in and of itself, undesir-able, unless that structure enables methods of incor-porating better features, explaining why maximum- entropy taggers and parsers have had such success despite the inferior performance of their basic skele-tal models."
We demonstrate a problem with the stan-dard technique for learning probabilistic decision lists.
"We describe a simple, in-cremental algorithm that avoids this prob-lem, and show how to implement it effi-ciently."
"We also show a variation that adds thresholding to the standard sorting algo-rithm for decision lists, leading to similar improvements."
"Experimental results show that the new algorithm produces substan-tially lower error rates and entropy, while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm."
"Decision lists[REF_CITE]have been used for a variety of natural language tasks, including accent restorati[REF_CITE], word sense disam-biguati[REF_CITE], finding the past tense of English verbs[REF_CITE], and sev-eral other problems."
"We show a problem with the standard algorithm for learning probabilistic deci-sion lists, and we introduce an incremental algorithm that consistently works better."
"While the obvious im-plementation for this algorithm would be very slow, we also show how to efficiently implement it."
"The new algorithm produces smaller lists, while simul-taneously substantially reducing entropy (by about 40%), and error rates (by about 25% relative.)"
"Decision lists are a very simple, easy to understand formalism."
"Consider a word sense disambiguation task, such as distinguishing the financial sense of the word “bank” from the river sense."
"We might want the decision list to be probabilistic[REF_CITE]so that, for instance, the probabilities can be propagated to an understanding algorithm."
The de-cision list for this task might be:
"IF “water” occurs nearby, output “river: .95”, “fi-nancial: .05”"
"ELSE IF “money” occurs nearby, output “river: .1”, “financial: .9”"
"ELSE IF word before is “left”, output “river: .8”, “financial: .2”"
"ELSE IF “Charles” occcurs nearby, output “river: .6”, “financial: .4”"
"In this section, we describe the traditional algorithm for decision list learning in more detail, and then mo-tivate our new algorithm, and finally, describe our new algorithm and variations on it in detail."
"For sim-plicity only, we will state all algorithms for the binary output case; it should be clear how to extend all of the algorithms to the general case."
Decision list learners attempt to find models that work well on test data.
"The test data consists of a se-ries of inputs x 1 , ..., x n , and we are trying to predict the corresponding results y 1 , ..., y n ."
"For instance, in a word sense disambiguation task, a given x i could represent the set of words near the word, and y i could represent the correct sense of the word."
"Given a model D which predicts probabilities P D (y|x), the standard way of defining how well D works is the entropy of the model on the test data, defined as ni=1 −log 2 P D (y i |x i )."
Lower entropy is better.
There are many justifications for minimizing entropy.
"Among others, the “true” probability distribution has the lowest possible entropy."
"Also, minimizing train-ing entropy corresponds to maximizing the probabil-ity of the training data."
"Now, consider trying to learn a decision list."
"As-sume we are given a list of possible questions, q 1 ,...,q n ."
"In our word sense disambiguation ex-ample, the questions might include “Does the word ‘water’ occur nearby,” or more complex ones, such as “does the word ‘Charles’ occur nearby and is the word before ‘river.”’"
"Let us assume that we have some training data, and that the system has two out-puts (values for y), 0 and 1."
"Let C(q i ,0) be the number of times that, when q i was true in the train-ing data, the output was 0, and similarly for C(q i , 1)."
Let C(q i ) be the total number of times that q i was true.
"Now, given a test instance, x, y for which q i (x) is true, what probability would we assign to y = 1?"
"The simplest answer is to just use the probability in the training data, C(q i , 1)/C(q i )."
"Unfortunately, this tends to overfit the training data."
"For instance, if q i was true only once in the training data, then, depend-ing on the value for y that time, we would assign a probability of 1 or 0."
"The former is clearly an over-estimate, and the latter is clearly an underestimate."
"Therefore, we smooth our estimates[REF_CITE]."
"In particular, we used the interpolated absolute discounting method."
"Since both the tradi-tional algorithm and the new algorithm use the same smoothing method, the exact smoothing technique will not typically affect the relative performance of the algorithms."
"Let C(0) be the total number of ys that were zero in the training, and let C(1) be the to-tal number of ys that were one."
"Then, the “unigram” probability y is P (y) = C(0C)+(yC)(1) ."
Let N(q i ) be the number of non-zero ys for a given question.
"In par-ticular, in the two class case, N(q i ) will be 0 if there were no occurences of the question q i , 1 if training samples for q i always had the same value, and 2 if both 1 and 0 values occurred."
"Now, we pick some value d (using heldout data) and discount all counts by d."
"Then, our probability distribution is"
"P (y|  q i ) =  (C(Cq i (,yq i ))−d) + dNC((qq ii ))"
"P(y) if C(q i , y) &gt; 0  dNC((qq i ) i )"
"P(y) otherwise Now, the predicted entropy for a question q i is just entropy(q i ) = −P (0|q i )log 2 P (0|q i )−P (1|q i )log 2 P (1|q i )"
The typical training algorithm for decision lists is very simple.
"Given the training data, compute the predicted entropy for each question."
"Then, sort the questions by their predicted entropy, and output a decision list with the questions in order."
"One of the questions should be the special question that is al-ways TRUE, which returns the unigram probability."
"Any question with worse entropy than TRUE will show up later in the list than TRUE, and we will never get to it, so it can be pruned away."
Consider two weathermen in Seattle in the winter.
Assume the following (overly optimistic) model of Seattle weather.
"If today there is no wind, then to-morrow it rains."
"On one in 50 days, it is windy, and, the day after that, the clouds might have been swept away, leading to only a 50% chance of rain."
"So, overall, we get rain on 99 out of 100 days."
"The lazy weatherman simply predicts that 99 out of 100 days, it will rain, while the smart weatherman gives the true probabilities (i.e. 100% chance of rain tomorrow if no wind today, 50% chance of rain tomorrow if wind today.)"
Consider the entropy of the two weathermen.
The lazy weatherman always says “There is a 99% chance of rain tomorrow; my average entropy is −.99×log 2 .99 − .01 × log 2 .01 = .081 bits.”
"The smart weatherman, if there is no wind, says “100% chance of rain tomorrow; my entropy is 0 bits.”"
"If there is wind, however, the smart weatherman says, “50% chance of rain tomorrow; my entropy is 1 bit.”"
"Now, if today is windy, who should we trust?"
"The smart weatherman, whose expected entropy is 1 bit, or the lazy weatherman, whose expected entropy is .08 bits, which is obviously much better."
The decision list equivalent of this is as follows.
"Using the classic learner, we learn as follows."
We have three questions: if TRUE then predict rain with probability .99 (expected entropy = .081).
If NO WIND then predict rain with probability 1 (expected entropy = 0).
If WIND then predict rain with proba-bility [Footnote_1]/2 (expected entropy = 1).
"1 This means we are building the tree bottom up; it would be interesting to explore building the tree top-down, similar to a decision tree, which would probably also work well."
"When we sort these by expected entropy, we get:"
"IF NO WIND, output “rain: 100%” (entropy 0)"
"ELSE IF TRUE, output “rain: 99%” (entropy .081)"
"ELSE IF WIND, output “rain: 50%” (entropy 1)"
"Of course, we never reach the third rule, and on windy days, we predict rain with probabiliy .99!"
The two weathermen show what goes wrong with a naive algorithm; we can easily do much better.
"For the new algorithm, we start with a baseline ques-tion, the question which is always TRUE and pre- dicts the unigram probabilities."
"Then, we find the question which if asked before all other questions would decrease entropy the most."
"This is repeated until some minimum improvement, , is reached. 1 Figure 1 shows the new algorithm; the notation entropy(list) denotes the training entropy of a poten-tial decision list, and entropy(prepend(q i , list)) indi-cates the training entropy of list with the question “If q i then output p(y|q i )” prepended."
Consider the Parable of the Two Weathermen.
The new learning algorithm starts with the baseline: If TRUE then predict rain with probability 99% (en-tropy .081).
Then it prepends the rule that reduces the entropy the most.
"The entropy reduction from the question “NO WIND” is .081 × .99 = .08, while the entropy for the question “WIND” is 1 bit for the new question, versus .5 × 1 + .5 × −log 2 .01 = .5 + .5 × 6.64 = 3.82, for the old, for a reduction of 2.82 bits, so we prepend the “WIND” question."
"Finally, we learn (at the top of the list), that if “NO WIND”, then rain 100%, yielding the following de-cision list:"
"IF NO WIND, output “rain: 100%” (entropy 0)"
"ELSE IF WIND, output “rain: 50%” (entropy 1)"
"ELSE IF TRUE, output “rain: 99%” (entropy .081)"
"Of course, we never reach the third rule."
"Clearly, this decision list is better."
Why did our entropy sorter fail us?
"Because sometimes a smart learner knows when it doesn’t know, while a dumb rule, like our lazy weatherman who ignores the wind, doesn’t know enough to know that in the current sit- list = {TRUE} for each training instance x j , y j instanceEnt(j) = −log 2 p(y j ) for each question q i //"
"Now we compute entropyReduce(i) = // entropy(TRUE) − entropy(q i , TRUE) entropyReduce(i) = 0 for each x j , y j such that q i (x j ) for each question q i compute entropy(i) list = questions sorted by entropy(i) remove questions worse than TRUE for each training instance x j , y j instanceEnt(j) = −log 2 p(y j ) for each question q i in list in reverse order entropyReduce = 0 for each x j , y j such that q i (x j ) entropyReduce(i) += log 2 p(y j ) − log 2 p(y j | i q ) entropyReduce += do l = arg max i entropyReduce(i) if entropyReduce(l) &lt; then return list else list = prepend(q l , list) for each x j , y j such that q l (x j ) for each k such that q k (x j ) entropyReduce(k) += instanceEnt(j) instanceEnt(j) = −log 2 p(y j |q l ) for each k such that q k (x j ) uation, the problem is harder than usual."
"Unfortunately, the algorithm of Figure 1, if imple-mented in a straight-forward way, will be extremely inefficient."
"The problem is the inner loop, which requires computing entropy(prepend(q i ,list))."
The naive way of doing this is to run all of the training data through each possible decision list.
"In practice, the actual questions tend to be pairs or triples of sim-ple questions."
"For instance, an actual question might be “Is word before ‘left’ and word after ‘of’?”"
"Thus, the total number of questions can be very large, and running all the data through the possible new decision lists for each question would be extremely slow."
"Fortunately, we can precompute entropyReduce(i) and incrementally update it."
"In order to do so, we also need to compute, for each training instance x j , y j the entropy with the current value of list."
"Furthermore, we store for each question q i the list of instances x j , y j such that q i (x j ) is true."
"With these changes, the algorithm runs very quickly."
Figure 2 gives the efficient version of the new algorithm.
"Note that this efficient version of the algorithm may consume a large amount of space, because of the need to store, for each question q i , the list of training instances for which the question is true."
There are a number of speed-space tradeoffs one can make.
"For instance, one could change the update loop from for each x j , y j such that q i (x j ) to for each x j , y j if q i (x j ) then ..."
There are other possible tradeoffs.
"For instance, typ-ically, each question q i is actually written as a con-junction of simple questions, which we will denote Q i j ."
"Assume that we store the list of instances that are true for each simple question Q i j , and that q i is of the form Q i 1 &amp;Q i 2 &amp;...&amp;Q i I ."
"Then we can write an update loop in which we first find the simple question with the smallest number of true instances, and loop over only these instances when finding the instances for which q i is true: k = arg min j number instances such that Q i j for each x j , y j such that Q i k (x j ) if q i (x j ) then ..."
Notice the original algorithm can actually allow rules which make things worse.
"For instance, in our lazy weatherman example, we built this decision list:"
"IF NO WIND, output “rain: 100%” (entropy 0) ELSE IF TRUE, output “rain: 99%” (entropy .081) ELSE IF WIND, output “rain: 50%” (entropy 1) Now, the second rule could simply be deleted, and the decision list would actually be much better (although in practice we never want to delete the “TRUE” ques-tion to ensure that we always output some probabil-ity.)"
"Since the main reason to use decision lists is be-cause of their understandability and small size, this optimization will be worth doing even if the full im-plementation of the new algorithm is too complex."
The compromise algorithm is displayed in Figure 3.
"When the value of is 0, only those rules that improve entropy on the training data are included."
"When the value of is −∞, all rules are included (the stan-dard algorithm)."
"Even when a benefit is predicted, this may be due to overfitting; we can get further improvements by setting the threshold to a higher value, such as 3, which means that only rules that save at least three bits – and thus are unlikely to lead to overfitting – are added."
"There has been a modest amount of previous work on improving probabilistic decision lists, as well as a fair amount of work in related fields, especially in transformation-based learning[REF_CITE]."
"First, we note that non-probabilistic decision lists and transformation-based learning (TBL) are actu-ally very similar formalisms."
"In particular, as ob-served[REF_CITE], in the two-class case, they are identical."
"Non-probabilistic decision lists learn rules of the form “If q i then output y” while TBLs output rules of the form “If q i and current-class is y , change class to y”."
"Now, in the two class case, a rule of the form “If q i and current-class is y , change class to y” is identical to one of the form “If q i change class to y”, since either way, all instances for which q i is TRUE end up with value y."
The other difference between decision lists and TBLs is the list ordering.
"With a two-class TBL, one goes through the rules from last-to-first, and finds the last one that applies."
"With a decision list, one goes through the list in or-der, and finds the first one that applies."
"Thus in the two-class case, simply by changing rules of the form “If q i and current-class is y , change class to y” to “If q i output y”, and reversing the rule order, we can change any TBL to an equivalent non-probabilistic decision list, and vice-versa."
"Notice that our incre-mental algorithm is analogous to the algorithm used by TBLs: in TBLs, at each step, a rule is added that minimizes the training data error rate."
"In our prob-abilistic decision list learner, at each step, a rule is added that minimizes the training data entropy."
Roth notes that this equivalence does not hold in an important case: when the answers to questions are not static.
"For instance, in part-of-speech tagging[REF_CITE], when the tag of one word is changed, it changes the answers to questions for nearby words."
We call such problems “dynamic.”
The near equivalence of TBLs and decision lists is important for two reasons.
"First, it shows the connec-tion between our work and previous work."
"In partic-ular, our new algorithm can be thought of as a prob-abilistic version of the[REF_CITE]algorithm, for speeding up TBLs."
"Just as that al-gorithm stores the expected error rate improvement of each question, our algorithm stores the expected entropy improvement. (Actually, the Ramshaw and Marcus algorithm is somewhat more complex, be-cause it is able to deal with dynamic problems such as part-of-speech tagging.)"
"Similarly, the space-efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL[REF_CITE]."
The second reason that the connection to TBLs is important is that it shows us that probabilistic de-cision lists are a natural way to probabilize TBLs.
"It involved conversion to a deci-sion tree, and then further growing of the tree."
Their technique does have the advantage that it correctly handles the multi-class case.
"That is, by using a decision tree, it is relatively easy to incorporate the current state, while the decision list learner ignores that state."
"However, this is not clearly an advantage – adding extra dependencies introduces data sparse-ness, and it is an empirical question whether depen-dencies on the current state are actually helpful."
"Our probabilistic decision lists can thus be thought of as a competitive way to probabilize TBLs, with the ad-vantage of preserving the list-structure and simplicity of TBL, and the possible disadvantage of losing the dependency on the current state."
"First, he suggests an op-tional, more complex smoothing algorithm than the one we applied."
"His technique involves estimating both a probability based on the global probability distribution for a question, and a local probability, given that no questions higher in the list were TRUE, and then interpolating between the two probabilities."
"He also suggests a pruning technique that eliminates 90% of the questions while losing 3% accuracy; as we will show in Section 4, our technique or varia-tions eliminate an even larger percentage of ques-tions while increasing accuracy."
The combination of this hybrid decision list and the improved smoothing was the best performer for par-ticipating systems in the 1998 senseval evaluation.
"Our technique could easily be combined with these techniques, presumably leading to even better results."
"However, since we build our decision lists from last to first, rather than first to last, the local probability is not available as the list is being built."
But there is no reason we could not interpolate the local probability into a final list.
"Similarly, in Yarowsky’s technique, the local probability is also not available at the time the questions are sorted."
Our algorithm can be thought of as a natural prob-abilistic version of a non-probabilistic decision list learner which prepends rules[REF_CITE].
One difficulty that that approach has is ranking rules.
"In the probabilistic framework, using entropy reduction and smoothing seems like a natural solution."
"In this section, we give experimental results, showing that our new algorithm substantially outperforms the standard algorithm."
"We also show that while accu-racy is competitive with TBLs, two linear classifiers are more accurate than the decision list algorithms."
"Many of the problems that probabilistic decision list algorithms have been used for are very similar: in a given text context, determine which of two choices is most appropriate."
"Accent restorati[REF_CITE], word sense disambiguati[REF_CITE], and other problems all fall into this framework, and typically use similar feature types."
"We thus chose one problem of this type, grammar checking, and believe that our results should carry over at least to these other, closely related problems."
"In partic-ular, we chose to use exactly the same training, test, problems, and feature sets used by Banko and Brill (2001a; 2001b)."
"These problems consisted of try-ing to guess which of two confusable words, e.g. “their” or “there”, a user intended."
"Banko and Brill chose this data to be representative of typical machine learning problems, and, by trying it across data sizes and different pairs of words, it exhibits a good deal of different behaviors."
"Banko and Brill used a standard set of features, including words within a window of 2, part-of-speech tags within a window of 2, pairs of word or tag features, and whether or not a given word occurred within a window of 9."
"Altogether, they had 55 feature types."
They used all features of each type that occurred at least twice in the training data.
We ran our comparisons using 7 different algo-rithms.
The first three were variations on the stan-dard probabilistic decision list learner.
"In particular, first we ran the standard sorted decision list learner, equivalent to the algorithm of Figure 3, with a thresh-old of negative infinity."
"That is, we included all rules that had a predicted entropy at least as good as the unigram distribution, whether or not they would ac-tually improve entropy on the training data."
We call this “Sorted: −∞.”
"Next, we ran the same learner with a threshold of 0 (“Sorted: 0”): that is, we in-cluded all rules that had a predicted entropy at least as good as the unigram distribution, and that would at least improve entropy on the training data."
"Then we ran the algorithm with a threshold of 3 (“Sorted: 3”), in an attempt to avoid overfitting."
"Next, we ran our incremental algorithm, again with a threshold of reducing training entropy by at least 3 bits."
"In addition to comparing the various decision list algorithms, we also tried several other algorithms."
"First, since probabilistic decision lists are probabilis-tic analogs of TBLs, we compared to TBL[REF_CITE]."
"Furthermore, after doing our research on de-cision lists, we had several successes using simple linear models, such as a perceptron model and a max-imum entropy (maxent) model[REF_CITE]."
"For the perceptron algorithm, we used a varia-tion that includes a margin requirement, τ (Zaragoza w j = 0 for 100 iterations or until no change and[REF_CITE])."
Figure 4 shows this incredibly simple algorithm.
"We use q(x j ) to represent the vec-tor of answers to questions about input x j ; w j is a weight vector; we assume that the output, y j is -1 or +1; and τ is a margin."
"We assume that one of the questions is TRUE, eliminating the need for a sepa-rate threshold variable."
"When τ = 0, the algorithm reduces to the standard perceptron algorithm."
The inclusion of a non-zero margin and running to con-vergence guarantees convergence for separable data to a solution that works nearly as well as a linear support vector machine[REF_CITE].
"Given the extreme simplicity of the algorithm and the fact that it works so well (not just compared to the algorithms in this paper, but compared to several oth-ers we have tried), the perceptron with margin is our favorite algorithm when we don’t need probabilities, and model size is not an issue."
Most of our algorithms have one or more parame-ters that need to be tuned.
"We chose 5 additional con-fusable word pairs for parameter tuning and chose parameter values that worked well on entropy and error rate across data sizes, as measured on these 5 additional word pairs."
For the smoothing discount value we used 0.7.
"For thresholds for both the sorted and the incremental learner, we used 3 bits."
"For the perceptron algorithm, we set τ to 20."
"For TBL’s min-imum number of errors to fix, the traditional value of 2 worked well."
"For the maxent model, for smooth-ing, we used a Gaussian prior with 0 mean and 0.3 variance."
"Since sometimes one learning algorithm is better at one size, and worse at another, we tried three training sizes: 1, 10 and 50 million words."
"In Figure 5, we show the error rates of each algo-rithm at different training sizes, averaged across the 10 words in the test set."
"We computed the geomet-ric mean of error rate, across the ten word pairs."
"We chose the geometric mean, because otherwise, words with the largest error rates would disproportionately dominate the results."
"Figure 6, shows the geometric mean of the model sizes, where the model size is the number of rules."
"For maxent and perceptron mod-els, we counted size as the total number of features, since these models store a value for every feature."
"For Sorted: −∞ and Sorted: 0, the size is similar to a maxent or perceptron model – almost every rule is used."
Sorted: 3 drastically reduces the model size – by a factor of roughly 20 – while improving perfor-mance.
"Incremental: 3 is smaller still, by about an additional factor of 2 to 5, although its accuracy is slightly worse than Sorted: 3."
Figure 7 shows the en-tropy of each algorithm.
"Since entropy is logarthmic, we use the arithmetic mean."
"Notice that the traditional probabilistic decision list learning algorithm – equivalent to Sorted: −∞ – always has a higher error rate, higher entropy, and larger size than Sorted: 0."
"Similarly, Sorted: 3 has lower entropy, higher accuracy, and smaller models than Sorted: 0."
"Finally, Incremental: 3 has slightly higher error rates, but slightly lower entropies, and 1/2 to 1/5 as many rules."
"If one wants a probabilistic decision list learner, this is clearly the algorithm to use."
"However, if probabilities are not needed, then TBL can produce lower error rates, with still fewer rules."
"On the other hand, if one wants either the low-est entropies or highest accuracies, then it appears that linear models, such as maxent or the perceptron algorithm with margin work even better, at the ex-pense of producing much larger models."
"Clearly, the new algorithm works very well when small size and probabilities are needed."
It would be interesting to try combining this algorithm with decision trees in some way.
"Notice however that the chief advantage of decision lists over linear models is their compact size and un-derstandability, and our techniques simultaneously improve those aspects; adding additional splits will almost certainly lead to larger models, not smaller."
"It would also be interesting to try more sophisticated smoothing techniques, such as those of Yarowsky."
"We have shown that a simple, incremental algo-rithm for learning probabilistic decision lists can pro-duce models that are significantly more accurate, have significantly lower entropy, and are significantly smaller than those produced by the standard sorted learning algorithm."
"The new algorithm comes at the cost of some increased time, space, and complexity, but variations on it, such as the sorted algorithm with thresholding, or the techniques of Section 2.2.1, can be used to trade off space, time, and list size."
"Over-all, given the substantial improvements from this al-gorithm, it should be widely used whenever the ad-vantages – compactness and understandability – of probabilistic decision lists are needed."
This paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task.
"It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Naïve Bayes, cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mix-ture models)."
The paper also includes in-depth per-formance analysis sensitive to properties of the fea-ture space and component classifiers.
"When eval-uated on the standard SENSEVAL 1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance ex-ceeds the best published results on these data sets."
"Classifier combination has been extensively stud-ied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging ([REF_CITE]; van[REF_CITE]), base noun phrase chunking[REF_CITE], parsing[REF_CITE]and word sense disambigua-ti[REF_CITE]."
There are several reasons why classifier combination is useful.
"First, by consulting the output of multiple classifiers, the system will im-prove its robustness."
"Second, it is possible that the problem can be decomposed into orthogonal feature spaces (e.g. linguistic constraints and word occur-rence statistics) and it is often better to train dif-ferent classifiers in each of the feature spaces and then combine their output, instead of designing a complex system that handles the multimodal infor-mation."
"Third, it has been shown[REF_CITE]that it is possible to reduce the clas-sification error by a factor of ( is the number of classifiers) by combination, if the classifiers’ errors are uncorrelated and unbiased."
"The target task studied here is word sense disam-biguation in the SENSEVAL evaluation framework[REF_CITE]with comparative tests in English, Spanish, Swedish and Basque lexical-sample sense tagging over a combined sample of 37730 instances of 234 polysemous words."
"This paper offers a detailed comparative evalu-ation and description of the problem of classifier combination over a structurally and procedurally diverse set of six both well established and orig-inal classifiers: extended Naïve Bayes, BayesRa-tio, Cosine, non-hierarchical Decision Lists, Trans-formation"
"Based Learning (TBL), and the MMVC classifiers, briefly described in Section 4."
"These systems have different space-searching strategies, ranging from discriminant functions (BayesRatio) to data likelihood (Bayes, Cosine) to decision rules (TBL, Decision Lists), and therefore are amenable to combination."
Related work in classifier combination is discussed throughout this article.
"For the specific task of word sense disambiguation, the first empirical study was presented[REF_CITE], where the authors combined the output of the par-ticipating SENSEVAL 1 systems via simple (non-weighted) voting, using either Absolute Majority, Relative Majority, or Unanimous voting."
"The feature space is a critical factor in classifier de-sign, given the need to fuel the diverse strengths of the component classifiers."
Thus its quality is of-ten highly correlated with performance.
"For this reason, we used a rich feature space based on raw words, lemmas and part-of-speech (POS) tags in a variety of positional and syntactical relationships to the target word."
"These positions include traditional unordered bag-of-word context, local bigram and trigram collocations and several syntactic relation-ships based on predicate-argument structure."
Their use is illustrated on a sample English sentence for the target word church in Figure 1.
"While an exten-sive evaluation of feature type to WSD performance is beyond the scope of this paper, Section 6 sketches an analysis of the individual feature contribution to each of the classifier types."
Part-of-speech tagger availability varied across the languages that are studied here.
"An electronically available transformation-based POS tagger[REF_CITE]was trained on standard labeled data for English (Penn Treebank), Swedish (SUC-1 corpus), and Basque."
"For Spanish, an minimally supervised tagger[REF_CITE]was used."
"Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods[REF_CITE]for all the other languages."
"The syntactic features extracted for a target word depend on the word’s part of speech: verbs: the head noun of the verb’s object, par-ticle/preposition and prepositional object; nouns: the headword of any verb-object, subject-verb or noun-noun relationships iden-tified for the target word; adjectives: the head noun modified by the ad-jective."
The extraction process was performed using heuris-tic patterns and regular expressions over the parts-of-speech surrounding the target word [Footnote_1] .
"1 The feature extraction on the in English data was per-formed by first identifying text chunks, and then using heuris-tics on the chunks to extract the syntactic information."
This section briefly introduces the 6 classifier mod-els used in this study.
"Among these models, the Naïve Bayes variants (NB henceforth)[REF_CITE]and Cosine dif-fer slightly from off-the-shelf versions, and only the differences will be described."
"Many of the systems used in this research share a common vector representation, which captures traditional bag-of-words, extended ngram and predicate-argument features in a single data struc-ture."
"In these models, a vector is created for each   document in the collection: , where is the number of times the feature appears in document , is the number of words in and is a weight associated with the feature [Footnote_2] ."
"2 The weight depends on the type of the feature : for the bag-of-word features, this weight is inversely proportional to the distance between the target word and the feature, while for predicate-argument and extended ngram features it is a em-pirically estimated weight (on a per language basis)."
"Confusion between the same word participat-ing in multiple feature roles is avoided by append-ing the feature values with their positional type (e.g. stands_Sbj, ancient_L are distinct from stands and ancient in unmarked bag-of-words context)."
"The notable difference between the extended models and others described in the literature, aside from the use of more sophisticated features than the traditional bag-of-words, is the variable weight-ing of feature types noted above."
"These differences yield a boost in the NB performance (relative to ba-sic Naïve Bayes) of between 3.5% (Basque) and 10% (Spanish), with an average improvement of 7.25% over the four languages."
"The BayesRatio model (BR henceforth) is a vector-based model using the likelihood ratio framework described[REF_CITE]:      ¾ where is the selected sense, denotes documents and denotes features."
"By utilizing the binary ra-tio for k-way modeling of feature probabilities, this approach performs well on tasks where the data is sparse."
The Mixture Maximum Variance Correction classi-fier (MMVC henceforth)[REF_CITE]is a two step classifier.
"First, the sense proba-bility is computed as a linear mixture  ¾ ¾ where the probability is estimated from data and is computed as a weighted normal-ized similarity between the word and the target word (also taking into account the distance in the document between and )."
"In a second pass, the sense whose variance exceeds a theoretically moti-vated threshold is selected as the final sense label (for details, see[REF_CITE])."
Two discriminative models are used in the exper-iments presented in Section 5 - a transformation-based learning system (TBL henceforth)[REF_CITE]and a non-hierarchical decision lists system (DL henceforth)[REF_CITE].
"For prediction, these systems utilize local n-grams around the target word (up to 3 words/lemma/POS to the left/right), bag-of-words and lemma/collocation ( 20 words around the tar-get word, grouped by different window sizes) and the syntactic features listed in Section 3.2."
"The TBL system was modified to include redun-dant rules that do not improve absolute accuracy on training data in the traditional greedy training al-gorithm, but are nonetheless positively correlated with a particular sense."
"The benefit of this approach is that predictive but redundant features in training context may appear by themselves in new test con-texts, improving coverage and increasing TBL base model performance by 1-2%."
One necessary property for success in combining classifiers is that the errors produced by the com-ponent classifiers should not be positively corre-lated.
"On one extreme, if the classifier outputs are strongly correlated, they will have a very high inter-agreement rate and there is little to be gained from the joint output."
"On the other extreme,[REF_CITE]show that, if the errors made by the classifiers are uncorrelated and unbiased, then by considering a classifier that selects the class that maximizes the posterior class probability average   (1)    the error is reduced by a factor of ."
"This case is mostly of theoretical interest, since in practice all the classifiers will tend to make errors on the “harder” samples."
"Figure 3(a) shows the classifier inter-agreement among the six classifiers presented in Section 4, on the English data."
"Only two of them, BayesRatio and cosine, have an agreement rate of over 80% [Footnote_3] , while the agreement rate can be as low as 63% (BayesRa-tio and TBL)."
3 The performance is measured using 5-fold cross validation on training data.
The average agreement is 71.7%.
The fact that the classifiers’ output are not strongly cor-related suggests that the differences in performance among them can be systematically exploited to im-prove the overall classification.
"All individual clas-sifiers have high stand-alone performance; each is individually competitive with the best single SEN - SEVAL 2 systems and are fortuitously diverse in rel-ative performance, as shown in Table 3(b)."
"A den-dogram of the similarity between the classifiers is shown in Figure 2, derived using maximum linkage hierarchical agglomerative clustering."
There are three major types of classifier combina-ti[REF_CITE].
The most general type is the case where the classifiers output a posterior class probability distribution for each sample (which can be interpolated).
"In the second case, systems only output a set of labels, together with a ordering of preference (likelihood)."
"In the third and most re-strictive case, the classifications consist of just a sin-gle label, without rank or probability."
Combining classifiers in each one of these cases has different properties; the remainder of this section examines models appropriate to each situation.
One of the simplest ways to combine the poste-rior probability distributions is via direct averaging (Equation (1)).
"Surprisingly, this method obtains reasonably good results, despite its simplicity and the fact that is not theoretically motivated under a Bayes framework."
Its success is highly dependent on the condition that the classifiers’ errors are un-correlated[REF_CITE].
The averaging method is a particular case of weighted mixture: [Footnote_4]   (2)   where is the weight assigned  istothetheposte-clas- sifier in the mixture and rior probability  distributionwe obtain outputEquationby(1classifier). ; for
"4 Note that we are computing a probability conditioned both on the target word and the document , because the docu-ments are associated with a particular target word ; this for-malization works mainly for the lexical choice task."
The mixture interpolation coefficients can be computed at different levels of granularity.
"For instance  , one can makeand thenthetheassumptioncoefficients   that will be computed at word level; if then the coefficients will be estimated on the entire data."
One way to estimate these parameters is by linear   regressi[REF_CITE]: estimate the coefficients that minimize the mean square error (MSE)   (3)  is the target vector of the cor- where rect classification of word in document d:
"Æ   ,  being the goldstan-dard sense of in and Æ the Kronecker function:"
Æ if if
"As shown[REF_CITE],[REF_CITE], the solution to the optimization problem (3) can be obtained by solving a linear set of equations."
The resulting classifier will have a lower square er-ror than the average classifier (since the average classifier is a particular case of weighted mixture).
Another common method to compute the pa-rameters is by using the Expectation-Maximization (EM) algorithm[REF_CITE].
"One can estimate the coefficients such as to max-imize the log-likelihood of the data,  ."
"In this particular opti- mization problem, the search space is convex, and therefore a solution exists and is unique, and it can be obtained by the usual EM algorithm (see[REF_CITE]for a detailed description)."
An alternative method for estimating the parame-ters is to approximate them with the performance of the th classifier (a performance-based combiner) (van[REF_CITE]) _is_correct (4) therefore giving more weight to classifiers that have a smaller classification error (the method will be re-ferred to as PB).
"The probabilities in Equation (4) are estimated directly from data, using the maxi-mum likelihood principle."
"In cases where there are reasons to believe that the posterior probability distribution output by a clas-sifier is poorly estimated [Footnote_5] , but that the relative or-dering of senses matches the truth, a combination strategy based on the relative ranking of sense pos-terior probabilities is more appropriate."
"5 For instance, in sparse classification spaces, the Naïve Bayes classifier will assign a probability very close to 1 to the most likely sense, and close to 0 for the other ones."
The sense posterior probability can be computed as   (5) ¼ where the rank of a sense is inversely proportional to the number of senses that are (strictly) more prob-  ¼ able than sense :  ¼
"This method will tend to prefer senses that appear closer to the top of the likelihood list for most of the classifiers, therefore being more robust both in cases where one classifier makes a large error and in cases where some classifiers consistently overestimate the posterior sense probability of the most likely sense."
Some classification methods frequently used in NLP directly minimize the classification error and do not usually provide a probability distribution over classes/senses (e.g. TBL and decision lists).
"There are also situations where the user does not have access to the probability distribution, such as when the available classifier is a black-box that only outputs the best classification."
A very common technique for combination in such a case is by vot-ing ([REF_CITE]; van[REF_CITE]).
"In the simplest model, each clas-sifier votes for its classification and the sense that receives the most number of votes wins."
"The behav-ior is identical to selecting the sense with the highest posterior probability, computed as"
Æ     Æ   ([Footnote_6]) where Æ is the Kronecker function and is the classification of the th classifier.
"6 When parameters needed to be estimated, a 3-1-1 split was used: the systems were trained on three parts, parameters esti-mated on the fourth (in a round-robin fashion) and performance tested on the fifth; special care was taken such that no “test” data was used in training classifiers or parameter estimation."
"The co-efficients can be either equal (in a perfect classifier democracy), or they can be estimated with any of the techniques presented in Section 5.2."
Section 6 presents an empirical evaluation of these tech-niques.
"Under this model, the conditional probability that the word sense is given that classifier  outputs and  classifier, is com-out-puts , puted on development data, and the posterior prob-ability is estimated as"
Æ Æ  (7) where   .   
Each classifier votes for its classification and every pair of classifiers votes for the sense that is most likely given the joint classification.
"In the experi-ments presented in van[REF_CITE], this method was the best performer among the presented methods."
"To empirically test the combination methods pre-sented in the previous section, we ran experiments on the SENSEVAL 1 English data and data from four SENSEVAL 2 lexical sample tasks: English(EN), Spanish(ES), Basque(EU) and Swedish(SV)."
"Un-less explicitly stated otherwise, all the results in the following section were obtained by performing 5-fold cross-validation 6 ."
"To avoid the potential for over-optimization, a single final evaluation system was run once on the otherwise untouched test data, as presented in Section 6.3."
"The data consists of contexts associated with a specific word to be sense tagged (target word); the context size varies from 1 sentence (Spanish) to 5 sentences (English, Swedish)."
Table 1 presents some statistics collected on the training data for the five data sets.
Some of the tasks are quite challeng-ing (e.g. SENSEVAL 2 English task) – as illustrated by the mean participating systems’ accuracies in Ta-ble 5.
"Outlining the claim that feature selection is im-portant for WSD, Table 2 presents the marginal loss in performance of either only using one of the po-sitional feature classes or excluding one of the po-sitional feature classes relative to the algorithm’s full performance using all available feature classes."
"It is interesting to note that the feature-attractive methods (NB,BR,Cosine) depend heavily on the BagOfWords features, while discriminative methods are most dependent on LocalContext features."
"For an extensive evaluation of factors influencing the WSD performance (including representational fea-tures), we refer the readers[REF_CITE]."
"Table 3 shows the fine-grained sense accuracy (per-cent of exact correct senses) results of running the classifier combination methods for 5 classifiers, NB (Naïve Bayes), BR (BayesRatio), TBL, DL and MMVC, including the average classifier accuracy and the best classification accuracy."
"Before examin-ing the results, it is worth mentioning that the meth-ods which estimate parameters are doing so on a smaller training size (3/5, to be precise), and this can have an effect on how well the parameters are estimated."
"After the parameters are estimated, how-ever, the interpolation is done between probability distributions that are computed on 4/5 of the train-ing data, similarly to the methods that do not esti-mate any parameters."
"The unweighted averaging model of probability interpolation (Equation (1)) performs well, obtain-ing over 1% mean absolute performance over the best classifier [Footnote_7] , the difference in performance is statistically significant in all cases except Swedish and Spanish."
"7 The best individual classifier differs with language, as shown in Figure 3(b)."
"Of the classifier combination tech-niques, rank-based combination and performance-based voting perform best."
Their mean 2% absolute improvement over the single best classifier is signif-icant in all languages.
"Also, their accuracy improve-ment relative to uniform-weight probability interpo-lation is statistically significant in aggregate and for all languages except Basque (where there is gener-ally a small difference among all classifiers)."
"To ensure that we benefit from the performance improvement of each of the stronger combination methods and also to increase robustness, a final av-eraging method is applied to the output of the best performing combiners (creating a stacked classi-fier)."
"The last line in Table 3 shows the results ob-tained by averaging the rank-based, EM-vote and"
PB-vote methods’ output.
"The difference in perfor-mance between the stacked classifier and the best classifier is statistically significant for all data sets at a significance level of at least  , as measured by a paired McNemar test."
"One interesting observation is that for all meth-ods of -parameter estimation (EM, PB and uniform weighting) the count-based and rank-based strate-gies that ignore relative probability magnitudes out-perform their equivalent combination models using probability interpolation."
This is especially the case when the base classifier scores have substantially different ranges or variances; using relative ranks effectively normalizes for such differences in model behavior.
"For the three methods that estimate the interpo-lation weights – MSE, EM and PB – three vari-ants were investigated."
"These were distinguished by the granularity at which  the weights), areat POSestimatedlevel: at word  level    ( ( ) and over the entire train-ing set ( )."
Table 4 displays the results obtained by estimating the parameters using EM at different sample granularities for the SENSEVAL 2 English data.
The number in the last column is ob-tained by interpolating the first three systems.
"Also displayed is cross-entropy, a measure of how well the combination classifier estimates the sense prob-abilities,     ."
An interesting issue pertaining to classifier combi-nation is what is the marginal contribution to final combined performance of the individual classifier.
A suitable measure of this contribution is the dif-ference in performance between a combination sys-tem’s behavior with and without the particular clas-sifier.
"The more negative the accuracy difference on omission, the more valuable the classifier is to the ensemble system."
"Figure 4(a) displays the drop in performance ob-tained by eliminating in turn each classifier from the 6-way combination, across four languages, while Figure 4(b) shows the contribution of each classifier on the SENSEVAL 2"
English data for different train-ing sizes (10%-80%) [Footnote_8] .
"8 The latter graph is obtained by sampling repeatedly a prespecified ratio of training samples from 3 of the 5 cross-validation splits, and testing on the other 2."
"Note that the classifiers with the greatest marginal contribution to the combined system performance are not always the best single performing classifiers (Table 3(b)), but those with the most effective original exploitation of the com-mon feature space."
"On average, the classifier that contributes the most to the combined system’s per-formance is the TBL classifier, with an average im-provement of across the 4 languages."
"Also, note that TBL and DL offer the greatest marginal contribution on smaller training sizes (Figure 4(b))."
"At all points in this article, experiments have been based strictly on the original SENSEVAL 1 and SEN - SEVAL 2 training sets via cross-validation."
The of-ficial SENSEVAL 1 and SENSEVAL 2 test sets were unused and unexamined during experimentation to avoid any possibility of indirect optimization on this data.
"But to provide results more readily compara-ble to the official benchmarks, a single consensus system was created for each language using linear average stacking on the top three classifier combi-nation methods in Table 3 for conservative robust-ness."
The final frozen consensus system for each language was applied once to the SENSEVAL test sets.
The fine-grained results are shown in Table 5.
"For each language, the single new stacked com-bination system outperforms the best previously re-ported SENSEVAL results on the identical test data [Footnote_9] ."
"9 To evaluate systems on the full disambiguation task, it is appropriate to compare them on their accuracy at 100% test-data coverage, which is equivalent to system recall in the offi-cial SENSEVAL scores. However, it can also be useful to con-sider performance on only the subset of data for which a sys-tem is confident enough to answer, measured by the secondary measure precision. One useful byproduct of the CBV method is the confidence it assigns to each sample, which we measured by the number of classifiers that voted for the sample. If one restricts system output to only those test instances where all participating classifiers agree, consensus system performance is 83.4% precision at a recall of 43%, for an F-measure of 56.7 on the SENSEVAL 2 English lexical sample task. This outper-forms the two supervised SENSEVAL 2 systems that only had partial coverage, which exhibited 82.9% precision at a recall of 28% (F=41.9) and 66.5% precision at 34.4% recall (F=47.9)."
"As far as we know, they represent the best published results for any of these five SENSEVAL tasks."
"In conclusion, we have presented a comparative evaluation study of combining six structurally and procedurally different classifiers utilizing a rich common feature space."
"Various classifier combi-nation methods, including count-based, rank-based and probability-based combinations are described and evaluated."
"The experiments encompass super-vised lexical sample tasks in four diverse languages: English, Spanish, Swedish, and Basque."
The experiments show substantial variation in J. Henderson and E. Brill. 1999.
Exploiting diversity in natural single classifier performance across different lan-guages and data sizes.
"They also show that this variation can be successfully exploited by 10 differ-ent classifier combination methods (and their meta-voting consensus), each of which outperforms both the single best classifier system and standard classi-fier combination models on each of the 4 focus lan-guages."
"Furthermore, when the stacking consensus systems were frozen and applied once to the other-wise untouched test sets, they substantially outper-formed all previously known SENSEVAL 1 and SEN - SEVAL 2 results on 4 languages, obtaining the best published results on these data sets."
"The authors would like to thank Noah Smith for his comments on an earlier version of this paper, and the anonymous reviewers for their useful comments."
This work was supported by NSF grant[REF_CITE]and ONR/MURI contract[REF_CITE]-01-1-0685.
This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classifica-tion tasks.
We present a new classification correc-tion technique that successfully addresses the prob-lem of under-estimation of infrequent classes in the training data.
"We show that the mixture models are boosting-friendly and that both Adaboost and our original correction technique can improve the re-sults of the raw model significantly, achieving state-of-the-art performance on several standard test sets in four languages."
"With substantially different out-put to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination."
The focus tasks of this paper are two re-lated problems in lexical ambiguity resolution: Word Sense Disambiguation (WSD) and Context-Sensitive Spelling Correction (CSSC).
"Word Sense Disambiguation has a long history as a computational task[REF_CITE], and the field has recently supported large-scale interna-tional system evaluation exercises in multiple lan-guages (S ENSEVAL -1,[REF_CITE], and S ENSEVAL -2,[REF_CITE])."
"General purpose Spelling Correction is also a long-standing task (e.g.[REF_CITE]), tradi-tionally focusing on resolving typographical errors such as transposition and deletion to find the clos-est “valid” word (in a dictionary or a morpholog-ical variant), typically ignoring context."
Yet[REF_CITE]observed that about 25-50% of the spelling errors found in modern documents are ei-ther context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling cor- rectors.
"Previous work has addressed the problem of CSSC from a machine learning perspective, in-cluding Bayesian and Decision List models[REF_CITE], Winnow[REF_CITE]and Transformation-Based Learning[REF_CITE]."
"Generally, both tasks involve the selection be-tween a relatively small set of alternatives per key-word (e.g. sense id’s such as church/ BUILDING and church/ INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntac-tic patterns to resolve between the set of alterna-tives."
"Thus both tasks can share a common feature space, data representation and algorithm infrastruc-ture."
"We present a framework of doing so, while in-vestigating the use of mixture models in conjunction with a new error-correction technique as competi-tive alternatives to Bayesian models."
"While several authors have observed the fundamental similarities between CSSC and WSD (e.g.[REF_CITE]and[REF_CITE]), to our knowledge no previous com-parative empirical study has tackled these two prob-lems in a single unified framework."
"The problem of lexical disambiguation can be mod-eled as a classification task, in which each in-stance of the word to be disambiguated (target word, henceforth), identified by its context, has to be  la-beled  with   one  of the established sense labels . [Footnote_1]"
"1 In the case of spelling correction, the classification labels : &gt;; @= ?GA? (I&lt; are represented by the confusion set rather than sense labels (for example )."
"The approaches  we investigate ! are statistical methods  , out-sense set given a context &quot;$#% ."
"The classifica-putting conditional probability distributions over the tion of a context &quot;8&amp; ()+,* &amp;.-02143/ &quot; is generally made by choosing , but we also present an alterna- tive approach in Section 4.1."
The contexts are represented as a collection of features.
"Previous work in WSD and CSSC[REF_CITE]has found diverse feature types to be useful, in-cluding inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic re-lationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations."
Examples of the feature types we employ are illustrated in Figures 1 and 2.
The syntactic features are intended to capture the predicate-argument relationships in the syn-tactic window in which the target word occurs.
Different relations are considered depending on the target word’s POS.
"For nouns, these relations are: verb-object, subject-verb, modifier-noun, and noun-modified_noun; for verbs: verb-object, verb-particle/preposition, verb-prepositional_object; for adjectives: modifying_adjective-noun."
"Also, words with the same POS as the target word that are linked to the target word by coordinating conjunctions are extracted as sibling features."
The extraction pro-cess is performed using simple heuristic patterns and regular expressions over the POS environment.
"As Figure 2 shows, we considered for the CSSC task the POS bigrams of the immediate left and right word pairs as additional features in order to solve POS ambiguity and capture more of the syntactic environment in which the target word occurs (the elements of a confusion set often have disjoint or very different syntactic functions)."
We investigate in this Section a direct statistical model that uses the same starting point as the algo-rithm presented[REF_CITE].
"We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task[REF_CITE], enhanced with the full richer fea-ture space beyond the traditional unordered bag-of-words."
Algorithm 1 Naïve Bayes K 5 K 57&quot; 8 57&quot; K 5 +L &quot; 8 Model M K L  O (1) [ ]/ \^K 5 K5 B8NMPRQ_
PRQ1 SUT`[REF_CITE]Z K L 57Z L _B8 (2)
It is known that Bayes decision rule is optimal if the distribution of the data of each class is known ([REF_CITE]ch. 2).
"However, the class-conditional distributions of the data are not known and have to be estimated."
"Both Naïve Bayes K + and L 8 the mixture model we investigated estimate 5 &quot; starting from mathematically correct formulations, and thus would be equivalent if the assumptions they make were correct."
Naïve Bayes makes the as-sumption (used to transform Equation (1) into (2)) that the features are conditionally independent given a sense label.
"The mixture model makes a simi-lar assumption, by regarding a document as being completely described by a union of independent fea-tures (Equation (3))."
"In practice, these are not true."
"Given the strong correlation and common redun- dancy of the features in the case of WSD-related tasks, in conjunction with the limited training data on which the probabilities are estimated and the high dimensionality of the feature space, these as-sumptions lead to substantial modeling problems."
Another important observation is that very many of the frequencies involved in the probability esti-mation are zero because of the very sparse feature space.
Naïve Bayes depends heavily on probabil-ities not being zero and therefore it has to rely on smoothing.
"On the other hand, the mixture model is more robust to unseen events, without the need for explicit smoothing."
"Under the proposed mixture model, the condi- -in a context &quot; is estimated as a mixture of the condi-tional probability of a sense given a target word tional sense probability distributions for individual context features:"
Algorithm K 5 +L&quot; 8 Q a1 SUT`VYX
K 5 +LZ &quot; 8[REF_CITE]Z L&quot; 8 O 2 Mixture Model (3) aQ1 SUT`VYX K 5 +L Z 8[REF_CITE]Z L &quot; 8 (4) probability of a sense given a context &quot; is derived as opposed to the Naïve Bayes model in which the from the prior probability of weighted by the con- 8 ditional probabilities of the contextual features bc57&quot;
"The probabilities 5 Z in (4) and K 57Z L  K +L 8 given the sense. in ([Footnote_2]) (MLE), by counting the co-occurrences of and Z can be computed as maximum likelihood estimates versus the occurrences of Z , respectively in the training data."
2[REF_CITE]show that the most ¡ important ¢£¥¤; words for CSSC are contained within a window of .
An extension to this classical estima-tion method is to use distance-weighted counts in-frequencies: / 8 relative [ [ Vn@m V21 1 o o prpq m d d 57Z75 Z &quot; &quot; 8 for ji0 the k9l / 8 stead of raw counts [Footnote_4]&apos; g h 57Z
4 A normalization step is required to output probability dis-tributions.
K 5 (LZ 8 dcf &apos;4g h 57Z ji k 8 tQ \ 1 dcf[REF_CITE]Zk9l / _8 / 8 (5) K 57Z L  [ (6) isk9l / i k denotes the i training k contexts of word - and
"Z is a syntactic headwordcorresponding, d 57Z to &quot; 8 the subset of sense ."
When raw count.
"When Z is a context wordis computed, d 57Z &quot; 8 by position y  w is computed - as a functionpositionsof  the { 57wx x 8 x { word in &quot; and the 8 [ of the y 8 Z target curs in &quot; : d 57Z &quot; ."
There are various { y .
Oneothernaturalways of choosing the weighting measure L y L way is to transform the distance { w+|~x 8 into a j close- l B .nGl ness measure by considering 57w x ([REF_CITE]ch. 14.1).
"This mea-sure proves to be effective for the spelling correc-tion task, where the words in the immediate vicinity are far more important than the rest of the context words 2 , but imposes counterproductive differences between the much wider context positions (such as +30 vs. +31 ) used in WSD, especially when con-sidering large context windows."
"Experimental re-sults indicate that it is more effective to level out the local positional differences given by a continu-ous weighting, by instead using weight-equivalent regions which { 8 can   q@ function [Footnote_5]x , (  is a constant be described with a simple step- [Footnote_3] )."
5 The collocational sense information is specific to the S ENSEVAL -2 English task and relies on the given inventory of collocation sense labels (e.g. art_gallery%1:06:00::).
3 The results shown were ¡ obtained for with term weights doubled within a context window. Various other functions and parameters values were tried on held-out parameter-optimization data for S ENSEVAL -2.
A filtering process based on the overall impor- -tance of a word Z for the disambiguation of also uu
"T employed Q prq m X is rjY  T Q o p X 09 p , using Q alterations k of the form , with  number of senses of target wordproportional - which toit co-the occurs with in the training set. 4 In this way, the words that occur only once in the training set, as well as those that occur with most of the senses of a word, providing no relevant information about the sense itself, are penalized."
Improvements obtained using weighted frequen-cies and filtering over MLE are shown in Table 1.
K 57Z L &quot; 8 dation on S ENSEVAL -2
English data mixture model formula (4).
"When Z is a word, can be seen as weighting factors in the"
"K 57Z L &quot; 8 expresses the positional relationship be- -tween the occurrences of Z and the target word in &quot; , and is computed using step-functions as de-scribed K previously L 8 ."
"When Z word, 57Z &quot; is chosen as the averageis a syntacticvalue ofhead-two ratios expressing the usefulness of the headword type for the given target word and respectively for the POS-class of the target word (adjective, noun, verb)."
These ratios are estimated by using a jack-knife (hold-one-out) procedure on the training set and counting the number times the headword type is a good predictor versus the number of times it is a bad predictor.
"As shown in Table 1, Bayes and mixture models yield comparable results for the given task."
"How-ever, they capture the properties of the feature space in distinct ways (example applications of the two models on the sentence in Figure 1 are illustrated in Figure 3) and therefore, are very appropriate to be used together in combination (see Section 5.4)."
We first present an original classification correction method based on the variation of posterior probabil-ity estimates across data and then the adaptation of the Adaboost method[REF_CITE]to the task of lexical classification.
One problem arising from the sparseness of training data is that mixture models tend to excessively fa-vor the best represented senses in the training set.
"A probable cause is that spurious words, which can not be considered general stopwords but do not carry sense-disambiguation information for a particular target word, may occur only by chance both in train-ing and test data. [Footnote_6] Another cause is the fact that mixture models search for decision surfaces linear in the feature space [Footnote_7] ; therefore, they can not make only correct classifications (unless the feature space can be divided by linear conditions) and the sam-ples for the under-represented senses are likely to be interpreted as outliers."
"6 For example, assuming that every context contains approx-imately the same number of such words, then given two senses, one represented in the training set by 20 examples, and the other one by 4, it is five times more likely that a spurious word in a test context co-occurs with the larger sampled sense."
"7[REF_CITE]shows that Bayes, TBL and Decision Lists also search for a decision surface which is a linear function in the feature space"
"To address this estimation problem, a second classification step is employed, based on the obser-vation that the deviation of a component of the pos-terior distribution from its expected value (as com-puted over the training set) can ,* be &amp;.-0 as 2143¨/ relevant K » 5 (L &quot; 8 as the maximum of the distribution ."
"In-stead of classifying each test context independently after estimating its sense probability distribution, we classify it by comparing it with the whole space of training contexts, for which the posterior distri-butions are computed using a jackknife procedure."
"Figure 4(a) illustrates such an example: each line in the table represents the posterior distribution over senses given a context, each column contains the values corresponding to a particular sense in the posterior  distributions of all contexts."
"Intuitively, sense ¼ may be preferred @¾ 5 .&amp; &apos; K» to 8 the most likely sense test @¾ 8 context &quot; the ¾ 8 fact that the 5 for K » the L &quot; despite &lt;¼ L &quot; @ is smaller (&amp; &apos; than 8 5 of the analogy with !&quot; ¿95 because ues” of the components corresponding toand the “expected  val-  and ."
"Unfortunately, we face again the problem of under-representation in the training data: the ex-pected values in the posterior distributions for the under-represented senses when they express the cor-rect classification can not be accurately estimated."
"Therefore, we have to look at the problem from an-other angle. 



"
"Assuming that the selections are representa-tive and there exist first and second order moments for the underlying distributions (conditions which we call “good statistical Ö| properties Õ of the”),classifieran improve-can ment in the accuracy be expected when Î choosing  ÒØ tional coefficient Ä×&quot; a sense with a varia-sifier distribution’s mode (&amp; &apos; )(*, instead .&amp; -0/ K 5 +L &quot; [Footnote_8] of the clas- (if such a sense exists)."
8 It is hard to judge how well estimated these statistics are without making any distributional assumptions.
"For example, knowing that the per-formance of the mixture model for S ENSEVAL -2 is approximatively coefficients is set to ."
"Because spurious words not only favor the better represented senses in the training set, but also can affect the variational coef-ficients of unlikely senses, some restrictions had to be imposed in our implementation to avoid the other extreme of favoring unlikely senses."
"The mixture model does not guarantee the re-quirements imposed by the MVC method are met, but it has the advantage over the Bayesian model that each of the components of the posterior distri-bution it computes can be seen as a weighted mix-ture of random variables corresponding to the indi-vidual features."
"In the simplest case, when consid-ering binary features, these variables are Bernoulli trials."
"Furthermore, if the trials have the same probability-mass function then a component of the posterior distribution will follow a binomial distri-bution, and therefore would have good statistical properties."
"In general, the underlying distributions can not be computed, but our experiments show that they usually have good statistical properties as re-quired by MVC."
AdaBoost is an iterative boosting algorithm intro-duced[REF_CITE]shown to be successful for several natural language classifica-tion tasks.
"AdaBoost successively builds classifiers based on a weak learner (base learning algorithm) by weighting differently the examples in the training space, and outputs the final classification by mix-ing the predictions of the iteratively built classifiers."
"Because sense disambiguation is a multi-class prob-lem, we chose to use version AdaBoost."
We could not apply AdaBoost straightforwardly to the problem of sense disambiguation because of the high dimensionality and sparseness of the fea- ture space.
"Superficial modeling of the training set can easily be achieved because of the singu-larity/rarity of many feature values in the context space, but this largely represents overfitting of the training data."
"In order to solve this problem, we partial updating technique."
"At each round, Ý clas-use AdaBoost in conjunction with jackknife and a sifiers are built using as training all the examples in the training set except the one to be classified, and the weights are updated at feature level rather than context level."
"This modified Adaboost algorithm could only be implemented for the mixture model, which “perceives” the contexts as additive mixture of features."
The Adaboost-enhanced mixture model is called AdaMixt henceforth.
"We present a comparative study for four languages (English, Swedish, Spanish, and Basque) by per-forming 5-fold cross-validation on the S ENSEVAL -2 lexical-sample training data, using the fine-grained sense inventory."
"For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm[REF_CITE]based[REF_CITE]was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented[REF_CITE]was employed."
"We also present the results obtained by the different algorithms on another WSD standard set, S ENSEVAL -1, also by performing 5-fold cross validation on the original training data."
"For CSSC, we tested our system on the identical data from the Brown corpus used[REF_CITE],[REF_CITE]and[REF_CITE]."
"Finally, we present the re-sults obtained by the investigated methods on a sin-gle run on the Senseval-1 and Senseval-2 test data."
The described models were initially trained and tested by performing 5-fold cross-validation on the S ENSEVAL -2 English lexical-sample-task training data.
"When parameters needed to be estimated, jackknife or a 3-1-1 split (training and/or parame-ter estimation - testing) were used."
The English training set for S ENSEVAL -2 is com-posed of 8861 instances representing 73 target words with an average number of 12.5 senses per word.
Table 2 illustrates the performance of each of the studied models broken down by part-of-speech.
"As observed in most experiments, the feature-enhanced Naïve Bayes has the tendency to outperform by a small margin the raw mixture model, but because the latter proved to be boosting-friendly, its augmented versions achieved the high-and enhanced Naïve Bayes  is significant (McNemar est final accuracies."
The difference between MMVC rejection risk of Þ~ ).
"Figure 5 shows both the performance of the mix-ture model alone and in conjunction with MVC, and highlights the improvement in performance achieved by the latter for each of the 4 languages. significant (for S ENSEVAL -2 English data, the rejec-   All MMVC versus MM differences are statistically tion probability of a paired McNemar test is )."
Figure 6 shows what is generally a log-linear in-crease in performance of MM alone and in combi-nation with the MVC method over increasing train-ing sizes.
"Because of the way the smallest training sets were created to include at least one example for each sense, they were more balanced as a side effect, and the compensations introduced by MVC were less productive as a result."
"Given more training data, MMVC starts to improve relative to the raw model both because the training sets become more unbal-anced in their sense distributions and because the empirical moments and the variational coefficients on which the method relies are better estimated."
"The systems used for S ENSEVAL -2 English data were also evaluated on the S ENSEVAL -1 training data (30 words, 12479 instances, with an average of 10.8 senses per word) by using 5-fold cross val-idation."
There was no further tuning of the feature space or model parameters to adapt them to the par-ticularities of this new test set.
Comparative perfor-mance is shown in Table 3.
The difference between MMVC and enhanced Naïve Bayes is statistically significant (McNemar rejection risk 0.036).
"Both MM and the enhanced Bayes model obtain vir-tually the same overall performance [Footnote_9] as the TriB-ayes system reported[REF_CITE], which uses a similar feature space."
9 All figures reported are for the standard 14 confusion sets; the accuracies for the 18 sets are generally higher.
"The correction and boosting methods we investigated marginally improve the performance of the mixture model, as can be seen in Table 4 but they do not achieve the performance[REF_CITE].1%[REF_CITE]and[REF_CITE].5%[REF_CITE], methods that include features more directly specialized for spelling correction."
"Because of the small size of the test set, the differences in performance are due to only 14 and 20 more incorrectly classified exam-ples respectively."
More important than this differ-ence [Footnote_10] may be the fact that the systems built for WSD were able to achieve competitive performance with little to no adaptation (we only enriched the feature space by adding the POS bigrams to the left and right of the target word and changed the weight-ing model as presented in Section 3 because spelling correction relies more on the immediate than long-distance context).
10 We did not have the actual classifications from the other systems to check the significance of the difference.
"Another important aspect that can be seen in Table 4 is that there was no model that constantly performed best in all situations, suggest-ing the advantage of developing a diverse space of models for classifier combination."
"The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naïve Bayes (9.6% averaged complementary rate, as defined[REF_CITE])."
"Table 5 shows the im-provement obtained by adding the MMVC model to empirically the best voting system we had us-ing Bayes, BayesRatio, TBL and Decision Lists (all classifier combination methods tried and their results are presented exhaustively in Florian and in both Wõ cases,  ¾ as measured by a paired Úö McNemar  ¾[REF_CITE])."
"The improvement is significant test: for S ENSEVAL -1 data, for S ENSEVAL -2 data."
MMVC is also the top performer of the 5 sys-
"We investigated the properties and performance of mixture models and two augmenting methods in an unified framework for Word Sense Disambiguation and Context-Sensitive Spelling Correction, showing experimentally that such joint models can success-fully match and exceed the performance of feature-enhanced Bayesian models."
"The new classifica-tion correction method (MVC) we propose suc-cessfully addresses the problem of under-estimation of less likely classes, consistently and significantly improving the performance of the main mixture model across all tasks and languages."
"Finally, since the mixture model and its improvements performed well on two major tasks and several multilingual data sets, we believe that they can be productively applied to other related high-dimensionality lexi-cal classification problems, including named-entity classification, topic classification, and lexical choice in machine translation."
"In this paper, we evaluate a vari-ety of knowledge sources and super-vised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data."
"Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations."
The learning al-gorithms evaluated include
"Support Vec-tor Machines (SVM), Naive Bayes, Ad-aBoost, and decision tree algorithms."
We present empirical results showing the rela-tive contribution of the component knowl-edge sources and the different learning algorithms.
"In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves ac-curacy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data."
Natural language is inherently ambiguous.
A word can have multiple meanings (or senses).
"Given an occurrence of a word in a natural language text, the task of word sense disambiguation (WSD) is to determine the correct sense of in that context."
WSD is a fundamental problem of natural language processing.
"For example, effective WSD is crucial for high quality machine translation."
One could envisage building a WSD system us-ing handcrafted rules or knowledge obtained from linguists.
"Such an approach would be highly labor-intensive, with questionable scalability."
Another ap-proach involves the use of dictionary or thesaurus to perform WSD.
"In this paper, we focus on a corpus-based, super-vised learning approach."
"In this approach, to disam-biguate a word , we first collect training texts in which instances of occur."
Each occurrence of is manually tagged with the correct sense.
"We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context."
"Two WSD evaluation exercises, SENSEVAL-1[REF_CITE]and SENSEVAL-2[REF_CITE], were conducted in 1998 and 2001, respectively."
"The lexical sample task in these two SENSEVALs focuses on evalu-ating WSD systems in disambiguating a subset of nouns, verbs, and adjectives, for which manually sense-tagged training data have been collected."
"In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs."
There is a large body of prior research on WSD.
"Due to space constraints, we will only highlight prior re-search efforts that have investigated (1) contribution of various knowledge sources, or (2) relative perfor-mance of different learning algorithms."
Early research efforts on comparing different learning algorithms[REF_CITE]tend to base their comparison on only one word or at most a dozen words.
The recent work[REF_CITE]and[REF_CITE]evaluated a variety of learning algorithms on the SENSEVAL-1 data set.
"However, all of these research efforts con-centrate only on evaluating different learning algo-rithms, without systematically considering their in-teraction with knowledge sources."
"However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms."
"Participating systems at SENSEVAL-1 and SENSEVAL-2 tend to report accuracy using a par-ticular set of knowledge sources and some partic-ular learning algorithm, without investigating the effect of varying knowledge sources and learning algorithms."
"In SENSEVAL-2, the various Duluth systems[REF_CITE]attempted to investigate whether features or learning algorithms are more im-portant."
"However, relative contribution of knowl-edge sources was not reported and only two main types of algorithms (Naive Bayes and decision tree) were tested."
"In contrast, in this paper, we systematically vary both knowledge sources and learning algorithms, and investigate the interaction between them."
"We also base our evaluation on both SENSEVAL-2 and SENSEVAL-1 official test data sets, and compare with the official scores of participating systems."
"To disambiguate a word occurrence , we consider four knowledge sources listed below."
Each training (or test) context of generates one training (or test) feature vector.
"We use  7 features   to  encode    this knowledge   source: , where ( ) is the"
"POS of the th token to the left (right) of , and is the POS of ."
"A token can be a word or a punc-tuation symbol, and each of these neighboring to-kens must be in the same sentence as ."
We use a sentence segmentation program[REF_CITE]and a POS tagger[REF_CITE]to segment the tokens surrounding into sentences and assign POS tags to these tokens.
"For example, to disambiguate the word bars in the POS-tagged sentence “Reid/NNP saw/VBD me/PRP looking/VBG at/IN the/DT iron  /NN  bars ! /NNS ! # ./.” &quot; , %$ the &amp;(%&apos; (*&apos; POS ) wherefeature &apos; vectordenotesis the POS tag of a null token."
"For this knowledge source, we consider all single words (unigrams) in the surrounding context of , and these words can be in a different sentence from ."
"For each training or test example, the SENSE-VAL data sets provide up to a few sentences as the surrounding context."
"In the results reported in this paper, we consider all words in the provided context."
"Specifically, all tokens in the surrounding context of are converted to lower case and replaced by their morphological root forms."
Tokens present in a list of stop words or tokens that do not contain at least an alphabet character (such as numbers and punctuation symbols) are removed.
All remaining tokens from all training contexts provided for are gathered.
Each remaining token + contributes one feature.
"In a training (or test) example, the feature corresponding to + is set to 1 iff the context of in that training (or test) example contains + ."
We attempted a simple feature selection method to investigate if a learning algorithm performs better with or without feature selection.
"The feature selec-tion method employed has one parameter: , ."
"A feature + is selected if + occurs in some sense of , or more times in the training data."
This param-eter is also -/ used . by (Ng -10 and[REF_CITE]).
"We have tried , and , (i.e., no feature selection) in the results reported in this paper."
"For example, if is the word bars and the set of selected unigrams is 2 chocolate, iron, beer 3 , the feature vector for the sentence ) “Reid saw me looking at the iron bars .” is 0, 1, 0 ."
"A local collocation 4 7 refers to the ordered se-quence of tokens in the local, narrow context of ."
"Offsets and 8 denote the starting and ending posi-tion (relative to ) of the sequence, where a neg-ative (positive) offset refers to a token to its left (right)."
"For example, let be the word bars in the sentence  “ 95: Reid saw me looking  at ;5 the iron bars &apos; .”"
"Then &apos; 4 is the iron and 4 is iron . , where denotes a null token."
"Like POS, a colloca-tion does not cross sentence boundary."
"To represent this knowledge source of local collocations, we ex-tracted 11 features ;:5  corresponding ;5: 95: to the 95 following 95: collocations ;:5 ;5 : 4 : , 4 95: , 4 ;5 , 4 , ;45 , 4 , 4 , 4 , 4 , 4 , and 4 ."
This set of 11 features is the union of the collocation fea-tures used[REF_CITE]and[REF_CITE].
"To extract 7 the feature values of the collocation feature 4 , we first collect all possible collocation strings  7 (converted into lower case) corresponding to 4 in all training contexts of ."
"Unlike the case for surrounding words, we do not remove stop words, numbers, or punctuation symbols."
Each collocation string is a possible feature value.
"Feature value se-lection using , , analogous to that used to select surrounding words, can be optionally applied."
"If a training (or test) context of has collocation =5 7 &lt; , and &lt; is a selected feature value, then the 4 feature of has value &lt; ."
"Otherwise, it has the value &gt; , denot-ing the null string."
"Note that each collocation 4 7 is represented by one feature that can have many possible feature val-ues (the local collocation strings), whereas each dis-tinct surrounding word is represented by one feature that takes binary values (indicating presence or ab-sence of that word)."
"For example, if is the word bars 95: and  suppose the set of selected collocations for 4 is 2 a chocolate, the wine, 95 the : iron 3 , then the feature value for collocation 4 in the sen-tence “Reid saw me looking at the iron bars .” is the iron."
We first parse the sentence containing with a sta-tistical parser[REF_CITE].
The constituent tree structure generated by Charniak’s parser is then con-verted into a dependency tree in which every word points to a parent headword.
"For example, in the sentence “Reid saw me looking at the iron bars .”, the word Reid points to the parent headword saw."
"Similarly, the word me also points to the parent headword saw."
"We use different types of syntactic relations, de-pending on the POS of ."
"If is a noun, we use four features: its parent headword ? , the POS of ? , the voice of ? (active, passive, or &gt; if ? is not a verb), and the relative position of ? from (whether ? is to the left or right of )."
"If is a verb, we use six features: the nearest word @ to the left of such that is the parent headword of @ , the nearest word A to the right of such that is the parent headword of A , the POS of @ , the POS of A , the POS of , and the voice of ."
"If is an adjective, we use two fea-tures: its parent headword ? and the POS of ? ."
"We also investigated the effect of feature selection on syntactic-relation features that are words (i.e., POS, voice, and relative position are excluded)."
Some examples are shown in Table 1.
"Each POS noun, verb, or adjective is illustrated by one exam-ple."
"For each example, (a) shows and its POS; (b) shows the sentence where occurs; and (c) shows the feature vector corresponding to syntactic rela-tions."
"We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and de-cision trees (DT)."
All the experimental results re-ported in this paper are obtained using the imple-mentation of these algorithms in W EKA[REF_CITE].
All learning parameters use the de-fault values in W EKA unless otherwise stated.
The SVM[REF_CITE]performs optimization to find a hyperplane with the largest margin that sep-arates training examples into two classes.
A test example is classified depending on the side of the hyperplane it lies in.
Input features can be mapped into high dimensional space before performing the optimization and classification.
A kernel function (linear by default) can be used to reduce the compu-tational cost of training and testing in high dimen-sional space.
"If the training examples - are B nonsep-arable, a regularization parameter 4 ( by de-fault) can be used to control the trade-off between achieving a large margin and a low training error."
"In W EKA ’s implementation of SVM, each nominal feature with C possible values is converted into C binary (0 or 1) features."
"If a nominal feature takes the th feature value, then the th binary feature is set to 1 and all the other binary features are set to 0."
"We tried higher order polynomial kernels, but they gave poorer results."
Our reported results in this pa-per used the linear kernel.
AdaBoost[REF_CITE]is a method of training an ensemble of weak learners such that the performance of the whole ensemble is higher than its constituents.
"The basic idea of boosting is to give more weights to misclassified training ex-amples, forcing the new classifier to concentrate on these hard-to-classify examples."
A test example is classified by a weighted vote of all trained classi-fiers.
We use the decision stump (decision tree with only the root node) as the weak learner in AdaBoost.
W EKA implements AdaBoost.M[Footnote_1].
"1[REF_CITE]training instances from the HECTOR dic-tionary used in SENSEVAL-1, together with 13,127 training in-stances from the training corpus supplied."
The Naive Bayes classifier[REF_CITE]assumes the features are independent given the class.
"During classification, it chooses the class with the highest posterior probability."
The default setting uses Laplace (“add one”) smoothing.
The decision tree algorithm[REF_CITE]parti-tions the training examples using the feature with the highest information gain.
It repeats this process re-cursively for each partition until all examples in each partition belong to one class.
A test example is clas-sified by traversing the learned decision tree.
"W EKA implements Quinlan’s C4.5 decision tree algorithm, with pruning by default."
"In the SENSEVAL-2 English lexical sample task, participating systems are required to disambiguate 73 words that have their POS predetermined."
"There are 8,611 training instances and 4,328 test instances tagged with W ORD N ET senses."
Our evaluation is based on all the official training and test data of SENSEVAL-2.
"For SENSEVAL-1, we used the 36 trainable words for our evaluation."
"For SENSEVAL-1, 4 trainable words belong to the indeterminate category, i.e., the POS is not provided."
"For these words, we first used a POS tagger[REF_CITE]to determine the correct POS."
"For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form."
"During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used."
"Otherwise, the classifier for is used."
We ran the different learning algorithms using var-ious knowledge sources.
Table 2 (Table 3) shows the accuracy figures for the different combinations of knowledge sources and learning algorithms for the SENSEVAL-2 (SENSEVAL-1) data set.
"The nine columns correspond to: (i) using only POS of neighboring words (ii) using only single words in the H-I surrounding . context with feature selection ( , ) (iii -J0 ) same as (ii) but without feature se- lection ( , ) (iv) using K-L only . local collocations with feature selection ( , without feature selection ( ,  )"
"M (v 0 )) same(vi) usingas (iv)onlybut syntactic F-N. relations with feature selection on words ( , ) ( ! vii -O0 ) same as (vi) but without feature se-lection ( , ) (viii) combining all four knowl-edge sources with feature selection (ix) combining all four knowledge sources without feature selec-tion."
SVM is only capable of handling binary class problems.
The usual practice to deal with multi-class problems is to build one binary classifier per output class (denoted “1-per-class”).
"The original AdaBoost, Naive Bayes, and decision tree algo- rithms can already handle multi-class problems, and we denote runs using the original AdB, NB, and DT algorithms as “normal” in Table 2 and Table 3."
"Accuracy for each word task can be measured by recall (r) or precision (p), defined by: - no. of test instances correctly labeled r no. of test instances in word task - no. of test instances correctly labeled p no. of test instances output in word task"
Recall is very close (but not always identical) to pre-cision for the top SENSEVAL participating systems.
"In this paper, our reported results are based on the official fine-grained scoring method."
"To compute an average recall figure over a set of words, we can either adopt micro-averaging (mi) or macro-averaging (ma), defined by: - total no. of test instances correctly labeled mi total no. of test instances in all word tasks - B acbed word f tasks d  ma recall for word task word tasks"
"That is, micro-averaging treats each test instance equally, so that a word task with many test instances will dominate the micro-averaged recall."
"On the other hand, macro-averaging treats each word task equally."
"As shown in Table 2 and Table 3, the best micro-averaged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowl-edge sources (without feature selection) and using SVM as the learning algorithm."
"In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down ac-cording to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words."
We also tabu-late analogous figures for the top three participating systems for both SENSEVALs.
"The top three sys-tems for SENSEVAL-2 are: JHU (S1)[REF_CITE], SMUls (S2)[REF_CITE], and KUNLP (S3)[REF_CITE]."
"The top three systems for SENSEVAL-1 are: hopkins (s1)[REF_CITE], ets-pu (s2)[REF_CITE], and tilburg (s3)[REF_CITE]."
"As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best offi-cial scores of both SENSEVALs."
We also conducted paired t test to see if one system is significantly better than another.
"The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value."
A large p value indicates that the two sys-tems are not significantly different from each other.
The comparison between our learning algorithms and the top three participating systems is given in Table 5.
"Note that we can only compare macro-averaged recall for SENSEVAL-1 systems, since the sense of each individual test instance output by the SENSEVAL-1 participating systems is not avail-able."
The comparison indicates that our SVM sys-tem is better than the best official SENSEVAL-2 and SENSEVAL-1 systems at the level of significance 0.05.
"Note that we are able to obtain state-of-the-art re-sults using a single learning algorithm (SVM), with-out resorting to combining multiple learning algo-rithms."
Several top SENSEVAL-2 participating sys-tems have attempted the combination of classifiers using different learning algorithms.
"In SENSEVAL-2, JHU used a combination of various learning algorithms (decision lists, cosine-based vector models, and Bayesian models) with various knowledge sources such as surrounding words, local collocations, syntactic relations, and morphological information."
"SMUls used a k-nearest neighbor algorithm with features such as keywords, collocations, POS, and name entities."
"KUNLP used Classification Information Model, an entropy-based learning algorithm, with local, topical, and bigram contexts and their POS."
"In SENSEVAL-1, hopkins used hierarchical de-cision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with fea-tures similar to those used[REF_CITE]. tilburg also used dictionary examples as additional training data."
"Based on our experimental results, there appears to be no single, universally best knowledge source."
"In-stead, knowledge sources and learning algorithms interact and influence each other."
"For example, lo-cal collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB."
NB even outperforms SVM if only POS is used.
"In addition, different learning algorithms benefit dif-ferently from feature selection."
"SVM performs best without feature selection, whereas h-J NB . performs best with some feature selection ( , )."
We will in- vestigate the effect of more elaborate feature selec-tion schemes on the performance of different learn-ing algorithms for WSD in future work.
"Also, using the combination of four knowledge sources gives better performance than using any single individual knowledge source for most al-gorithms."
"On the SENSEVAL-2 test set,[REF_CITE].4% (all 4 knowledge sources), 64.8% (remove syntactic relations), 61.8% (further remove POS), and 60.5% (only collocations) as knowledge sources are removed one at a time."
"Before concluding, we note that the SENSEVAL-2 participating system UMD-SST[REF_CITE]also used SVM, with surrounding words and local collocations as features."
"However, they re-ported recall of only 56.8%."
"In contrast, our im-plementation of SVM using the two knowledge sources of surrounding words and local collocations achieves recall of 61.8%."
"Following the description[REF_CITE], our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%."
The performance drop from 61.8% may be due to the different collocations used in the two systems.
"It has been suggested that some forms of speech dis uencies, most notable interjections and par-entheticals, tend to occur disproportionally at major clause boundaries [6] and thus might serve to aid parsers in establishing these bound-aries."
We have tested a current statistical parser [1] on Switchboard text with and without inter-jections and parentheticals and found that the parser performed better when not faced with these extra phenomena.
"This suggest that for current parsers, at least, interjection and paren-thetical placement does not help in the parsing process."
It is generally recognized that punctuation helps in parsing text.
"For example, Roark [5] nds that removing punctuation decreases his parser&apos;s accuracy from 86.6% to 83.8%."
Our experiments with the parser described in [1] show a similar fallo .
"Unfortunately spoken English does not come with punctuation, and even when transcriptions add punctuation, as in the Switchboard [4] corpus of transcribed (and parsed) telephone calls, it&apos;s utility is small [5] For this and other reasons there is considerable interest in nding other aspects of speech that might serve as a replacement."
One suggestion in this vein is that the place-ment of some forms of speech errors might encode useful linguistic information.
"Speech, of course, contains many kinds of errors that can make it more diÆcult to parse than text."
"Roughly speaking the previously mentioned Switchboard corpus distinguishes three kinds of errors: interjections ( lled pauses) | \I, um, want to leave&quot; parentheticals | \I, you know, want to leave&quot; speech repairs | \I can, I want to leave&quot;"
"Of these, speech repairs are the most injurious to parsing."
"Furthermore, even if one&apos;s parser can parse the sentence as it stands, that is not suÆcient."
"For example, in \I can, I want to leave&quot;, it is not necessarily the case that the speaker believes that he or she can, in fact, leave, only that he or she wants to leave."
"Thus in [2] speech repairs were rst detected in a sep-arate module, and deleted before handing the remaining text to the parser."
The parser then produced a parse of the text without the re-paired section.
"The other two kinds of errors, interjec-tions, and parentheticals, (henceforth INTJ s and PRN s) are less problematic."
"In particular, if they are left in the text either their seman-tic content is compatible with the rest of the utterance or there is no semantic content at all."
"For example, Table 1 gives the 40 most common INTJ s, which comprise 97% of the total. (Un-listed INTJ s comprise the remaining 3%.)"
"They are easily recognized as not carrying much, if any PRN , contents are.more diverse."
Table 2 lists the 40 most common PRN s.
"They only comprise 65% of all cases, and many do contain semantics content."
"In such cases, however, the semantic content is compatible with the rest of the sen-tence, so leaving them in is perfectly acceptable."
"Thus [2], while endeavoring to detect and re-move speech repairs, left interjections and par-entheticals in the text for the parser to cope with."
Indeed [6] nds that both interjections and parentheticals tend to occur at major sentence boundaries.
Also [7] suggest that this prop- erty accounts for their observation that remov-ing these dis uencies does not help in language modeling perplexity results.
This strongly sug-gests that INTJ /
"PRN location information in speech text might in fact, improve parsing per-formance by helping the parser locate con-stituent boundaries with high accuracy."
"That is, a statistic parser such as [1] or [3] when trained on parsed Switchboard text with these phenom-ena left in, might learn the statistical correla-tions between them and phrase boundaries just as they are obviously learning the correlations between punctuation and phrase boundaries in written text."
"In this paper then we wish to determine if the presence of INTJ s and PRN s do help parsing, at least for one state-of-the-art statistical parser [1]."
The experimental design used was more com-plicated than we initially expected.
We had an-ticipated that the experiments would be con-ducted analogously to the \no punctuation&quot; ex-periments previously mentioned.
"In those ex-periments one removes punctuation from all of the corpus sentences, both for testing and train-ing, and then one reports the results before and after this removal. (Note that one must remove punctuation from the training data as well so that it looks like the non-punctuated testing data it receives.)"
"Parsing accuracy was mea-sured in the usual way, using labeled precision recall."
"Note, however, and this is a critical point, that precision and recall are only mea-sured on non-preterminal constituents."
"That is, if we have a constituent (PP (IN of) (NP (DT the) (NN book))) our measurements would note if we correctly found the PP and the NP , but not the preter-minals IN , DT , and NN ."
"The logic of this is to avoid confusing parsing results with part-of-speech tagging, a much simpler problem."
"Initially we conducted similarly designed ex-periments, except rather than removing punc-tuation, we removed INTJ s and PRN s and com-pared before and after precision/recall numbers."
"These numbers seemed to con rm the antici-pated results: the \after&quot; numbers, the numbers without INTJ /"
"PRN s were signi cantly worse, suggesting that the presence of INTJ /"
PRN s helped the parser.
"Unfortunately, although ne for punctuation, this experimental design is not suÆcient for measuring the e ects of INTJ /"
PRN s on parsing.
The di erence is that punctuation itself is not measured in the precision-recall numbers.
"That is, if we had a phrase like (NP (NP (DT a) (NN sentence)) (, ,) (ADJP (JJ like) (NP (DT this) (DT one)))) we would measure our accuracy on the three NP&apos;s and the ADJP, but not on the pretermi-nals, and it is only at the preterminal level that punctuation appears."
The same cannot be said for INTJ /
"PRN s. Consider the (slightly simpli ed) Switchboard parse for a sentence like \I, you know, want to leave&quot;: (S (NP I) (PRN , you know ,) (VP want (S to leave)))"
The parenthetical PRN is a full non-terminal and thus is counted in precision/recall measure-ments.
Thus removing preterminals is chang-ing what we wish to measure.
"In particu-lar, when our initial results showed that re-moval of INTJ / PRN s lowered precision/recall we worried that it might be that INTJ /"
"PRN s are particularly easy to parse, and thus removing them made things worse, not because of col-lateral damage on our ability to parse other constituents, but simply because we removed a body of easily parseable constituents, leaving the more diÆcult constituents to be measured."
The above tables of INTJ s and PRN s lends cre-dence to this concern.
Thus in the experiments below all measure-ments are obtained in the following fashion: 1.
The parser is trained on switchboard data with/without INTJ /
"PRN s or punctuation, creating eight con gurations: 4 for neither, both, just INTJ s, and just PRN s, times two for with and without punctuation."
We tested with and without punctuation to con rm Roark&apos;s earlier results showing that they have little in uence in Switchboard text. 2.
The parser reads the gold standard testing examples and depending on the con gura-tion INTJ s and/or PRN S are removed from the gold standard parse. 3.
Finally the resulting parse is compared with the gold standard.
"However, any re-maining PRN s or INTJ s are ignored when computing the precision and recall statis-tics for the parse."
"To expand a bit on point (3) above, for an experiment where we are parsing with INTJ s, but not PRN s, the resulting parse will, of course, contain INTJ s, but (a) they are not counted as present in the gold standard (so we do not a ect recall statistics), and (b) they are not evaluated in the guessed parse (so if one were labeled, say, an S, it would not be counted against the parse)."
"The intent, again, is to not allow the results to be in uenced by the fact that interjections and parentheticals are much easier to nd than most (if not) all other kinds of constituents."
As in [2] the Switchboard parsed/merged cor-pus directories two and three were used for training.
"In directory four, les sw4004.mrg to sw4153.mrg were used for testing, and sw4519.mrg to sw4936 for development."
"To avoid confounding the results with problems of edit detection, all edited nodes were deleted from the gold standard parses."
The results of the experiment are given in table 3.
We have shown results separately with and without punctuation.
A quick look at the data indicates that both sets show the same trends but with punctuation helping per-formance by about 1.0% absolute in both pre-cision and recall.
"Within both groups, as is al-ways the case, we see that the parser does better when restricted to shorter sentences (40 words and punctuation or less)."
"We see that removing PRN s or INTJ s separately both improve parsing accuracy (e.g., from 87.201% to 87.845|that the e ect of removing both is approximately additive (e.g., from 87201% to 88.863%, again on the with-punctuation data)."
Both with and without punctuation results hint that removing parentheticals was usually more helpful than re-moving interjections.
"However in one case the reverse was true (with-punctuation, sentences 40) and in all cases the di erences are at or under the edge of statistical reliability."
"In con-trast, the di erences between removing neither, removing one, or removing both INJs and PRNs are quite comfortably statistically reliable."
Based upon Tabel 3 our tentative conclusion is that the information present in parentheticals and interjections does not help parsing.
"There are, however, reasons that this is a tentative con-clusion."
"First, in our e ort to prevent the ease of recognizing these constructions from giving an unfair advantage to the parser when they are present, it could be argued that we have given the parser an unfair advantage when they are absent."
"After all, even if these constructions are easily recognized, the parser is not perfect on them."
"While our labeled precision/recall mea-surements are made in such a way that a mis-take in the label of, say, an interjection, would not e ect the results, a mistake on it&apos;s position typically would have an e ect because the po-sitions of constituents either before or after it would be made incorrect."
Thus the parser has a harder task set for it when these constituents are left in.
"It would be preferable to have an experimen-tal design that would somehow equalize things, but we have been unable to nd one."
Fur-thermore it is instructive to contrast this situ-ation with that of punctuation in Wall Street
If we had found that parsing without punctuation made things easier a sim-ilar argument could be made that the without-punctuation case was given an unfair advantage since it had a lot fewer things to worry about.
But punctuation in well-edited text contains more than enough information to overcome the disadvantage.
This does not seem to be the case with INTJ s and PRN s.
Here the net information content here seems to be negative.
"A second, and in our estimation more serious, objection to our conclusion is that we have only done the experiment with one parser."
Perhaps there is something speci c to this parser that systematically underestimates the usefulness of INTJ /
"While we feel reason-ably con dent that any other current parser would nd similar e ects, it is at least possi-ble to imagine that quite di erent parsers might not."
Statistical parsers condition the probabil-ity of a constituent on the types of neighbor-ing constituents.
"Interjections and parenthet-icals have the e ect of increasing the kinds of neighbors one might have, thus splitting the data and making it less reliable."
"The same is true for punctuation, of course, but it seems plausible that well edited punctuation is suÆ-ciently regular that this problem is not too bad, while spontaneous interjections and parentheti-cals would not be so regular."
"Of course, nding a parser design that might overcome this prob-lem (assuming that this is the problem) is far from obvious."
We have tested a current statistical parser [1] on Switchboard text with and without interjections and parentheticals and found that the parser performs better when not faced with these ex-tra phenomena.
"This suggest that for current parsers, at least, interjection and parenthetical placement does not help in the parsing process."
"This is, of course, a disappointing result."
"The phenomena are not going to go away, and what this means is that there is probably no silver lining."
We should also note that the idea that they might help parsing grew from the observation that interjections and parentheticals typically occur at major clause boundaries.
One might then ask if our results cast any doubt on this claim as well.
We do not think so.
Interjections and parentheticals do tend to identify clause boundaries.
"The problem is that many other things do so as well, most notably normal gram-matical word ordering."
The question is whether the information content of dis uency placement is suÆcient to overcome the disruption of word ordering that it entails.
"The answer, for current parsers at least, seems to be &quot;no&quot;."
"We would like to acknowledge the members of the Brown Laboratory for Linguistic Informa-tion Processing, This research has been sup-ported in part by NSF grants[REF_CITE]and[REF_CITE]."
Most machine learning solutions to noun phrase coreference resolution recast the problem as a classification task.
"We ex-amine three potential problems with this reformulation, namely, skewed class dis-tributions, the inclusion of “hard” training instances, and the loss of transitivity in-herent in the original coreference relation."
We show how these problems can be han-dled via intelligent sample selection and error-driven pruning of classification rule-sets.
"The resulting system achieves an F-measure of 69.5 and 63.4 on the MUC-6 and MUC-7 coreference resolution data sets, respectively, surpassing the perfor-mance of the best MUC-6 and MUC-7 coreference systems."
"In particular, the system outperforms the best-performing learning-based coreference system to date."
Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a doc-ument.
"Machine learning approaches to this prob-lem have been reasonably successful, operating pri-marily by recasting the problem as a classification task (e.g.[REF_CITE],[REF_CITE],[REF_CITE])."
"Specifically, an inductive learning algorithm is used to train a classi-fier that decides whether or not two NPs in a docu- ment are coreferent."
"Training data are typically cre-ated by relying on coreference chains from the train-ing documents: training instances are generated by pairing each NP with each of its preceding NPs; in-stances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise. [Footnote_1]"
1 Two NPs are in the same coreference chain if and only if they are coreferent.
A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference clas-sification decisions and constructs a partition on the set of NPs with one cluster for each set of corefer-ent NPs.
"Although, in principle, any clustering algo-rithm can be used, most previous work uses a single-link clustering algorithm to impose coreference par-titions. [Footnote_2]"
"2 One exception is Kehler’s work on probabilistic corefer-ence[REF_CITE], in which he applies Dempster’s Rule of Combinati[REF_CITE]to combine all pairwise proba-bilities of coreference to form a partition."
"An implicit assumption in the choice of the single-link clustering algorithm is that coreference resolution is viewed as anaphora resolution, i.e. the goal during clustering is to find an antecedent for each anaphoric NP in a document. [Footnote_3]"
"3 In this paper, we consider an NP anaphoric if it is part of a coreference chain but is not the head of the chain."
"Three intrinsic properties of coreference [Footnote_4] , how-ever, make the formulation of the problem as a classification-based single-link clustering task po-tentially undesirable: Coreference is a rare relation."
"4 Here, we use the term coreference loosely to refer to either the problem or the binary relation defined on a set of NPs. The particular choice should be clear from the context."
"That is, most NP pairs in a document are not coreferent."
"Con- sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the num-ber of positive instances is overwhelmed by the number of negative instances."
"For example, the stan-dard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances."
"Un-fortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g.[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE])."
Coreference is a discourse-level problem with dif-ferent solutions for different types of NPs.
"The interpretation of a pronoun, for example, may be de-pendent only on its closest antecedent and not on the rest of the members of the same coreference chain."
"Proper name resolution, on the other hand, may be better served by ignoring locality constraints alto-gether and relying on string-matching or more so-phisticated aliasing techniques."
"Consequently, gen-erating positive instances from all pairs of NPs from the same coreference chain can potentially make the learning task harder: all but a few coreference links derived from any chain might be hard to identify based on the available contextual cues."
Coreference is an equivalence relation.
Recast-ing the problem as a classification task precludes en-forcement of the transitivity constraint.
"After train-ing, for example, the classifier might determine that A is coreferent with B, and B with C, but that A and C are not coreferent."
"Hence, the clustering mecha-nism is needed to coordinate these possibly contra-dictory pairwise classifications."
"In addition, because the coreference classifiers are trained independent of the clustering algorithm to be used, improvements in classification accuracy do not guarantee correspond-ing improvements in clustering-level accuracy, i.e. overall performance on the coreference resolution task might not improve."
This paper examines each of the above issues.
"First, to address the problem of skewed class dis-tributions, we apply a technique for negative in-stance selection similar to that proposed[REF_CITE]."
"In contrast to results reported there, how-ever, we show empirically that system performance increases noticeably in response to negative example selection, with increases in F-measure of 3-[Footnote_5]%."
"5 By definition, exactly valid instances can be created from NPs in a given document."
"Second, in an attempt to avoid the inclusion of “hard” training instances, we present a corpus-based method for implicit selection of positive instances."
The approach is a fully automated variant of the ex-ample selection algorithm introduced[REF_CITE].
"With positive example selection, sys-tem performance (F-measure) again increases, by 12-14%."
"Finally, to more tightly tie the classification- and clustering-level coreference decisions, we propose an error-driven rule pruning algorithm that opti-mizes the coreference classifier ruleset with respect to the clustering-level coreference scoring function."
"Overall, the use of pruning boosts system perfor-mance from an F-measure of 69.3 to 69.5, and from 57.2 to 63.4 for the MUC-6 and MUC-7 data sets, respectively, enabling the system to achieve perfor-mance that surpasses that of the best MUC corefer-ence systems by 4.6% and 1.6%."
"In particular, the system outperforms the best-performing learning-based coreference system[REF_CITE]by 6.9% and 3.0%."
The remainder of the paper is organized as fol-lows.
"In sections 2 and 3, we present the machine learning framework underlying the baseline corefer-ence system and examine the effect of negative sam-ple selection."
Section 4 presents our corpus-based algorithm for selection of positive instances.
Section 5 describes and evaluates the error-driven pruning algorithm.
We conclude with future work in section 6.
"Our machine learning framework for coreference resolution is a standard combination of classification and clustering, as described above."
Creating an instance.
An instance in our machine learning framework is a description of two NPs in a document.
"More formally, let NP  be the th NP in document ."
An instance formed from NP and NP is denoted by  .
A valid instance is an instance     such that NP precedes NP . 5
"Following previous work ([REF_CITE],"
"Each instance consists of 25 features, which are described in Table 1. [Footnote_6]"
6[REF_CITE]for a detailed description of the features.
The clas-sification associated with a training instance is one of COREFERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated train-ing text. [Footnote_7]
"7 In all of the work presented here, NPs are identified, and feature values computed entirely automatically."
Building an NP coreference classifier.
"We use RIPPER[REF_CITE], an information gain-based propositional rule learning system, to train a classi-fier that, given a test instance   , decides whether or not NP and NP are coreferent."
"Specifi-cally, RIPPER sequentially covers the positive train-ing instances and induces a ruleset that determines when two NPs are coreferent."
"When none of the rules in the ruleset is applicable to a given NP pair, a default rule that classifies the pair as not coreferent is automatically invoked."
The output of the classifier is either COREFERENT or NOT COREFERENT along with a number between 0 and 1 that indicates the confidence of the classification.
Applying the classifier to create coreference chains.
"After training, the resulting ruleset is used by a best-first clustering algorithm to impose a par-titioning on all NPs in the test texts, creating one cluster for each set of coreferent NPs."
Texts are processed from left to right.
"Each NP encountered, NP , is compared in turn to each preceding NP, NP , from right to left."
"For each pair, a test instance is created as during training and is presented to the coreference classifier."
"The NP with the highest con-fidence value among the preceding NPs that are clas-sified as being coreferent with NP is selected as the antecedent of NP ; otherwise, no antecedent is se-lected for NP ."
"As noted above, skewed class distributions arise when generating all valid instances from the train-ing texts."
"A number of methods for handling skewed distributions have been proposed in the machine learning literature, most of which modify the learn- ing algorithm to incorporate a loss function with a much larger penalty for minority class errors than for instances from the majority classes (e.g.[REF_CITE],[REF_CITE])."
"We investigate here a different approach to handling skewed class distributions — negative sample selection, i.e. the selection of a smaller subset of negative instances from the set of available negative instances."
"In the case of NP coreference, we hypothesize that reduc-ing the number of negative instances will improve recall but potentially reduce precision: intuitively, the existence of fewer negative instances should al-low RIPPER to more liberally induce positive rules."
"We propose a method for negative sample selection that, for each anaphoric NP, NP , retains only those negative instances for non-coreferent NPs that lie between NP and its farthest preceding antecedent, ( ( NP )."
"The algorithm for negative sample selec-tion, NEG-SELECT, is shown in Figure 1."
"NEG-SELECT takes as input the set of all possible neg-ative instances in the training texts, i.e. the set of valid instances ! & quot;#$% such that NP and NP are not in the same coreference chain."
The intuition behind this approach is very simple.
"Let - ( NP ) be the set of preceding antecedents of NP , and . ( NP , NP ) be the set consisting of NPs NP ,[REF_CITE]/ , 3%343 , NP ."
"Recall that the goal dur-ing clustering is to compute, for each NP NP , the set - ( NP ) from which the element with the high-est confidence is selected as the antecedent of NP ."
"Since (1) - ( NP ) is a subset of . ( ( ( NP ), NP ) [Footnote_8] and (2) NP is compared to each preceding NP from right to left by the clustering algorithm, it follows that the set of negative instances whose classifica-tions the classifier needs to determine in order to compute 7 - ( NP ) is a superset of the set of instances ( NP ) formed by pairing NP with each of its non-coreferent preceding 7 NPs in . ( ( ( NP ), NP )."
"8 We define 5 ( 6 ( NP ), NP ) to be the empty set if 6 ( NP ) does not exist (i.e. NP is not anaphoric)."
"Con-sequently, ( NP ) is the minimal set of (nega-tive) instances whose classifications will be required during clustering."
"In principle, to perform the classi-fications accurately, the classifier needs to be trained on the corresponding set of negative 7 instances from the training set, which is ( NP ), where NP is now the 8 th NP in training document ."
NEG-SELECT is designed essentially to compute this set.
"Next, we examine the effects of this minimalist ap-proach to negative sample selection."
"We evaluate the coreference system with negative sample selection on the MUC-6 and MUC-7 coreference data sets in each case, train-ing the coreference classifier on the 30 “dry run” texts, and applying the coreference resolution algo-rithm on the 20–30 “formal evaluation” texts."
"Re-sults are shown in rows 1 and 2 of Table 2 where performance is reported in terms of recall, precision, and F-measure using the model-theoretic MUC scor-ing program[REF_CITE]."
"The Baseline sys-tem employs no sample selection, i.e. all available training examples are used."
Row 2 shows the per-formance of the Baseline after incorporating NEG-SELECT.
"With negative sample selection, the per-centage of positive instances rises from 2% to 8% for the MUC-6 data set and from 2% to 7% for the MUC-7 data set."
"For both data sets, we see statis-tically significant increases in recall and statistically significant, but much larger drops in precision. 9 The resulting F-measure scores, however, increase non-trivially from 52.4 to 55.2 (for MUC-6), and from 41.3 to 46.0 (for MUC-7). 10"
"Since not all of the coreference relationships de-rived from coreference chains are equally easy to identify, training a classifier using all possible coref-erence relationships can potentially lead to the in-duction of inaccurate rules."
"Given the observa-tion that one antecedent is sufficient to resolve an anaphor, it may be desirable to learn only from easy positive instances."
"Similar observations are made[REF_CITE], who point out that in-telligent selection of positive instances can poten-tially minimize the amount of knowledge required to perform coreference resolution accurately."
They assume that the easiest types of coreference rela-tionships to resolve are those that occur with high frequencies in the data.
"Consequently, they mine by hand three sets of coreference rules for cov-ering positive instances from the training data by finding the coreference knowledge satisfied by the largest number of anaphor-antecedent pairs."
"While the Harabagiu et al. algorithm attempts to mine easy coreference rules from the data by hand, nei-ther the rule creation process nor stopping condi-tions are precisely defined."
"In addition, a lot of human intervention is required to derive the rules."
"In this section, we describe an automatic positive sample selection algorithm that coarsely mimics the Harabagiu et al. algorithm by finding a confident an-tecedent for each anaphor."
"Overall, our goal is to avoid the inclusion of hard training instances by au-tomating the process of deriving easy coreference rules from the data."
"The positive sample selection al-gorithm, POS-SELECT, is shown in Figure 2."
"It as-sumes the existence of a rule learner, L, that pro-duces an ordered set of positive rules."
POS-SELECT first uses L to induce a ruleset on the training in-stances and picks the first rule from the ruleset.
"For any training instance &quot; $   correctly covered by this rule, an antecedent NP has been identi-fied for the anaphor NP ."
"As a result, all (positive and negative) training instances formed with NP as the anaphor are no longer needed and are sub-sequently removed from the training data. [Footnote_11] The process is repeated until L cannot induce a rule to cover the remaining positive instances."
"11 We speculate that retaining the negative instances would hurt performance, but this remains to be verified."
The output of POS-SELECT is a set of positive rules selected during each iteration of the algorithm.
"Hence, posi-tive sample selection in POS-SELECT is implicit in the sense that it is embedded within the rule induc-tion process."
Results are shown in rows 3 and 4 of Table 2.
"As in the previous experiments, the rule learner is RIPPER."
"We run the system twice, first with POS-SELECT only and then with both POS-SELECT and NEG-SELECT."
"With POS-SELECT only, the system achieves an F-measure of 64.1 (for MUC-6) and 53.8 (for MUC-7)."
"When POS-SELECT and NEG-SELECT are used in combina-tion, however, the system achieves an F-measure of 69.3 (for MUC-6) and 57.2 (for MUC-7)."
The experimental results are largely consistent with our hypothesis.
System performance improves dramatically with positive sample selec-tion using POS-SELECT both in the absence and presence of negative sample selection.
"Without neg-ative sample selection, F-measure increases from 52.4 to 64.1 (for MUC-6), and from 41.3 to 53.8 (for MUC-7)."
"Similarly, with negative sample selection, F-measure increases from 55.2 to 69.3 (for MUC-6), and from 46.0 to 57.2 (for MUC-7)."
"In addition, our results indicate that applying both negative and pos-itive sample selection leads to better performance than applying positive sample selection alone: F-measure increases from 64.1 to 69.3, and from 53.8 to 57.2 for the MUC-6 and MUC-7 data sets, respec-tively."
"Nevertheless, reducing the number of neg-ative instances (via negative sample selection) im-proves recall but damages precision: we see sta-tistically significant gains in recall and statistically significant drops in precision for both data sets."
"In particular, precision drops precipitously from 78.0 to 55.1 for the MUC-7 data set."
We hypothesize that POS-SELECT does not guarantee that hard pos-itive instances will be avoided and that the inclu-sion of these hard instances is responsible for the poorer precision of the system.
Anaphors that do not have easy antecedents can never be removed auto-matically via the induction of new rules using POS-SELECT.
"In fact, RIPPER will possibly induce rules to handle these hard instances as long as such kind of anaphors occur sufficiently frequently in the data set relative to the number of negative instances. [Footnote_12] Al-though it might be beneficial to acquire these rules at the classification level (according to the learning algorithm), they can be detrimental to system per-formance at the clustering level, especially if the rules cover a large number of examples with a lot of exceptions."
"12 More precisely, RIPPER will induce a new rule if the rule is more than 50% accurate and the resulting description length is fewer than 64 bits larger than the smallest description length obtained so far."
"Consequently, it is necessary to know which rules are worthy of keeping at the clustering level and not the classification level."
We will address this issue in the next section.
"As noted in the introduction, machine learning ap-proaches to coreference resolution that rely only on pairwise NP coreference classifiers will not neces-sarily enforce the transitivity constraint inherent in the coreference relation."
"Although approaches to coreference resolution that rely only on clustering could easily enforce transitivity (as[REF_CITE]), they have not performed as well as state-of-the-art approaches to coreference."
"In this section, we propose a method for resolving this con-flict: we introduce an error-driven rule pruning al-gorithm that considers rules induced by the coref-erence classifier and discards those that cause the ruleset to perform poorly with respect to the global, clustering-level coreference scoring function."
The error-driven pruning algo-rithm is inspired by the backward elimination al-gorithm commonly used for feature selection (see[REF_CITE]) and is shown in Figure 3.
"The algorithm, RULE-SELECT, takes as input a ruleset learned from a training corpus for perform-ing coreference resolution, a pruning corpus (dis-joint from the training corpus), and a clustering-level r := the rule in R whose removal yields a ruleset with which the coreference system achieves the best score b on P w.r.t."
S. if b E BestScore then BestScore A@ = b; coreference scoring function that is the same as the one being used for evaluating the final output of the system. [Footnote_13]
"13 Importantly, RULE-SELECT assumes no knowledge of the inner workings of the scoring function."
"At each iteration, RULE-SELECT greed-ily discards the rule whose removal yields a rule-set with which the coreference system performs the best (with respect to the coreference scoring func-tion) on the pruning corpus."
"As a hill-climbing pro-cedure, the algorithm terminates when removal of any of the rules in the ruleset fails to improve per-formance."
"In contrast to most existing algorithms for coreference resolution, RULE-SELECT establishes a tighter connection between the classification- and clustering-level decisions for coreference resolution and ensures that system performance is optimized with respect to the coreference scoring function."
"We hypothesize that this optimization of the coreference classifier will improve performance of the resulting coreference system, in particular by increasing its precision."
Evaluation and Discussion.
Results are shown in row 5 of Table 2.
"In the Pruning experiment, the MUC-7 formal evaluation corpus is the pruning cor-pus for the MUC-6 run; the MUC-6 formal evalu-ation corpus is the pruning corpus for the MUC-7 run."
"In addition, the quantity that RULE-SELECT optimizes for a given ruleset is the F-measure re-turned by the MUC scoring function. [Footnote_14]"
14 RULE-SELECT can be used in conjunction with any coref-erence scoring function. The MUC scorer is chosen here to fa-cilitate comparison with previous results.
"In compar-ison to the Combined results, we see an improve-ment of 0.2% (for MUC-6) and 6.2% (for MUC-7) in F-measure."
"In particular, we see statistically sig-nificant gains in precision (from 55.1 to 73.6) and statistically significant, but much smaller, drops in recall (from 59.5 to 54.2) for the MUC-7 data set."
"In general, our results support the hypothesis that rule pruning can be used to improve system perfor-mance; moreover, the technique is especially effec-tive at enhancing the precision of the system."
"How-ever, performance gains may be negligible when pruning is used in systems with high precision, as can be seen from the results for the MUC-6 data set."
"To determine whether performance improvements are instead attributable to the availability of addi-tional “training” data provided by the pruning cor-pus, we train a classifier (using the same setting as the Combined experiments) on both the training and the pruning corpora."
The performance of the system using this unpruned ruleset is shown in the last row of Table 2.
"In comparison to the Combined results, F-measure drops from 69.3 to 67.6 (for MUC-6), and rises from 57.2 and 57.8 (for MUC-7)."
These re-sults indicate that the RULE-SELECT algorithm has made a more effective use of the additional data than the learning algorithm without rule pruning by ex-ploiting the feedback provided by the scoring func-tion.
We have examined three problems with recasting noun phrase coreference resolution as a classifica-tion task.
"To handle these problems, we presented a minimalist negative sample selection algorithm to reduce the skewness of the class distributions, and an automatic positive sample selection algorithm to select easy positive instances."
"In addition, our ex-periments indicate that the positive sample selection algorithm does not guarantee that hard instances can be entirely excluded."
"As a result, we proposed an error-driven rule pruning algorithm that can effec-tively enhance the precision of the system by dis- carding rules that cause the ruleset to perform poorly with respect to the coreference scoring function."
The resulting system outperformed the best MUC-6 and MUC-7 coreference systems as well as the best-performing learning-based system on the cor-responding MUC data sets.
"Nevertheless, there is substantial room for improvement."
"For example, it is important to know how sensitive system perfor-mance is with respect to the size of the pruning cor-pus."
"In addition, although we use RIPPER as the un-derlying learning algorithm in our coreference sys-tem, we expect that the techniques described in this paper can be used in conjunction with other learn-ing algorithms."
We plan to explore this possibility in future work.
This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs.
To estimate the parameters is to discover a notion of relatedness among context-free rules such that related rules tend to have related probabilities.
The prior favors grammars in which the relationships are simple to describe and have few major excep-tions.
A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank (20% reduction of rule perplexity over the best previous method).
This paper uses a new kind of statistical model to smooth the probabilities of PCFG rules.
It focuses on “flat” or “dependency-style” rules.
"These resemble subcategoriza-tion frames, but include adjuncts as well as arguments."
"The verb put typically generates 3 dependents—a subject NP at left, and an object NP and goal PP at right: • S → NP put NP PP: Jim put [the pizza] [in the oven] But put may also take other dependents, in other rules: • S → NP Adv put NP PP: Jim often put [a pizza] [in the oven] • S → NP put NP PP PP: Jim put soup [in an oven] [at home] • S → NP put NP: Jim put [some shares of IBM stock] • S → NP put Prt NP: Jim put away [the sauce] • S → TO put NP PP: to put [the pizza] [in the oven] • S → NP put NP PP SBAR:"
Jim put it [to me] [that . . . ]
"These other rules arise if put can add, drop, reorder, or retype its dependents."
These edit operations on rules are semantically motivated and quite common (Table 1).
"We wish to learn contextual probabilities for the edit operations, based on an observed sample of flat rules."
"In English we should discover, for example, that it is quite common to add or delete PP at the right edge of a rule."
These contextual edit probabilities will help us guess the true probabilities of novel or little-observed rules.
"However, rules are often idiosyncratic."
Our smooth-ing method should not keep us from noticing (given enough evidence) that put takes a PP more often than most verbs.
Hence this paper’s proposal is a Bayesian smoothing method that allows idiosyncrasy in the gram-mar while presuming regularity to be more likely a priori.
The model will assign a positive probability to each of the infinitely many formally possible rules.
"The fol-lowing bizarre rule is not observed in training, and seems very unlikely."
"But there is no formal reason to rule it out, and it might help us parse an unlikely test sentence."
So the model will allow it some tiny probability: • S → NP Adv PP put PP PP PP NP AdjP S
A PCFG is a conditional probability function p(RHS | LHS). [Footnote_1]
"1 Nonstandardly, this allows infinitely many rules with p&gt;0."
"For example, p(V NP PP | VP) gives the proba-bility of the rule VP → V NP PP."
"With lexicalized non-terminals, it has form p(V put NP pizza PP in | VP put )."
Usually one makes an independence assumption and defines this as p(V put NP PP | VP put ) times factors that choose dependent headwords pizza and in according to the selectional preferences of put.
"This paper is about estimating the first factor, p(V put NP PP | VP put )."
"In supervised learning, it is simplest to use a max-imum likelihood estimate (perhaps with backoff from put)."
"However, there are four reasons not to use a Treebank grammar."
"First, ignoring unseen rules necessarily sacri-fices some accuracy."
"Second, we will show that it im-proves accuracy to flatten the parse trees and use flat, dependency-style rules like p(NP put NP PP | S put ); this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable."
"Third, backing off from the word is a crude technique that does not distinguish among words. [Footnote_2] Fourth, one would eventually like to re-duce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars."
"2 One might do better by backing off to word clusters, which[REF_CITE]did find provided a small benefit."
"To smooth the distribution p(RHS | LHS), one can de-fine it in terms of a set of parameters and then estimate those parameters."
Most researchers have used an n-gram model[REF_CITE]or more general Markov model[REF_CITE]to model the sequence of nonterminals in the RHS.
The sequence V put NP PP in our example is then assumed to be emitted by some Markov model of VP put rules (again with backoff from put).
"Collins (1997, model [Footnote_2]) uses a more sophisticated model in which all arguments in this sequence are gener-ated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments."
"2 One might do better by backing off to word clusters, which[REF_CITE]did find provided a small benefit."
"While Treebank models overfit the training data, Markov models underfit."
"A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a Markov."
"Like this paper’s main proposal, it can learn well-observed id-iosyncratic rules but generalizes when data are sparse. [Footnote_3]"
3[REF_CITE]used a similar hybrid technique
"These models are beaten by our rather different model, transformational smoothing, which learns common rules and common edits to them."
"The comparison is a direct one, based on the perplexity or cross-entropy of the trained models on a test set of S → · · · rules. [Footnote_4]"
"4 All the methods evaluated here apply also to full PCFGs, but verb-headed rules S → · · · present the most varied, inter-esting cases. Many researchers have tried to learn verb subcat-egorization, though usually not probabilistic subcategorization."
A subtlety is that two annotation styles are possible.
"In the Penn Treebank, put is the head of three constituents (V, VP, and S, where underlining denotes a head child) and joins with different dependents at different levels: • [ S [ NP Jim] [ VP [ V put] [ NP pizza] [ PP in the oven]]]"
"In the flattened or dependency version that we prefer, each word joins with all of its dependents at once: • [ S [ NP Jim] put [ NP pizza] [ PP in the oven]]"
A PCFG generating the flat structure must estimate p(NP put NP PP | S put ).
"A non-flat PCFG adds the dependents of put in 3 independent steps, so in ef-fect it factors the flat rule’s probability into 3 suppos-edly independent “subrule probabilities,” p(NP VP put | S put ) · p(V put NP PP | VP put ) · p(put | V put )."
Our evaluation judges the estimates of flat-rule prob-abilities.
"Is it better to estimate these directly, or as a product of estimated subrule probabilities? [Footnote_5] Transforma-tional smoothing is best applied to the former, so that the edit operations can freely rearrange all of a word’s depen-dents."
"5 In testing the latter case, we sum over all possible internal bracketings of the rule. We do train this case on the true internal bracketing, but it loses even with this unfair advantage."
We will see that the Markov and Treebank/Markov models also work much better this way—a useful finding.
"This section outlines the Bayesian approach to learning probabilistic grammars (for us, estimating a distribution over flat CFG rules)."
"By choosing among the many grammars that could have generated the training data, the learner is choosing how to generalize to novel sentences."
"To guide the learner’s choice, one can explicitly spec-ify a prior probability distribution p(θ) over possible grammars θ, which themselves specify probability dis-tributions over strings, rules, or trees."
"A learner should seek θ that maximizes p(θ) · p(D | θ), where D is the set of strings, rules, or trees observed by the learner."
"The first factor favors regularity (“pick an a priori plausible grammar”), while the second favors fitting the idiosyn-crasies of the data, especially the commonest data. [Footnote_6]"
6 This approach is called semi-Bayesian or Maximum A Pos-
"Priors can help both unsupervised and supervised learning. (In the semi-supervised experiments here, train-ing data is not raw text but a sparse sample of flat rules.)"
Indeed a good deal of syntax induction work has been carried out in just this framework[REF_CITE].
"However, all such work to date has adopted rather simple prior distributions."
"Typically, it has defined p(θ) to favor PCFGs whose rules are few, short, nearly equiprobable, and defined over a small set of nonterminals."
"Such definitions are conve-nient, especially when specifying an encoding for MDL, but since they treat all rules alike, they may not be good descriptions of linguistic plausibility."
"For example, they will never penalize the absence of a predictable rule."
"A prior distribution can, however, be used to encode various kinds of linguistic notions."
"After all, a prior is really a soft form of Universal Grammar: it gives the learner enough prior knowledge of grammar to overcome Chomsky’s “poverty of the stimulus” (i.e., sparse data). • A preference for small or simple grammars, as above. • Substantive preferences, such as a preference for verbs to take 2 nominal arguments, or to allow PP adjuncts. • Preferences for systematicity, such as a preference for the rules to be consistently head-initial or head-final."
This paper shows how to design a prior that favors a certain kind of systematicity.
Lexicalized grammars for natural languages are very large—each word specifies a distribution over all possible dependency rules it could head—but they tend to have internal structure.
"The new prior prefers grammars in which a rule’s probability can be well-predicted from the probabilities of other rules, us-ing linguistic transformations such as edit operations."
"For example, p(NP Adv w put NP PP | S w ) cor-relates with p(NP w NP PP | S w )."
"Both numbers are high for w = put, medium for w = fund, and low for w = sleep."
The slope of the regression line has to do with the rate of preverbal Adv-insertion in English.
"The correlation is not perfect (some verbs are espe-cially prone to adverbial modification), which is why we will only model it with a prior."
"To just the extent that evi-dence about w is sparse, the prior will cause the learner to smooth the two probabilities toward the regression line."
"Before spelling out our approach, let us do a sanity check."
"A frame is a flat rule whose headword is replaced with teriori learning, since it is equivalent to maximizing p(θ | D)."
It is also equivalent to Minimum Description
"Length (MDL) learning, which minimizes the total number of bits `(θ)+`(D | θ) needed to encode grammar and data, because one can choose an encoding scheme where `(x) = − log 2 p(x), or conversely, define probability distributions by p(x) = 2 −`(x) . the variable “ ” (corresponding to w above)."
"Table 1 il-lustrates that in the Penn Treebank, if frequent rules with frame α imply matching rules with frame β, there are usually edit operations (section 1) to easily turn α into β."
"How about rare rules, whose probabilities are most in need of smoothing?"
Are the same edit transformations that we can learn from frequent cases (Table 1) appropri-ate for predicting the rare cases?
The very rarity of these rules makes it impossible to create a table like Table 1.
"However, rare rules can be measured in the aggregate, and the result suggests that the same kinds of transforma-tions are indeed useful—perhaps even more useful—in predicting them."
"Let us consider the set R of 2,809,545 possible flat rules that stand at edit distance 1 from the set of S → · · · rules observed in our English training data."
"That is, a rule such as S put → NP put NP is in R if it did not appear in training data itself, but could be derived by a single edit from some rule that did appear."
"A bigram Markov model (section 2) was used to iden-tify 2,714,763 rare rules in R—those that were predicted to occur with probability &lt; 0.0001 given their head-words. 79 of these rare rules actually appeared in a development-data set of 1423 rules."
"The bigram model would have expected only 26.2 appearances, given the lexical headwords in the test data set."
"The difference is statistically significant (p &lt; 0.001, bootstrap test)."
"In other words, the bigram model underpredicts the edit-distance “neighbors” of observed rules by a factor of 3. [Footnote_7] One can therefore hope to use the edit transforma-tions to improve on the bigram model."
"7 Similar results are obtained when we examine just one par-ticular kind of edit operation, or rules of one particular length."
"For example, the"
"Delete Y transformation recognizes that if · · · X Y Z · · · has been observed, then · · · X Z · · · is plausible even if the bigram X Z has not previously been observed."
"Presumably, edit operations are common because they modify a rule in semantically useful ways, allowing the filler of a semantic role to be expressed (Insert), sup-pressed (Delete), retyped (Substitute), or heavy-shifted (Swap)."
Such “valency-affecting operations” have re-peatedly been invoked by linguists; they are not confined to English. [Footnote_8]
"8[REF_CITE]writes that whenever linguists run into the problem of systematic redundancy in the syntactic lexicon, they design a scheme in which lexical entries can be derived from one another by just these operations. We are doing the same thing. The only twist that the lexical entries (in our case, flat PCFG rules) have probabilities that must also be derived, so we will assume that the speaker applies these operations (randomly from the hearer’s viewpoint) at various rates to be learned."
So a learner of an unknown language can reasonably expect a priori that flat rules related by edit operations may have related probabilities.
"However, which edit operations varies by language."
"Each language defines its own weighted, contextual, asymmetric edit distance."
So the learner will have to dis-cover how likely particular edits are in particular con-texts.
"For example, it must learn the rates of prever-bal Adv-insertion and right-edge PP-insertion."
Evidence about these rates comes mainly from the frequent rules.
The form of our new model is shown in Figure 1.
"The vertices are flat context-free rules, and the arcs between them represent edit transformations."
The set of arcs leav- ing any given vertex has total probability 1.
The learner’s job is to discover the probabilities.
"Fortunately, the learner does not have to learn a sep-arate probability for each of the (infinitely) many arcs, since many of the arcs represent identical or similar edits."
"As shown in Figure 1, an arc’s probability is determined from meaningful features of the arc, using a conditional log-linear model of p(arc | source vertex)."
The learner only has to learn the finite vector θ of feature weights.
"Arcs that represent similar transformations have similar features, so they tend to have similar probabilities."
This transformation model is really a PCFG with un-usual parameterization.
"That is, for any value of θ, it defines a language-specific probability distribution over all possible context-free rules (graph vertices)."
"To sam-ple from this distribution, take a random walk from the special vertex S TART to the special vertex H ALT ."
The rule at the last vertex reached before H ALT is the sample.
This sampling procedure models a process where the speaker chooses an initial rule and edits it repeatedly.
The random walk might reach S fund → To fund NP in two steps and simply halt there.
This happens with probability 0.0011 · expZ θ 1 · expZ θ 0 .
"Or, having 1 2 arrived at S fund → To fund NP, it might transform it into S fund → To fund PP NP and then further to S fund → To fund NP PP before halting."
"Thus, p θ (S fund → To fund NP PP) denotes the probability that the random walk somehow reaches S fund → To fund NP PP and halts there."
"Condi-tionalizing this probability gives p θ (To NP PP | S fund ), as needed for the PCFG. 9"
"Given θ, it is nontrivial to solve for the probability dis-tribution over grammar rules e. Let I θ (e) denote the flow to vertex e."
"This is defined to be the total probability of all paths from S TART to e. Equivalently, it is the expected number of times e would be visited by a random walk from S TART ."
The following recurrence defines p θ (e): [Footnote_10]
"10 Where δ x,y = 1 if x = y, else δ x,y = 0."
"I θ (e) = δ e,S TART + P e 0"
I θ (e 0 ) · p(e 0 → e) (1) p θ (e) =
I θ (e) · p(e →
"H ALT ) (2) Since solving the large linear system (1) would be pro-hibitively expensive, in practice we use an approximate relaxation algorithm[REF_CITE]that propagates flow through the graph until near-convergence."
In general this may underestimate the true probabilities somewhat.
"Now consider how the parameter vector θ affects the distribution over rules, p θ (e), in Figure 1: • By raising the initial weight θ 1 , one can increase the flow to S fund → To fund NP, S merge → To merge NP, and the like."
"By equa-tion (2), this also increases the probability of these rules."
"But the effect also feeds through the graph to increase the flow and probability at those rules’ descendants in the graph, such as S merge → To merge NP PP."
"So a single parameter θ 1 controls a whole complex of rule probabilities (roughly speaking, the infinitival transi-tives)."
"The model thereby captures the fact that, although rules are mutually exclusive events whose probabilities sum to 1, transformationally related rules have positively correlated probabilities that rise and fall together. • The exception weight θ [Footnote_9] appears on all and only the arcs to S merge → To merge NP PP."
9 The experiments of this paper do not allow transformations
"That rule has even higher probability than predicted by PP-insertion as above (since merge, unlike fund, actually tends to sub-categorize for PP with )."
"To model its idiosyncratic prob-ability, one can raise θ 9 ."
This “lists” the rule specially in the grammar.
"Rules derived from it also increase in probability (e.g., S merge → To Adv merge NP PP), since again the effect feeds through the graph. • The generalization weight θ 3 models the strength of the PP-insertion relationship."
"Equations (1) and (2) im-ply that p θ (S fund → To fund NP PP) is modeled as a linear combination of the probabilities of that rule’s parents in the graph. θ 3 controls the coefficient of p θ (S fund → To fund NP) in this linear combination, with the coefficient approaching zero as θ 3 → −∞. • Narrower generalization weights such as θ 4 and θ 5 control where PP is likely to be inserted."
To learn the feature weights is to learn which features of a transfor-mation make it probable or improbable in the language.
"Note that the vertex labels, graph topology, and arc parameters are language independent."
"That is, Figure 1 is supposed to represent Universal Grammar: it tells a learner what kinds of generalizations to look for."
"The language-specific part is θ, which specifies which gener-alizations and exceptions help to model the data."
The model has more parameters than data.
"Beyond the initial weights and generalization weights, in practice we allow one exception weight (e.g., θ 8 , θ 9 ) for each rule that appeared in training data. (This makes it possible to learn arbitrary exceptions, as in a Treebank grammar.)"
"Parameter estimation is nonetheless possible, using a prior to help choose among the many values of θ that do a reasonable job of explaining the training data."
"The prior constrains the degrees of freedom: while many parame-ters are available in principle, the prior will ensure that the data are described using as few of them as possible."
"The point of reparameterizing a PCFG in terms of θ, as in Figure 1, is precisely that only one parameter is needed per linguistically salient property of the PCFG."
Making θ 3 &gt; 0 creates a broadly targeted transforma-tion.
"Making θ 9 6= 0 or θ 1 6= 0 lists an idiosyncratic rule, or class of rules, together with other rules derived from them."
"But it takes more parameters to encode less sys-tematic properties, such as narrowly targeted edit trans-formations (θ 4 , θ 5 ) or families of unrelated exceptions."
A natural prior for the parameter vector θ ∈ R k is therefore specified in terms of a variance σ 2 .
"We simply say that the weights θ 1 , θ 2 , . . . θ k are independent sam-ples from the normal distribution with mean 0 and vari-ance σ 2 &gt; 0[REF_CITE]:"
"Θ ∼ N(0, σ 2 ) × N(0, σ 2 ) × · · · × N(0, σ 2 ) (3) or equivalently, that θ is drawn from a multivariate Gaus-sian with mean ~0 and diagonal covariance matrix σ 2"
"I, i.e., Θ ∼ N(~0, σ 2 I)."
"This says that a priori, the learner expects most fea-tures in Figure 1 to have weights close to zero, i.e., to be irrelevant."
Maximizing p(θ) · p(D | θ) means finding a relatively small set of features that adequately describe the rules and exceptions of the grammar.
Reducing the variance σ 2 strengthens this bias toward simplicity.
"For example, if S fund → To fund NP PP and S merge → To fund NP PP are both observed more often than the current p θ distribution predicts, then the learner can follow either (or both) of two strategies: raise θ 8 and θ 9 , or raise θ 3 ."
The former strategy fits the training data only; the latter affects many disparate arcs and leads to generalization.
The latter strategy may harm p(D | θ) but is preferred by the prior p(θ) because it uses one pa-rameter instead of two.
"If more than two words act like merge and fund, the pressure to generalize is stronger."
"In experiments, we have found that a slight variation on this model gets slightly better results."
"Let θ e denote the exception weight (if any) that allows one to tune the prob-ability of rule e. We eliminate θ e and introduce a different parameter π e , called a perturbation, which is used in the following replacements for equations (1) and (2):"
"I θ (e) = δ e,S TART + X I θ (e 0 ) · exp π e · p(e 0 → e)(4) e 0 p θ (e) ="
I θ (e) · exp π e · p(e →
H ALT )/Z (5) where Z is a global normalizing factor chosen so that P e p θ (e) = 1.
The new prior on π e is the same as the old prior on θ e .
Increasing either θ e or π e will raise p θ (e); the learner may do this to account for observations of e in training data.
The probabilities of other rules consequently de-crease so that P e p θ (e) = 1.
"When π e is raised, all rules’ probabilities are scaled down slightly and equally (because Z increases)."
"When θ e is raised, e steals proba-bility from its siblings, [Footnote_11] but these are similar to e so tend to appear in test data if e is in training data."
"11 Raising the probability of an arc from e 0 to e decreases the probabilities of arcs from e 0 to siblings of e, as they sum to 1."
"Raising θ e without disproportionately harming e’s siblings requires manipulation of many other parameters, which is discour-aged by the prior and may also suffer from search error."
We speculate that this is why π e works better.
"To evaluate the quality of generalization, we used pre-parsed training data D and testing data E (Table 3)."
Each dataset consisted of a collection of flat rules such as S put → NP put NP PP extracted from the Penn Tree-bank[REF_CITE].
"Thus, p(D | θ,π) and p(E | θ, π) were each defined as a product of rule prob-abilities of the form p θ,π (NP put NP PP | S put )."
"The learner attempted to maximize p(θ,π) · p(D | θ,π) by gradient ascent."
This amounts to learning the generalizations and exceptions that related the training rules D.
"The evaluation measure was then the perplex-ity on test data, − log 2 p(E | θ, π)/|E| ."
"To get a good (low) perplexity score, the model had to assign reason-able probabilities to the many novel rules in E (Table 3)."
"For many of these rules, even the frame was novel."
"Note that although the training data was preparsed into rules, it was not annotated with the paths in Figure 1 that generated those rules, so estimating θ and π was still an unsupervised learning problem."
The transformation graph had about 14 features per arc (Table 2).
"In the finite part of the transformation graph that was actually explored (including bad arcs that com-pete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred a Back off from Treebank grammar with Katz vs. one-count back[REF_CITE](Note: One-count was al-ways used for backoff within the n-gram and Collins models.) b See section 2 for discussion c Collins (1997, model 2) d"
"Average of transformation model with best other model feature weights were substantial, and only a few thousand were even far enough from zero to affect performance."
There was also a parameter π e for each observed rule e.
"Results are given in Table 4a, which compares the transformation model to various competing models dis-cussed in section 2."
The best (smallest) perplexities ap-pear in boldface.
"The key results: • The transformation model was the winner, reducing perplexity by 20% over the best model replicated from previous literature (a bigram model). • Much of this improvement could be explained by the transformation model’s ability to model exceptions."
"Adding this ability more directly to the bigram model, using the new Treebank/Markov approach of section 2, also reduced perplexity from the bigram model, by 6% or 14% depending on whether Katz or one-count backoff was used, versus the transformation model’s 20%. • Averaging the transformation model with the best com-peting model (Treebank/bigram) improved it by an addi-tional 6%."
"So using transformations yields a total per-plexity reduction of [Footnote_12]% over Treebank/bigram, and 24% over the best previous model from the literature (bigram). • What would be the cost of achieving such a perplexity improvement by additional annotation?"
"12 See[REF_CITE]for full details of data preparation, model structure, parameter initialization, backoff levels for the comparison models, efficient techniques for computing the ob-jective and its gradient, and more analysis of the results."
"Training the av-eraged model on only the first half of the training set, with no further tuning of any options (Table 4b), yielded a test set perplexity of 118.0."
"So by using transformations, we can achieve about the same perplexity as the best model without transformations (Treebank/bigram, 116.2), using only half as much training data. • Furthermore, comparing Tables 4a and 4b shows that the transformation model had the most graceful perfor-mance degradation when the dataset was reduced in size."
"This is an encouraging result for the use of the method in less supervised contexts (although results on a noisy dataset would be more convincing in this regard). • The competing models from the literature are best used to predict flat rules directly, rather than by summing over their possible non-flat internal structures, as has been done in the past."
This result is significant in itself.
"Ex-tending[REF_CITE], it shows the inappropriateness of the traditional independence assumptions that build up a frame by several rule expansions (section 2)."
"Figure 2 shows that averaging the transformation model with the Treebank/bigram model improves the lat-ter not merely on balance, but across the board."
"In other words, there is no evident class of phenomena for which incorporating transformations would be a bad idea. • Transformations particularly helped raise the estimates of the low-probability novel rules in test data, as hoped. • Transformations also helped on test rules that had been observed once in training with relatively infrequent words. (In other words, the transformation model does not discount singletons too much.) • Transformations hurt slightly on balance for rules ob-served more than once in training, but the effect was tiny."
"All these differences are slightly exaggerated if one com-pares the transformation model directly with the Tree-bank/bigram model, without averaging."
The transformation model was designed to use edit operations in order to generalize appropriately from a word’s observed frames to new frames that are likely to appear with that word in test data.
"To directly test the model’s success at such generalization, we compared it to the bigram model on a pseudo-disambiguation task."
"Each instance of the task consisted of a pair of rules from test data, expressed as (word, frame) pairs (w 1 , f 1 ) and (w 2 , f 2 ), such that f 1 and f 2 are “novel” frames that did not appear in training data (with any headword)."
"Each model was then asked: Does f 1 go with w 1 and f 2 with w 2 , or vice-versa?"
"In other words, which is big-ger, p(f 1 | w 1 ) · p(f 2 | w 2 ) or p(f 2 | w 1 ) · p(f 1 | w 2 )?"
"Since the frames were novel, the model had to make the choice according to whether f 1 or f 2 looked more like the frames that had actually been observed with w 1 in the past, and likewise w 2 ."
What this means depends on the model.
The bigram model takes two frames to look alike if they contain many bigrams in common.
The transformation model takes two frames to look alike if they are connected by a path of probable transformations.
"The test data contained 62 distinct rules (w,f) in which f was a novel frame."
"13 An obvious tie is an instance where f 1 = f 2 , or where both w 1 and w 2 were novel headwords. ([REF_CITE]rules included 11 with novel headwords.) In such cases, neither the bigram nor the transformation model has any basis for making its decision: the probabilities being compared will necessarily be equal."
Baseline performance on this difficult task is 50% (ran-dom guess).
The bigram model chose correctly in 1595 of the 1811 instances (88.1%).
"Parameters for “memo-rizing” specific frames do not help on this task, which in-volves only novel frames, so the Treebank/bigram model had the same performance."
"By contrast, the transforma-tion model got 1669 of 1811 correct (92.2%), for a more-than-34% reduction in error rate. (The development set showed similar results.)"
"However, since the 1811 task instances were derived non-independently from just 62 novel rules, this result is based on a rather small sample."
This paper has presented a nontrivial way to reparameter-ize a PCFG in terms of “deep” parameters representing transformations and exceptions.
A linguistically sensible prior was natural to define over these deep parameters.
Famous examples of “deep reparameterization” are the Fourier transform in speech recognition and the SVD transform for Latent Semantic Analysis in IR.
"Like our technique, they are intended to reveal significant structure through the leading parameters while relegating noise and exceptions to minor parameters."
"Such representations make it easier to model the similarity or probability of the objects at hand (waveforms, documents, or grammars)."
"Beyond the fact that it shows at least a good perplex-ity improvement (it has not yet been applied to a real task), an exciting “big idea” aspect of this work is its flexibility in defining linguistically sensible priors over grammars."
Our reparameterization is made with refer-ence to a user-designed transformation graph (Figure 1).
"The graph need not be confined to edit distance transfor-mations, or to the simple features of Table 2 (used here for comparability with the Markov models), which con-dition a transformation’s probability on local context."
"In principle, the approach could be used to capture a great many linguistic phenomena."
"Figure 1 could be extended with more ambitious transformations, such as gapping, gap-threading, and passivization."
The flat rules could be annotated with internal structure (as in TAG) and thematic roles.
"Finally, the arcs could bear further fea-tures."
"For example, the probability of unaccusative move-ment (someone sank the boat → the boat sank) should de-pend on whether the headword is a change-of-state verb."
"Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism."
The vertices represent lex-ical entries and the arcs represent probabilistic lexical re-dundancy rules or metarules (see footnote 8).
"The trans-formation model approach is therefore a full stochas-tic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although[REF_CITE]give an ad hoc approach."
See[REF_CITE]for more discussion.
It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical mod-els: they allow similar patterns of deductive and abduc-tive inference from observations.
"However, the vertices of a transformation graph do not represent different ran-dom variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional log-linear (maximum entropy) models."
"As an alternative, one could directly build a conditional log-linear model of p(RHS | LHS)."
"However, such a model would learn probabilities, not relationships."
"A feature weight would not really model the strength of the relationship between two frames e, e 0 that share that feature."
It would only in-fluence both frames’ probabilities.
"If the probability of e were altered by some unrelated factor (e.g., an exception weight), then the probability of e 0 would not respond. • A transformation model can be regarded as a proba-bilistic FSA that consists mostly of . (Rules are only emitted on the arcs to H ALT .)"
This perspective allows use of generic methods for finite-state parameter estimati[REF_CITE].
"We are strongly interested in improving the speed of such methods and their ability to avoid local maxima, which are currently the major diffi-culty with our system, as they are for many unsupervised learning techniques."
We expect to further pursue trans-formation models (and simpler variants that are easier to estimate) within this flexible finite-state framework.
"The interested reader is encouraged to look[REF_CITE]for a much more careful and wide-ranging discus-sion of transformation models, their algorithms, and their relation to linguistic theory, statistics, and parsing."
Chap-ter 1 provides a good overview.
"For a brief article high-lighting the connection to linguistics, see[REF_CITE]."
 dj i¡ U¡ UiDW0U ¡ W0`j0j¡ D0&apos;B¡ \WUd£ H¡¤L ¡ U 0dW @j{ ¦0d  j¡ ¡  \0¤U$  ¡ ¯§ (¡ % º0±¼6½B¾@¿0ÀDÁ@J\     Ui%dW  ¡ D
ÀB¾iÈiÃDÁ%Å0ÉiÃBÇ6ÅBÀDÁ@Â`ÆLÀjÊBÃBÇ6ÅBÀDÁ \ %     0  \= ¡  { ¡
"We consider the problem of classifying doc-uments not by topic, but by overall senti-ment, e.g., determining whether a review is positive or negative."
"Using movie re-views as data, we find that standard ma-chine learning techniques definitively out-perform human-produced baselines."
"How-ever, the three machine learning methods we employed (Naive Bayes, maximum en-tropy classification, and support vector ma-chines) do not perform as well on sentiment classification as on traditional topic-based categorization."
We conclude by examining factors that make the sentiment classifica-tion problem more challenging.
"Today, very large amounts of information are avail-able in on-line documents."
"As part of the effort to better organize this information for users, researchers have been actively investigating the problem of au-tomatic text categorization."
"The bulk of such work has focused on topical cat-egorization, attempting to sort documents accord-ing to their subject matter (e.g., sports vs. poli-tics)."
"However, recent years have seen rapid growth in on-line discussion groups and review sites (e.g., the New York Times’ Books web page) where a cru-cial characteristic of the posted articles is their senti-ment, or overall opinion towards the subject matter — for example, whether a product review is pos-itive or negative."
"Labeling these articles with their sentiment would provide succinct summaries to read-ers; indeed, these labels are part of the appeal and value-add of such sites[URL_CITE]which both labels movie reviews that do not con-tain explicit rating indicators and normalizes the different rating schemes that individual reviewers use."
"Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye’s Lexant system [URL_CITE] ) and recommender systems (e.g.,[REF_CITE],[REF_CITE]), where user input and feedback could be quickly summarized; in-deed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization."
"Moreover, there are also potential applications to message filtering; for exam-ple, one might be able to use sentiment information to recognize and discard “flames”[REF_CITE]."
"In this paper, we examine the effectiveness of ap-plying machine learning techniques to the sentiment classification problem."
"A challenging aspect of this problem that seems to distinguish it from traditional topic-based classification is that while topics are of-ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner."
"For example, the sentence “How could anyone sit through this movie?” contains no single word that is obviously negative. (See Section 7 for more examples)."
"Thus, sentiment seems to require more understanding than the usual topic-based classification."
"So, apart from presenting our results obtained via machine learning techniques, we also analyze the problem to gain a better under-standing of how difficult it is."
This section briefly surveys previous work on non-topic-based text categorization.
"One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variati[REF_CITE]serving as an important cue."
"Examples in-clude author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow)[REF_CITE]."
"Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories[REF_CITE]."
Other work explicitly attempts to find features indicating that subjective language is being used[REF_CITE].
"But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opin-ion, they do not address our specific classification task of determining what that opinion actually is."
Most previous research on sentiment-based classi-fication has been at least partially knowledge-based.
"Some of this work focuses on classifying the semantic orientation of individual words or phrases, using lin-guistic heuristics or a pre-selected set of seed words[REF_CITE]."
Past work on sentiment-based cat-egorization of entire documents has often involved either the use of models inspired by cognitive lin-guistics[REF_CITE]or the manual or semi-manual construction of discriminant-word lex-icons[REF_CITE].
"Interestingly, our baseline exper-iments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words."
Turney’s (2002) work on classification of reviews is perhaps the closest to ours. [Footnote_2]
"2 Indeed, although our choice of title was completely independent of his, our selections were eerily similar."
"He applied a spe-cific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mu-tual information is computed using statistics gath-ered by a search engine."
"In contrast, we utilize sev-eral completely prior-knowledge-free supervised ma-chine learning methods, with the goal of understand-ing the inherent difficulty of the task."
"For our experiments, we chose to work with movie reviews."
"This domain is experimentally convenient because there are large on-line collections of such re-views, and because reviewers often summarize their overall sentiment with a machine-extractable rat-ing indicator, such as a number of stars; hence, we did not need to hand-label the data for supervised learning or evaluation purposes."
"We also note th[REF_CITE]found movie reviews to be the most difficult of several domains for sentiment classifica-tion, reporting an accuracy of 65.83% on a 120-document set (random-choice performance: 50%)."
"But we stress that the machine learning methods and features we use are not specific to movie reviews, and should be easily applicable to other domains as long as sufficient training data exists."
Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup. [URL_CITE] We selected only reviews where the au-thor rating was expressed either with stars or some numerical value (other conventions varied too widely to allow for automatic processing).
"Ratings were automatically extracted and converted into one of three categories: positive, negative, or neutral."
"For the work described in this paper, we concentrated only on discriminating between positive and nega-tive sentiment."
"To avoid domination of the corpus by a small number of prolific reviewers, we imposed a limit of fewer than 20 reviews per author per sen-timent category, yielding a corpus of 752 negative and 1301 positive reviews, with a total of 144 re-viewers represented."
This dataset will be available on-line[URL_CITE](the URL contains hyphens only around the word “review”).
Intuitions seem to differ as to the difficulty of the sen-timent detection problem.
An expert on using ma-chine learning for text categorization predicted rela-tively low performance for automatic methods.
"On the other hand, it seems that distinguishing positive from negative reviews is relatively easy for humans, especially in comparison to the standard text catego-rization problem, where topics can be closely related."
"One might also suspect that there are certain words people tend to use to express strong sentiments, so that it might suffice to simply produce a list of such words by introspection and rely on them alone to classify the texts."
"To test this latter hypothesis, we asked two gradu-ate students in computer science to (independently) choose good indicator words for positive and nega-tive sentiments in movie reviews."
"Their selections, shown in Figure 1, seem intuitively plausible."
We then converted their responses into simple decision procedures that essentially count the number of the proposed positive and negative words in a given doc-ument.
"We applied these procedures to uniformly-distributed data, so that the random-choice baseline result would be 50%."
"As shown in Figure 1, the accuracy — percentage of documents classified cor-rectly — for the human-based classifiers were 58% and 64%, respectively. 4 Note that the tie rates — percentage of documents where the two sentiments were rated equally likely — are quite high 5 (we chose a tie breaking policy that maximized the accuracy of the baselines)."
"While the tie rates suggest that the brevity of the human-produced lists is a factor in the relatively poor performance results, it is not the case that size alone necessarily limits accuracy."
"Based on a very preliminary examination of frequency counts in the entire corpus (including test data) plus introspection, we created a list of seven positive and seven negative words (including punctuation), shown in Figure 2."
"As that figure indicates, using these words raised the accuracy to 69%."
"Also, although this third list is of comparable length to the other two, it has a much lower tie rate of 16%."
"We further observe that some of the items in this third list, such as “?” or “still”, would probably not have been proposed as possible candidates merely through introspection, although upon reflection one sees their merit (the question mark tends to occur in sentences like “What was the director thinking?”; “still” appears in sentences like “Still, though, it was worth seeing”)."
"We conclude from these preliminary experiments that it is worthwhile to explore corpus-based tech-niques, rather than relying on prior intuitions, to se-lect good indicator features and to perform sentiment classification in general."
"These experiments also pro-vide us with baselines for experimental comparison; in particular, the third baseline of 69% might actu-ally be considered somewhat difficult to beat, since it was achieved by examination of the test data (al-though our examination was rather cursory; we do not claim that our list was the optimal set of four-teen words)."
"Our aim in this work was to examine whether it suf-fices to treat sentiment classification simply as a spe-cial case of topic-based categorization (with the two “topics” being positive sentiment and negative sen-timent), or whether special sentiment-categorization methods need to be developed."
"We experimented with three standard algorithms: Naive Bayes clas-sification, maximum entropy classification, and sup-port vector machines."
"The philosophies behind these three algorithms are quite different, but each has been shown to be effective in previous text catego-rization studies."
"To implement these machine learning algorithms on our document data, we used the following stan-dard bag-of-features framework."
"Let {f 1 , . . . , f m } be a predefined set of m features that can appear in a document; examples include the word “still” or the bigram “really stinks”."
Let n i (d) be the num-ber of times f i occurs in document d.
"Then, each document d is represented by the document vector d~ := (n 1 (d), n 2 (d), . . . , n m (d))."
One approach to text classification is to assign to a given document d the class c ∗ = argmax c P(c | d).
"We derive the Naive Bayes (NB) classifier by first observing that by Bayes’ rule,"
"P(c)P(d | c) P(c | d) = , P(d) where P(d) plays no role in selecting c ∗ ."
"To estimate the term P(d | c), Naive Bayes decomposes it by as-suming the f i ’s are conditionally independent given d’s class: m P(c) ¡Q P(d) i=1"
P(f i | c) n i (d) ¢. P NB (c | d) :=
"Our training method consists of relative-frequency estimation of P(c) and P(f i | c), using add-one smoothing."
"Despite its simplicity and the fact that its con-ditional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well[REF_CITE]; indeed,[REF_CITE]show that Naive Bayes is optimal for certain problem classes with highly dependent features."
"On the other hand, more sophisticated algorithms might (and of-ten do) yield better results; we examine two such algorithms next."
"Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural lan-guage processing applications[REF_CITE]."
Its estimate of P(c | d) takes the fol-lowing exponential form:
"P ME (c | d) := Z(d) exp ÃX λ i,c F i,c (d, c)! , 1 i where Z(d) is a normalization function."
"F i,c is a fea-ture/class function for feature f i and class c, defined as follows: [Footnote_6] i (d) &gt; 0 and c 0 = c ."
6 We use a restricted definition of feature/class func-tions so that MaxEnt relies on the same sort of feature information as Naive Bayes.
"F i,c (d, c 0 ) := n 01, otherwisen"
"For instance, a particular feature/class function might fire if and only if the bigram “still hate” ap-pears and the document’s sentiment is hypothesized to be negative. [Footnote_7] Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relation-ships between features, and so might potentially per-form better when conditional independence assump-tions are not met."
7 The dependence on class is necessary for parameter induction.[REF_CITE]for additional moti-vation.
"The λ i,c ’s are feature-weight parameters; inspec-tion of the definition of P ME shows that a large λ i,c means that f i is considered a strong indicator for class c."
"The parameter values are set so as to max-imize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the under-lying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intu-itive sense."
"We use ten iterations of the improved iterative scaling algorithm[REF_CITE]for parameter training (this was a sufficient num-ber of iterations for convergence of training-data ac-curacy), together with a Gaussian prior to prevent overfitting[REF_CITE]."
"Support vector machines (SVMs) have been shown to be highly effective at traditional text categorization, generally outperforming Naive Bayes[REF_CITE]."
"They are large-margin, rather than proba-bilistic, classifiers, in contrast to Naive Bayes and MaxEnt."
"In the two-category case, the basic idea be-hind the training procedure is to find a hyperplane, represented by vector w~, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible."
"This search corresponds to a con-strained optimization problem; letting c j ∈ {1,−1} (corresponding to positive and negative) be the cor-rect class of document d j , the solution can be written as w~ := X α j c j d~ j , α j ≥ 0, j where the α j ’s are obtained by solving a dual opti-mization problem."
"Those d~ j such that α j is greater than zero are called support vectors, since they are the only document vectors contributing to w~. Clas-sification of test instances consists simply of deter-mining which side of w’s~ hyperplane they fall on."
"We used Joachim’s (1999) SV M light package [URL_CITE] for training and testing, with all parameters set to their default values, after first length-normalizing the doc-ument vectors, as is standard (neglecting to normal-ize generally hurt performance slightly)."
We used documents from the movie-review corpus described in Section 3.
"To create a data set with uni-form class distribution (studying the effect of skewed class distributions was out of the scope of this study), we randomly selected 700 positive-sentiment and 700 negative-sentiment documents."
"We then divided this data into three equal-sized folds, maintaining bal-anced class distributions in each fold. (We did not use a larger number of folds due to the slowness of the MaxEnt training procedure.)"
"All results reported below, as well as the baseline results from Section 4, are the average three-fold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune)."
"To prepare the documents, we automatically re-moved the rating indicators and extracted the tex-tual information from the original HTML docu-ment format, treating punctuation as separate lex-ical items."
No stemming or stoplists were used.
One unconventional step we took was to attempt to model the potentially important contextual effect of negation: clearly “good” and “not very good” in-dicate opposite sentiment orientations.
"Adapting a technique[REF_CITE], we added the tag NOT to every word between a negation word (“not”, “isn’t”, “didn’t”, etc.) and the first punctuation mark following the negation word. (Preliminary ex-periments indicate that removing the negation tag had a negligible, but on average slightly harmful, ef-fect on performance.)"
"For this study, we focused on features based on unigrams (with negation tagging) and bigrams."
"Be-cause training MaxEnt is expensive in the number of features, we limited consideration to (1) the 16165 unigrams appearing at least four times in our 1400-document corpus (lower count cutoffs did not yield significantly different results), and (2) the 16165 bi-grams occurring most often in the same data (the selected bigrams all occurred at least seven times)."
"Note that we did not add negation tags to the bi-grams, since we consider bigrams (and n-grams in general) to be an orthogonal way to incorporate con-text."
Initial unigram results The classification accu-racies resulting from using only unigrams as fea-tures are shown in line (1) of Figure 3.
"As a whole, the machine learning algorithms clearly surpass the random-choice baseline of 50%."
"They also hand-ily beat our two human-selected-unigram baselines of 58% and 64%, and, furthermore, perform well in comparison to the 69% baseline achieved via limited access to the test-data statistics, although the im-provement in the case of SVMs is not so large."
"On the other hand, in topic-based classification, all three classifiers have been reported to use bag-of-unigram features to achieve accuracies of 90% and above for particular categories[REF_CITE][Footnote_9] — and such results are for set-tings with more than two classes."
"9[REF_CITE]used stemming and stoplists; in some of their experiments,[REF_CITE], like us, did not."
"This provides suggestive evidence that sentiment categorization is more difficult than topic classification, which cor-responds to the intuitions of the text categoriza-tion expert mentioned above. [Footnote_10]"
"10 We could not perform the natural experiment of at-tempting topic-based categorization on our data because the only obvious topics would be the film being reviewed; unfortunately, in our data, the maximum number of re-views per movie is 27, too small for meaningful results."
"Nonetheless, we still wanted to investigate ways to improve our senti-ment categorization results; these experiments are reported below."
"Feature frequency vs. presence Recall that we represent each document d by a feature-count vector (n 1 (d), . . . , n m (d))."
"However, the definition of the"
"MaxEnt feature/class functions F i,c only reflects the presence or absence of a feature, rather than directly incorporating feature frequency."
"In order to investi-gate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting n i (d) to 1 if and only feature f i appears in d, and reran Naive Bayes and SV M light on these new vec-tors. [Footnote_11]"
"11 Alternatively, we could have tried integrating fre-quency information into MaxEnt. However, feature/class functions are traditionally defined as binary[REF_CITE]; hence, explicitly incorporating frequencies would require different functions for each count (or count bin), making training impractical. But cf.[REF_CITE]."
"As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for fea-ture presence, not feature frequency."
"Interestingly, this is in direct opposition to the observations[REF_CITE]with respect to Naive Bayes topic classification."
We speculate that this in-dicates a difference between sentiment and topic cat-egorization — perhaps due to topic being conveyed mostly by particular content words that tend to be repeated — but this remains to be verified.
"In any event, as a result of this finding, we did not incor-porate frequency information into Naive Bayes and SVMs in any of the following experiments."
"Bigrams In addition to looking specifically for negation words in the context of a word, we also studied the use of bigrams to capture more context in general."
"Note that bigrams and unigrams are surely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes’ conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly[REF_CITE]."
"Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bi-grams does not seriously impact the results, even for Naive Bayes."
"This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact,[REF_CITE]found that bigrams alone can be effective features for word sense disambiguation."
"However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points."
"Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at cap-turing it in our setting."
Parts of speech We also experimented with ap-pending POS tags to every word via Oliver Mason’s Qtag program. [URL_CITE]
"This serves as a crude form of word sense disambiguati[REF_CITE]: for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment)."
"However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the per-formance of MaxEnt is unchanged."
"Since adjectives have been a focus of previous work in sentiment detecti[REF_CITE][Footnote_13] , we looked at the performance of using adjectives alone."
13 Turney’s (2002) unsupervised algorithm uses bi-grams containing an adjective or an adverb.
"Intuitively, we might ex-pect that adjectives carry a great deal of informa-tion regarding a document’s sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech."
"Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than uni-gram presence."
"Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2))."
This may imply that applying explicit feature-selection algo-rithms on unigrams could improve performance.
"Position An additional intuition we had was that the position of a word in the text might make a dif-ference: movie reviews, in particular, might begin with an overall sentiment statement, proceed with a plot discussion, and conclude by summarizing the author’s views."
"As a rough approximation to deter-mining this kind of structure, we tagged each word according to whether it appeared in the first quar-ter, last quarter, or middle half of the document [Footnote_14] ."
"14 We tried a few other settings, e.g., first third vs. last third vs middle third, and found them to be less effective."
"The results (line (8)) didn’t differ greatly from using unigrams alone, but more refined notions of position might be more successful."
The results produced via machine learning tech-niques are quite good in comparison to the human-generated baselines discussed in Section 4.
"In terms of relative performance, Naive Bayes tends to do the worst and SVMs tend to do the best, although the differences aren’t very large."
"On the other hand, we were not able to achieve ac-curacies on the sentiment classification problem com-parable to those reported for standard topic-based categorization, despite the several different types of features we tried."
"Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consis-tently better performance once unigram presence was incorporated."
"Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous obser-vations made in topic-classification work[REF_CITE]."
"What accounts for these two differences — dif-ficulty and types of information proving useful — between topic and sentiment classification, and how might we improve the latter?"
"To answer these ques-tions, we examined the data further. (All examples below are drawn from the full 2053-document cor-pus.)"
"As it turns out, a common phenomenon in the doc-uments was a kind of “thwarted expectations” narra-tive, where the author sets up a deliberate contrast to earlier discussion: for example, “This film should be brilliant."
"It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance."
"However, it can’t hold up” or “I hate the Spice Girls. ... [3 things the author hates about them]..."
"Why I saw this movie is a really, really, really long story, but I did, and one would think I’d despise every minute of it."
"Okay, I’m really ashamed of it, but I enjoyed it."
"I mean, I admit it’s a really awful movie ...the ninth floor of hell..."
The plot is such a mess that it’s terrible.
But I loved it.” [Footnote_15]
"15 This phenomenon is related to another common theme, that of “a good actor trapped in a bad movie”: “AN AMERICAN WEREWOLF IN PARIS is a failed at-tempt... Julie Delpy is far too good for this movie. She im-bues Serafine with spirit, spunk, and humanity. This isn’t necessarily a good thing, since it prevents us from relax-ing and enjoying AN AMERICAN WEREWOLF IN PARIS as a completely mindless, campy entertainment experience. Delpy’s injection of class into an otherwise classless produc-tion raises the specter of what this film could have been with a better script and a better cast ... She was radiant, charismatic, and effective ....”"
"In these examples, a human would easily detect the true sentiment of the review, but bag-of-features classifiers would presumably find these instances dif-ficult, since there are many words indicative of the opposite sentiment to that of the entire review."
"Fun-damentally, it seems that some form of discourse analysis is necessary (using more sophisticated tech- niques than our positional feature mentioned above), or at least some way of determining the focus of each sentence, so that one can decide when the author is talking about the film itself. ([REF_CITE]makes a similar point, noting that for reviews, “the whole is not necessarily the sum of the parts”.)"
"Further-more, it seems likely that this thwarted-expectations rhetorical device will appear in many types of texts (e.g., editorials) devoted to expressing an overall opinion about some topic."
"Hence, we believe that an important next step is the identification of features indicating whether sentences are on-topic (which is a kind of co-reference problem); we look forward to addressing this challenge in future work."
This paper describes improved HMM-based word level alignment models for statistical machine translation.
"We present a method for using part of speech tag information to improve alignment accu-racy, and an approach to modeling fertility and cor-respondence to the empty word in an HMM align-ment model."
"We present accuracy results from eval-uating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4."
The results show up to 16% alignment error reduc-tion.
to model the string translation probabilityThe main task in statistical machine translation   is in one language  is translated  where the string into another language as string .
We refer to as the source language string and as the target language string in accordance with the noisy chan-nel terminology used in the IBM models[REF_CITE].
Word-level translation models assume a pairwise mapping between the words of the source and target strings.
This mapping is generated by alignment models.
In this paper we present exten-sions to the HMM alignment model[REF_CITE].
Some of our extensions are applicable to other alignment models as well and are of general utility. [Footnote_1]
1 This paper was supported in part by the National Science Foundation under Grants[REF_CITE]. The authors would also like to thank the various reviewers for their helpful comments on earlier versions.
For most language pairs huge amounts of parallel corpora are not readily available whereas monolin-gual resources such as taggers are more often avail-able.
Little research has gone into exploring the po- tential of part of speech information to better model translation probabilities and permutation probabili-ties.
They use English POS classes as states of the Markov Model to generate Chinese language words.
In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order vari-ation.
We show that using this information can help in the translation modeling task.
"Many alignment models assume a one to many mapping from source language words to target lan-guage words, such as the IBM models 1-5[REF_CITE]and the HMM alignment model[REF_CITE]."
"In addition, the  IBM  Models 3,where is4 and 5 include a fertility model the number of words aligned to a source word ."
In HMM-based alignment word fertilities are not mod-eled.
The alignment positions of target words are the states  in an HMM.
The alignment probabilities for depend  only on the alignment of the pre- word vious word if using a first order HMM.
"There-fore, source words are not awarded/penalized for be-ing aligned to more than one target word."
We present an extension to HMM alignment that approximately models word fertility.
Another assumption of existing alignment mod-els is that there is a special Null word in the source sentence from which all target words that do not have other correspondences in the source language are generated.
Use of such a Null word has proven problematic in many models.
We also assume the existence of a special Null word in the source lan-guage that generates words in the target language.
"However, we define a different model that better constrains and conditions generation from Null."
We assume that the generation probability of words by Null depends on other words in the target sentence.
Next we present the general equations for decom-position of the translation probability using part of speech tags and later we will go into more detail of our extensions.
Augmenting the model    with part of speech tag information  leads to the following equa-tions.
"We use , or vector notation e, f to de-note English and French strings. ( and represent the lengths of the French and English strings respec-tively.)"
Let us define eT and fT as possible POS tag the string translation probabilitysequences of the sentences e and f  .
"We  can  rewriteas fol-lows (using Bayes rule to give the last line):  #! &quot; &amp;$ %)*( ,+ !#&quot; $&amp;% .+ . / *( .+ ! &quot;$&amp;%.+ &quot;01% )(*+ 3*( ,+ $ &amp;$ % &gt;0 01&lt;% ? ! &quot; $&amp;%.+ 6&quot;01%.+  :&lt;; :&lt;= ; $ = &amp;$ %@= ? #"
If we also assume that the taggers in both languages generate a single tag sequence for each sentence then the equation for machine translation by the noisy channel model simplifies to
A B*CEDF0  )!
"A B*0 .[Footnote_3](*,+ ) *( +"
"3 In order for the model not to be deficient, we normalize the jump probabilities at each EM step so that jumping outside of the borders of the sentence is not possible."
This is the decomposition of the string translation probability into a language model and translation model.
In this paper we only address the transla-tion model and assume that there exists a one-to-one alignment from target to source words.
"Therefore, 3*( ,+ *( + K! [Footnote_3](*,+ (* )*( +&quot;   ( One possible way to rewrite  P( )Q (*R  without loss of generality is:  ( )QV (*RW )(*+ 4!"
"3 In order for the model not to be deficient, we normalize the jump probabilities at each EM step so that jumping outside of the borders of the sentence is not possible."
P^_   R  (P  P( )Q  ((**( +)*(+ c d_ ] [ \Z ` a_ )
"RR (P(P  (PP( ))QQ   ( ( *( *((*)(*+ + e _ (1) Here each  R gives the index of the word ,g to which is aligned."
"The models we present in this paper will differ in the decompositions of align-ment probabilities, tag translation and word trans-lation probabilities in Eqn. 1."
Section 3 describes the baseline model in more detail.
Section 4 illus-trates examples where the baseline model performs poorly.
Section 5 presents our extensions and Sec-tion 6 presents experimental results.
Translation of French and English sentences shows a strong localization effect.
Words close to each other in the source language remain close in the transla-tion.
"Furthermore, most of the time the alignment shows monotonicity."
This means that pairwise align- (kjl ments stay close to the diagonal line of the plane.
It has been shown[REF_CITE]that HMM based alignment models are effective at capturing such lo-calization.
We use as a baseline the model presented[REF_CITE].
"A basic bigram HMM-based model gives us  U! &quot;fPmn ] [ \    ( . ,g qp (2)"
"In this HMM model, [Footnote_2] alignment probabilities are independent  of word  position and depend only on). 3[REF_CITE]jump width ( model includes refinements including special treat-ment of a jump to Null and smoothing with a uni-form prior which we also included in our initial model."
2 Each HMM state is [ x ] emitting  as output.
As in their model we set the probability vu6w! forjump from any state to Null to a fixed value ( ) which we estimated from gzy|{kl} ~ held-out data g .
"Although the baseline Hidden Markov alignment model successfully generates smooth alignments, there are a fair number of alignment examples where pairwise match shows local irregularities."
One in-stance of this is the transition of the NP  JJ NN rule to NP  NN JJ from English to French.
We can list two main reasons why word translation proba-bilities may not catch such irregularities to mono-tonicity.
"First, it may be the case that both the English adjective and noun are words that are un-known."
In this case the translation probabilities will be close to each other after smoothing.
"Second, the adjective-noun pair may consist of words that are frequently seen together in English."
"National re-serve and Canadian parliament, are examples of such pairs."
As a result there will be an indirect asso-ciation between the English noun and the translation of the English adjective.
"In both cases, word transla-tion probabilities will not be differentiating enough and alignment probabilities become the dominating factor to determine where aligns."
Figure 1 illustrates how our baseline HMM model makes an alignment mistake of this sort.
"The ta-ble in the figure displays alignment and translation probabilities of two competing alignments (namely alignments, the shownAln1 and Aln2) for the last three ,f g words."
In bothand are periods at the end of the French and English sentences.
"The first alignment maps .f gk n nationale ! to national f,g* and unité tounity. (i.e. national and =unity)."
The unity (i.e.second alignment .gk n ! maps both nationale .gk/! and unité tounity and unity).
"Start-ing from the unity-unité alignment, the jump width and Aln2 are  r  , Mr  sequences  ( R"
"R   )  for Aln1 ), ( 2  and  0, 1  respectively."
The table shows that the gain from use of monotonic alignment probabilities dominates over the lowered word translation probability.
"Although national and translation probabilities, jump widths ofnationale are strongly correlated according r to theand 2 are less probable than jump widths of 0 and 1."
In this section we describe our improvements on the HMM model.
We present evaluation results in Sec-tion 6 after describing the technical details of our models here.
Our model with part of speech tags for translation probabilities uses the following simplification of the translation probability shown in Eqn. 1. [Footnote_4] /*( .+ )* ( + ! &quot;f mn [ \ ` __a^ )Q . f.(g ce d__ (3)
"4 Since we are only concerned with alignment here  and not generation of candidate translations the factor P(  e,eT) can be ignored and we omit it from the equations for the rest of the paper."
In this model we introduce tag translation probabil-ities as an extra factor to Eqn. 2.
Intuitively the role of this factor is to boost the translation probabilities for words of parts of speech that can often be trans-lations of each other.
Thus this probability distribu-tion provides prior knowledge of the possible trans-
"However, P(lations of a word )Q  based ,f g only on its part of speech.) should not be too sharp or probabilitiesit will dominate  the  alignment probabilities and the."
"We use the following linear interpolation to smooth tag translation probabilities: )Q  ,f g 4!MU )"
"Q  ,f g   (4) T is the size of the French tag set and is set to be  0.1 in our experiments."
The tag translation model is so heavily smoothed with a uniform distribution be-cause in EM the tag translation probabilities quickly become very sharp and can easily overrule the align-ment and word translation probabilities.
"The Results section shows that the addition of this factor reduces the alignment error rate, with the improvement being especially large when the training data size is small."
This section describes an extension to the bigram HMM model that uses source and target language tag sequences as conditioning information when predicting the alignment of target language words. bilityIn the / decomposition *( .F+ *( &lt;N (*+ of the joint proba-shown in Eqn. 1 the  R factor  (P  for P( )Q alignment  ( *( )(*+ probabilities is .
R A bigram HMM model assumes independence of tionfrom R  anything but the previous alignment posi-and the length of the English sentence.
"As conditioning directly on words would yield a large number of parameters and would be imprac-tical, they cluster the words automatically into bilin-gual word classes."
The question arises then whether we would have larger gains by conditioning on the part of speech tags of those words or even more words around the alignment position.
"For example, if we use the fol-lowing conditioning   information  ( (*) : *( + s!  R  (P ( P,( g*) Q n .gk n ( ,g* .n ( we could model probabilities of transpositions and insertion of function words in the target language that have .g no corresponding words in the source lan-guage ( is Null) similarly to the channel oper-ations of the[REF_CITE]syntax- based statistical translation model."
"Since the syntac-tic knowledge provided by POS tags is quite limited, this is a crude model of transpositions and Null in-sertions at the preterminal level."
However we could still expect that it would help in modeling local word order variations.
"For example, in the sentence J’aime la  chute ! ‘I ) love ! the fall’ the probability of la ,g* (  n ! DT) to the  will .gk n, be  boosted ! aligning by knowing VBP and DT."
"Similarly, in the sentence J’aime des  chiens ! ‘I love dogs’ the probability of aligning zQ f.gk n ! la to Null will ,* g  be .n  increased ! by knowing"
VBP and NNS.
VBP followed by NNS crudely conducts the information that the verb is followed by a noun phrase which does not include a determiner.
"We conducted a series of experiments where the alignment probabilities are conditioned on different n ( subsets ,*g  n ( of .gk the n. (P) part  of (P) speech )P( )Q P( ) , *g  tags . "
"In order to be able to condition on when generating an alignment position for , we have to change the generative model for the sentence f and its tag sequence fT to generate the part of speech tags for the French words before choosing alignment positions for them."
"The French a prior distributionPOS tags could be  generated )Q) for example fromor from the previous French tags as in an HMM for part-of-speech 3*( ,+  tag- s! ging."
The generative model becomes: .+ 8&quot;f mn ] [ \ o   R gk n (P) P( )Q g.¡ n ( . .qp
This model makes the assumption that target words are independent of their tags given the correspond-ing source word and models only the dependence of alignment positions on part of speech tags.
A major advantage of the IBM models 3–5 over the HMM alignment model is the presence of a model of source word fertility.
Thus knowledge that some words translate as phrases in the target language is incorporated in the model.
"The HMM model has no memory, apart from the previous alignment, about how many words it has aligned to a source word."
Yet even this memory is not used to decide whether to generate more words from a given English word.
The decision to gener- ate again (to make a jump of size 0) is independent of the word and is estimated over all words in the corpus.
"We extended the HMM model to decide whether to generate ,g* n more words from the previous English or to move on to a different word ,*g de-  n word pending on the identity of  the English f,*g  n word."
We introduced a factor stay where the boolean random  variable stay depends on the En-glish word aligned to.
"Since in most cases words with fertility greater than one generate words that are consecutive in the target language, this extension approximates fertility modeling."
"More specifically, the baseline model (i.e., Eqn. 2) is changed as follows:  I! &quot;fPmn ] [ \ .¢   R  ( .gk n ( . ,g qp where £  ( ,*g  n ( )#!  /*( R  . ,* g  n_d @ `^__a #R   (*R ,*g  n k ce_ stay  k stay ( ([Footnote_5]) ¤2£   in Eqn. 5 is the Kronecker delta func-tion  ."
5 E[X] = ª = « + ¬|/¯ + + ... where X is the number of Bernoulli trials until the first success.
"Basically ( , the new alignment probabilitiesstate that a jump width of zero de-pends on the English word."
"If we define the fertility of a word as the number of consecutive words from the target language it generates, then the probabil-ity distribution for the fertility of an English word e according to this  model is geometric  with a proba-of &gt; .§ success ? bility  &lt;: ; stay ."
"The expectation is stay . 5 Even though the fit of this distribution to the real fertility distribution may not be very good, this approximation improves alignment accuracy in practice. ities P(staySparsity is  a .gk problem n in estimating stay probabil-)."
We use the probability of a jump of size zero from the baseline model as our prior to do smoothing as follows: stay n } *g  n W!n &lt; .   } g* n stay (6)  Z in this equation is the alignment probability from  Z ! the baseline !   model ²! j* with ( . zero jump distance.
"As originally proposed[REF_CITE], words in the target sentence for which there are no corresponding English words are assumed to be gen-erated by the special English word Null."
Null ap-pears in every English sentence and often serves to generate syntactic elements in the target language that are  missing  in the source.
A probability distri-Null for generation probabilities of thebution Null is re-estimated from a training corpus.
Modeling a Null word has proven problematic.
It has required many special fixes to keep models from aligning everything to Null or to keep them from aligning nothing to Null[REF_CITE].
This might stem from the problem that the Null is respon-sible for generating syntactic elements of the target language as well as generating words that make the target language sentence more idiomatic and stylis-tic.
The intuition for our model of translation proba-bilities for target words that do not have correspond-ing source words is that these words are generated from the special English Null and also from the next word in the target language by a mixture model.
The pair la confédération in Figure 1 is an example of such case where confédération contributes extra in-formation in generation of la.
The formula for the probability of a target word given that it does not have a corresponding aligning word in the source is:  R !X³3!).(f.g.! g!
Null Null (7)  We   (.g re-estimate ! the probabilities
Null from the training cor-pus using EM.
"The dependence of a French word on the next French word requires a change in the generative model to first propose align-ments for all words in the French sentence and to then generate the French words given their alignments, starting from the end of the sentence and going towards the beginning."
For the new model there is an efficient dynamic programming algorithm for computations in EM similar to the forward-backward algorithm.
The probability  uTuTu1*( !· jP(P 
TuTuTu( ]  again decom-poses into forward and backward kj( . ! probabilities /!ºjl»Y .
The forward probability ! *j P(   is ¸*(  uTuTu1  probability is ¼ (kj.t!  uTuTu] and the R backward ¾jP! *(  .
These can be computed recursively and used for efficient computation of posteriors in EM.
We present results on word level alignment accu-racy using  the Hansards corpus.
Our test data con-sists of ¿ manually aligned sentences which are the same data set used[REF_CITE]. 6
In the annotated sentences every alignment between two words is labeled as either a sure (S) or possible (P) alignment. (S À P).
"We used the following quan-tity (called alignment error rate or AER) to evaluate the alignment quality of our models, which is also the evaluation metric Ã¦[Footnote_6]Ä-ÅÆ Ã¦ÄÁ ]j,! used by (Och and Ney Ã&apos;Ä ,  2000b):"
6 We want to thank Franz Och for sharing the annotated data with us.
ÃUËÌÍ! set ofWe divided  this annotated data into a validation /w  sentences and a final test set of sen-tences.
"The validation  set was used to select tuning parameters such as in Eqn. 4, 6 and w/ 7  ."
We report AER results on the final test P( Ï Î ¿ setEnglishof andsentences Ð (* which contain a total of Î French words.
We experimented with training cor-pora of different sizes ranging from 5K to 50K sen-tences.
We concentrated on small to medium data sets to assess the ability of our models to deal with sparse data.
Table 1 shows the percentage of words in the cor-pus that were seen less than the specified number w of times.
"For example, in our 10K training corpus  of all word types were seen only once."
As seen from the table the sparsity is great even for large corpora.
The models we implemented and compare in this section are the following:
Ó Baseline is the baseline HMM model described Ó inTagssectionis an2
HMM model that includes tags for translation probabilities (section 5.1) models on training corpora of increasing size.
"The model Null outperforms the baseline at every data set size,with the error reduction being larger for big-ger training sets (up to 9.2% error reduction)."
The SG model reduces the baseline error rate by up to 10%.
The model Tags reduces the error rate for the smallest dataset by 7.6%.
The combination of Tags and the SG or Null models outperforms the individ-ual models in the combination since they address different problems and make orthogonal mistakes.
The combination of SG and Tags reduces the base-line error rate by up to 16% and the combination of Null and Tags reduces the error rate by up to 12.3%.
All of these  error !vuÔ³ ¿ reductionsconfidencearelevelstatisticallyaccordingsignifi-to the cant at the paired t-test.
The combination Tags+Null+SG fur-ther reduces the error rate.
"For small datasets, there seems to be a stronger overlap between the strengths of the Null and SG models because some fertility related phenomena can be accounted for by both models."
"When an English word is wrongly align-ing to several consecutive French words because of indirect association, while the correct alignment of some of them is to the empty word, both the Null and SG models can combat the problem— one by better modeling correspondence to Null, and the other by discouraging large fertilities."
"Figure 2 displays learning curves for three mod-els: Och, Tags, and Tags+Null."
Och is the HMM alignment model[REF_CITE].
To ob-tain results from the Och model we ran GIZA++. [Footnote_7] Both the Tags and Och models use word classes.
7 GIZA++ can be downloaded[URL_CITE]Õ och/software/GIZA++.html
However the word classes used in the latter are learned automatically from parallel bilingual cor-pora while the classes used in the former are hu-man defined part of speech tags.
Figure 2 shows that the Tags model outperforms the Och model when the training data size is small.
As the train- ing size increases the Och model catches up with the Tags model and even surpasses it slightly.
This suggests that when large amounts of parallel text are not available monolingual part of speech classes can improve alignment quality more than automatically induced classes.
When more data is available au-tomatically induced bilingual word classes seem to provide more improvement but it still remains to be explored whether the combination of part-of-speech knowledge with induction of bilingual classes will perform even better.
The third curve in the figure for Tags+Null illustrates the relative improvement of the Null model over the Tags model as the training set size increases.
We see that the performance gap between the two models becomes wider for larger training data sizes.
This reflects the improved esti-mation of the generation probabilities for Null which require target word specific parameters.
We used both paired t-test and Wilcoxon signed rank tests to show the improvements ? are statistically significant.
The signed ¡ IØ rank ¡ ? test  uses the normalized test statis-tic ×Ù Ú *f Û ;; × ¡ positive signs × ..
Ü is the sum of the ranks that have are assigned the average rank of the tied group.
"Since there are 400 test sentences, we have 400 paired samples where the elements of each pair are the AERs of the models being compared."
The difference between Och  and Í³JuÔ³!
"Tags ¿ levelat 5Kaccord-, 10K, and 15K is significant at the ing to both tests."
The difference between Och and Tags  + ³JuÔ³!
Null ¿ islevelsignificant. for all training set sizes at the
We also assessed the gains from using part of speech tags in the alignment probabilities according to the model described in section 5.2.
"Table 3 shows the error rate of the basic HMM alignment model as compared to an HMM model that conditions on tag sequences of source and target  word tags in the neighborhood ,*g  n of the French word and the English word for a training set size of 10K. The results we achieved showed an improvement of our model over a model that does not include conditioning on tags."
The improvement in accuracy is best when using the current and previous French word parts of speech and does not increase when adding more conditioning information.
"The improvement from part of speech tag sequences for alignment proba-bilities was not as good as we had expected, how-ever, which leads us to believe that more sophisti- cated syntax is needed to model local word order variation."
In Figure 3 we compare the IBM- model to our w because IBM- uses a fertility model for EnglishSG+Tags model w .
Such a comparison makes sense words and SG w approximates fertility modeling andbecause IBM- uses word classes as does our Tags model.
For smaller training set w sizes our model per-forms much better w than IBM- but when more datais available IBM- becomes slightly better.
This confirms the observation from Figure 2 that auto-matically induced bilingual classes perform better when trained on large amounts of data.
"Also as our glish word and IBM- estimates as many parame-fertility model estimates w one parameter for each En-ters as the maximum fertility allowed, at small train-ing set sizes our model parameters can be estimated more reliably."
In this paper we presented three extensions to HMM-based alignment models.
We showed that incorporating part of speech tag information of the source and target languages in the translation model improves word alignment accuracy.
We also pre-sented a method for approximately modeling fertil-ity in an HMM-based model and a new generative model for target language words that do not have correspondences in the source language.
This paper presents a technique for discover-ing translationally equivalent texts.
It is com-prised of the application of a matching algo-rithm at two different levels of analysis and a well-founded similarity score.
This approach can be applied to any multilingual corpus us-ing any kind of translation lexicon; it is there-fore adaptable to varying levels of multilingual resource availability.
"Experimental results are shown on two tasks: a search for matching thirty-word segments in a corpus where some segments are mutual translations, and classifi-cation of candidate pairs of web pages that may or may not be translations of each other."
"The latter results compare competitively with pre-vious, document-structure-based approaches to the same problem."
"As in most areas of natural language process-ing, recent approaches to machine translation have turned increasingly to statistical modeling of the phenomenon (translation models)[REF_CITE]."
"Such models are learned auto-matically from data, typically parallel corpora: texts in two or more languages that are mutual translations."
"As computational resources have become more powerful and less expensive, the task of training translation models has become feasible[REF_CITE], as has the task of translating (or “decoding”) text using such models[REF_CITE]."
"However, the success of the statistical approach to trans-lation (and also to other multilingual applica-tions that utilize parallel text) hangs crucially on the quality, quantity, and diversity of data used in parameter estimation."
"If translation is a generative process, then one might consider its reverse process of recognition:"
"Given two documents, might it be determined fully automatically whether they are transla-tions of each other?"
The ability to detect translations of a doc-ument has numerous applications.
The most obvious is as a means to build a parallel corpus from a set of multilingual documents that con-tains some translation pairs.
Examples include mining the World-Wide Web for parallel text[REF_CITE]and building parallel corpora from comparable corpora such as multilingual collec-tions of news reports.
Another use of trans-lation detection might be as an aid in align-ment tasks at any level.
"For example, consider the task of aligning NP chunks (and perhaps also the extra-NP material) in an NP-bracketed parallel corpus; a chunk-level similarity score[REF_CITE]built from a word-level model could be incorporated into a framework that involves bootstrapping more complex mod-els of translation from simpler ones[REF_CITE]."
"Finally, reliable cross-lingual dupli-cate detection might improve performance in n-best multilingual information retrieval systems; at the same time, by detecting the existence of a translation in a multilingual corpus, the cost of translating a document of interest is eliminated."
"I present here an algorithm for classifying document pairs as either translationally equiv-alent or not, which can be built upon any kind of word-to-word translation lexicon (au-tomatically learned or hand-crafted)."
"I pro-pose a score of translational similarity, then describe an evaluation task involving a con-strained search for texts (of arbitrary size) that are translation pairs, in a noisy space, and present precision/recall results."
"Finally, I show that this algorithm performs competitively with the approach[REF_CITE], in which only structural information (HTML-markup) is used to detect translation pairs, though the new algo-rithm does not require structural information."
"This section shows how to compute a cross-lingual similarity score, tsim, for two texts. 1 Suppose parallel texts are generated according to Melamed’s (2000) symmetric word-to-word model (Model A)."
"Let a link be a pair (x,y) where x is a word in language L [Footnote_1] and y is a word in L 2 ."
1 I use the term “text” to refer to a piece of text of any length.
"Within a link, one of the words may be NULL, but not both."
The model con-sists of a bilingual dictionary that gives a prob-ability distribution over all possible link types.
"In the generative process, a sequence of inde-pendent link tokens is generated according to that distribution."
The links are not observed; only the lexical (non-NULL) words in each language are ob-served.
"The texts whose similarity score is to be computed, X and Y , correspond to the mono-lingual lexical projections of the links."
"For the purposes of this discussion, the texts are viewed as unordered bags of words; scrambling of the link tokens in the two texts is not modeled."
"An example is illustrated in Figure 1; there are seven link tokens shown, five of which are lexi-cal in X (the English side) and six of which are lexical in Y (the French side)."
The next step is to compute the probability of the most probable sequence that could have ac-counted for the two texts.
"All permutations of a given link sequence will have the same prob-ability (since the links are generated indepen-dently), so the order of the sequence is not im-portant."
"As noted[REF_CITE], under the assumption that the quality of a link col-lection is the sum of the quality of the links, then this problem of finding the best set of links is equivalent to the maximum-weighted bi-partite matching (MWBM) problem: Given a weighted bipartite graph G = (V 1 ∪ V 2 , E) with |V 1 | = |V 2 | and edge weights c i,j (i ∈ V 1 , j ∈ V 2 ), find a matching M ⊆ E such that each ver-tex has at most one edge in M, and P e∈M c i,j is maximized."
The fastest known MWBM al-gorithm runs in O(ve + v 2 logv) time[REF_CITE].
"Applied to this problem, that is O(max(|X|, |Y |) 3 )."
The similarity score should be high when many of the link tokens in the best link col-lection do not involve NULL tokens.
"Further, it should normalize for text length."
"Specifically, the score I use is: log Pr(two-word links in best matching) tsim = log Pr(all links in best matching) (1)"
"This score is an example of Lin’s (1998) math-ematical definition of similarity, which is moti-vated by information theory: log Pr(common(X, Y )) sim(X, Y ) = ([Footnote_2]) log Pr(description(X, Y )) where X and Y are any objects generated by a probabilistic model. 2"
"2 Another approach, due to Jason Eisner (personal communication) would be to use a log-likelihood ratio of two hypotheses: joint vs. separate generation of the Pr(all links in the best sequence) two texts (log Pr(all words in X)Pr(all words in Y ) ). In order to make this value (which is the Viterbi approximation to point-wise mutual information between the two texts) a score suitable for comparison between different pairs of texts, it must be normalized by length. With normal-ization, this score is monotonic in Lin’s (1998) sim if a uniform unigram model is assumed for the tokens in the single-language models (the denominator terms)."
"In this research, I seek to show how multiple linguistic resources can be exploited together to recognize translation."
"The measure in (1) is simplified by assuming that all links in a given translation lexicon are equiprobable. (In some cases I use an automatically induced translation lexicon that assigns probabilities to the entries, but for generality the probabilities are ignored.)"
This reduces the formula in (1) to # two-word links in best matching tsim = . (3) # links in best matching
"Further, to compute tsim under the equiprob-ability assumption, we need not compute the MWBM, but only find the maximum cardi-nality bipartite matching (MCBM), since all potential links have the same weight."
O(e v) (or O(|X| · |Y | · p |X| + |Y |) for this purpose) algorithm exists for MCBM[REF_CITE].
"If the matching shown in Figure 1 is the MCBM (for some translation lexicon), then tsim(X,Y ) = 47 under the simplifying assump-tion."
"If Equation (3) is applied to pairs of docu-ments in the same language, with a “transla-tion lexicon” defined by the identity relation, then tsim is a variant of resemblance (r), as de-fined[REF_CITE]for the problem of monolingual duplicate detection: r(X, Y ) = |S(X) ∩ S(Y )| (4) |S(X) ∪ S(Y )| where S(Z) is a shingling of the words in Z; a shingling is the set of unique n-gram types in the text for some fixed n[REF_CITE]."
"Unlike Broder et al.’s r, however, tsim is token-based, incorporating word frequency."
"Specifically, the intersection of two bags (rather than sets) of to-kens contains the minimum count (over the in-tersected bags) of each type; the union contains the maximum counts, e.g., {a, a, a, b, b} ∩ {a, a, b, b, b} = {a, a, b, b} {a, a, a, b, b} ∪ {a, a, b, b, b} = {a, a, a, b, b, b}"
"With the assumption of equiprobability, any translation lexicon (or, importantly, union thereof) containing a set of word-to-word en-tries, can be used in computing tsim."
"Formally, the evaluation task I propose can be described as follows: Extract all translation pairs from a pool of 2n texts, where n of them are known to be in language L 1 and the other n are known to be in L 2 ."
Each text can have one or zero translations in the corpus; let the number of true translation pairs be k.
The general technique for completing the task is to first find the best matching of words in text pairs (posed as a bipartite matching prob-lem) in order to compute the tsim similarity score.
"Next, to extract translation pairs of texts from a corpus, find the best matching of texts based on their pairwise tsim scores, which can be posed as a “higher-level” MWBM problem: by matching the texts using their pair-wise sim-ilarity scores, a corpus of pairs of highly similar texts is extracted from the pool."
"If k is known, then the text-matching problem is a generalization of MWBM:"
"Given a weighted bipartite graph G = (V 1 ∪V 2 , E) with |V 1 | = |V 2 | and edge weights c i,j , find a matching M ⊆ E of size k such that each vertex has at most one edge in M, and P e∈M c i,j is maximized."
"The set of texts in L 1 is V 1 , and the set of texts in L 2 is V 2 ; the weights c i,j are the scores tsim(v i , v j )."
I do not seek a solution to the generalized prob-lem here; one way of approximating it is by tak-ing the top k tsim-scored elements from the set M (the MWBM).
"If k is not known, it can be estimated (via sampling and human evaluation); I take the ap-proach of varying the estimate of k by applying a threshold τ on the tsim scores, then comput-ing precision and recall for those pairs in M whose score is above τ (call this set M τ ): prec τ = |M τ ∩ T|, rec τ = |M τ ∩ T| (5) |M τ | k where T is the set of k true translation pairs."
"Performance results are presented as (precision, recall) pairs as τ is lowered. [Footnote_3]"
"3 The selection of an appropriate τ will depend on the application, the corpus, the lexicons, etc. In my evaluation on WWW data, I use a small development set to choose a threshold that maximizes one measure of performance."
"A heap-based im-plementation of competitive linking runs in O(max(|X|, |Y |) log max(|X|, |Y |))."
"In the first experiment, I show a performance comparison between MWBM and competitive linking."
This experiment used the Hong Kong Hansard English-Chinese parallel corpus.
"The training corpus is aligned at the sentence level, with seg-ment lengths averaging fifteen words (in each language)."
"The test corpus is aligned at the two-sentence level, with segment lengths averaging thirty words."
"The first experiment involved ten-fold cross-validation with (for each fold) a train-ing corpus of 9,400 sentence pairs and a test corpus of 1,000 two-sentence pairs."
"The corpus was randomly divided into folds, and no noise was introduced (i.e., k = n). 4"
The main translation lexicon of interest is a union of three word-to-word translation lexicons from different sources.
I refer to this translation lexicon as UTL.
"The first component translation lexicon, DICT, was made from the union of two English-Chinese electronic dictionaries, specifi-cally, those[REF_CITE]and[REF_CITE](a total of 735,908 entries, many of which are not one-to-one)."
"To make the dictio-nary exclusively one-to-one entries, each n-to-m entry was processed by removing all function words in either side of the entry (according to a language-specific stoplist), then, if both sides have one or two words (no more), adding all word-pairs in the cross-product (otherwise the entry is discarded). [Footnote_5] The resulting translation lexicon contains 577,655 word pairs, 48,193 of which contain two words that are present in the corpus."
5 The limit of two words per side is an arbitrary choice intended to minimize the noise introduced by this pro-cessing step.
"This translation lexicon has the advan-tage of broad coverage, though it does not gen-erally contain names or domain-specific words, which are likely to be informative, and does not capture morphological variants."
"The second translation lexicon, TMTL, is au-tomatically generated by training a symmet-ric word-to-word translation model (Model A,[REF_CITE]) on the training corpus. [Footnote_6]"
"6 In parameter estimation, I used the aforementioned MWBM algorithm (instead of Melamed’s (2000) com-petitive linking), which is the maximum posterior ap-proximation to EM. It is not clear, however, that this change yields performance gains."
All word pairs with nonzero probability were added to the translation lexicon (no smoothing or thresholding was applied).
"On average (over ten folds), this translation lexicon contained [Footnote_6],282 entries."
"6 In parameter estimation, I used the aforementioned MWBM algorithm (instead of Melamed’s (2000) com-petitive linking), which is the maximum posterior ap-proximation to EM. It is not clear, however, that this change yields performance gains."
"The TMTL translation lexicons are ex-pected to capture words specific to the domain (Hong Kong government transcripts), as well as common inflections of words, though they will also contain noise."
"The third translation lexicon, STR, is the string identity lexicon: (x,y) is in the trans-lation lexicon iff the string x is identical to the string y."
"This translation lexicon captures punctuation, numerals, alphanumeric strings used to label sections, and English words in-cluded as-is in the Chinese corpus."
"There were 3,083 such pairs of word types in the corpus."
"In order to avoid computing tsim scores for all pairs in the cross-product, I eliminated all segment pairs whose lengths are outliers in a linear regression model estimated from the training corpus."
"Earlier experiments (on a different corpus) showed that, if a (1 − p)-confidence interval is used, the size of the search space reduces exponentially as p increases, while the number of correct translation pairs that do not pass the filter is only linear in p (i.e., the filter gives high recall and high precision)."
"For these experiments, p = 0.05; this value was se-lected based on the results presented[REF_CITE]."
"When the length filter was applied to the 1,000,000 possible pairs in the cross-product, 47.9% of the pairs were eliminated, while 94.5% of the correct pairs were kept, on average (over ten folds). tsim was computed for each pair that passed the filter, then each matching al-gorithm (MWBM and competitive linking) was applied."
"As discussed above, a threshold can then be applied to the matching to select the pairs about whose translational equivalence the score is most confident."
Precision and recall plots are shown in Figure 2a.
"Each line corre-sponds to a (translation lexicon, matching algo-rithm) pair, showing average precision and re-call over the ten folds as the threshold varies."
"The plots should be read from left to right, with recall increasing as the threshold is lowered."
"When many resources are used, the technique is highly adept at selecting the translation pairs."
"TMTL alone outperforms DICT alone, proba-bly due to its coverage of domain-specific terms."
"The competitive linking algorithm lags behind MWBM in most cases, though its performance was slightly better in the case of TMTL."
"In the case of UTL, for recall up to 0.8251, the thresholded MWBM matching had significantly higher precision than the thresholded competi-tive linking matching at a comparable level of recall (based on a Sign Test over the ten cross-validation folds, p &lt; 0.01)."
Table 1 shows the maximum performance (by F-score) for each translation lexicon under MWBM and competitive linking.
"Next, I performed an experiment to test the technique’s robustness to noise."
"In this case, the test corpus contained 300 known transla-tion pairs (again, two-sentence texts)."
From 0 to 2700 additional English texts and the same number of Chinese texts were added.
"These “noise” texts were from the same corpus and were guaranteed not to be aligned with each other. [Footnote_7] The length filter eliminated 48.6% of the 9,000,000 possible pairs in the cross-product, keeping 95.7% of the true pairs."
"7 In general, robustness to noise will depend on the source of the noise and how much the noise looks like the true translations. Hence the results presented here may be better or worse than those achieved in specific applications to which this technique might be applied, depending on those factors, filtering, etc."
"The filtered pairs were tsim-scored using UTL, then the MWBM was computed."
Precision and recall are plotted for various levels of noise in Fig-ure 2b. [Footnote_8] Only in the highest-noise condition ( nk = 0.1) do we observe a situation where a sufficiently strict threshold cannot be used to guarantee an extracted corpus of (nearly) ar-bitrarily high precision.
"8 Experiments were carried out for the TMTL and DICT translation lexicons, and also under competitive linking. Space does not permit a full discussion, though it is worth mentioning that, as in the noiseless experi-ment, UTL outperformed the others, likewise MWBM outperformed competitive linking."
"For example, if 90% precision is required, 88.3%, 60.3%, and 43.7% recall can be guaranteed when nk is 1, 0.5, and 0.25, respectively."
"These experiments show that with a strict threshold this technique is capable of produc-ing a highly precise matching of parallel text from a noisy corpus, though attainable recall levels drop as noise is added."
Performance can be boosted by incorporating additional bilingual resources.
"Finally, even a fast, greedy approxi- mation to the best matching can be useful."
An important application of translation recog-nition is the construction of parallel text cor-pora.
"One source of raw text in this task is the World-Wide Web, for which several paral-lel text search systems currently exist[REF_CITE]."
"These systems propose candidate pairs of pages, which are then classified as either translation-ally equivalent or not."
"The STRAND system[REF_CITE], for example, uses structural markup information from the pages, without looking at their content, to attempt to align them."
"If the tsim technique can provide a classi-fier that rivals or complements the structural one, using as it does an entirely orthogonal set of features, then perhaps a combined classifier could provide even greater reliability."
"In addi-tion, custom-quality parallel corpora could be generated from comparable corpora that lack structural features."
This experiment also shows that tsim is scalable to larger texts.
"In this experiment, the language pair is English-French."
"Multiple sources for the translation lex-icon are used in a manner similar to Section 4.1. • An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [Footnote_9]"
9 This dictionary was generated using a dictionary de-rived from one available[URL_CITE]
It contains morphological variants but does not include character accents.
Each n-to-m entry was processed by stoplisting and then extracting all word-pairs in the remaining cross-product as in section 4.1.
"Result: 39,348 word pairs, 9,045 of which contain two words present in the corpora. • A word-to-word translation model[REF_CITE]trained on a verse-aligned Bible using MWBM (15,548 verses, averaging 25.5[REF_CITE].4 French words after tokenization)."
"Result: 13,762 word pairs. • English-French cognate pairs, identified us-ing the method[REF_CITE]."
"Space does not permit a full description of the tech-nique; I simply note that cognates were iden-tified by thresholding on a specially-trained string-similarity score based on language-specific character-to-character weights. [Footnote_10] Re-sult: 35,513 word pairs."
"10 Tiedemann trained these weights using a list of known cognates; I use a noisy list of weighted translation pairs (specifically, TMTL) Hence the resources required to extract cognates in this way are no different from those required for the translation model."
"An additional set of 11,264 exact string matches were added."
These entries are quite noisy.
"The union of these translation lexicons consists of 68,003 unique word pairs."
The experiment used only this union translation lexicon.
"In order to compare tsim with structural simi-larity scoring, I applied it to 325 English-French web-document pairs."
"These were the same pairs for which human evaluations were carried out[REF_CITE]. [Footnote_11] Note that this is not a match-ing task; the documents are presented as candi-date pairs, and there is no competition among pages for matches in the other language."
11 One additional pair was thrown out because it con-tained compressed data; it is assumed that pair would not pass a language identification filter.
"At dif-ferent thresholds, a κ score of agreement (with each of Resnik’s (1999) two judges and their intersection) may be computed for comparison with Resnik’s STRAND system, along with re-call and precision against a gold standard (for which I use the intersection of the judges—the set of examples where the judges agreed)."
"Note that recall in this experiment is relative to the candidate set proposed by the STRAND search module, not the WWW or even the set of pages encountered in the search."
The estimate of tsim (MWBM on the words in the document pair) is not computationally feasible for very large documents and transla-tion lexicons.
"In preliminary comparisons, I found that representing long documents by as few as their first 500 words results in excel-lent performance on the κ measure."
This al-lows O(1) estimation of tsim for two documents: look only at the first (fixed) n words of each document.
"Further, the competitive linking al-gorithm appears to be as reliable as MWBM."
The results reported here approximated tsim in using competitive linking on the first 500 words.
Maximizing κ on this set yielded a value of τ = 0.15. [Footnote_12] κ scores against each judge and their intersection were then com-puted at that threshold on the test set (the re-maining 293 pairs).
12 One could select such a threshold to maximize any objective function over the development set.
"These are compared to κ scores of the STRAND system, on the same test set, in Table 3."
"In every case, the tsim classifier agreed more strongly with the human evalua-tions."
"At τ = 0.15, precision was 0.680 and re-call was 0.921, F = 0.782 (on the same set, STRAND structural classification achieved 0.963 precision and 0.684 recall, F = 0.800)."
"Figure 3 shows κ, precision, and recall plotted against τ."
"The success of this approach suggests a way to construct parallel corpora from any large, seg-mented comparable corpus: start with a trans-lation model estimated on a small, high-quality parallel text, and a core dictionary; then extract document pairs with high similarity (tsim) and add them to the parallel corpus."
"Next, esti-mate word-level translational equivalence em-pirically from the enlarged corpus and update the translation lexicon; extract documents iter-atively."
"The experiments presented here show that, even in highly noisy search spaces, tsim can be used with a threshold to extract a high-precision parallel corpus at moderate recall."
It is worth noting that the STRAND classi-fier and the tsim classifier disagreed 15% of the time on the test set.
"A simple combination by disjunction (i.e., “(X, Y ) is a translation pair if either classifier says so”) yields precision 0.768, recall 0.961, F = 0.854, and κ (with the judges’ intersection) at 0.878."
"In future work, more so-phisticated combinations of the two classifiers might integrate the advantages of both."
I have proposed a language-independent ap-proach to the detection of translational equiva-lence in texts of any size that works at various bilingual resource levels.
"Fast, effective approx-imations have also been described, suggesting scalability to very large corpora."
"Notably, tsim is adaptable to any probabilistic model of trans-lational equivalence, because it is an instance of a model-independent definition of similarity."
"The core of the technique is the computation of optimal matchings at two levels: between words, to generate the tsim score, and between texts, to detect translation pairs."
I have demonstrated the performance of this technique on English-Chinese and English-French. [Footnote_13]
13 Comparable experiments using another version of the score showed performance for English-Spanish on the matching task to be even better than for English-Chinese (using that same score)[REF_CITE].
"It is capable of pulling parallel texts out of a large multilingual collection, and it rivals the performance of structure-based ap-proaches to pair classificati[REF_CITE], having better κ agreement with human judges."
"We describe an LR parser of parts-of-speech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with lim-ited amount of backtracking."
"We evaluate the parser using the Penn Treebank show-ing that the method yield very fast parsers with at least reasonable accuracy, confirm-ing the intuition that LR parsing benefits from the use of rich grammars."
"The LR approach for parsing has long been con-sidered for natural language parsing[REF_CITE], but it was not until a more recent past, with the advent of corpus-based techniques made possible by the availability of large treebanks, that parsing results and evalu-ation started being reported[REF_CITE]."
"The appeal of LR parsing[REF_CITE]derives from its high capacity of postponement of struc-tural decisions, therefore allowing for much of the spurious local ambiguity to be automatically dis-carded."
"But it is still the case that conflicts arise in the LR table for natural language grammars, and in large quantity."
The key question is how one can use the contextual information contained in the parsing stack to cope with the remaining (local) ambiguity manifested as conflicts in the LR tables.
The afore-mentioned work has concentrated on LR parsing for CFGs which has a clear deficiency in making avail-able sufficient context in the LR states.[REF_CITE]hints at the relevance of rich grammars on this respect.
They use Tree Adjoining Grammars (TAGs)[REF_CITE]to defend the possibility of granular in-cremental computations in LR parsing.
"Incidentally or not, they make use of disambiguation contexts that are only possible in a state of a conceptual LR parser for a rich grammar formalism such as TAG, but not for a CFG."
"Concrete LR-like algorithms for TAGs have only recently been proposed[REF_CITE], though their evaluation was restricted to the quality of the parsing table (see also[REF_CITE]for earlier at-tempts)."
"In this paper, we revisit the LR parsing technique, applied to a rich grammar formalism: TAG."
"Follow-ing[REF_CITE], conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configu-ration: a stack, representing the control memory of the LR parser, and a lookahead sequence, here lim-ited to one symbol. [Footnote_1]"
"1 Un[REF_CITE]’s approach who tries to transpose PCFG probabilities to LR tables, facing difficulties which, to the best of our knowledge, have not been yet solved to content (cf. also[REF_CITE])."
"However, while Briscoe and Carroll invested on massive parallel computation of the possible parsing paths, with pruning and posterior ranking, we ex- periment with a simple greedy depth-first technique with limited amount of backtracking, that resembles to a certain extent the commitment/recovery models from the psycholinguistic research on human lan-guage processing, supported by the occurrence of “garden paths”. 2"
"We use the Penn Treebank WSJ corpus, release [Footnote_2][REF_CITE], to evaluate the approach."
"2 See, e.g.,[REF_CITE]for a survey on human sentence comprehension."
Table 1 shows the architecture of our parsing appli-cation.
"We extract a TAG from a piece of the Penn Treebank, the training corpus, and submit it to an LR parser generator."
The same training corpus is used again to extract statistical information that is used by the driver as follows.
The grammar gen-eration process generates as a subproduct the TAG derivation trees for the annotated sentences compat-ible with the extracted grammar trees.
This deriva-tion tree is then converted into the sequence of LR parsing actions that would have to be used by the parser to generate exactly that analysis.
"A parser ex-ecution simulation is then performed, guided by the obtained sequence of parsing actions, collecting the statistical information defined in Section 3."
"In possession of the LR table, grammar and sta-tistical information, the parser is then able to parse fast natural language sentences annotated for parts-of-speech."
"Our target grammar is extracted with a customized version of the extractor defined[REF_CITE], which we will not describe here."
"However, a key as-pect to mention is that grammar trees are extracted by factoring of recursion."
"Even constituents anno-tated flat in the Treebank are first given a more hi-erarchical, recursive structure."
Therefore the trees generated during parsing will be richer than those in the Treebank.
We will return to this point later.
"Before grammar extraction, Treebank labels are merged to allow for the generation of a more com-pact grammar and parsing table, and to concentrate statistical information (e.g., NN and NNS; NNP and NNPS; all labels for finite verb forms)."
The gram- mar extractor assigns a plausibility judgment to each extracted tree.
"When a tree is judged implausible, it is discarded from the grammar, and so are the sen-tences in the training corpus in which the tree is used."
This reduced our training corpus by about 15 %.
We used the grammar generator[REF_CITE].
"In this section we only present a parsing example, to il-lustrate the kinds of action inserted in the generated tables; details concerning how the table is generated are omitted."
Consider the TAG fragment in Figure 2 for simple sentences with modal and adverb adjunc-tion.
Figure 3 contains a derivation for the sentence “John will leave soon”.
We sketch in Figure 4 the sequence of actions ex-ecuted by the parser.
"Technically, each element of the stack would be a pair: the second element being an LR state; the first can be either a grammar sym-bol or an embedded stack."
"Although the state is the only relevant component of the pair for parsing, in the figure, for presentational purposes, we omit the state and instead show only the symbol/embedded stack component (despite the misleading presence of embedded stacks, actions are executed in con-stant time)."
Stacks are surrounded by square brack-ets.
Only the parts of speech have been represented.
The bpack action is not standard in LR parsing.
It represents an earlier partial commitment for struc-ture.
"In its first appearance, it acknowledges that some material will be adjoined to a VP that domi-nates the element at the top of the stack (in fact it dominates the topmost elements, where is the second parameter of bpack)."
The material is then enclosed in a substack (the subscript VP at the left bracket is for presentation purposes only; that infor-mation is in fact in the LR state that would pair with withthe substack  ).
"The next line contains another bpack, that proposes another adjunction, dom-inating the VB and the RB."
Reductions for auxiliary trees leave no visible trace in the stack after they are executed.
The parser executes reductions in a bot-tom up order with respect to the derivation tree [Footnote_3]
"3 Notice that the notion of top-down/bottom-up parsing can-not be defined on the derived tree for TAGs, unless one wants to break the actions into independent subatomic units, e.g., single-level context-free-like expansions."
In this session we focus on how to resolve con-flicts in the generated parsing table to obtain a single “best” parse for each input sentence.
At each step of the parsing process the driver is faced with the task of choosing among a certain number of available ac-tions.
"At the end, the sequence of actions taken will uniquely define one derivation tree that represents the chosen syntactic analysis."
"In our approach, the parser proceeds greedily try-ing to compute a single successful sequence of ac-tions."
"Whenever it fails, a backtracking strategy is employed to re-conduce the parser to an alternative path, up to a certain limited number of attempts pro-vided as a parameter to the parser."
Choices are made locally.
"We have not tried to maximize any global measure, such as the probability of the sentence, the probability of a parse given a string, etc."
An instantaneous description (or “configuration”) can be characterized by two components: 1.
"The current content of the stack, which in-cludes the current (automaton) state; 2."
The suffix of the input not yet shifted into the stack.
The basic parsing approach has two main compo-nents: 1.
A strategy for ranking the actions available at any given instantaneous description; 2.
A greedy strategy for computing the sequence of parsing actions.
At each instantaneous de-scription:
Choose the highest-ranked action that has not been tried yet and execute it.
"If there is no action available, then back-track: move back to one of the instanta-neous descriptions previously entered and choose an alternative action."
Let   be  the number  of positions in the stack and  be the sequence of states in the positions. [Footnote_4] is the current state.
4 Recall each position in the stack contains a pair where the second element is an automaton state and the first element is either a symbol or an embedded stack.
"Let  be the lookahead symbol, the leftmost symbol in the not yet shifted suffix."
Let be the (finite) set of possible actions given by the grammar.
We use two basic ranking functions:  and  .
"The first, naive one considers only the cur-rent automaton state (the state at the top of the stack, ) and the lookahead symbol,  ."
"It is a poor statis-tic but it does not suffer of sparse data problems in our training corpus, and hence is used for smooth-ing."
"For any instantaneous description as described above, we trivially define the  &quot;! , for any ac-tion $#% , as a probability estimate for , given the current state  and lookahead symbol  : & apos; &amp;  ( ) * +!&quot;! .- /  , * &quot;!21 ., - /   2 +! , .- /   / &apos; 76 ! is the number of times where / &apos; 76 occurs in an instantaneous description when 0 parsing the annotated corpus."
It can be observed that individual actions tend to depend on different additional states in the stack.
"For a shift action there is no reason to assume that state, say,  , is not."
"But for a : the previous state is particularly relevant ;/ , 6 or, or &lt; &apos; that , action - we - should suspect that the state from where the = 0 action is taken is highly relevant."
"So, for instance, the action ?&gt; 3 reduce 0 , where the number of non-empty leaves of 0 is , would have strong de-pendency on the state @ ."
An approximation of its rank would be:  &apos;A&amp; ) @ * &quot;! .
This &quot;! observation is certainly not new.
A similar ranking function is in fact used by Briscoe and Carroll.
"How-ever, an inconsistency immediately arises: we can-not compare probabilities of events drawn on dis-tinct sample spaces."
"For instance, if we have two competing actions , an C3&gt; reduce 0 , and , a shift, and we affirm that depends on @ , then, it can-not be true that the shift does not depend on @ ."
"In fact, it has to be the case that it depends on @ as much as ."
"One could suggest calculating the prob-abilities for all actions conditional to the same set of states, D @  ."
"But, in general, we have many more than two actions to decide among."
And they are likely to stress their dependencies on different past states.
"We see that this is not going to work; the number of dependencies, and hence the number of parameters, will grow too big."
"A striking solution arises from a notable fact from LR parsing theory for CFGs: If state contains an action reduce p, where &apos; is a production with sym-bols on its right side, then, the pair ( @ ), from the instantaneous description, uniquely identifies the entire sequence @ HGA@   ."
"Although this property does not hold for the parser generation algorithm we are using, it is still a good approxima-tion to the true statistical dependencies. [Footnote_5]"
"5 That the property does not hold in the algorithm we are us-ing is a consequence of the way the goto function for adjunction is defined[REF_CITE], as a function of two states (instead of just a simple transition in the automaton). A detailed argument is beyond the scope of this paper and can be found[REF_CITE], available upon request to the author. That the statement is a good approximation to the true statistical dependencies fol-lows from: (1) adjuncts (that can cause distinct states to inter-vene between the considered pair), are generally regarded as not restricting the syntactic possibilities of the clause they ad-join to; and (2) in practice, the intermediate states at positions that could be distinct for theoretically different sequences most often have exactly the same characteristics, i.e., they are likely to “accidentally” collapse to the same state."
"We can use this “approximately correct” property in our benefit: “if state contains an action re-duce or bpack for a number of leaves , then the dependency on the sequence @ JGA@   can be approximated by a dependency on the pair ( @ )”."
"So a natural candidate for the second @ , where state P *! to= maxbe considered )  has anisactionthe statebpack(X,l) for some X or has some action reduce for a tree with non-empty leaves E ."
We define our second ranking func-tion based on that. & apos; &amp;    +!
"A )  @ +! , .- /   @ &quot;!"
"S- /    * @ +! ,"
"We have a (quite narrow) notion of confidence for parsing paths: as long as our sequence of decisions allows the parser to proceed we trust the sequence, and if it has taken us to an acceptance state, we be-lieve we have the correct parse."
"On the other hand, a crash is our other binary value for confidence, an untrustworthy parsing sequence."
"In these cases, we know we have made a bad decision somewhere in the path and that we have to start again from that point by following another alternative."
"This is a backtracking strategy, although not in the common sense of a depth-first walk, (i.e., exploring all the possibilities left before undoing some earlier action)."
We want to explore strategies of intelligently guess-ing the restart point.
We use a simple strategy of returning to the deci-sion point that left the highest amount of probability mass unexplored.
"In order to implement it, we main-tain a tree with the entire parsing attempts’ history."
"There will be one path from the root correspond-ing to the current parsing attempt, the leaf being the current instantaneous description."
All other leaves correspond to instantaneous descriptions that have been abandoned (crashing points).
"If the current leaf crashes, all nodes in the tree (except for the leaves) compete to be the restart point."
Keeping all nodes in the tree alive is a direct consequence of the fact that we do not intend to do exhaustive backtracking.
"We trade space (a tree instead of just a sequence) for time: presumably, by doing smart backtracking we can find a successful path by trying only a fraction of the possible ones."
"Moreover, we want to find the best (or approximately best) successful path, and a crashing point is a good point to re-evaluate the pro-cess."
"Limits may be added through parameters, so that the parser may give up after a certain amount of attempts or time."
"In addition to the instantaneous description, each node contains a record of the alternatives previously tried (the edges to its child nodes in the tree) with their corresponding probabilities, plus a ranked list of the alternatives not yet tried."
In particular we maintain the probability mass left unexplored in a node: the sum of the probabilities of the actions not yet tried.
Notice that alternatives already tried are in-directly kept alive through their corresponding child nodes.
Let A! be the set of actions not yet tried at node .
The probability mass left is &apos; !  N   ! . [Footnote_6] The backtracking process chooses # ! for which  ! is maxi-mum (efficiently maintained &apos; using ! 3 a  priority queue ! and).Then we update &apos; ! start another branch in the tree by executing .
"6 Where can be any of the ranking functions, at the state , applied to . Elsewhere in the paper we have omitted the explicit reference to the state."
"We evaluated the approach using the Penn Treebank WSJ Corpus, release 2[REF_CITE], using Sections 2 to 21 for grammar extraction and training, section 0 for development and 23 for testing."
Only parts-of-speech were used as input. [Footnote_7] [Footnote_8]
"7 However, two new categories were defined: one for time nouns, namely those that appear in the Penn Treebank as heads of constituents marked “TMP” (for temporal); another for the word “that”. This is similar[REF_CITE]’s and[REF_CITE]’s definition of a separate category for auxiliary verbs."
"8 We also included some punctuation symbols among the ter-minals such as comma, colon and semicolon. They are extracted into the grammar as if they were regular modifiers. Their main use is in guiding parsing decisions."
A smoothed ranking function is defined as fol-lows:
"K  , - / 0  * @ +  ! . if then  else"
The best was experimentally determined to be 1.
"That is: in general, even if there is minimal evidence of the context including the second state, the statis-tics using this context lead to a better result than us-ing only one state."
For each sentence there is an initial parsing at-tempt using only  as the ranking function with an a maximum of 500 backtracking occurrences K .
"If,it fails, then the sentence is parsed using  with a maximum of 3,000 backtracking occurrences."
"In table 1 we report the following figures for the development set (Section 0) and test set[REF_CITE]: %failed is the percentage of sentences for which the parser failed (in the two attempts). ! recall and prec. are the labeled parsing re-call and precision, respectively, as defined[REF_CITE](slightly different[REF_CITE]). is their harmonic average. tput is the average number of sentences parsed per second."
"To obtain the average, the number of sentences submitted as input (not only those that parsed successfully) is divided by the to-tal time (excluded the time overhead before it starts parsing the first sentence)."
"The programs were run under Linux, in a PC with a[REF_CITE]MHz processor."
The first two lines report the measures for the parsed sentences as originally generated by the parser.
We purposefully do not report precision.
"As we mentioned in the beginning of the paper, the parser assigns to the sentences a much richer hierar-chical structure than the Penn Treebank does, which is penalized by the precision measure."
"The reason for such increase in structure is not quite a particular decision of ours, but a consequence of using a sound grammar under the TAG grammatical formalism. [Footnote_9]"
"9 By sound we mean a grammar that properly factors recur-sion in one way or another. Grammars have been extracted where the right side of a rule reflects exactly each single-level expansion found in the Penn Treebank. We are also aware of a few alternatives in grammatical formalisms that could capture such flatness, e.g., sister adjuncti[REF_CITE]."
"However, having concluded our manifesto, we un-derstand that algorithms that try to keep precision as high as the recall necessarily have losses in recall compared to if they ignored the precision, and there-fore in order to have fair comparison with them and to improve the credibility of our results, we flattened the parse trees in a post-processing step, using a sim-ple rule-based technique on top of some frequency measures for individual grammar trees gathered[REF_CITE]and the result is presented in the bottom lines of the table."
The most salient positive result is that the parser is able to parse sentences at a rate of about 20 sen-tences per second.
Most of the medium-to-high ac-curacy parsers take at least a few seconds per sen-tence under the same conditions. [Footnote_10]
"10 The fastest parser we are aware of is from BBN, with a throughput of 3 sentences per second under similar conditions. We also emphasize we have not taken particular care with opti-mization for speed yet."
This is an enor-mous speed-up.
"As for the accuracy, it is not far from the top performing parser for parts-of-speech that we are aware of, reported  by (Sima’an, 2000): recall/precision =  91"
Perhaps the most similar work to ours is Briscoe and Carroll’s (1993; 1995; 1992; 1996).
"They im-plemented a standard LR parser for CFGs, and a probabilistic method for conflict resolution similar to ours in that the decisions are conditioned to the LR states but with different methods."
"In particular, they proceed in a parallel way accumulating proba-bilities along the paths and using a Viterbi decoder at the end."
"Their best published result is of unlabeled bracket recall and precision of 74 % and 73 %, pars-ing the Susanne corpus."
"Since the unlabeled bracket measures are much easier than the ones we are re-porting, on labeled brackets, our results are clearly superior to theirs."
Also the Susanne corpus is easier than the Penn Treebank.
There are two additional points we want to make.
"One is with respect to the ranking function  , based on two states."
"It is a very rich statistic, but suffers from sparse data problems."
"Parsing section 0 with only this statistics (no form of smoothing), with backtracking limit of 3,000 attempts, we could parse only 31 % of the sentences but the non-flattened recall was 88.33 %, which is quite high for using only parts-of-speech."
The second observation is that K when parsing with the smoothed function  most of the sentences use very few number of back-tracking attempts.
In fact a graph relating number of backtracking attempts with number of sentences that parse using attempts shows an   relation characteristic of Zipf’s law.
"Most of the time spent with computation is spent with sentences that either fail parsing or parse with difficulty, showing low bracketing accuracy."
The results presented here suggest that: (1) the use of a rich grammar as the underlying formalism for the LR techniques makes available enough informa-tion to the driver so as to allow for a greedy strategy to achieve reasonable parsing accuracy. (2) LR pars-ing allows for very fast parsing with at least reason-able accuracy.
"The approach seems to have much yet to be ex-plored, mostly to improve the accuracy side."
In par-ticular we have not yet come with a solid approach to lexicalization.
"Using words (as opposed to pos tags) as the terminals of the grammar to be pre-compiled leads to an explosion in the size of the table: not only the average number of transitions per state grows, but also the number of states it-self grows wildly."
"One very promising approach for a partial solution is to expand the set of terminals by adding some selected syntactic sub-categories that have distinguished syntactic behavior, as we re-ported in this paper for time nouns, or by individuat-ing frequent words with peculiar behavior, as we did for the word “that”."
"Although we have also done some initial work on a more general approach to clustering words according to their syntactic distri-bution, they are not still adequate for our purposes."
"Finally, an earlier simple experiment of adding a de-pendency on the lookahead’s word (recall that  in  and  was the pos tag only), gave us a small improvement of about a couple of percents in the accuracy measures."
"A limited amount of parallelism is an important topic to be considered, perhaps together with a bet-ter notion (non-binary) of confidence."
"The high reli-ability of  , suggests that we should look for a way to enrich the parsing table."
LR parser for the full class of TAGs is prob-lematic.
The bpack action of early structural com-mitment is involved in most of the decision points where the wrong action is taken.
"We are currently working on a version of the LR parser for a subclass of TAGs, the Tree Insertion Grammars[REF_CITE], for which efficient true LR parsers can be obtained."
"In the field of empirical natural language processing, researchers constantly deal with large amounts of marked-up data; whether the markup is done by the researcher or someone else, human nature dictates that it will have errors in it."
This paper will more fully characterise the problem and discuss whether and when (and how) to correct the errors.
The discussion is illustrated with specific examples involving function tagging in the Penn treebank.
"A cliché, but in the field of empir-ical natural language processing, we know it to be true: on a daily basis, we work with large corpora created by, and often marked up by, humans."
"Falli-ble as ever, these humans have made errors."
"For the errors in content, be they spelling, syntax, or some-thing else, we can hope to build more robust systems that will be able to handle them."
But what of the errors in markup?
"In this paper, we propose a system for cataloguing corpus errors, and discuss some strategies for dealing with them as a research community."
"Finally, we will present an example (function tagging) that demon-strates the appropriateness of our methods."
"The easiest errors, which we have dubbed “Type A”, are those that can be automatically detected and fixed."
"These typically come up when there would be multiple reasonable ways of tagging a certain in-teresting situation: the markup guidelines arbitrarily choose one, and the human annotator unthinkingly uses the other."
"The canonical example of this sort of thing is the treebank’s LGS tag, representing the “logical sub-ject” of a passive construction."
"It makes a great deal of sense to put this tag on the NP object of the ‘by’ construction; it makes almost as much sense to tag the PP itself, especially since (given a choice) most other function tags are put there."
The tree-bank guidelines specifically choose the former: “It attaches to the NP object of by and not to the PP node itself.”[REF_CITE]
"Nevertheless, in sev-eral cases the annotators put the tag on the PP, as shown in Figure 1."
We can automatically correct this error by algorithmically removing the LGS tag from any such PP and adding it to the object thereof.
The unifying feature of all Type A errors is that the annotator’s intent is still clear.
"In the LGS case, the annotator managed to clearly indicate the pres-ence of a passive construction and its logical subject."
"Since the transformation from what was marked to what ought to have been marked is straightforward and algorithmic, we can easily apply this correction to all data."
"Next, we come to the Type B errors, those which are fixable but require human intervention at some point in the process."
"In theory, this category could include errors that could be found automatically but require a human to fix; this doesn’t happen in prac-tice, because if an error is sufficiently systematic that an algorithm can detect it and be certain that it is in fact an error, it can usually be corrected with cer-tainty as well."
"In practice, the instances of this class of error are all cases where the computer can’t detect the error for certain."
"However, for all Type B errors, once detected, the correction that needs to be made is clear, at least to a human observer with access to the annotation guidelines."
Certain Type B errors are moderately easy to find.
"When annotators misunderstand a complicated markup guideline, they mismark in a somewhat pre-dictable way."
"While not being totally systematically detectable, an algorithm can leverage these patterns to extract a list of tags or parses that might be incor-rect, which a human can then examine."
Some errors of this type (henceforth “Type B [Footnote_1] ”) include: • VBD / VBN.
"1 There is a subclass of this error which is Type A: when we find a VBD whose grandparent is a VP headed by a form of ‘have’, we can deterministically retag it as VBN."
"Often the past tense form of a verb (VBD) and its past participle (VBN) have the same form, and thus annotators sometimes mis-take one for the other, as in Figure 2."
"Some such cases are not detectable, which is why this is not Type A. 1 • IN / RB / RP."
"There are specific tests and guide-lines for telling these three things apart, but fre-quently a preposition (IN) is marked when an adverb (RB) or particle (PRT) would be more appropriate."
"If an IN is occurring somewhere other than under a PP, it is likely to be a mistag."
"Occasionally, an extracted list of maybe-errors will be “perfect”, containing only instances that are ac-tually corpus errors."
"This happens when the pat-tern is a very good heuristic, though not necessarily valid (which is why the errors are Type B 1 , and not Type A)."
"When filing corrections for these, it is still best to annotate them individually, as the correc-tions may later be applied to an expanded or modi- fied data set, for which the heuristic would no longer be perfect."
Other fixable errors are pretty much isolated.
"These isolated errors (resulting, presumably, from a typo or a moment of inattention on the part of the annotator) are not in any way predictable, and can be found essentially only by examining the out-put of one’s algorithm, analysing the “errors”, and noticing that the treebank was incorrect, rather than (or in addition to) the algorithm."
We will call these Type B [Footnote_2] .
"2 In particular, the annotators of sections 05, 09, 12, 17, 20, and 24 used IN sometimes, while the others tagged all occurrences of ‘ago’ as adverbs."
"Sometimes, there is a construction that the markup guidelines writers didn’t think about, didn’t write up, or weren’t clear about."
"In these cases, annota-tors are left to rely on their own separate intuitions."
"This leaves us with markup that is inconsistent and therefore clearly partially in error, but with no obvi-ous correction."
"There is really very little to be done about these, aside from noting them and perhaps controlling for them in the evaluation."
Some Type C errors in the treebank include: • ‘ago’.
English’s sole postposition seems to have given annotators some difficulty.
"Lacking a postposition tag, many tagged such occurrences of ‘ago’ as a preposition (IN); others used the adverb tag (RB) exclusively. 2 Since some occur-rences really are adverbs, this just makes a big mess. • ADVP-MNR."
The MNR tag is meant to be ap-plied to constituents denoting manner or instru-ment.
"Some annotators (but not all) seemed to decide that any adverbial phrase (ADVP) headed by an ‘-ly’ word must get a MNR tag, ap-plying it to words like ‘suddenly’, ‘significantly’, and ‘clearly’."
"The hallmark of a Type C error is that even what ought to be correct isn’t always clear, and as a result, any plan to correct a group of Type C errors will have to first include discussion on what the correct markup guideline should be."
"In order to effect these changes in some communi-cable way, we have implemented a program called tsed, by analogy with and inspired by the already prevalent tgrep search program. [Footnote_3] It takes a search pattern and a replacement pattern, and after find-ing the constituent(s) that match the search pattern, modifies them and prints the result."
"3 tgrep was written by Richard Pito of the University of Pennsylvania, and comes with the treebank."
"For those al-ready familiar with tgrep search syntax, this should be moderately intuitive."
"To the basic pattern-matching syntax of tgrep, we have added a few extra restriction patterns (for spec-ifying sentence number and head word), as well as a way of marking nodes for later reference in the re-placement pattern (by simply wrapping a constituent in square brackets instead of parentheses)."
"The replacement syntax is somewhat more com-plicated, because wherever possible we want to be able to construct the new trees by reference to the old tree, in order to preserve modifiers and structure we may not know about when we write the pattern."
"For full details of the program’s abilities, consult the program documentation, but here are the main ones: • Relabelling."
Constituents can be relabelled with no change to any of their modifiers or children. • Tagging.
"A tag can be added to or removed from a constituent, without changing any modifiers or children. • Reference."
Constituents in the search pattern can be included by reference in the replacement pattern. • Construction.
"New structure can be built by specifying it in the usual S-expression format, e.g. (NP (NN snork))."
Usually used in combi-nation with Reference patterns.
"Along with tsed itself, we distribute a Perl pro-gram wsjsed to process treebank change scripts like the following:"
This script would make a batch modification to the zeroth sentence of the 29th file in section 24.
The batch includes two corrections: the first matches a noun phrase (NP) whose sister is an ADJP and whose parent is a VP headed by the word ‘keep’.
The matched NP node is replaced by a (created) S node whose children will be that very NP and its sister ADJP.
The second correction then finds an NP that ends in the word ‘markets’ and marks it with the SBJ function tag.
Distributing changes in this form is important for two reasons.
"First of all, by giving changes in their minimal, most general forms, they are small and easy to transmit, and easy to merge."
"Perhaps more im-portantly, since corpora are usually copyrighted and can only be used by paying a fee to the controlling body (usually LDC or ELDA), we need a way to dis-tribute only the changes, in a form that is useless without having bought the original corpus."
"Scripts for tsed, or for wsjsed, serve this purpose."
These programs are available from our website. 4
"Now that we have analysed the different types of errors that can occur and how to correct them, we can discuss when and whether to do so."
"In virtually all empirical NLP work, the training set is going to encompass the vast majority of the data."
"As such, it is usually impractical for a human (or even a whole lab of humans) to sit down and revise the training."
"Type A errors can be corrected easily enough, as can some Type B 1 errors whose heuristics have a high yield."
"Purely on grounds of practicality, though, it would be difficult to effect significant cor-rection on a training set of any significant size (such as for the treebank)."
"Practicality aside, correcting the training set is a bad idea anyway."
"After expending an enormous ef-fort to perfect one training set, the net result is just one correct training set."
"While it might make certain things easier and probably will improve the results of most algorithms, those improved results will not be valid for those same algorithms trained on other, non-perfect data; the vast majority of corpora will still be noisy."
"If a user of an algorithm, e.g. an ap-plication developer, chooses to perfect a training set to improve the results, that would be helpful, but it is important that researchers report results that are likely to be applicable more generally, to more than one training set."
"Furthermore, robustness to errors in the training, via smoothing or some other mech-anism, will also make an algorithm robust to sparse data, the ever-present spectre that haunts nearly ev-ery problem in the field; thus eliminating all errors in the training ought not to have as much of an effect on a strong algorithm."
"Testing data is another story, however."
"In terms of practicality, it is more feasible, as the test set is usu-ally at least an order of magnitude smaller than the training."
"More important, though, is the issue of fairness."
"We need to continue using noisy training data in order to better model real-world use, but it is unfair and unreasonable to have noise in the gold standard [Footnote_5] , which causes an algorithm to be penalised where it is more correct than the human annotation."
"5 Sometimes more of a pyrite standard, really."
"As performance on various tasks improves, it be-comes ever more important to be able to correct the testing data."
"A ‘mere’ 1% improvement on a result of 75% is not impressive, as it represents just a 4% re-duction in apparent error, but the same 1% improve-ment on a result of 95% represents a 20% reduction in apparent error!"
"In the end, a noisy gold standard sets an upper bound of less than 100% on perfor-mance, which is if nothing else counterintuitive."
"Of course, we cannot simply go about changing the corpus willy-nilly."
We refer the reader to chapter 7 of David Magerman’s thesis (1994) for a cogent dis-cussion of why changing either the training or the testing data is a bad idea.
"However, we believe that there are now some changed circumstances that war-rant a modification of this ethical dictum."
"First, we are not allowed to look at testing data."
"How to correct it, then?"
An initial reaction might be to “promise” to forget everything seen while cor-recting the test corpus; this is not reasonable.
"Another solution exists, however, which is nearly as good and doesn’t raise any ethical questions."
"Many research groups already use yet another sec-tion, separate from both the training and testing, as a sort of development corpus. [Footnote_6] When developing an algorithm, we must look at some output for de-bugging, preliminary evaluation, and parameter esti-mation; so this development section is used for test-ing until a piece of work is ready for publication, at which point the “true” test set is used."
"6 In the treebank, this is usually section 24."
"Since we are all reading this development output already anyway, there is no harm in reading it to perform corrections thereon."
"In publication, then, one can publish the results of an algorithm on both the unaltered and corrected versions of the development section, in ad-dition to the results on the unaltered test section."
We can then presume that a corrected version of the test corpus would result in a perceived error reduc-tion comparable to that on the development corpus.
"Another problem mentioned in that chapter is of a researcher quietly correcting a test corpus, and pub-lishing results on the modified data (without even noting that it was modified)."
"The solution to this is simple: any results on modified data will need to acknowledge that the data is modified (to be hon-est), and those modifications need to be made public (to facilitate comparisons by later researchers)."
"For Type A errors fixed by a simple rule, it may be rea-sonable to publish them directly in the paper that gives the results. [Footnote_7]"
7 The rule we used to fix the LGS problem noted in section 2.1 is as follows:
"For Type B errors, it would be more reasonable to simply publish them on a web-site, since there are bound to be a large number of them. [Footnote_8] {24*-bg}&lt;&lt;EOF"
8[REF_CITE]corrections made to section 24 are available[URL_CITE]
NP !- LGS &gt; (PP - LGS) - LGS
PP - LGS !
"Finally, we would like to note that one of the rea-sons Magerman was ready to dismiss error in the testing was that the test data had “a consistency rate much higher than the accuracy rate of state-of-the-art parsers”."
This is no longer true.
"As multiple researchers each begin to impose their own corrections, there are several new issues that will come up."
"First of all, even should everyone pub-lish their own corrections, and post comparisons to previous researchers’ corrected results, there is some danger that a variety of different correction sets will exist concurrently."
"To some extent this can be mit-igated if each researcher posted both their own cor-rections by themselves, and a full list of all correc-tions they used (including their own)."
"Even so, from time to time these varied correction sets will need to be collected and merged for the whole community to use."
"More difficult to deal with is the fact that, in-evitably, there will be disputes as to what is correct."
Sometimes these will be between the treebank ver-sion and a proposed correction; there will probably also be cases where multiple competing corrections are suggested.
There really is no good systematic policy for dealing with this.
"Disputes will have to be handled on a case-by-case basis, and researchers should probably note any disputes to their correc-tions that they know of when publishing results, but beyond that it will have to be up to each researcher’s personal sense of ethics."
"In all cases, a search-and-replace pattern should be made as general as possible (without being too general, of course), so that it interacts well with other modifications."
"Various researchers are already working with (deterministically) different versions of corpora—with new tags added, or empty nodes re-moved, or some tags collapsed, for instance, not to mention other corrections already performed—and it would be a bad idea to distribute corrections that are specific to one version of these."
"When in doubt, one should favour the original form of the corpus, natu-rally."
"The final issue is not a practical problem, but an observation: once a researcher publishes a correction set, any further corrections by other researchers are likely to decrease the results of the first researcher’s algorithm, at least somewhat."
This is due to the fact that that researcher is usually not going to notice corpus errors when the algorithm errs in the same way.
"This unfortunate consequence is inevitable, and hopefully will prove minor."
"As a sort of case study in the meta-algorithms pre-sented in the previous sections, we will look at the problem of function tagging in the treebank."
"We trained this algorithm on sec-tions 02–21 of the treebank and ran it on section 24 (the development corpus), then analysed the output."
"First, we printed out every constituent with a func-tion tag error."
"We then examined the sentence in which each occurred, and determined whether the error was in the algorithm or in the treebank, or elsewhere, as reported in Table 1."
"Of the errors we examined, less than half were due solely to an algo-rithmic failure in the function tagger itself."
"The next largest category was parse error: this function tag-ging algorithm requires parsed input, and in these cases, that input was incorrect and led the function tagger astray; had the tagger received the treebank parse, it would have given correct output."
"In just under a fifth of the reported “errors”, the algorithm was correct and the treebank was definitely wrong."
"The remainder of cases we have identified either as Type C errors—wherein the tagger agreed with many training examples, but the “correct” tag agreed with many others—or at least “dubious”, in the cases that weren’t common enough to be systematic inconsis-tencies but where the guidelines did not clearly pre-fer the treebank tag over the tagger output, or vice versa."
"Next, we compiled all the noted treebank errors and their corrections."
"The most common correc-tion involved simply adding, removing, or changing a function tag to what the algorithm output (with a net effect of improving our score)."
"However, it should be noted that when classifying reported errors, we examined their contexts, and in so doing discovered other sorts of treebank error."
Mistags and misparses did not directly affect us; some function tag correc-tions actually decreased our score.
"All corrections were applied anyway, in the hope of cleaner evalua-tions for future researchers."
"In total, we made 235 corrections, including about 130 simple retags."
"Finally, we re-evaluated the algorithm’s output on the corrected development corpus."
"Table 2 shows the resulting improvements. [Footnote_9] Precision, recall, and F-measure are calculated as[REF_CITE]."
"9 We did not run corrections on, nor do we show re-sults for, Blaheta and Charniak’s “misc” grouping, both because there were very many of them in the reported error list and because they are very frequently wrong in the treebank."
"The false error rate is simply the percent by which the error is reduced; in terms of the per-formance on the treebank version (t) and the fixed version (f),"
False error = f − t × 100% 1.0 − t
This is the percentage of the reported errors that are due to treebank error.
"The topicalisation result is nice, but since the TPC tag is fairly rare (121 occurrences in section 24), these numbers may not be robust."
"It is interesting, though, that the false error rate on the two major tag groups is so similar—roughly 20% in precision and 5% in recall for each, leading to 10% in F-measure."
"First of all, this parallelism strengthens our assertion that the false error rate, though calculated on a devel-opment corpus, can be presumed to apply equally to the test corpus, since it indicates that the hu-man missed tag and mistag rates may be roughly constant."
"Second, the much higher improvement on precision indicates that the majority of treebank er-ror (at least in the realm of function tagging) is due to human annotators forgetting a tag."
"In this paper, we have given a new characterisation of the sorts of noise one finds in empirical NLP, and a roadmap for dealing with it in the future."
"For many of the problems in the field, the state of the art is now sufficiently advanced that evaluation error is becoming a significant factor in reported results; we show that it is correctable within the constraints of practicality and ethics."
"Although our examples all came from the Penn treebank, the taxonomy presented is applicable to any corpus annotation project."
"As long as there are typographical errors, there will be Type B errors; and unclear or counterintuitive guidelines will forever engender Type A and Type C errors."
"Furthermore, we expect that the experimental improvement shown in Section 5 will be reflected in projects on other an-notated corpora—perhaps to a lesser or greater de-gree, depending on the difficulty of the annotation task and the prior performance of the computer sys-tem."
"An effect of the continuing improvement of the state of the art is that researchers will begin (or have begun) concentrating on specific subproblems, and will naturally report results on those subprob-lems."
"These subproblems are likely to involve the complicated cases, which are presumably also more subject to annotator error, and are certain to involve smaller test sets, thus increasing the performance ef-fect of each individual misannotation."
"As the sizes of the subproblems decrease and their complexity in-creases, the ability to correct the evaluation corpus will become increasingly important."
We describe and evaluate the application of a spectral clustering technique[REF_CITE]to the unsupervised clustering of German verbs.
Our previous work has shown that standard clustering techniques succeed in inducing Levin-style semantic classes from verb subcategorisa-tion information.
But clustering in the very high dimensional spaces that we use is fraught with technical and conceptual difficulties.
"Spec-tral clustering performs a dimensionality reduc-tion on the verb frame patterns, and provides a robustness and efficiency that standard cluster-ing methods do not display in direct use."
The clustering results are evaluated according to the alignment[REF_CITE]between the Gram matrix defined by the cluster output and the corresponding matrix defined by a gold standard.
Standard multivariate clustering technology (such as k-Means) can be applied to the problem of inferring verb classes from information about the estimated prevalence of verb frame patterns (Schulte im[REF_CITE]).
But one of the problems with multivariate clustering is that it is something of a black art when applied to high-dimensional natural language data.
"The search space is very large, and the available techniques for searching this large space do not offer guarantees of global optimality."
"In response to this insight, the present work applies a spectral clustering technique[REF_CITE]to the verb frame patterns."
At the heart of this approach is a transformation of the original input into a set of orthogonal eigen-vectors.
"We work in the space defined by the first few eigenvectors, using standard clustering techniques in the reduced space."
"The spectral clustering technique has been shown to han-dle difficult clustering problems in image pro-cessing, offers principled methods for initializ-ing cluster centers, and (in the version that we use) has no random component."
The clustering results are evaluated accord-ing to their alignment with a gold standard.
"Alignment is Pearson correlation between corre-sponding elements of the Gram matrices, which has been suggested as a measure of agreement between a clustering and a distance measure[REF_CITE]."
We are also able to use this measure to quantify the fit between a clustering result and the distance matrix that serves as input to clustering.
The evidence is that the spectral technique is more effective than the methods that have previously been tried.
The data in question come from a subcate-gorization lexicon induced from a large Ger-man newspaper corpus (Schulte im[REF_CITE]).
The verb valency information is pro-vided in form of probability distributions over verb frames for each verb.
"There are two condi-tions: the first with 38 relatively coarse syntac-tic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus preposition), re-sulting in 171 possible frame types."
The verb frame types contain at most three arguments.
"Possible arguments in the frames are nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns (r), prepo-sitional phrases (p), expletive es (x), non-finite clauses (i), finite clauses (s-2 for verb second clauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and cop- ula constructions (k)."
"For example, subcate-gorising a direct (accusative case) object and a non-finite clause would be represented by nai."
Table 1 shows an example distribution for the verb glauben ‘to think/believe’.
"The more delicate version of subcategorisation frame was done by distributing the frequency mass of prepositional phrase frame types (np, nap, ndp, npr, xp) over the prepositional phrases, accord-ing to their frequencies in the corpus."
"Preposi-tional phrases are referred to by case and prepo-sition, such as ‘Dat.mit’, ‘Akk.für’."
"The present work uses the latter, more delicate, verb valency descriptions. 3 Problems with standard clustering Our previous work on the valency data ap-plied k-Means (a standard technique) to the task of inducing semantic classes for German verbs (Schulte im[REF_CITE])."
"We compared the results of k-Means clustering with a gold standard set prepared according to the principles of verb classification advocated[REF_CITE], and reported on the sensitivity of the classes to linguistically motivated ”lesion-ing” of the input verb frame."
The verb classes we used are listed in Table 2.
The verb classes are closely related to Levin’s English classes.
They also agree with the Ger-man verb classification[REF_CITE]as far as the relevant verbs appear in his less extensive semantic ‘fields’.
The rough glosses and the references to Levin’s classes in the table are primarily to aid the intuition of non-native speakers of German.
Clustering can be thought of as a process that finds a discrete approximation to a distance measure.
"For any data set of n items over which a distance measure is defined, the Gram matrix is the symmetric n-by-n matrix whose elements M ij are the distances between items i and j. The diagonal elements M ii of this matrix will all be 0."
Every clustering corresponds to a block-diagonal Gram matrix.
Clustering n items into k classes corresponds to the choice of an order-ing for the labels on the axes of the Gram matrix and the choice of k − 1 change points marking the boundaries between the blocks.
"Thus, the search space of clusters is very large."
The avail-able techniques for searching this large space do not (and probably cannot) offer guarantees of global optimality.
"Standard nostrums include transformations of the underlying data, and the deployment of different strategies for initializ-ing the cluster centers."
"These may produce in-tuitively attractive clusters, but when we apply these ideas to our verb frame data many ques-tions remain, including the following: • When the solutions found by clustering dif-fer from our intuitions, is this because of failures in the features used, the clustering techniques, or the intuitions? • How close are the local optima found by the clustering techniques to the best solutions in the space defined by the data? • Is it even appropriate to use frequency in-formation for this problem?"
"Or would it suffice to characterize verb classes by the pattern of frames that their members can inhabit, without regard to frequency? • Does the data support some clusters more strongly than others?"
Are all the distinc-tions made in classifications such as Levin’s of equal validity?
"In response to these questions, the present paper describes an application to the verb data of a particular spectral clustering technique[REF_CITE]."
At the heart of this approach is a transformation of the original verb frame data into a set of orthogonal eigenvectors.
"We work in the space defined by the first few eigenvec-tors, using standard clustering techniques in the transformed space."
The spectral clustering algorithm takes as in-put a matrix formed from a pairwise similarity function over a set of data points.
"In image segmentation two pixels might be declared sim-ilar if they have similar intensity, similar hue or similar location, or if a local edge-detection al-gorithm does not place an edge between them."
"The technique is generic, and[REF_CITE]point out, originated not in computer science or AI but in molecu-lar physics."
"Most of the authors nevertheless “adopt the terminology of image segmentation (i.e. the data points are pixels and the set of pix-els is the image), keeping in mind that all the results are also valid for similarity-based clus-tering”[REF_CITE]."
"Our natural lan-guage application of the technique uses straight-forward similarity measures based on verb frame statistics, but nothing in the algorithm hinges on this, and we plan in future work to elabo-rate our similarity measures."
"Although there are several roughly analogous spectral cluster-ing techniques in the recent literature[REF_CITE], we use the algorithm[REF_CITE]because it is simple to implement and understand."
"Here are the key steps of that algorithm: Given a set of points S = {s 1 , . . . , s n } in a high dimensional space. 1. Form a distance matrix D ∈ R 2 ."
"For[REF_CITE]this distance measure is Euclidean, but other measures also make sense. 2. Transform the distance matrix to an affin- D 2ij ity matrix by A ij = exp(− σ 2 ) if i =6 j, 0 if i = j. The free parameter σ 2 controls the rate at which affinity drops off with dis-tance. 3. Form the diagonal matrix D whose (i,i) el-ement is the sum of A’s ith row, and create the matrix L = D −1/2 AD −1/2 . 4. Obtain the eigenvectors and eigenvalues of L. 5. Form a new matrix from the vectors associ-ated with the k largest eigenvalues."
Choose k either by stipulation or by picking suffi-cient eigenvectors to cover 95% of the vari-ance 1 . 6.
Each item now has a vector of k co-ordinates in the transformed space.
Nor-malize these vectors to unit length. 7.
Cluster in k-dimensional space.
"Following[REF_CITE]we use k-Means for this purpose, but any other algorithm that pro-duces tight clusters could fill the same role."
In[REF_CITE]an analysis demon-strates that there are likely to be k well-separated clusters.
We carry out the whole procedure for a range of values of σ.
"In our experiments σ is searched in steps of 0.[Footnote_1] from 0.01 to 0.059, since that always sufficed to find the best aligned set of clusters."
1 Srini Parthasarathy suggested this dodge for allow-ing the eigenvalues to select the appropriate number of clusters.
"If σ is set too low no useful eigenvec-tors are returned, but this situation is easy to detect."
We take the solution with the best align-ment (see definition below) to the (original) dis-tance measure.
"This is how[REF_CITE]choose the best solution, while[REF_CITE]explain that they choose the solution with the tightest clusters, without being specific on how this is done."
"In general it matters how initialization of cluster centers is done for algorithms like k- Means.[REF_CITE]provide a neat ini-tialization strategy, based on the expectation that the clusters in their space will be orthog-onal."
"They select the first cluster center to be a randomly chosen data point, then search the remaining data points for the one most orthog-onal to that."
"For the third data point they look for one that is most orthogonal to the previ-ous two, and so on until sufficient have been obtained."
"We modify this strategy slightly, re-moving the random component by initializing n times, starting out at each data point in turn."
"This is fairly costly, but improves results, and is less expensive than the random initializations and multiple runs often used with k-Means."
We clustered the verb frames data using our ver-sion of the algorithm[REF_CITE].
"To cal-culate the distance d between two verbs v 1 and v 2 we used a range of measures: the cosine of the angle between the two vectors of frame proba-bilities, a flattened version of the cosine mea-sure in which all non-zero counts are replaced by 1.0 (labelled bcos, for binarized cosine, in Ta-ble 3), and skew divergence, recently shown as an effective measure for distributional similar-ity[REF_CITE]."
"This last is defined in terms of KL-divergence, and includes a free weight pa-rameter w, which we set to 0.9, following[REF_CITE], Skew-divergence is asymmetric in its ar-guments, but our technique needs a symmet-ric measure,so we calculate it in both directions and use the larger value."
"Table 3 contains four results for each of three distance measures (cos,bcos and skew)."
The first line of each set gives the results when the spec-tral algorithm is provided with the prior knowl-edge that k = 14.
"The second line gives the results when the standard k-Means algorithm is used, again with k = 14."
"In the third line of each set, the value of k is determined from the eigenvalues, as described above."
"The final line of each set gives the results when the standard algorithm is used, but k is set to the value selected for that distance measure by the spectral method."
"For standard k-Means, the initialization strategy[REF_CITE]does not ap-ply (and does not work well in any case), so we used 100 random replications of the initial-ization, each time initializing the cluster centers with k randomly chosen data points."
We report the result that had the highest alignment with the distance measure (cf. Section 5.1).[REF_CITE]provide analysis in-dicating that their MNcut algorithm (another spectral clustering technique) will be exact when the eigenvectors used for clustering are piecewise constant.
"Figure 1 shows the top 16 eigenvectors of a distance matrix based on skew divergence, with the items sorted by the first eigenvector."
"Most of the eigenvectors appear to be piecewise constant, suggesting that the con-ditions for good performance in clustering are indeed present in the language data."
Many of the eigenvectors appear to correspond to a par-tition of the data into a small number of tight clusters.
Taken as a whole they induce the clus-terings reported in Table 3.
Pearson correlation between corresponding ele-ments of the Gram matrices has been suggested as a measure of agreement between a cluster-ing and a distance measure[REF_CITE].
"Since we can convert a clustering into a distance measure, alignment can be used in a number of ways, including comparison of clus-terings against each other."
"For evaluation, three alignment-based mea-sures are particularly relevant: • The alignment between the gold standard and the distance measure reflects the pres-ence or absence in the distance measure of evidential support for the relationships that the clustering algorithm is supposed to infer."
This is the column labelled “Sup-port” in Table 3. • The alignment between the clusters in-ferred by the algorithm and the distance measure reflects the confidence that the al-gorithm has in the relationships that it has chosen.
This is the column labelled “Con-fidence” in Table 3. • The alignment between the gold standard and the inferred clusters reflects the quality of the result.
This is the column labelled “Quality” in Table 3.
"We hope that when the algorithms are confi-dent they will also be right, and that when the data strongly supports a distinction the algo-rithms will find it."
Table 3 contains our data.
The columns based on various forms of alignment have been dis-cussed above.
"Clusterings are also sets of pairs, so, when the Gram matrices are discrete, we can also provide the standard measures of pre-cision, recall and F-measure."
"Usually it is ir-relevant whether we choose alignment or the standard measures, but the latter can yield un-expected results for extreme clusterings (many small clusters or few very big clusters)."
The remaining columns provide these conventional performance measures.
"For all the evaluation methods and all the distance measures that we have tried, the algo-rithm[REF_CITE]does better than di-rect clustering, usually finding a clustering that aligns better with the distance measure than does the gold standard."
"Deficiencies in the re-sult are due to weaknesses in the distance mea-sures or the original count data, rather than search errors committed by the clustering al-gorithm."
"Skew divergence is the best distance measure, cosine is less good and cosine on bina-rized data the worst."
"All three alignment measures can be applied to a clustering as whole, as above, or restricted to a subset of the Gram matrix."
These can tell us how well each verb and each cluster matches the distance measure (or indeed the gold stan-dard).
To compute alignment for a verb we cal- culate Spearman correlation over its row of the Gram matrix.
"For a cluster we do the same, but over all the rows corresponding to the cluster members."
"The second column of Table 4, la-belled “Support”, gives the contribution of that verb to the alignment between the gold stan-dard clustering and the skew-divergence dis-tance measure (that is, the empirical support that the distance measure gives to the human-preferred placement of the verb)."
"The third col-umn, labelled “Confidence” contains the contri-bution of the verb to the alignment between the skew-divergence and the clustering inferred by our algorithm (this is the measure of the confi-dence that the clustering algorithm has in the correctness of its placement of the verb, and is what is maximized by Ng’s algorithm as we vary σ)."
"The fourth column, labelled “Correct-ness”, measures the contribution of the verb to the alignment between the inferred cluster and the gold standard (this is the measure of how correctly the verb was placed)."
To get a feel for performance at the cluster level we mea-sured the alignment with the gold standard.
We merged and ranked the lists proposed by skew divergence and binary cosine.
"The figure of merit, labelled “Score” is the geometric mean of the alignments for the members of the clus-ter."
"The second column, labelled “Method”, indicates which distance measure or measures produced this cluster."
Table 5 shows this rank-ing.
Two highly ranked clusters (Emotion and a large subset of Weather) are selected by both distance measures.
"The highest ranked clus-ter proposed only by binary cosine is a sub- set of Position, but this is dominated by skew-divergence’s correct identification of the whole class (see Table 2 for a reminder of the defini-tions of these classes)."
The systematic superi-ority of the probabilistic measure suggests that there is after all useful information about verb classes in the non-categorical part of our verb frame data.
Levin’s[REF_CITE]classification has pro-voked several studies that aim to acquire lex-ical semantic information from corpora using cues pertaining to mainly syntactic structure ([REF_CITE]; Schulte im[REF_CITE]).
Other work has used Levin’s list of verbs (in conjunction with related lexical resources) for the creation of dictionaries that exploit the systematic correspondence between syntax and meaning[REF_CITE].
"Most statistical approaches, including ours, treat verbal meaning assignment as a semantic clustering or classification task."
The underly-ing question is the following: how can corpus information be exploited in deriving the seman-tic class for a given verb?
"Despite the unify-ing theme of using corpora and corpus distri-butions for the acquisition task, the approaches differ in the inventory of classes they employ, in the methodology used for inferring semantic classes and the specific assumptions concerning the verbs to be classified (i.e., can they be pol-ysemous or not).[REF_CITE]use grammati-cal features (acquired from corpora) to classify verbs into three semantic classes: unergative, unaccusative, and object-drop."
These classes are abstractions of Levin’s[REF_CITE]classes and as a result yield a coarser classification.
The classifier used is a decision tree learner. (Schulte im[REF_CITE]) uses subcategoriza-tion information and selectional restrictions to cluster verbs[REF_CITE]compatible se-mantic classes.
Subcategorization frames are in-duced from the BNC using a robust statistical parser[REF_CITE].
The selec-tional restrictions are acquired using Resnik’s[REF_CITE]information-theoretic measure of selectional association which combines distribu-tional and taxonomic information in order to formalise how well a predicate associates with a given argument.
We have described the application to natural language data of a spectral clustering technique[REF_CITE]closely related to kernel PCA[REF_CITE].
We have presented evidence that the dimensionality reduction in-volved in the clustering technique can give k- Means a robustness that it does not display in direct use.
"The solutions found by the spec-tral clustering are always at least as well-aligned with the distance measure as is the gold stan-dard measure produced by human intuition, but this does not hold when k-Means is used directly on the untransformed data."
"Since we work in a transformed space of low dimensionality, we gain efficiency, and we no longer have to sum and average data points in the original space associated with the verb frame data."
"In principle, this gives us the freedom to use, as is standardly done with SVMs[REF_CITE], extremely high dimensional representations for which it would not be convenient to use k-Means directly."
We could for instance use features which are derived not from the counts of a single frame but of two or more.
"This is linguistically desirable, since Levin’s verb classes are defined primarily in terms of alternations rather than in terms of single frames."
We plan to explore this possibility in future work.
"It is also clearly against the spirit[REF_CITE]to insist that verbs should belong to only one cluster, since, for example, both the Ger-man “dämmern” and the English “dawn” are clearly related both to verbs associated with weather and natural phenomena (because of “Day dawns.”) and to verbs of cognition (be-cause of “It dawned on Kim that . .. ”)."
"In or-der to accommodate this, we are exploring the consequences of replacing the k-Means step of our algorithm with an appropriate soft cluster-ing technique."
We present a bootstrapping method that uses strong syntactic heuristics to learn semantic lexicons.
"The three sources of information are appositives, compound nouns, and ISA clauses."
"We apply heuris-tics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training."
Results on WSJ articles and a pharmaceutical cor-pus show that this method obtains high precision and finds a large number of terms.
Syntactic structure helps us understand the seman-tic relationships between words.
"Given a text cor-pus, we can use knowledge about syntactic struc-tures to obtain semantic knowledge."
"For example, Hearst[REF_CITE]learned hyponymy relation-ships by collecting words in lexico-syntactic expres-sions, such as “NP, NP, and other NPs”, and Roark and Charniak[REF_CITE]gener-ated semantically related words by applying statisti-cal measures to syntactic contexts involving apposi-tives, lists, and conjunctions."
"Exploiting syntactic structures to learn semantic knowledge holds great promise, but can run into problems."
"First, lexico-syntactic expressions that explicitly indicate semantic relationships (e.g., “NP, NP, and other NPs”) are reliable but a lot of semantic information occurs outside these expressions."
"Sec-ond, general syntactic structures (e.g., lists and con-junctions) capture a wide range of semantic rela-tionships."
"For example, conjunctions frequently join items of the same semantic class (e.g., “cats and dogs”), but they can also join different seman-tic classes (e.g., “fire and ice”)."
Some researchers[REF_CITE]have applied statistical methods to identify the strongest semantic associations.
"This approach has produced reasonable results, but the accuracy of these techniques still leaves much room for improve-ment."
"We adopt an intermediate approach that learns semantic lexicons using strong syntactic heuristics, which are both common and reliable."
"We have identified certain types of appositives, compound nouns, and identity (ISA) clauses that indicate spe-cific semantic associations between words."
We em-bed syntactic heuristics in a bootstrapping process and present empirical results demonstrating that this bootstrapping process produces high-quality seman-tic lexicons.
"In another set of experiments, we in-corporate a co-training[REF_CITE]mechanism to combine the hypotheses generated by different types of syntactic structures."
"Co-training produces a synergistic effect across different heuris-tics, substantially increasing the coverage of the lex-icons while maintaining nearly the same level of ac-curacy."
The goal of our research is to automatically gener-ate a semantic lexicon.
"For our purposes, we de-fine a semantic lexicon to be a list of words with semantic category labels."
"For example, the word “bird” might be labeled as an ANIMAL and the word “car” might be labeled as a VEHICLE ."
"Semantic lexicons have proven to be useful for many lan- guage processing tasks, including anaphora resolu-ti[REF_CITE], prepositional phrase attachment[REF_CITE], information extracti[REF_CITE], and question answering[REF_CITE]."
"Some general-purposes semantic dictionaries al-ready exist, such as WordNet[REF_CITE]."
"Word-Net has been used for many applications, but it may not contain the vocabulary and jargon needed for specialized domains."
"For example, WordNet does not contain much of the vocabulary found in medical texts."
"In previous research on semantic lexicon in-duction, Roark and Charniak[REF_CITE]showed that 3 of every 5 words learned by their system were not present in WordNet."
"Further-more, they used relatively unspecialized text cor-pora: Wall Street Journal articles and terrorism news stories."
"Our goal is to develop techniques for seman-tic lexicon induction that could be used to enhance existing resources such as WordNet, or to create dic-tionaries for specialized domains."
Several techniques have been developed to gen-erate semantic knowledge using weakly supervised learning techniques.
Hearst[REF_CITE]ex-tracted information from lexico-syntactic expres-sions that explicitly indicate hyponymic relation-ships.
Hearst’s work is similar in spirit to our work in that her system identified reliable syntac-tic structures that explicitly reveal semantic associa-tions.
Meta-bootstrapping[REF_CITE]is a semantic lexicon learning technique very differ-ent from ours which utilizes information extraction patterns to identify semantically related contexts.
"Named entity recognizers (e.g.,[REF_CITE]) can be trained to recognize proper names associated with semantic categories such as PER - SON or ORGANIZATION , but they typically are not aimed at learning common nouns such as “surgeon” or “drugmaker”."
"Several researchers have used some of the same syntactic structures that we exploit in our research, namely appositives and compound nouns."
"For ex-ample, Riloff and Shepherd[REF_CITE]developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind."
Roark and Char-niak[REF_CITE]followed up on this work by using a parser to explicitly capture these structures.
Caraballo[REF_CITE]also exploited these syntactic structures and applied a co-sine vector model to produce semantic groupings.
"In our view, these previous systems used “weak” syn-tactic models because the syntactic structures some-times identified desirable semantic associations and sometimes did not."
"To compensate, statistical mod-els were used to separate the meaningful semantic associations from the spurious ones."
"In contrast, our work aims to identify “strong” syntactic heuristics that can isolate instances of general structures that reliably identify the desired semantic relations."
"For the purposes of this research, we will define two distinct types of lexicons."
"One lexicon will con-sist of proper noun phrases, such as “Federal Avi-ation Administration”."
We will call this the PNP (proper noun phrase) lexicon.
"The second lexicon will consist of common (non-proper) nouns, such as “airplane”."
We will call this the GN (general noun) lexicon.
The reason for creating these distinct lexi-cons is that our algorithm takes advantage of syntac-tic relationships between proper nouns and general nouns.
Our goal is to build a semantic lexicon of words that belong to the same semantic class.
"More specifi-cally, we aim to find words that have the same hyper-nym, for example “dog” and “frog” would both have the hypernym ANIMAL . [Footnote_1]"
"1 The appropriate granularity of a set of semantic classes, or the organization of a semantic hierarchy, is always open to debate. We chose categories that seem to represent important and relatively general semantic distinctions."
We will refer to words that have the same immediate hypernym as semantic sib-lings.
We hypothesize that some syntactic structures can be used to reliably identify semantic siblings.
"We have identified three candidates: appositives, com-pound nouns, and identity clauses whose main verb is a form of “to be” (we will call these ISA clauses)."
"While these structures often do capture semantic sib-lings, they frequently capture other types of seman-tic relationships as well."
Therefore we use heuristics to isolate subsets of these syntactic structures that consistently contain semantic siblings.
Our heuris-tics are based on the observation that many of these structures contain both a proper noun phrase and a general noun phrase which are co-referent and usu-ally belong to the same semantic class.
"In the fol-lowing sections, we explain the heuristics that we use for each syntactic structure, and how those struc-tures are used to learn new lexicon entries."
Appositives are commonly occurring syntactic structures that contain pairs of semantically related noun phrases.
"A simple appositive structure con-sists of a noun phrase (NP), followed by a comma, followed by another NP, where the two NPs are co-referent."
"However, appositives often signify hyper-nym relationships (e.g., “the dog, a carnivorous ani-mal”)."
"To identify semantic siblings, we only use appos-itives that contain one proper noun phrase and one general noun phrase."
"For example, “George Bush, the president” or “the president, George Bush”."
"The-oretically, such appositives could also indicate a hy-pernym relationship (e.g., “George Bush, a mam-mal”), but we have found that this rarely happens in practice."
Compound nouns are extremely common but they can represent a staggering variety of semantic rela-tionships.
We have found one type of compound noun that can be reliably used to harvest seman-tic siblings.
"We loosely define these compounds as “GN + PNP” noun phrases, where the compound noun ends with a proper name but is modified with one or more general nouns."
Examples of such com-pounds are “violinist James Braum” or “software maker Microsoft”.
"One of the difficulties with rec-ognizing these constructs, however, is resolving the ambiguity between adjectives and nouns among the modifiers (e.g., “violinist” is a noun)."
We only use constructs in which the GN modifier is unambigu-ously a noun.
Certain “to be” clauses can also be harvested to extract semantic siblings.
"We define an ISA clause as an NP followed by a VP that is a form of “to be”, followed by another NP."
"These identity clauses also exhibit a wide range of semantic relationships, but harvesting clauses which contain one proper NP and one general NP can reliably identify noun phrases of the same semantic class."
"We found that this struc-ture yields semantic siblings when the subject NP is constrained to be a proper NP and the object NP is constrained to be a general NP (e.g., “Jing Lee is the president of the company”)."
Figure 1 illustrates the bootstrapping model for each of the three syntactic structures.
"Initially, the lex-icons contain only a few manually defined seed words: some proper noun phrases and some general nouns."
The syntactic heuristics are then applied to the text corpus to collect potentially “harvestable” structures.
"Each heuristic identifies structures with one proper NP and one general NP, where one of them is already present in the lexicon as a member of a desired semantic class."
The other NP is then as-sumed to belong to the same semantic class and is added to a prospective word list.
"Finally, statistical filtering is used to divide the prospective word lists into exclusive and non-exclusive subsets."
We will describe the motivation for this in Section 3.2.2.
"The exclusive words are added to the lexicon, and the bootstrapping process repeats."
"In the remainder of this section, we explain how the bootstrapping pro-cess works in more detail."
The input to our system is a small set of seed words for the semantic categories of interest.
"To identify good seed words, we sorted all nouns in the corpus by frequency and manually identified the most frequent nouns that belong to each targeted se-mantic category."
Each bootstrapping iteration alternates between using either the PNP lexicon or the GN lexicon to grow the lexicons.
"As a motivating example, as-sume that (1) appositives are the targeted syntactic structure (2) bootstrapping begins by using the PNP lexicon, and (3) PEOPLE is the semantic category of interest."
The system will then collect all appositives that contain a proper noun phrase known to be a per-son.
"So if “Mary Smith” belongs to the PNP lexicon and the appositive “Mary Smith, the analyst” is en-countered, the head noun “analyst” will be learned as a person."
"The next bootstrapping iteration uses the GN lex-icon, so the system will collect all appositives that contain a general noun phrase known to be a person."
"If the appositive “John Seng, the financial analyst” is encountered, then “John Seng” will be learned as a person because the word “analyst” is known to be a person from the previous iteration."
"The boot-strapping process will continue, alternately using the PNP lexicon and the GN lexicon, until no new words can be learned."
We treat proper noun phrases and general noun phrases differently during learning.
"When a proper noun phrase is learned, the full noun phrase is added to the lexicon."
"But when a general noun phrase is learned, only the head noun is added to the lexi-con."
This approach gives us generality because head nouns are usually (though not always) sufficient to associate a common noun phrase with a semantic class.
"Proper names, however, often do not exhibit this generality (e.g., “Saint Louis” is a location but “Louis” is not)."
"However, using full proper noun phrases can limit the ability of the bootstrapping process to acquire new terms because exact matches are relatively rare."
"To compensate, head nouns and modifying nouns of proper NPs are used as predictor terms to recognize new proper NPs that belong to the same semantic class."
We identify reliable predictor terms using the evidence and exclusivity measures that we will de-fine in the next section.
"For example, the word “Mr.” is learned as a good predictor term for the person cat-egory."
These predictor terms are only used to clas-sify noun phrases during bootstrapping and are not themselves added to the lexicon.
"Our syntactic heuristics were designed to reliably identify words belonging to the same semantic class, but some erroneous terms still slip through for vari-ous reasons, such as parser errors and idiomatic ex-pressions."
Perhaps the biggest problem comes from ambiguous terms that can belong to several seman-tic classes.
"For instance, in the financial domain “leader” can refer to both people and corporations."
"If “leader” is added to the person lexicon, then it will pull corporation terms into the lexicon during subsequent bootstrapping iterations and the person lexicon will be compromised."
"To address this problem, we classify all candidate words as being exclusive to the semantic category or non-exclusive."
"For example, the word “president” nearly always refers to a person so it is exclusive to the person category, but the word “leader” is non-exclusive."
Only the exclusive terms are added to the semantic lexicon during bootstrapping to keep the lexicon as pure (unambiguous) as possible.
The non-exclusive terms can be added to the final lexicon when bootstrapping is finished if polysemous terms are acceptable to have in the dictionary.
Exclusivity filtering is the only step that uses statistics.
Two measures determine whether a word is exclusive to a semantic category.
"First, we use an evidence measure:"
"Evidence(w, c) = SS ww,c where S w is the number of times word w was found in the syntactic structure, and S w,c is the number of times word w was found in the syntactic structure collocated with a member of category c."
The evi-dence measure is the maximum likelihood estimate that a word belongs to a semantic category given that it appears in the targeted syntactic structure (a word is assumed to belong to the category if it is collo-cated with another category member).
"Since few words are known category members initially, we use a low threshold value (.25) which simply ensures that a non-trivial proportion of instances are collo-cated with category members."
"The second measure that we use, exclusivity, is the number of occurrences found in the given cate-gory’s prospective list divided by the number of oc-currences found in all other categories’ prospective lists."
"Exclusivity(w, c) = SS ww,,¬cc where S w,c is the number of times word w was found in the syntactic structure collocated with a member of category c, and S w,¬c is the number of times word w was found in the syntactic structure collocated with a member of a different semantic class."
We apply a threshold to this ratio to ensure that the term is exclusive to the targeted semantic category.
We evaluated our system on several semantic cate-gories in two domains.
"In one set of experiments, we generated lexicons for PEOPLE and ORGANIZA -[REF_CITE]Wall Street Journal articles from the Penn Treebank[REF_CITE]."
"In the sec-ond set of experiments, we generated lexicons for PEOPLE , ORGANIZATIONS , and PRODUCTS using approximately 1350 press releases from pharmaceu-tical companies. 2"
Our seeding consisted of 5 proper nouns and 5 general nouns for each semantic category.
We used a threshold of 25% for the evidence measure and 5 for the exclusivity ratio.
"We ran the bootstrapping pro-cess until no new words were learned, which ranged from 6-14 iterations depending on the category and syntactic structure."
Table 1 shows 10 examples of words learned for each semantic category in each domain.
"The people and organization lists illustrate (1) how dramatically the vocabulary can differ across domains, and ([Footnote_2]) that the lexicons may include domain-specific word meanings that are not the most common meaning of a word in general."
2 We found these texts using Yahoo’s financial industry pages[URL_CITE]
"For example, the word “par-ent” generally refers to a person, but in a financial domain it nearly always refers to an organization."
"The pharmaceutical product category contains many nouns (e.g., drug names) that may not be in a general purpose lexicon such as WordNet."
Tables 2 and 3 show the results of our evaluation.
We ran the bootstrapping algorithm on each type of syntactic structure independently.
The Total column shows the total number of lexicon entries generated by each syntactic structure.
"The Correct column contains two accuracy numbers: X/Y. The first value (X) is the percentage of entries that were judged to be correct, and the second value (Y) is the accuracy after removing entries resulting from parser errors. [Footnote_3]"
"3 For example, our parser frequently mistags adjectives as nouns, so many adjectives were hypothesized to be people. If the parser had tagged them correctly, they would not have been allowed in the lexicon."
"The PNP lexicons were substantially larger than the GN lexicons, in part because we saved full noun phrases in the PNP lexicon but only head nouns in the GN lexicon."
"Probably the main reason, however, is that there are many more proper names associated with most semantic categories than there are general nouns."
"Consequently, we evaluated the PNP and GN lexicons differently."
"For the GN lexicons, a volun-teer (not one of the authors) labeled every word as correct or incorrect."
"Due to the large size of the PNP lexicons, we randomly sampled 100 words for each syntactic structure and semantic category and asked volunteers to label these samples."
"Consequently, the PNP evaluation numbers are estimates of the true ac-curacy."
"The Union column tabulates the results obtained from unioning the lexicons produced by the three syntactic structures independently. [Footnote_4] Although there is some overlap in their lexicons, we found that many different words are being learned."
"4 Since the number of words contributed by each syntac-tic structure varied greatly, we evaluated the Union results for the PNP lexicon by randomly sampling 100 words from the unioned lexicons regardless of which structure generated them. This maintained the same distribution in our evaluation set as exists in the lexicon as a whole. However, this sampling strat-egy means that the evaluation results in the Union column are not simply the sum of the results in the preceding columns."
"This indi-cates that the three syntactic structures are tapping into different parts of the search space, which sug-gests that combining them in a co-training model could be beneficial."
Co-training[REF_CITE]is a learn-ing technique which combines classifiers that sup-port different views of the data in a single learning mechanism.
"The co-training model allows examples learned by one classifier to be used by the other clas-sifiers, producing a synergistic effect."
The three syn-tactic structures that we have discussed provide three different ways to harvest semantically related noun phrases.
"Figure 2 shows our co-training model, with each syntactic structure serving as an independent classi-fier."
"The words hypothesized by each classifier are put into a single PNP lexicon and a single GN lex-icon, which are shared by all three classifiers."
"We used an aggressive form of co-training, where all terms hypothesized by a syntactic structure with fre-quency ≥ θ are added to the shared lexicon."
The threshold ensures some confidence in a term before it is allowed to be used by the other learners.
We used a threshold of θ=3 for the WSJ corpus and θ=2 for the pharmaceutical corpus since it is sub-stantially smaller.
"We ran the bootstrapping process until no new words were learned, which was 12 it-erations for the WSJ corpus and 10 iterations for the pharmaceutical corpus. [Footnote_5]"
"5 After co-training finished, we also added terms to the lexi-con that were hypothesized by an individual classifier with fre-quency &lt; θ if they had not previously been labeled."
"Table 4 shows the size of the learned lexicons with co-training and without co-training (i.e., running the classifiers separately)."
"In almost all cases, many ad-ditional words were learned using the co-training model."
Tables 5 and 6 show the evaluation results for the lexicons produced by co-training.
"The co-training model produced substantially better cover-age, while achieving nearly the same accuracy."
"One exception was organizations in the pharmaceutical domain, which suffered a sizeable loss in precision."
This is most likely due to the co-training loop be-ing too aggressive.
"If one classifier produces a lot of mistakes (in this case, the compound noun classi-fier), then those mistakes can drag down the overall accuracy of the lexicon."
We have presented a method for learning seman-tic lexicons that uses strong syntactic heuristics in a bootstrapping algorithm.
"We exploited three types of syntactic structures (appositives, compound NPs, and ISA clauses) in combination with heuristics to identify instances of these structures that contain both a proper and general noun phrase."
"Each syntac-tic structure generated many lexicon entries, in most cases with high accuracy."
We also combined the three classifiers using co-training.
"The co-training model increased the number of learned lexicon en-tries, while maintaining nearly the same level of ac-curacy."
One limitation of this work is that it can only learn semantic categories that are commonly found as proper nouns and general nouns.
This research illustrates that common syntactic structures can be combined with heuristics to iden-tify specific semantic relationships.
"So far we have experimented with three structures and one type of heuristic (proper NP/general NP collocations), but we believe that this approach holds promise for other semantic learning tasks as well."
"In future work, we hope to investigate other types of syntactic struc-tures that may be used to identify semantically re-lated terms, and other types of heuristics that can reveal specific semantic relationships."
This research was supported by the National Science
Foundation under award[REF_CITE].
"Thanks to Erin Davies, Brijesh Garabadu, Dominic Jones, and Henry Longmore for labeling data."
"We present a joint probability model for statistical machine translation, which au-tomatically learns word and phrase equiv-alents from bilingual corpora."
Transla-tions produced with parameters estimated using the joint model are more accu-rate than translations produced using IBM Model 4.
Most of the noisy-channel-based models used in statistical machine translation (MT)[REF_CITE]are conditional probability models.
"In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM tech-niques[REF_CITE]."
The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.
"A variety of methods are used to account for the re-ordering stage: word-based[REF_CITE], template-based[REF_CITE], and syntax-based[REF_CITE], to name just a few."
"Although these models use different generative processes to explain how translated words are re-ordered in a tar-get language, at the lexical level they are quite sim-ilar; all these models assume that source words are individually translated into target words. [Footnote_1]"
"1 The individual words may contain a non-existent element, called NULL."
We suspect that MT researchers have so far cho-sen to automatically learn translation lexicons de-fined only over words for primarily pragmatic rea-sons.
Large scale bilingual corpora with vocabu-laries in the range of hundreds of thousands yield very large translation lexicons.
Tuning the probabil-ities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.
"Unfortunately, trad-ing space requirements and efficiency for explana-tory power often yields non-intuitive results."
"Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1."
"Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c."
"Sentence pair (S2, T2) of-fers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortu-nately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM mod-els[REF_CITE]— it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4[REF_CITE], for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a. [Footnote_2]"
"2 To train the IBM-4 model, we used Giza[REF_CITE]."
"Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(x b) and p(z b) and low probability to p(x c).)"
"In this paper, we describe a translation model that assumes that lexical correspondences can be estab-lished not only at the word level, but at the phrase level as well."
"In constrast with many previous ap-proaches[REF_CITE], our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Tar-get sentences can be generated simultaneously."
"In other words, in the style[REF_CITE], we es-timate a joint probability model that can be easily marginalized in order to yield conditional probabil-ity models for both source-to-target and target-to-source machine translation applications."
The main difference between our work and that of Melamed is that we learn joint probability models of trans-lation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexi-cons but also for the automatic translation of unseen sentences.
"In the rest of the paper, we first describe our model (Section 2) and explain how it can be imple-mented/trained (Section 3)."
We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).
We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.
"In developing our joint probability model, we started out with a very simple generative story."
We assume that each sentence pair in our corpus is generated by the following stochastic process: 1. Generate a bag of concepts . 2.
"For each concept  , generate a pair of phrases  , according to the distribution  , where and each contain at least one word. 3. Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus."
"For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions."
"We do not assume that  is a hidden variable that generates the pair   , but rather that   ."
"Un-der these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts  is given by the product of all phrase-to-phrase translation probabilities, ! that yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated us-ing two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair."
"However, the same sentence pair cannot be gener-ated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”."
"Similarly, the pair can-not be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”."
"We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrases and that characterize all concepts &quot;# ."
"We denote this property us-ing the predicate %$ &apos;&amp; )*( +, ."
"Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts /. that can be linearized to (E, F)."
D 0 &apos;&amp; )( 3   (1)  9;=: ?&lt; &gt; @A&gt; CB
"Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good align-ments."
"However, this model is clearly unsuited for translating unseen sentences as it imposes no con-straints on the ordering of the phrases associated with a given concept."
"In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions."
The generative story of Model 2 is this: 1. Generate a bag of concepts . 2. Initialize E and F to empty sequences G . 3.
"Randomly take a concept + and generate a pair of phrases  , according to the dis- tribution  , where and each contain at least one word."
"Remove then + from . 4. Append phrase at the end of F. Let J be the start position of in F. 5. Insert phrase at position K in E provided that no other phrase occupies any of the positions between K and  , where gives the length of the phrase ."
We hence create the alignment between the two phrases and with probability MON
D 7 QMVUP  0  !+
RTS where U \[) ] is a position-based distortion dis- tribution. 6.
Repeat steps 3 to 5 until is empty.
"In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where 0_E`^ denotes the position of word J of phrase in sen-tence F and _0"
E`^   denotes the position in sen-tence E of the center of mass of phrase .
D 0 &amp;&apos;)( 3   9;:=?&lt; &gt; @A&gt; CB cb M 7 Q P  (2) MD U 0_E`^ + _0 E`^   S4e
"Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3."
We have tried many types of distortion models.
We eventually settled for the model discussed here be-cause it produces better translations during decod-ing.
"Since the number of factors involved in com-puting the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predis-posed to produce translations that are shorter than the Source sentences given as input."
Training the models described in Section 2 is com-putationally challenging.
"Since there is an exponen-tial number of alignments that can generate a sen-tence pair (E, F), it is clear that we cannot apply the"
EM training algorithm exhaustively.
"To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below."
"If one assumes from the outset that any phrases  and * can be generated from a con-cept  , one would need a supercomputer in order to store in the memory a table that models the   distribution."
"Since we don’t have access to comput-ers with unlimited memory, we initially learn t distri-bution entries only for the phrases that occur often in the corpus and for unigrams."
"Then, through smooth-ing, we learn t distribution entries for the phrases that occur rarely as well."
"In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus."
"Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning."
"In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability."
"Under these conditions, the evidence that a sentence pair (E, F) contributes to the fact that  are generated by the same con-cept  is given by the number of alignments that can be built between (E, F) that have a concept  that is linked to phrase in sentence E and phrase in sentence F divided by the total number of align- ments that can be built between the two sentences."
Both these numbers can be easily approximated.
"Given a sentence E of K words, there are m* ways in which the K words can be partitioned into J non-empty sets/concepts, where *m  is the Stir-ling number of second M6s kind. e  J m *Jrq 3 Sut   o"
Tx [ { (3) [
There are also *m \V|  ways in which the | words of a sentence F can be partitioned into J non-empty sets.
"Given that any words in E can be mapped } Ma :=~&gt; aB to any words in F, it follows that there are S4{e Jrq+m**\V|  alignments that can be built between two sentences (E, F) of lengths K and | , respectively."
"When a concept  generates two phrases of length  and  , respectively, there are only Kv&apos; and | &quot;v  words left to link."
"Hence, in the absence of any other information, the probabil-ity that phrases and are generated by the same concept  s; is  given s; by formula (4)."
M } a S4{e :~ M &gt; a B Jrq+*m _ * \|  (4) } a S4{e : ~ &gt; aB Jrq+*m *\V|
Note that the fractional counts returned by equa-tion (4) are only an approximation of the t distri-bution that we are interested in because the Stir-ling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.
"However, since for-mula (4) overestimates the numerator and denomi-nator equally, the approximation works well in prac-tice."
"In the second step of the algorithm, we apply equation (4) to collect fractional counts for all un-igram and high-frequency n-gram pairs in the carte-sian product defined over the phrases in each sen-tence pair (E, F) in a corpus."
We sum over all these t-counts and we normalize to obtain an initial joint distribution .
This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.
"Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time."
"Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities."
"We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging con-cepts, swapping words between concepts, and mov-ing words across concepts."
We compute the prob-abilities associated with all the alignments we gen-erate during the hillclimbing process and collect t counts over all concepts in these alignments.
We apply this Viterbi-based EM training proce-dure for a few iterations.
The first iterations estimate the alignment probabilities using Model 1.
The rest of the iterations estimate the alignment probabilities using Model 2.
"During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus."
"At the end of the training procedure, we take marginals U on the joint probability distributions and ."
This yields U conditional 0_E`^ ( _0
E`^ probability &amp;+ whichdistributionswe use forand decoding.
"When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e."
"At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build align-ments between the smallest phrases possible."
"How-ever, note that the choice made by our model is quite reasonable."
"After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing."
"The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one."
"Also note that although the joint distribution puts the second hypothesis at an advantage, the condi-tional distribution does not."
"The conditional distri- bution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”."
The condi-tional distribution mirrors perfectly our intuitions.
"For decoding, we have implemented a greedy pro-cedure similar to that proposed[REF_CITE]."
"Given a Foreign sentence F, we first pro-duce a gloss of it by selecting phrases in h i that maximize the probability 0 &amp;&apos;)( ."
We then itera-tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula 0 &amp; 0 (  .
We hillclimb by modifying an exist- &amp; ing alignment/translation through a set of operations that modify locally the aligment/translation built un-til a given time.
"These operations replace the En-glish side of an alignment with phrases of differ-ent probabilities, merge and break existing concepts, and swap words across concepts."
The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit[REF_CITE].
The language model is estimated at the word (not phrase) level.
Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrêter là .”
Each intermediate transla-tion in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.
"To evaluate our system, we trained both Giza (IBM Model 4)[REF_CITE]and our joint probability model on a French-English parallel cor-pus of 100,000 sentence pairs from the Hansard corpus."
The sentences in the corpus were at most 20 words long.
"The English side had a total of 1,073,480 words (21,484 unique tokens)."
"The French side had a total of 1,177,143 words (28,132 unique tokens)."
"For each group of 100 sentences, we manu-ally determined the number of sentences translated perfectly by the IBM model decoder[REF_CITE]and the decoder that uses the joint prob- ability model."
"We also evaluated the translations automatically, using the IBM-Bleu metric[REF_CITE]."
The results in Table 1 show that the phrased-based translation model proposed in this pa-per significantly outperforms IBM Model 4 on both the subjective and objective metrics.
The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.
"To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side."
"Also, the swap, break, and merge operations used dur-ing the Viterbi training are computationally expen-sive."
We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.
"Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases."
"The English word “not”, for example, is often translated into two French words, “ne” and “pas”."
But “ne” and “pas” al-most never occur in adjacent positions in French texts.
"At the outset of this work, we attempted to de-velop a translation model that enables concepts to be mapped into non-contiguous phrases."
But we were not able to scale and train it on large amounts of data.
The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.
"However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”)."
A number of researchers have already gone be-yond word-level translations in various MT set-tings.
"For example,[REF_CITE]uses word-level alignments in order to learn translations of non-compositional compounds."
"However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words."
"As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the data."
"In our approach, phrases are not treated differ-ently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased."
Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.
"For example, in our pre-vious work[REF_CITE], we have used a statis-tical translation memory of phrases in conjunction with a statistical translation model[REF_CITE]."
The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza[REF_CITE]and re-used in decoding.
"The decoder described[REF_CITE]starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incremen-tally, in the style described in Section 4."
"How-ever, because the decoder hill-climbs on a word-for-word translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability."
The decoder in Section 4 does not have this problem because it hill-climbs on translation model probabilities in which phrases play a crucial role.
We present Minimum Bayes-Risk word alignment for machine translation.
"This statistical, model-based approach attempts to minimize the expected risk of align-ment errors under loss functions that mea-sure alignment quality."
"We describe var-ious loss functions, including some that incorporate linguistic analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards."
"The automatic determination of word alignments in bilingual corpora would be useful for Natural Lan-guage Processing tasks such as statistical machine translation, automatic dictionary construction, and multilingual document retrieval."
"The development of techniques in all these areas would be facili-tated by automatic performance metrics, and align-ment and translation quality metrics have been pro-posed[REF_CITE]."
"However, given the difficulty of judging translation quality, it is unlikely that a single, global metric will be found for any of these tasks."
It is more likely that specialized metrics will be developed to mea-sure specific aspects of system performance.
"This is even desirable, as these specialized metrics could be used in tuning systems for particular applications."
We have applied Minimum Bayes-Risk (MBR) procedures developed for automatic speech recog-niti[REF_CITE]to word alignment of bitexts.
This is a modeling approach that can be used with statistical models of speech and language to de-velop algorithms that are optimized for specific loss functions.
"We will discuss loss functions that can be used for word alignment and show how the over-all alignment process can be improved by the use of loss functions that incorporate linguistic features, such as parses and part-of-speech tags."
We will study the problem of aligning an English sentence to a French sentence and we will use the word alignment of the IBM statistical translation models[REF_CITE].
Let and denote a pair of translated English and French sentences.
"An English word is defined as an ordered pair , where the index refers to the posi-tion of the word in the English sentence; is the vocabulary of English; and the word at position is the NULL word to which “spurious” French words may be aligned."
"Similarly, a French word is written as ."
An alignment between and is defined to be a sequence where .
"Under the alignment , the French word is connected to the English word ."
"For every alignment , we define a link set defined as whose ele-ments are given by the alignment links ."
In this section we introduce loss functions to mea-sure the quality of automatically produced align-ments.
"Suppose we wish to compare an automat-ically produced alignment to a reference align-ment , which we assume was produced by a com-petent translator."
We will define various loss func-tions that measure the quality of relative to through their link sets and .
The desirable qualities in translation are fluency and adequacy.
We assume here that both word se-quences are fluent and adequate translations but that the word and phrase correspondences are unknown.
It is these correspondences that we wish to deter-mine and evaluate automatically.
We now present two general classes of loss func-tions that measure alignment quality.
"In subsequent sections, we will give specific examples of these and show how to construct decoders that are optimized for each loss function."
The Alignment Error Rate (AER) introduced[REF_CITE]measures the fraction of links by which the automatic alignment differs from the reference alignment.
Links to the NULL word are ignored.
This is done by defining modified link sets for the reference alignment and the automatic alignment .
The reference annotation procedure allowed the human transcribers to identify which links in they judged to be unambiguous.
"In addition to the ref-erence alignment, this gives a set of sure links (S) which is a subset of ."
"AER is defined[REF_CITE](1) Since our modeling techniques require loss func-tions rather than error rates, we introduce the Align-ment Error loss function (2) We consider error rates to be “normalized” loss functions."
"We also note that, unlike AER, does not distinguish between ambiguous and unambigu-ous links."
"However, if a decoder generates an align-ment for which is zero, the AER is also zero."
"Therefore if AER is the metric of inter-est, we will design alignment procedures to mini-mize ."
We are interested in extending the Alignment Er-ror loss function to incorporate various linguistic features into the measurement of alignment quality.
The Generalized Alignment Error loss is defined as (3) where and (4)
Here we have introduced the word-to-word distance measure which compares the links and as a function of the words in the translation. refers to all loss functions that have the form of Equation 3.
Specific loss functions are determined through the choice of .
"To see the value in this, suppose is a verb in the French sen-tence and that it is aligned in the reference alignment to , the verb in the English sentence."
"If our goal is to ensure verb alignment, then can be constructed to penalize any link in the automatic align-ment in which is not a verb."
"We will later give ex-amples of distances in which is based on Part-of-Speech (POS) tags, parse tree distances, and au-tomatically determined word clusters."
"We note that the can almost be reduced to , except for the treatment of NULL in the English sentence."
We present the Minimum Bayes-Risk alignment for-mulation and derive MBR alignment procedures un-der the loss functions of Section 3.
"Given a translated pair of English-French sen-tences , the decoder produces an align-ment ."
"Relative to a reference align-ment , the decoder performance is measured as ."
Our goal is to find the decoder that has the best performance over all translated sen-tences.
This is measured through the Bayes Risk .
The ex-pectation is taken with respect to the true distribu-tion that describes “human quality” align-ments of translations as they are found in bitext.
"Given a loss function and a probability distribu-tion, it is well known that the decision rule which minimizes the Bayes Risk is given by the follow-ing expression ([REF_CITE]; Goel and"
Several modeling assumptions have been made to obtain this form of the decoder.
We do not have ac-cess to the true distribution over translations.
We therefore use statistical MT models to approximate .
"We furthermore assume that the space of alignment alternatives can be restricted to an align-ment lattice , which is a compact representation of the most likely word alignments of the sentence pair under the baseline models."
It is clear from Equation 5 that the MBR de-coder is determined by the loss function.
"The Sen-tence Alignment Error refers to the loss function that gives a penalty of 1 for any errorful alignment: , where is the indi-cator function of the set ."
The MBR decoder un-der this loss can easily be seen to be the Maximum Likelihood (ML) alignment under the MT models:.
This illustrates why we are interested in MBR decoders based on other loss functions: the ML decoder is optimal with respect to a loss function that is overly harsh.
It does not dis-tinguish between different types of alignment errors and good alignments receive the same penalty as poor alignments.
"Moreover, such a harsh penalty is particularly inappropriate when unambiguous word-to-word alignments cannot be provided in all cases even by human translators who produce the refer-ence alignments."
The AER makes an explicit dis-tinction between ambiguous and unambiguous word alignments.
"Ideally, the decoder should be able to do so as well."
"Motivated by this, the MBR hypothesis can be thought of as the consensus hypothesis un-der a particular loss function: Equation 5 selects the hypothesis that is, in an average sense, close to the other likely hypotheses."
"In this way, ambiguity can be reduced by selecting the hypothesis that is “most similar” to the collection of most likely competing hypotheses."
We now describe the alignment lattice (Sec-tion 4.1) and introduce the lattice based probabilities required for the MBR alignment (Section 4.2).
The derivation of the MBR alignment under the AE and GAE loss functions is presented in Sections 4.3 and 4.4.
"The lattice is represented as a Weighted Finite State Transducer (WFST)[REF_CITE]with a finite set of states , a set of transition labels , an initial state , the set of fi-nal states , and a finite set of transitions ."
"A transition in this WFST is given by where is the starting state, is the ending state, is the alignment link and is the weight."
"For an English sentence of length and a French sen-tence of length , we define as ."
A complete path through the WFST is a sequence of transitions given by such that and .
Each complete path defines an alignment link set .
"When we write , we mean that is derived from a complete path through ."
"This allows us to use alignment models in which the probability of an alignment can be written as a sum over alignment link weights, i.e. ."
We first introduce the lattice transition posterior probability of each transition in the lattice (6) where is if and otherwise.
The lattice transition posterior probability is the sum of the posterior probabilities of all lattice paths pass-ing through the transition .
This can be com-puted very efficiently with a forward-backward al-gorithm on the alignment lattice[REF_CITE]. is the posterior probability of an alignment link set which can be written as (7)
We now define the alignment link posterior prob-ability for a link (8) where .
This is the probability that any two words are aligned given all the alignments in the lattice .
In this section we derive MBR alignment under the Alignment Error loss function (Equation 2).
The op-timal decoder has the form (Equation 5) (9)
The summation is equal to
"If is the subset of transitions ( ) that do not contain links with the NULL word, we can simplify the bracketed term as"
For an alignment link we note that .
"Therefore, the MBR alignment (Equation 9) can be found in terms of the modified link weight for each alignment link (10) We can rewrite the above equation as (11)"
We now derive MBR alignment under the Gener-alized Alignment Error loss function (Equation 3).
The optimal decoder has the form (Equation 5)
The summation can be rewritten as where and .
We can simplify the bracketed term as where and .
The MBR alignment[REF_CITE]can be found in terms of the modified link weight for each align-ment link (13)
The MBR alignment procedures under the and loss functions begin with a WFST that con-tains the alignment probabilities as de-scribed in Section 4.1.
To build the MBR decoder for each loss function the weights on the transitions ( ) of the WFST are modified ac-cording to either[REF_CITE]( ) or[REF_CITE]( ).
"Once the weights are modified, the search procedure for the MBR alignment is the same in each case."
The search is carried out using a shortest-path algorithm[REF_CITE].
We present here examples of Generalized Align-ment Error loss functions based on three types of linguistic features and show how they can be incor-porated into a statistical MT system to obtain auto-matic alignments.
Suppose a parser is available that generates a parse-tree for the English sentence.
Our goal is to con-struct an alignment loss function that incorporates features from the parse.
One way to do this is to define a graph distance (14) Here and are the parse-tree leaf nodes cor-responding to the English words and .
This quantity is computed as the sum of the distances from each node to their closest common ancestor.
It gives a syntactic distance between any pair of English words based on the parse-tree.
This dis-tance has been used to measure word association for information retrieval[REF_CITE].
It reflects how strongly the words and are bound together by the syntactic structure of the English sentence as determined by the parser.
Fig-ure 1 shows the parse tree for an English sentence in the test data with the pairwise syntactic distances between the English words corresponding to the leaf nodes.
"To obtain these distances, Ratnaparkhi’s part-of-speech (POS) tagger[REF_CITE]and Collins’ parser[REF_CITE]were used to obtain parse trees for the English side of the test corpus."
With defined as[REF_CITE]the Generalized Alignment Error loss function (Equation 3) is called the Parse-Tree Syntactic Distance ( ).
Suppose a Part-of-Speech(POS) tagger is available to tag each word in the English sentence.
"If POS denotes the POS of the English word , we can de-fine the word-to-word distance measure (Equa-tion 4) as"
Ratnaparkhi’s POS tagger[REF_CITE]was used to obtain POS tags for each word in the English sentence.
With specified[REF_CITE]the Generalized Alignment Error loss func-tion (Equation 3) is called the Part-Of-Speech Dis-tance ( ).
Suppose we are working in a language for which parsers and POS taggers are not available.
In this situation we might wish to construct the loss func-tions based on word classes determined by auto-matic clustering procedures.
"If specifies the word cluster for the English word , then we define the distance (16)"
In our experiments we obtained word clusters for English words using a statistical learning proce-dure[REF_CITE]where the total number of word classes is restricted to be 100.
With as defined[REF_CITE]the Generalized Alignment Error loss function (Equation 3) is called the Auto-matic Word Class Distance ( ).
"Since the true distribution over alignments is not known, we used the IBM-3 statistical transla-tion model[REF_CITE]to approximate ."
This model is specified through four components: Fertility probabilities for words; Fer-tility probabilities for NULL; Word Translation probabilities; and Distortion probabilities.
We used a modified version of the IBM-3 distortion model[REF_CITE]in which each of the possible permutations of the French sentence is equally likely.
"The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 par-allel sentences[REF_CITE]."
"The vocab-ulary size was 18,499[REF_CITE]198 for French."
The GIZA++ toolkit[REF_CITE]was used for training the IBM-3 models (as[REF_CITE]).
We obtained word alignments under the modified IBM-3 models using the finite state translation framework introduced[REF_CITE].
The finite state operations were carried out using the AT&amp;T Finite State Machine Toolkit[REF_CITE].
The WFST framework involves building a trans-ducer for each constituent of the IBM-3 Alignment Models: the word fertility model ; the NULL fer-tility model ; and the word translation model (Section 5.4).
For each sentence pair we also built a finite state acceptor that accepts the English sen-tence and another acceptor which accepts all legal permutations of the French sentence.
The alignment lattice for the sentence pair was then obtained by the following weighted finite state composition .
"In practice, the WFST ob-tained by the composition was pruned to a maximum of 10,000 states using a likelihood based pruning op-eration."
"In terms of AT&amp;T Finite State Toolkit shell commands, these operations are given as: fsmcompose E M fsmcompose - N fsmcompose - T fsmcompose - F fsmprune -n 10000"
The finite state composition and pruning were per-formed using lazy implementations of algorithms provided in AT&amp;T Finite State libraries[REF_CITE].
"This made the computation efficient be-cause even though five WFSTs are composed into a potentially huge transducer, only a small portion of it is actually searched during the pruning used to generate the final lattice."
A heavily pruned alignment lattice for a sentence-pair from the test data is shown in Fig-ure 2.
"For clarity of presentation, each alignment link in the lattice is shown as an ordered pair where and are the English and French words on the link."
"For each sentence, we also computed the lattice path with the highest probability ."
This gives the ML alignment under the statistical MT models that will give our baseline performance under the various loss functions.
Our unseen test data consisted of 207 French-English sentence pairs from the Hansards cor-pus[REF_CITE].
These sentence pairs had at most 16 words in the French sentence; this restric-tion on the sentence length was necessary to control the memory requirements of the composition.
"In the previous sections we introduced a total of four loss functions: , , and ."
"Using either[REF_CITE]or 13, an MBR decoder can be constructed for each."
"These decoders are called MBR-AE, MBR-PTSD, MBR-POSD, and MBR-AWCD, respectively."
The performance of the four decoders was mea-sured with respect to the alignments provided by hu-man experts[REF_CITE].
The first eval-uation metric used was the Alignment Error Rate (Equation 1).
We also evaluated each decoder un-der the Generalized Alignment Error Rates (GAER).
These are defined as: (17)
There are six variants of GAER.
"These arise when is specified by , or ."
There are two versions of each of these: one version is sensitive only to sure (S) links.
The other version considers all (A) links in the reference alignment.
"We there-fore have the following six Generalized Alignment Error Rates: PTSD-S, POSD-S, AWCD-S, and PTSD-A, POSD-A, AWCD-A."
We say we have a matched condition when the same loss function is used in both the error rate and the decoder design.
The performance of the decoders under various loss functions is given in Table 1.
We observe that in none of these experiments was the ML decoder found to be optimal.
"In all instances, the MBR decoder tuned for each loss function was the best performing decoder under the corresponding error rate."
"In particular, we note that alignment perfor-mance as measured under the AER metric can be improved by using MBR instead of ML alignment."
This demonstrates the value of finding decoding pro-cedures matched to the performance criterion of in-terest.
We observe some affinity among the loss func-tions.
"In particular, the ML decoder performs better under the AER than any of the MBR-GAE decoders."
"This is because the loss, for which the ML de-coder is optimal, is closer to the loss than any of the loss functions."
"The NULL symbol is treated quite differently under and , and this leads to a large mismatch between the MBR-GAE decoders and the AER metric."
"Similarly, the performance of the MBR-POS decoder degrades significantly under the AWCD-S and AWCD-A met-rics."
"Since there are more word clusters (100) than[REF_CITE], the MBR-POS decoder is therefore incapable of producing hypotheses that can match the word clusters used in the AWCD metrics."
We have presented a Minimum Bayes-Risk decod-ing strategy for obtaining word alignments of bilin-gual texts.
MBR decoding is a general formulation that allows the construction of specialized decoders from general purpose models.
The strategy aims at direct minimization of the expected risk of align-ment errors under a given alignment loss function.
We have introduced several alignment loss func-tions to measure the alignment quality.
"These in-corporate information from varied analyses, such as parse trees, POS tags, and automatically derived word clusters."
We have derived and implemented lattice based MBR consensus decoders under these loss functions.
These decoders rescore the lattices produced by maximum likelihood decoding to pro-duce the optimal MBR alignments.
We have chosen to present MBR decoding using the IBM-3 statistical MT models implemented via WFSTs.
However MBR decoding is not restricted to this framework.
It can be applied more broadly using other MT model architectures that might be selected for reasons of modeling fidelity or compu-tational efficiency.
We have presented these alignment loss functions to explore how linguistic knowledge might be in-corporated into machine translation systems without building detailed statistical models of these linguis-tic features.
However we stress that the MBR decod-ing procedures described here do not preclude the construction of complex MT models that incorporate linguistic features.
"The application of such mod-els, which could be trained using conventional max-imum likelihood estimation techniques, should still benefit by the application of MBR decoding tech-niques."
In future work we will investigate loss functions that incorporate French and English parse-tree infor-mation into the alignment decoding process.
"Our ul-timate goal, towards which this work is the first step, is to construct loss functions that take advantage of linguistic structures such as syntactic dependencies found through monolingual analysis of the sentences to be aligned."
Recent work[REF_CITE]sug-gests that translational corresponence of linguistic structures can indeed be useful in projecting parses across languages.
Our ideal would be to construct MBR decoders based on loss functions that are sen-sitive both to word alignment as well as to agreement in higher level structures such as parse trees.
In this way ambiguity present in word-to-word alignments will be resolved by the alignment of linguistic struc-tures.
MBR alignment is a promising modeling frame-work for the detailed linguistic annotation of bilin-gual texts.
It is a simple model rescoring formalism that improves well trained statistical models by tun-ing them for particular performance criteria.
"Ideally, it will be used to produce decoders optimized for the loss functions that actually measure the qualities that we wish to see in newly developed automatic systems."
Text prediction is a form of interactive machine translation that is well suited to skilled translators.
In principle it can as-sist in the production of a target text with minimal disruption to a translator’s nor-mal routine.
"However, recent evaluations of a prototype prediction system showed that it significantly decreased the produc-tivity of most translators who used it."
"In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the ex-pected benefit to the translator, rather than just trying to anticipate some amount of upcoming text."
"Using a model of a “typ-ical translator” constructed from data col-lected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator."
The idea of using text prediction as a tool for trans-lators was first introduced by Church and Hovy as one of many possible applications for “crummy” machine translation technology[REF_CITE].
Text prediction can be seen as a form of in-teractive MT that is well suited to skilled transla-tors.
"Compared to the traditional form of IMT based on Kay’s original work[REF_CITE]—in which the user’s role is to help disambiguate the source text— prediction is less obtrusive and more natural, allow-ing the translator to focus on and directly control the contents of the target text."
"Predictions can benefit a translator in several ways: by accelerating typing, by suggesting translations, and by serving as an im-plicit check against errors."
"The first implementation of a predictive tool for translators was described[REF_CITE], in the form of a simple word-completion system based on statistical models."
"Various enhancements to this were carried out as part of the TransType project[REF_CITE], including the addition of a re-alistic user interface, better models, and the capabil-ity of predicting multi-word lexical units."
"In the fi-nal TransType prototype for English to French trans-lation, the translator is presented with a short pop-up menu of predictions after each character typed."
These may be incorporated into the text with a spe-cial command or rejected by continuing to type nor-mally.
"Although TransType is capable of correctly antic-ipating over 70% of the characters in a freely-typed translation (within the domain of its training cor-pus), this does not mean that users can translate in 70% less time when using the tool."
"In fact, in a trial with skilled translators, the users’ rate of text pro-duction declined by an average of 17% as a result of using TransType[REF_CITE]."
There are two main reasons for this.
"First, it takes time to read the system’s proposals, so that in cases where they are wrong or too short, the net effect will be to slow the translator down."
"Second, translators do not always act “rationally” when confronted with a pro-posal; that is, they do not always accept correct pro-posals and they occasionally accept incorrect ones."
"Many of the former cases correspond to translators simply ignoring proposals altogether, which is un-derstandable behaviour given the first point."
This paper describes a new approach to text pre-diction intended to address these problems.
"The main idea is to make predictions that maximize the expected benefit to the user in each context, rather than systematically proposing a fixed amount of text after each character typed."
"The expected benefit is estimated from two components: a statistical trans-lation model that gives the probability that a can-didate prediction will be correct or incorrect, and a user model that determines the benefit to the trans-lator in either case."
"The user model takes into ac-count the cost of reading a proposal, as well as the random nature of the decision to accept it or not."
"This approach can be characterized as making fewer but better predictions: in general, predictions will be longer in contexts where the translation model is confident, shorter where it is less so, and absent in contexts where it is very uncertain."
"Other novel aspects of the work we describe here are the use of a more accurate statistical translation model than has previously been employed for text prediction, and the use of a decoder to generate pre-dictions of arbitrary length, rather than just single words or lexicalized units as in the TransType pro-totype."
The translation model is based on the max-imum entropy principle and is designed specifically for this application.
"To evaluate our approach to prediction, we simu-lated the actions of a translator over a large corpus of previously-translated text."
The result is an increase of over 10% in translator productivity when using the predictive tool.
This is a considerable improve-ment over the -17% observed in the TransType trials.
"In the basic prediction task, the input to the predictor is a source sentence s and a prefix h of its translation (ie, the target text before the current cursor position); the output is a proposed extension x to h. Figure 1 gives an example."
"Unlike the TransType prototype, which proposes a set of single-word (or single-unit) suggestions, we assume that each prediction consists of only a single proposal, but one that may span an arbitrary number of words."
"As described above, the goal of the predictor is to find the prediction x̂ that maximizes the expected benefit to the user: x̂ = argmax"
"B(x, h, s), (1) x where B(x, h, s) measures typing time saved."
"This obviously depends on how much of x is correct, and how long it would take to edit it into the desired text."
A major simplifying assumption we make is that the user edits only by erasing wrong characters from the end of a proposal.
"Given a TransType-style interface where acceptance places the cursor at the end of a proposal, this is the most common editing method, and it gives a conservative estimate of the cost at-tainable by other methods."
"With this assumption, the key determinant of edit cost is the length of the correct prefix of x, so the expected benefit can be written as: l B(x, h, s) = X p(k|x, h, s) B(x, h, s, k), (2) k=0 where p(k|x, h, s) is the probability that exactly k characters from the beginning of x will be correct, l is the length of x, and B(x, h, s, k) is the benefit to the user given that the first k characters of x are correct."
"Equations (1) and (2) define three main problems: estimating the prefix probabilities p(k|x, h, s), esti-mating the user benefit function B(x, h, s, k), and searching for x̂."
The following three sections de-scribe our solutions to these.
"The correct-prefix probabilities p(k|x, h, s) are derived from a word-based statistical translation model."
The first step in the derivation is to con-vert these into a form that deals explicitly with char-acter strings.
"This is accomplished by noting that p(k|x, h, s) is the probability that the first k charac-ters of x are correct and that the k + 1th character (if there is one) is incorrect."
"For k &lt; l: p(k|x, h, s) = p(x k1 |h, s) − p(x k1+1 |h, s) where x k1 = x 1 ...x k ."
"If k = l, p(k|x,h,s) = p(x|h, s)."
"Also, p(x 01 ) ≡ [Footnote_1]."
1 Here we ignore the distinction between previous words that have been sanctioned by the translator and those that are hy-pothesized as part of the current prediction.
The next step is to convert string probabilities into word probabilities.
"To do this, we assume that strings map one-to-one into token sequences, so that: p(x 1k |h, s) ≈ p(v 1 , w 2 , . . . , w m−1 , u m |h, s), where v 1 is a possibly-empty word suffix, each w i is a complete word, and u m is a possibly empty word prefix."
"For example, if x in figure 1 were evenir aux choses, then x 141 would map to v 1 = evenir, w 2 = aux, and u 3 = cho."
The one-to-one assumption is reasonable given that entries in our lexicon contain neither whitespace nor internal punctuation.
"To model word-sequence probabilities, we apply the chain rule: p(v 1 , w 2 , . .. , w m−1 , u m |h, s) = m−1 p(v 1 |h, s) Y p(w i |h, v 1 , w 2i−1 , s) × i=2 p(u m |h, v 1 , w m−12 , s). (3)"
The probabilities of v 1 and u m can be expressed in terms of word probabilities as follows.
"Letting u 1 be the prefix of the word that ends in v 1 (eg, r in figure 1), w 1 = u 1 v 1 , and h = h 0 u 1 : p(v 1 |h, s) = p(w 1 |h 0 , s)/ X p(w|h 0 , s), w:w=u 1 v where the sum is over all words that start with u 1 ."
"Similarly: p(u m |h 0 , w m−11 , s) = X p(w|h 0 , w 1m−1 , s). (4) w:w=u m v"
"Thus all factors in (3) can be calculated from probabilities of the form p(w|h,s) which give the likelihood that a word w will follow a previous se-quence of words h in the translation of s. 1"
This is the family of distributions we have concentrated on modeling.
"Our model for p(w|h, s) is a log-linear combina-tion of a trigram language model for p(w|h) and a maximum-entropy translation model for p(w|s), de-scribed[REF_CITE]."
"The trans-lation component is an analog of the IBM model 2[REF_CITE], with parameters that are op-timized for use with the trigram."
The combined model is shown[REF_CITE]to have signif-icantly lower test corpus perplexity than the linear combination of a trigram and IBM 2 used in the TransType experiments[REF_CITE].
"Both models support O(mJV 3 ) Viterbi-style searches for the most likely sequence of m words that follows h, where J is the number of tokens in s and V is the size of the target-language vocabulary."
"Compared to an equivalent noisy-channel combi-nation of the form p(t)p(s|t), where t is the tar-get sentence, our model is faster but less accurate."
"It is faster because the search problem for noisy-channel models is NP-complete[REF_CITE], and even the fastest dynamic-programming heuristics used in statistical MT[REF_CITE], are polynomial in J—for in-stance O(mJ 4 V 3 )[REF_CITE]."
"It is less accurate because it ignores the alignment rela-tion between s and h, which is captured by even the simplest noisy-channel models."
"Our model is there-fore suitable for making predictions in real time, but not for establishing complete translations unassisted by a human."
"The most expensive part of the calculation in equa-tion (3) is the sum in (4) over all words in the vo-cabulary, which according to (2) must be carried out for every character position k in a given prediction x. We reduce the cost of this by performing sums only at the end of each sequence of complete tokens in x (eg, after revenir and revenir aux in the above example)."
"At these points, probabilities for all pos-sible prefixes of the next word are calculated in a single recursive pass over the vocabulary and stored in a trie for later access."
"In addition to the exact calculation, we also ex-perimented with establishing exact probabilities via p(w|h, s) only at the end of each token in x, and as-suming that the probabilities of the intervening char-acters vary linearly between these points."
"As a re-sult of this assumption, p(k|x, h, s) = p(x k1 |h, s) − p(x k1+1 |h, s) is constant for all k between the end of one word and the next, and therefore can be factored out of the sum in equation (2) between these points."
"The purpose of the user model is to determine the expected benefit B(x, h, s, k) to the translator of a prediction x whose first k characters match the text that the translator wishes to type."
"This will depend on whether the translator decides to accept or reject the prediction, so the first step in our model is the following expansion:"
"B(x, h, s, k) = Xp(a|x, h, s, k) B(x, h, s, k, a), a∈{0,1} where p(a|x, h, s, k) is the probability that the trans-lator accepts or rejects x, B(x, h, s, k, a) is the ben-efit they derive from doing so, and a is a random variable that takes on the values 1 for acceptance and 0 for rejection."
"The first two quantities are the main elements in the user model, and are described in fol-lowing sections."
"The parameters of both were esti-mated from data collected during the TransType trial described[REF_CITE], which involved nine accomplished translators using a prototype pre-diction tool for approximately half an hour each."
"In all cases, estimates were made by pooling the data for all nine translators."
"Ideally, a model for p(a|x, h, s, k) would take into account whether the user actually reads the proposal before accepting or rejecting it, eg: p(a|x, h, s, k) = X p(a|r, x, h, s, k)p(r|x, h, s, k) r∈{0,1} where r is a boolean “read” variable."
"However, this information is hard to extract reliably from the avail-able data; and even if were obtainable, many of the factors which influence whether a user is likely to read a proposal—such as a record of how many pre-vious predictions have been accepted—are not avail-able to the predictor in our formulation."
"We thus model p(a|x, h, s, k) directly."
"Our model is based on the assumption that the probability of accepting x depends only on what the user stands to gain from it, defined according to the editing scenario given in section 2 as the amount by which the length of the correct prefix of x exceeds the length of the incorrect suffix: p(a|x, h, s, k) ≈ p(a|2k − l), where k −(l −k) = 2k −l is called the gain."
"For in-stance, the gain for the prediction in figure 1 would be 2 × 7 − 8 = 6."
"The strongest part of this assump-tion is dropping the dependence on h, because there is some evidence from the data that users are more likely to accept at the beginnings of words."
"How-ever, this does not appear to have a severe effect on the quality of the model."
Figure 2 shows empirical estimates of p(a = 1|2k−l) from the TransType data.
"There is a certain amount of noise intrinsic to the estimation proce-dure, since it is difficult to determine x ∗ , and there-fore k, reliably from the data in some cases (when the user is editing the text heavily)."
"Nonetheless, it is apparent from the plot that gain is a useful abstrac- tion, because the empirical probability of acceptance is very low when it is less than zero and rises rapidly as it increases."
This relatively clean separation sup-ports the basic assumption in section 2 that benefit depends on k.
"The points labelled smoothed in figure 2 were obtained using a sliding-average smoother, and the model curve was obtained using two-component Gaussian mixtures to fit the smoothed empirical likelihoods p(gain|a = 0) and p(gain|a = 1)."
The model probabilities are taken from the curve at in-tegral values.
"As an example, the probability of ac-cepting the prediction in figure 1 is about .25."
"The benefit B(x, h, s, k, a) is defined as the typing time the translator saves by accepting or rejecting a prediction x whose first k characters are correct."
"To determine this, we assume that the translator first reads x, then, if he or she decides to accept, uses a special command to place the cursor at the end of x and erases its last l − k characters."
"Assuming inde-pendence from h, s as before, our model is:"
"B(x, k, a) = −R 1 (x) + T (x, k) − E(x, k), a = 1 −R 0 (x), a = 0 where R a (x) is the cost of reading x when it ul-timately gets accepted (a = 1) or rejected (a = 0), T(x,k) is the cost of manually typing x k1 , and E(x, k) is the edit cost of accepting x and erasing to the end of its first k characters."
"A natural unit for B(x,k,a) is the number of keystrokes saved, so all elements of the above equa-tion are converted to this measure."
"This is straight-forward in the case of T(x, k) and E(x, k), which are estimated as k and l − k + 1 respectively—for E(x,k), this corresponds to one keystroke for the command to accept a prediction, and one to erase each wrong character."
"This is likely to slightly un-derestimate the true benefit, because it is usually harder to type n characters than to erase them."
"As in the previous section, read costs are inter-preted as expected values with respect to the proba-bility that the user actually does read x, eg, assuming 0 cost for not reading, R 0 (x) = p(r=1|x)R 00 (x), where R 00 (x) is the unknown true cost of reading and rejecting x. To determine R a (x), we measured the average elapsed time in the TransType data from the point at which a proposal was displayed to the point at which the next user action occurred—either an acceptance or some other command signalling a rejection."
Times greater than 5 seconds were treated as indicating that the translator was distracted and were filtered out.
"As shown in figure 3, read times are much higher for predictions that get accepted, re-flecting both a more careful perusal by the translator and the fact the rejected predictions are often simply ignored. [Footnote_2]"
"2 Here the number of characters read was assumed to include the whole contents of the TransType menu in the case of rejec-tions, and only the proposal that was ultimately accepted in the case of acceptances."
"In both cases there is a weak linear rela- tionship between the number of characters read and the time taken to read them, so we used the least-squares lines shown as our models."
"Both plots are noisy and would benefit from a more sophisticated psycholinguistic analysis, but they are plausible and empirically-grounded first approximations."
To convert reading times to keystrokes for the benefit function we calculated an average time per keystroke (304 milliseconds) based on sections of the trial where translators were rapidly typing and when predictions were not displayed.
"This gives an upper bound for the per-keystroke cost of reading— compare to, for instance, simply dividing the total time required to produce a text by the number of characters in it—and therefore results in a conser-vative estimate of benefit."
"To illustrate the complete user model, in the fig-ure 1 example the benefit of accepting would be 7−2−4.2 = .8 keystrokes and the benefit of reject-ing would be −.2 keystrokes."
"Combining these with the acceptance probability of .25 gives an overall ex-pected benefit B(x, h, s, k = 7) for this proposal of 0.05 keystrokes."
Searching directly through all character strings x in order to find x̂ according to equation (1) would be very expensive.
"The fact that B(x, h, s) is non-monotonic in the length of x makes it difficult to or-ganize efficient dynamic-programming search tech-niques or use heuristics to prune partial hypotheses."
"Because of this, we adopted a fairly radical search strategy that involves first finding the most likely se-quence of words of each length, then calculating the benefit of each of these sequences to determine the best proposal."
The algorithm is: 1.
"For each length m = 1...M, find the best word sequence: ŵ m = p(w 1m |h 0 , s),argmax w 1 :(w 1 =u 1 v),w 2m where u 1 and h 0 are as defined in section 3. 2. Convert each ŵ m to a corresponding character string x̂ m . 3."
"Output x̂ = argmax m B(x̂ m ,h,s), or the empty string if all B(x̂ m , h, s) are non-positive."
"In all experiments reported below, M was set to a maximum of 5 to allow for convenient testing."
Step 1 is carried out using a Viterbi beam search.
"To speed this up, the search is limited to an active vo-cabulary of target words likely to appear in transla-tions of s, defined as the set of all words connected by some word-pair feature in our translation model to some word in s. Step 2 is a trivial deterministic procedure that mainly involves deciding whether or not to introduce blanks between adjacent words (eg yes in the case of la + vie, no in the case of l’ + an)."
This also removes the prefix u 1 from the pro-posal.
Step 3 involves a straightforward evaluation of m strings according to equation (2).
"Table 1 shows empirical search timings for vari-ous values of M, for the MEMD model described in the next section."
Times for the linear model are similar.
"Although the maximum times shown would cause perceptible delays for M &gt; 1, these occur very rarely, and in practice typing is usually not no-ticeably impeded when using the TransType inter-face, even at M = 5."
"We evaluated the predictor for English to French translation on a section of the Canadian Hansard corpus, after training the model on a chronologi-cally earlier section."
"The test corpus consisted of 5,020 sentence pairs and approximately 100k words in each language; details of the training corpus are given[REF_CITE]."
"To simulate a translator’s responses to predic-tions, we relied on the user model, accepting prob-abilistically according to p(a|x, h, s, k), determin-ing the associated benefit using B(x, h, s, k, a), and advancing the cursor k characters in the case of an acceptance, 1 otherwise."
Here k was obtained by comparing x to the known x ∗ from the test corpus.
"It may seem artificial to measure performance ac-cording to the objective function for the predictor, but this is biased only to the extent that it misrepre-sents an actual user’s characteristics."
"There are two cases: either the user is a better candidate—types more slowly, reacts more quickly and rationally— than assumed by the model, or a worse one."
"The predictor will not be optimized in either case, but the simulation will only overestimate the benefit in the second case."
"By being conservative in estimating the parameters of the user model, we feel we have minimized the number of translators who would fall into this category, and thus can hope to obtain real-istic lower bounds for the average benefit across all translators."
Table 2 contains results for two different trans-lation models.
The top portion corresponds to the MEMD2B maximum entropy model described[REF_CITE]; the bottom portion corresponds to the linear combination of a trigram and IBM 2 used in the TransType experiments[REF_CITE].
Columns give the maximum permitted number of words in predictions.
"Rows show different predic- tor configurations: fixed ignores the user model and makes fixed M-word predictions; linear uses the lin-ear character-probability estimates described in sec-tion 3.1; exact uses the exact character-probability calculation; corr is described below; and best gives an upper bound on performance by choosing m in step 3 of the search algorithm so as to maximize B(x, h, s, k) using the true value of k."
Table 3 illustrates the effects of different compo-nents of the user model by showing results for sim-ulated users who read infinitely fast and accept only predictions having positive benefit (superman); who read normally but accept like superman (rational); and who match the standard user model (real).
"For each simulation, the predictor optimized benefits for the corresponding user model."
Several conclusions can be drawn from these re-sults.
"First, it is clear that estimating expected bene-fit is a much better strategy than making fixed-word-length proposals, since the latter causes an increase in time for all values of M. In general, making “ex-act” estimates of string prefix probabilities works better than a linear approximation, but the difference is fairly small."
"Second, the MEMD2B model significantly out-performs the trigram+IBM2 combination, produc-ing better results for every predictor configuration tested."
"The figure of -11.5% in bold corresponds to the TransType configuration, and corroborates the validity of the simulation. [Footnote_3]"
"3 Although the drop observed with real users was greater at about 20% (= 17% reduction in speed), there are many dif-ferences between experimental setups that could account for the discrepancy. For instance, part of the corpus used for the TransType trials was drawn from a different domain, which would adversely affect predictor performance."
"Third, there are large drops in benefit due to read-ing times and probabilistic acceptance."
"The biggest cost is due to reading, which lowers the best possi-ble keystroke reduction by almost 50% for M = 5."
Probabilistic acceptance causes a further drop of about 15% for M = 5.
The main disappointment in these results is that performance peaks at M = 3 rather than continu-ing to improve as the predictor is allowed to con-sider longer word sequences.
"Since the predictor knows B(x, h, s, k), the most likely cause for this is that the estimates for p(ŵ m |h, s) become worse with increasing m. Significantly, performance lev- els off at three words, just as the search loses di-rect contact with h through the trigram."
"To correct for this, we used modified probabilities of the form λ m p(ŵ m |h, s), where λ m is a length-specific cor-rection factor, tuned so as to optimize benefit on a cross-validation corpus."
"The results are shown in the corr row of table 2, for exact character-probability estimates."
"In this case, performance improves with M, reaching a maximum keystroke reduction of 12.6% at M = 5."
We have described an approach to text prediction for translators that is based on maximizing the benefit to the translator according to an explicit user model whose parameters were set from data collected in user evaluations of an existing text prediction proto-type.
"Using this approach, we demonstrate in sim-ulated results that our current predictor can reduce the time required for an average user to type a text in the domain of our training corpus by over 10%."
We look forward to corroborating this result in tests with real translators.
There are many ways to build on the work de-scribed here.
The statistical models which are the backbone of the predictor could be improved by making them adaptive—taking advantage of the user’s input—and by adding features to capture the alignment relation between h and s in such a way as to preserve the efficient search properties.
"The user model could also be made adaptive, and it could be enriched in many other ways, for instance so as to capture the propensity of translators to accept at the beginnings of words."
We feel that the idea of creating explicit user mod-els to guide the behaviour of interactive systems is likely to have applications in areas of NLP apart from translators’ tools.
"For one thing, most of the approach described here carries over more or less directly to monolingual text prediction, which is an important tool for the handicapped[REF_CITE]."
Other possibilities include virtually any ap-plication where a human and a machine communi-cate through a language-rich interface.
É» É ¹£¾¼¾Ç½pÆIÊ É #ºÃÂ@Á@#º¼Â@ËI¾ÇÄÌÄhÂS¸kÄhÂ#½pÄ ~¹¸Í~Á ¹£»2#Á ÄBÁ~ºÇËIÁ@Äh»k¸ÎÊ#Æ£Ï~¹Ïzº¼¾¼ºÇ¸4ÈT¹£½aÐ ½pÆ£ÅcÑ#º¼Â@@Ä¿ÀÆxÑ@~!#ÄhÑ ÆIÂÒÑ#@&quot;Ä&gt;@ÆxÑÌ×Æ£Å ½pÆIÂ~»k¸kÅ É #ËØ¹ØÙ ~# ~.# ½vº¼ÄhÂS¸ Ù #Ä/¹£Ñ@#@ ÁYÈxÊ.#&quot;!Ä  
É # àzÂ@ÄhÑÚ¾¼¹£Â#Ë É ~#Äh¾áÓ â
Äh» É #(ÆIÂ(@@Ð å @Â %~~º¼¾&quot; É »vÓ ç è *
A µ£HI5% Ê @@@Ë ì-¹&gt;ËIºÇÛ£ÄhÂZ»kÆ É @Ë#@#)¿À¹£½cÁ#º¼Â@ # íïî ñ í ð òvòvò ívó òvòvò í î º¼»¸kÆÎÏ.(Ä # @ ñ ô ðïòvòvò ôvö òvòvò ô õ ò÷ !ÆIÂ ÷ ¹hÈ£Äh»hø @Ñ  É ¾ÇÄ£ù¶Ù Ä ½cÁ@#Ä »k¸kÅcº¼Â@@Ä Á#ºÇËIÁ6Ð ~~¹Ï~º¼¾ÃºÇ¸êÈ.ì ú õðûñ
"An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and cor-responding natural language realizations."
"Typically, labor-intensive knowledge-based methods are used to construct the dictio-nary."
"We instead propose to acquire it automatically via a novel multiple-pass al-gorithm employing multiple-sequence align-ment, a technique commonly used in bioin-formatics."
"Crucially, our method lever-ages latent information contained in multi-parallel corpora — datasets that supply several verbalizations of the corresponding semantics rather than just one."
"We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis."
"For example, in evaluations involv-ing a dozen human judges, our system pro-duced output whose readability and faith-fulness to the semantic input rivaled that of a traditional generation system."
One or two homologous sequences whisper ...a full multiple alignment shouts out loud[REF_CITE].
Today’s natural language generation systems typically employ a lexical chooser that translates complex semantic concepts into words.
"The lex-ical chooser relies on a mapping dictionary that lists possible realizations of elementary seman-tic concepts; sample entries might be [Parent [sex:female]] → mother or love(x,y )→ {x loves y, x is in love with y}. [Footnote_1]"
"1 Throughout, fonts denote a mapping dictionary’s two information types: semantics and realizations."
"To date, creating these dictionaries has involved human analysis of a domain-relevant corpus com-prised of semantic representations and correspond-ing human verbalizations[REF_CITE]."
"The corpus analysis and knowledge engineering work required in such an approach is substantial, pro-hibitively so in large domains."
"But, since corpus data is already used in building lexical choosers by hand, an appealing alternative is to have the system learn a mapping dictionary directly from the data."
"Clearly, this would greatly reduce the human effort involved and ease porting the system to new domains."
"Hence, we address the following problem: given a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations, learn a semantics-to-words mapping dictionary automatically."
"Now, we could simply apply standard statistical machine translation methods, treating verbalizations as “translations” of the semantics."
"These meth-ods typically rely on one-parallel corpora consist-ing of text pairs, one in each “language” (but cf."
"However, learning the kind of semantics-to-words mapping that we desire from one-parallel data alone is difficult even for hu-mans."
"First, given the same semantic input, differ-ent authors may (and do) delete or insert informa-tion (see Figure 1); hence, direct comparison between a semantic text and a single verbalization may not provide enough information regarding their underly-ing correspondences."
"Second, a single verbalization certainly fails to convey the variety of potential lin-guistic realizations of the concept that an expressive lexical chooser would ideally have access to."
The multiple-sequence idea Our approach is motivated by an analogous situation that arises in computational biology.
"In brief, an important bioin- formatics problem —[REF_CITE]refers to it as “The Holy Grail” — is to determine commonalities within a collection of biological sequences such as proteins or genes."
"Because of mutations within indi-vidual sequences, such as changes, insertions, or dele-tions, pair-wise comparison of sequences can fail to reveal which features are conserved across the entire group."
"Hence, biologists compare multiple sequences simultaneously to reveal hidden structure character-istic to the group as a whole."
Our work applies multiple-sequence alignment techniques to the mapping-dictionary acquisition problem.
The main idea is that using a multi-parallel corpus — one that supplies several alternative ver-balizations for each semantic expression — can en-hance both the accuracy and the expressiveness of the resulting dictionary.
"In particular, matching a semantic expression against a composite of the com-mon structural features of a set of verbalizations ameliorates the effect of “mutations” within indi-vidual verbalizations."
"Furthermore, the existence of multiple verbalizations helps the system learn several ways to express concepts."
"To illustrate, consider a sample semantic expres-sion from the mathematical theorem-proving do-main."
"The expression show-from(a=0,b=0,a∗b=0) means “assuming the two premises a = 0 and b = 0, show that the goal a ∗ b = 0 holds”."
Figure 1 shows three human verbalizations of this expression.
"Even for so formal a domain as mathematics, the verbal- izations vary considerably, and none directly matches the entire semantic input."
"For instance, it is not ob-vious without domain knowledge that “Given a and b as in the theorem statement” matches “a=0” and “b=0”, nor that “their product” and “a∗b” are equiv-alent."
"Moreover, sentence (3) omits the goal argu-ment entirely."
"However, as Figure 2 shows, the com-bination of these verbalizations, as computed by our multiple-sequence alignment method, exhibits high structural similarity to the semantic input: the indi-cated “sausage” structures correspond closely to the three arguments of show-from."
This section describes general multiple-sequence alignment; we discuss its application to learning mapping dictionaries in the next section.
"A multiple-sequence alignment algorithm takes as input n strings and outputs an n-row correspondence table, or multiple-sequence alignment (MSA). (We explain how the correspondences are actually com-puted below.)"
"The MSA’s rows correspond to se-quences, and each column indicates which elements of which strings are considered to correspond at that point; non-correspondences, or “gaps”, are repre-sented by underscores ( )."
See Figure 3(i).
"From an MSA, we can compute a lattice ."
"Each lattice node, except for “start” and “end”, corre- sponds to an MSA column."
The edges are induced by traversing each of the MSA’s rows from left to right.
See Figure 3(ii).
Alignment computation The sum-of-pairs dy-namic programming algorithm and pairwise iterative alignment algorithm sketched here are described in full[REF_CITE]and[REF_CITE].
"Let Σ be the set of elements making up the se-quences to be aligned, and let sim(x,y), x and y ∈ Σ∪{ }, be a domain-specific similarity function that assigns a score to every possible pair of alignment el-ements, including gaps."
"Intuitively, we prefer MSAs in which many high-similarity elements are aligned."
"In principle, we can use dynamic programming over alignments of sequence prefixes to compute the highest-scoring MSA, where the sum-of-pairs score for an MSA is computed by summing sim(x, y) over each pair of entries in each column."
"Unfortunately, these computations are exponential in n, the number of sequences. (In fact, finding the optimal MSA when n is a variable is NP-complete[REF_CITE].)"
"Therefore, we use iterative pairwise align-ment, a commonly-used polynomial-time approxi-mation procedure, instead."
This algorithm greedily merges pairs of MSAs of (increasingly larger) subsets of the n sequences; which pair to merge is determined by the average score of all pairwise alignments of se-quences from the two MSAs.
"Aligning lattices We can apply the above se-quence alignment algorithm to lattices as well as sequences, as is indeed required by pairwise itera-tive alignment."
We simply treat each lattice as a sequence whose ith symbol corresponds to the set of nodes at distance i from the start node.
"We mod-ify the similarity function accordingly: any two new symbols are equivalent to subsets S 1 and S 2 of Σ, so we define the similarity of these two symbols as max (x,y)∈S 1 ×S 2 sim(x, y)."
Our goal is to produce a semantics-to-words map-ping dictionary by comparing semantic sequences to MSAs of multiple verbalizations.
"We assume only that the semantic representation uses predicate-argument structure, so the elementary semantic units are either terms (e.g., 0), or predicates taking arguments (e.g., show-from(prem1, prem[Footnote_2], goal), whose arguments are two premises and a goal)."
"2 These values were hand-tuned on a held-out develop-ment corpus, described later. Because we use progressive alignment, the case x = y = does not occur."
Note that both types of units can be verbalized by multi-word sequences.
"Now, semantic units can occur several times in the corpus."
"In the case of predicates, we would like to combine information about a given pred- icate from all its appearances, because doing so would yield more data for us to learn how to ex-press it."
"On the other hand, correlating verbaliza-tions across instances instantiated with different ar-gument values (e.g., show-from(a=0,b=0,a*b=0) vs. show-from(c&gt;0,d&gt;0,c/d&gt;0)) makes alignment harder, since there are fewer obvious matches (e.g., “a∗b=0” does not greatly resemble “c/d&gt;0”); this seems to discourage aligning cross-instance verbal-izations."
"We resolve this apparent paradox by a novel three-phase approach: • In the per-instance alignment phase (Section 3.1), we handle each separate instance of a se-mantic predicate individually."
"First, we com-pute a separate MSA for each instance’s ver-balizations."
"Then, we abstract away from the particular argument values of each instance by replacing lattice portions corresponding to ar-gument values with argument slots, thereby cre-ating a slotted lattice. • In the cross-instance alignment phase (Section 3.2), for each predicate we align together all the slotted lattices from all of its instances. • In the template induction phase (Section 3.3), we convert the aligned slotted lattices into tem-plates — sequences of words and argument po-sitions — by tracing slotted lattice paths."
"Finally, we enter the templates into the mapping dic-tionary."
"As mentioned above, the first job of the per-instance alignment phase is to separately compute for each in-stance of a semantic unit an MSA of all its verbaliza-tions."
"To do so, we need to supply a scoring function capturing the similarity in meaning between words."
"Since such similarity can be domain-dependent, we use the data to induce — again via sequence align-ment — a paraphrase thesaurus T that lists linguis-tic items with similar meanings. (This process is described later in section 3.1.1.)"
"We then set 1 x = y, x ∈ Σ; sim(x, y) =  0.5 x ≈ y; −0.01 exactly one of x, y is ;  −0.5 otherwise (mismatch) where Σ is the vocabulary and x ≈ y denotes that T lists x and y as paraphrases. 2 Figure 2 shows the lat-tice computed for the verbalizations of the instance show-from(a=0,b=0,a∗b=0) listed in Figure 1."
The structure of the lattice reveals why we informally re-fer to lattices as “sausage graphs”.
"Next, we transform the lattices into slotted lat-tices."
"We use a simple matching process that finds, for each argument value in the semantic expression, a sequence of lattice nodes such that each node con-tains a word identical to or a paraphrase of (accord-ing to the paraphrase thesaurus) a symbol in the argument value (these nodes are shaded in Figure 2)."
"The sequences so identified are replaced with a “slot” marked with the argument variable (see Fig-ure 4). [Footnote_3] Notice that by replacing the argument val-ues with variable labels, we make the commonalities between slotted lattices for different instances more clear, which is useful for the cross-instance alignment step."
"3 This may further change the topology by forcing other nodes to be removed as well. For example, the slotted lattice in Figure 4 doesn’t contain the node se-quence “their product”."
Recall that the paraphrase thesaurus plays a role both in aligning verbalizations and in matching lattice nodes to semantic argument values.
"The main idea behind our para-phrase thesaurus induction method, motivated[REF_CITE], is that paths through lattice “sausages” often correspond to al-ternate verbalizations of the same concept, since the sausage endpoints are contexts common to all the sausage-interior paths."
"Hence, to extract para-phrases, we first compute all pairwise alignments of parallel verbalizations, discarding those of score less than four in order to eliminate spurious matches. [Footnote_4] Parallel sausage-interior paths that appear in sev-eral alignments are recorded as paraphrases."
"4 Pairwise alignments yield fewer candidate alignments from which to select paraphrases, allowing simple scoring functions to produce decent results."
"Then, we iterate, realigning each pair of sentences, but with previously-recognized paraphrases treated as identi-cal, until no new paraphrases are discovered."
"While the majority of the derived paraphrases are single words, the algorithm also produces several multi-word paraphrases, such as “are equal to” for “=”."
"To simplify subsequent comparisons, these phrases (e.g., “are equal to”) are treated as single tokens."
"Here are four paraphrase pairs we extracted from the mathematical-proof domain: (conclusion, result) (0, zero) (applying, by) (expanding, unfolding) (See Section 4.2 for a formal evaluation of the para-phrases.)"
"We treat thesaurus entries as degenerate slotted lattices containing no slots; hence, terms and predicates are represented in the same way."
"Figure 4 is an example where the verbalizations for a single instance yield good information as to how to realize a predicate. (For example, “Assume [prem1] and [prem2], prove [goal]”, where the brackets en-close arguments marked with their type.)"
"Some-times, though, the situation is more complicated."
"Figure 5 shows two slotted lattices for different in-stances of rewrite(lemma,goal) (meaning, rewrite goal by applying lemma); the first slotted lattice is problematic because it contains context-dependent information (see caption)."
"Hence, we engage in cross-instance alignment to merge information about the predicate."
"That is, we align the slotted lattices for all instances of the predicate (see Figure 6); the re-sultant unified slotted lattice reveals linguistic ex-pressions common to verbalizations of different in-stances."
"Notice that the argument-matching process in the per-instance alignment phase helps make these commonalities more evident by abstracting over dif-ferent values of the same argument (e.g., lemma100 and lemma104 are both relabeled “lemma”)."
"Finally, it remains to create the mapping dictionary from unified slotted lattices."
"While several strate-gies are possible, we chose a simple consensus se-quence method."
"Define the node weight of a given slotted lattice node as the number of verbalization paths passing through it (downweighted if it contains punctuation or the words “the”, “a”, “to”, “and”, or “of”)."
"The path weight of a slotted lattice path is a length-normalized sum of the weights of its nodes. [Footnote_5] We produce as a template the words from the consen-sus sequence, defined as the maximum-weight path, which is easily computed via dynamic programming."
"5 Shorter paths are preferred, but we discard sequences shorter than six words as potentially spurious."
"For example, the template we derive from Figure 6’s slotted lattice is We use lemma [lemma] to get [goal]. each instance had two verbalizations."
"In instance (I), both verbalizations contain the context-dependent in-formation “ an = −n−a ” (the statement of lemma100); also, argument-matching failed on the context-dependent phrase “the fact about division”."
"While this method is quite efficient, it does not fully exploit the expressive power of the lattice, which may encapsulate several valid realizations."
We leave to future work experimenting with alternative template-induction techniques; see Section 5.
"We implemented our system on formal mathemati-cal proofs created by the Nuprl system, which has been used to create thousands of proofs in many mathematical fields[REF_CITE]."
Gen-erating natural-language versions of proofs was first addressed several decades ago[REF_CITE].
"But now, large formal-mathematics libraries are available on-line. [Footnote_6] Unfortunately, they are usually encoded in highly technical languages (see Figure 7(i))."
"Natural-language versions of these proofs would make them more widely accessible, both to users lacking famil-iarity with a specific prover’s language, and to search engines which at present cannot search the symbolic language of formal proofs."
"Besides these practical benefits, the formal math-ematics domain has the further advantage of being particularly suitable for applying statistical genera-tion techniques."
Training data is available because theorem-prover developers frequently provide verbal-izations of system outputs for explanatory purposes.
"In our case, a multi-parallel corpus of Nuprl proof verbalizations already exists[REF_CITE]and forms the core of our training corpus."
"Also, from a research point of view, the examples from Figure 1 show that there is a surprising variety in the data, making the problem quite challenging."
All evaluations reported here involved judgments from graduate students and researchers in computer science.
We authors were not among the judges.
Our training corpus consists of 30 Nuprl proofs and 83 verbalizations.
"On average, each proof consists of 5.08 proof steps, which are the basic semantic unit in Nuprl; Figure 7(i) shows an example of three Nuprl steps."
"An additional five proofs, disjoint from the test data, were used as a development set for setting the values of all parameters. [Footnote_7] Pre-processing First, we need to divide the ver-balization texts into portions corresponding to in-dividual proof steps, since per-instance alignment handles verbalizations for only one semantic unit at a time."
7[URL_CITE]for all our data.
"Fortunately,[REF_CITE]showed that for Nuprl, one proof step roughly corre-sponds to one sentence in a natural language verbal-ization."
"So, we align Nuprl steps with verbalization sentences using dynamic programming based on the number of symbols common to both the step and the verbalization."
We also did some manual cleaning on the training data to reduce noise for subsequent stages. [Footnote_8]
"8 We employed pattern-matching tools to fix incorrect sentence boundaries, converted non-ascii symbols to a human-readable format, and discarded a few verbaliza-tions which were unrelated to the underlying proof."
"We first evaluated three individual components of our system: paraphrase thesaurus induction, argument-value selection in slotted lattice induction, and template induction."
"We also validated the utility of multi-parallel, as opposed to one-parallel, data."
Paraphrase thesaurus We presented two judges with all 71 paraphrase pairs produced by our system.
Argument-value selection We next measured how well our system matches semantic argument val-ues with lattice node sequences.
We randomly se-lected 20 Nuprl steps and their corresponding verbal-izations.
"From this sample, a Nuprl expert identified the argument values that appeared in at least one corresponding verbalization; of the 46 such values, our system correctly matched lattice node sequences to 91%."
"To study the relative effectiveness of using multi-parallel rather than one-parallel data, we also implemented a baseline system that used only one (randomly-selected) verbalization among the multi-ple possibilities."
"This single-verbalization baseline matched only 44% of the values correctly, indicating the value of a multi-parallel-corpus approach."
"Templates Thirdly, we randomly selected 20 in-duced templates; of these, a Nuprl expert determined that 85% were plausible verbalizations of the corre-sponding Nuprl."
"This was a very large improvement over the single-verbalization baseline’s 30%, again validating the multi-parallel-corpus approach."
"Finally, we evaluated the quality of the text our system generates by comparing its output against the system[REF_CITE], which produces accurate and readable Nuprl proof verbal-izations."
Their system has a hand-crafted lexical chooser derived via manual analysis of the same cor-pus that our system was trained on.
"To run the ex-periments, we replaced Holland-Minkley et. al’s lexi-cal chooser with the mapping dictionary we induced. (An alternative evaluation would have been to com-pare our output with human-authored texts."
"But this wouldn’t have allowed us to evaluate the perfor-mance of the lexical chooser alone, as human proof generation may differ in aspects other than lexical choice.)"
"The test set serving as input to the two sys-tems consisted of 20 held-out proofs, unseen through-out the entirety of our algorithm development work."
We evaluated the texts on two dimensions: readabil-ity and fidelity to the mathematical semantics.
Readability[REF_CITE]judges to compare the readability of the texts produced from the same Nuprl proof input: Figure 7(ii) and (iii) show an example text pair. [Footnote_9] (The judges were not given the original Nuprl proofs.)
"9 To prevent judges from identifying the system pro-ducing the text, the order of presentation of the two sys-tems’ output was randomly chosen anew for each proof."
Figure 8 shows the results.
"Good entries are those that are not preferences for the traditional system, since our goal, after all, is to show that MSA-based techniques can produce out-put as good or better than a hand-crafted system."
"We see that for every lemma and for every judge, our system performed quite well."
"Furthermore, for more than half of the lemmas, more than half the judges found our system’s output to be distinctly better than the traditional system’s."
"Fidelity We asked a Nuprl-familiar expert in formal logic to determine, given the Nuprl proofs and output texts, whether the texts preserved the main ideas of the formal proofs without introducing ambiguities."
"Nuprl creates proofs at a higher level of abstrac-tion than other provers do, so we were able to learn verbalizations directly from the Nuprl proofs them-selves."
"In other natural-language proof generation systems[REF_CITE]and other generation applications, the seman-tic expressions to be realized are the product of the system’s content planning component, not the proof or data."
"But our techniques can still be incorporated into such systems, because we can map verbalizations to the content planner’s output."
"Hence, we believe our approach generalizes to other settings."
Previous research on statistical generation has ad-dressed different problems.
"Some systems learn from verbalizations annotated with semantic con-cepts[REF_CITE]; in contrast, we use un-annotated corpora."
"Other work focuses on surface realization — choosing among different lexical and syntactic options sup-plied by the lexical chooser and sentence planner — rather than on creating the mapping dictionary; although such work also uses lattices as input to the stochastic realizer, the lattices themselves are constructed by traditional knowledge-based means[REF_CITE]."
An exciting direction for future research is to apply these statistical surface realization meth-ods to the lattices our method produces.
Word lattices are commonly used in speech recog-nition to represent different transcription hypothe-ses.
"Using alignment for grammar and lexicon in-duction has been an active area of research, both in monolingual settings (van[REF_CITE]) and in machine translation (MT)[REF_CITE]— interestingly, statistical MT techniques have been used to derive lexico-semantic mappings in the “reverse” direction of language understanding rather than generati[REF_CITE]."
"In a preliminary study, applying IBM-style alignment models in a black-box manner (i.e., without modifi-cation) to our setting did not yield promising results[REF_CITE]."
"On the other hand, MT systems can often model crossing alignment situations; these are rare in our data, but we hope to account for them in future work."
"While recent proposals for evaluation of MT sys-tems have involved multi-parallel corpora[REF_CITE], statis-tical MT algorithms typically only use one-parallel data."
"Simard’s (1999) trilingual (rather than multi-parallel) corpus method, which also computes MSAs, is a notable exception, but he reports mixed exper-imental results."
"In contrast, we have shown that through application of a novel composition of align-ment steps, we can leverage multi-parallel corpora to create high-quality mapping dictionaries supporting effective text generation."
We describe a hybrid approach to improv-ing search performance by providing a natural language front end to a traditional keyword-based search engine.
"The key component of the system is iterative query formulation and retrieval, in which one or more queries are automatically formulated from the user’s question, issued to the search engine, and the results accumulated to form the hit list."
"New queries are gener-ated by relaxing previously-issued queries using transformation rules, applied in an order obtained by reinforcement learning."
"This statistical component is augmented by a knowledge-driven hub-page identi-fier that retrieves a hub-page for the most salient noun phrase in the question, if possible."
"Evaluation on an unseen test set over the[URL_CITE]public web-site with 1.3 million webpages shows that both components make substantial contri-bution to improving search performance, achieving a combined 137% relative im-provement in the number of questions cor-rectly answered, compared to a baseline of keyword queries consisting of two noun phrases."
Keyword-based search engines have been one of the most highly utilized internet tools in recent years.
"Nevertheless, search performance remains unsatis-factory at most e-commerce sites[REF_CITE]."
"Librarians and search professionals have tra-ditionally favored Boolean keyword search systems, which, when successful, return a small set of rele-vant hits."
"However, the success of these systems crit-ically depends on the choice of the right keywords and the appropriate Boolean operators."
"As the pop-ulation of search engine users has grown beyond a small dedicated search professional community and as these new users are less familiar with the contents they are searching, it has become harder for them to formulate successful keyword queries."
"To improve search performance, one can improve search engine accuracy with respect to fixed keyword queries, or provide the search engine with better queries, those more likely to retrieve good results."
"While there is much on-going work in the IR community on the former topic, we have taken the latter approach by providing a natural language search interface and automatically generating keyword queries that uti-lize advanced search features typically unused by end users."
"We believe that natural language ques-tions are easier for users to construct than keyword queries, thus shifting the burden of optimal query formulation from the user to the system."
Such ques-tions also eliminate much of the ambiguity of key-word queries that often leads to poor results.
"Fur-thermore, the methodology we describe may be ap-plied to different search engines with only minor modification."
"To transform natural language input into a search query, the system must identify information perti-nent for search and utilize it to formulate keyword queries likely to retrieve relevant answers."
"We de-scribe and evaluate a hybrid system, RISQUE, that adopts an iterative approach to query formulation and retrieval for search on the[URL_CITE]pub- lic website with 1.3 million webpages."
"RISQUE may issue multiple queries per question, where a new query is generated by relaxing a previously is-sued query via transformation rule application, in an order obtained by reinforcement learning."
"In ad-dition, RISQUE identifies a hub-page for the most salient noun phrase in the question, if possible, utilizing traditional knowledge-driven mechanisms."
"Evaluation on an unseen test set showed that both the machine-learned and knowledge-driven compo-nents made substantial contribution to improving RISQUE’s performance, resulting in a combined 137% relative improvement in the number of ques-tions correctly answered, compared to a baseline ob-tained by queries consisting of two noun phrases (2NP baseline)."
"The popularity of natural language search is evi-denced by the growing number of search engines, such as AskJeeves, Electric Knowledge, and North-ern Light, [Footnote_1] that offer such functionality."
"For most sites, we were only able to perform a cursory ex-amination of their proprietary techniques."
"Adopt-ing a similar approach as FAQFinder[REF_CITE], AskJeeves maintains a database of questions and webpages that provide answers to them."
"User questions are compared against those in the database, and links to webpages for the closest matches are returned."
"Similar to our approach, Elec-tric Knowledge transforms a natural language ques-tion into a series of increasingly more general key-word queries[REF_CITE]."
"However, their query formulation process utilizes hard-crafted regular ex-pressions, while we adopt a more general machine learning approach for transformation rule applica-tion."
"Our work is also closely related to question an-swering in the question analysis component (e.g.,[REF_CITE])."
"In partic-ular,[REF_CITE]also iteratively refor-mulate queries based partly on the search results."
"However, their mechanism for query reformulation is heuristic-based."
We utilized machine learning to optimize the query formulation process.
"To generate optimal keyword queries from natural language questions, we first analyzed a set of 502 questions related to the purchasing and support of ThinkPads (notebook computers) and their acces-sories, such as “How do I set up hibernation for my ThinkPad?” and “Show me all p3 laptops.”"
Our analysis focused on three tasks.
"First, we attempted to identify an exhaustive set of correct webpages for each question, where a correct webpage is one that contains either an answer to the question or a hyper-link to such a page."
"Second, we manually formu-lated successful keyword queries from the question, i.e., queries which retrieved at least one correct web-page."
"Third, we attempted to discover general pat-terns in how the natural language questions may be transformed into successful keyword queries."
Our analysis eliminated 110 questions for which no correct webpage was found.
"Of the remaining 392 questions, we identified, on average, 4.37 cor-rect webpages and 1.58 successful queries per ques-tion."
We found that the characteristics of success-ful queries varied greatly.
"In the simplest case, a successful query may contain all the content bear-ing NPs in the question, such as thinkpad AND “answering machine” for “Can I use my ThinkPad as an answering machine?” [Footnote_2]"
"2 Our search engine ([URL_CITE]accepts a con-junction of terms (a word, quoted phrase, or disjunction of words/phrases), and inclusion/exclusion of text strings in the URL, such as +url:support."
"In the vast majority of cases, however, more complex transformations were applied to the question to result in a successful query."
"For instance, a successful query for “How do I hook an external mouse to my laptop?” is (mouse OR mice) AND thinkpad AND +url:support."
"In this case, the head noun mouse was inflected, [Footnote_3] the premodifier external was dropped, hook was deleted, laptop was replaced by thinkpad, and a URL con-straint was applied."
3 Many commercial search engines purposely do not inflect search words to avoid overgeneralization of queries.
"We observed that in our corpus, most success-ful queries can be derived by applying one or more transformation rules to the NPs and verbs in the questions."
Table 1 shows the manually in- duced commonly-used transformation rules based on our corpus analysis.
"Though the rules were quite straightforward to identify, the order in which they should be applied to yield optimal overall perfor-mance was non-intuitive."
"In fact, the best order we manually derived did not yield sufficient per-formance improvement over our baseline (see Sec-tion 7)."
We further hypothesize that the optimal rule application sequence may be dependent on ques-tion characteristics.
"For example, DropVerb may be a higher priority rule for buy questions than for support questions, since the verbs indicative of buy questions (typically “buy” or “sell”) are often ab-sent in the target product pages."
"Therefore, we in-vestigated a machine learning approach to automat-ically obtain the optimal rule application sequence."
Our problem consists of obtaining an optimal strat-egy for choosing transformation rules to generate successful queries.
"A key feature of this problem is that feedback during training is often delayed, i.e., the positive effect of applying a rule may not be ap-parent until a successful query is constructed after the application of other rules."
"Thus, we adopt a rein-forcement learning approach to obtain this optimal strategy."
"We adopted the Q learning paradigm[REF_CITE]to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state."
"While in state s ∈ S and performing action a ∈ A, the learner receives a reward r(s,a), and advances to state s 0 = δ(s, a)."
"To learn an optimal control strategy that maxi- mizes the cumulative reward over time, an evalua-tion function Q(s, a) is defined as follows:"
"Q(s, a) ≡ r(s, a) + γ max a 0 Q(δ(s, a), a 0 ) (1)"
"In other words, Q(s,a) is the immediate reward, r(s, a), plus the discounted maximum future reward starting from the new state δ(s, a)."
"The Q learning algorithm iteratively selects an ac-tion and updates Q̂, an estimate of Q, as follows:"
"Q̂ n (s, a) ← (1 − α n )Q̂ n−1 (s, a) + (2) α n (r(s, a) + max a 0 Q̂ n−1 (s 0 , a 0 )) where s 0 = δ(s, a) and α n is inversely proportional to the number of times a state/action pair &lt;s,a&gt; has been visited up to the nth iteration of the algorithm. [Footnote_4] Once the system learns Q̂, it can select from the possible actions in state s based on Q̂(s, a i )."
4 Equation (2) modifies (1) by taking a decaying weighted average of the current Q̂ value and the new value to guarantee convergence of Q̂ in non-deterministic environments. We ex-plain in the next section why our query formulation problem in the Q learning framework is non-deterministic.
"To formulate our problem in the Q learning paradigm, we represent a state as a 6-tuple, &lt;qtype, url constraint, np phrase, num nps, num modifiers, num verbs&gt;, where: • qtype is buy or support depending on question classification. • url constraint is true or false, and determines if manually predefined URL restrictions will be applied in the query. • np phrase is true or false, and determines whether each NP will be searched for as a phrase or a conjunction of words. • num nps is an integer between 1 and 3, and determines how many NPs will be included in the query. • num modifiers is an integer between 0 and 2, and indicates the maximum number of premod-ifiers in each NP. • num verbs is 0 or 1, and determines if the verb will be included in the query."
This representation is chosen based on the rules identified in Section 3.
"The actions, A, include the first [Footnote_5] actions in Table 1, and the “undo” counterpart for each action. [Footnote_5]"
"5 Morphological and synonym expansions are applied at the outset, which was shown to result in better performance than optional application of those rules."
"5 Morphological and synonym expansions are applied at the outset, which was shown to result in better performance than optional application of those rules."
"Except for qtype, which remains static for a question, each remaining element in the tuple can be altered by one of the [Footnote_5] pairs of actions in a straightforward manner."
"5 Morphological and synonym expansions are applied at the outset, which was shown to result in better performance than optional application of those rules."
"The state, s, and the question, q, generate a unique keyword query which results in a hit list, h(s, q)."
"The hit list is evaluated for correctness, whose result is used to define the reward function as follows:   1 if h(s 0 , q) contains at least onecorrect webpage if h(s 0 , q) has no correct page &amp; r(s, a) =  0 |h(s 0 , q)| &lt; 10   −1 otherwise where s 0 = δ(s, a)."
"Note that our system operates in a non-deterministic environment because the reward is dependent not only on s and a, but also on q. [Footnote_6]"
"6 It is theoretically possible to encode pertinent information in q in the state representation, thus making the environment deterministic. However, data sparseness problems associated with such a representation makes it impractical."
"Having defined S, A, δ, and r, Q̂ is determined by applying the Q learning algorithm, using the update function in (2), to our corpus of 392 questions."
"For each question, an initial state is randomly selected within the bounds of the question."
"The system then iteratively selects and applies actions, and updates Q̂ until a successful query is generated or the maxi-mum number of iterations is reached (in our imple-mentation, 15)."
The training algorithm iterates over all questions in the training set and terminates when Q̂ converges.
"In addition to motivating machine learning based query transformation as our central approach to nat-ural language search, our analysis revealed the need for several other key system components."
"As shown in Figure 1, RISQUE adopts a hybrid architecture that combines the utility of traditional knowledge-based methods and statistical approaches."
"Given a question, RISQUE first performs question analy-sis by extracting pertinent information to be used in query formulation, such as the NPs, VPs, and ques-tion type, and then orders the NPs in terms of their relative salience."
This information is then used for hit list construction by two modules.
"The first com-ponent is the hub-page identifier, which retrieves, if possible, a hub page for the most salient NP in the question."
The second component is the Q learning based query formulation and retrieval module that iteratively generates queries via transformation rule application and issues them to the search engine.
The results from both processes are combined and accumulated until n distinct hits are retrieved.
"In addition to the above components, RISQUE employs an ontology for the ThinkPad domain, which consists of 1) a hierarchy of about 500 do-main objects, 2) nearly 400 instances of relation-ships, such as isa and accessory-of, between objects, and 3) a synonym dictionary containing about 1000 synsets."
The ontology was manually constructed and took approximately 2 person-months for cov-erage in the ThinkPad domain.
"It provides perti-nent information to the question pre-processing and query formulation modules, which we will describe in the next sections."
RISQUE’s question understanding component is based primarily on a rule-driven parser in the slot grammar framework[REF_CITE].
The result-ing parse tree is first analyzed for NP/VP extrac-tion.
"Each NP includes the head noun and up to two premodifiers, which covers most NPs in our do-main."
"The NPs are further processed by a named-entity recognizer[REF_CITE], with reference to domain-specific proper names in our ontology."
"Recognized compound terms, such as “hard drive”, are treated as single en-tities, rather than as head nouns (“drive”) with pre-modifiers (“hard”)."
This prevents part of the com-pound term from being dropped when the DropMod-ifier transformation rule is applied.
The parse tree is also used to classify the question as buy or support.
The classifier utilizes a set of rules based on lexical and part-of-speech information.
"For example, “how” tagged as a adverb (as in “How do I ...”) suggests a support question, while “buy/sell” used as a verb indicates a buy question."
These rules were manually derived based on our training data.
"Our analysis showed that when a successful query is to contain fewer NPs than in the question, it is not straightforward to determine which NPs to elim-inate, as it requires both domain and content knowl-edge."
"However, we observed that less salient NPs are often removed first, where salience indicates the importance of the term in the search process."
"The relative salience of NPs in this context can, for the most part, be determined based on the ontological relationship between the NPs and knowledge about the website organization."
"For instance, if A is an accessory-of B, then A is more salient than B since, on our website, accessories typically have their own webpages with significantly more information than pages about, for instance, the ThinkPads with which they are compatible."
"Our NP sequencer utilizes a rule-based reasoning mechanism to rank a set of NPs based on their rel-ative salience, as determined by their relationship in the ontology. [Footnote_7] Objects not present in the ontol- ogy are considered less important than those present."
7 We are aware that factors involving deeper question under-
This process produces a list of NPs ranked in de-creasing order of salience.
"As with most websites, the ThinkPad pages[URL_CITE]are organized hierarchically, with a dozen or so hub-pages that serve as good starting points for specific sub-topics, such as mobile acces-sories and personal printers."
"However, since these hub-pages are typically not content-rich, they often do not receive high scores from the search engine (over which we have no control)."
"Thus, we devel-oped a mechanism to explicitly retrieve these hub-pages when appropriate, and to combine its results with the outcome of the actual search process."
The hub-page identifier consists of a mapping from a subset of the named entities in the ontology to their corresponding hub-pages. [Footnote_8]
"8 For reasons of robustness, we actually map a named entity to manually selected keywords which, when issued to the search engine, retrieves the desired hub-page as the first hit."
"For each question, the hub-page identifier retrieves the hub-page for the most salient NP, if possible, which is presented as the first entry in the hit list."
"This main component of RISQUE iteratively formu-lates queries, issues them to the search engine, and accumulates the results to construct the hit list."
"The query formulation process starts with the most con-strained query, and each new query is a relaxation of a previously issued query, obtained by applying one or more transformation rules to the current query."
The transformation rules are applied in the order ob-tained by the Q learning algorithm as described in Section 4.2.
"The initial state of the query formulation process is as follows: url constraint and np phrase are set to true, while the other attributes are set to their re-spective maximum values based on the outcome of the question understanding process."
"This initial state represents the most constrained query possible for the given question, and allows for subsequent relax-ation via the application of transformation rules."
"When a state s, is visited, a query is generated based on s and the question."
"The query terms are instantiated based on the values of np phrase, num nps, num modifiers, and num verbs in s and the question itself, while URL constraints may be applied based on url constraint and qtype."
"Finally, synonyms expansion is applied based on the syn-onym dictionary in the ontology, while morphologi-cal expansion is performed on all NPs using a rule-based inflection procedure."
"After a query is issued, the search results are incorporated into the hit list, and duplicate hits are removed."
"A transformation rule a max = argmax a Q̂(s, a) is applied to yield the new state."
"Q̂(s, a max ) is then decreased to remove it from fur-ther consideration."
This iterative process continues until the hit list contains 10 or more elements.
"To illustrate RISQUE’s end-to-end operation, con-sider the question “Do you sell a USB hub for a ThinkPad?”"
"The question is classified as a buy question, given presence of the verb sell."
"In addition, two NPs are identified:"
NP1: head = USB hub NP2: head = ThinkPad
Note that “USB hub” is identified as a compound noun by our named-entity recognizer.
The NP se-quencer determines that USB hub is more salient than ThinkPad since the former is an accessory of the latter.
"The hub-page identifier finds the networking de-vices hub-page for USB hub, presented as the first entry in the hit list in Figure 2, where correct web-pages are boldfaced."
"Next, RISQUE invokes its iterative query for-mulation process to populate the remaining hit list entries."
"The initial state is &lt;qtype = buy, url constraint = true, np phrase = true, num nps = 2, num modifiers = 0, num verbs = 0&gt;."
"This state generates the query shown as “Query 2” in Fig-ure 2, which results in 6 hits, of which 4 are correct."
"RISQUE selects the optimal transformation rule for the current state, which is ReinstateModifier."
"Since neither NP has any modifier, a second rule, RelaxNP is attempted, which resulted in a new query that did not retrieve any previously unseen hits."
"Next, RISQUE selects ConstrainNP and RelaxURL, resulting in the query shown as “Query 3” in Fig-ure 2. [Footnote_9]"
9 A set of negative URL constraints is applied at all times to best exclude parts of the website unrelated to ThinkPads.
Note that relaxing the URL constraint results in retrieval of USB hub support pages.
We evaluated RISQUE’s performance on 102 ques-tions in the ThinkPad domain previously unseen to both RISQUE’s knowledge-based and statistical components.
"A 2NP baseline was ob-tained by extracting up to two most salient NPs in each question, searching for the conjunction of all words in the NPs, and manually evaluating the [Footnote_10] top hits returned."
"10 This is likely too high an estimate for current keyword search performance, since the majority of user queries employ only one noun phrase."
"We selected the 2NP baseline based on statistics of keyword query logs on our website, which show that 98.2% of all queries contain 4 keywords or less."
"Furthermore, most three and four-word queries con-tain two distinct noun phrases, such as “visualage for java” and “application framework for e-business”."
"Thus, we use the 2NP baseline as an approximation of user keyword search performance for our natural language questions. [Footnote_10]"
"10 This is likely too high an estimate for current keyword search performance, since the majority of user queries employ only one noun phrase."
We compared RISQUE’s performance to the baseline using three metrics: [Footnote_11] 1.
"11 We chose not to evaluate our results using the traditional IR recall measure because for our task, it is often sufficient to return one page that answers the question instead of attempting to retrieve all relevant pages."
Total correct: number of questions for which at least one correct webpage is retrieved. 2.
Average correct: average number of correct webpages retrieved per question. 3.
Average rank: average rank of the first correct webpage in the hit list.
"The evaluation results are summarized in Table 2, where the first and last rows show the 2NP base-line and RISQUE’s performance, respectively."
"The results show that RISQUE correctly answered 71 questions, a 137% relative improvement over the baseline."
"Furthermore, the average number of cor-rect answers found nearly tripled, while, on average, the rank of the first correct answer improved from 4.0 to 2.11."
"Table 2 further shows performance figures that evaluate the individual contribution of RISQUE’s two main components, the hub-page identifier and the iterative query formulation module."
"Comparison between the last two rows in Table 2 shows the effec-tiveness of the hub-page identifier, which substan-tially increased the number of questions correctly answered, but resulted in only minor gain using the other two performance metrics."
"To assess the effec-tiveness of the query formulation module, we used the best manually-derived rule application sequence obtained in Section 3."
We compared these fixed or-der performance figures to those for RISQUE w/o hub identifier which shows that applying Q learning to derive an optimal state-dependent rule application order resulted in fairly substantial improvement us-
"One of RISQUE’s parameters, maxq, specifies the maximum number of distinct queries it can issue to the search engine for each question."
Table 3 shows the average number of queries actually issued for select values of maxq. [Footnote_12] Figure 3 shows how per-formance degrades when fewer queries are issued as a result of lowering maxq for both RISQUE and RISQUE without the hub-page identifier.
12[REF_CITE]for the results in Table 2.
"It shows that, with the exception of RISQUE’s performance when only one query is issued, 13 the number of questions answered have a near-linear relationship with the number of queries issued for both sys-tems."
"Notice that without the hub-page identifier, RISQUE’s performance when issuing an average of 1.93 queries per question is nearly the same as that of the 2NP baseline, while it performs worse than the baseline when issuing only one query per ques-tion."
"This is because our iterative query formula-tion process intentionally begins with the most con-strained query, resulting in an empty hit list in many cases."
"In this paper, we described and evaluated RISQUE, a hybrid system for performing natural language search on a large company public website."
RISQUE utilizes a two-pronged approach to generate hit lists for answering natural language questions.
"On the one hand, RISQUE employs a hub-page identi-fier to retrieve, when possible, a hub-page for the most salient NP in the question."
"On the other hand, RISQUE adopts a statistical iterative query formulation and retrieval mechanism that generates new queries by applying transformation rules to previously-issued queries."
"By employing these two components in parallel, RISQUE takes advantages of both knowledge-driven and machine learning ap-proaches, and achieves an overall 137% relative im-provement in the number of questions correctly an-swered on an unseen test set, compared to a baseline of 2NP keyword queries."
"In our current work, we are focusing on expand-ing system coverage to other domains."
"In particu-lar, we plan to investigate semi-automatic methods for extracting ontological knowledge from existing webpages and databases."
"While recent retrieval techniques do not limit the number of index terms, out-of-vocabulary (OOV) words are crucial in speech recognition."
"Aiming at retrieving information with spoken queries, we fill the gap between speech recognition and text retrieval in terms of the vocabulary size."
"Given a spoken query, we gener-ate a transcription and detect OOV words through speech recognition."
"We then cor-respond detected OOV words to terms in-dexed in a target collection to complete the transcription, and search the collection for documents relevant to the completed tran-scription."
We show the effectiveness of our method by way of experiments.
"Automatic speech recognition, which decodes hu-man voice to generate transcriptions, has of late become a practical technology."
"It is feasible that speech recognition is used in real-world human lan-guage applications, such as information retrieval."
"Initiated partially by TREC-6, various methods have been proposed for “spoken document retrieval (SDR),” in which written queries are used to search speech archives for relevant informati[REF_CITE]."
"State-of-the-art SDR methods, where speech recognition error rate is 20-30%, are comparable with text retrieval methods in perfor-mance[REF_CITE], and thus are already practical."
Possible rationales include that recogni-tion errors are overshadowed by a large number of words correctly transcribed in target documents.
"However, “speech-driven retrieval,” where spo-ken queries are used to retrieve (textual) informa-tion, has not fully been explored, although it is re-lated to numerous keyboard-less applications, such as telephone-based retrieval, car navigation systems, and user-friendly interfaces."
"Unlike spoken document retrieval, speech-driven retrieval is still a challenging task, because recogni-tion errors in short queries considerably decrease re-trieval accuracy."
A number of references addressing this issue can be found in past research literature.
"They used as test queries 35 topics in the TREC col-lection, dictated by a single male speaker."
"How-ever, these cases focused on improving text retrieval methods and did not address problems in improv-ing speech recognition."
"As a result, errors in recog-nizing spoken queries (error rate was approximately 30%) considerably decreased the retrieval accuracy."
"Although we showed that the use of target docu-ment collections in producing language models for speech recognition significantly improved the per-formance of speech-driven retrieval[REF_CITE], a number of issues still re-main open questions."
Section 2 clarifies problems addressed in this pa-per.
Section 3 overviews our speech-driven text retrieval system.
Sections 4-6 elaborate on our methodology.
"Section 7 describes comparative ex-periments, in which an existing IR test collection was used to evaluate the effectiveness of our method."
Section 8 discusses related research literature.
One major problem in speech-driven retrieval is re-lated to out-of-vocabulary (OOV) words.
"On the one hand, recent IR systems do not limit the vocabulary size (i.e., the number of index terms), and can be seen as open-vocabulary systems, which allow users to input any keywords contained in a tar-get collection."
It is often the case that a couple of million terms are indexed for a single IR system.
"On the other hand, state-of-the-art speech recog-nition systems still need to limit the vocabulary size (i.e., the number of words in a dictionary), due to problems in estimating statistical language mod-els[REF_CITE]and constraints associated with hardware, such as memories."
"In addition, compu-tation time is crucial for a real-time usage, including speech-driven retrieval."
"In view of these problems, for many languages the vocabulary size is limited to a couple of ten thousands ([REF_CITE]; Steeneken and van[REF_CITE]), which is incomparably smaller than the size of in-dexes for practical IR systems."
"In addition, high-frequency words, such as func-tional words and common nouns, are usually in-cluded in dictionaries and recognized with a high accuracy."
"However, those words are not necessarily useful for retrieval."
"On the contrary, low-frequency words appearing in specific documents are often ef-fective query terms."
"To sum up, the OOV problem is inherent in speech-driven retrieval, and we need to fill the gap between speech recognition and text retrieval in terms of the vocabulary size."
"In this paper, we pro-pose a method to resolve this problem aiming at open-vocabulary speech-driven retrieval."
"Figure 1 depicts the overall design of our speech-driven text retrieval system, which consists of speech recognition, text retrieval and query com-pletion modules."
"Although our system is cur-rently implemented for Japanese, our methodology is language-independent."
We explain the retrieval process based on this figure.
"Given a query spoken by a user, the speech recognition module uses a dictionary and acous-tic/language models to generate a transcription of the user speech."
"During this process, OOV words, which are not listed in the dictionary, are also de-tected."
"For this purpose, our language model in-cludes both words and syllables so that OOV words are transcribed as sequences of syllables."
"For example, in the case where “kankitsu (cit-rus)” is not listed in the dictionary, this word should be transcribed as /ka N ki tsu/. How-ever, it is possible that this word is mistak-enly transcribed, such as /ka N ke tsu/ and /ka N ke tsu ke ko/."
"To improve the quality of our system, these sylla-ble sequences have to be transcribed as words, which is one of the central issues in this paper."
"In the case of speech-driven retrieval, where users usually have specific information needs, it is feasible that users utter contents related to a target collection."
"In other words, there is a great possibility that detected OOV words can be identified as index terms that are pho-netically identical or similar."
"However, since a) a single sound can potentially correspond to more than one word (i.e., homonyms) and b) searching the entire collection for phoneti-cally identical/similar terms is prohibitive, we need an efficient disambiguation method."
"Specifically, in the case of Japanese, the homonym problem is mul-tiply crucial because words consist of different char-acter types, i.e., “kanji,” “katakana,” “hiragana,” al-phabets and other characters like numerals [Footnote_1] ."
"1 In Japanese, kanji (or Chinese character) is the idiogram, and katakana and hiragana are phonograms."
"To resolve this problem, we use a two-stage re-trieval method."
"In the first stage, we delete OOV words from the transcription, and perform text re-trieval using remaining words, to obtain a specific number of top-ranked documents according to the degree of relevance."
"Even if speech recognition is not perfect, these documents are potentially associ-ated with the user speech more than the entire col- lection."
"Thus, we search only these documents for index terms corresponding to detected OOV words."
"Then, in the second stage, we replace detected OOV words with identified index terms so as to complete the transcription, and re-perform text re-trieval to obtain final outputs."
"However, we do not re-perform speech recognition in the second stage."
"In the above example, let us assume that the user also utters words related to “kankitsu (citrus),” such as “orenji (orange)” and “remon (lemon),” and that these words are correctly recognized as words."
"In this case, it is possible that retrieved documents contain the word “kankitsu (citrus).”"
"Thus, we re-place the syllable sequence /ka N ke tsu/ in the query with “kankitsu,” which is additionally used as a query term in the second stage."
"It may be argued that our method resembles the notion of pseudo-relevance feedback (or local feed-back) for IR, where documents obtained in the first stage are used to expand query terms, and final out-puts are refined in the second stage[REF_CITE]."
"However, while relevance feedback is used to improve only the retrieval accuracy, our method im-proves the speech recognition and retrieval accuracy."
"The speech recognition module generates word se-quence , given phone sequence ."
"In a stochastic speech recognition framework (Bahl et al.,  1983), the task is to select the maximizing  , which is transformed as in Equation (1) through the Bayesian theorem.         (1)"
"Here,  models a probability that word se-quence is transformed into phone sequence , and  models a probability that is linguis-tically acceptable."
"These factors are usually called acoustic and language models, respectively."
"For the speech recognition module, we use the Japanese dictation toolkit[REF_CITE][URL_CITE] , which includes the “Julius” recognition engine and acoustic/language models."
"The acoustic model was produced by way of the ASJ speech database (ASJ-JNAS)[REF_CITE], which contains approximately 20,000 sentences uttered by 132 speakers including the both gender groups."
This toolkit also includes development softwares so that acoustic and language models can be pro-duced and replaced depending on the application.
"While we use the acoustic model provided in the toolkit, we use a new language model including both words and syllables."
"For this purpose, we used the “ChaSen” morphological analyzer [URL_CITE] to extract words from ten years worth of “Mainichi Shimbun” news-paper articles (1991-2000)."
"Then, we selected 20,000 high-frequency words to produce a dictionary."
"At the same time, we seg-mented remaining lower-frequency words into syl-lables based on the Japanese phonogram system."
The resultant number of syllable types was approxi-mately 700.
"Finally, we produced a word/syllable-based trigram language model."
"In other words, OOV words were modeled as sequences of syllables."
"Thus, by using our language model, OOV words can easily be detected."
"In spoken document retrieval, an open-vocabulary method, which combines recognition methods for words and syllables in target speech documents, was also proposed[REF_CITE]."
"However, this method requires an additional computation for rec-ognizing syllables, and thus is expensive."
"In con-trast, since our language model is a regular statistical -gram model, we can use the same speech recog-nition framework as in Equation (1)."
"The text retrieval module is based on the “Okapi” probabilistic retrieval method[REF_CITE], which is used to compute the rel-evance score between the transcribed query and each document in a target collection."
"To produce an in-verted file (i.e., an index), we use ChaSen to extract content words from documents as terms, and per-form a word-based indexing."
We also extract terms from transcribed queries using the same method.
"As explained in Section 3, the basis of the query completion module is to correspond OOV words de-tected by speech recognition (Section 4) to index terms used for text retrieval (Section 5)."
"However, to identify corresponding index terms efficiently, we limit the number of documents in the first stage re-trieval."
"In principle, terms that are indexed in top-ranked documents (those retrieved in the first stage) and have the same sound with detected OOV words can be corresponding terms."
"However, a single sound often corresponds to multiple words."
"In addition, since speech recog-nition on a syllable-by-syllable basis is not per-fect, it is possible that OOV words are incor-rectly transcribed."
"For example, in some cases the Japanese word “kankitsu (citrus)” is transcribed as /ka N ke tsu/."
"Thus, we also need to con-sider index terms that are phonetically similar to OOV words."
"To sum up, we need a disambiguation method to select appropriate corresponding terms, out of a number of candidates."
"Intuitively, it is feasible that appropriate terms: have identical/similar sound with OOV words detected in spoken queries, ! frequently appear in a top-ranked document set, and appear in higher-ranked documents."
"From the viewpoint of probability theory, possi-ble representations for the above three properties include Equation (2), where each property corre-sponds to different parameters."
"Our task is to select the maximizing the value computed by this equa-tion as the corresponding term for OOV word ! . ! ) *  *+ , &quot; #&amp;(% &apos;$ (2)"
"Here, . is the top-ranked document , set retrieved in the first stage, given query . ! is a probabil-ity that index term can be replaced with detected * OOV word ! , in terms of phonetics. * is */ the , relative frequency of term in * document .   , is a probability that document is relevant to query , which is associated with the score formalized in the Okapi method."
"However, from the viewpoint of empiricism, Equation (2) is not necessarily effective."
"First, it is not easy to estimate ! based on the probabil-ity theory."
"Second, the probability score computed by the Okapi method is an approximation focused mainly on relative superiority among retrieved / * , doc-uments, and thus it is difficult to estimate  in a rigorous manner."
"Finally, it is also difficult to deter-mine the degree to which each parameter influences in the final probability score."
"In view of these problems, through preliminary experiments we approximated Equation (2) and for-malized a method to compute the degree (not the probability) to which given index term corresponds to OOV word ! ."
"First, we estimate ! by the ratio between the number of syllables commonly included in both ! and and the total number of syllables in ! ."
"We use a DP matching method to identify the number of cases related to deletion, insertion, and substitution in ! , on a syllable-by-syllable basis."
"Second * , ! *+0, should be more influential than  and  in Equation (2), although the last two parameters are effective in the case where a large number of candidates phonetically similar * to are *+ , obtained."
"To decrease the effect of  and  , we tentatively use logarithms of these pa-rameters."
"In addition, we /* use , the score computed by the Okapi method as  ."
"According to the above approximation, we com-pute the score of as in Equation (3). ! 0)2 3  * ) */ , 4 $&quot; #&amp;%(&apos; (3)"
"It should be noted that Equation (3) is indepen-dent of the indexing method used, and therefore can be any sequences of characters contained in - . ."
"In other words, any types of indexing methods (e.g., word-based and phrase-based indexing meth-ods) can be used in our framework."
"Since computation time is crucial for a real-time us-age, we preprocess documents in a target collection so as to identify candidate terms efficiently."
This process is similar to the indexing process performed in the text retrieval module.
"In the case of text retrieval, index terms are orga-nized in an inverted file so that documents including terms that exactly match with query keywords can be retrieved efficiently."
"However, in the case of query completion, terms that are included in top-ranked documents need to be retrieved."
"In addition, to minimize a score computa-tion (for example, DP matching is time-consuming), it is desirable to delete terms that are associated with a diminished phonetic similarity value, ! , prior to the computation of Equation (3)."
"In other words, an index file for query completion has to be organized so that a partial matching method can be used."
"For example, /ka N ki tsu/ has to be re-trieved efficiently in response to /ka N ke tsu/."
"Thus, we implemented a forward/backward partial-matching method, in which entries can be re-trieved by any substrings from the first/last charac-ters."
"In addition, we index words and word-based bigrams, because preliminary experiments showed that OOV words detected by our speech recognition module are usually single words or short phrases, such as “ozon-houru (ozone hole).”"
"To evaluate the performance of our speech-driven re-trieval system, we used the IREX collection 4 ."
"This test collection, which resembles one used in the TREC ad hoc retrieval track, includes 30 Japanese topics (information need) and relevance assessment (correct judgement) for each topic, along with target documents."
"The target documents are 211,853 ar-ticles collected from two years worth of “Mainichi Shimbun” newspaper (1994-1995)."
"Each topic consists of the ID, description and nar-rative."
"While descriptions are short phrases related to the topic, narratives consist of one or more sen-tences describing the topic."
Figure 2 shows an exam-ple topic in the SGML form (translated into English by one of the organizers of the IREX workshop).
"However, since the IREX collection does not con-tain spoken queries, we asked four speakers (two males/females) to dictate the narrative field."
"Thus, we produced four different sets of 30 spoken queries."
"By using those queries, we compared the following different methods: 1. text-to-text retrieval, which used written narra-tives as queries, and can be seen as a perfect speech-driven text retrieval, 2. speech-driven text retrieval, in which only words listed in the dictionary were modeled in the language model (in other words, the OOV word detection and query completion modules were not used), 3. speech-driven text retrieval, in which OOV words detected in spoken queries were simply deleted (in other words, the query completion module was not used), [URL_CITE]. speech-driven text retrieval, in which our method proposed in Section 3 was used."
"In cases of methods 2-4, queries dictated by four speakers were used independently."
"Thus, in practice we compared 13 different retrieval results."
"In addi-tion, for methods 2-4, ten years worth of Mainichi Shimbun Japanese newspaper articles (1991-2000) were used to produce language models."
"However, while method 2 used only 20,000 high-frequency words for language modeling, methods 3 and 4 also used syllables extracted from lower-frequency words (see Section 4)."
"Following the IREX workshop, each method re-trieved 300 top documents in response to each query, and non-interpolated average precision values were used to evaluate each method."
"First, we evaluated the performance of detecting OOV words."
"Table 1 shows the results on a speaker-by-speaker basis, where “#Detected” and “#Correct” denote the total number of OOV words detected by our method and the number of OOV words correctly detected, respectively."
"In addition, “#Completed” denotes the number of detected OOV words that were corresponded to correct index terms in 300 top documents."
"It should be noted that “#Completed” was greater than “#Correct” because our method often mistak-enly detected words in the dictionary as OOV words, but completed them with index terms correctly."
"We estimated recall and precision for detecting OOV words, and accuracy for query completion, as in Equation (4)."
"Looking at Table 1, one can see that recall was gen-erally greater than precision."
"In other words, our method tended to detect as many OOV words as pos-sible."
"In addition, accuracy of query completion was relatively low."
"Figure 3 shows example words in spoken queries, detected as OOV words and correctly completed with index terms."
"In this figure, OOV words are transcribed with syllables, where “:” denotes a long vowel."
"Hyphens are inserted between Japanese words, which inherently lack lexical segmentation."
"Second, to evaluate the effectiveness of our query completion method more carefully, we compared re-trieval accuracy for methods 1-4 (see Section 7.1)."
"Table 2 shows average precision values, averaged over the 30 queries, for each method [Footnote_5] ."
"5 Average precision is often used to evaluate IR systems, which should not be confused with evaluation measures in Equation (4)."
"The average precision values of our method (i.e., method 4) was approximately 87% of that for text-to-text retrieval."
"By comparing methods 2-4, one can see that our method improved average precision values of the other methods irrespective of the speaker."
"To put it more precisely, by comparing methods 3 and 4, one can see the effectiveness of the query comple-tion method."
"In addition, by comparing methods 2 and 4, one can see that a combination of the OOV word detection and query completion methods was effective."
It may be argued that the improvement was rel-atively small.
"However, since the number of OOV words inherent in 30 queries was only 14, the effect of our method was overshadowed by a large number of other words."
"In fact, the number of words used as query terms for our method, averaged over the four speakers, was 421."
"Since existing test collec-tions for IR research were not produced to explore the OOV problem, it is difficult to derive conclu-sions that are statistically valid."
Experiments using larger-scale test collections where the OOV problem is more crucial need to be further explored.
"Finally, we investigated the time efficiency of our method, and found that CPU time required for the query completion process per detected OOV word was 3.5 seconds (AMD[REF_CITE]+)."
"How-ever, an additional CPU time for detecting OOV words, which can be performed in a conventional speech recognition process, was not crucial."
We manually analyzed seven cases where the av-erage precision value of our method was signifi-cantly lower than that obtained with method 2 (the total number of cases was the product of numbers of queries and speakers).
"Among these seven cases, in five cases our query completion method selected incorrect index terms, although correct index terms were included in top-ranked documents obtained with the first stage."
"For example, in the case of the query 1021 dictated by a female speaker, the word “seido (institution)” was mistakenly transcribed as /se N do/. As a result, the word “sendo (freshness),” which is associated with the same syllable sequences, was selected as the index term."
The word “seido (institution)” was the third candidate based on the score computed by Equation (3).
"To reduce these errors, we need to en-hance the score computation."
"In another case, our speech recognition module did not correctly recognize words in the dictionary, and decreased the retrieval accuracy."
"In the final case, a fragment of a narrative sen-tence consisting of ten words was detected as a sin-gle OOV word."
"As a result, our method, which can complete up to two word sequences, mistak-enly processed that word, and decreased the retrieval accuracy."
"However, this case was exceptional."
"In most cases, functional words, which were recog-nized with a high accuracy, segmented OOV words into shorter fragments."
The method proposed[REF_CITE]and our method are similar in the sense that both methods use target collections as language models for speech recognition to realize open-vocabulary speech-driven retrieval.
"Kupiec et al’s method, which is based on word recognition and accepts only short queries, derives multiple transcription candidates (i.e., possible word combinations), and searches a target collection for the most plausible word combination."
"However, in the case of longer queries, the number of candidates increases, and thus the searching cost is prohibitive."
This is a reason why operational speech recognition systems have to limit the vocabulary size.
"In contrast, our method, which is based on a re-cent continuous speech recognition framework, can accept longer sentences."
"Additionally, our method uses a two-stage retrieval principle to limit a search space in a target collection, and disambiguates only detected OOV words."
"Thus, the computation cost can be minimized."
"To facilitate retrieving information by spoken queries, the out-of-vocabulary problem in speech recognition needs to be resolved."
"In our proposed method, out-of-vocabulary words in a query are de-tected by speech recognition, and completed with terms indexed for text retrieval, so as to improve the recognition accuracy."
"In addition, the completed query is used to improve the retrieval accuracy."
We showed the effectiveness of our method by using dictated queries in the IREX collection.
Future work would include experiments using larger-scale test collections in various domains.
"In this paper, we address the problem of dealing with a large collection of data and propose a method for text classifi-cation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Ma-chines(SVMs)."
"NB is based on the as-sumption of word independence in a text, which makes the computation of it far more efficient."
"SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to pro-duce better performance."
"The training data for SVMs are extracted using NB classifiers according to the category hier-archies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy."
"As the volume of online documents has drastically increased, text classification has become more im-portant, and a growing number of statistical and ma-chine learning techniques have been applied to the task[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE]."
"Most of them use the[REF_CITE]articles [Footnote_1] in the evalu- ations of their methods, since the corpus has be-come a benchmark, and their results are thus eas-ily compared with other results."
"1 The[REF_CITE]distribution 1.0, is comprised of 21,578 documents, representing what remains of the original[REF_CITE]corpus after the elimination of 595 duplicates by Steve Lynch and David[REF_CITE]."
"It is generally agreed that these methods using statistical and ma-chine learning techniques are effective for classifi-cation task, since most of them showed significant improvement (the performance over 0.85 F1 score)[REF_CITE],[REF_CITE],[REF_CITE]."
"More recently, some researchers have applied their techniques to larger corpora such as web pages in Internet applications[REF_CITE],[REF_CITE],[REF_CITE]."
"The increasing number of documents and categories, however, often hampers the develop-ment of practical classification systems, mainly due to statistical, computational, and representational problems[REF_CITE]."
There are at least two strategies for solving these problems.
One is to use category hierarchies.
"The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are of-ten employed to make the large collection of cate-gories more manageable."
McCallum et. al. pre-sented a method called ‘shrinkage’ to improve pa-rameter estimates by taking advantage of a hierar-chy[REF_CITE].
"They tested their method us-ing three different real-world datasets: 20,000 ar-ticles from UseNet, 6,440 web pages from the in-dustry sector, and 14,831 pages from Yahoo, and showed improved performance."
"Dumais et. al. used SVMs and classified hierarchical web content consisting of 50,078 web pages for training, and 10,024 for testing, with promising results[REF_CITE]."
The other is to use  methods which are learning algorithms that construct a set of classifiers and then classify new data by taking a (weighted) vote of their predictions[REF_CITE].
One of the methods for constructing ensembles manipu-lates the training examples to generate multiple hy-potheses  .
The most straightforward way is called .
It presents the learning algorithm with a training set that consists of a sample of examples drawn randomly with replacement from the original training set.
The second method is to construct the training sets by leaving out disjoint subsets of the training data.
The third is illustrated by the AD-ABOOST algorithm[REF_CITE].
Dietterich has compared these methods[REF_CITE].
"He reported that in low-noise data, AD-ABOOST performs well, while in high-noise cases, it yields overfitting because ADABOOST puts a large amount of weight on the mislabeled examples."
"Bagging works well on both the noisy and the noise-free data because it focuses on the statistical prob-lem which arises when the amount of training data available is too small, and noise increases this sta-tistical problem."
"However, it is not clear whether ‘works well’ means that it exponentially reduces the amount of computation necessary for classification, while sacrificing only a small amount of accuracy, or whether it is statistically significantly better than other methods."
"In this paper, we address the problem of dealing with a large collection of data and report on an em-pirical study for text classification which manipu-lates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs)."
NB probabilistic classifiers are = based on the assumption of word independence in a text which makes the computation of the NB classi-fiers far more efficient.
"SVMs, on the other hand, have the potential to handle large feature spaces, since SVMs use overfitting protection which does not necessarily depend on the number of features, and thus makes it possible to produce better perfor-mance."
The basic idea of our approach is quite sim-ple: We solve simple classification problems using NB and more complex and difficult problems using SVMs.
"As in previous research, we use category hierarchies."
We use all the training data for NB.
"The training data for SVMs, on the other hand, is extracted using NB classifiers."
"The training data is learned by NB using cross-validation according to the hierarchical structure of categories, and only the documents which could not classify correctly by NB classifiers in each category level are extracted as the training data of SVMs."
The rest of the paper is organized as follows.
The next section provides the basic framework of NB and SVMs.
We then describe our classification method.
"Finally, we report some experiments using 279,303 documents in the[REF_CITE]corpus with a discus-sion of evaluation."
Naive Bayes(NB) probabilistic classifiers are com-monly studied in machine learning[REF_CITE].
The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document.
The NB assumption is that all the words in a text are conditionally independent given the value of a clas-sification variable.
There are several versions of the NB classifiers.
Recent studies on a Naive Bayes classifier which is proposed by McCallum et. al. reported high performance over some other com-monly used versions of NB on several data collec-tions[REF_CITE].
"We use the model of NB by McCallum et. al. which is shown in formula (1). ! %#&quot; &apos;$ &amp; ( )*#&quot; $ +-, .0/ , (5 . / 7 *#&quot; $ :, 89243, ( : )#*&quot; $ + ,.;2/&lt;3, (5 . /(6 7 : %#&quot; $ ?&gt;@ (  #*&quot; $D&amp; , GH, (5 B  $ (  $  F , N@,"
"O 2 3 ,[REF_CITE]I (5 O  $ ( $ , GH, ( *#&quot; D$ &amp; (  0T$ (1) 243"
"W XYW W [Z W refers to the number of vocabularies, de-notes W ] \ W the number of labeled training documents W ^)`W_ , and shows the number of categories. denotes document length. =ba 1 is the word in position c of document , where the subscript of = , ^ (_ d indicates ^ _ an index into the vocabulary. [e f hg= ;i ^ _;j denotes the number k ^ of _nj times word = gk occurs ^ in _ojqpsr document ^S_ , and  is defined by  0,1 t ."
SVMs are introduced by Vapnik[REF_CITE]for solving two-class pattern recognition problems.
It is defined over a vector space where the problem is to find a decision surface(classifier) that ‘best’ sep-arates a set of positive examples from a set of nega-tive examples by introducing the maximum ‘margin’ between two sets.
The margin is defined by the dis-tance from the hyperplane to the nearest of the pos-itive and negative examples.
"The decision surface produced by SVMs for linearly separable space is a hyperplane pzyh{ which p|y can be written as  + = 0 ( x , u , ), where x is an arbitrary data point, and u = ( ~= } ,  , = { ) and are learned from a train-ing set of linearly separable data."
Figure 1 shows an example of a simple two-dimensional problem that is linearly separable [Footnote_2] .
"2 We focused on linear hypotheses in this work , while SVMs can handle nonlinear hypotheses using  functions."
"In the linearly separable case maximizing the margin can be expressed as an optimization problem:   F 3 |SJ  243   &lt;%  (2)  s.t  243   &amp;      &amp;    (3) _ _ } _ where _ x = (  ,  ,  { ) is the -th training example and  is a label corresponding the -th training d ex-ample."
"In formula (3), each element of w, = (1    ) corresponds to each word  in ¢ the £_ training ) ¤ _ _ _(d ex-amples, and the larger value of = is,   the more the word = d features positive examples."
"We note that SVMs are basically introduced for solving binary classification, while text classifica-tion is a multi-class, multi-label classification prob-lem."
"Several methods using SVMs which were in-tended for multi-class, multi-label data have been proposed   (  Weston @&gt;  and[REF_CITE])."
We use ¥  - - - ¨ version of the SVMs model in the work j« .
"A time j complexity of SVMs is known as ¥©f ª ¥bf ­¬ , where is the number of train-ing  data  . @&gt; We consider  a time complexity of ¥  - - - ¨ method."
Let be the number of training data with c categories.
The average d£ size of the training j data per category is .
"Let also cR®¯v4°©f ® be the j time needed to train all cat-egories, where °±f ® represents the time for learn-ing one binary classifier using ® training data, and c ® is the number of binary classifier j ."
"The time for learning j²¢ \ one binary classifier \ , °©f ® is represented   as °©f ® v ® ¬ , where is a constant. ¥  - - @&gt; - ¨  method is thus done in time \ c 0¬ ."
"A well-known technique for classifying a large, het-erogeneous collection such as web content is to use category hierarchies."
"Following the approaches of Koller and Sahami[REF_CITE], and Dumais’s[REF_CITE], we employ a hi-erarchy by learning separate classifiers at each in-ternal node of the tree, and then labeling a docu-ment using these classifiers to greedily select sub-branches until a leaf is reached."
Our hypothesis regarding NB is that it can work well for documents which are assigned to only one cate-gory within the same category level in the hierar-chical structure.
"We base this on some recent papers claiming that NB methods perform surprisingly well for an ‘accuracy’ measure which is equivalent to the standard precision under the one-category-per-document assumption on classifiers and also equiva-lent to the standard recall, assuming that each docu-ment has one and only one correct category per cat- egory level[REF_CITE],[REF_CITE]."
"SVMs, on the other hand, have the potential to handle more complex problems without sacrificing accuracy, even though the computation of the SVM classifiers is far less efficient than NB."
"We thus use NB for simple classification problems and SVMs for more complex data, i.e., the data which cannot be classified correctly by NB classifiers."
We use ten-fold cross validation: All of the training data were randomly shuffled and divided into ten equal folds.
Nine folds were used to train the NB clas-sifiers while the remaining fold(held-out test data) was used to evaluate the accuracy of the classifica-tion.
"For each category level, we apply the following procedures."
"Let eb³ be the total number of nine folds training documents, and e´ be the number of the re-maining fold in each class level."
"Figure 2 illustrates the flow of our system. 1. Extracting training data using NB 1-1 NB is applied to the ~e ³ documents, and clas-sifiers for each category are induced."
"They are evaluated using the held-out test data, the e ´ documents. 1-2 This process is repeated ten times so that each fold serves as the source of the test data once."
"The threshold, the probability value which pro-duces the most accurate classifier through ten runs, is selected. 1-3 The held-out test data which could not be clas-sified correctly by NB classifiers with the opti-mal parameters are extracted ( e-µn¶n¶C·C¶ in Figure 2)."
They are used to train SVMs.
"The procedure is applied to each category level. 2. Classifying test data 2-1 We use all the training data, e~³ + e¸´ , to train NB classifiers and the data which is produced by procedure 1-3 to train SVMs. 2-2 NB classifiers are applied to the test data."
"The test data is judged to be the category l whose probability is larger than the threshold which is obtained by 1-2. 2-3 If the test data is not assigned to any one of the categories, the test data is classified by SVMs classifiers."
The test data is } judged to be the cat-egory l whose distance ¹ ¹`» º ¹¼¹ is larger than zero.
We employ the hierarchy by learning separate classi-fiers at each internal node of the tree and then assign categories to a document by using these classifiers to greedily select sub-branches until a leaf is reached.
We evaluated the method using the 1996 Reuters corpus recently made available.
"The corpus from 20th Aug. to 31st Dec. consists of 279,303 doc-uments."
These documents are organized into 126 categories with a four level hierarchy.
"The number of cate-gories in each level is 25 top level, 33 second level, 43 third level, and 1 fourth level, respectively."
Table 1 shows the number of documents in each top level category.
"After eliminating unlabelled documents, we ob-tained 271,171 documents."
"We divided these docu-ments into two sets: a training set from 20th Aug. to 31th Oct. which consists of 150,939 documents, and test set from 1th Nov. to 31st Dec. which consists of 120,242 documents."
"We obtained a vo-cabulary of 183,400 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger[REF_CITE], and stop word re-moval."
Figure 3 illustrates the category distribution in the training set.
The number of categories per document is 3.2 on average.
We use ten-fold cross validation for learning NB parameters.
"For evaluating the effectiveness of cate-gory assignments, we use the standard recall, preci-sion, and ½¿¾ measures."
Recall denotes the ratio of correct assignments by the system divided by the to-tal number of correct assignments.
Precision is the ratio of correct assignments by the system divided by the total number of the system’s assignments.
The ½b¾ measure which combines recall ( A ) and pre-cision ( À ) with an equal weight is   À jq¢ ª ¶CÁ)Á .
"The result is shown in Table 2. ‘NB’, ‘SVMs’, and ‘Manipulating data’ denotes the result using Naive Bayes, SVMs classifiers, and our method, respectively. ‘miR’, ‘miP’, and ‘miF1’ refers to the micro-averaged recall, precision, and F1, respectively. ‘all’ in Table 2 shows the results of all 102 categories."
The micro-averaged F1 score of our method in ‘all’ (0.704) is higher than the NB (0.519) and SVMs scores (0.285).
We note that the F1 score of SVMs (0.285) is significantly lower than other models.
"This is because we could not obtain a classifier to judge the category ‘corporate/industrial’ in the top level within 10 days using a standard 2.4 GHz Pentium IV PC with 1,500 MB of RAM."
We thus eliminated the category and its child categories from the 102 categories.
"The number of the remain-ing categories in each level is 24 top, 14 second, 29 third, and 1 fourth level. ‘Parts’ in Table 2 de-notes the results."
"There is no significant difference between ‘all’ and ‘parts’ in our method, as the F1 score of ‘all’ was 0.704 and ‘parts’ was 0.700."
The F1 of our method in ‘parts’ is also higher than the NB and SVMs scores.
Table 3 denotes the amount of training data used to train NB and SVMs in our method and test data judged by each classifier.
"We can see that our method makes the computation of the SVMs more efficient, since the data trained by SVMs is only 23,243 from 150,939 documents."
Table 4 illustrates the results of three methods ac-cording to each category level. ‘Training’ in ‘Ma-nipulating data’ denotes the number of documents used to train SVMs.
"The overall F1 value of NB, SVMs, and our method for the 25 top-level cate- gories is 0.693, 0.341, and 0.715, respectively."
"Clas-sifying large corpora with similar categories is a difficult task, so we did not expect to have excep-tionally high accuracy[REF_CITE](0.85 F1 score)."
"Performance on the original training set us-ing SVMs is 0.285 and using NB is 0.519, so this is a difficult learning task and generalization to the test set is quite reasonable."
"There is no significant difference between the overall F1 value of the second(0.608) and third level categories(0.606) in our method, while the accuracy of the other methods drops when classifiers select sub-branches, in third level categories."
"As Dumais et. al. mentioned, the results of our experiment show that performance varies widely across cate-gories."
"The highest F1 score is 0.864 (‘Commodity markets’ category), and the lowest is 0.284 (‘Eco- nomic performance’ category)."
The overall F1 values obtained by three methods for the fourth level category (‘Annual result’) are low.
"This is because there is only one category in the level, and we thus used all of the training data, 150,939 documents, to learn models."
"The contribution of the hierarchical structure is best explained by looking at the results with and without category hierarchies, as illustrated in Table 5."
"It is interesting to note that the results of both NB and our method clearly demonstrate that incor-porating category hierarchies into the classification method improves performance, whereas hierarchies degraded the performance of SVMs."
This shows that the separation of one top level category(C) from the set of the other 24 top level categories is more dif-ficult than separating C from the set of all the other 101 categories in SVMs.
Table 6 illustrates sample words which have the highest weighted value calculated using formula d (3).
"Recall that in SVMs each value of word = (1  ) is calculated d using formula (3), and d the cV larger value of = is, the more the word = fea-tures positive examples."
Table 6 denotes the results of two binary classifiers.
"One is a classifier that separates documents assigned the ‘Economics’ cat-egory(positive examples) from documents assigned a set of the other 24 top level categories, i.e. ‘hier-archy’."
"The other is a classifier that separates doc-uments with the ‘Economics’ category from doc-uments with a set of the other 101 categories, i.e., ‘non-hierarchy’."
"Table 6 shows that in ‘Non-hierarchy’, words such as ‘economic’, ‘economy’ and ‘company’ which feature the category ‘Eco-nomics’ have a high weighted value, while in ‘hi-erarchy’, words such as ‘year’ and ‘month’ which do not feature the category have a high weighted value."
Further research using various subsets of the top level categories is necessary to fully understand the influence of the hierarchical structure created by
"Finally, we compare our results with a well-known technique,   strategies."
"In the ex-periment using ensemble, we divided a training set into ten folds for each category level."
Once the indi-vidual classifiers are trained by SVMs they are used to classify test data.
Each classifier votes and the test data is assigned to the category that receives more than 6 votes [Footnote_3] .
3 6 votes was the best results among 10 different voting schemes in the experiment.
The result is illustrated in Table 7.
"In Table 7, ‘Non-hierarchy’ and ‘Hierarchy’ denotes the result of the 102 categories treated as a flat non-hierarchical problem, and the result using hierarchi-cal structure, respectively."
We can find that the re-sult of %  with hierarchy(0.704 F1) outper-forms the result with non-hierarchy(0.532 F1).
A necessary and sufficient condition for an ensemble of classifiers to be more accurate than any of its in-dividual ^   members is if the classifiers are lloÃ A  and[REF_CITE].
An ac-curate classifier is one that has an error rate bet-ter than random guessing on new test data.
Two classifiers are diverse if they make different errors on new data points.
"Given our result, it may be safely said, at least regarding the[REF_CITE]cor-pus, that hierarchical structure is more effective for constructing ensembles, i.e., an ensemble of clas-sifiers which are constructed by the training data with fewer than ^ 30   categories in each level is more  A  and ."
Table 7 shows that our method and %  perform equally (0.704 F1 score) when we use hierarchical structure.
"How-ever, the computation of the former is far more ef-ficient than the latter."
"Furthermore, we see that our method (0.596 F1 score) slightly outperforms   (0.532 F1 score) when the 102 categories are treated as a flat non-hierarchical problem."
We have reported an approach to text classifica-tion which manipulates large corpora using NB and SVMs.
Our main conclusions are:
"Æ Our method outperforms the baselines, since the micro-averaged ½b¾ score of our method was 0.704 and the baselines were 0.519 for NB and 0.285 for SVMs."
"Æ As shown in previous researches, hierarchical structure is effective for classification, since the result of our method using hierarchical struc-ture led to as much as a 10.8% reduction in er-ror rates, and up to 1.3% with NB."
Æ There is no significant difference between the F1 scores of our method and the   method with hierarchical structure.
"However, the computation of our method is more efficient than the   method in the experiment."
"Future work includes (i) extracting features which discriminate between categories within the same top-level category, (ii) investigating other machine learning techniques to obtain further advantages in efficiency in the manipulating data approach, and (iii) evaluating the manipulating data approach us-ing automatically generating hierarchies[REF_CITE]."
¦¨§;©«ªI¬#¢/%¦ ¢t¬##¥8©«§; ª°´ªµt©«ª° /#/¹~ ¼ ªI¬# ¢tÁ8§;©«ªI£ ²³¬C®À¥¤ ¦´¢t¬#¥8­®X¦%²eµ £ ¢±ª½;²«­Á8° ª­;/½ ¦ÂªI·¢±°MÁ¢±­¢t£ ! µtªr» ;½5Ä&apos;¥£ ½Ã°#¢±­P°#¢Å½;  % ¼ ª²e©«ªI¾P©³¢M¤R¥£ ¦%¢t¬#¦ (;£ ¢±°#¢±­a¬Mª_µtª°#¢q°#;½® /­Pª¦´¢±°tÉ&amp;;È ;+Á  µb¥£ ÈP§;;;­;#¢±½´¤O¥£ ¦ ¢t¬#¥8­a®U¦®Ëª°ÌÄZ¢±©e©!&gt;%  ¼ ²«°#  ;¦Ð¥8­ !§;!;¢ µb¥8­¬#£ ²³¾! ;#° ¢±½Ñ²«­ÑÄZ¥£ ½ ;°#;²«° ª¦¾P²«Á8§; #¬ ¥x¦´¢t¬##» (¸ Ò Ó &lt;068Ô&amp;=&gt;Õ&gt;X08BHÔ&lt; Ö+¢t¬#¥8­a®U¦®4²«°Âª/;¶ Á8§£ ¢+¥¤Ï°# º:;²«µ º×¥8­¢ ¢bÍUÈ;;###;ªI£ ½ :#¢±½Ü¥8­¢ÎÝÅÞvßªI·¥Ià{ª­;½Çá¥8ºP­;°#¥8­(É â±ãäIåaæ !©³¢ÂÞ âæ É Þ âæ êgëìUí :î &amp;&gt;ûIøvüvøOýþÎÿ  Ïìaøeù ìUí
This paper describes a bootstrapping al-gorithm called Basilisk that learns high-quality semantic lexicons for multiple cate-gories.
"Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category."
Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
We evaluate Basilisk on six semantic categories.
"The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several cate-gories showing substantial improvement."
"In recent years, several algorithms have been devel-oped to acquire semantic lexicons automatically or semi-automatically using corpus-based techniques."
"For our purposes, the term semantic lexicon will refer to a dictionary of words labeled with semantic classes (e.g., “bird” is an animal and “truck” is a vehicle)."
"Semantic class information has proven to be useful for many natural language processing tasks, includ-ing information extracti[REF_CITE], anaphora resolu-ti[REF_CITE], question answering[REF_CITE], and prepositional phrase attachment[REF_CITE]."
"Although some semantic dictionaries do exist (e.g., WordNet[REF_CITE]), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains."
"Even for rela-tively general texts, such as the Wall Street Journal[REF_CITE]or terrorism articles (MUC-4[REF_CITE]), Roark and Charniak[REF_CITE]reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet."
"These results suggest that auto-matic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized domains."
We have developed a weakly supervised bootstrap-ping algorithm called Basilisk that automatically generates semantic lexicons.
Basilisk hypothesizes the semantic class of a word by gathering collective evidence about semantic associations from extraction pattern contexts.
"Basilisk also learns multiple se-mantic classes simultaneously, which helps constrain the bootstrapping process."
"First, we present Basilisk’s bootstrapping algo-rithm and explain how it differs from previous work on semantic lexicon induction."
"Second, we present empirical results showing that Basilisk outperforms a previous algorithm."
"Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as an-other bootstrapping algorithm."
"Finally, we present results showing that learning multiple semantic cat-egories simultaneously improves performance."
Evidence from Extraction Patterns Basilisk (Bootstrapping Approach to SemantIc Lexicon Induction using Semantic Knowledge) is a weakly supervised bootstrapping algorithm that au-tomatically generates semantic lexicons.
Figure 1 shows the high-level view of Basilisk’s bootstrapping process.
The input to Basilisk is an unannotated text corpus and a few manually defined seed words for each semantic category.
"Before bootstrapping begins, we run an extraction pattern learner over the corpus which generates patterns to extract ev-ery noun phrase in the corpus."
The bootstrapping process begins by selecting a subset of the extraction patterns that tend to ex-tract the seed words.
We call this the pattern pool.
The nouns extracted by these patterns become can-didates for the lexicon and are placed in a candidate word pool.
Basilisk scores each candidate word by gathering all patterns that extract it and measur-ing how strongly those contexts are associated with words that belong to the semantic category.
"The five best candidate words are added to the lexicon, and the process starts over again."
"In this section, we describe Basilisk’s bootstrapping algorithm in more detail and discuss related work."
The input to Basilisk is a text corpus and a set of seed words.
We generated seed words by sorting the words in the corpus by frequency and manually identifying the 10 most frequent nouns that belong to each cat-egory.
These seed words form the initial semantic lexicon.
In this section we describe the learning pro-cess for a single semantic category.
In Section 3 we will explain how the process is adapted to handle multiple categories simultaneously.
"To identify new lexicon entries, Basilisk relies on extraction patterns to provide contextual evi-dence that a word belongs to a semantic class."
"As our representation for extraction patterns, we used the AutoSlog system[REF_CITE]."
"AutoSlog’s extraction patterns represent linguistic expressions that extract a noun phrase in one of three syntac-tic roles: subject, direct object, or prepositional phrase object."
"For example, three patterns that would extract people are: “&lt;subject&gt; was arrested”, “murdered &lt;direct object&gt;”, and “collaborated with &lt;pp object&gt;”."
Extraction patterns represent linguis-tic contexts that often reveal the meaning of a word by virtue of syntax and lexical semantics.
Extraction patterns are typically designed to capture role rela-tionships.
"For example, consider the verb “robbed” when it occurs in the active voice."
"The subject of “robbed” identifies the perpetrator, while the direct object of “robbed” identifies the victim or target."
"Before bootstrapping begins, we run AutoSlog ex-haustively over the corpus to generate an extraction pattern for every noun phrase that appears."
The patterns are then applied to the corpus and all of their extracted noun phrases are recorded.
"Figure 2 shows the bootstrapping process that follows, which we explain in the following sections."
The first step in the bootstrapping process is to score the extraction patterns based on their tendency to extract known category members.
All words that are currently defined in the semantic lexicon are con-sidered to be category members.
Basilisk scores each pattern using the RlogF metric that has been used for extraction pattern learning[REF_CITE].
The score for each pattern is computed as:
F i RlogF(pattern i ) =
N i ∗ log 2 (F i ) (1) where F i is the number of category members ex-tracted by pattern i and N i is the total number of nouns extracted by pattern i .
"Intuitively, the RlogF metric is a weighted conditional probability; a pat-tern receives a high score if a high percentage of its extractions are category members, or if a moderate percentage of its extractions are category members and it extracts a lot of them."
The top N extraction patterns are put into a pat-tern pool.
"Basilisk uses a value of N=20 for the first iteration, which allows a variety of patterns to be considered, yet is small enough that all of the pat-terns are strongly associated with the category. [Footnote_1]"
"1 “Depleted” patterns are not included in this set. A pattern is depleted if all of its extracted nouns are already defined in the lexicon, in which case it has no unclassified words to contribute."
The purpose of the pattern pool is to narrow down the field of candidates for the lexicon.
Basilisk col-lects all noun phrases (NPs) extracted by patterns in the pattern pool and puts the head noun of each NP into the candidate word pool.
Only these nouns are considered for addition to the lexicon.
"As the bootstrapping progresses, using the same value N=20 causes the candidate pool to become stagnant."
"For example, let’s assume that Basilisk performs perfectly, adding only valid category words to the lexicon."
"After some number of iterations, all of the valid category members extracted by the top 20 patterns will have been added to the lexicon, leav-ing only non-category words left to consider."
"For this reason, the pattern pool needs to be infused with new patterns so that more nouns (extractions) become available for consideration."
"To achieve this effect, we increment the value of N by one after each boot-strapping iteration."
This ensures that there is always at least one new pattern contributing words to the candidate word pool on each successive iteration.
The next step is to score the candidate words.
"For each word, Basilisk collects every pattern that ex-tracted the word."
"All extraction patterns are used during this step, not just the patterns in the pat-tern pool."
"Initially, we used a scoring function that computes the average number of category members extracted by the patterns."
The formula is:
X P i F j j=1 score(word i ) =
"P i (2) where P i is the number of patterns that extract word i , and F j is the number of distinct category members extracted by pattern j. A word receives a high score if it is extracted by patterns that also have a tendency to extract known category members."
"As an example, suppose the word “Peru” is in the candidate word pool as a possible location."
Basilisk finds all patterns that extract “Peru” and computes the average number of known locations extracted by those patterns.
Let’s assume that the three patterns shown below extract “Peru” and that the underlined words are known locations. “Peru” would receive a score of (2+3+2)/3 = 2.3.
"Intuitively, this means that patterns that extract “Peru” also extract, on average, 2.3 known location words. “was killed in &lt;np&gt;”"
"Extractions: Peru, clashes, a shootout, El Salvador,"
Colombia “&lt;np&gt; was divided”
"Extractions: the country, the Medellin cartel, Colombia, Peru, the army, Nicaragua “ambassador to &lt;np&gt;”"
"Extractions: Nicaragua, Peru, the UN, Panama"
"Unfortunately, this scoring function has a problem."
The average can be heavily skewed by one pattern that extracts a large number of category members.
"For example, suppose word w is extracted by 10 pat-terns, 9 which do not extract any category members but the tenth extracts 50 category members."
The average number of category members extracted by these patterns will be 5.
"This is misleading because the only evidence linking word w with the semantic category is a single, high-frequency extraction pat-tern (which may extract words that belong to other categories as well)."
"To alleviate this problem, we modified the scor-ing function to compute the average logarithm of the number of category members extracted by each pat-tern."
The logarithm reduces the influence of any sin-gle pattern.
"We will refer to this scoring metric as the AvgLog function, which is defined below."
"Since log 2 (1) = 0, we add one to each frequency count so that patterns which extract a single category mem-ber contribute a positive value."
X P i log 2 (F j + 1) j=1 AvgLog(word i ) = (3) P i
"Using this scoring metric, all words in the candi-date word pool are scored and the top five words are added to the semantic lexicon."
"The pattern pool and the candidate word pool are then emptied, and the bootstrapping process starts over again."
Several weakly supervised learning algorithms have previously been developed to generate seman-tic lexicons from text corpora.
"Riloff and Shepherd[REF_CITE]developed a bootstrap-ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak[REF_CITE]refined this algorithm to focus more explicitly on certain syntactic structures."
"Hale, Ge, and Charniak[REF_CITE]devised a technique to learn the gender of words."
Caraballo[REF_CITE]and Hearst[REF_CITE]created techniques to learn hypernym/hyponym relationships.
None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations.
"Several learning algorithms have also been de-veloped for named entity recognition (e.g.,[REF_CITE]).[REF_CITE]used contextual informa-tion of a different sort than we do."
"Furthermore, our research aims to learn general nouns (e.g., “artist”) rather than proper nouns, so many of the features commonly used to great advantage for named entity recognition (e.g., capitalization and title words) are not applicable to our task."
"The algorithm most closely related to Basilisk is meta-bootstrapping[REF_CITE], which also uses extraction pattern contexts for semantic lexicon induction."
Meta-bootstrapping identifies a single extraction pattern that is highly correlated with a semantic category and then assumes that all of its extracted noun phrases belong to the same cat-egory.
"However, this assumption is often violated, which allows incorrect terms to enter the lexicon."
Riloff and Jones acknowledged this issue and used a second level of bootstrapping (the “Meta” boot-strapping level) to alleviate this problem.
"While meta-bootstrapping trusts individual extraction pat-terns to make unilateral decisions, Basilisk gath-ers collective evidence from a large set of extrac-tion patterns."
"As we will demonstrate in Sec-tion 2.2, Basilisk’s approach produces better re-sults than meta-bootstrapping and is also consid-erably more efficient because it uses only a single bootstrapping loop (meta-bootstrapping uses nested bootstrapping)."
"However, meta-bootstrapping pro-duces category-specific extraction patterns in addi-tion to a semantic lexicon, while Basilisk focuses ex-clusively on semantic lexicon induction."
"To evaluate Basilisk’s performance, we ran experi-ments with the MUC-4 corpus (MUC-4[REF_CITE]), which contains 1700 texts associated with ter-rorism."
"We used Basilisk to learn semantic lexicons for six semantic categories: building, event, hu-man, location, time, and weapon."
"Before we ran these experiments, one of the authors manually la-beled every head noun in the corpus that was found by an extraction pattern."
These manual annota-tions were the gold standard.
Table 1 shows the breakdown of semantic categories for the head nouns.
These numbers represent a baseline: an algorithm that randomly selects words would be expected to get accuracies consistent with these numbers.
"Three semantic lexicon learners have previously been evaluated on the MUC-4 corpus[REF_CITE], and of these meta-bootstrapping achieved the best results."
So we implemented the meta-bootstrapping algorithm ourselves to directly compare its performance with that of Basilisk.
A difference between the original implementation and ours is that our version learns individual nouns (as does Basilisk) instead of noun phrases.
"We believe that learning individual nouns is a more conservative approach because noun phrases often overlap (e.g., “high-power bombs” and “incendiary bombs” would count as two different lexicon entries in the origi-nal meta-bootstrapping algorithm)."
"Consequently, our meta-bootstrapping results differ from those re-ported[REF_CITE]."
Figure 3 shows the results for Basilisk (ba-1) and meta-bootstrapping (mb-1).
"We ran both algorithms for 200 iterations, so that 1000 words were added to the lexicon (5 words per iteration)."
"The X axis shows the number of words learned, and the Y axis shows how many were correct."
The Y axes have different ranges because some categories are more prolific than others.
"Basilisk outperforms meta-bootstrapping for every category, often substantially."
"For the human and location categories, Basilisk learned hundreds of words, with accuracies in the 80-89% range through much of the bootstrapping."
"It is worth noting that Basilisk’s performance held up well on the human and location categories even at the end, achieving 79.5% (795/1000) accuracy for humans and 53.2% (532/1000) accuracy for locations."
We also explored the idea of bootstrapping multiple semantic classes simultaneously.
Our hypothesis was that errors of confusion [Footnote_2] between semantic categories can be lessened by using information about multi-ple categories.
2 We use the term confusion to refer to errors where a word is labeled as category X when it really belongs to category Y .
This hypothesis makes sense only if a word cannot belong to more than one semantic class.
"In general, this is not true because words are often polysemous."
"But within a limited domain, a word usually has a dominant word sense."
Therefore we make a “one sense per domain” assumption (similar to the “one sense per discourse” observati[REF_CITE]) that a word belongs to a single semantic category within a limited domain.
"All of our ex-periments involve the MUC-4 terrorism domain and corpus, for which this assumption seems appropriate."
Figure 4 shows one way of viewing the task of se-mantic lexicon induction.
The set of all words in the corpus is visualized as a search space.
"Each cate-gory owns a certain territory within the space (de-marcated with a dashed line), representing the words that are true members of that category."
"Not all ter-ritories are the same size, since some categories have more members than others."
Figure 4 illustrates what happens when a semantic lexicon is generated for a single category.
"The seed words for the category (in this case, category C) are represented by the solid black area in category C’s territory."
The hypothesized words in the growing lexicon are represented by a shaded area.
The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category’s true territory.
"If the shaded area expands beyond the category’s true territory, then incorrect words have been added to the lexicon."
"In Figure 4, category C has claimed a significant number of words that belong to categories B and E. When generating a lexicon for one category at a time, these confusion errors are impossible to detect because the learner has no knowledge of the other categories."
Figure 5 shows the same search space when lexi-cons are generated for six categories simultaneously.
"If the lexicons cannot overlap, then we constrain the ability of a category to overstep its bounds."
Cate-gory C is stopped when it begins to encroach upon the territories of categories B and E because words in those areas have already been claimed.
The easiest way to take advantage of multiple cate-gories is to add simple conflict resolution that en-forces the “one sense per domain” constraint.
"If more than one category tries to claim a word, then we use conflict resolution to decide which category should win."
"We incorporated a simple conflict reso-lution procedure into Basilisk, as well as the meta-bootstrapping algorithm."
"For both algorithms, the conflict resolution procedure works as follows. (1) If a word is hypothesized for category A but has already been assigned to category B during a previous iter-ation, then the category A hypothesis is discarded. (2) If a word is hypothesized for both category A and category B during the same iteration, then it is assigned to the category for which it receives the highest score."
"In Section 3.4, we will present empiri-cal results showing how this simple conflict resolution scheme affects performance."
"Simple conflict resolution helps the algorithm recognize when it has encroached on another cate-gory’s territory, but it does not actively steer the bootstrapping in a more promising direction."
A more intelligent way to handle multiple categories is to incorporate knowledge about other categories directly into the scoring function.
We modified Basilisk’s scoring function to prefer words that have strong evidence for one category but little or no evidence for competing categories.
"Each word w i in the candidate word pool receives a score for category c a based on the following formula: diff(w i ,c a ) ="
"AvgLog(w i ,c a ) - max (AvgLog(w i ,c b )) b6=a where AvgLog is the candidate scoring function used previously by Basilisk (see Equation 3) and the max function returns the maximum AvgLog value over all competing categories."
"For example, the score for each candidate location word will be its AvgLog score for the location category minus its maxi-mum AvgLog score for all other categories."
A word is ranked highly only if it has a high score for the targeted category and there is little evidence that it belongs to a different category.
This has the effect of steering the bootstrapping process away from am-biguous parts of the search space.
"We will use the abbreviation 1CAT to indicate that only one semantic category was bootstrapped, and MCAT to indicate that multiple semantic categories were simultaneously bootstrapped."
Figure 6 com-pares the performance of Basilisk-MCAT with con-flict resolution (ba-M) against Basilisk-1CAT (ba-1).
"Most categories show small performance gains, with the building, location, and weapon categories benefitting the most."
"However, the improvement usually doesn’t kick in until many bootstrapping it-erations have passed."
This phenomenon is consistent with the visualization of the search space in Figure 5.
"Since the seed words for each category are not gener-ally located near each other in the search space, the bootstrapping process is unaffected by conflict reso-lution until the categories begin to encroach on each other’s territories."
Figure 7 compares the performance of Meta-Bootstrapping-MCAT with conflict resolution (mb-M) against Meta-Bootstrapping-1CAT (mb- 1).
Learning multiple categories improves the performance of meta-bootstrapping dramatically for most categories.
We were surprised that the improvement for meta-bootstrapping was much more pronounced than for Basilisk.
"It seems that Basilisk was already doing a better job with errors of confusion, so meta-bootstrapping had more room for improvement."
"Finally, we evaluated Basilisk using the diff scoring function to handle multiple categories."
"Figure 8 com-pares all three MCAT algorithms, with the smarter diff version of Basilisk labeled as ba-M+."
"Over-all, this version of Basilisk performs best, showing a small improvement over the version with simple conflict resolution."
Both multiple category versions of Basilisk also consistently outperform the multiple category version of meta-bootstrapping.
Table 2 summarizes the improvement of the best version of Basilisk over the original meta-bootstrapping algorithm.
The left-hand column rep-resents the number of words learned and each cell in-dicates how many of those words were correct.
These results show that Basilisk produces substantially bet-ter accuracy and coverage than meta-bootstrapping.
Figure 9 shows examples of words learned by Basilisk.
Inspection of the lexicons reveals many un-usual words that could be easily overlooked by some-one building a dictionary by hand.
"For example, the words “deserter” and “narcoterrorists” appear in a variety of terrorism articles but they are not com-monly used words in general."
"We also measured the recall of Basilisk’s lexicons after 1000 words had been learned, based on the gold standard data shown in Table 1."
"The recall results range from 40-60%, which indicates that a good per-centage of the category words are being found, al-though there are clearly more category words lurking in the corpus."
"Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction pat-terns can be used to infer semantic category associ-ations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process."
"The accuracy achieved by Basilisk is sub-stantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance."
"We also demon- strated that learning multiple semantic categories si-multaneously improves the meta-bootstrapping algo-rithm, which suggests that this is a general observa-tion which may improve other bootstrapping algo-rithms as well."
This research was supported by the National Science
Foundation under award[REF_CITE].
Ensemble methods are state of the art for many NLP tasks.
Recent work[REF_CITE]suggests that this would not necessarily be true if very large training corpora were available.
"However, their results are limited by the simplic-ity of their evaluation task and individual classifiers."
Our work explores ensemble efficacy for the more complex task of automatic the-saurus extraction on up to 300 million words.
"We examine our conflicting results in terms of the constraints on, and com-plexity of, different contextual representa-tions, which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
Ensemble learning is a machine learning technique that combines the output of several different classi-fiers with the goal of improving classification per-formance.
"The classifiers within the ensemble may differ in several ways, such as the learning algorithm or knowledge representation used, or data they were trained on."
"Ensemble learning has been successfully applied to numerous NLP tasks, including POS tag-ging ([REF_CITE]; van[REF_CITE]), chunking (Tjong[REF_CITE]), word sense dis-ambiguati[REF_CITE]and statistical pars-ing[REF_CITE]."
Ensemble methods ameliorate learner bias by amortising individual classifier bias of over differ-ent systems.
"For an ensemble to be more effec-tive than its constituents, the individual classifiers must have better than 50% accuracy and must pro-duce diverse erroneous classifications[REF_CITE]."
"Although ensem-bles are often effective on problems with small train-ing sets, recent work suggests this may not be true as dataset size increases."
One limitation of their results is the simplicity of the task and methods used to examine the efficacy of ensemble methods.
"However, both the task and applied methods are constrained by the ambitious use of one billion words of training material."
Dis-ambiguation is relatively simple because confusion sets are rarely larger than four elements.
"The indi-vidual methods must be inexpensive because of the computational burden of the massive training set, so they must perform limited processing of the training corpus and can only consider a fairly narrow context surrounding each instance."
"We explore the value of ensemble methods for the more complex task of automatic thesaurus extrac-tion, training on corpora of up to 300 million words."
"The increased complexity leads to results contradict-ing[REF_CITE], which we explore using ensembles of different contextual complexity."
"This work emphasises the link between contextual com-plexity and the problems of representation sparse-ness and noise as corpus size increases, which in turn impacts on learner bias and ensemble efficacy."
"The development of large thesauri and semantic re-sources, such as WordNet[REF_CITE], has al-lowed lexical semantic information to be leveraged to solve NLP tasks, including collocation discov-ery[REF_CITE], model estimati[REF_CITE]and text classificati[REF_CITE]."
"Unfortunately, thesauri are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage."
"In addition, thesaurus compilers cannot keep up with constantly evolving language use and cannot afford to build new thesauri for the many sub-domains that NLP techniques are being applied to."
There is a clear need for automatic thesaurus extrac-tion methods.
Much of the existing work on thesaurus extrac-tion and word clustering is based on the observa-tions that related terms will appear in similar con-texts.
These systems differ primarily in their defi-nition of “context” and the way they calculate simi-larity from the contexts each term appears in.
"Many systems extract co-occurrence and syntactic infor-mation from the words surrounding the target term, which is then converted into a vector-space repre-sentation of the contexts that each target term ap-pears[REF_CITE]."
The context models used in our experiments are described in Section 3.
"We define a context relation instance as a tuple (w r w 0 ) where w is a thesaurus term, occurring in a, , relation of type r, with another word w 0 in the sen-tence."
"We refer to the tuple (r w 0 ) as an attribute, of w. The relation type may be grammatical or it may label the position of w 0 in a context window: e.g. the tuple (dog, direct-obj, walk) indicates that the term dog , was the direct object of the verb walk ."
"After the contexts have been extracted from the raw text, they are compiled into attribute vec-tors describing all of the contexts each term appears in."
The thesaurus extractor then uses clustering or nearest-neighbour matching to select similar terms based on a vector similarity measure.
"Our experiments use k-nearest-neighbour match- ing for thesaurus extraction, which calculates the pairwise similarity of the target term with every po-tential synonym."
"Given n terms and up to m at-tributes for each term, the asymptotic time complex-ity of k-nearest-neighbour algorithm is O(n 2 m)."
We reduce the number of terms by introducing a mini-mum occurrence filter that eliminates potential syn-onyms with a frequency less than five.
The individual methods in these ensemble experi-ments are based on different extractors of contex-tual information.
All the systems use the J ACCARD similarity metric and TT EST weighting function that were found to be most effective for thesaurus extrac-tion[REF_CITE].
The simplest and fastest contexts to extract are the word(s) surrounding each thesaurus term up to some fixed distance.
"These window methods are la-belled W(L 1 R 1 ), where L 1 R 1 indicates that window extends one word on either side of the target term."
"Methods marked with an asterisk, e.g. W(L 1 R 1 ∗), do not record the word’s position in the relation type."
The more complex methods extract grammatical relations using shallow statistical tools or a broad coverage parser.
"We use the grammatical relations extracted from the parse trees of Lin’s broad cov-erage principle-based parser, M INIPAR[REF_CITE]and Abney’s cascaded finite-state parser, C ASS[REF_CITE]."
"Finally, we have implemented our own relation extractor, based on Grefenstette’s S EXTANT[REF_CITE], which we describe below as an example of the NLP system used to extract relations from the raw text."
Processing begins with POS tagging and NP / VP chunking using a Naı̈ve Bayes classifier trained on the Penn Treebank.
"Noun phrases separated by prepositions and conjunctions are then concate-nated, and the relation attaching algorithm is run on the sentence."
"This involves four passes over the sen- tence, associating each noun with the modifiers and verbs from the syntactic contexts they appear in: 1. nouns with pre-modifiers (left to right) 2. nouns with post-modifiers (right to left) 3. verbs with subjects/objects (right to left) 4. verbs with subjects/objects (left to right)"
This results in relations representing the contexts: 1. term is the subject of a verb 2. term is the (direct/indirect) object of a verb 3. term is modified by a noun or adjective 4. term is modified by a prepositional phrase
The relation tuple is then converted to root form using the Sussex morphological analyser[REF_CITE]and the POS tags are stripped.
The re-lations for each term are collected together produc-ing a context vector of attributes and their frequen-cies in the corpus.
Figure 1 shows the most strongly weighted attributes and their frequencies for idea .
Our experiments use a large quantity of text which we have grouped into a range of corpus sizes.
We then create corpus subsets down to 1281 th (2.3 million words) of the original corpus by randomly sentence selection.
Ensemble voting methods for this task are quite interesting because the result consists of an ordered set of extracted synonyms rather than a single class label.
To test for subtle ranking effects we imple-mented three different methods of combination:
M EAN mean rank of each term over the ensemble;
H ARMONIC the harmonic mean rank of each term;
M IXTURE ranking based on the mean score for each term.
The individual extractor scores are not normalised because each extractor uses the same similarity measure and weight function.
We assigned a rank of 201 and similarity score of zero to terms that did not appear in the 200 syn-onyms returned by the individual extractors.
"Finally, we build ensembles from all the available extractor methods (e.g. M EAN (∗)) and the top three perform-ing extractors (e.g. M EAN (3))."
To measure the complementary disagreement be-tween ensemble constituents we calculated both the complementarity C and the Spearman rank-order correlation R s .
"C(A B) = (1 − |errors(A) ∩ errors(B)|) ∗ 100% (1) | errors(A)| ,"
R s (A B) =
"P i (r(A i ) − r(A))(r(B i ) − r(B)) (2), qP i (r(A i ) − r(A)) 2 qP i (r(B i ) − r(B)) 2 where r(x) is the rank of synonym x."
The Spearman rank-order correlation coefficient is the linear corre-lation coefficient between the rankings of elements of A and B. R s is a useful non-parametric compari-son for when the rank order is more relevant than the actual values in the distribution.
The evaluation is performed on thesaurus entries ex-tracted for 70 single word noun terms.
"To avoid sample bias, the words were randomly selected from WordNet such that they covered a range of values for the following word properties: frequency Penn Treebank and BNC frequencies; number of senses WordNet and Macquarie senses; specificity depth in the WordNet hierarchy; concreteness distribution across WordNet subtrees."
Table 2 shows some of the selected terms with fre-quency and synonym set information.
For each term we extracted a thesaurus entry with 200 potential synonyms and their similarity scores.
The simplest evaluation measure is direct com-parison of the extracted thesaurus with a manually-created gold standard[REF_CITE].
"How-ever, on smaller corpora direct matching is often too coarse-grained and thesaurus coverage is a problem."
"To help overcome limited coverage, our evaluation uses a combination of three electronic thesauri: the topic-ordered Macquarie[REF_CITE]and Ro-get’s[REF_CITE]thesauri and the head ordered Mo[REF_CITE]thesaurus."
Since the extracted thesaurus does not separate senses we transform Ro-get’s and Macquarie into head ordered format by collapsing the sense sets containing the term.
"With this gold standard resource in place, it is pos-sible to use precision and recall measures to evaluate the quality of the extracted thesaurus."
"To help over-come the problems of coarse-grained direct com-parisons we use several measures of system per-formance: direct matches (D IRECT ), inverse rank (I NV R), and top n synonyms precision (P(n))."
"I NV R is the sum of the inverse rank of each matching synonym, e.g. gold standard matches at ranks 3, 5 and 28 give an inverse rank score of 3 + 15 + 281 ≈ 0 569."
"With at most 200 synonyms, 1 . the maximum I NV R score is 5 878."
Top n precision. is the percentage of matching synonyms in the top n extracted synonyms.
"We use n = 1 5 and 10.,"
Figure 2 shows the performance trends for the indi-vidual extractors on corpora ranging from 2.3 mil-lion up to 300 million words.
"The best individ-ual context extractors are S EXTANT , M INIPAR and W(L 1 R 1 ), with S EXTANT outperforming M INIPAR beyond approximately 200 million words."
These three extractors are combined to form the top-three ensemble.
C ASS and the other window methods per-form significantly worse than S EXTANT and M INI - PAR .
"Interestingly, W(L 1 R 1 ∗) performs almost as well as W(L 1 R 1 ) on larger corpora, suggesting that position information is not as useful with large cor-pora, perhaps because the left and right set of words for each term becomes relatively disjoint."
Table 3 presents the evaluation results for all the individual extractors and the six ensembles on the full corpus.
These results disagree with those[REF_CITE]obtained for confusion set disambiguation.
"The best performing ensembles, M IXTURE (∗) and M EAN (∗), combine the results from all of the individual ex-tractors."
"M IXTURE (∗) performs approximately 5% better than S EXTANT , the best individual extractor."
Figure 3 compares the performance behaviour over the range of corpus sizes for the best three individ- ual methods and the full ensembles.
S EXTANT is the only competitive individual method as the corpus size increases.
Figure 3 shows that ensemble meth-ods are of more value (at least in percentage terms) for smaller training sets.
"The trend in the graph sug-gests that the individual extractors will not outper-form the ensemble methods, unless the behaviour changes as corpus size is increased further."
"From Table 3 we can also see that full ensembles, combining all the individual extractors, outperform ensembles combining only the top three extractors."
"This seems rather surprising at first, given that the other individual extractors seem to perform signifi-cantly worse than the top three."
It is interesting to see how the weaker methods still contribute to the ensembles performance.
"Firstly, for thesaurus extraction, there is no clear concept of accuracy greater than 50% since it is not a simple classification task."
"So, although most of the evaluation results are significantly less than 50%, this does not represent a failure of a necessary condi-tion of ensemble improvement."
"If we constrain the-saurus extraction to selecting a single synonym clas-sification using the P(1) scores, then all of the meth-ods achieve 50% or greater accuracy."
Considering the complementarity and rank-order correlation co-efficients for the constituents of the different ensem-bles proves to be more informative.
Table 4 shows these values for the smallest and largest corpora and Table 5 shows the pairwise complementarity for the ensemble constituents.
It turns out that the average Spearman rank-order correlation is not sensitive enough to errors for the purposes of comparing favourable disagreement within ensembles.
"However, the average comple-mentarity clearly shows the convergence of the en-semble constituents, which partially explains the re-duced efficacy of ensemble methods for large cor-pora."
"Since the top-three ensembles suffer this to a greater degree, they perform significantly worse at 300 million words."
"Further, the full ensembles can amortise the individual biases better since they aver-age over a larger number of ensemble methods with different biases."
Understanding ensemble behaviour on very large corpora is important because ensemble classifiers are state of the art for many NLP tasks.
This section explores possible explanations for why our results disagree[REF_CITE].
Thesaurus extraction and confusion set disam- biguation are quite different tasks.
"In thesaurus ex-traction, contextual information is collected from the entire corpus into a single description of the environ-ments that each term appears in and classification, as such, involves comparing these collections of data."
"In confusion set disambiguation on the other hand, each instance must be classified individually with only a limited amount of context."
The disambiguator has far less information available to determine each classification.
"This has implications for representa-tion sparseness and noise that a larger corpus helps to overcome, which in turn, affects the performance of ensemble methods against individual classifiers."
The complexity of the contextual representation and the strength of the correlation between target term and the context also plays a significant role.
"Further, structural and grammatical relation methods can encode extra syntactic and se-mantic information in the relation type."
"Although the contextual representation is less susceptible to noise, it is often sparse because fewer context rela-tions are extracted from each sentence."
The less complex window methods exhibit the op-posite behaviour.
"Depending on the window param-eters, the context relations can be poorly correlated with the target term, and so we find a very large number of irrelevant relations with low and unstable frequency counts, that is, a noisy contextual repre-sentation."
"Since confusion set disambiguation uses limited contexts from single occurrences, it is likely to suffer the same problems as the window thesaurus extractors."
"To evaluate an ensemble’s ability to reduce the data sparseness and noise problems suffered by dif-ferent context models, we constructed ensembles based on context extractors with different levels of complexity and constraints."
"Table 6 shows the performance on the full cor-pus for the three syntactic extractors, the top three performing extractors and their corresponding mean rank ensembles."
"For these more complex and con-strained context extractors, the ensembles continue to outperform individual learners, since the context representation are still reasonably sparse."
The aver- age complementarity is greater than 50%.
Table 7 shows the performance on the full cor-pus for a wide range of window-based extractors and corresponding mean rank ensembles.
Most of the individual learners perform poorly because the extracted contexts are only weakly correlated with the target terms.
"Although the ensemble performs better than most individuals, they fail to outperform the best individual on direct match evaluation."
"Since the average complementarity for these ensembles is similar to the methods above, we must conclude that it is a result of the individual methods themselves."
"In this case, the most correlated context extractor, e.g. W(L 1 R 1 ), extracts a relatively noise free representa-tion which performs better than amortising the bias of the other noisy ensemble constituents."
"Finally, confusion set disambiguation yields a sin-gle classification from a small set of classes, whereas thesaurus extraction yields an ordered set contain-ing every potential synonym."
The more flexible set of ranked results allow ensemble methods to exhibit more subtle variations in rank than simply selecting a single class.
"We can contrast the two tasks using the single syn- onym, P(1), and rank sensitive, I NV R, evaluation measures."
"The results for P(1) do not appear to form any trend, although the results show that ensemble methods do not always improve single class selec-tion."
"However, if we consider the I NV R measure, all of the ensemble methods outperform their con-stituent methods, and we see a significant improve-ment of approximately 10% with the M EAN (3) en-semble."
This paper demonstrates the effectiveness of ensem-ble methods for thesaurus extraction and investigates the performance of ensemble extractors on corpora ranging up to 300 million words in size.
"Contrary to work reported[REF_CITE], the en-semble methods continue to outperform the best in-dividual systems for very large corpora."
The trend in Figure 3 suggests that this may continue for corpora even larger than we have experimented with.
"Further, this paper examines the differences be-tween thesaurus extraction and confusion set dis-ambiguation, and links ensemble efficacy to the na-ture of each task and the problems of representation sparseness and noise."
This is done by evaluating en-sembles with varying levels of contextual complex-ity and constraints.
"The poorly constrained window methods, where contextual correlation is often low, outperformed the ensembles, which parallels results[REF_CITE]."
This suggests that large train-ing sets ameliorate the predominantly noise-induced bias of the best individual learner better than amor-tising the bias over many similar ensemble con-stituents.
"Noise is reduced as occurrence counts sta-bilise with larger corpora, improving individual clas-sifier performance, which in turn causes ensemble constituents to converge, reducing complementarity."
This reduces the efficacy of classifier combination and contributes to individual classifiers outperform-ing the ensemble methods.
"For more complex, constrained methods the same principles apply."
"Since the correlation between context and target is much stronger, there is less noise in the representation."
"However, the added constraints reduce the number of contextual rela-tions extracted from each sentence, leading to data sparseness."
These factors combine so that ensemble methods continued to outperform the best individual methods.
"Finally, corpus size must be considered with re-spect to the parameters of the contextual representa-tion extracted from the corpus."
The value of larger corpora is partly dependent on how much informa-tion is extracted from each sentence of training ma-terial.
"We fully expect individual thesaurus extrac-tors to eventually outperform ensemble methods as sparseness and complementarity are reduced, but this is not true for 100 or 300 million words since the best performing representations extract very few contexts per sentence."
"We would like to further investigate the relation-ship between contextual complexity, data sparse-ness, noise and learner bias on very large corpora."
This includes extending these experiments to an even larger corpus with the hope of establishing the cross over point for thesaurus extraction.
"Finally, al-though wider machine learning research uses large ensembles, many NLP ensembles use only a handful of classifiers."
It would be very interesting to exper-iment with a large number of classifiers using bag-ging and boosting techniques on very large corpora.
This paper shows that the web can be em-ployed to obtain frequencies for bigrams that are unseen in a given corpus.
"We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine."
"We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus."
"We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments."
"In two recent papers, Banko and Brill (2001a; 2001b) criticize the fact that current NLP algo-rithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of mag-nitude larger are available, at least for some tasks."
"Banko and Brill go on to demonstrate that learning algorithms typically used for NLP tasks benefit sig-nificantly from larger training sets, and their perfor-mance shows no sign of reaching an asymptote as the size of the training set increases."
"Arguably, the largest data set that is available for NLP is the web, which currently consists of at least 968 million pages. [Footnote_1] Data retrieved from the web therefore provides enormous potential for training NLP algorithms, if Banko and Brill’s findings generalize."
1 This is the number of pages indexed by Google[REF_CITE]as estimated by Search Engine Showdown ([URL_CITE]
There is a small body of existing research that tries to harness the potential of the web for NLP.
"A particularly interesting application is pro-posed[REF_CITE], who uses the web for example-based machine translation."
"His task is to translate compounds from French into English, with corpus evidence serving as a filter for candi-date translations."
As an example consider the French compound groupe de travail .
"There are five transla-tion of groupe and three translations for travail (in the dictionary th[REF_CITE]is using), re-sulting in 15 possible candidate translations."
"Only one of them, viz., work group has a high corpus frequency, which makes it likely that this is the correct translation into English."
"However,[REF_CITE]demonstrates, this problem can be overcome by obtaining counts through web searches, instead of relying on the BNC."
"While this is an important initial result, it raises the question of the generality of the proposed ap-proach to overcoming data sparseness."
It remains to be shown that web counts are generally useful for approximating data that is sparse or unseen in a given corpus.
"It seems possible, for instance, that Grefenstette’s (1998) results are limited to his par-ticular task (filtering potential translations) or to his particular linguistic phenomenon (noun-noun com-pounds)."
"Another potential problem is the fact that web counts are far more noisy than counts obtained from a well-edited, carefully balanced corpus such as the BNC."
The effect of this noise on the useful-ness of the web counts is largely unexplored.
The aim of the present paper is to generalize Grefenstette’s (1998) findings by testing the hypoth-esis that the web can be employed to obtain frequen-cies for bigrams that are unseen in a given corpus.
"Instead of having a particular task in mind (which would introduce a sampling bias), we rely on sets of bigrams that are randomly selected from the corpus."
"We use a web-based approach not only for noun-noun bigrams, but also for adjective-noun and verb-object bigrams, so as to explore whether this ap-proach generalizes to different predicate-argument combinations."
"We evaluate our web counts in two different ways: (a) comparison with actual corpus frequencies, and (b) task-based evaluation (predict-ing human plausibility judgments)."
"Two types of adjective-noun bigrams were used in the present study: seen bigrams, i.e., bigrams that occur in a given corpus, and unseen bigrams, i.e., bigrams that fail to occur in the corpus."
"For the seen adjective-noun bigrams, we used the data[REF_CITE], who compiled a set of 90 bi-grams as follows."
"Gsearch[REF_CITE], a chart parser which detects syntactic patterns in a tagged corpus by exploiting a user-specified con- text free grammar and a syntactic query, was used to extract all nouns occurring in a head-modifier re-lationship with one of the 30 adjectives."
Bigrams in-volving proper nouns or low-frequency nouns (less than 10 per million) were discarded.
"For each ad-jective, the set of bigrams was divided into three fre-quency bands based on an equal division of the range of log-transformed co-occurrence frequencies."
Then one bigram was chosen at random from each band.
"For each adjective, the Gsearch chunker was used to compile a list of all nouns that failed to co-occur in a head-modifier relationship with the adjec-tive."
Proper nouns and low-frequency nouns were discarded from this list.
Then each adjective was paired with three randomly chosen nouns from its list of non-co-occurring nouns.
"For the present study, we applied the procedure used[REF_CITE]and[REF_CITE]to noun-noun bigrams and to verb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each type of predicate-argument relationship."
All nouns modifying one of the 30 nouns were extracted from the BNC using a heuristic which looks for consecutive pairs of nouns that are neither preceded nor succeeded by another noun[REF_CITE].
"Verb-object bigrams for the 30 preselected verbs were obtained from the BNC using Cass[REF_CITE], a robust chunk parser de-signed for the shallow analysis of noisy text."
The parser’s output was post-processed to remove brack-eting errors and errors in identifying chunk cate-gories that could potentially result in bigrams whose members do not stand in a verb-argument relation-ship (see[REF_CITE]for details on the filtering process).
Only nominal heads were retained from the objects returned by the parser.
"As in the adjec-tive study, noun-noun bigrams and verb-object bi-grams with proper nouns or low-frequency nouns (less than 10 per million) were discarded."
The sets of noun-noun and verb-object bigrams were divided into three frequency bands and one bigram was cho-sen at random from each band.
"The procedure described[REF_CITE]was followed for creating sets of unseen noun-noun and verb-object bigrams: for each of noun or verb, we compiled a list of all nouns with which it failed to co-occur with in a noun-noun or verb-object bi-gram in the BNC."
"Again, Lauer’s (1995) heuristic and Abney’s (1996) partial parser were used to iden-tify bigrams, and proper nouns and low-frequency nouns were excluded."
"For each noun and verb, three bigrams were randomly selected from the set of their non-co-occurring nouns."
Table 1 lists examples for the seen and unseen noun-noun and verb-object bigrams generated by this procedure.
Web counts for bigrams were obtained using a sim-ple heuristic based on queries to the search engines Altavista and Google.
All search terms took into account the inflectional morphology of nouns and verbs.
"The search terms for verb-object bigrams matched not only cases in which the object was directly ad-jacent to the verb (e.g., fulfill obligation ), but also cases where there was an intervening determiner (e.g., fulfill the/an obligation )."
"The following search terms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively: (1) &quot;A N&quot;, where A is the adjective and N is the sin-gular or plural form of the noun. (2) &quot;N1 N2&quot; where N1 is the singular form of the first noun and N2 is the singular or plural form of the second noun. (3) & quot;V Det N&quot; where V is the infinitive, singular present, plural present, past, perfect, or gerund for of the verb, Det is the determiner the , a or the empty string, and N is the singular or plural form of the noun."
"Note that all searches were for exact matches, which means that the search terms were required to be di-rectly adjacent on the matching page."
This is en-coded using quotation marks to enclose the search term.
All our search terms were in lower case.
"For Google, the resulting bigram frequencies were obtained by adding up the number of pages that matched the expanded forms of the search terms in (1), (2), and (3)."
"Altavista returns not only the number of matches, but also the number of words that match the search term."
"We used this count, as it takes multiple matches per page into account, and is thus likely to produce more accurate frequencies."
"The process of obtaining bigram frequencies from the web can be automated straightforwardly using a script that generates all the search terms for a given bigram (from (1)–(3)), issues an Altavista or Google query for each of the search terms, and then adds up the resulting number of matches for each bigram."
"We applied this process to all the bigrams in our data set, covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams, i.e., 540 bigrams in total."
"A small number of bigrams resulted in zero counts, i.e., they failed to yield any matches in the web search."
Table 2 lists the number of zero bigrams for both search engines.
"Note that Google returned fewer zeros than Altavista, which presumably indi-cates that it indexes a larger proportion of the web."
We adjusted the zero counts by setting them to one.
This was necessary as all further analyses were car-ried out on log-transformed frequencies.
Table 3 lists the descriptive statistics for the bigram counts we obtained using Altavista and Google.
"From these data, we computed the average fac-tor by which the web counts are larger than the BNC counts."
"The results are given in Table 4 and indicate that the Altavista counts are between 331 and 467 times larger than the BNC counts, while the Google counts are between 759 and 977 times larger than the BNC counts."
"As we know the size of the BNC (100 million words), we can use these figures to estimate the number of words on the web: between 33.1 and 46.7 billion words for Altavista, and between 75.9 and 97.7 billion words for Google."
These estimates are in the same order of magnitude as Grefenstette and Nioche’s (2000) estimate that 48.1 billion words of English are available on the web (based on Altavista counts[REF_CITE]).
"While the procedure for obtaining web counts de-scribed in Section 2.2 is very straightforward, it also has obvious limitations."
"Most importantly, it is based on bigrams formed by adjacent words, and fails to take syntactic variants into account (other than in-tervening determiners for verb-object bigrams)."
"In the case of Google, there is also the problem that the counts are based on the number of matching pages, not the number of matching words."
"Finally, there is the problem that web data is very noisy and unbal- anced compared to a carefully edited corpus like the BNC."
"Given these limitations, it is necessary to explore if there is a reliable relationship between web counts and BNC counts."
"Once this is assured, we can ex-plore the usefulness of web counts for overcoming data sparseness."
We carried out a correlation analy-sis to determine if there is a linear relationship be-tween the BNC counts and Altavista and Google counts.
The results of this analysis are listed in Ta-ble 5.
All correlation coefficients reported in this pa-per refer to Pearson’s r and were computed on log-transformed counts.
"A high correlation coefficient was obtained across the board, ranging from .675 to .822 for Altavista counts and from .737 to .849 for Google counts."
"This indicates that web counts approximate BNC counts for the three types of bigrams under inves-tigation, with Google counts slightly outperform-ing Altavista counts."
We conclude that our simple heuristics (see (1)–(3)) are sufficient to obtain use-ful frequencies from the web.
"It seems that the large amount of data available for web counts outweighs the associated problems (noisy, unbalanced, etc.)."
"Note that the highest coefficients were obtained for adjective-noun bigrams, which probably indi-cates that this type of predicate-argument relation-ship is least subject to syntactic variation and thus least affected by the simplifications of our search heuristics."
Previous work has demonstrated that corpus counts correlate with human plausibility judgments for adjective-noun bigrams.
This results holds for both seen bigrams[REF_CITE]and for unseen bigrams whose counts were recreated using smooth-ing techniques[REF_CITE].
"Based on these findings, we decided to evaluate our web counts on the task of predicting plausibility ratings."
"If the web counts for bigrams correlate with plausibility judg-ments, then this indicates that the counts are valid, in the sense of being useful for predicting intuitive plausibility."
"Magnitude estimation is an exper-imental technique standardly used in psychophysics to measure judgments of sensory stimuli[REF_CITE], which[REF_CITE]and"
Magnitude estimation requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion.
"Subjects are first exposed to a modulus item, which they assign an arbitrary number."
All other stimuli are rated proportional to the modulus.
"In the experiments conducted[REF_CITE]and[REF_CITE], native speakers of English were presented with adjective- noun bigrams and were asked to rate the degree of adjective-noun fit proportional to the modulus item."
The resulting judgments were normalized by dividing them by the modulus value and by log-transforming them.
"For unseen adjective-noun bigrams,[REF_CITE]found a correlation of .356 be-tween mean judgments and frequencies recreated using class-based smoothing[REF_CITE]."
"In the present study, we used the plausibil-ity judgments collected[REF_CITE]and[REF_CITE]for adjective-noun bigrams and conducted additional experiments to obtain noun-noun and verb-object judgments for the materi-als described in Section 2.1."
We used the same experimental procedure as the original study (see[REF_CITE]and[REF_CITE]for de-tails).
"Four experiments were carried out, one each for seen and unseen noun-noun bigrams, and for seen and unseen verb-object bigrams."
"Unlike the adjective-noun and the noun-noun bigrams, the verb-object bigrams were not presented to subjects in isolation, but embedded in a minimal sentence context involving a proper name as the subject (e.g., Paul fulfilled the obligation )."
The experiments were conducted over the web using the WebExp software package[REF_CITE].
"A series of previous studies has shown that data obtained using WebExp closely replicates re-sults obtained in a controlled laboratory setting; this was demonstrated for acceptability judgments[REF_CITE], co-reference judg-ments[REF_CITE], and sentence completions[REF_CITE]."
These references also provide a detailed discussion of the WebExp experimental setup.
"Table 6 lists the descriptive statistics for all six judgment experiments: the original experiments[REF_CITE]and[REF_CITE]for adjective-noun bigrams, and our new ones for noun-noun and verb-object bigrams."
"We used correlation analysis to compare web counts with plausibility judgments for seen adjective-noun, noun-noun, and verb-object bi-grams."
Table 7 (top half) lists the correlation coefficients that were obtained when correlat- ing log-transformed web and BNC counts with log-transformed plausibility judgments.
The results show that both Altavista and Google counts correlate with plausibility judgments for seen bigrams.
"Google slightly outperforms Altavista: the correlation coefficient for Google ranges from .624 to .693, while for Altavista, it ranges from .638 to .685."
"A surprising result is that the web counts con-sistently achieve a higher correlation with the judg-ments than the BNC counts, which range from .488 to .569."
We carried out a series of one-tailed t-tests to determine if the differences between the correla-tion coefficients for the web counts and the corre-lation coefficients for the BNC counts were signifi-cant.
"For the adjective-noun bigrams, the difference between the BNC coefficient and the Altavista coef-ficient failed to reach significance (t(87) = 1.46, p &gt; .05), while the Google coefficient was significantly higher than the BNC coefficient (t(87) = 1.78, p &lt; .05)."
"For the noun-noun bigrams, both the Altavista and the Google coefficients were significantly higher than the BNC coefficient (t(87) = 2.94, p &lt; .01 and t(87) = 3.06, p &lt; .01)."
"Also for the verb-object bi-grams, both the Altavista coefficient and the Google coefficient were significantly higher than the BNC coefficient (t(87) = 2.21, p &lt; .05 and t(87) = 2.25, p &lt; .05)."
"In sum, for all three types of bigrams, the correlation coefficients achieved with Google were significantly higher than the ones achieved with the BNC."
"For Altavista, the noun-noun and the verb-object coefficients were higher than the coefficients obtained from the BNC."
"Table 7 (bottom half) lists the correlations co-efficients obtained by comparing log-transformed judgments with log-transformed web counts for un-seen adjective-noun, noun-noun, and verb-object bi-grams."
"We observe that the web counts consistently show a significant correlation with the judgments, the coefficient ranging from .466 to .588 for Al- tavista counts, and from .446 to .611 for the Google counts."
Note that a small number of bigrams pro-duced zero counts even in our web queries; these fre-quencies were set to one for the correlation analysis (see Section 2.2).
"To conclude, this evaluation demonstrated that web counts reliably predict human plausibility judg-ments, both for seen and for unseen predicate-argument bigrams."
"In the case of Google counts for seen bigrams, we were also able to show that web counts are a better predictor of human judg-ments than BNC counts."
"These results show that our heuristic method yields useful frequencies; the sim-plifications we made in obtaining the counts, as well as the fact that web data are noisy, seem to be out-weighed by the fact that the web is up to three orders of magnitude larger than the BNC (see our estimate in Section 2.2)."
This paper explored a novel approach to overcoming data sparseness.
"If a bigram is unseen in a given cor-pus, conventional approaches recreate its frequency using techniques such as back-off, linear interpo-lation, class-based smoothing or distance-weighted averaging (see[REF_CITE]and[REF_CITE]for overviews)."
"The approach proposed here does not recreate the missing counts, but instead re-trieves them from a corpus that is much larger (but also much more noisy) than any existing corpus: it launches queries to a search engine in order to deter-mine how often a bigram occurs on the web."
"We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verb-object bigrams)."
We first applied the approach to seen bigrams randomly sampled from the BNC.
"We found that the counts obtained from the web are highly correlated with the counts obtained from the BNC, which indicates that web queries can generate frequencies that are compara-ble to the ones obtained from a balanced, carefully edited corpus such as the BNC."
"Secondly, we performed a tasked-based evalua-tion that used the web frequencies to predict hu-man plausibility judgments for predicate-argument bigrams."
"The results show that web counts corre-late reliably with judgments, for all three types of predicate-argument bigrams tested, both seen and unseen."
"For the seen bigrams, we showed that the web frequencies correlate better with judged plausi-bility than the BNC frequencies."
"To summarize, we have proposed a simple heuris-tic for obtaining bigram counts from the web."
"Using two different types of evaluation, we demonstrated that this simple heuristic is sufficient to obtain useful frequency estimates."
It seems that the large amount of data available outweighs the problems associated with using the web as a corpus (such as the fact that it is noisy and unbalanced).
"In future work, we plan to compare web counts for unseen bigrams with counts recreated using standard smoothing algorithms, such as similarity-based smoothing[REF_CITE]or class-based smoothing[REF_CITE]."
"If web counts correlate reliable with smoothed counts, then this provides further evidence for our claim that the web can be used to overcome data sparseness."
"The purpose of a language model (LM) is to de-termine the a priori probability of a word sequence w 1 , . . ., w n , P (w 1 , . . ., w n )."
Language modeling is es-sential in a wide variety of applications; we focus on speech recognition in our research.
"Although word-based LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have ex-plored a variety of ways to improve LM performance[REF_CITE]."
Class-based LMs attempt to deal with data sparse-ness and generalize better to unseen word sequences by first grouping words into classes and then using these classes to compute n-gram probabilities.
Part-of-Speech (POS) tags were initially used as classes[REF_CITE]in a conditional probabilistic model (which predicts the tag sequence for a word sequence first and then uses it to predict the word sequence):
X Y N Pr(w 1N ) ≈ Pr(t i |t i−11 )
"Pr(w i |t i ) (1) t 1 ,t 2 ,...,t N i=1"
"However, Jelinek’s POS LM is less effective at pre-dicting word candidates than an n-gram word-based LM because it deletes important lexical information for predicting the next word."
Heeman’s (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead redefining the speech recog-nition problem as determining:
"W ∗ , T ∗ = arg max P(W, T|A) W,T = arg max P(W, T)P(A|W, T) W,T ≈ arg max P(W, T)P(A|W)"
"W,T where T is the POS sequence t N1 associated with the word sequence W = w 1N given the speech utterance A."
"The LM P (W, T) is a joint probabilistic model that accounts for both the sequence of words w N1 and their tag assignments t 1N by estimating the joint probabilities of words and tags:"
"Y N P(w 1N , t N1 ) ="
"P(w i , t i |w 1i−1 , t i−11 ) (2) i=1"
"Recently, there has been good progress in devel-oping structured models[REF_CITE]that incorporate syntactic infor-mation."
"These LMs capture the hierarchical char-acteristics of a language rather than specific infor-mation about words and their lexical features (e.g., case, number)."
"In an attempt to incorporate even more knowledge into a structured LM,[REF_CITE]has developed a probabilistic feature gram-mar (PFG) that conditions not only on structure but also on a small set of grammatical features (e.g., number) and has achieved parse accuracy improve-ment."
Goodman’s work suggests that integrating lexical features with word identity and syntax would benefit LM predictiveness.
"PFG uses only a small set of lexical features because it integrates those features at the level of the production rules, causing a signif-icant increase in grammar size and a concomitant data sparsity problem that preclude the addition of richer features."
This sparseness problem can be ad-dressed by associating lexical features directly with words.
We hypothesize that high levels of word predic-tion capability can be achieved by tightly integrat-ing structural constraints and lexical features at the word level.
"Hence, we develop a new dependency-grammar almost-parsing LM, SuperARV LM, which uses enriched tags called SuperARVs."
"In Section 2, we introduce our SuperARV LM."
Section 3 compares the performance of the SuperARV LM to other LMs.
Section 4 investigates the knowledge source contribu-tions by constraint relaxation.
Conclusions appear in Section 5.
The SuperARV LM is a highly lexicalized probabilis-tic LM based on the Constraint Dependency Gram-mar (CDG)[REF_CITE].
CDG represents a parse as assignments of dependency re-lations to functional variables (denoted roles) asso-ciated with each word in a sentence.
Consider the parse for What did you learn depicted in the white box of Figure 1.
Each word in the parse has a lexi-cal category and a set of feature values.
"Also, each word has a governor role (denoted G) which is as-signed a role value, comprised of a label as well as a modifiee, which indicates the position of the word’s governor or head."
"For example, the role value as-signed to the governor role of did is vp-1, where its label vp indicates its grammatical function and its modifiee 1 is the position of its head what."
"The need roles (denoted N1, N2, and N3) are used to ensure the grammatical requirements (e.g., subcategoriza-tion) of a word are met, as in the case of the verb did, which needs a subject and a base form verb (but since the word takes no other complements, the mod- ifiee of the role value assigned to N3 is set equal to its own position)."
"Including need roles also provides a mechanism for using non-headword dependencies to constrain parse structures, which[REF_CITE]has shown contributes to improved parsing accuracy."
"During parsing, the grammaticality of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments[REF_CITE]."
"Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical[REF_CITE]."
"In order to derive the constraints directly from CDG annotated sentences, we have de-veloped an algorithm to extract grammar relations using information derived directly from annotated sentences ([REF_CITE];"
"Using the relationship between a role value’s position and its modifiee’s position, unary and bi-nary constraints can be represented as a finite set of abstract role values (ARVs) and abstract role value pairs (ARVPs), respectively."
The light gray box of Figure 1 shows an example of an ARV and an ARVP.
"The ARV for the governor role value of did indicates its lexical category, lexical features, role, label, and positional relation information. (PX[Footnote_1] &gt; MX1) in-dicates that did is governed by a word that precedes it."
"1 We have annotated a moderate-sized corpus, DARPA Naval Resource Management[REF_CITE], with CDG parse relations as reported[REF_CITE]."
Note that the constraints of a CDG can be ex-tracted from a corpus of parsed sentences.
"A super abstract role value (SuperARV) is an ab-straction of the joint assignment of dependencies for a word, which provides a mechanism for lexicaliz-ing CDG parse rules."
The dark gray box of Figure 1 presents an example of a SuperARV for the word did.
The SuperARV structure provides an explicit way to organize information concerning one consistent set of dependency links for a word that can be directly de-rived from its parse assignments.
SuperARVs encode lexical information as well as syntactic and semantic constraints in a uniform representation that is much more fine-grained than POS.
A SuperARV can be thought of as providing admissibility constraints on syntactic and lexical environments in which a word may be used.
"A SuperARV is formally defined as a four-tuple for a word, hC, F , (R, L, UC, MC)+, DCi, where C is the lexical category of the word, F = {Fname 1 = Fvalue 1 , ...,FName f = FV alue f } is a fea-ture vector (where Fname i is the name of a feature and Fvalue i is its corresponding value), (R, L, UC, MC)+ is a list of one or more four-tuples, each rep-resenting an abstraction of a role value assignment, where R is a role variable, L is a functionality la-bel, UC represents the relative position relation of a word and its dependent, MC is the lexical cat-egory of the modifiee for this dependency relation, and DC represents the relative ordering of the po-sitions of a word and all of its modifiees."
"The fol-lowing features are used in our SuperARV LM: agr, case, vtype (e.g., progressive), mood, gapp (e.g., gap or not), inverted, voice, behavior (e.g., mass, count), type (e.g., interrogative, relative)."
These lexical features constitute a much richer set than the features used by the parser-based LMs in Section 1.
Words typically have more than one SuperARV to indicate different types of word usage.
"The average number of SuperARVs for words of different lexical categories vary, with verbs having the greatest Su-perARV ambiguity."
This is mostly due to the vari-ety of feature combinations and variations on com-plement types and positions.
"We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size in-creases; the moderate-sized Resource Management corpus[REF_CITE]with 25,168 words pro-duces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set[REF_CITE], and 791 for the 37 million word training set of the WSJ contin-uous speech recognition task."
"SuperARVs can be accumulated from a corpus an-notated with CDG relations and stored directly with words in a lexicon, so we can learn their frequency of occurrence for the corresponding word."
A Super-ARV can then be selected from the lexicon and used to generate role values that meet their constraints.
"Since there are no large benchmark corpora anno-tated with CDG information 1 , we have developed a methodology to automatically transform constituent bracketing found in available treebanks into CDG annotations."
"In addition to generating dependency structures by headword percolati[REF_CITE], our transformer also utilizes a rule-based method to determine lexical features and need role values for words, as described[REF_CITE]."
Our SuperARV LM estimates the joint probability of words w N1 and their SuperARV tags t N1 :
Y N Pr(w N1 t N1 ) =
Pr(w i t i |w i−11 t i−11 ) i=1
Y N =
Pr(t i |w 1i−1 t i−11 ) ·
Pr(w i |w 1i−1 t i1 ) i=1
Y N ≈ Pr(t i |w i−1i−2 t i−1i−2 ) ·
Pr(w i |w i−1i−2 t ii−2 ) (3) i=1
Notice we use a joint probabilistic model to enable the joint prediction of words and their SuperARVs so that word form information is tightly integrated at the model level.
Our SuperARV LM does not encode the word identity directly at the data structure level as was done[REF_CITE]since this could cause serious data sparsity problems.
"To estimate the probability distributions in Equa-tion (3) from training data, we use recursive lin-ear interpolation among probability estimations of different orders."
"Representing each multiplicand in Equation (3) as the conditional probability P̂(x|y 1 , y 2 , . . ., y n ) where y 1 , y 2 , . . ., y n belong to a mixed set of words and SuperARVs, the recursive linear interpolation is calculated as follows:"
"P̂ n (x|y 1 , y 2 , . .. , y n ) = λ(x, y 1 , y 2 , . . . , y n ) · P n (x|y 1 , y 2 , . . . , y n ) +(1 − λ(x, y 1 , y 2 , . .. , y n )) · P̂ n−1 (x|y 1 , y 2 , . . . , y n−1 ) where: • y 1 ,y 2 ,...,y n is the context of order n-gram to predict x; • P n (x|y 1 , y 2 , . . ., y n ) is the order n-gram maximum likelihood estimation."
Table 1 enumerates the n-grams and their order for the interpolation smoothing of the two distributions in Equation (3).
The ordering was based on our hy-pothesis that n-grams with more fine-grained history information should be ranked higher in the n-gram list since that information should be more helpful for discerning word and SuperARVs based on their history.
The SuperARV LM hypothesizes categories for out-of-vocabulary words using the leave-one-out technique[REF_CITE].
Table 1: The enumeration and order of n-grams for smoothing the distributions in Equation (3). n-grams P̂(t i |w i−2i−1 t i−1i−2 )
P̂(w i |w i−2i−1 t ii−2 ) highest
P̂(t i |w i−2i−1 t i−1i−2 )
P̂(w i |w i−2i−1 t ii−2 )
P̂(t i |w i−1 t i−1i−2 )
P̂(w i |w i−2i−1 t ii−1 )
P̂(t i |w i−1i−2 t i−1 )
P̂(w i |w i−1 t ii−2 )
P̂(t i |t i−1i−2 )
P̂(w i |w i−1 t ii−1 )
P̂(t i |w i−1 t i−1 )
P̂(w i |w i−1 t i )
P̂(t i |t i−1 )
P̂(w i |t ii−1 ) lowest P̂(w i |t i )
P̂(t i )
"In preliminary experiments, we compared several algorithms for smoothing the probability estima-tions for our SuperARV LM."
"The best performance was achieved by using the modified Kneser-Ney smoothing algorithm initially introduced[REF_CITE]and adapting it by employing a heldout data set to optimize parameters, includ-ing cutoffs for rare n-grams, by using Powell’s search[REF_CITE]."
Parameters are chosen to opti-mize the perplexity on a heldout set.
"In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL):"
Equation (4) is used by class-based LMs to calculate word perplexity[REF_CITE].
Parser-based LMs use a similar procedure that sums over parses.
The SuperARV LM is most closely related to the almost-parsing-based LM developed[REF_CITE].
"Srinivas’ LM, based on the notion of a su-pertag, the elementary structure of Lexicalized Tree-Adjoining Grammar, achieved a perplexity reduction compared to a conditional POS n-gram LM[REF_CITE]."
"By comparison, our LM in-corporates dependencies directly on words instead of through nonterminals, uses more lexical features than the supertag LM, uses joint instead of con-ditional probability estimations, and uses modified Kneser-Ney rather than Katz smoothing."
"Traditionally, the LM quality in speech recognition is evaluated on two metrics: perplexity and WER, with the former commonly selected as a less computation-ally expensive alternative."
"We carried out two exper-iments, one using the Wall Street Journal Penn Tree-bank (WSJ PTB), a text corpus on which perplexity can be measured and compared to other LMs, and the Wall Street Journal Continuous Speech Recog-nition (WSJ CSR) task, a speech corpus on which both perplexity and WER can be evaluated after LM rescoring."
"These two experiments compare our Su-perARV LM to a baseline trigram, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs."
"Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSu-perARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of us-ing joint probability estimations."
"For the WSJ PTB task, we compare the Super-ARV LMs to the parser LMs developed[REF_CITE],[REF_CITE], and[REF_CITE]."
"Al-though[REF_CITE]developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB 2 and a trainable supertag LM is unavailable."
"Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison."
"The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; how-ever, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not considered in the CSR task."
"Note that for lattice rescoring, however, Roark found that Chelba’s model achieves a greater reduction on WER than his LM[REF_CITE]."
"To evaluate the perplexity of the LMs on the WSJ PTB task, we adopted the conventions[REF_CITE],[REF_CITE], and[REF_CITE]for pre-processing the data."
"The vocabulary is limited to the most common 10K words, with all words outside this vocabulary mapped to hUNKi."
All punctuation is removed and no case information is retained.
"All symbols and digits are replaced by the symbol N. Sections 0-20 (929,564 words) are used as the train-ing set for collecting counts, sections 21-22 (73,760 words) as the development set for tuning parameters, and sections 23-24 (82,430 words) for testing."
The baseline trigram uses Katz back-off model with Good-Turing discounting for smoothing.
"The POS, cSuperARV, and SuperARV LMs were imple-mented as described previously."
The results for the parser-based LMs were initially taken from the lit-erature.
The perplexity on the test set using each LM and their interpolation with the corresponding trigram (and the interpolation weight) are shown in the top six rows of Table 2.
"As can be seen in Table 2, the SuperARV LM ob-tains the lowest perplexity of all of the LMs (and so it is depicted in bold face)."
"The SuperARV LM achieves the greatest perplexity reduction of 29.19% compared to the trigram, with Charniak’s interpo-lated trihead LM a close second at 24.91%."
"The cSu-perARV LM is clearly inferior to the SuperARV LM, even after interpolation."
"This result highlights the value of tight coupling of word, lexical feature, and syntactic knowledge both at the data structure level (which is the same for the SuperARV and cSuper- ARV LMs) and at the probability model level (which is different)."
"Notice that the cSuperARV, Chelba’s, Roark’s, and Charniak’s LMs obtain an improvement in per-formance when interpolated with a trigram; whereas, the POS LM and the SuperARV LM do not benefit from trigram interpolation [Footnote_3] ."
"3 In the remaining experiments, the POS LM and the SuperARV LM are not interpolated with a trigram."
"To gain more insight into why a trigram is effectively interpolated with some, but not all, of the LMs, we calculate the cor-relation of the trigram with each LM."
A standard correlation is calculated between the probabilities as-signed to each test set sentence by the trigram LM and the LM in question.
This technique has been used[REF_CITE]to identify whether two LMs can be effectively interpolated.
"Since we have access to an executable ver-sion of Charniak’s LM trained on the WSJ PTB[URL_CITE]and a trainable ver-sion of Chelba’s LM, we are able to calculate their correlations with our trigram LM."
Chelba’s LM was retrained using more parameter reestimation itera-tions than[REF_CITE]to optimize the per-formance.
Table 2 shows the correlation between each of the executable LMs and the trigram LM.
"The POS LM has the highest correlation with the trigram, closely followed by the SuperARV LM."
"Be-cause these two LMs tightly integrate the word infor-mation jointly with the tag distribution, the trigram information is already represented."
"In contrast, the cSuperARV LM and Chelba’s and Charniak’s parser-based LMs have much lower correlations, indicating they have much lower overlap with the trigram."
"Be-cause the cSuperARV LM only uses weak word dis-tribution information in probability estimations, it leaves room for the trigram LM to compensate for the lack of word knowledge."
The correlations for the parser-based LMs suggest that they capture different aspects of the words’ distributions in the language than the words themselves.
"Next we compare the effectiveness of using the tri-gram word-based, POS, cSuperARV, SuperARV, and Chelba’s LMs in rescoring hypotheses generated by a speech recognizer."
"The training set of the WSJ CSR task is composed of the 1987-1989 files con-taining 37,243,300 words."
"The speech data for the training set is used for building the acoustic model; whereas, the parse trees for the training set are gen-erated following the policy that if the context-free grammar constituent bracketing can be found in the WSJ PTB, it becomes the parse tree for the training sentence; otherwise, we use the corresponding tree in the BLLIP treebank[REF_CITE]."
"Since WSJ CSR is a speech corpus, there is no punctua-tion or case information."
All words outside the pro-vided vocabulary are mapped to hUNKi.
"Note that the word-level tokenization of treebank texts differs from that used in the speech recognition task with the major differences being: numbers (e.g., “1.[Footnote_2]%” versus “one point two percent”), dates (e.g., “Dec. 20, 2001” versus “December twentieth, two thou-sand one”) , currencies (e.g., “$10.25” versus “ten dollars and twenty five cents”), common abbrevia-tions (e.g., “Inc.” versus “Incorporated”), acronyms (e.g., “I.B.M.” versus “I. B. M.”), hyphenated and period-delimited phrases (e.g., “red-carpet” versus “red carpet”), and contractions and possessives (e.g., “do n’t” versus “don’t”)."
"2 Using the same 180,000 word training and 20,000 word test set[REF_CITE], our SuperARV LM ob-tains a perplexity of 92.76, compared to a perplexity of 101 obtained by the supertag LM."
"The POS, parser-based, and SuperARV LMs are all trained using the text-based tokenization from the treebank."
"Hence, during testing, a transformation converts the output of the recognizer to a form compatible with the text-based tokenizati[REF_CITE]for rescoring."
"For testing the LMs, we use the four available WSJ CSR evaluation sets: 1992 5K closed vocab-ulary (denoted 92-5k) with 330 utterances and 5,353 words, 1993 5K closed vocabulary (93-5k) with 215 utterances and 3,849 words, 1992 20K open vocabu-lary (92-20k) with 333 utterances and 5,643 words, and 1993 20K (93-20k) with 213 utterances and 3,446 words."
"We also employ a development set for each vocabulary size: 93-5k-dt (513 utterances and 8,635 words) and 93-20k-dt (252 utterances and 4,062 words)."
The trigram provided by LDC for the CSR task was used due to its high quality.
"Before evaluation, all the other LMs (i.e., the POS LM, the cSuperARV and SuperARV LMs, and Chelba’s LM) are retrained on the training set trees for the CSR task."
Parameter tuning for the LMs on each task uses the correspond-ing development set [Footnote_4] .
"4 The interpolation weight for cSuperARV for lattice rescoring was 0.63 on the 5k tasks and 0.60 on the 20k tasks, and 0.68 and 0.65 for Chelba’s LM, respectively."
Perplexity Results Table 3 shows the perplexity results for each test set with the best result for each in bold face.
"The SuperARV LM yields the lowest perplexity, with Chelba’s LM a close second."
"The perplexity reductions for the SuperARV LM over the trigram across the test sets are 53.19%, 53.63%, 34.33%, and 32.05%, which is even higher than on the WSJ PTB task."
This is probably due to the fact that more training data was used for the CSR task (37 million words versus 1 million words).
"Rescoring Lattices Next using the same LMs, we rescored the lattices generated by an acoustic recog-nizer built using HTK[REF_CITE]."
"For each test set sentence, we generated a word lattice."
We tuned the parameters of the LMs using the lattices on the corresponding development sets to minimize WER.
Lattices were rescored using a Viterbi search for each LM.
"Table 4 shows the WER and sentence accuracy (SAC) after rescoring lattices using each LM, with the lowest WER and highest SAC for each test set presented in bold face."
We also give the lattice WER/SAC which defines the best accuracy possible given perfect knowledge.
"As can be seen from Table 4, the SuperARV LM produces the best reduction in WER with Chelba’s LM the second best."
"When rescoring lattices on the 92-5k, 93-5k, 92-20k, and 93-20k test sets, the SuperARV LM yields a relative WER reduction of 13.54%, 9.70%, 8.64%, and 3.12% compared to the trigram, respectively."
"SAC results are similar: the SuperARV LM achieves an absolute increase on SAC of 4.24%, 6.97%, 2.7%, and 3.75%, compared to the trigram."
"Note that Chelba’s LM tied once with the SuperARV[REF_CITE]-20k SAC, but always obtained higher WER across the four test sets."
"Because Chelba’s LM focuses on developing the complete parse structure for a word sequence, it en-forces more strict pruning based on the entire sen-tence."
"As can be seen in Table 4, the cSuperARV LM, even when interpolated with a trigram LM, ob-tains a lower accuracy than our SuperARV LM."
This result is consistent with the hypothesis that a con-ditional model suffers from label bi[REF_CITE].
The WER reported[REF_CITE]on the 93- 20k test set was 13.0%.
This WER is lower than what we obtained for Chelba’s retrained LM on the same task.
"This disparity is due to the fact that a higher quality acoustic decoder was used[REF_CITE], which is not available to us."
"We further com-pare the LMs on Dr. Chelba’s 93-20K lattices kindly provided by him, with the rescoring results shown in the last column of Table 4."
"We observe that Chelba’s retrained LM improves his original result, but the SuperARV LM still obtains a greater accu-racy."
"Sign tests show that the differences between the accuracies achieved by the SuperARV LM and the trigram, POS, and cSuperARV LMs are statis-tically significant."
"Although there is no significant difference between the SuperARV LM and Chelba’s LM, the SuperARV LM has a much lower complexity than Chelba’s LM."
"Next, we attempt to explain the contrast between the encouraging results from our SuperARV LM and the reported poor performance of several probabilis-tic dependency grammar models, i.e., the traditional probabilistic dependency grammar (PDG) LM, the probabilistic link grammar (PLG)[REF_CITE]LM, and Zeman’s probabilistic dependency grammar model (ZPDG)[REF_CITE]."
ZPDG was evaluated on the Prague Dependency Treebank[REF_CITE]during the 1998 Johns Hopkins sum-mer workshop[REF_CITE]and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%)[REF_CITE].
"Additionally, only a modest im-provement on the bigram was achieved[REF_CITE]revised the model to make grammar rule learning feasible."
"One possible reason for their poor performance, es- pecially in the light of our SuperARV LM results, is that these probabilistic dependency grammar mod-els do not utilize sufficient knowledge to achieve a high level of accuracy."
"The knowledge sources the SuperARV LM uses, represented as components of the structure shown in Figure 1, include: lexical category (denoted c), lexical features (denoted f), role label or link type information (denoted L), a governor role dependency relation constraint (R, L, UC) (denoted g), a set of need role dependency rela-tion constraints (R, L, UC)+ (denoted n), and mod-ifiee constraints represented as the lexical category of the modifiee for each role (denoted m)."
Table 5 summarizes the knowledge sources that each of the probabilistic dependency grammar models uses.
"To determine whether the poor performance of the three probabilistic dependency grammar models re-sults from our hypothesis that they utilize insufficient knowledge, we will evaluate our SuperARV LM af-ter eliminating those knowledge sources that are not used by each of these models."
"Additionally, we will evaluate the contribution of each of the knowledge sources to the predictiveness of our SuperARV LM."
"We use the methodology of selectively ignoring dif-ferent types of knowledge as constraints to evaluate the knowledge source contributions to our SuperARV LM, as well as to approximate the performance of the other probabilistic dependency grammar models."
"The framework of CDG, on which our SuperARV LM is built, allows constraints to be tightened by adding more knowledge sources or loosened by ignoring cer-tain knowledge."
"The SuperARV structure inherits this capability from CDG; selective constraint re-laxation is implemented by eliminating one or more knowledge source in K = {c, f, L, g, n, m} from the SuperARV structure."
"We have constructed nine dif-ferent LMs based on reduced SuperARV structures denoted SARV-k (i.e., a SuperARV structure after removing k with k ⊆ K), where −k represents the deletion of a subset of knowledge types (e.g., f, mn, cgmn)."
Each model is described next.
"Modifiee constraints potentially hamper grammar generality, and so we consider their impact by delet-ing them from the LM by using the SARV-m struc-ture."
"Need roles are important for capturing the structural requirements of different types of words (e.g., subcategorization), and we investigate their ef-fects by using the SARV-n structure."
The model based on SARV-L is built to investigate the im-portance of link type information.
"We can investi-gate the contribution of the combination of m and n, fundamental to the enforcement of valency con-straints, by using the SARV-mn structure."
The model based on SARV-f is used to evaluate whether lexical features improve or degrade LM quality.
"The model based on SARV-fmn is very similar to the standard probabilistic dependency grammar LM, in which only word, POS, link type, and link di-rection information is used for probability estima-tions."
"The model based on SARV-gmn uses a fea-ture augmentation of POS, and the model based on SARV-cgmn uses lexical features only."
"Addition-ally, we built the model ZPDG-SARV to approxi-mate ZPDG."
"Zeman’s PDG[REF_CITE]dif-fers significantly from our original SuperARV LM in that it ignores label information L and some lexi-cal feature information (the morphological tags do not include some lexical features having influence on syntax, denoted syntactic lexical features, i.e., gapp, inverted, mood, type, case, voice), and does not enforce valency constraints (instead, the model only counts the number of links associated with a word without discriminating whether the links repre-sent governing or linguistic structural requirements)."
"Also, word identity information is not used, instead, the model uses a loose integration of a word’s lemma and its morphological tag."
"Given this analysis, we built the model ZPDG-SARV based on a structure including lexical category, morphological features, and (G, UC, MC)."
"Table 6 shows the perplexity results on the WSJ CSR test sets ordered from highest to lowest for each test set, with the best result for each in bold face."
The full SuperARV LM yields the lowest per-plexity.
"We found that ignoring modifiee constraints (SARV-m) increases perplexity the least, and ignor-ing link type information (SARV-L) and need role constraints (SARV-n) are a little worse than that."
"Ignoring both knowledge sources (SARV-mn) should result in even greater degradation, which is verified by the results."
"However, ignoring lexical features (SARV-f) produces an even greater increase in per-plexity than relaxing both m and n. The SARV-fmn, which is closest to the traditional probabilistic dependency grammar LM, shows fairly poor qual-ity, not much better than the POS LM."
One might hypothesize that lexical features individually con-tribute the most to the overall performance of the Su-perARV LM.
"However, using this knowledge source by itself (SARV-cgmn) results in dramatic degrada-tion on perplexity, in fact even worse than that of the POS LM, but still slightly better than the baseline trigram."
"However, as demonstrated by SARV-gmn, the constraints from lexical features are strength-ened by combining them with POS."
"Given the de- scriptions in Table 5, we can approximate PLG by a model based on a SuperARV structure eliminat-ing f and m (which should have a quality between SARV-f and SARV-fmn)."
"It is noticeable that with-out word identity information, syntactic lexical fea-tures, and valency constraints, the ZPDG-SARV LM performs worse than the POS-based LM and only slightly better than the LM based on SARV-cgmn."
This suggests that ZPDG can be strengthened by incorporating more knowledge.
"The same ranking of the performance of the LMs was obtained for WER/SAC after rescoring the lat-tices using each LM, as shown in Table 7."
"Our exper-iments with relaxed SuperARV LMs suggest likely methods for improving PDG, PLG, and ZPDG mod-els."
"The tight integration of word identity, lexical category, lexical features, and structural dependency constraints is likely to improve their performance."
"Clearly the investigated knowledge sources are quite synergistic, and their tight integration achieves the greatest improvement on both perplexity and WER."
"We have compared our SuperARV LM to a variety of LMs and found that it achieves both perplexity and WER reductions compared to a trigram, and despite the fact that it is an almost-parsing LM, it outper-forms (or performs comparably to) the more com-plex parser-based LMs on both perplexity and rescor-ing accuracy."
Additional experiments reveal that se-lecting a joint instead of a conditional probabilistic model is an important factor in the performance of our SuperARV LM.
The SuperARV structure pro-vides a flexible framework that tightly couples a va-riety of knowledge sources without combinatorial ex-plosion.
"We found that although each knowledge source contributes to the performance of the LM, it is the tight integration of the word level knowledge sources (word identity, POS, and lexical features) to-gether with the structural information of governor and subcategorization dependencies that produces the best level of LM performance."
We are currently extending the almost-parsing SuperARV LM to a full parser-based LM.
"This research was supported by Intel, Purdue Re-search Foundation, and National Science Founda-tion under Grant No.[REF_CITE]-04358,[REF_CITE]-17388, and[REF_CITE]."
We would like to thank the anonymous reviewers for their comments and sug-gestions.
"We would also like to thank Dr. Charniak, Dr. Chelba, and Dr. Srinivas for their help with this research effort."
"Finally, we would like to thank Yang"
Liu (Purdue University) for providing us with the WSJ CSR test set lattices.
This paper presents several practical ways of incorporating linguistic structure into language models.
A headword detector is first applied to detect the headword of each phrase in a sentence.
A permuted headword trigram model (PHTM) is then generated from the annotated corpus.
"Finally, PHTM is extended to a cluster PHTM (C-PHTM) by defining clusters for similar words in the corpus."
We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion.
Experiments show that C-[REF_CITE]% error rate reduction over the word trigram model.
"This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model."
"In spite of its deficiencies, trigram-based language modeling still dominates the statistical language modeling community, and is widely applied to tasks such as speech recognition and Asian language text input[REF_CITE]."
"Word trigram models are deficient because they can only capture local dependency relations, taking no advantage of richer linguistic structure."
"Many proposals have been made that try to incorporate linguistic structure into language models (LMs), but little improvement has been achieved so far in realistic applications because (1) capturing longer distance word dependency leads to higher-order n-gram models, where the number of parameters is usually too large to estimate; (2) capturing deeper linguistic relations in a LM requires a large amount of annotated training corpus and a decoder that assigns linguistic structure, which are not always available."
This paper presents several practical ways of incorporating long distance word dependency and linguistic structure into LMs.
A headword detector is first applied to detect the headwords in each phrase in a sentence.
A permuted headword trigram model (PHTM) is then generated from the annotated corpus.
"Finally, PHTM is extended to a cluster model (C-PHTM), which clusters similar words in the corpus."
"Our models are motivated by three assumptions about language: (1) Headwords depend on previous headwords, as well as immediately preceding words; (2) The order of headwords in a sentence can freely change in some cases; and (3) Word clusters help us make a more accurate estimate of the probability of word strings."
"We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion, which converts phonetic Kana strings into proper Japanese orthography."
Results show that C-PHTM achieves a 15% error rate reduction over the word trigram model.
"This demonstrates that the use of simple methods can effectively capture long distance word dependency, and substantially outperform the word trigram model."
"Although the techniques in this paper are described in the context of Japanese Kana-Kanji conversion, we believe that they can be extended to other languages and applications."
This paper is organized as follows.
Sections 2 and 3 describe the techniques of using headword dependency and clustering for language modeling.
Section 4 reviews related work.
"Section 5 introduces the evaluation methodology, and Section 6 presents the results of our main experiments."
Section 7 concludes our discussion.
"Japanese linguists have traditionally distinguished two types of words 1 , content words (jiritsugo) and function words (fuzokugo), along with the notion of the bunsetsu (phrase)."
"Each bunsetsu typically consists of one content word, called a headword in this paper, and several function words."
Figure 1 shows a Japanese example sentence and its English translation [Footnote_2] . [ 治療+に][専念+して][全快+まで][十分+な][療養+に][努め+る] [chiryou + ni] [sennen + shite] [zenkai + made] [treatment+to][concentration+do][full-recovery+until] [juubun + na] [ryouyou + ni] [tsutome + ru] [enough+ ADN ] [rest+for] [try+ PRES ] &apos;(One) concentrates on the treatment and tries to rest enough until full recovery&apos;
"2 Square brackets demarcate the bunsetsu boundary, and + the morpheme boundary; the underlined words are the headwords. ADN indicates an adnominal marker, and PRES indicates a present tense marker."
A Japanese example sentence with bunsetsu and headword tags
"In Figure 1, we find that some headwords in the sentence are expected to have a stronger dependency relation with their preceding headwords than with their immediately preceding function words."
"For example, the three headwords 治療 ~ 専念 ~ 全快 (chiryou &apos;treatment&apos; ~ sennen &apos;concentrate&apos; ~ zenkai &apos;full recovery&apos;) form a trigram with very strong semantic dependency."
"Therefore, we can hypothesize (in the trigram context) that headwords may be conditioned not only by the two immediately preceding words, but also by two previous headwords."
This is our first assumption.
We also note that the order of headwords in a sentence is flexible in some sense.
"From the example in Figure 1, we find that if 治療 ~ 専念 ~ 全快 (chiryou &apos;treatment&apos; ~ sennen &apos;concentrate&apos; ~ zenkai &apos;full recovery&apos;) is a meaningful trigram, then its permutations (such as 全快 ~ 治療 ~ 専念 (zenkai &apos;full recovery&apos; ~ chiryou &apos;treatment&apos; ~ sennen &apos;concentrate&apos;)) should also be meaningful, because headword trigrams tend to capture an order-neutral semantic dependency."
"This reflects a characteristic of Japanese, in which arguments and modifiers of a predicate can freely change their word order, a phenomenon known as &quot;scrambling&quot; in linguistic literature."
We can then introduce our second assumption: headwords in a trigram are permutable.
"Note that the permutation of headwords should be useful more generally beyond Japanese: for example, in English, the book Mary bought and Mary bought a book can be captured by the same headword trigram (Mary ~ bought ~ book) if we allow such permutations."
"In this subsection, we have stated two assumptions about the structure of Japanese that can be exploited for language modeling."
We now turn to discuss how to incorporate these assumptions in language modeling.
A trigram model predicts the next word w i by estimating the conditional probability
"P(w i |w i-2 w i-1 ), assuming that the next word depends only on two preceding words, w i-2 and w i-1 ."
The PHTM is a simple extension of the trigram model that incorporates the dependencies between headwords.
"If we assume that each word token can uniquely be classified as a headword or a function word, the PHTM can be considered as a cluster-based language model with two clusters, headword H and function word F. We can then define the conditional probability of w i based on its history as the product of the two factors: the probability of the category (H or F), and the probability of w i given its category."
"Let h i or f i be the actual headword or function word in a sentence, and let H i or F i be the category of the word w i ."
The PHTM can then be formulated as:
P(w i | Φ(w 1 ...w i−1 )) = ([Footnote_1]) P(H i | Φ(w 1 ...w i−1 )) ×
"1 Or more correctly, morphemes. Strictly speaking, the LMs discussed in this paper are morpheme-based models rather than word-based, but we will not make this distinction in this paper."
P(w i | Φ(w 1 ...w i−1 )
H i ) +P(F i | Φ(w 1 ...w i−1 )) ×
P(w i | Φ(w 1 ...w i−1 )
F i ) where Φ is a function that maps the word history (w 1 …w i-1 ) onto equivalence classes.
"P(H i |Φ(w 1 …w i-1 )) and P(F i |Φ(w 1 …w i-1 )) are category probabilities, and P(w i |Φ(w 1 …w i-1 )"
F i ) is the word probability given that the category of w i is function word.
"For these three probabilities, we used the standard trigram estimate (i.e., Φ(w 1 …w i-1 ) = (w i-2 w i-1 ))."
"The estimation of headword probability is slightly more elaborate, reflecting the two assumptions described in Section 2.1:"
P(w i | Φ(w 1 ...w i−1 )
H i ) = λ 1 ( λ 2
P(w i | h i−2 h i−1 H i ) (2) +(1− λ 2 )
P(w i | h i−1 h i−2 H i )) +(1− λ 1 )
P(w i | w i−2 w i−1 H i ) .
This estimate is an interpolated probability of three probabilities:
P(w i |h i-2 h i-1 H i ) and
"P(w i |h i-1 h i-2 H i ), which are the headword trigram probability with or without permutation, and P(w i |w i-2 w i-1 H i ), which is the probability of w i given that it is a headword, where h i-1 and h i-2 denote the two preceding headwords, and λ 1 , λ 2 ∈ [0,1] are the interpolation weights optimized on held-out data."
"The use of λ 1 in Equation (2) is motivated by the first assumption described in Section 2.1: headwords are conditioned not only on two immediately preceding words, but also on two previous headwords."
"In practice, we estimated the headword probability by interpolating the conditional probability based on two previous headwords"
"P(w i |h i-2 h i-1 H i ) (and P(w i |h i-1 h i-2 H i ) with permutation), and the conditional probability based on two preceding words P(w i |w i-2 w i-1 H i )."
"If λ 1 is around zero, it indicates that this assumption does not hold in real data."
Note that we did not estimate the conditional probability
"P(w i |w i-2 w i-1 h i-2 h i-1 H i ) directly, because this is in the form of a 5-gram, where the number of parameters are too large to estimate."
The use of λ 2 in Equation (2) comes from the second assumption in Section 2.1: headword trigrams are permutable.
"This assumption can be formulated as a co-occurrence model for headword prediction: that is, the probability of a headword is determined by the occurrence of other headwords within a window."
"However, in our experiments, we instead used an interpolated probability λ 2 ×P(w i |h i-2 h i-1 H i ) + (1–λ 2 )×P(w i |h i-1 h i-2 H i ) for two reasons."
"First, co-occurrence models do not predict words from left to right, and are thus very difficult to interpolate with trigram models for decoding."
"Second, if we see n-gram models as one extreme that predicts the next word based on a strictly ordered word sequence, co-occurrence models go to the other extreme of predicting the next word based on a bag of previous words without taking word order into account at all."
"We prefer models that lie somewhere between the two extremes, and consider word order in a more flexible way."
"In PHTM of Equation (2), λ 2 represents the impact of word order on headword prediction."
"When λ 2 = 1 (i.e., the resulting model is a non-permuted headword trigram model, referred to as HTM), it indicates that the second assumption does not hold in real data."
"When λ 2 is around 0.5, it indicates that a headword bag model is sufficient."
Assume that all conditional probabilities in Equation (1) are estimated using maximum likelihood estimation (MLE).
Then P(w i | w i−2 w i−1 ) =
P(F i | w i−2 w i−1 )
"P(w i | w i−2 w i−1 F i ) , w i : function word  is a strict equality when each word token is uniquely classified as a headword or a function word."
This can be trivially proven as follows.
Let C i represent the category of w i (H i or F i in our case).
P(C i | w i−2 w i−1 )×P(w i |w i−2 w i−1 C i ) =
P(w i−2 w i−i C i ) ×
P(w i−2 w i−1 C i w i )
P(w i−2 w i−1 )
P(w i−2 w i−1 C i ) =
P(w i−2 w i−1 C i w i ) (3) P(w i−2 w i−1 )
"Since each word is uniquely assigned to a category, P(C i |w i )=1, and thus it follows that"
P(w i−2 w i−1 C i w i ) =
P(w i−2 w i−1 w i ) ×P(C i | w i−2 w i−1 w i ) =
P(w i−2 w i−1 w i ) × P(C i | w i ) =
P(w i−2 w i−1 w i ) . (4)
"Substituting Equation (4) into Equation (3), we get"
P(C i | w i−2 w i−1 ) ×
P(w i | w i−2 w i−1 C i ) =
P(w i−2 w i−1 w i ) =
P(w | w w ) . (5) i−2 i−1i
P(w i−2 w i−1 )
"Now, by separating the estimates of probabilities of headwords and function words, Equation (1) can be rewritten as:"
P(w i |Φ(w 1 …w i-1 ))= (6)  λ 1 (P(H i | w i−2 w i−1 )( λ 2
P(w i | h i−2 h i−1 )  + (1− λ 2 )
P(w i | h i−1 h i−2 ))   + (1− λ 1 )
P(w i | w i−2 w i−1 )   w i : headword  
P(w i | w i−2 w i−1 ) w i : function word
There are three probabilities to be estimated in Equation (6): word trigram probability
"P(w i |w i-2 w i-1 ), headword trigram probability"
P(w i |h i-2 h i-1 ) and
"P(w i |h i-1 h i-2 ) (where w i is a headword), and category probability P(H i |w i-2 w i-1 )."
"In order to deal with the data sparseness problem of MLE, we used a backoff scheme[REF_CITE]for the parameter estimation."
The backoff scheme recursively estimates the probability of an unseen n-gram by utilizing (n–1)-gram estimates.
"To keep the model size manageable, we also removed all n-grams with frequency less than 2."
"In order to classify a word uniquely as H or F, we needed a mapping table where each word in the lexicon corresponds to a category."
The table was generated in the following manner.
We first assumed that the mapping from part-of-speech (POS) to word category is fixed.
"The tag set we used included 1,187 POS tags, of which 102 count as headwords in our experiments."
"We then used a POS-tagger to generate a POS-tagged corpus, from which we generated the mapping table 3 ."
"If a word could be mapped to both H and F, we chose the more frequent category in the corpus."
"Using this mapping table, we achieved a 98.5% accuracy of headword detection on the test data we used."
"Through our experiments, we found that P(H i |w i-2 w i-1 ) is a poor estimator of category probability; in fact, the unigram estimate P(H i ) achieved better results in our experiments as shown in Section 6.1."
"Therefore, we also used the unigram estimate for word category probability in our experiments."
The alternative model that uses the unigram estimate is given below:
P(w i |Φ(w 1 …w i-1 ))= (7)  λ 1 (P(H i )( λ 2
P(w i | h i−2 h i−1 )  + (1− λ 2 )
P(w i |h i−1 h i−2 ))   + (1− λ 1 )
P(w i | w i−2 w i−1 )  w i : headword  
P(w i | w i−2 w i−1 )  w i : function word
"We will denote the models using trigram for category probability estimation of Equation (6) as T-PHTM, and the models using unigram for category probability estimation of Equation (7) as U-PHTM."
Clustering techniques attempt to make use of similarities between words to produce a better estimate of the probability of word strings[REF_CITE].
"We have mentioned in Section 2.2 that the headword trigram model can be thought of as a cluster-based model with two clusters, the headword and the function word."
"In this section, we describe a method of clustering automatically similar words and headwords."
"We followed the techniques described[REF_CITE]and[REF_CITE], and performed experiments using predictive clustering along with headword trigram models."
"Consider a trigram probability P(w [Footnote_3] |w 1 w 2 ), where w [Footnote_3] is the word to be predicted, called the predicted word, and w 1 and w 2 are context words used to predict w 3 , called the conditional words."
"3 Since the POS-tagger does not identify phrases, our implementation does not identify precisely one headword for a phrase, but identify multiple headwords in the case of compounds."
"3 Since the POS-tagger does not identify phrases, our implementation does not identify precisely one headword for a phrase, but identify multiple headwords in the case of compounds."
Let w i be the cluster which word w i belongs to.
"In this study, we performed word clustering for words and headwords separately."
"As a result, we have the following two predictive clustering models, (8) for words and (9) for headwords:"
P(w i | w i−2 w i−1 ) =
P(w i | w i−2 w i−1 ) ×
P(w i | w i−2 w i−1 w i ) (8) (9)
P(w i | h i−2 h i−1 ) =
P(w i | h i−2 h i−1 ) ×
P(w i | h i−2 h i−1 w i ) w i : headword
"Substituting Equations (8) and (9) into Equation (7), we get the cluster-based PHTM[REF_CITE], referred to as C-PHTM."
P(w i |Φ(w 1 …w i-1 ))= (10)  λ 1 (P(H i )( λ 2
P(w i | h i−2 h i−1 ) ×
P(w i | h i−2 h i−1 w i )  + (1− λ 2 )
P(w i |h i−1 h i−2 )×P(w i |h i−1 h i−2 w i ))   + (1− λ 1 )
P(w i | w i−2 w i−1 ) ×P(w i |w i−2 w i−1 w i )  w i : headword  
P(w |w w )×P(w |w w w)  i i−2 i−1 i i−2 i−1 i  w i : function word
"In constructing clustering models, two factors were considered: how to find optimal clusters, and the optimal number of clusters."
The clusters were found automatically by attempting to minimize perplexity[REF_CITE].
"In particular, for predictive clustering models, we tried to minimize the perplexity of the training data of P(w i | w i−1 ) ×"
P(w i | w i ) .
"Letting N be the size of the training data, we have"
P(w i | w i−1 ) ×
P(w i | w i ) i=1
P(w i−1 w i ) ×
P(w i w i ) = ∏ i=1
P(w i−1 )
P(W i )
P(w i w i ) ×
P(w i−1 w i ) = ∏ i=1
P(w i−1 )
P(w i ) N
P(w i ) ×
P(w | w ) = ∏ i−1 i i=1
P(w i−1 )
"Now, P(w i ) is independent of the clustering used."
P(w i−1 )
"Therefore, in order to select the best clusters, it is sufficient to try to maximize ∏ Ni=1"
P(w i−1 | w i ) .
The clustering technique we used creates a binary branching tree with words at the leaves.
"By cutting the tree at a certain level, it is possible to achieve a wide variety of different numbers of clusters."
"For instance, if the tree is cut after the sixth level, there will be roughly 2 6 =64 clusters."
"In our experiments, we always tried the numbers of clusters that are the powers of 2."
This seems to produce numbers of clusters that are close enough to optimal.
Our LMs are similar to a number of existing ones.
"One such model was proposed by ATR[REF_CITE], which we will refer to as ATR model below."
"In ATR model, the probability of each word in a sentence is determined by the preceding content and function word pair."
One significant difference between the ATR model and our own lies in the use of predictive clustering.
"Another difference is that our models use separate probability estimates for headwords and function words, as shown in Equations (6) and (7)."
"In contrast, ATR models are conceptually more similar to skipping models[REF_CITE], where only one probability estimate is applied for both content and function words, and the word categories are used only for the sake of finding the content and function word pairs in the context."
"Another model similar to ours is[REF_CITE], where the headwords of the two phrases immediately preceding the word as well as the last two words were used to compute a word probability."
The resulting model is similar to a 5-gram model.
A sophisticated interpolation formula had to be used since the number of parameters is too large for direct estimation.
Our models are easier to learn because they use trigrams.
They also differ from Jelinek&apos;s model in that they separately estimate the probability for headwords and function words.
A significant number of sophisticated techniques for language modeling have recently been proposed in order to capture more linguistic structure from a larger context.
"Unfortunately, most of them suffer from either high computational cost or difficulty in obtaining enough manually parsed corpora for parameter estimation, which make it difficult to apply them successfully to realistic applications."
"For example, maximum entropy (ME) models[REF_CITE]provide a nice framework for incorporating arbitrary knowledge sources, but training and using ME models is computationally extremely expensive."
"Another interesting idea that exploits the use of linguistic structure is structured language modeling (SLM,[REF_CITE])."
"SLM uses a statistical parser trained on an annotated corpus in order to identify the headword of each constituent, which are then used as conditioning words in the trigram context."
"Though SLMs have been shown to significantly improve the performance of the LM measured in perplexity, they also pose practical problems."
"First, the performance of SLM is contingent on the amount and quality of syntactically annotated training data, but such data may not always be available."
"Second, SLMs are very time-intensive, both in their training and use."
"They both report improvements in perplexity[REF_CITE]on the Wall Street Journal section of the Penn Treebank data, suggesting that syntactic structure can be further exploited for language modeling."
"The kind of linguistic structure used in our models is significantly more modest than that provided by parser-based models, yet offers practical benefits for realistic applications, as shown in the next section."
The most common metric for evaluating a language model is perplexity.
Perplexity can be roughly interpreted as the expected branching factor of the test document when presented to a language model.
Perplexity is widely used due to its simplicity and efficiency.
"However, the ultimate quality of a language model must be measured by its effect on the specific task to which it is applied, such as speech recognition."
"Lower perplexities usually result in lower error rates, but there are numerous counterexamples to this in the literature."
"In this study, we evaluated our language models on the application of Japanese Kana-Kanji conversion, which is the standard method of inputting Japanese text by converting the text of syllabary-based Kana string into the appropriate combination of ideographic Kanji and Kana."
"This is a similar problem to speech recognition, except that it does not include acoustic ambiguity."
"Performance on this task is generally measured in terms of the character error rate (CER), which is the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript."
The role of the language model is to select the word string (in a combination of Kanji and Kana) with the highest probability among the candidate strings that match the typed phonetic (Kana) string.
Current products make about 5-10% errors in conversion of real data in a wide variety of domains.
"For our experiments, we used two newspaper corpora:"
Nikkei and Yomiuri Newspapers.
Both corpora have been word-segmented.
We built language models from a 36-million-word subset of the Nikkei Newspaper corpus.
"We performed parameter optimization on a 100,000-word subset of the Yomiuri Newspaper (held-out data)."
"We tested our models on another 100,000-word subset of the Yomiuri Newspaper corpus."
"The lexicon we used contains 167,107 entries."
"In our experiments, we used the so-called “N-best rescoring” method."
"In this method, a list of hypotheses is generated by the baseline language model (a word trigram model in this study [Footnote_4] ), which is then rescored using a more sophisticated LM."
"4 For the detailed description of the baseline trigram model, see[REF_CITE]."
"Due to the limited number of hypotheses in the N-best list, the second pass may be constrained by the first pass."
"In this study, we used the 100-best list."
"The “oracle” CER (i.e., the CER among the hypotheses with the minimum number of errors) is presented in Table 1."
This is the upper bound on performance in our experiments.
The performance of the conversion using the baseline trigram model is much better than the state-of-the-art performance currently available in the marketplace.
"This may be due to the large amount of training data we used, and to the similarity between the training and the test data."
"We also notice that the “oracle” CER is relatively high due to the high out-of-vocabulary rate, which is 1.14%."
"Because we have only limited room for improvement, the reported results of our experiments in this study may be underestimated."
We applied a series of language models proposed in this paper to the Japanese Kana-Kanji conversion task in order to test the effectiveness of our techniques.
The results are shown in Table 2.
The baseline result was obtained by using a conventional word trigram model.
"HTM stands for the headword trigram model of Equation (6) and (7) without permutation (i.e., λ 2 =1), while PHTM is the model with permutation."
The T- and U-prefixes refer to the models using trigram (Equation (6)) or unigram (Equation (7)) estimate for word category probability.
"The C-prefix, as in C-PHTM, refers to PHTM with predictive clustering[REF_CITE]."
"For comparison, we also include in Table 2 the results of using the predictive clustering model without taking word category into account, referred to as predictive clustering trigram model (PCTM)."
"In PCTM, the probability for all words is estimated by P(w i | w i−2 w i−1 ) ×"
P(w i | w i−2 w i−1 w i ) .
"In Table 2, we find that for both PHTM and HTM, models U-HTM and U-PHTM achieve better performance than models T-HTM and T-PHTM."
"Therefore, only models using unigram for category probability estimation are used for further experiments, including the models with predictive clustering."
"By comparing U-HTM with the baseline model, we can see that the headword trigram contributes greatly to the CER reduction: U-HTM outperformed the baseline model by about 8.6% in error rate reduction."
HTM with headword permutation (U-PHTM) achieves further improvements of 10.5% CER reduction against the baseline.
The contribution of predictive clustering is also very encouraging.
"Using predictive clustering alone (PCTM), we reduced the error rate by 7.8%."
"What is particularly noteworthy is that the combination of both techniques leads to even larger improvements: for both HTM and PHTM, predictive clustering (C-HTM and C-PHTM) brings consistent improvements over the models without clustering, achieving the CER reduction of 13.4% and 15.0% respectively against the baseline model, or 4.8% and 4.5% against the models without clustering."
"In sum, considering the good performance of our baseline system and the upper bound on performance improvement due to the 100-best list as shown in Table 1, the improvements we obtained are very promising."
These results demonstrate that the simple method of using headword trigrams and predictive clustering can be used to effectively improve the performance of word trigram models.
"In this subsection, we present a comparison of our models with some of the previously proposed models, including the higher-order n-gram models, skipping models, and the ATR models."
Higher-order n-gram models refer to those n-gram models in which n&gt;3.
"Although most of the previous research showed little improvement,[REF_CITE]showed recently that, with a large amount of training data and sophisticated smoothing techniques, higher-order n-gram models could be superior to trigram models."
"The headword trigram model proposed in this paper can be thought of as a variation of a higher order n-gram model, in that the headword trigrams capture longer distance dependencies than trigram models."
"In order to see how far the dependency goes within our headword trigram models, we plotted the distribution of headword trigrams (y-axis) against the n of the word n-gram were it to be captured by the word n-gram (x-axis) in Figure 2."
"For example, given a word sequence w 1 w 2 w 3 w 4 w 5 w 6 , and if w 1 , w 3 and w 6 are headwords, then the headword trigram P(w 6 |w 3 w 1 ) spans the same distance as the word 6-gram model."
"From Figure 2, we can see that approximately 95% of the headword trigrams can be captured by the higher-order n-gram model with the value of n smaller than 7."
"Based on this observation, we built word n-gram models with the values of n=4, 5 and 6."
"For all n-gram models, we used the interpolated modified absolute discount smoothing method[REF_CITE], which, in our experiments, achieved the best performance among the state-of-the-art smoothing techniques."
"Results showed that the performance of the higher-order word n-gram models becomes saturated quickly as n grows: the best performance was achieved by the word 5-gram model, with the CER of 3.71%."
"Skipping models are an extension of an n-gram model in that they predict words based on n conditioning words, except that these conditioning words may not be adjacent to the predicted word."
"For instance, instead of computing P(w i |w i-2 w i-1 ), a skipping model might compute P(w i |w i-3 w i-1 ) or P(w i |w i-4 w i-2 )."
"Our results confirm his results and suggest that simply extending the context window by brute-force can achieve little improvement, while the use of even the most modest form of structural information such as the identification of headwords and automatic clustering can help improve the performance."
"We also compared our models with the trigram version of the ATR models discussed in Section 4, in which the probability of a word is conditioned by the preceding content and function word pair."
We performed experiments using the ATR models as described[REF_CITE].
"The results show that the CER of the ATR model alone is much higher than that of the baseline model, but when interpolated with a word trigram model, the CER is slightly reduced by 1.6% from 3.73% to 3.67%."
These results are consistent with those reported in previous work.
"The difference between the ATR model and our models indicates that the predictions of headwords and function words can better be done separately, as they play different semantic and syntactic roles capturing different dependency structure."
"In order to better understand the effect of the headword trigram, we have manually inspected the actual improvements given by PHTM."
"As expected, many of the improvements seem to be due to the use of larger context: for example, the headword trigram 消 費 ~ 支 出 ~ 減 少 (shouhi &apos;consume&apos; ~ shishutsu &apos;expense&apos; ~ genshou &apos;decrease&apos;) contributed to the correct conversion of the phonetic string げんしょう genshou into 減少 genshou &apos;decrease&apos; rather than 現 象 genshou &apos;phenomenon&apos; in the context of 消費支出初めての減 少 shouhi shishutsu hajimete no genshou &apos;consumer spending decreases for the first time&apos;."
"On the other hand, the use of headword trigrams and predictive clustering is not without side effects."
"The overall gain[REF_CITE]% as we have seen above, but a closer inspection of the conversion results reveals that while C-PHTM corrected the conversion errors of the baseline model in 389 sentences (8%), it also introduced new conversion errors in 201 sentences (4.1%)."
"Among the newly introduced errors, one type of error is particularly worth noting: these are the errors where the candidate conversion preferred by the HTM is grammatically impossible or unlikely."
"For example, 米 国 に 侵 攻 で き る (beikoku-ni shinkou-dekiru, USA-to invade-can &apos;can invade USA&apos;) was misconverted as 米 国 に 新 興 で き る (beikoku-ni shinkou-dekiru, USA-to new-can), even though 侵 攻 shinkou &apos;invade&apos; is far more likely to be preceded by the morpheme に ni &apos;to&apos;, and 新興 shinkou &apos;new&apos; practically does not precede できる dekiru &apos;can&apos;."
"The HTM does not take these function words into account, leading to a grammatically impossible or implausible conversion."
Finding the types of errors introduced by particular modeling assumptions in this manner and addressing them individually will be the next step for further improvements in the conversion task.
"We proposed and evaluated a new language model, the permuted headword trigram model with clustering (C-PHTM)."
We have shown that the simple model that combines the predictive clustering with a headword detector can effectively capture structure in language.
Experiments show that the proposed model achieves an encouraging 15% CER reduction over a conventional word trigram model in a Japanese Kana-Kanji conversion system.
"We also compared C-PTHM to several similar models, showing that our model has many practical advantages, and achieves substantially better performance."
One issue we did not address in this paper was the language model size: the models that use HTM are larger than the baseline model we compared the performance with.
"Though we did not pursue the issue of size reduction in this paper, there are many known techniques that effectively reduce the model size while minimizing the loss in performance."
One area of future work is therefore to reduce the model size.
Other areas include the application of the proposed model to a wider variety of test corpora and to related tasks.
We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy.
The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers.
"Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer."
"Question answering has recently received attention from the information retrieval, information extrac-tion, machine learning, and natural language proc-essing communities ([REF_CITE]2001)."
"The goal of a question answering system is to retrieve answers to questions rather than full documents or best-matching passages, as most information re-trieval systems currently do."
"The TREC Question Answering Track, which has motivated much of the recent work in the field, focuses on fact-based, short-answer questions such as “Who killed Abra-ham Lincoln?” or “How tall is Mount Everest?”"
"In this paper we describe our approach to short answer tasks like these, although the techniques we propose are more broadly applicable."
Most question answering systems use a va-riety of linguistic resources to help in understand- ing the user’s query and matching sections in documents.
"The most common linguistic resources include: part-of-speech tagging, parsing, named entity extraction, semantic relations, dictionaries, WordNet, etc. (e.g.,[REF_CITE])."
We chose instead to focus on the Web as a gigantic data re-pository with tremendous redundancy that can be exploited for question answering.
"We view our approach as complimentary to more linguistic ap-proaches, but have chosen to see how far we can get initially by focusing on data per se as a key resource available to drive our system design."
"Re-cently, other researchers have also looked to the web as a resource for question answering[REF_CITE]."
"These systems typically perform complex parsing and entity extraction for both queries and best matching Web pages, and maintain local caches of pages or term weights."
Our approach is distinguished from these in its simplicity and effi-ciency in the use of the Web as a large data re-source.
"Automatic QA from a single, small infor-mation source is extremely challenging, since there is likely to be only one answer in the source to any user’s question."
"Given a source, such as the TREC corpus, that contains only a relatively small number of formulations of answers to a query, we may be faced with the difficult task of mapping questions to answers by way of uncovering com-plex lexical, syntactic, or semantic relationships between question string and answer string."
"The need for anaphor resolution and synonymy, the presence of alternate syntactic formulations and indirect answers all make answer finding a poten-tially challenging task."
"However, the greater the answer redundancy in the source data collection, the more likely it is that we can find an answer that occurs in a simple relation to the question."
"There-fore, the less likely it is that we will need to solve the aforementioned difficulties facing natural lan-guage processing systems."
"In this paper, we describe the architecture of the AskMSR Question Answering System and evaluate contributions of different system compo-nents to accuracy."
"Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answer-ing system is likely to give an incorrect answer."
"As shown in Figure 1, the architecture of our sys-tem can be described by four main steps: query-reformulation, n-gram mining, filtering, and n-gram tiling."
"In the remainder of this section, we will briefly describe these components."
A more detailed description can be found in [[REF_CITE]].
"Given a question, the system generates a number of weighted rewrite strings which are likely sub-strings of declarative answers to the question."
"For example, “When was the paper clip invented?” is rewritten as “The paper clip was invented”."
We then look through the collection of documents in search of such patterns.
"Since many of these string rewrites will result in no matching documents, we also produce less precise rewrites that have a much greater chance of finding matches."
"For each query, we generate a rewrite which is a backoff to a sim-ple ANDing of all of the non-stop words in the query."
The rewrites generated by our system are simple string-based manipulations.
"We do not use a parser or part-of-speech tagger for query refor-mulation, but do use a lexicon for a small percent-age of rewrites, in order to determine the possible parts-of-speech of a word as well as its morpho-logical variants."
"Although we created the rewrite rules and associated weights manually for the cur-rent system, it may be possible to learn query-to-answer reformulations and their weights (e.g.,[REF_CITE])."
"Once the set of query reformulations has been gen-erated, each rewrite is formulated as a search en-gine query and sent to a search engine from which page summaries are collected and analyzed."
"From the page summaries returned by the search engine, n-grams are collected as possible answers to the question."
"For reasons of efficiency, we use only the page summaries returned by the engine and not the full-text of the corresponding web page."
"The returned summaries contain the query terms, usually with a few words of surrounding context."
The summary text is processed in accor-dance with the patterns specified by the rewrites.
"Unigrams, bigrams and trigrams are extracted and subsequently scored according to the weight of the query rewrite that retrieved it."
These scores are summed across all summaries containing the n- gram (which is the opposite of the usual inverse document frequency component of docu-ment/passage ranking schemes).
We do not count frequency of occurrence within a summary (the usual tf component in ranking schemes).
"Thus, the final score for an n-gram is based on the weights associated with the rewrite rules that generated it and the number of unique summaries in which it occurred."
"Next, the n-grams are filtered and reweighted ac-cording to how well each candidate matches the expected answer-type, as specified by a handful of handwritten filters."
The system uses filtering in the following manner.
"First, the query is analyzed and assigned one of seven question types, such as who-question, what-question, or how-many-question."
"Based on the query type that has been assigned, the system determines what collection of filters to apply to the set of potential answers found during the collection of n-grams."
"The candidate n-grams are analyzed for features relevant to the fil-ters, and then rescored according to the presence of such information."
A collection of 15 simple filters were devel-oped based on human knowledge about question types and the domain from which their answers can be drawn.
"These filters used surface string fea-tures, such as capitalization or the presence of dig-its, and consisted of handcrafted regular expression patterns."
"Finally, we applied an answer tiling algorithm, which both merges similar answers and assembles longer answers from overlapping smaller answer fragments."
"For example, &quot;A B C&quot; and &quot;B C D&quot; is tiled into &quot;A B C D.&quot; The algorithm proceeds greedily from the top-scoring candidate - all sub-sequent candidates (up to a certain cutoff) are checked to see if they can be tiled with the current candidate answer."
"If so, the higher scoring candi-date is replaced with the longer tiled n-gram, and the lower scoring candidate is removed."
The algo-rithm stops only when no n-grams can be further tiled.
For experimental evaluations we used the first 500 TREC-9 queries (201-700)[REF_CITE].
We used the patterns provided by NIST for automatic scoring.
A few patterns were slightly modified to accommodate the fact that some of the answer strings returned using the Web were not available for judging in TREC-9.
"We did this in a very conservative manner allowing for more spe-cific correct answers (e.g., Edward J. Smith vs. Edward Smith) but not more general ones (e.g., Smith vs. Edward Smith), and also allowing for simple substitutions (e.g., 9 months vs. nine months)."
"There also are substantial time differ-ences between the Web and TREC databases (e.g., the correct answer to Who is the president of Bo-livia? changes over time), but we did not modify the answer key to accommodate these time differ-ences, because it would make comparison with earlier TREC results impossible."
"These changes influence the absolute scores somewhat but do not change relative performance, which is our focus here."
"All runs are completely automatic, starting with queries and generating a ranked list of 5 can-didate answers."
For the experiments reported in this paper we used Google as a backend because it provides query-relevant summaries that make our n-gram mining efficient.
"Candidate answers are a maximum of 50 bytes long, and typically much shorter than that."
"We report the Mean Reciprocal Rank (MRR) of the first correct answer, the Num-ber of Questions Correctly Answered (NAns), and the proportion of Questions Correctly Answered (%Ans)."
"Using our current system with default settings we obtain a MRR of 0.507 and answers 61% of the queries correctly (Baseline, Table 1)."
"The average answer length was 12 bytes, so the system is re-turning short answers, not passages."
"Although it is impossible to compare our results precisely with TREC-9 groups, this is very good performance and would place us near the top of 50-byte runs for TREC-9."
Table 1 summarizes the contributions of the differ-ent system components to this overall perform-ance.
We report summary statistics as well as percent change in performance when components are removed (%Drop MRR).
Above we described how components contributed to improving the performance of the system.
In this section we look at what components errors are attributed to.
"In Table 2, we show the distribution of error causes, looking at those questions for which the system returned no correct answer in the top five hypotheses."
The biggest error comes from not knowing what units are likely to be in an answer given a question (e.g. How fast can a Corvette go Æ xxx mph).
Number retrieval problems come from the fact that we cannot query the search engine for a num-ber without specifying the number.
"For example, a good rewrite for the query How many islands does Fiji have would be « Fiji has &lt;NUM&gt; islands », but we are unable to give this type of query to the search engine."
"Typically, when deploying a question answering system, there is some cost associated with return-ing incorrect answers to a user."
"Therefore, it is important that a QA system has some idea as to how likely an answer is to be correct, so it can choose not to answer rather than answer incor-rectly."
"In the TREC QA track, there is no distinc-tion made in scoring between returning a wrong answer to a question for which an answer exists and returning no answer."
"However, to deploy a real system, we need the capability of making a trade-off between precision and recall, allowing the system not to answer a subset of questions, in hopes of attaining high accuracy for the questions which it does answer."
Most question-answering systems use hand-tuned weights that are often combined in an ad-hoc fashion into a final score for an answer hy-pothesis ([REF_CITE];
"Soubbotin &amp;[REF_CITE]; Brill et. al., 2001)."
Is it still possible to induce a useful precision-recall (ROC) curve when the sys-tem is not outputting meaningful probabilities for answers?
We have explored this issue within the AskMSR question-answering system.
"Ideally, we would like to be able to deter-mine the likelihood of answering correctly solely from an analysis of the question."
"If we can deter-mine we are unlikely to answer a question cor-rectly, then we need not expend the time, cpu cycles and network traffic necessary to try to an-swer that question."
"We built a decision tree to try to predict whether the system will answer correctly, based on a set of features extracted from the question string: word unigrams and bigrams, sentence length (QLEN), the number of capitalized words in the sentence, the number of stop words in the sentence (NUMSTOP), the ratio of the number of nonstop words to stop words, and the length of longest word (LONGWORD)."
We use a decision tree be-cause we also wanted to use this as a diagnostic tool to indicate what question types we need to put further developmental efforts into.
The decision tree built from these features is shown in Figure 2.
The first split of the tree asks if the word “How” appears in the question.
"Indeed, the system per-forms worst on “How” question types."
We do best on short “Who” questions with a large number of stop words.
We can induce an ROC curve from this decision tree by sorting the leaf nodes from the highest probability of being correct to the lowest.
Then we can gain precision at the expense of recall by not answering questions in the leaf nodes that have the highest probability of error.
"The result of doing this can be seen in Figures 3 and 4, the line labeled “Question Features”."
The decision tree was trained on Trec 9 data.
"Figure 3 shows the results when applied to the same training data, and Figure 4 shows the results when testing[REF_CITE]data."
"As we can see, the decision tree overfits the training data and does not generalize sufficiently to give useful results on the[REF_CITE](test) data."
"Next, we explored how well answer cor-rectness correlates with answer score in our sys-tem."
"As discussed above, the final score assigned to an answer candidate is a somewhat ad-hoc score based upon the number of retrieved passages the n-gram occurs in, the weight of the rewrite used to retrieve each passage, what filters apply to the n-gram, and the effects of merging n-grams in an-swer tiling."
"In Table 3, we show the correlation coefficient calculated between whether a correct answer appears in the top 5 answers output by the system and (a) the score of the system’s first ranked answer and (b) the score of the first ranked answer minus the score of the second ranked an-swer."
"A correlation coefficient of 1 indicates strong positive association, whereas a correlation of –1 indicates strong negative association."
"We see that there is indeed a correlation between the scores output by the system and the answer accu-racy, with the correlation being tighter when just considering the score of the first answer."
"Because a number of answers returned by our system are correct but scored wrong according to the TREC answer key because of time mis-matches, we also looked at the correlation, limiting ourselves to Trec 9 questions that were not time-sensitive."
"Using this subset of questions, the corre-lation coefficient between whether a correct an-swer appears in the system’s top five answers, and the score of the #1 answer, increases from .363 to .401."
"In Figure 3 and 4, we show the ROC curve induced by deciding when not to answer a question based on the score of the first ranked answer (the line labeled “score of #1 answer”)."
"Note that the score of the top ranked answer is a significantly better predictor of accuracy than what we attain by considering features of the question string, and gives consistent results across two data sets."
"Finally, we looked into whether other at-tributes were indicative of the likelihood of answer correctness."
"For every question, a set of snippets is gathered."
Some of these snippets come from AND queries and others come from more refined exact string match rewrites.
"In Table 4, we show MRR as a function of the number of non-AND snippets retrieved."
"For instance, when all of the snippets come from AND queries, the resulting MRR was found to be only .238."
"For questions with 100 to 400 snippets retrieved from exact string match re-writes, the MRR was .628."
"We built a decision tree to predict whether a correct answer appears in the top 5 answers, based on all of the question-derived features de-scribed earlier, the score of the number one rank-ing answer, as well as a number of additional features describing the state of the system in proc-essing a particular query."
"Some of these features include: the total number of matching passages retrieved, the number of non-AND matching pas-sages retrieved, whether a filter applied, and the weight of the best rewrite rule for which matching passages were found."
"We show the resulting deci-sion tree in Figure 5, and resulting ROC curve con-structed from this decision tree, in Figure 3 and 4 (the line labeled “All Features”)."
"In this case, the decision tree does give a useful ROC curve on the test data (Figure 4), but does not outperform the simple technique of using the ad hoc score of the best answer returned by the system."
"Still, the deci-sion tree has proved to be a useful diagnostic in helping us understand the weaknesses of our sys-tem."
"We have presented a novel approach to question-answering and carefully analyzed the contributions of each major system component, as well as ana-lyzing what factors account for the majority of er-rors made by the AskMSR question answering system."
"In addition, we have demonstrated an approach to learning when the system is likely to answer a question incorrectly, allowing us to reach any desired rate of accuracy by not answering some portion of questions."
We are currently ex-ploring whether these techniques can be extended beyond short answer QA to more complex cases of information access.
K@  8¦V¦C¬À¹Ü ¨±$;Î  ¦N¸t©N»¼½N¦N²4µ¼¨Y­0®2¬/;Í $; ; Î1¦h¦C¬Y¨Yµ¶§z½gµ¶¬7¨±¦ÆÎ;«0µÌ»¶ºj©*­0µ¼½C­;»¼¿  ;#Î;¿ ¹  ¬ Ò  ¦N² ¬ N¬Yµ¶®2¥;»¼ª r  ¬ &amp;$ ¬ ¦C§0»¼¿@@ ;
Î ©N¬±ªx³a»¶©N¬Y¬Yµ¼Í;;µ¶¨&amp;µ¶¬&amp;.*;©N¬±ª:  µÌ¬.;»¼ª¨Y­; ;²Y¦C«z¨Yµ¶§zªNÃ$$ Î;©N¬±ª ;Ã § ý $  4Â8¨Y­zªÉÎ è ;©N¬±ª*; ©Î;©N¬±ª.;ªa ø ² %þ ± ÿ Ã d ;»¼¿eµ¶§z½2¨Y­0ª.;Î ©N¬±ª $@º0©¨Y ©eÂ¨Y­zªjÎ1¦h¦C¬Y¨Yµ¶§z½Ù©N»¼½N¦¹ ;®ö©N»¶¬±¦#:  ÿ ¦ÕØNªa².&amp;¨Y­zª¯¨±²4©Nµ¶§0µ¶§z½:­z¦Õ¾ïµ¶®*.ª´·e©N®.;¨ ³þ ÿ ³´¦N²±¹¨Y­zª å ©N»¶»¼¿NÂh¨Y­zª&amp;ª´·z©N®.;;¥ /.; ;;©N¬±ª°³a»Ì©N¬Y¬YµÐÍ;ªa²4¬¯¾&amp;µÌ»¶»&amp;Î1ª &amp;¨±¦$ª d  ¬ ¨ ­ ¯ª´·e©N®2¥ ¬ *¨Y­zªÉÎ;Ã; Ä ©N¬±ª¬ &quot; ;©¥;µ¼²Yª:©N§;; Û ¬ Ú¨Y­0©¨aÂÂ þ 8¦V¦C¬À¹    r   $  B ¦N² ¦C«z¨±¥;$;®:Î1ªa² §  þ ;ä f 1ã2¾ ã &amp;­z¦C¬±ªÆ¬Yµ¼½C§&amp;©N§0º Öä¾&amp;­z¦C¬YªÉ®2©½C§0µ¶¨Y«0ºz;³´ªNÃ ­0ª¯¦C«z¨±¥ª ;«0¨#¦N¸ ;ªa² ôµ¶¬ &lt;ç t þ ä fã ã  ç ¸ ø
H¬ ä ©N»¶ f »ê1 ã ³aã À »¶©N Þ µÐÍ;ªa ÿ  ²? ¬ þþ ÿ ä f :Ã1ã4 ã î&amp;Âz­zµùÃëªNÃÖª#¨Y©N­z»Ðª¹Ø\¬Y©N«0»Ì«z®þ¦N¸t¥0³´²Y¦C§eº0µ¶³´º0¨Yµ¶¦C§0§0¬&amp;¦N¦N¸¸ ; ÿ ªa² É³a©N§$ ©Î;;#]¹ ¨Yµ¼¦C§ P z¨Y­;©¨&amp;µ¶¬aÂz¾8ª: æ  æ ê ¨±¦É³a»Ì©N¬Y¬$Ô §°¸q©N³´¨aÂ ;.¨Y­0 © ´ ¨  ã ©N¬ ;µ¼¨Yµ¼¦C§0©N» . §0©N®. ¿ »¶§ äÀæ  ! ê #$&quot; %&amp;&quot; ã æ ¸ ñ 8¦V¦C¬±¨±ª´·e¨±ªa²g¨±¦Ù¨Y­zªK®#«0»¶¨Yµ¶³a»¶©N¬Y¬  ;µ¼²YªK©N§0º æ çNçNçCã4Ã5Ü5µ¼¨Y­Ú¨Y­0µ¶¬. &apos;¦V¦C¬±¨±ª´·e¨±ªa²$µÌ¬.©Î;»¼ª  ÔÀ§Û¦C«z²j©¥z¹ ;$ ¥*;º0©²Y¿¯©N§0º*: ³a»¶©N¬Y¬#¾&amp;:¨Y­;©¨ ² 8¦V¦C¬±¨±ª´·e¨±ªa²::*Ô :;¨¨Y­zª ;µ¶§z½gº0©¨Y©eÃ ö&quot; . ¦C§0½É¦N¨Y­zªa²4¬#µ¼¨xµ¶¬: ¾ µ¼¨Y­jµ¶§;;.¨±ª´·V¨aÂ ©N§0ºKµ¶¬ &amp;.¸q¦N²/ #;¾&amp;µ¼¨Y­ ¨±ª´·e¨aÂ&amp; ;»Ìµ¶³a©¨Yµ¼¦C§0¬Å¾8ª ¦C§0»¼¿$«0¬±ª:«;!
"Äêµ¼½C«z²Yª.  æ &apos;î&amp;&amp;;ªa² ¦N¸x¨Y­zª°³a»¶©N«0¬Yµ¼Í;&amp;¬ ¨Y­zªÅ¨Y­z²YªaªÅº0µ &amp; . ;  ²Yªa¥;@©N§;;©¨Yµ¶§z½ï³´¦C§ÿ±«0§;³´¨Yµ¼¦C§0¬aÃ ;&amp;¨Y­;©¨8¨Y­;;.&amp;    ;¬±¨Yµ¼¸q¿#;]¹ &amp;¨Y­;©¨t¨¿V¥;# ²Yªa¥; &amp;#  ( m %e    tSÏZmöÌwcfe{iriSj lYm o  &quot; ;.&amp;©.;©²Y¿ *¦N²#©³´¦C§\ÿ±«;§0³´¨Yµ¼¦C§K¨Y©½zÃ ¦ ! &quot;  ;.µ¶§0ºzª´¹ æ:\;#Ô ;# .Í;$æ /²Y ¦Õ¾&amp;¬  r #\¹ ;ºG²4µ¼½C­¨$;©²Y¨À¹¦N¸Ñ¹ *³´¦C§¨±ª´·e¨Y¬aÃ * + L B0Ah - 5B , ;DMC?1D&apos;E * ? /? ) ÔÀ§¨Y­0µ¶¬ ; ¨Y­zª: . #; ®./¸q¦N² ;;ªa² .*® ¨Y­zª:³a»Ì©N«0¬YµÐÍ;ªa²¢Ã uû½ü . / e%tke 10 {o0e%tSjf£q zÄ ¦N²7¨Y­0µÌ¬&amp;. ¨Y­zª5¤e¾&amp;; &quot; ;;&amp;.µ¶§ª  äåá*ªa¨±ªaªa²©N§0ºÚ¦N¨Y­e¹ ¬±¥;©N§ ¦¢ØNªa²K¨Y«z²4§0¬aÂ7¾8ªË³´¦C»¶»Ì©¥;µ¶¬*;;  &quot; ©N§0ºË¬Yµ¶º0&amp;ª § :¦N¸&amp;©Éº;µ¶©N»¼¦N½j©N¬x©Æ¬Yµ¶§0½C»¼ªÊ¬±¨±²4µ¶§z½&amp;*¦N¸ ¨Y©N»e¦N¸ P+  ; ¨Y©Nµ¶§0µÌ§z½*;#¦N¸ ¦C§0ª¯¥; ©²Y¨YµÌ³aµ¼¥;©N§h¨:µ¶§ .;$¨Y«z²4§ÛÎ1¦C«0§0ºe¹¬ 3 ª´·z³´ªa¥0¨ 2 ;Ã &amp;#©N§0ºK²Yªa¥;;&amp;µ¼¨Y­  ; ¨Y­0©¨j­0©Nºí³´¦C®.¥;»¼ª´·;µ¼¨*@¦N¸2¨Y­0ªË¬±¨±²4µ¶§z½C¬  ;µ¶¬# &amp;µ¼¨Y­ Û : &amp;¾ µ¼¨Y­.©¥;¾  e !"
Â H! ! @ ! ;*¬ äqª´·e³a»Ì«0º0µ¶§z½K©N§0§0¦N¨Y©¨Yµ¼¦C§0¬4ã4Ãôî&amp; ;¬ ç   ( *ä ½ eæ e !
Â H! !  ! # ;ç è V Á Â  # Ã ) »¶» &quot; /:@¨Y­0µ¶¬ ;¥  ;&amp;º0©¨Y©eÃ .   uû 65 Ë %tSjf£q 87 mwtSo0jfÌi ÜKªíªaØ©N§0ºg³´¦C®2¥­0©N \©N»¶«;;. *©½C©Nµ¶§;¬±¨&amp; © ¥1¦N§0²Y¨xºz¹²Y©N§0³a§z©N»¶¦N»Å¨Y©©N¨±§0ºËº°¥0ØN²Yªa²4³a¬Yµ¶µ¼¦Cµ¼§K¦C§@¦N¸Å¨Y¦N­z²Yª.¦C¬±§Ù¨x¬±ªa©N¨aÃ¯­ÓÜKªÊ¨Y²Y­zª´ª¹
WU  ¹! VU è è !
VU ! ©N§;º #( WU ;©¨2µ¶§0³a»¶«0º;µ¶§z½ CX ´ý ..¨Y­zª¥1ªa²Y¸q¦N²±¹ ®2©N§0³´ª:æ $ U Ã .
"Y. uû bdcfe{irikj_ Z7  ;/¨Y­zª&amp;³a»Ì ©N¬Y¬YµÐÍ;ªa²4¬Ö¾8©N¬/¥0²Yªa¥;.Îh¿ @ ;©²Y¿ô¾&amp; ¨Y©½C¬:§ Î ©N§;:¬Y­z¦Õ¾&amp;§ µ¶§î ©Î;»¼ª.æÃ ù  &quot; ;©²Y¨ ;³a»¶©N¬À¹ ;©²Y¿NÂ;\ÿY«0§0³]¹ *: ;&amp; ¦C§zª$¬±¨±ªa¥Ë©N§;#§0¦N¨# ©N¬Y¬Y«;®.¥0¨Yµ¼¦C§0¬ ©N¬&amp; uû [. bñ£Ñ) }¤ bdcfe{ikikj_lnm o î ©Î;»¼ª = K¬Y­0¦¢¾ ( &amp;¬.¨Y­0ªg¥1ªa²Y¸Ñ¦N²4®2©N§0³´ª¦N¸7©°³´¦C®: ;#;Î  &amp;³´¦C®.&amp;³´¦C®: ;ªa²: ¬ ªaC½ .® ª h§ Y¨ © Y¨ µ¼¦C§ Âëª ;º ¼µ Y¨ ¬&amp; ©N0§ º³´ ¦C§ÿ±«0;§ ´³ Y¨ µ¼C¦ 0§ a¬ &apos;Ã ¤eÌµ 0§ ³´ª ¨Y­0µ¶¬± ;#;«z¨7¦N¸ ; ,;Ã Q  &quot; ª ;  É ;;ªa²gÎ1¦N¨Y­Û ©¨É¬Yªa½C®.Î  .¥; ©²Yªg³´¦C»Ì«0®2§ 7 ©N§0º*ç.¦N¸¨Y©Î;»¶ª ñ  ( &amp;µ¼¨Y­³´¦C»Ì«0®2 $ § . ( ©N§0º $  + ª ñ  . ] \ uû prqZ¤Yj 5 j_\bdcfe{irikj_ î ©Î;¬ Q zÂ + èV Á Â ¬Y­z¦Õ¾ì¨Y­zª#;³´ª# ; ³a©N¬Y³a©Nº0ª&amp;¬Y­z¦Õ¾&amp;§. ;#©²Yª ¾&amp;.&amp;§Úµ¶§0¥; :&amp;§Éµ¶§ ý î ©;Î »¼ª2úhÃî&amp;: . µ¶§z½µ¶§;;¬¾&amp;;µ¼½C­zªa² @;. ³´¦C»¶«0®¯§¦N¸ #úx¾&amp; ¨Y©Î;»¼ª ñ Cã4Ã (  ;B D&apos;\M &apos;c /1?  ;  c \D b ##ª &amp; $. ¬Y­z¦Õ¾&amp;¬.Â / ® º0µ¼¨ê©N§0º2ñ8¦C§\ÿY«0§0³´¨Yµ¶¦C§x³a»Ì ©N¬Y¬YµÐÍ;ªa²;.ªa²4¬aÃî«0¬Yµ¶§z½j¦C§0»¶¿©Î;»¼ª Ä ©N§0º :¤°¨Y©½C¬¯©N§0ºÙ¾&apos;¦N²4º0¬a;»¼¿°¥;Â ä: dX  :¨Y©½C¬aÂ&apos;¾&apos;¦N²4º0¬ &amp; e  X . aX ´ý @Ô .¨ µ¶¬. ¾&amp;«0¬YµÌ§z½#¦C§;»¼¿ \ÿ±«;; f ´ý X ;¬aÃ &gt;  &quot; ;º0µ¶§z½  .²4©¨±ªg³´¦C®.;¥ ;¬Yµ¶§z½ ¦C§0»¼¿#.®  "
X .;³´ª8¦N¸ ;ªa²4¬aÃ &amp;î ­0µ¶¬êµ¶¬   ª g ´ý X * @*§z¦N¨ &amp; ³´¦C§h¨±ª´·V¨Y«0©N»: &amp; ä   º N¦C¬Y­;µùÂ# ¨Y«z²Yª»¼¦C§z½¹º0µÌ¬±¨Y©N§0³´ª&amp;ª / .&amp;. ¨Y­zª:¬Yªa½C®.
Ô ¨&amp;&amp;#¨±¦Ê§z¦N¨±ª#¨Y­0©¨&amp;¨Y­zª# ;;¬* /§z¦N¨8³´¦C§¨±²4µ¼Î¬ ¤ ;¿ VÃ è #ï©N¬8³´¦C®¨Y­0ª&amp;.¥; hiX § ý  ;»¼¿ % j Þø VÃ ( ç W 2ã4Ã U 8¿ %
Þí( ¬Yªa¥ W ; U  äqªa½zÃÊ«/®..ªÕã4Â0©N§;; »¼ªa¸q¨7¦C§0»¼¿É¾;&amp;&amp;:;/ ® º0µ¼¨¨Y©½C¬aÂ ¾8ª¯©²Yª ¨Y­zª*@$;&amp;§Úµ¶§Ó¨Y­0ª*¸q¦C«z²Y¨Y­ ³´¦C»¶«0®¯§*¦N¸8î©Î¸ &amp;;»¶ª VÃ ø  É ©N§0º.;:¥1ªa²±¹ ¨Y­0©N§¨Y­0©¨Å¦N Z ¸ n / ® º0µ¼¨Y¬8µ¶§0º;µ¶³a©¨Yµ¶§z½ ì #¨Y­0©¨ µ¼¨   ¨Y©N¬±Ò@½ = / ® º0µ¶¨Y¬aÃKÔ ;§  ¨Yµ¼¸q¿VµÌ§z½:¿ ì C ( ú þ¦¢ØNªa²&amp;.&amp;­0µ¶³4­Ê«;
"In this paper, we propose a method for learning a classifier which combines out-puts of more than one Japanese named entity extractors."
"The proposed combi-nation method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of sev-eral classifiers at the first stage by learn-ing a second stage classifier to combine those outputs at the first stage."
"Individ-ual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other consid-ers those of variable lengths according to the number of constituent morphemes of named entities."
"As an algorithm for learn-ing the second stage classifier, we employ a decision list learning method."
Experi-mental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum en-tropy models.
"In the recent corpus-based NLP research, sys-tem combination techniques have been successfully applied to several tasks such as parts-of-speech tagging (van[REF_CITE]), base noun phrase chunking (Tjong[REF_CITE]), and pars-ing[REF_CITE]."
The aim of system combination is to combine portions of the individual systems’ outputs which are partial but can be regarded as highly ac-curate.
"The process of system combination can be decomposed into the following two sub-processes: 1. Collect systems which behave as differently as possible: it would help a lot if at least the col-lected systems tend to make errors of differ-ent types, because simple voting technique can identify correct outputs."
Previously studied techniques for collecting
"In this paper, we propose a method for combining outputs of (Japanese) named entity chunkers, which belongs to the family of stacking techniques."
"In the sub-process 1, we focus on models which dif-fer in the lengths of preceding/subsequent contexts to be incorporated in the models."
"As the base model for supervised learning of Japanese named entity chunking, we employ a model based on the maxi-mum entropy model[REF_CITE], which performed the best in IREX (Information Retrieval and Extraction Exercise) Workshop (IREX[REF_CITE]) among those based on machine learning techniques."
"In this paper, we train several maximum entropy models which differ in the lengths of preceding/subsequent contexts, and then combine their outputs."
"As the sub-process 2, we propose to apply a stack-ing technique which learns a classifier for com-bining outputs of several named entity chunkers."
This second stage classifier learns rules for accept-ing/rejecting outputs of several individual named en-tity chunkers.
"The proposed method can be applied to the cases where the number of constituent systems is quite small (e.g., two)."
"Actually, in the experimen-tal evaluation, we show that the results of combining the best performing model[REF_CITE]with the one which performs poorly but extracts named entities quite different from those of the best performing model can help improve the perfor-mance of the best model."
The task of named entity recognition of the IREX workshop is to recognize eight named entity types in Table 1[REF_CITE].
"The organizer of the IREX workshop provided 1,174 newspaper articles which include 18,677 named entities as the training data."
"In the formal run (general domain) of the workshop, the participating systems were re-quested to recognize 1,510 named entities included in the held-out 71 newspaper articles."
We first provide our definition of the task of Japanese named entity chunking[REF_CITE].
Suppose that a sequence of morphemes is given as below: ( ContextLeft ) ( ContextRight ) (Named Entity) · ·· M −kL ·· · M −1L M 1NE · ·· M NEi · ·· M mNE M R1 ·· ·M lR · ·· ↑ (Current Position)
"Given that the current position is at the morpheme M iNE , the task of named entity chunking is to assign a chunking state (to be described in section 2.3.1) to the morpheme M iNE at the current position, consid-ering the patterns of surrounding morphemes."
"Note that in the supervised learning phase, we can use the chunking information on which morphemes consti-tute a named entity, and which morphemes are in the left/right contexts of the named entity."
"In the maximum entropy model[REF_CITE], the conditional probability of the output y given the context x can be estimated as the follow-ing p λ (y | x) of the form of the exponential family, where binary-valued indicator functions called fea-ture functions f i (x, y) are introduced for expressing a set of “features”, or “attributes” of the context x and the output y."
"A parameter λ i is introduced for each feature f i , and is estimated from a training data.  exp λ i f i (x, y) i p λ (y | x) = exp λ i f i (x, y) y i"
"This amounts to 9×4 = 36 classes. • Three more classes are distinguished for mor-phemes immediately preceding/following a named entity, as well as the one between two named entities. • Other morphemes are assigned the class “OTHER”."
"More specifically, the following three types of feature functions are used: 1 [Footnote_1]. 2052 lexical items that are observed five times or more within two morphemes from named entities in the training corpus. [Footnote_2]. parts-of-speech tags of morphemes 2 . 3. character types of morphemes (i.e., Japanese (hiragana or katakana), Chinese (kanji), num-bers, English alphabets, symbols, and their combinations)."
"1 Minor modifications from those[REF_CITE]are: i) we used character types of morphemes because they are known to be useful in the Japanese named entity chunking, and ii) the sets of parts-of-speech tags are different."
"2 As a Japanese morphological analyzer, we used BREAK - FAST[REF_CITE]with the set of about 300 part-of-speech tags.[REF_CITE].6% part-of-speech accu-racy against newspaper articles."
"As for the number of preceding/subsequent mor-phemes as contextual clues, we consider the follow-ing models:"
"This model considers the preceding two mor-phemes M −2 , M −1 as well as the subsequent two morphemes M 1 , M 2 as the contextual clue."
"Both[REF_CITE]and in this paper, this is the model which performs the best among all the indi-vidual models without system combination. ( ContextLeft ) ( PositionCurrent ) ( ContextRight ) · · · M −2 M −1 M 0 M 1 M 2 · · ·"
The following gives the training and test data sets for our framework of learning to combine outputs of named entity chunkers. 1.
TrI: training data set for learning individual named entity chunkers. 2.
TrC: training data set for learning a classifier for combining outputs of individual named en-tity chunkers. [Footnote_3]. T s: test data set for evaluating the classifier for combining outputs of individual named entity chunkers.
"3 Note that, as opposed to the training phase, the length of preceding/subsequent contexts is fixed in the testing phase of this model. Although this discrepancy between training and testing damages the performance of this single model (sec-tion 4.1), it is more important to note that this model tends to have distribution of correct/over-generated named entities dif-ferent from that of the 5-gram model. In section 4, we exper-imentally show that this difference is the key to improving the named entity chunking performance by system combination."
"The following gives the procedure for learning the classifier to combine outputs of named entity chun-kers using T rI and T rC. 1. Train the individual named entity chunkers NEchk i (i = 1, . . ., n) using T rI. 2. Apply the individual named entity chunkers NEchk i (i = 1, . . ., n) to T rC, respectively, and obtain the list of chunked named entities NEList i (TrC) for each named entity chun-ker NEchk i ."
"TrC, and obtain the event expression TrCev of T rC. 4. Train the classifier NEchk cmb for combining outputs of individual named entity chunkers us-ing the event expression T rCev."
"The following gives the procedure for applying the learned classifier to T s. 1. Apply the individual named entity chunkers NEchk i (i = 1, . . ., n) to Ts, respectively, and obtain the list of chunked named entities NEList i (Ts) for each named entity chunker NEchk i . 2. Align the lists NEList i (T s) (i = 1, . . ., n) of chunked named entities according to the posi-tions of the chunked named entities in the text Ts, and obtain the event expression Tsev of T s."
"The event expression T rCev of T rC is obtained by aligning the lists NEList i (T rC) (i = 1, . . ., n) of chunked named entities, and is represented as a sequence of segments, where each segment is a set of aligned named entities."
Chunked named enti-ties are aligned under the constraint that those which share at least one constituent morpheme have to be aligned into the same segment.
"Examples of seg-ments, into which named entities chunked by two systems are aligned, are shown in Table 2."
"In the first segment SegEv i , given the sequence of the two morphemes, the system No.0 decided to extract two named entities, while the system No.1 chunked the two morphemes into one named entity."
"In those event expressions, systems indicates the list of the indices of the systems which output the named en-tity, mlength gives the number of the constituent morphemes, NEtag gives one of the nine named entity types, POS gives the list of parts-of-speech of the constituent morphemes, and class NE indi-cates whether the named entity is a correct one com-pared against the gold standard (“+”), or the one over-generated by the systems (“−”)."
"In the second segment SegEv i+1 , only the sys-tem No.1 decided to extract a named entity from the sequence of the three morphemes."
"In this case, the event expression for the system No.0 is the one which indicates that no named entity is extracted by the system No.0."
"In the training phase, each segment SegEv j of event expression constitutes a minimal unit of an event, from which features for learning the classi-fier are extracted."
"In the testing phase, the classes of each system’s outputs are predicted against each segment SegEv j ."
"In principle, features for learning the classifier for combining outputs of named entity chunkers are rep-resented as a set of pairs of the system indices list , . . .,  and a feature expression F of the named entity: f = =, . . . , , F ··· = , . . . , q , F (2)"
"In the training phase, any possible feature of this form is extracted from each segment SegEv j of event expression."
"The system indices list , . . .,  indicates the list of the systems which output the named entity."
"A feature expression F of the named entity can be any possible subset of the full feature expression {mlength = · · · , NEtag = · · · , POS = · · ·}, or the set indicating that the system outputs no named entity within the segment.   any subset of  mlength=· · · , F =  classNEtag=· · · , POS =· · · sys =“no outputs”"
"In the training and testing phases, within each segment SegEv j of event expression, a class is as-signed to each system, where each class class isys for the i-th system is represented as a list of the classes of the named entities output by the system: class isys = +/−, . . . , +/− (i = 1, . . . , n)“no output”"
We apply a simple decision list learning method to the task of learning a classifier for combining outputs of named entity chunkers [Footnote_4] .
4 It is quite straightforward to apply any other supervised learning algorithms to this task.
"A decision list[REF_CITE]is a sorted list of decision rules, each of which decides the value of class given some features f of an event."
"Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when ap-plying the decision list to some new test data."
"In this paper, we simply sort the decision list according to the conditional probability P(class i | f) of the class i of the i-th system’s output given a feature f."
We experimentally evaluate the performance of the proposed system combination method using the IREX workshop’s training and test data.
"First, Table 3 shows the performance of the indi-vidual models described in the section 2.3.2, where trained with the IREX workshop’s training data, and tested against the IREX workshop’s test data as T s."
The 5-gram model performs the best among those individual models.
"Next, assuming that each of the models other than the 5-gram model is combined with the 5-gram model, Table 4 compares the named entities of their outputs."
"Recall rate of the correct named entities in the union of their outputs, as well as the overlap rate [Footnote_5] of the over-generated entities against those included in the output of the 5-gram model are shown."
"5 For a model X, the overlap rate of the over-generated enti-ties against those included in the output of the 5-gram model is defined as: (# of the intersection of the over-generated entities output by the 5-gram model and those output by the model X)/ (# of the over-generated entities output by the 5-gram model)."
"From the Tables 3 and 4, it is clear that the 7-gram and 9-gram models are quite similar to the 5-gram model both in the performance and in the distribu-tion of correct/over-generated named entities."
"On the other hand, variable length models have distri-bution of correct/over-generated named entities a lit- tle different from that of the 5-gram model."
Vari-able length models have lower performance mainly because of the difference between the training and testing phases with respect to the modeling of con-text lengths.
"Especially, the variable length model with “all” features of M {−4,−3,3,4} has much lower performance as well as significantly different dis-tribution of correct/over-generated named entities."
"This is because character types features are so gen-eral that many (erroneous) named entities are over-generated, while sometimes they contribute to find-ing named entities that are never detected by any of the other models."
"This section reports the results of combining the out-put of the 5-gram model with that of 7-gram models, 9-gram models, and the variable length models."
"As the training data sets T rI and T rC, we evaluate the following two assignments (a) and (b), where D CRL denotes the IREX workshop’s training data: (a) TrI:"
D CRL − D[REF_CITE](200 articles from D CRL )
D[REF_CITE](b) TrI =
TrC = D CRL
We use the IREX workshop’s test data for Ts.
"In the assignment (a), TrI and TrC are disjoint, while in the assignment (b), individual named entity chunkers are applied to their own training data, i.e., closed data."
The assignment (b) is for the sake of avoiding data sparseness in learning the classifier for combining outputs of two named entity chunkers.
Table 5 shows the peformance in F-measure (β = 1) for both assignments (a) and (b).
"For both (a) and (b), “5-gram + variable length (All)” significantly outperforms the 5-gram model, which is the best model among all the individual models without sys-tem combination."
It is remarkable that models which perform poorly but extract named entities quite dif-ferent from those of the best performing model can actually help improve the best model by the pro-posed method.
The performance for the assignment (b) is better than that for the assignment (a).
This re-sult claims that the training data size should be larger when learning the classifier for combining outputs of two named entity chunkers.
"In the Table 6, for the best performing result (i.e., 5-gram + variable length (All)) as well as the con-stituent individual models (5-gram model and vari-able length model (All)), we classify the system output according to the number of constituent mor-phemes of each named entity."
"In the Table 7, we classify the system output according to the named entity types."
The following summarizes several re-markable points of these results: i) the benefit of the system combination is more in the improvement of precision rather than in that of recall.
"This means that the proposed system combination technique is useful for detecting over-generation of named en-tity chunkers, ii) the combined outputs of the 5-gram model and the variable length model improve the re-sults of chunking longer named entities quite well compared with shorter named entities."
This is the effect of the variable length features of the variable length model.
This paper proposed a method for learning a classi-fier to combine outputs of more than one Japanese named entity chunkers.
"Experimental evaluation showed that the proposed method achieved improve-ment in F-measure over the best known results with an ME model[REF_CITE], when a com-plementary model extracted named entities quite dif-ferently from the best performing model."
"0p¢«|¢¡`¬=§I­G«\, ¨l¥=[­¹¤,­¹¾[ [­¹£½0º¦[[«¿Àm¥G¤l°UÁr«¿¥=¢¡`¬ [|«¿µ«ª¨l¥G¤&apos;Æ Á +­G¡[¦Ç«¿µ­¹¤,¿¥=¡I´½¥GÀ8§I¡I´&apos;³µ¡MÉÊ¥G¤,¦I´ =ÂË¨l¥=[[ ¥G¤,¢¡`¬.!É . ¾ [¢«|¢¡`¬=§I­G«=&quot;[¦ ¿¥=[[§ [­¹£.[¢ÎGµ¦h­G§`£&apos;¥_£&apos;µ¡=[ÂÏ­¹£JÆ¨lµ´³¸ ×G×­G¡I¦Ý«¿µ­¹¤ÄØGÙGÚÁ[=[[§I´0¥GÀ× ¸ £&apos;µ¡I¨lµ´¨l¥=[­¹¤,,· ,Ä ­¹¾,[«¿\[§I´Ê¥GÀ9Þ¹Ä ×¹ß [\´ ¶JÞ³ÑGÑnÄÒÑGÑGÑÏ´&apos;µ¡nÆ¿¥==Â ­ ãà á KPDG_;=?^UD=XYAK? ¢¨³­G«&quot;Â±­G¨,¼I[ ¿¥=¡¾[=! × · ¢µ´&quot;¼`µ­µÎn¢«¿Ì.¥=|´&apos;£&apos;µ¡I¨l ¥GÀnÁ[|  Àu¥G¤.|¿¥=[¿¥=  .æ  ¢«èÆ ­¹¾[«¿¥=¡I«¿Ìº¢¡é«¢¢ÂÏ==¡I«¢Ì ´&apos;Á µ¨³ ã ­G¡[­G¦I¢­G¡Á ¥=|« =¡`¬0íÍ¥=¡`¬ «¢­êÉ!´³¸[,=¡`¥=|« ¢¡`¬=!¢¡ ¼I¿¬=|¡3ÂÏ­G¡_=Â±­G¢¡[´E­G¡ [ =¬ §I­¹¬Gµ´³Ä ß [=  µ¡`lë[\ë[µ«|"
",Az))(u8*F* u}FL (u¡¢L1*g¤£¥*(,1AS_&apos;L\uM8*]LaA*a(.( ¦ L(u ({Lga¬ASL, !guAA]@OLz ©`ª«** S*z(*zL. ! gu8z`z}( *. ]u ® &gt;**u©2,g u**z\S*a¯L*g\ ! gu±z¤S* a¬AQ¨,**\zz(*(QLF².³µ´L¶S·¸\Lg **L,*SLLu A¨,( )¤]L*(*L ( {! {.]uA¬A*&gt; {u­¨,ºL}u * A*a¬ga¤*gMu(*z\S**z @ m !! * { Lz½¾*g¿]a (\.guG©u{³m¨,ga*¹@ (*\¬A(u¯(»{A¼ * AÁuA *u]LaAz*a¿]Â! u}*   * )L*(u ³ a¯¦A*(,Ã( ]*(uÁ¦}c^( ) ¯*u]LaA  zaZ(! ( (u^©*"
There has been much interest in us-ing phrasal movement to improve statis-tical machine translation.
"We explore how well phrases cohere across two lan-guages, specifically English and French, and examine the particular conditions un-der which they do not."
"We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system."
We also compare three variant syntactic representations to deter-mine which one has the best properties with respect to cohesion.
Statistical machine translation (SMT) seeks to de-velop mathematical models of the translation pro-cess whose parameters can be automatically esti-mated from a parallel corpus.
"The first work in SMT, done at IBM[REF_CITE], developed a noisy-channel model, factoring the translation pro-cess into two portions: the translation model and the language model."
The translation model captures the translation of source language words into the target language and the reordering of those words.
The language model ranks the outputs of the translation model by how well they adhere to the syntactic con-straints of the target language. [Footnote_1]
1 Though usually a simple word n-gram model is used for the language model.
The prime deficiency of the IBM model is the re-ordering component.
"Even in the most complex of the five IBM models, the reordering operation pays little attention to context and none at all to higher-level syntactic structures."
Many attempts have been made to remedy this by incorporating syntactic in-formation into translation models.
"These have taken several different forms, but all share the basic as-sumption that phrases in one language tend to stay together (i.e. cohere) during translation and thus the word-reordering operation can move entire phrases, rather than moving each word independently.[REF_CITE]states that during their work on noun phrase bracketing they found a strong cohesion among noun phrases, even when compar-ing English to Czech, a relatively free word or-der language."
"Other than this, there is little in the SMT literature to validate the coherence assump-tion."
Several studies have reported alignment or translation performance for syntactically augmented translation models[REF_CITE]and these results have been promis-ing.
"However, without a focused study of the be-havior of phrases across languages, we cannot know how far these models can take us and what specific pitfalls they face."
The particulars of cohesion will clearly depend upon the pair of languages being compared.
"Intu-itively, we expect that while French and Spanish will have a high degree of cohesion, French and Japanese may not."
"It is also clear that if the cohesion between two closely related languages is not high enough to be useful, then there is no hope for these meth-ods when applied to distantly related languages."
"For this reason, we have examined phrasal cohesion for French and English, two languages which are fairly close syntactically but have enough differences to be interesting."
"An alignment is a mapping between the words in a string in one language and the translations of those words in a string  in another  language  ,. andGivena Frenchan En-glish string  ,  be represented by  , an alignment  ."
"Each % &apos;(*&quot; )+% a  canis string, of indices into where ! #&quot; $&amp;% a , set .% -indicates that , word in the French sentence 0/ is aligned with word in the English , sentence.indicates that English word has no corresponding French word."
"Given an alignment   , theandspanan Englishis a pairphrasewherecov-the first element is @2+8 35476 8 &gt;81 : 8 1;: ering words ement is 2+&gt;&lt; ?= 6 and the second el- ."
"Thus, the span includes all words between the two extrema of the alignment, whether or not they too are part of the translation."
"If phrases cohere perfectly across languages, the span of one phrase will never overlap the span of another."
"If two spans do overlap, we call this a crossing."
Figure 1 shows an example of an English parse along with the alignment between the English and French words (shown with dotted lines).
"The En-glish word “not” is aligned to the two French words “ne” and “pas” and thus has a span of [1,3]."
"The main English verb “change” is aligned to the French “modifie” and has a span of [2,2]."
The two spans overlap and thus there is a crossing.
This definition is asymmetric (i.e. what is a crossing when mov-ing from English to French is not guaranteed to be a crossing when moving from French to English).
"However, we only pursue translation direction since that is the one for which we have parsed data."
"To calculate spans, we need aligned pairs of En-glish and French sentences along with parses for the English sentences."
Our aligned data comes from a corpus described[REF_CITE]which contains 500 sentence pairs randomly selected from the Canadian Hansard corpus and manually aligned.
The alignments are of two types: sure (S) and pos-sible (P).
S alignments are those which are unam- biguous while P alignments are those which are less certain.
"P alignments often appear when a phrase in one language translates as a unit into a phrase in the other language (e.g. idioms, free translations, miss-ing function words) but can also be the result of gen-uine ambiguity."
"When two annotators disagree, the union of the P alignments produced by each anno-tator is recorded as the P alignment in the corpus. exist a P alignment such that P A S."
The English sen-
"When an S alignment exists, there will always also tences were parsed using a state-of-the-art statistical parser[REF_CITE]trained on the University of Pennsylvania Treebank[REF_CITE]."
"Since P alignments often align phrasal transla- tions, the number of crossings when P alignments are used will be artificially inflated."
"For example, in Figure 2 note that every pair of English and French words under the verb phrase is aligned."
"This will generate five crossings, one each between the pairs VBP-PP, IN-NP , NP -PP, NN-DT, and IN-NP ."
"However, what is really happening is that the whole verb phrase is first being moved without crossing anything else and then being translated as a unit."
For our purposes we want to count this example as pro-ducing zero crossings.
"To accomplish this, we de-fined a simple heuristic to detect phrasal translations so we can filter them if desired."
"After calculating the French spans from the English parses and alignment information, we counted cross-ings for all pairs of child constituents in each con-stituent in the sentence, maintaining separate counts for those involving the head constituent of the phrase and for crossings involving modifiers only."
We did this while varying conditions along two axes: align-ment type and phrasal translation filtering.
"Recalling the two different types of alignments, S and P, we examined three different conditions: S alignments only, P alignments only, or S alignments where present falling back to P alignments (S P)."
"For each of these conditions, we counted crossings both with and without using the phrasal  translationS, S P,Pfilter, let.  "
For 6 8 a given :  alignment $ typeand :  cross $ if theeachphrasalother and ) otherwise.ifLetphrases 6 8 translation filter is turned off.
"If the filter is on, ) 6  if and : are part of a phrasal translation $ in alignment otherwise"
"Then, for a given phrase with head constituent , modifier  constituentsand for a particular, and child  constituents alignment type , the number of head crossings and modifier crossings can be calculated recursively: 6 :  6 : &quot;! 6 : &quot;*$ !) +(% $&amp;&apos;-%(,#.$&apos; 0) /1 $ 6  &apos; 8 &apos; : 6 6 &apos;  8 &apos; 8: &apos; :  6 8 &apos; : 6 :"
Table 1 shows the average number of crossings per sentence.
"The table is split into two sections, one for results when the phrasal filter was used and one for when it was not. “Alignment Type” refers to whether we used S, P or S P as the alignment data."
"The “Head Crossings” line shows the results when comparing the span of the head constituent of a phrase with the spans of its modifier constituents, and “Modifier Crossings” refers to the case where we compare the spans of pairs of modifiers."
The “Phrasal Translations” line shows the average num-ber of phrasal translations detected per sentence.
"For S alignments, the results are quite promising, with an average of only 0.236 head crossings per sentence and an even smaller average for modifier crossings (0.056)."
"However, these results are overly optimistic since often many words in a sentence will not have an S alignment at all, such as “coming”, “in”, and “before” in following example: the full report will be coming in before the fall le rapport complet sera déposé de ici le automne prochain"
"When we use P alignments for these unaligned words (the S P case), we get a more meaningful result."
Both types of crossings are much more fre-quent (4.790 for heads and 0.88 for modifiers) and phrasal translation filtering has a much larger effect (reducing head average to 2.772 and modifier aver-age to 0.516).
"Phrasal translations account for al-most half of all crossings, on average."
This effect is even more pronounced in the case where we use P alignments only.
This reinforces the importance of phrasal translation in the development of any trans-lation system.
"Even after filtering, the number of crossings in the S P case is quite large."
"This is discouraging, however there are reasons why this result should be looked on as more of an upper bound than anything precise."
"For one thing, there are cases of phrasal translation which our heuristic fails to recognize, an example of which is shown in Figure 3."
"The align-ment of “explorer” with “this” and “matter” seems to indicate that the intention of the annotator was to align the phrase “work this matter out”, as a unit, to “de explorer la question”."
"However, possibly due to an error during the coding of the alignment, “work” and “out” align with “de” (indicated by the solid lines) while “this” and “matter” do not."
This causes the phrasal translation heuristic to fail resulting in a crossing where there should be none.
"Also, due to the annotation guidelines, P align-ments are not as consistent as would be ideal."
"Re-call that in cases of annotator disagreement, the P alignment is taken to be the union of the P align-ments of both annotators."
"Thus, it is possible for the P alignment to contain two mutually conflict-ing alignments."
These composite alignments will likely generate crossings even where the alignments of each individual annotator would not.
"While re-flecting genuine ambiguity, an SMT system would likely pursue only one of the alternatives and only a portion of the crossings would come into play."
Our results show a significantly larger number of head crossings than modifier crossings.
"One possi-bility is that this is due to most phrases having a head and modifier pair to test, while many do not have multiple modifiers and therefore there are fewer op-portunities for modifier crossings."
"Thus, it is infor-mative to examine how many potential crossings ac-tually turn out to be crossings."
Table 2 provides this result in the form of the percentage of crossing tests which result in detection of a crossing.
"To calculate this, we kept totals for the number of head ( ) and modifier ( ) crossing tests per-formed as well as the number of phrasal translations detected (  )."
"Note that when the phrasal transla-tion filter is turned on, these totals differ for each of the different alignment types (S, S P, and P)."
The percentages are calculated after summing over all sentences in the corpus:  &quot ;! $$ #$  $ ) )    $  ) )  $ ) ) &amp;% #$#$
There are still many more crossings in the S P and P alignments than in the S alignments.
"The S alignment has 1.58% head crossings while the S P and P alignments have 32.16% and 35.47% respec-tively, with similar relative percentages for modi-fier crossings."
"Also as before, half to two-thirds of crossings in the S P and P alignments are due to phrasal translations."
"More interestingly, we see that modifier crossings remain significantly less preva-lent than head crossings (e.g. 14.45% vs. 32.16% for the S P case) and that this is true uniformly across all parameter settings."
This indicates that heads are more intimately involved with their modifiers than modifiers are with each other and therefore are more likely to be involved in semi-phrasal constructions.
"Since it is clear that crossings are too prevalent to ignore, it is informative to try to understand exactly what constructions give rise to them."
"To that end, we examined by hand all of the head crossings produced using the S alignments with phrasal filtering."
Table 3 shows the results of this analysis.
The first thing to note is that by far most of the crossings do not reflect lack of phrasal cohe-sion between the two languages.
"Instead, they are caused either by errors in the syntactic analysis or the fact that translation as done by humans is a much richer process than just replication of the source sen-tence in another language."
"Sentences are reworded, clauses are reordered, and sometimes human trans-lators even make mistakes."
Errors in syntactic analysis consist mostly of at-tachment errors.
Rewording and reordering ac-counted for a large number of crossings as well.
In most of the cases of rewording (see Figure 4) or re- ordering (see Figure 5) a more “parallel” translation would also be valid.
"Thus, while it would be difficult for a statistical model to learn from these examples, there is nothing to preclude production of a valid translation from a system using phrasal movement in the reordering phase."
The rewording and reorder-ing examples were so varied that we were unable to find any regularities which might be exploited by a translation model.
"Among the cases which do result from language differences, the most common is the “ne ...pas” construction (e.g. Figure 1)."
Fifteen percent of the 86 total crossings are due to this construction.
"Be-cause “ne . . . pas” wraps around the verb, it will al-ways result in a crossing."
"However, the types of syn-tactic structures (categorized as context-free gram-mar rules) which are present in cases of negation are rather restricted."
"In addition, the crossings associated with these particular structures were un-ambiguously caused by negation (i.e. for each struc-ture, only negation-related crossings were present)."
Next most common is the case where the En-glish contains a modal verb which is aligned with the main verb in the French.
"In the example in Fig-ure 6, “will be” is aligned to “sera” (indicated by the solid lines) and because of the constituent struc-ture of the English parse there is a crossing."
"As with negation, this type of crossing is quite regular, re-sulting uniquely from only two different syntactic structures."
"Adverbs are a third common cause, as they typ-ically follow the verb in French while preceding it in English."
Figure 7 shows an example where the span of “simplement” overlaps with the span of the verb phrase beginning with “tells” (indicated by the solid lines).
"Unlike negation and modals, this case is far less regular."
It arises from six different syntac-tic constructions and two of those constructions are implicated in other types of crossings as well.
Many of the causes listed above are related to verb phrases.
"In particular, some of the adverb-related crossings (e.g. Figure 1) and all of the modal-related crossings (e.g. Figure 6) are artifacts of the nested verb phrase structure of our parser."
This nesting usu-ally does not provide any extra information beyond what could be gleaned from word order.
"Therefore, we surmised that flattening verb phrases would elim-inate some types of crossings without reducing the utility of the parse."
The flattening operation consists of identifying all nested verb phrases and splicing the children of the nested phrase into the parent phrase in its place.
This procedure is applied recursively until there are no nested verb phrases.
An example is shown in Fig-ure 8.
Crossings can be calculated as before.
Flattening reduces the number of potential head crossings while increasing the number of potential modifier crossings.
"Therefore, we would expect to see a comparable change to the number of cross-ings measured, and this is exactly what we find, as shown in Tables 4 and 5."
"For example, for S P alignments, the average number of head crossings decreases from 2.772 to 2.252, while the average number of modifier crossings increases from 0.516 to 0.86."
We see similar behavior when we look at the percentage of crossings per chance (Tables 6 and 7).
"For the same alignment type, the percentage of head crossings decreases from 18.61% to 15.12%, while the percentage of modifier crossings increases from 8.47% to 10.59%."
"One thing to note, however, is that the total number of crossings of both types detected in the corpus decreases as compared to the baseline, and thus the benefits to head crossings outweigh the detriments to modifier crossings."
"Our intuitions about the cohesion of syntactic struc-tures follow from the notion that translation, as a meaning-preserving operation, preserves the depen-dencies between words, and that syntactic structures encode these dependencies."
"Therefore, dependency structures should cohere as well as, or better than, their corresponding syntactic structures."
"To exam-ine the validity of this, we extracted dependency structures from the parse trees (with flattened verb phrases) and calculated crossings for them."
Figure 9 shows a parse tree and its corresponding dependency structure.
The procedure for counting modifier crossings in a dependency structure is identical to the procedure for parse trees.
"For head crossings, the only differ-ence is that rather than comparing spans of two sib-lings, we compare the spans of a child and its parent."
"Again focusing on the S P alignment case, we see that the average number of head crossings (see Table 4) continues to decrease compared to the pre-vious case (from 2.252 to 1.88), and that the aver-age number of modifier crossings (see Table 5) con-tinues to increase (from 0.86 to 1.498)."
"This time, however, the percentages for both types of crossings (see Tables 6 and 7) decrease relative to the case of flattened verb phrases (from 15.12% to 12.62% for heads and from 10.59% to 9.22% for modifiers)."
The percentage of modifier crossings is still higher than in the base case (9.22% vs. 8.47%).
"Overall, however, the dependency representation has the best cohesion properties."
"We have examined the issue of phrasal cohesion be-tween English and French and discovered that while there is less cohesion than we might desire, there is still a large amount of regularity in the constructions where breakdowns occur."
This reassures us that re-ordering words by phrasal movement is a reasonable strategy.
"Many of the initially daunting number of crossings were due to non-linguistic reasons, such as rewording during translation or errors in syntactic analysis."
"Among the rest, there are a small number of syntactic constructions which result in the major-ity of the crossings examined in our analysis."
One practical result of this skewed distribution is that one could hope to discover the major problem ar-eas for a new language pair by manually aligning a small number of sentences.
"This information could be used to filter a training corpus to remove sen-tences which would cause problems in training the translation model, or for identifying areas to focus on when working to improve the model itself."
We are interested in examining different language pairs as the opportunity arises.
"We have also examined the differences in cohe-sion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures."
Our results indicate that the highest degree of co-hesion is present in dependency structures.
"There-fore, in an SMT system which is using some type of phrasal movement during reordering, dependency structures should produce better results than raw parse trees."
"In the future, we plan to explore this hypothesis in an actual translation system."
The work reported here was supported in part by the Defense Advanced Research Projects Agency under contract number[REF_CITE]-00-C-8008.
"The views and conclusions contained in this document are those of the author and should not be interpreted as neces-sarily representing the official policies, either ex-pressed or implied, of the Defense Advanced Re-search Projects Agency or the United States Gov- ernment."
We would like to thank Franz Och for providing us with the manually annotated data used in these experiments.
We report on experiments in reference res-olution using a decision tree approach.
"We started with a standard feature set used in previous work, which led to moderate re-sults."
"A closer examination of the perfor-mance of the features for different forms of anaphoric expressions showed good re-sults for pronouns, moderate results for proper names, and poor results for definite noun phrases."
"We then included a cheap, language and domain independent feature based on the minimum edit distance be-tween strings."
"This feature yielded a sig-nificant improvement for data sets consist-ing of definite noun phrases and proper names, respectively."
When applied to the whole data set the feature produced a smaller but still significant improvement.
For the automatic understanding of written or spo-ken natural language it is crucial to be able to iden-tify the entities referred to by referring expressions.
The most common and thus most important types of referring expressions are pronouns and definite noun phrases (NPs).
Supervised machine learning algorithms have been used for pronoun resoluti[REF_CITE]and for the resolution of definite NPs[REF_CITE].
An unsupervised ap-proach to the resolution of definite NPs was applied[REF_CITE].
"However, though machine learning algorithms may deduce to make best use of a given set of features for a given prob-lem, it is a linguistic question and a non-trivial task to identify a set of features which describe the data sufficiently."
"We report on experiments in the resolution of anaphoric expressions in general, including definite noun phrases, proper names, and personal, posses-sive and demonstrative pronouns."
Based on the work mentioned above we started with a feature set including NP-level and coreference-level features.
Applied to the whole data set these features led only to moderate results.
"Since the NP form of the anaphor (i.e., whether the anaphoric expression is realized as pronoun, definite NP or proper name) ap-peared to be the most important feature, we divided the data set into several subsets based on the NP form of the anaphor."
This led to the insight that the moderate performance of our system was caused by the low performance for definite NPs.
"We adopted a new feature based on the minimum edit distance[REF_CITE]between anaphor and antecedent, which led to a significant improvement on definite NPs and proper names."
When applied to the whole data set the feature yielded a smaller but still significant improvement.
"In this paper, we first discuss features that have been found to be relevant for the task of reference resolution (Section 2)."
"Then we describe our cor-pus, the corpus annotation, and the way we prepared the data for use with a binary machine learning clas-sifier (Section 3)."
In Section 4 we first describe the feature set used initially and the results it produced.
We then introduce the minimum edit distance fea-ture and give the results it yielded on different data sets.
"Driven by the necessity to provide robust systems for the MUC system evaluations, researchers began to look for those features which were particular im-portant for the task of reference resolution."
"While most features for pronoun resolution have been de-scribed in the literature for decades, researchers only recently began to look for robust and cheap features, i.e., features which perform well over several do-mains and can be annotated (semi-) automatically."
"In the following, we describe a few earlier contri-butions to reference resolution with respect to the features used."
"Decision tree algorithms were used for ref-erence resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al. (2001, C5.0)."
"This approach requires the definition of a set of features describing pairs of anaphors and their antecedents, and col-lecting a training corpus annotated with them."
"They do not mention all of these explicitly but emphasize the features POS-tag, grammatical role, semantic class and distance."
The set of semantic classes they use appears to be rather elaborated and highly domain-dependent.
"They mention that it was important for the training data to contain transitive positives, i.e., all possible coreference relations within an anaphoric chain."
They dis-tinguish between features which focus on individ-ual noun phrases (e.g. Does noun phrase contain a name?) and features which focus on the anaphoric relation (e.g. Do both share a common NP?).
It was criticized[REF_CITE]that the features used[REF_CITE]are highly id- iosyncratic and applicable only to one particular do-main.
"However, only a defined subset of all possible ref-erence resolution cases was considered relevant in the MUC-5 task description, e.g., only entity refer-ences."
"For this case, the domain-dependent features may have been particularly important, making it dif-ficult to compare the results of this approach to oth-ers working on less restricted domains."
"They also report on the relative contribution of the features where the three features weak string identity, alias (which maps named entities in order to resolve dates, per-son names, acronyms, etc.) and appositive seem to cover most of the cases (the other nine features con-tribute only 2.3% F-measure for MUC-6 texts and 1% F-measure for MUC-7 texts)."
"They only used pairs of anaphors and their closest antecedents as positive examples in training, but evaluated accord-ing[REF_CITE]."
"They use the features shown in Table 2, all of which are obtained auto-matically without any manual tagging."
The feature semantic class used[REF_CITE]seems to be a domain-dependent one which can only be used for the MUC domain and similar ones.
"Our corpus consists of 242 short German texts (to-tal 36924 tokens) about sights, historic events and persons in Heidelberg."
The average length is 151 to- kens.
The texts were POS-tagged using TnT[REF_CITE].
"A basic identification of markables (refer-ring expressions, i.e. NPs) was obtained by using the NP-Chunker Chunkie[REF_CITE]."
The POS-tagger was also used for assigning attributes like e.g. the NP form to markables.
The automatic annotation was followed by a manual correction and annotation phase in which the markables were anno-tated with further tags (e.g. semantic class).
In this phase manual coreference annotation was performed as well.
In our annotation coreference is represented in terms of a member attribute on markables.
Mark-ables with the same value in this attribute are con-sidered coreferring expressions.
The annotation was performed by two students.
The reliability of the an-notations was checked using the kappa statistic[REF_CITE].
"The problem of coreference resolution can easily be formulated as a binary classification: Given a pair of potential anaphor and potential antecedent, clas-sify as positive if the antecedent is in fact the closest antecedent, and as negative otherwise."
In anaphoric chains only the immediately adjacent pairs are clas-sified as positive.
We generated data suitable as in-put to a machine learning algorithm from our corpus using a straightforward algorithm which combined potential anaphors and their potential antecedents.
"We then applied the following filters to the resulting pairs: Discard an antecedent-anaphor pair if the anaphor is an indefinite NP, if one entity is embedded into the other, e.g. if the potential anaphor is the head of the poten-tial antecedent NP (or vice versa), if both entities have different values in their se-mantic class attributes 1 , if either entity has a value other than 3rd person singular or plural in its agreement feature, if both entities have different values in their agreement features 2 ."
"For some texts, these heuristics (which were ap-plied to the entire corpus) reduced to up to 50% the potential anaphor-antecedent pairs all of which would have been negative cases."
We consider the cases discarded as irrelevant because they do not contribute any knowledge for the classifier.
"After ap-plication of the filters, the remaining candidate pairs were labeled as follows:"
Pairs of anaphors and their direct (i.e. clos-est) antecedents were labeled P.
This means that each anaphoric expression produced ex-actly one positive instance.
Pairs of anaphors and those non-antecedents which occurred closer to the anaphor than the direct antecedent were labeled N.
"The number of negative instances that each expression pro-duced thus depended on the number of non-antecedents occurring between the anaphor and the direct antecedent (or, the beginning of the text if there was none)."
Pairs of anaphors and non-antecedents which oc-cured further away than the direct antecedent as well as pairs of anaphors and non-direct (transitive) an-tecedents were not considered in the data sets.
"The features for our study were selected according to three criteria: relevance according to previous research, low annotation cost and/or high reliability of automatic tagging, domain-independence."
We distinguish between features assigned to noun phrases and features assigned to the potential coref-erence relation.
All features are listed in Table 3 to-gether with their respective possible values.
The grammatical function of referring expres-sions has often been claimed to be an important fac-tor for reference resolution and was therefore in-cluded (features [Footnote_2] and 6).
2 This filter applies only if the anaphor is a pronoun. This restriction of the filter is necessary because German allows for cases where an antecedent is referred back to by a non-pronoun anaphor which has a different grammatical gender.
The surface realization of referring expressions seems to have an influence on coreference relations as well (features 3 and 7).
"Since we use a German corpus and in this language the gender and the semantic class do not necessar-ily coincide (i.e., objects are not necessarily neuter as they are in English) we also provide a semantic class feature (5 and 9) which captures the difference between human, concrete objects, and abstract ob-jects."
"This basically corresponds to the gender at-tribute in English, for which we introduced an agree-ment feature (4 and 8)."
"The feature wdist (10) cap-tures the distance in words between anaphor and an-tecedent, while the feature ddist (11) does the same in terms of sentences and mdist (12) in terms of markables."
"The equivalence in grammatical func-tion between anaphor and potential antecedent is captured in the feature syn par (13), which is true if both anaphor and antecedent are subjects or both are objects, and false in the other cases."
The string ident feature (14) appears to be of major importance since it provides for high precision in reference resolution (it almost never fails) while the substring match fea-ture (15) could potentially provide better recall.
"Using the features of Table 3, we trained decision tree classifiers using C5.0, with standard settings for pre and post pruning."
"As several features are dis-crete, we allowed the algorithm to use subsets of feature values in questions such as “Is ana npform in PPER, PPOS, PDS ?”."
"We also let C5.0 construct rules from the decision trees, as we found them to give superior results."
"In our experiments, the value of the ana semanticclass attribute was reset to miss-ing for pronominal anaphors, because in a realistic setting the semantic class of a pronoun obviously is not available prior to its resolution."
Always guessing the by far more frequent negative class would give an er-ror rate of 2.88% (70019 out of 72093 cases).
"The precision for finding positive cases is 88.60%, the recall is 45.32%."
The equally weighted F-measure [Footnote_3] is 59.97%.
3 computed as
Since we were not satisfied with this result we examined the performance of the features.
"Surpris-ingly, against our linguistic intuition the ana npform feature appeared to be the most important one."
"Thus, we expected considerable differences in the perfor-mance of our classifier with respect to the NP form of the anaphor under consideration."
We split the data into subsets defined by the NP form of the anaphor and trained the classifier on these data sets.
"The re-sults confirmed that the classifier performed poorly on definite NPs (defNP) and demonstrative pronouns (PDS), moderately on proper names (NE) and quite good on personal pronouns (PPER) and possessive pronouns (PPOS) (the results are reported in Ta-ble 4)."
"As definite NPs account for 792 out of 2074 (38.19%) of the positive cases (and for 48125 (66.75%) of all cases), it is evident that the weak performance for the resolution of definite NPs, es-pecially the low recall of only 8.71% clearly impairs the overall results."
"Demonstrative pronouns appear only in 0.87% of the positive cases, so the inferior performance is not that important."
"Proper names (NE) however are more problematic, as they have to be considered in 644 or 31.05% of the positive cases (22.96% of all)."
"Since definite noun phrases constitute more than a third of the anaphoric expressions in our corpus, we investigated why the resolution performed so poorly for these cases."
"The major reason may be that the resolution algorithm relies on surface features and does not have access to world or domain knowl-edge, which we did not want to depend upon since we were mainly interested in cheap features."
"How-ever, the string ident and substring match features did not perform very well either."
The string ident feature had a very high precision (it almost never failed) but a poor recall.
The substring match fea-ture was not too helpful either as it does not trigger in many cases.
"So, we investigated ways to raise the recall of the string ident and substring match fea-tures without losing too much precision."
"A look at some relevant cases (Table 5) sug-gested that a large number of anaphoric defi-nite NPs shared some substring with their an-tecedent, but they were not identical nor com-pletely included."
What is needed is a weakened form of the string ident and substring match fea-tures.
Other researchers[REF_CITE]used information about the syntactic structure and compared only the syntactic heads of the phrases.
"However, the feature used[REF_CITE]is neither sufficient nor language dependent, the one used[REF_CITE]is not cheap since it relies on a syntactic analysis."
We were looking for a feature which gave us the improvements of the features used by other re-searchers without their associated costs.
"Hence we considered the minimum edit distance (MED)[REF_CITE], which has been used for spelling correction and in speech recognizer evalu-ations (termed “accuracy” there) in the past."
"The MED computes the similarity of strings by taking into account the minimum number of editing oper-ations (substitutions, insertions, deletions) needed to transform one string into the other (see also Jurafsky and Martin (2000, p.153ff. and p.271))."
We included MED into our feature set by comput-ing one value for each editing direction.
Both val-ues share the number of editing operations but they differ when anaphor and antecedent have a differ-ent length.
"The features ante med (16) and ana med (17) are computed from the number of substitutions &lt; , insertions = , deletions and the length of the po-tential antecedent or anaphor as in Table 6."
The inclusion of the[REF_CITE]and 17 led to a significant improvement (Table 7).
"The F-measure is improved to 67.98%, an improvement of about 8%."
"Considering the classifiers trained and tested on the data partitions according to ana npform, we can see that the improvements mainly stem from defNP and"
"With respect to definite NPs we gained about 18% F-measure, with respect to proper names about 11% F-measure."
"For pronouns, the results did not vary much."
It is common practice to evaluate coreference reso-lution systems according to a scheme originally de-veloped for MUC evaluation[REF_CITE].
"In order to be able to apply it to our classifier, we first implemented a simple reference resolution al-gorithm."
This algorithm incrementally processes a real text by iterating over all referring expressions.
"Upon encountering a possibly anaphoric expression, it moves upwards (i.e. in the direction of the be-ginning of the text) and submits each pair of po-tential anaphor and potential antecedent to a clas-sifier trained on the features described above."
"For the reasons mentioned in Section 4.2, the value of the ana semanticclass attribute is reset to missing if the potential anaphor is a pronominal form."
The algorithm then selects the first (if any) pair which the classifier labels as coreferential.
"Once a text has been completely processed, the resulting coref-erence classes are evaluated by comparing them to the original annotation according to the scheme pro-posed[REF_CITE]."
This scheme takes into account the particularities of coreference reso-lution by abstracting from the question if individ-ual pairs of anaphors and antecedents are found.
"Instead, it focusses on whether sets of coreferring expressions are correctly identified."
"In contrast to the experiments reported in Section 4.2 and 4.4, our algorithm did not use a C5.0, but a J48 [Footnote_4] decision tree classifier, which is a Java re-implementation of"
"4 Part of the Weka machine learning library, cf.[URL_CITE]"
"This was done for technical reasons, J48 being more easily integrated into our system."
Accompany-ing experimentation revealed that J48’s performance is only slightly inferior to that of C5.0 for our data.
In this paper we described the influence of features based on the minimum edit distance (MED) be-tween anaphor and antecedent on reference resolu-tion.
"Though previous research used several differ-ent string similarity measures, to our knowledge, the MED feature was not used in previous work on ref-erence resolution."
We showed that the feature led to a significant improvement over the standard set of features we started with.
It improved the recall for definite NPs and proper names considerably with-out losing too much precision.
"Also, it did not have any negative effect on pronouns."
The MED feature is easy to compute and language and domain inde-pendent.
"In contrast, features used in previous work were either language dependent (e.g. the weak string identity feature as used[REF_CITE]), do-main dependent (their alias feature or similar fea-tures used[REF_CITE]), or relied on information on the syntactic structure[REF_CITE]."
We consider the MED feature as a generalization of these features.
It is more abstract than the features used by other researchers but deliv-ers similar information.
We showed that our approach performs very well for personal and possessive pronouns and for proper names.
"For definite NPs, although they benefit from the MED features as well, there is still much room for improvement."
We are curious to investigate fur-ther “cheap” features and compare them to what could be obtained when taking domain or world knowledge into account.
"The work presented here has been partially funded by the German Ministry of Research and Technology as part of the E MBASSI project (01[REF_CITE]D/2, 01[REF_CITE]S 8), by Sony International (Europe) GmbH and by the Klaus Tschira Foundation."
"We would like to thank our an-notators Anna Björk Nikulásdôttir, Berenike Loos and Lutz Wind."
Voicemail is not like email.
Even such ba-sic information as the name of the caller/ sender or a phone number for returning calls is not represented explicitly and must be obtained from message transcripts or other sources.
We discuss techniques for doing this and the challenges these tasks present.
"When you’re away from the phone and someone takes a message for you, at the very least you’d ex-pect to be told who called and whether they left a number for you to call back."
"If the same call is picked up by a voicemail system, even such basic in-formation like the name of the caller and their phone number may not be directly available, forcing one to listen to the entire message [Footnote_1] in the worst case."
1 The average message length in the corpus described below is 36 seconds.
"By contrast, information about the sender of an email message has always been explicitly represented in the message headers, starting with early standard-ization attempts[REF_CITE]and including the two decade old current standard[REF_CITE]."
"Applications that aim to present voicemail messages through an email-like interface – take as an example the idea of a “uniform inbox” presentation of email, voicemail, and other kinds of messages [Footnote_2] – must deal with the problem of how to obtain information anal-ogous to what would be contained in email headers."
"2 Similar issues arise with FAX messages, for example."
"Here we will discuss one way of addressing this problem, treating it exclusively as the task of extract-ing relevant information from voicemail transcripts."
"In practice, e.g. in the context of a sophisticated voicemail front-end[REF_CITE]that is tightly integrated with an organization-wide voice-mail system and private branch exchange (PBX), ad-ditional sources of information may be available: the voicemail system or the PBX might provide infor-mation about the originating station of a call, and speaker identification can be used to match a caller’s voice against models of known callers[REF_CITE]."
"Restricting our attention to voicemail transcripts means that our focus and goals are sim-ilar to those[REF_CITE], but the features and techniques we use are very different."
"While the present task may seem broadly similar to named entity extraction from broadcast news[REF_CITE], it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very dif-ferent from broadcast news and certain aspects of this structure can be exploited for extracting caller names."
"While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall."
"Our phone number extrac-tor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the de-sirable candidates."
"This allows for more or less inde- pendent optimization of recall and precision, some-what similar to the PNrule classifier learner[REF_CITE]."
"We shall see that hand-crafted rules achieve very good recall, just[REF_CITE]had observed, and the prun-ing phase successfully eliminates most undesirable candidates without affecting recall too much."
Over-all performance of our method is better than if we employ a log-linear model with trigram features.
The success of the method proposed here is also due to the use of a rich set of features for candi-date classification.
"For example, the majority of phone numbers in voicemail messages has either four, seven, or ten digits, whereas nine digits would indicate a social security number."
In our two-phase approach it is straightforward for the second-phase classifier to take the length of a candidate phone number into account.
"On the other hand, standard named entity taggers that use trigram features do not exploit this information, and doing so would entail significant changes to the underlying models and pa-rameter estimation procedures."
The rest of this paper is organized as follows.
A brief overview of the data we used in §2 is followed by a discussion of methods for extracting two kinds of caller information in §3.
"Methods for extracting telephone numbers are discussed in §4, and §5 sum-marizes and concludes."
"Development and evaluation was done using a pro-prietary corpus of almost 10,000 voicemail mes-sages that had been manually transcribed and marked up for content."
Some more details about this corpus can be found[REF_CITE].
The relevant content labeling is perhaps best illustrated with an (anonymized) excerpt form a typical mes-sage transcript: hgreetingi hi Jane h/greetingi hcalleri this is Pat Caller h/calleri I just wanted to I know you’ve probably seen this or maybe you already know about it . . . so if you could give me a call at htelnoi one two three four five h/telnoi when you get the message I’d like to chat about it hope things are well with you hclosingi talk to you soon h/closingi
"This transcript is representative of a large class of messages that start out with a short greeting fol-lowed by a phrase that identifies the caller either by name as above or by other means (‘hi, it’s me’)."
"A phone number may be mentioned as part of the caller’s self-identification, or is often mentioned near the end of the message."
"It may seem natu-ral and obvious that voicemail messages should be structured in this way, and this prototypical struc-ture can therefore be exploited for purposes of lo-cating caller information or deciding whether a digit string constitutes a phone number."
The next sections discuss this in more detail.
"The corpus was partitioned into two subsets, with 8120 messages used for development and 1869 for evaluation."
Approximately 5% of all messages are empty.
Empty messages were not discarded from the evaluation set since they constitute realistic sam-ples that the information extraction component has to cope with.
The development set contains 7686 non-empty messages.
"Of the non-empty messages in the development set, 7065 (92%) transcripts contain a marked-up caller phrase."
"Extracting caller information can be broken down into two slightly different tasks: we might want to reproduce the existing caller annota-tion as closely as possible, producing caller phrases like ‘this is Pat Caller’ or ‘it’s me’; or we might only be interested in caller names such as ‘Pat Caller’ in our above example."
"We make use of the fact that for the overwhelming majority of cases, the caller’s self-identification occurs somewhere near the begin-ning of the message."
Most caller phrases tend to start one or two words into the message.
This is because they are typi-cally preceded by a one-word (‘hi’) or two-word (‘hi Jane’) greeting.
Figure 1 shows the empiri-cal distribution of the beginning of the caller phrase across the 7065 applicable transcripts in the devel-opment data.
"As can be seen, more than 97% of all caller phrases start somewhere between one and seven words from the beginning of the message, though in one extreme case the start of the caller phrase occurred 135 words into the message."
"These observations strongly suggest that when ex-tracting caller phrases, positional cues should be taken into account."
"This is good news, especially since intrinsic features of the caller phrase may not be as reliable: a caller phrase is likely to contain names that are problematic for an automatic speech recognizer."
"While this is less of a problem when evaluating on manual transcriptions, the experience reported[REF_CITE]suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name ex-traction on automatically generated transcripts."
"We therefore avoid using anything but a small number of greetings and commonly occurring words like ‘hi’, ‘this’, ‘is’ etc. and a small number of common first names for extracting caller phrases and use po-sitional information in addition to word-based fea-tures."
We locate caller phrases by first identifying their start position in the message and then predicting the length of the phrase.
The empirical distribu-tion of caller phrase lengths in the development data is shown in Figure 2.
"Most caller phrases are be-tween two and four words long (‘it’s Pat’, ‘this is Pat Caller’) and there are moderately good lexical indicators that signal the end of a caller phrase (‘I’, ‘could’, ‘please’, etc.)."
"Again, we avoid the use of names as features and rely on a small set of fea-tures based on common words, in addition to phrase length, for predicting the length of the caller phrase."
"We have thus identified two classes of features that allow us to predict the start of the caller phrase relative to the beginning of the message, as well as the end of the caller phrase relative to its start."
"Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task."
A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand.
We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory.
"Since a direct comparison to the log-linear named entity tagger described[REF_CITE](we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with tri-gram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins)."
"Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP log-linear repeats the results of the best model[REF_CITE]; row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section."
"The numbers could be made to look better by using con-tainment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material."
"While this may be more useful in practice (see be-low), it is not the objective that was maximized dur-ing training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts."
"While the results for the approach proposed here appear clearly worse than those reported[REF_CITE], we hasten to point out that this is most likely not due to any difference in the cor-pora that were used."
"This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model[REF_CITE]by using a generic named entity tagger that was not adapted in any way to the par-ticular task at hand."
"The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly."
"In fact, it seems rather likely that the log-linear models and the fea-tures they employ over-fit the training data."
"This becomes clearer when one evaluates on un-seen transcripts produced by an automatic speech recognizer (ASR), [Footnote_3] as summarized in Table 2."
3 An automatic transcription is the single best word hypoth-esis of the ASR for a given voicemail message.
Rows HZP strict and HZP containment repeat the figures for the best model[REF_CITE]when evaluated on automatic transcriptions.
"The differ-ence is that HZP strict uses the strict evaluation cri-terion described above, whereas HZP containment uses the weaker criterion of containment, i.e., an extracted phrase counts as correct if it contains ex-actly one whole actual phrase."
Row JA containment summarizes the performance of our approach when evaluated on 101 unseen automatically transcribed messages.
"Since we did not have any labeled au-tomatic transcriptions available to compare with the predicted caller phrase labels using the strict crite-rion, we only report results based on the weaker criterion of containment."
"In fact, we count caller phrases as correct as long as they contain the full name of the caller, since this is the common denom-inator in the otherwise somewhat heterogeneous la-beling of our training corpus; more on this issue in the next section."
"The difference between the approach[REF_CITE]and ours may be partly due to the perfor-mance of the ASR components:[REF_CITE]report a word error rate of ‘about 35%’, whereas we used a recognizer[REF_CITE]with a word error rate of only 23%."
"Still, the reduced perfor-mance of the HZP model on ASR transcripts com-pared with manual transcripts is points toward over-fitting, or reliance on features that do not generalize to ASR transcripts."
"Our main approach, on the other hand, uses classifiers that are extremely knowledge-poor in comparison with the many features of the log-linear models for the various named entity tag-gers, employing no more than a few dozen categori-cal features."
"Extracting an entire caller phrase like ‘this is Pat Caller’ may not be all that relevant in practice: the prefix ‘this is’ does not provide much useful infor-mation, so simply extracting the name of the caller should suffice."
This is more or less a problem with the annotation standard used for marking up voice-mail transcripts.
We decided to test the effects of changing that standard post hoc.
"This was relatively easy to do, since proper names are capitalized in the message transcripts."
We heuristically identify caller names as the leftmost longest contiguous sub- sequence of capitalized words inside a marked-up caller phrase.
This leaves us with 6731 messages with caller names in our development data. [Footnote_4]
"4 The vast majority of messages that do not mention a name as part of their caller phrase employ the caller phrase ‘it’s me’, which would be easy to detect and treat separately."
"As we did for caller phrases, we briefly examine the distributions of the start position of caller names (see Figure 3) as well as their lengths (see Figure 4)."
Comparing the entropies of the empirical distribu-tions with the corresponding ones for caller phrases suggests that we might be dealing with a simpler extraction task here.
"The entropy of the empirical name length distribution is not much more than one bit, since predicting the length of a caller name is mostly a question of deciding whether a first name or full name was mentioned."
The performance comparison in Table 3 shows that we are in fact dealing with a simpler task.
No-tice however that our method has not changed at all.
"We still use one classifier to predict the beginning of the caller name and a second classifier to predict its length, with the same small set of lexical features that do not include any names other than a handful of common first names."
"The development data contain 5303 marked-up phone numbers, for an average of almost 0.7 phone numbers per non-empty message."
"These phone numbers fall into the following categories based on their realization: • 4472 (84%) consist exclusively of spoken num-bers • 679 (13%) consist of spoken numbers and the words ‘area’, ‘code’, and ‘extension’ • 152 (3%) have additional material, due to cor-rections, hesitations, fragments, and question-able markup"
"Note that phone numbers in the North American Numbering Plan are either ten or seven digits long, depending on whether the Numbering Plan Area code is included or not."
"Two other frequent lengths for phone numbers in the development data are four (for internal lines) and, to a lesser extent, eleven (when the long distance dialing prefix is included, as in ‘one eight hundred . . . ’)."
"This allows us to formulate the following baseline approach: find all maximal substrings consisting of spoken digits (‘zero’ through ‘nine’) and keep those of length four, seven, and ten."
"Simple as it may seem, this approach (which we call digits below) performs surprisingly well."
"Its precision is more than 78%, partly because in our corpus there do not occur many seven or ten digit numbers that are not phone numbers."
Named entity taggers based on conditional mod-els with trigram features are not particularly suited for this task.
The reason is that trigrams do not pro-vide enough history to allow the tagger to judge the length of a proposed phone number: it inserts begin-ning and end tags without being able to tell how far apart they are.
"Data sparseness is another problem, since we are dealing with 1000 distinct trigrams over digits alone, so a different event model that replaces all spoken digits with the same representative token might be better suited, also because it avoids over-fitting issues like accidentally learning area codes and other number patterns that are frequent in the development data."
"However, there is a more serious problem."
"Even if the distance between the start and end tags that a named entity tagger predicts could be taken into ac-count, this would not help with all spoken renditions of phone numbers."
"For example, ‘327-1025’ could be read aloud using only six words (‘three two seven ten twenty five’), and might be incorrectly rejected because it appears to be of a length that is not very common for phone numbers."
"We therefore approach the phone number extrac-tion task differently, using a two-phase procedure."
In the first phase we use a hand-crafted grammar to propose candidate phone numbers.
"This avoids all of the problems mentioned so far, yet the complex-ity of the task remains manageable because of the rather simple structure of most phone numbers in our development data noted above."
"The advantage is that it allows us to simultaneously convert spo-ken digits and numbers to a numeric representation, whose length can then be used as an important fea-ture for deciding whether to keep or throw away a candidate."
"Note that such a conversion process is desirable in any case, since a text-based application would presumably want to present digit strings like ‘327-1025’ to a user, rather than ‘three two seven ten twenty five’."
"This conversion step is not entirely trivial, though: for example, one might transcribe the spoken words ‘three hundred fourteen ninety nine’ as either ‘300-1499’ or ‘314.99’ depending on whether they are preceded by ‘call me back at’ vs. ‘I can sell it to you for’, for example."
"But since we are only interested in finding phone numbers, the extrac- tion component can treat all candidates it proposes as if they were phone numbers."
Adjustments of the hand-crafted grammar were only made in order to increase recall on the devel-opment data.
"The grammar should locate as many actual phone numbers in the development corpus as possible, but was free to also propose spurious can-didates that did not correspond to marked-up phone numbers."
"While it has recently been argued that such separate optimization of recall and precision is generally desirable for certain learning tasks[REF_CITE], the main advantage in connection with hand-crafted compo-nents is simplified development."
"Since we noted above that 97% of all phone numbers in our devel-opment data are expressed fairly straightforwardly in terms of digits, numbers, and a few other words particular to the phone number domain, we might expect to achieve recall figures close to 97% without doing anything special to deal with the remaining 3% of difficult cases."
"It was very easy to achieve this recall figure on the development data, while the ratio of proposed phone numbers to actual phone numbers was about 3.2 at worst. [Footnote_5]"
5 It would of course be trivial to achieve 100% recall by ex-tracting all possible substrings of a transcript. The fact that our grammar extracts only about three times as many phrases as needed is evidence that it falls within the reasonable subset of possible extraction procedures.
"A second phase is now charged with the task of weeding through the set of candidates proposed dur-ing the first phase, retaining those that correspond to actual phone numbers."
"This is a simple binary clas-sification task, and again many different techniques can be applied."
"As a baseline we use a classifier that accepts any candidate of length four or more (now measured in terms of numeric digits, rather than spoken words), and rejects candidates of length three and less."
"Without this simple step (which we refer to as prune below), the precision of our hand-crafted extraction grammar is only around 30%, but by pruning away candidate phone numbers shorter than four digits precision almost doubles while re-call is unaffected."
We again used a decision tree learner to automat-ically infer a classifier for the second phase.
"The features we made available to the learner were the length of the phone number in numeric digits, its distance from the end of the message, and a small number of lexical cues in the surrounding context of a candidate number (‘call’, ‘number’, etc.)."
This ap-proach (which we call classify below) increases the precision of the combined two steps to acceptable levels without hurting recall too much.
A comparison of performance results is presented in Table 4.
Rows HZP rules and HZP log-linear re-fer to the rule-based baseline and the best log-linear model[REF_CITE]and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths.
Our main results appear in the re-maining rows.
"The performance of our hand-crafted extraction grammar (in row JA extract) was about what we had seen on the development data before, with recall being as high as one could reasonably ex-pect."
"As mentioned above, using a simple pruning step in the second phase (see JA extract + prune) results in a doubling of precision and leaves recall essentially unaffected (a single fragmentary phone number was wrongly excluded)."
"Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall."
Our two-phase procedure outper-forms all other methods we considered.
We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction.
"The results are summarized in Table 5, which also re-peats the best results[REF_CITE], us-ing the same terminology as earlier: rows HZP strict and HZP containment refer to the best model[REF_CITE]– corresponding to row HZP log-linear in Table 4 – when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model – corresponding to row JA extract + classify in Ta-ble 4."
It is not very plausible that the differences be-tween the approaches in Table 5 would be due to a difference in the performance of the ASR compo-nents that generated the message transcripts.
"From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate."
"Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used[REF_CITE]and ourselves to be extremely similar in terms of mark-up of phone numbers."
So the observed performance difference is most likely due to the difference in extraction methods.
The novel contributions of this paper can be summa-rized as follows: • We demonstrated empirically that positional cues can be an important source of information for locating caller names and phrases. • We showed that good performance on the task of extracting caller information can be achieved using a very small inventory of lexical and po-sitional features. • We argued that for extracting telephone num-bers it is extremely useful to take the length of their numeric representation into account.
Our grammar-based extractor translates spoken numbers into such a numeric representation. • Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall.
"This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline[REF_CITE]. • The combined performance of our simple ex-traction grammar and the second-phase clas-sifier exceeded the performance of all other methods, including the current state of the art[REF_CITE]."
Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks.
"Generic methods like the named entity tagger used[REF_CITE]may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extract-ing phone numbers."
"We also believe that using all available lexical information for extracting caller in-formation can easily lead to over-fitting, which can partly be avoid by not relying on names being tran-scribed correctly by an ASR component."
"In practice, determining the identity of a caller might have to take many diverse sources of infor-mation into account."
"The self-identification of a caller and the phone numbers mentioned in the same message are not uncorrelated, since there is usually only a small number of ways to reach any particular caller."
"In an application we might therefore try to use a combination of speaker identificati[REF_CITE], caller name extraction, and recognized phone numbers to establish the identity of the caller."
An investigation of how to combine these sources of information is left for future research.
